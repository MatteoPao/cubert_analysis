{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_singlepredict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+h4/ZDn0aMCQUqgMbC+0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoPao/cubert_keras/blob/main/Test_singlepredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYudVZKz8TU1"
      },
      "outputs": [],
      "source": [
        "!git clone https://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@github.com/MatteoPao/cubert_keras.git\n",
        "%cd cubert_keras/src/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive                                               \n",
        "drive.mount('/content/drive/')\n",
        "%cp -r /content/drive/MyDrive/Tesi_ModelData/cubert_pretrained_model_exceptions /content/cubert_keras/cubert_pretrained_model_exceptions"
      ],
      "metadata": {
        "id": "I2wLi6UJ8qxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_transformer\n",
        "!pip install tensor2tensor\n",
        "!pip install bert-tensorflow\n",
        "!pip install javalang"
      ],
      "metadata": {
        "id": "g48B5iuh9RiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cubert_keras.src.keras_bert import load_trained_model_from_checkpoint, get_checkpoint_paths\n",
        "from cubert.full_cubert_tokenizer import FullCuBertTokenizer, CuBertExceptionClassificationProcessor, InputExample\n",
        "from cubert import tokenizer_registry\n",
        "import numpy as np\n",
        "\n",
        "paths = get_checkpoint_paths(\"../cubert_pretrained_model_exceptions\")\n",
        "\n",
        "model = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, training=True, seq_len=128)\n",
        "print(\"model loaded\")\n",
        "\n",
        "tokenizer = FullCuBertTokenizer(code_tokenizer_class=tokenizer_registry.TokenizerEnum.PYTHON.value, vocab_file=paths.vocab)\n",
        "print(\"tokenizer loaded\")\n",
        "\n",
        "processor = CuBertExceptionClassificationProcessor()\n",
        "label_list = processor.get_labels()\n"
      ],
      "metadata": {
        "id": "CWo7vSz18yJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phenotype = \"def resolve_include(self, t):\\n        \\\"\\\"\\\"Resolve a tuple-ized #include line.\\n\\n        This handles recursive expansion of values without \\\"\\\" or <>\\n        surrounding the name until an initial \\\" or < is found, to handle\\n                #include FILE\\n        where FILE is a #define somewhere else.\\n        \\\"\\\"\\\"\\n        s = t[1]\\n        while not s[0] in '<\\\"':\\n            #print \\\"s =\\\", s\\n            try:\\n                s = self.cpp_namespace[s]\\n            except __HOLE__:\\n                m = function_name.search(s)\\n                s = self.cpp_namespace[m.group(1)]\\n                if callable(s):\\n                    args = function_arg_separator.split(m.group(2))\\n                    s = s(*args)\\n            if not s:\\n                return None\\n        return (t[0], s[0], s[1:-1])\"\n",
        "\n",
        "print(\"Fenotipo di input:\\\n",
        "\\n--------------------------------------\\n\" \n",
        "+ phenotype + \n",
        "\"\\n--------------------------------------\")\n",
        "\n",
        "guess_nt = InputExample(guid=\"test-0\", text_a=phenotype, text_b=None, label=\"ValueError\")\n",
        "guess = tokenizer.convert_single_example(guess_nt, label_list, 128)\n",
        "\n",
        "prediction = model.predict([np.expand_dims(np.array(guess.input_ids), axis=0),\n",
        "                                  np.expand_dims(np.array(guess.segment_ids), axis=0),\n",
        "                                  np.expand_dims(np.array(guess.input_mask), axis=0)])\n",
        "\n",
        "print(\"OUTPUT:\")\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "CWbYOj1sBL1Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{"function": "\n\ndef getUpdatedBatchJob(self, maxWait):\n    while True:\n        try:\n            (jobID, status, wallTime) = self.updatedJobsQueue.get(timeout=maxWait)\n        except Empty:\n            return None\n        try:\n            self.runningJobs.remove(jobID)\n        except KeyError:\n            pass\n        else:\n            return (jobID, status, wallTime)\n", "label": 0}
{"function": "\n\ndef _validate_simple_authn(self, username, credentials):\n    '\\n        When the login() method is called, this method is used with the \\n        username and credentials (e.g., password, IP address, etc.). This\\n        method will only check the SimpleAuthn instances.\\n        '\n    try:\n        (login, role_name, user_auths) = self._db.retrieve_role_and_user_auths(username)\n    except DbUserNotFoundError:\n        return self._process_invalid()\n    errors = False\n    for user_auth in user_auths:\n        if user_auth.is_simple_authn():\n            try:\n                authenticated = user_auth.authenticate(login, credentials)\n            except:\n                log.log(LoginManager, log.level.Warning, ('Username: %s with user_auth %s: ERROR' % (login, user_auth)))\n                log.log_exc(LoginManager, log.level.Warning)\n                errors = True\n                traceback.print_exc()\n                continue\n            if authenticated:\n                log.log(LoginManager, log.level.Debug, ('Username: %s with user_auth %s: SUCCESS' % (login, user_auth)))\n                return ValidDatabaseSessionId(login, role_name)\n            else:\n                log.log(LoginManager, log.level.Warning, ('Username: %s with user_auth %s: FAIL' % (login, user_auth)))\n    if errors:\n        raise LoginErrors.LoginError('Error checking credentials. Contact administrators!')\n    return self._process_invalid()\n", "label": 0}
{"function": "\n\ndef test_error_load_single_field_type(single_schema):\n    (data, errors) = single_schema.load({\n        'child': {\n            'id': 'foo',\n        },\n    })\n    assert (not data)\n    assert (errors == {\n        'child': {\n            'id': [fields.Integer().error_messages['invalid']],\n        },\n    })\n", "label": 0}
{"function": "\n\ndef format_author(self, entry):\n    try:\n        persons = entry.persons['author']\n        if (sys.version_info[0] == 2):\n            authors = [unicode(au) for au in persons]\n        elif (sys.version_info[0] == 3):\n            authors = [str(au) for au in persons]\n    except KeyError:\n        authors = ['']\n    authors = self.strip_chars('; '.join(authors))\n    return authors\n", "label": 0}
{"function": "\n\ndef test_try_failure_bad_arg(self):\n    rv = self.app.get('/trivial_fn?nothing=1')\n    assert (rv.status_code == 200)\n    data = rv.data.decode('utf8')\n    jsn = json.loads(data)\n    assert (jsn['success'] == False), 'We expect this call failed as it has the wrong argument'\n", "label": 0}
{"function": "\n\ndef test_delete_all_lines_inversed(self):\n    text = '1\\n22\\n3\\n44\\n5\\n66\\n'\n    self.fillAndClear(text)\n    self.buffer.delete(Range(6, 1))\n    assert (str(self.buffer) == '')\n    assert (self.buffer.lines == [])\n    assert (self.deleted('afterPosition') == Position(1, 1))\n    assert (self.deleted('startPosition') == Position(7, 1))\n", "label": 0}
{"function": "\n\ndef run_osprey(self, config):\n    '\\n        Run osprey-worker.\\n\\n        Parameters\\n        ----------\\n        config : str\\n            Configuration string.\\n        '\n    (fh, filename) = tempfile.mkstemp(dir=self.temp_dir)\n    with open(filename, 'wb') as f:\n        f.write(config)\n    args = Namespace(config=filename, n_iters=1, output='json')\n    execute_worker.execute(args, None)\n    dump = json.loads(execute_dump.execute(args, None))\n    assert (len(dump) == 1)\n    assert (dump[0]['status'] == 'SUCCEEDED'), dump[0]['status']\n", "label": 0}
{"function": "\n\ndef __init__(self, dateTime, frequency):\n    super(IntraDayRange, self).__init__()\n    assert isinstance(frequency, int)\n    assert (frequency > 1)\n    assert (frequency < bar.Frequency.DAY)\n    ts = int(dt.datetime_to_timestamp(dateTime))\n    slot = int((ts / frequency))\n    slotTs = (slot * frequency)\n    self.__begin = dt.timestamp_to_datetime(slotTs, (not dt.datetime_is_naive(dateTime)))\n    if (not dt.datetime_is_naive(dateTime)):\n        self.__begin = dt.localize(self.__begin, dateTime.tzinfo)\n    self.__end = (self.__begin + datetime.timedelta(seconds=frequency))\n", "label": 0}
{"function": "\n\ndef sample(problem, N, num_levels, grid_jump, optimal_trajectories=None, local_optimization=False):\n    \"Generates model inputs using for Method of Morris.\\n    \\n    Returns a NumPy matrix containing the model inputs required for Method of\\n    Morris.  The resulting matrix has N rows and D columns, where D is the\\n    number of parameters.  These model inputs are intended to be used with\\n    :func:`SALib.analyze.morris.analyze`.\\n    \\n    Three variants of Morris' sampling for elementary effects is supported:\\n    \\n    - Vanilla Morris\\n    - Optimised trajectories when optimal_trajectories is set (using \\n      Campolongo's enhancements from 2007 and optionally Ruano's enhancement from 2012)\\n    - Groups with optimised trajectories when optimal_trajectores is set and \\n      the problem definition specifies groups\\n    \\n    At present, optimised trajectories is implemented using a brute-force\\n    approach, which can be very slow, especially if you require more than four\\n    trajectories.  Note that the number of factors makes little difference,\\n    but the ratio between number of optimal trajectories and the sample size\\n    results in an exponentially increasing number of scores that must be\\n    computed to find the optimal combination of trajectories.  We suggest going\\n    no higher than 4 from a pool of 100 samples.\\n    \\n    Update: With local_optimization = True, it is possible to go higher than the previously suggested 4 from 100.\\n    \\n    Parameters\\n    ----------\\n    problem : dict\\n        The problem definition\\n    N : int\\n        The number of samples to generate\\n    num_levels : int\\n        The number of grid levels\\n    grid_jump : int\\n        The grid jump size\\n    optimal_trajectories : int\\n        The number of optimal trajectories to sample (between 2 and N)\\n    local_optimization : bool\\n        Flag whether to use local optimization according to Ruano et al. (2012) \\n        Speeds up the process tremendously for bigger N and num_levels.\\n        Stating this variable to be true causes the function to ignore gurobi.\\n    \"\n    if (grid_jump >= num_levels):\n        raise ValueError('grid_jump must be less than num_levels')\n    if problem.get('groups'):\n        sample = sample_groups(problem, N, num_levels, grid_jump)\n    else:\n        sample = sample_oat(problem, N, num_levels, grid_jump)\n    if optimal_trajectories:\n        assert (type(optimal_trajectories) == int), 'Number of optimal trajectories should be an integer'\n        if (optimal_trajectories < 2):\n            raise ValueError('The number of optimal trajectories must be set to 2 or more.')\n        if (optimal_trajectories >= N):\n            raise ValueError('The number of optimal trajectories should be less than the number of samples.')\n        if ((_has_gurobi == False) and (local_optimization == False) and (optimal_trajectories > 10)):\n            raise ValueError('Running optimal trajectories greater than values of 10 will take a long time.')\n        sample = compute_optimised_trajectories(problem, sample, N, optimal_trajectories, local_optimization)\n    scale_samples(sample, problem['bounds'])\n    return sample\n", "label": 1}
{"function": "\n\ndef load_proposal_rlp(self, blockhash):\n    try:\n        prlp = self.chainservice.db.get(('blockproposal:%s' % blockhash))\n        assert isinstance(prlp, bytes)\n        return prlp\n    except KeyError:\n        return None\n", "label": 0}
{"function": "\n\ndef test_dispose1():\n    h = event.HasEvents()\n\n    @h.connect('x1', 'x2')\n    def handler(*events):\n        pass\n    handler_ref = weakref.ref(handler)\n    del handler\n    gc.collect()\n    assert (handler_ref() is not None)\n    handler_ref().dispose()\n    gc.collect()\n    assert (handler_ref() is None)\n", "label": 0}
{"function": "\n\ndef facettupletrees(table, key, start='start', stop='stop', value=None):\n    '\\n    Construct faceted interval trees for the given table, where each node in\\n    the tree is a row of the table.\\n\\n    '\n    import intervaltree\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    assert (start in flds), 'start field not recognised'\n    assert (stop in flds), 'stop field not recognised'\n    getstart = itemgetter(flds.index(start))\n    getstop = itemgetter(flds.index(stop))\n    if (value is None):\n        getvalue = tuple\n    else:\n        valueindices = asindices(hdr, value)\n        assert (len(valueindices) > 0), 'invalid value field specification'\n        getvalue = itemgetter(*valueindices)\n    keyindices = asindices(hdr, key)\n    assert (len(keyindices) > 0), 'invalid key'\n    getkey = itemgetter(*keyindices)\n    trees = dict()\n    for row in it:\n        k = getkey(row)\n        if (k not in trees):\n            trees[k] = intervaltree.IntervalTree()\n        trees[k].addi(getstart(row), getstop(row), getvalue(row))\n    return trees\n", "label": 1}
{"function": "\n\ndef test_get_children_duplicates(self):\n    from psutil._compat import defaultdict\n    table = defaultdict(int)\n    for p in psutil.process_iter():\n        try:\n            table[p.ppid] += 1\n        except psutil.Error:\n            pass\n    pid = max(sorted(table, key=(lambda x: table[x])))\n    p = psutil.Process(pid)\n    try:\n        c = p.get_children(recursive=True)\n    except psutil.AccessDenied:\n        pass\n    else:\n        self.assertEqual(len(c), len(set(c)))\n", "label": 0}
{"function": "\n\ndef install_ssl_certs(instances):\n    certs = []\n    if CONF.object_store_access.public_identity_ca_file:\n        certs.append(CONF.object_store_access.public_identity_ca_file)\n    if CONF.object_store_access.public_object_store_ca_file:\n        certs.append(CONF.object_store_access.public_object_store_ca_file)\n    if (not certs):\n        return\n    with context.ThreadGroup() as tg:\n        for inst in instances:\n            tg.spawn(('configure-ssl-cert-%s' % inst.instance_id), _install_ssl_certs, inst, certs)\n", "label": 0}
{"function": "\n\ndef test_node_site():\n    s = Site(TEST_SITE_ROOT)\n    r = RootNode(TEST_SITE_ROOT.child_folder('content'), s)\n    assert (r.site == s)\n    n = Node(r.source_folder.child_folder('blog'), r)\n    assert (n.site == s)\n", "label": 0}
{"function": "\n\ndef create_debianization(distribution):\n    if exists('debian'):\n        raise NotImplementedError()\n    name = distribution.get_name()\n    name = ('python-%s' % name.replace('_', '-').lower())\n    maintainer = distribution.get_maintainer()\n    maintainer_email = distribution.get_maintainer_email()\n    if (maintainer == 'UNKNOWN'):\n        maintainer = 'CH content team'\n    if (maintainer_email == 'UNKNOWN'):\n        maintainer_email = 'pg-content-dev@chconf.com'\n    maintainer = ('%s <%s>' % (maintainer, maintainer_email))\n    version = distribution.get_version()\n    if (not version):\n        version = '0.0.0'\n    now = datetime.now()\n    utcnow = datetime.utcnow()\n    tzdiff = get_tzdiff(now, utcnow)\n    nowstring = ('%s %s' % (now.strftime('%a, %d %b %Y %H:%M:%S'), tzdiff))\n    description = distribution.get_description()\n    description = description.strip().replace('\\n', '\\n ')\n    architecture = 'all'\n    if distribution.has_ext_modules():\n        architecture = 'any'\n    copytree(join(dirname(__file__), 'default_debianization'), 'debian')\n    for (root, dirs, files) in os.walk('debian'):\n        for f in files:\n            file = join(root, f)\n            with open(file) as fin:\n                content = fin.read()\n            for (key, value) in (('#NAME#', name), ('#MAINTAINER#', maintainer), ('#VERSION#', version), ('#DATE#', nowstring)):\n                content = content.replace(key, value)\n            with open(file, 'w') as fout:\n                fout.write(content)\n    cf = ControlFile(filename='debian/control')\n    src = cf.source\n    p = cf.packages[0]\n    src['Source'] = p['Package'] = name\n    src['Maintainer'] = maintainer\n    p['Description'] = description\n    p['Architecture'] = architecture\n    install_requires = distribution.install_requires\n    if install_requires:\n        for package in install_requires:\n            p['Depends'].append(parse_setuppy_dependency(package))\n    cf.dump('debian/control')\n", "label": 1}
{"function": "\n\ndef test_column_expr(self):\n    c = Column('x', Integer)\n    is_(inspect(c), c)\n    assert (not c.is_selectable)\n    assert (not hasattr(c, 'selectable'))\n", "label": 0}
{"function": "\n\ndef _apply_filters(self, query, count_query, joins, count_joins, filters):\n    for (idx, flt_name, value) in filters:\n        flt = self._filters[idx]\n        alias = None\n        count_alias = None\n        if isinstance(flt, sqla_filters.BaseSQLAFilter):\n            path = self._filter_joins.get(flt.column, [])\n            (query, joins, alias) = self._apply_path_joins(query, joins, path, inner_join=False)\n            if (count_query is not None):\n                (count_query, count_joins, count_alias) = self._apply_path_joins(count_query, count_joins, path, inner_join=False)\n        clean_value = flt.clean(value)\n        try:\n            query = flt.apply(query, clean_value, alias)\n        except TypeError:\n            spec = inspect.getargspec(flt.apply)\n            if (len(spec.args) == 3):\n                warnings.warn(('Please update your custom filter %s to include additional `alias` parameter.' % repr(flt)))\n            else:\n                raise\n            query = flt.apply(query, clean_value)\n        if (count_query is not None):\n            try:\n                count_query = flt.apply(count_query, clean_value, count_alias)\n            except TypeError:\n                count_query = flt.apply(count_query, clean_value)\n    return (query, count_query, joins, count_joins)\n", "label": 1}
{"function": "\n\n@property\ndef vcf(self):\n    'serialize to VCARD as specified in RFC2426,\\n        if no UID is specified yet, one will be added (as a UID is mandatory\\n        for carddav as specified in RFC6352\\n        TODO make shure this random uid is unique'\n    import string\n    import random\n\n    def generate_random_uid():\n        \"generate a random uid, when random isn't broken, getting a\\n            random UID from a pool of roughly 10^56 should be good enough\"\n        choice = (string.ascii_uppercase + string.digits)\n        return ''.join([random.choice(choice) for _ in range(36)])\n    if ('UID' not in self.keys()):\n        self['UID'] = [(generate_random_uid(), dict())]\n    collector = list()\n    collector.append('BEGIN:VCARD')\n    collector.append('VERSION:3.0')\n    for key in ['FN', 'N']:\n        try:\n            collector.append(((key + ':') + self[key][0][0]))\n        except IndexError:\n            collector.append((key + ':'))\n    for prop in self.alt_keys():\n        for line in self[prop]:\n            types = self._line_helper(line)\n            collector.append((((prop + types) + ':') + line[0]))\n    collector.append('END:VCARD')\n    return '\\n'.join(collector)\n", "label": 0}
{"function": "\n\ndef draw_outlines(context, box, enable_hinting):\n    width = box.style.outline_width\n    color = box.style.get_color('outline_color')\n    style = box.style.outline_style\n    if ((box.style.visibility == 'visible') and (width != 0) and (color.alpha != 0)):\n        outline_box = ((box.border_box_x() - width), (box.border_box_y() - width), (box.border_width() + (2 * width)), (box.border_height() + (2 * width)))\n        for side in SIDES:\n            with stacked(context):\n                clip_border_segment(context, enable_hinting, style, width, side, outline_box)\n                draw_rect_border(context, outline_box, (4 * (width,)), style, styled_color(style, color, side))\n    if isinstance(box, boxes.ParentBox):\n        for child in box.children:\n            if isinstance(child, boxes.Box):\n                draw_outlines(context, child, enable_hinting)\n", "label": 1}
{"function": "\n\ndef get_dates(self, resource):\n    '\\n        Retrieve dates from mercurial\\n        '\n    try:\n        commits = subprocess.check_output(['hg', 'log', '--template={date|isodatesec}\\n', resource.path]).split('\\n')\n        commits = commits[:(- 1)]\n    except subprocess.CalledProcessError:\n        self.logger.warning(('Unable to get mercurial history for [%s]' % resource))\n        commits = None\n    if (not commits):\n        self.logger.warning(('No mercurial history for [%s]' % resource))\n        return (None, None)\n    created = parse(commits[(- 1)].strip())\n    modified = parse(commits[0].strip())\n    return (created, modified)\n", "label": 0}
{"function": "\n\ndef _unit_file(self, name):\n    for extension in ['service', 'yaml']:\n        file_path = '{0}.{1}'.format(name, extension)\n        if path.exists(file_path):\n            with open(file_path) as handle:\n                if (extension == 'service'):\n                    return handle.read()\n                data = yaml.load(handle)\n                if (self._global and ('global' in data)):\n                    return data['global']\n                if (self._name in data):\n                    return data[self._name]\n                raise ValueError('No unit found for {0}'.format(self._name))\n    raise ValueError('No unit file: '.format(name))\n", "label": 0}
{"function": "\n\ndef do_access_token_response(self, access_token, atinfo, state, refresh_token=None):\n    _tinfo = {\n        'access_token': access_token,\n        'expires_in': atinfo['exp'],\n        'token_type': 'bearer',\n        'state': state,\n    }\n    try:\n        _tinfo['scope'] = atinfo['scope']\n    except KeyError:\n        pass\n    if refresh_token:\n        _tinfo['refresh_token'] = refresh_token\n    return AccessTokenResponse(**by_schema(AccessTokenResponse, **_tinfo))\n", "label": 0}
{"function": "\n\ndef get_dir(self, path, dest='', saltenv='base', gzip=None, cachedir=None):\n    '\\n        Get a directory recursively from the salt-master\\n        '\n    ret = []\n    path = self._check_proto(path).rstrip('/')\n    separated = path.rsplit('/', 1)\n    if (len(separated) != 2):\n        prefix = ''\n    else:\n        prefix = separated[0]\n    for fn_ in self.file_list(saltenv, prefix=path):\n        try:\n            if (fn_[len(path)] != '/'):\n                continue\n        except IndexError:\n            continue\n        minion_relpath = fn_[len(prefix):].lstrip('/')\n        ret.append(self.get_file(salt.utils.url.create(fn_), '{0}/{1}'.format(dest, minion_relpath), True, saltenv, gzip))\n    try:\n        for fn_ in self.file_list_emptydirs(saltenv, prefix=path):\n            try:\n                if (fn_[len(path)] != '/'):\n                    continue\n            except IndexError:\n                continue\n            minion_relpath = fn_[len(prefix):].lstrip('/')\n            minion_mkdir = '{0}/{1}'.format(dest, minion_relpath)\n            if (not os.path.isdir(minion_mkdir)):\n                os.makedirs(minion_mkdir)\n            ret.append(minion_mkdir)\n    except TypeError:\n        pass\n    ret.sort()\n    return ret\n", "label": 1}
{"function": "\n\ndef _download_pdf(self, url, base_path):\n    local_file_path = os.path.join(base_path, 'billing-temp-document.pdf')\n    response = requests.get(url, stream=True)\n    should_wipe_bad_headers = True\n    with open(local_file_path, 'wb') as out_file:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                if should_wipe_bad_headers:\n                    pdf_header_pos = chunk.find('%PDF-')\n                    if (pdf_header_pos > 0):\n                        chunk = chunk[pdf_header_pos:]\n                    should_wipe_bad_headers = False\n                out_file.write(chunk)\n                out_file.flush()\n    return local_file_path\n", "label": 0}
{"function": "\n\ndef _dispatch(self, inst, kws):\n    assert (self.current_block is not None)\n    fname = ('op_%s' % inst.opname.replace('+', '_'))\n    try:\n        fn = getattr(self, fname)\n    except AttributeError:\n        raise NotImplementedError(inst)\n    else:\n        try:\n            return fn(inst, **kws)\n        except errors.NotDefinedError as e:\n            if (e.loc is None):\n                e.loc = self.loc\n            raise e\n", "label": 0}
{"function": "\n\ndef test_review_comments(self):\n    \"Show that one can iterate over a PR's review comments.\"\n    cassette_name = self.cassette_name('review_comments')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        for comment in p.review_comments():\n            assert isinstance(comment, github3.pulls.ReviewComment)\n", "label": 0}
{"function": "\n\ndef test_input_extra_rewrite(self):\n    self.client_job_description.rewrite_paths = True\n    extra_file = os.path.join(self.input1_files_path, 'moo', 'cow.txt')\n    os.makedirs(os.path.dirname(extra_file))\n    open(extra_file, 'w').write('Hello World!')\n    command_line = ('test.exe %s' % extra_file)\n    self.client_job_description.command_line = command_line\n    self.client.expect_command_line('test.exe /pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt')\n    self.client.expect_put_paths(['/pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt'])\n    self._submit()\n    uploaded_file1 = self.client.put_files[0]\n    assert (uploaded_file1[1] == 'input')\n    assert (uploaded_file1[0] == extra_file)\n", "label": 0}
{"function": "\n\ndef test_grad_s(self):\n    'tests that the gradients with respect to s_i are 0 after doing a mean field update of s_i '\n    model = self.model\n    e_step = self.e_step\n    X = self.X\n    assert (X.shape[0] == self.m)\n    model.test_batch_size = X.shape[0]\n    init_H = e_step.init_H_hat(V=X)\n    init_Mu1 = e_step.init_S_hat(V=X)\n    prev_setting = config.compute_test_value\n    config.compute_test_value = 'off'\n    (H, Mu1) = function([], outputs=[init_H, init_Mu1])()\n    config.compute_test_value = prev_setting\n    H = broadcast(H, self.m)\n    Mu1 = broadcast(Mu1, self.m)\n    H = np.cast[config.floatX](self.model.rng.uniform(0.0, 1.0, H.shape))\n    Mu1 = np.cast[config.floatX](self.model.rng.uniform((- 5.0), 5.0, Mu1.shape))\n    H_var = T.matrix(name='H_var')\n    H_var.tag.test_value = H\n    Mu1_var = T.matrix(name='Mu1_var')\n    Mu1_var.tag.test_value = Mu1\n    idx = T.iscalar()\n    idx.tag.test_value = 0\n    S = e_step.infer_S_hat(V=X, H_hat=H_var, S_hat=Mu1_var)\n    s_idx = S[:, idx]\n    s_i_func = function([H_var, Mu1_var, idx], s_idx)\n    sigma0 = (1.0 / model.alpha)\n    Sigma1 = e_step.infer_var_s1_hat()\n    mu0 = T.zeros_like(model.mu)\n    trunc_kl = ((- model.entropy_hs(H_hat=H_var, var_s0_hat=sigma0, var_s1_hat=Sigma1)) + model.expected_energy_vhs(V=X, H_hat=H_var, S_hat=Mu1_var, var_s0_hat=sigma0, var_s1_hat=Sigma1))\n    grad_Mu1 = T.grad(trunc_kl.sum(), Mu1_var)\n    grad_Mu1_idx = grad_Mu1[:, idx]\n    grad_func = function([H_var, Mu1_var, idx], grad_Mu1_idx)\n    for i in xrange(self.N):\n        Mu1[:, i] = s_i_func(H, Mu1, i)\n        g = grad_func(H, Mu1, i)\n        assert (not contains_nan(g))\n        g_abs_max = np.abs(g).max()\n        if (g_abs_max > self.tol):\n            raise Exception(((('after mean field step, gradient of kl divergence wrt mean field parameter should be 0, but here the max magnitude of a gradient element is ' + str(g_abs_max)) + ' after updating s_') + str(i)))\n", "label": 0}
{"function": "\n\n@app.task(bind=True)\n@only_one(key='analyze_databases_service_task', timeout=6000)\ndef analyze_databases(self, task_history=None):\n    (endpoint, healh_check_route, healh_check_string) = get_analyzing_credentials()\n    user = User.objects.get(username='admin')\n    worker_name = get_worker_name()\n    task_history = TaskHistory.register(task_history=task_history, request=self.request, user=user, worker_name=worker_name)\n    task_history.update_details(persist=True, details='Loading Process...')\n    AuditRequest.new_request('analyze_databases', user, 'localhost')\n    try:\n        analyze_service = AnalyzeService(endpoint, healh_check_route, healh_check_string)\n        with transaction.atomic():\n            databases = Database.objects.filter(is_in_quarantine=False)\n            today = datetime.now()\n            for database in databases:\n                (database_name, engine, instances, environment_name, databaseinfra_name) = setup_database_info(database)\n                for execution_plan in ExecutionPlan.objects.all():\n                    if database_can_not_be_resized(database, execution_plan):\n                        continue\n                    params = execution_plan.setup_execution_params()\n                    result = analyze_service.run(engine=engine, database=database_name, instances=instances, **params)\n                    if (result['status'] == 'success'):\n                        task_history.update_details(persist=True, details='\\nDatabase {} {} was analised.'.format(database, execution_plan.plan_name))\n                        if (result['msg'] != instances):\n                            continue\n                        for instance in result['msg']:\n                            insert_analyze_repository_record(today, database_name, instance, engine, databaseinfra_name, environment_name, execution_plan)\n                    else:\n                        raise Exception('Check your service logs..')\n        task_history.update_status_for(TaskHistory.STATUS_SUCCESS, details='Analisys ok!')\n    except Exception:\n        try:\n            task_history.update_details(persist=True, details='\\nDatabase {} {} could not be analised.'.format(database, execution_plan.plan_name))\n            task_history.update_status_for(TaskHistory.STATUS_ERROR, details='Analisys finished with errors!\\nError: {}'.format(result['msg']))\n        except UnboundLocalError:\n            task_history.update_details(persist=True, details='\\nProccess crashed')\n            task_history.update_status_for(TaskHistory.STATUS_ERROR, details='Analisys could not be started')\n    finally:\n        AuditRequest.cleanup_request()\n", "label": 1}
{"function": "\n\ndef create_initial_revisions(self, app, model_class, comment, batch_size, verbosity=2, database=None, **kwargs):\n    'Creates the set of initial revisions for the given model.'\n    try:\n        import_module(('%s.admin' % app.__name__.rsplit('.', 1)[0]))\n    except ImportError:\n        pass\n    if default_revision_manager.is_registered(model_class):\n        if (verbosity >= 2):\n            print(('Creating initial revision(s) for model %s ...' % force_text(model_class._meta.verbose_name)))\n        created_count = 0\n        content_type = ContentType.objects.db_manager(database).get_for_model(model_class)\n        versioned_pk_queryset = Version.objects.using(database).filter(content_type=content_type).all()\n        live_objs = model_class._default_manager.using(database).all()\n        if has_int_pk(model_class):\n            live_objs = live_objs.exclude(pk__in=versioned_pk_queryset.values_list('object_id_int', flat=True))\n        else:\n            live_objs = live_objs.exclude(pk__in=list(versioned_pk_queryset.values_list('object_id', flat=True).iterator()))\n        ids = list(live_objs.values_list(model_class._meta.pk.name, flat=True).order_by())\n        total = len(ids)\n        for i in range(0, total, batch_size):\n            chunked_ids = ids[i:(i + batch_size)]\n            objects = live_objs.in_bulk(chunked_ids)\n            for (id, obj) in objects.items():\n                try:\n                    default_revision_manager.save_revision((obj,), comment=comment, db=database)\n                except:\n                    print(('ERROR: Could not save initial version for %s %s.' % (model_class.__name__, obj.pk)))\n                    raise\n                created_count += 1\n            reset_queries()\n            if (verbosity >= 2):\n                print(('Created %s of %s.' % (created_count, total)))\n        if (verbosity >= 2):\n            print(('Created %s initial revision(s) for model %s.' % (created_count, force_text(model_class._meta.verbose_name))))\n    elif (verbosity >= 2):\n        print(('Model %s is not registered.' % force_text(model_class._meta.verbose_name)))\n", "label": 1}
{"function": "\n\ndef clean_message(self):\n    message = self.cleaned_data['message']\n    try:\n        message = message.decode('base64')\n    except TypeError as e:\n        raise ValidationError(('Cannot convert to binary: %r' % e.msg))\n    if (len(message) % 16):\n        raise ValidationError('Wrong block size for message !')\n    if (len(message) <= 16):\n        raise ValidationError('Message too short or missing IV !')\n    return message\n", "label": 0}
{"function": "\n\ndef get_ud(self, cardinal, user, channel, msg):\n    try:\n        word = msg.split(' ', 1)[1]\n    except IndexError:\n        cardinal.sendMsg(channel, 'Syntax: .ud <word>')\n        return\n    try:\n        url = (URBANDICT_API_PREFIX + word)\n        f = urlopen(url).read()\n        data = json.loads(f)\n        word_def = data['list'][0]['definition']\n        link = data['list'][0]['permalink']\n        response = ('UD for %s: %s (%s)' % (word, word_def, link))\n        cardinal.sendMsg(channel, response.encode('utf-8'))\n    except Exception:\n        cardinal.sendMsg(channel, ('Could not retrieve definition for %s' % word))\n", "label": 0}
{"function": "\n\ndef store_and_use_artifact(self, cache_key, src, results_dir=None):\n    'Read the content of a tarball from an iterator and return an artifact stored in the cache.'\n    with self._tmpfile(cache_key, 'read') as tmp:\n        for chunk in src:\n            tmp.write(chunk)\n        tmp.close()\n        tarball = self._store_tarball(cache_key, tmp.name)\n        artifact = self._artifact(tarball)\n        if (results_dir is not None):\n            safe_rmtree(results_dir)\n        artifact.extract()\n        return True\n", "label": 0}
{"function": "\n\ndef _prepare_ivy_xml(self, frozen_resolution, ivyxml, resolve_hash_name_for_report):\n    default_resolution = frozen_resolution.get('default')\n    if (default_resolution is None):\n        raise IvyUtils.IvyError(\"Couldn't find the frozen resolution for the 'default' ivy conf.\")\n    try:\n        jars = default_resolution.jar_dependencies\n        IvyUtils.generate_fetch_ivy(jars, ivyxml, self.confs, resolve_hash_name_for_report)\n    except Exception as e:\n        raise IvyUtils.IvyError('Failed to prepare ivy resolve: {}'.format(e))\n", "label": 0}
{"function": "\n\ndef loadWordFile(self, pre_processor=None):\n    filename = self.getDictionaryPath()\n    with codecs.open(filename, 'r', 'utf-8') as fp:\n        for word in fp.readlines():\n            if pre_processor:\n                self.add(pre_processor(word.strip()))\n            else:\n                self.add(word.strip())\n    return\n", "label": 0}
{"function": "\n\ndef create_security_groups(self):\n    for hostdef in self.blueprint.host_definitions.all():\n        sg_name = 'stackdio-managed-{0}-stack-{1}'.format(hostdef.slug, self.pk)\n        sg_description = 'stackd.io managed security group'\n        account = hostdef.cloud_image.account\n        if (not account.create_security_groups):\n            logger.debug('Skipping creation of {0} because security group creation is turned off for the account'.format(sg_name))\n            continue\n        driver = account.get_driver()\n        try:\n            sg_id = driver.create_security_group(sg_name, sg_description, delete_if_exists=True)\n        except Exception as e:\n            err_msg = 'Error creating security group: {0}'.format(str(e))\n            self.set_status('create_security_groups', self.ERROR, err_msg, Level.ERROR)\n        logger.debug('Created security group {0}: {1}'.format(sg_name, sg_id))\n        for access_rule in hostdef.access_rules.all():\n            driver.authorize_security_group(sg_id, {\n                'protocol': access_rule.protocol,\n                'from_port': access_rule.from_port,\n                'to_port': access_rule.to_port,\n                'rule': access_rule.rule,\n            })\n        self.security_groups.create(account=account, blueprint_host_definition=hostdef, name=sg_name, description=sg_description, group_id=sg_id, is_managed=True)\n", "label": 0}
{"function": "\n\ndef get_style(self, attribute):\n    \"Get the document's named style at the caret's current position.\\n\\n        If there is a text selection and the style varies over the selection,\\n        `pyglet.text.document.STYLE_INDETERMINATE` is returned.\\n\\n        :Parameters:\\n            `attribute` : str\\n                Name of style attribute to retrieve.  See\\n                `pyglet.text.document` for a list of recognised attribute\\n                names.\\n\\n        :rtype: object\\n        \"\n    if ((self._mark is None) or (self._mark == self._position)):\n        try:\n            return self._next_attributes[attribute]\n        except KeyError:\n            return self._layout.document.get_style(attribute, self._position)\n    start = min(self._position, self._mark)\n    end = max(self._position, self._mark)\n    return self._layout.document.get_style_range(attribute, start, end)\n", "label": 0}
{"function": "\n\ndef __call__(self, feature=None):\n    if (not current_app):\n        log.warn(\"Got a request to check for {feature} but we're outside the request context. Returning False\".format(feature=feature))\n        return False\n    try:\n        return self.model.check(feature)\n    except NoResultFound:\n        raise NoFeatureFlagFound()\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('model_class', (ModelWithVanillaMoneyField, ModelWithChoicesMoneyField))\ndef test_currency_querying(self, model_class):\n    model_class.objects.create(money=Money('100.0', moneyed.ZWN))\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.USD)).count() == 0)\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.ZWN)).count() == 1)\n", "label": 0}
{"function": "\n\ndef query_lookupd(self):\n    self.logger.debug('querying lookupd...')\n    lookupd = next(self.iterlookupds)\n    try:\n        producers = lookupd.lookup(self.topic)['producers']\n        self.logger.debug(('found %d producers' % len(producers)))\n    except Exception as error:\n        msg = 'Failed to lookup %s on %s (%s)'\n        self.logger.warn((msg % (self.topic, lookupd.address, error)))\n        return\n    for producer in producers:\n        conn = Nsqd((producer.get('broadcast_address') or producer['address']), producer['tcp_port'], producer['http_port'], **self.conn_kwargs)\n        self.connect_to_nsqd(conn)\n", "label": 0}
{"function": "\n\ndef full_clean(self):\n    '\\n        Cleans all of self.data and populates self._errors.\\n        '\n    self._errors = []\n    if (not self.is_bound):\n        return\n    for i in range(0, self.total_form_count()):\n        form = self.forms[i]\n        self._errors.append(form.errors)\n    try:\n        self.clean()\n    except ValidationError as e:\n        self._non_form_errors = self.error_class(e.messages)\n", "label": 0}
{"function": "\n\ndef test_deepcopy_shared_container(self):\n    (a, x) = T.scalars('ax')\n    h = function([In(a, value=0.0)], a)\n    f = function([x, In(a, value=h.container[a], implicit=True)], (x + a))\n    try:\n        memo = {\n            \n        }\n        ac = copy.deepcopy(a)\n        memo.update({\n            id(a): ac,\n        })\n        hc = copy.deepcopy(h, memo=memo)\n        memo.update({\n            id(h): hc,\n        })\n        fc = copy.deepcopy(f, memo=memo)\n    except NotImplementedError as e:\n        if e[0].startswith('DebugMode is not picklable'):\n            return\n        else:\n            raise\n    h[a] = 1\n    hc[ac] = 2\n    self.assertTrue((f[a] == 1))\n    self.assertTrue((fc[ac] == 2))\n", "label": 0}
{"function": "\n\ndef test_rerun_after_depletion_calls_once(self):\n    'Ensure MessageIterator works when used manually.'\n    from furious.batcher import MessageIterator\n    payload = '[\"test\"]'\n    task = Mock(payload=payload, tag='tag')\n    iterator = MessageIterator('tag', 'qn', 1)\n    with patch.object(iterator, 'queue') as queue:\n        queue.lease_tasks_by_tag.return_value = [task]\n        results = [payload for payload in iterator]\n        self.assertEqual(results, [payload])\n        results = [payload for payload in iterator]\n    queue.lease_tasks_by_tag.assert_called_once_with(60, 1, tag='tag', deadline=10)\n", "label": 0}
{"function": "\n\ndef update(self, action, action_id):\n    updated_action = {\n        \n    }\n    updated_action['freezer_action'] = utils.create_dict(**action)\n    try:\n        if (action['mandatory'] != ''):\n            updated_action['mandatory'] = action['mandatory']\n    except KeyError:\n        pass\n    try:\n        if (action['max_retries'] != ''):\n            updated_action['max_retries'] = action['max_retries']\n    except KeyError:\n        pass\n    try:\n        if (action['max_retries_interval'] != ''):\n            updated_action['max_retries_interval'] = action['max_retries_interval']\n    except KeyError:\n        pass\n    return self.client.actions.update(action_id, updated_action)\n", "label": 0}
{"function": "\n\ndef test_remove_unpickables_http_exception(self):\n    try:\n        urllib2.urlopen('http://localhost/this.does.not.exist')\n        self.fail('exception expected')\n    except urllib2.URLError as e:\n        pass\n    except urllib2.HTTPError as e:\n        pass\n    removed = mapper.remove_unpickables(e)\n    pickled = pickle.dumps(removed)\n    pickle.loads(pickled)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(CreateFlavorInfoAction, self).clean()\n    name = cleaned_data.get('name')\n    flavor_id = cleaned_data.get('flavor_id')\n    try:\n        flavors = api.nova.flavor_list(self.request, None)\n    except Exception:\n        flavors = []\n        msg = _('Unable to get flavor list')\n        exceptions.check_message(['Connection', 'refused'], msg)\n        raise\n    if (flavors is not None):\n        for flavor in flavors:\n            if (flavor.name == name):\n                raise forms.ValidationError((_('The name \"%s\" is already used by another flavor.') % name))\n            if (flavor.id == flavor_id):\n                raise forms.ValidationError((_('The ID \"%s\" is already used by another flavor.') % flavor_id))\n    return cleaned_data\n", "label": 0}
{"function": "\n\ndef _get_method(self, request, action, content_type, body):\n    'Look up the action-specific method and its extensions.'\n    try:\n        if (not self.controller):\n            meth = getattr(self, action)\n        else:\n            meth = getattr(self.controller, action)\n    except AttributeError:\n        if ((not self.wsgi_actions) or (action not in (_ROUTES_METHODS + ['action']))):\n            raise\n    else:\n        return (meth, self.wsgi_extensions.get(action, []))\n    if (action == 'action'):\n        action_name = action_peek(body)\n    else:\n        action_name = action\n    return (self.wsgi_actions[action_name], self.wsgi_action_extensions.get(action_name, []))\n", "label": 0}
{"function": "\n\ndef test_id(self):\n    'Each test annotation should be created with a unique ID.'\n    annotation_1 = factories.Annotation()\n    annotation_2 = factories.Annotation()\n    assert annotation_1.get('id')\n    assert annotation_2.get('id')\n    assert (annotation_1['id'] != annotation_2['id'])\n", "label": 0}
{"function": "\n\n@testing.requires.threading_with_mock\n@testing.requires.timing_intensive\ndef test_timeout_race(self):\n    dbapi = MockDBAPI()\n    p = pool.QueuePool(creator=(lambda : dbapi.connect(delay=0.05)), pool_size=2, max_overflow=1, use_threadlocal=False, timeout=3)\n    timeouts = []\n\n    def checkout():\n        for x in range(1):\n            now = time.time()\n            try:\n                c1 = p.connect()\n            except tsa.exc.TimeoutError:\n                timeouts.append((time.time() - now))\n                continue\n            time.sleep(4)\n            c1.close()\n    threads = []\n    for i in range(10):\n        th = threading.Thread(target=checkout)\n        th.start()\n        threads.append(th)\n    for th in threads:\n        th.join(join_timeout)\n    assert (len(timeouts) > 0)\n    for t in timeouts:\n        assert (t >= 3), ('Not all timeouts were >= 3 seconds %r' % timeouts)\n        assert (t < 14), ('Not all timeouts were < 14 seconds %r' % timeouts)\n", "label": 0}
{"function": "\n\ndef test_length(session):\n    set_ = session.set(key('test_sortedset_length'), S('abc'), SortedSet)\n    assert (len(set_) == 3)\n    setx = session.set(key('test_sortedsetx_length'), S([1, 2, 3]), IntSet)\n    assert (len(setx) == 3)\n", "label": 0}
{"function": "\n\ndef cold_evacuate(config, compute_api, instance_id, dst_host):\n    '\\n    Evacuate VM by shutting it down, booting another VM with same ephemeral\\n    volume on different host and deleting original VM.\\n\\n    :param config: CloudFerry configuration\\n    :param compute_api: Compute API client (NovaClient) instance\\n    :param instance_id: VM instance identifier to evacuate\\n    :param dst_host: destination host name\\n    '\n    LOG.debug('Cold evacuating VM %s to %s', instance_id, dst_host)\n    state_change_timeout = cfglib.CONF.evacuation.state_change_timeout\n    migration_timeout = cfglib.CONF.evacuation.migration_timeout\n    if (not change_to_pre_migration_state(compute_api, instance_id)):\n        instance = compute_api.servers.get(instance_id)\n        LOG.warning(\"Can't migrate VM in %s status\", instance.status)\n        return\n    instance = compute_api.servers.get(instance_id)\n    src_host = getattr(instance, INSTANCE_HOST_ATTRIBUTE)\n    if (src_host == dst_host):\n        LOG.warning('Skipping migration to the same host')\n        return\n    original_status = instance.status.lower()\n    if (original_status != SHUTOFF):\n        compute_api.servers.stop(instance)\n        wait_for_condition(is_vm_status_in, compute_api, instance, [SHUTOFF], timeout=state_change_timeout)\n    fix_post_cobalt_ephemeral_disk(config, instance)\n    with install_ssh_keys(config, src_host, dst_host):\n        with disable_all_nova_compute_services(compute_api):\n            with enable_nova_compute_services(compute_api, dst_host, src_host):\n                compute_api.servers.migrate(instance)\n                wait_for_condition(is_vm_status_in, compute_api, instance, [VERIFY_RESIZE], timeout=migration_timeout)\n                compute_api.servers.confirm_resize(instance)\n                wait_for_condition(is_vm_status_in, compute_api, instance, [ACTIVE], timeout=state_change_timeout)\n    if (original_status == SHUTOFF.lower()):\n        LOG.debug('Starting replacement VM %s', instance_id)\n        compute_api.servers.stop(instance_id)\n", "label": 0}
{"function": "\n\ndef _validate_python(self, field_dict, state):\n    try:\n        ref = field_dict[self.field_names[0]]\n    except TypeError:\n        raise Invalid(self.message('notDict', state), field_dict, state)\n    except KeyError:\n        ref = ''\n    errors = {\n        \n    }\n    for name in self.field_names[1:]:\n        if (field_dict.get(name, '') != ref):\n            if self.show_match:\n                errors[name] = self.message('invalid', state, match=ref)\n            else:\n                errors[name] = self.message('invalidNoMatch', state)\n    if errors:\n        error_list = sorted(six.iteritems(errors))\n        error_message = '<br>\\n'.join((('%s: %s' % (name, value)) for (name, value) in error_list))\n        raise Invalid(error_message, field_dict, state, error_dict=errors)\n", "label": 1}
{"function": "\n\ndef iter_keys(self, filename):\n    with open(filename, 'rb') as f:\n        header = f.read(8)\n        self._verify_header(header)\n        current_offset = 8\n        file_size_bytes = os.path.getsize(filename)\n        while True:\n            current_contents = f.read(8)\n            current_offset += 8\n            if (len(current_contents) < 8):\n                if (len(current_contents) > 0):\n                    raise DBMLoadError('Error loading db: partial header read')\n                else:\n                    return\n            (key_size, val_size) = struct.unpack('!ii', current_contents)\n            key = f.read(key_size)\n            if (len(key) != key_size):\n                raise DBMLoadError(('Error loading db: key size does not match (expected %s bytes, got %s instead.' % (key_size, len(key))))\n            value_offset = (current_offset + key_size)\n            if ((value_offset + val_size) > file_size_bytes):\n                return\n            (yield (key, value_offset, val_size))\n            if (val_size == _DELETED):\n                val_size = 0\n            skip_ahead = ((key_size + val_size) + 4)\n            current_offset += skip_ahead\n            if (current_offset > file_size_bytes):\n                raise DBMLoadError('Error loading db: reading past the end of the file (file possibly truncated)')\n            f.seek(current_offset)\n", "label": 1}
{"function": "\n\ndef test_iter_smart_pk_range(self):\n    seen = []\n    for (start_pk, end_pk) in Author.objects.iter_smart_pk_ranges():\n        seen.extend(Author.objects.filter(id__gte=start_pk, id__lt=end_pk).values_list('id', flat=True))\n    all_ids = list(Author.objects.order_by('id').values_list('id', flat=True))\n    assert (seen == all_ids)\n", "label": 0}
{"function": "\n\ndef get_command(self, ctx, name):\n    \"\\n        Get a specific command by looking up the module.\\n\\n        :param ctx: Click context\\n        :param name: Command name\\n        :return: Module's cli function\\n        \"\n    try:\n        if (sys.version_info[0] == 2):\n            name = name.encode('ascii', 'replace')\n        mod = __import__(('cli.commands.cmd_' + name), None, None, ['cli'])\n    except ImportError as e:\n        logging.error('Error importing module {0}:\\n{0}'.format(name, e))\n        exit(1)\n    return mod.cli\n", "label": 0}
{"function": "\n\ndef bayesdb_generator_column_stattype(bdb, generator_id, colno):\n    'Return the statistical type of the column `colno` in `generator_id`.'\n    sql = '\\n        SELECT stattype FROM bayesdb_generator_column\\n            WHERE generator_id = ? AND colno = ?\\n    '\n    cursor = bdb.sql_execute(sql, (generator_id, colno))\n    try:\n        row = cursor.next()\n    except StopIteration:\n        generator = bayesdb_generator_name(bdb, generator_id)\n        sql = '\\n            SELECT COUNT(*)\\n                FROM bayesdb_generator AS g, bayesdb_column AS c\\n                WHERE g.id = :generator_id\\n                    AND g.tabname = c.tabname\\n                    AND c.colno = :colno\\n        '\n        cursor = bdb.sql_execute(sql, {\n            'generator_id': generator_id,\n            'colno': colno,\n        })\n        if (cursor_value(cursor) == 0):\n            raise ValueError(('No such column in generator %s: %d' % (generator, colno)))\n        else:\n            raise ValueError(('Column not modelled in generator %s: %d' % (generator, colno)))\n    else:\n        assert (len(row) == 1)\n        return row[0]\n", "label": 0}
{"function": "\n\ndef _mount_shares_to_instance(self, instance):\n    for share in self.shares:\n        share.handler.allow_access_to_instance(instance, share.share_config)\n    with instance.remote() as remote:\n        share_types = set((type(share.handler) for share in self.shares))\n        for share_type in share_types:\n            share_type.setup_instance(remote)\n        for share in self.shares:\n            share.handler.mount_to_instance(remote, share.share_config)\n", "label": 0}
{"function": "\n\ndef parse(cls, signed_request, application_secret_key):\n    'Parse a signed request, returning a dictionary describing its payload.'\n\n    def decode(encoded):\n        padding = ('=' * (len(encoded) % 4))\n        return base64.urlsafe_b64decode((encoded + padding))\n    try:\n        (encoded_signature, encoded_payload) = (str(string) for string in signed_request.split('.', 2))\n        signature = decode(encoded_signature)\n        signed_request_data = json.loads(decode(encoded_payload).decode('utf-8'))\n    except (TypeError, ValueError):\n        raise SignedRequestError('Signed request had a corrupt payload')\n    if (signed_request_data.get('algorithm', '').upper() != 'HMAC-SHA256'):\n        raise SignedRequestError('Signed request is using an unknown algorithm')\n    expected_signature = hmac.new(application_secret_key.encode('utf-8'), msg=encoded_payload.encode('utf-8'), digestmod=hashlib.sha256).digest()\n    if (signature != expected_signature):\n        raise SignedRequestError('Signed request signature mismatch')\n    return signed_request_data\n", "label": 0}
{"function": "\n\ndef _get_url(self, url):\n    if (self.access == 'public'):\n        url = url.replace('https://', 'http://')\n        req = urllib.request.Request(url)\n        try:\n            return urllib.request.urlopen(req).read()\n        except urllib.error.HTTPError:\n            raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n    else:\n        raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n", "label": 0}
{"function": "\n\ndef LVMPathSpecGetVolumeIndex(path_spec):\n    'Retrieves the volume index from the path specification.\\n\\n  Args:\\n    path_spec: the path specification (instance of PathSpec).\\n  '\n    volume_index = getattr(path_spec, 'volume_index', None)\n    if (volume_index is None):\n        location = getattr(path_spec, 'location', None)\n        if ((location is None) or (not location.startswith('/lvm'))):\n            return\n        volume_index = None\n        try:\n            volume_index = (int(location[4:], 10) - 1)\n        except ValueError:\n            pass\n        if ((volume_index is None) or (volume_index < 0)):\n            return\n    return volume_index\n", "label": 0}
{"function": "\n\n@responses.activate\ndef test_bitly_total_clicks_bad_response():\n    body = '20'\n    params = urlencode(dict(link=shorten, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/link/clicks', params)\n    responses.add(responses.GET, url, body=body, status=400, match_querystring=True)\n    body = shorten\n    params = urlencode(dict(uri=expanded, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/shorten', params)\n    responses.add(responses.GET, url, body=body, match_querystring=True)\n    s.short(expanded)\n    assert (s.total_clicks() == 0)\n    assert (s.total_clicks(shorten) == 0)\n", "label": 0}
{"function": "\n\ndef prune_overridden(ansi_string):\n    'Remove color codes that are rendered ineffective by subsequent codes in one escape sequence then sort codes.\\n\\n    :param str ansi_string: Incoming ansi_string with ANSI color codes.\\n\\n    :return: Color string with pruned color sequences.\\n    :rtype: str\\n    '\n    multi_seqs = set((p for p in RE_ANSI.findall(ansi_string) if (';' in p[1])))\n    for (escape, codes) in multi_seqs:\n        r_codes = list(reversed(codes.split(';')))\n        try:\n            r_codes = r_codes[:(r_codes.index('0') + 1)]\n        except ValueError:\n            pass\n        for group in CODE_GROUPS:\n            for pos in reversed([i for (i, n) in enumerate(r_codes) if (n in group)][1:]):\n                r_codes.pop(pos)\n        reduced_codes = ';'.join(sorted(r_codes, key=int))\n        if (codes != reduced_codes):\n            ansi_string = ansi_string.replace(escape, (('\\x1b[' + reduced_codes) + 'm'))\n    return ansi_string\n", "label": 1}
{"function": "\n\ndef capture(self, money, authorization, options=None):\n    options = (options or {\n        \n    })\n    params = {\n        'checkout_id': authorization,\n    }\n    token = options.pop('access_token', self.we_pay_settings['ACCESS_TOKEN'])\n    try:\n        response = self.we_pay.call('/checkout/capture', params, token=token)\n    except WePayError as error:\n        transaction_was_unsuccessful.send(sender=self, type='capture', response=error)\n        return {\n            'status': 'FAILURE',\n            'response': error,\n        }\n    transaction_was_successful.send(sender=self, type='capture', response=response)\n    return {\n        'status': 'SUCCESS',\n        'response': response,\n    }\n", "label": 0}
{"function": "\n\ndef update_success(self, update_dict, raw=False):\n    ' This method serves as an easy way to update your success attributes\\n        that are passed to the start Node rendering context, or passed back in\\n        JSON. It automatically recalls whether the last validation call was to\\n        json_validate or validate_render and modifys the correct dictionary\\n        accordingly.\\n\\n        :param update_dict: The dictionary of values to update/add.\\n        :type data: dictionary\\n\\n        :param raw: Whether you would like a pre-compiled JSON\\n            string returned, or the raw dictionary.\\n        :type raw: bool\\n\\n        :return: Return value is either the new JSON string (or raw dict if\\n            requested) if json_validate was your last validation call, or a\\n            re-render of the form with updated error messages if validate_render\\n            was your last call.\\n        '\n    if (self._last_valid == 'render'):\n        try:\n            self.start.errors[(- 1)].update(update_dict)\n        except IndexError:\n            raise IndexError('Error updating your error dictionary for the start Node. There were no errors to modify.')\n        except AttributeError:\n            raise AttributeError('This method is designed to update an error dictionary, yet your errors are not dictionaries')\n        return self.render()\n    else:\n        try:\n            self._last_raw_json['success_blob'].update(update_dict)\n        except KeyError:\n            raise KeyError('Either your json_validate method has not been run yet, or your success_header_generate does not produce output')\n        if raw:\n            return self._last_raw_json\n        else:\n            return json.dumps(self._last_raw_json)\n", "label": 0}
{"function": "\n\ndef test_getitem_slice_big():\n    slt = SortedList(range(4))\n    lst = list(range(4))\n    itr = ((start, stop, step) for start in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for stop in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for step in [(- 3), (- 2), (- 1), 1, 2, 3])\n    for (start, stop, step) in itr:\n        assert (slt[start:stop:step] == lst[start:stop:step])\n", "label": 0}
{"function": "\n\ndef clear(self):\n    'od.clear() -> None.  Remove all items from od.'\n    try:\n        for node in self.__map.itervalues():\n            del node[:]\n        root = self.__root\n        root[:] = [root, root, None]\n        self.__map.clear()\n    except AttributeError:\n        pass\n    dict.clear(self)\n", "label": 0}
{"function": "\n\ndef Validate(self):\n    'Attempt to validate the artifact has been well defined.\\n\\n    This is used to enforce Artifact rules. Since it checks all dependencies are\\n    present, this method can only be called once all artifacts have been loaded\\n    into the registry. Use ValidateSyntax to check syntax for each artifact on\\n    import.\\n\\n    Raises:\\n      ArtifactDefinitionError: If artifact is invalid.\\n    '\n    self.ValidateSyntax()\n    try:\n        for dependency in self.GetArtifactDependencies():\n            dependency_obj = REGISTRY.GetArtifact(dependency)\n            if dependency_obj.error_message:\n                raise ArtifactDefinitionError(('Dependency %s has an error!' % dependency))\n    except ArtifactNotRegisteredError as e:\n        raise ArtifactDefinitionError(e)\n", "label": 0}
{"function": "\n\ndef dfs_labeled_edges(G, source=None):\n    \"Produce edges in a depth-first-search (DFS) labeled by type.\\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n\\n    source : node, optional\\n       Specify starting node for depth-first search and return edges in\\n       the component reachable from source.\\n\\n    Returns\\n    -------\\n    edges: generator\\n       A generator of edges in the depth-first-search labeled with 'forward',\\n       'nontree', and 'reverse'.\\n\\n    Examples\\n    --------\\n    >>> G = nx.path_graph(3)\\n    >>> edges = (list(nx.dfs_labeled_edges(G,0)))\\n\\n    Notes\\n    -----\\n    Based on http://www.ics.uci.edu/~eppstein/PADS/DFS.py\\n    by D. Eppstein, July 2004.\\n\\n    If a source is not specified then a source is chosen arbitrarily and\\n    repeatedly until all components in the graph are searched.\\n    \"\n    if (source is None):\n        nodes = G\n    else:\n        nodes = [source]\n    visited = set()\n    for start in nodes:\n        if (start in visited):\n            continue\n        (yield (start, start, {\n            'dir': 'forward',\n        }))\n        visited.add(start)\n        stack = [(start, iter(G[start]))]\n        while stack:\n            (parent, children) = stack[(- 1)]\n            try:\n                child = next(children)\n                if (child in visited):\n                    (yield (parent, child, {\n                        'dir': 'nontree',\n                    }))\n                else:\n                    (yield (parent, child, {\n                        'dir': 'forward',\n                    }))\n                    visited.add(child)\n                    stack.append((child, iter(G[child])))\n            except StopIteration:\n                stack.pop()\n                if stack:\n                    (yield (stack[(- 1)][0], parent, {\n                        'dir': 'reverse',\n                    }))\n        (yield (start, start, {\n            'dir': 'reverse',\n        }))\n", "label": 1}
{"function": "\n\ndef __init__(self, pattern, flags=0):\n    'The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.'\n    super(Regex, self).__init__()\n    if isinstance(pattern, basestring):\n        if (len(pattern) == 0):\n            warnings.warn('null string passed to Regex; use Empty() instead', SyntaxWarning, stacklevel=2)\n        self.pattern = pattern\n        self.flags = flags\n        try:\n            self.re = re.compile(self.pattern, self.flags)\n            self.reString = self.pattern\n        except sre_constants.error:\n            warnings.warn(('invalid pattern (%s) passed to Regex' % pattern), SyntaxWarning, stacklevel=2)\n            raise\n    elif isinstance(pattern, Regex.compiledREtype):\n        self.re = pattern\n        self.pattern = self.reString = str(pattern)\n        self.flags = flags\n    else:\n        raise ValueError('Regex may only be constructed with a string or a compiled RE object')\n    self.name = _ustr(self)\n    self.errmsg = ('Expected ' + self.name)\n    self.mayIndexError = False\n    self.mayReturnEmpty = True\n", "label": 0}
{"function": "\n\ndef get_cpu_state(self):\n    '\\n        Retrieves CPU state from client\\n        '\n    state = c_int(0)\n    self.library.Cli_GetPlcStatus(self.pointer, byref(state))\n    try:\n        status_string = cpu_statuses[state.value]\n    except KeyError:\n        status_string = None\n    if (not status_string):\n        raise Snap7Exception(('The cpu state (%s) is invalid' % state.value))\n    logging.debug(('CPU state is %s' % status_string))\n    return status_string\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    from pennyblack.models import Link, Newsletter\n    if ('mail' not in context):\n        return '#'\n    mail = context['mail']\n    newsletter = mail.job.newsletter\n    if newsletter.is_workflow():\n        job = newsletter.get_default_job()\n    else:\n        job = mail.job\n    try:\n        link = job.links.get(identifier=self.identifier)\n    except job.links.model.DoesNotExist:\n        link = Newsletter.add_view_link_to_job(self.identifier, job)\n    return (context['base_url'] + reverse('pennyblack.redirect_link', args=(mail.mail_hash, link.link_hash)))\n", "label": 0}
{"function": "\n\ndef _setup_units(self, connections, params_dict, unknowns_dict):\n    '\\n        Calculate unit conversion factors for any connected\\n        variables having different units and store them in params_dict.\\n\\n        Args\\n        ----\\n        connections : dict\\n            A dict of target variables (absolute name) mapped\\n            to the absolute name of their source variable and the\\n            relevant indices of that source if applicable.\\n\\n        params_dict : OrderedDict\\n            A dict of parameter metadata for the whole `Problem`.\\n\\n        unknowns_dict : OrderedDict\\n            A dict of unknowns metadata for the whole `Problem`.\\n        '\n    to_prom_name = self.root._sysdata.to_prom_name\n    for (target, (source, idxs)) in iteritems(connections):\n        tmeta = params_dict[target]\n        smeta = unknowns_dict[source]\n        if (('units' not in tmeta) or ('units' not in smeta)):\n            continue\n        src_unit = smeta['units']\n        tgt_unit = tmeta['units']\n        try:\n            (scale, offset) = get_conversion_tuple(src_unit, tgt_unit)\n        except TypeError as err:\n            if (str(err) == 'Incompatible units'):\n                msg = \"Unit '{0}' in source {1} is incompatible with unit '{2}' in target {3}.\".format(src_unit, _both_names(smeta, to_prom_name), tgt_unit, _both_names(tmeta, to_prom_name))\n                self._setup_errors.append(msg)\n                continue\n            else:\n                raise\n        if ((scale != 1.0) or (offset != 0.0)):\n            tmeta['unit_conv'] = (scale, offset)\n", "label": 1}
{"function": "\n\ndef message_user(self, request, message, level=messages.INFO, extra_tags='', fail_silently=False):\n    '\\n        Send a message to the user. The default implementation\\n        posts a message using the django.contrib.messages backend.\\n\\n        Exposes almost the same API as messages.add_message(), but accepts the\\n        positional arguments in a different order to maintain backwards\\n        compatibility. For convenience, it accepts the `level` argument as\\n        a string rather than the usual level number.\\n        '\n    if (not isinstance(level, int)):\n        try:\n            level = getattr(messages.constants, level.upper())\n        except AttributeError:\n            levels = messages.constants.DEFAULT_TAGS.values()\n            levels_repr = ', '.join((('`%s`' % l) for l in levels))\n            raise ValueError(('Bad message level string: `%s`. Possible values are: %s' % (level, levels_repr)))\n    messages.add_message(request, level, message, extra_tags=extra_tags, fail_silently=fail_silently)\n", "label": 0}
{"function": "\n\ndef test_active_contributors(self):\n    'Test the active_contributors util method.'\n    start_date = self.start_date\n    en_us_contributors = active_contributors(from_date=start_date, locale='en-US')\n    es_contributors = active_contributors(from_date=start_date, locale='es')\n    all_contributors = active_contributors(from_date=start_date)\n    eq_(3, len(en_us_contributors))\n    assert (self.user in en_us_contributors)\n    assert (self.en_us_old.creator not in en_us_contributors)\n    eq_(4, len(es_contributors))\n    assert (self.user in es_contributors)\n    assert (self.es_old.creator not in es_contributors)\n    eq_(6, len(all_contributors))\n    assert (self.user in all_contributors)\n    assert (self.en_us_old.creator not in all_contributors)\n    assert (self.es_old.creator not in all_contributors)\n", "label": 1}
{"function": "\n\ndef _cleanup_groups(self):\n    exception_list = list()\n    for group in self.cloud.list_groups():\n        if group['name'].startswith(self.group_prefix):\n            try:\n                self.cloud.delete_group(group['id'])\n            except Exception as e:\n                exception_list.append(str(e))\n                continue\n    if exception_list:\n        raise OpenStackCloudException('\\n'.join(exception_list))\n", "label": 0}
{"function": "\n\ndef _expand_glob_path(file_roots):\n    '\\n    Applies shell globbing to a set of directories and returns\\n    the expanded paths\\n    '\n    unglobbed_path = []\n    for path in file_roots:\n        try:\n            if glob.has_magic(path):\n                unglobbed_path.extend(glob.glob(path))\n            else:\n                unglobbed_path.append(path)\n        except Exception:\n            unglobbed_path.append(path)\n    return unglobbed_path\n", "label": 0}
{"function": "\n\ndef listKeysAndSizes(self, bucketName):\n    'Return a list of (name, size) pairs of keys in the bucket'\n    with self.state_.lock:\n        if (bucketName not in self.state_.buckets_):\n            raise S3Interface.BucketNotFound(bucketName)\n        self.state_.validateAccess(bucketName, self.credentials_)\n        return [(key, len(val.value), val.mtime) for (key, val) in self.state_.buckets_[bucketName].iteritems()]\n", "label": 0}
{"function": "\n\ndef test_issue_7754(self):\n    old_cwd = os.getcwd()\n    config_dir = os.path.join(integration.TMP, 'issue-7754')\n    if (not os.path.isdir(config_dir)):\n        os.makedirs(config_dir)\n    os.chdir(config_dir)\n    for fname in ('master', 'minion'):\n        pid_path = os.path.join(config_dir, '{0}.pid'.format(fname))\n        with salt.utils.fopen(self.get_config_file_path(fname), 'r') as fhr:\n            config = yaml.load(fhr.read())\n            config['log_file'] = config['syndic_log_file'] = 'file:///tmp/log/LOG_LOCAL3'\n            config['root_dir'] = config_dir\n            if ('ret_port' in config):\n                config['ret_port'] = (int(config['ret_port']) + 10)\n                config['publish_port'] = (int(config['publish_port']) + 10)\n            with salt.utils.fopen(os.path.join(config_dir, fname), 'w') as fhw:\n                fhw.write(yaml.dump(config, default_flow_style=False))\n    ret = self.run_script(self._call_binary_, '--config-dir={0} --pid-file={1} -l debug'.format(config_dir, pid_path), timeout=5, catch_stderr=True, with_retcode=True)\n    if os.path.exists(pid_path):\n        with salt.utils.fopen(pid_path) as fhr:\n            try:\n                os.kill(int(fhr.read()), signal.SIGKILL)\n            except OSError:\n                pass\n    try:\n        self.assertFalse(os.path.isdir(os.path.join(config_dir, 'file:')))\n        self.assertIn('Failed to setup the Syslog logging handler', '\\n'.join(ret[1]))\n        self.assertEqual(ret[2], 2)\n    finally:\n        self.chdir(old_cwd)\n        if os.path.isdir(config_dir):\n            shutil.rmtree(config_dir)\n", "label": 0}
{"function": "\n\n@classmethod\ndef _from_line(cls, remote, line):\n    'Create a new PushInfo instance as parsed from line which is expected to be like\\n            refs/heads/master:refs/heads/master 05d2687..1d0568e as bytes'\n    (control_character, from_to, summary) = line.split('\\t', 3)\n    flags = 0\n    try:\n        flags |= cls._flag_map[control_character]\n    except KeyError:\n        raise ValueError(('Control character %r unknown as parsed from line %r' % (control_character, line)))\n    (from_ref_string, to_ref_string) = from_to.split(':')\n    if (flags & cls.DELETED):\n        from_ref = None\n    else:\n        from_ref = Reference.from_path(remote.repo, from_ref_string)\n    old_commit = None\n    if summary.startswith('['):\n        if ('[rejected]' in summary):\n            flags |= cls.REJECTED\n        elif ('[remote rejected]' in summary):\n            flags |= cls.REMOTE_REJECTED\n        elif ('[remote failure]' in summary):\n            flags |= cls.REMOTE_FAILURE\n        elif ('[no match]' in summary):\n            flags |= cls.ERROR\n        elif ('[new tag]' in summary):\n            flags |= cls.NEW_TAG\n        elif ('[new branch]' in summary):\n            flags |= cls.NEW_HEAD\n    else:\n        split_token = '...'\n        if (control_character == ' '):\n            split_token = '..'\n        (old_sha, new_sha) = summary.split(' ')[0].split(split_token)\n        old_commit = remote.repo.commit(old_sha)\n    return PushInfo(flags, from_ref, to_ref_string, remote, old_commit, summary)\n", "label": 1}
{"function": "\n\ndef init(self, modelDocument):\n    super(ModelRssItem, self).init(modelDocument)\n    try:\n        if (self.modelXbrl.modelManager.rssWatchOptions.latestPubDate and (self.pubDate <= self.modelXbrl.modelManager.rssWatchOptions.latestPubDate)):\n            self.status = _('tested')\n        else:\n            self.status = _('not tested')\n    except AttributeError:\n        self.status = _('not tested')\n    self.results = None\n    self.assertions = None\n", "label": 0}
{"function": "\n\ndef test_pos_list_append_with_nonexistent_key(self):\n    '\\n        Invoke list_append() with non-existent key\\n        '\n    charSet = 'abcdefghijklmnopqrstuvwxyz1234567890'\n    minLength = 5\n    maxLength = 30\n    length = random.randint(minLength, maxLength)\n    key = ('test', 'demo', (''.join(map((lambda unused: random.choice(charSet)), range(length))) + '.com'))\n    status = self.as_connection.list_append(key, 'abc', 122)\n    assert (status == 0)\n    (key, _, bins) = self.as_connection.get(key)\n    self.as_connection.remove(key)\n    assert (status == 0)\n    assert (bins == {\n        'abc': [122],\n    })\n", "label": 0}
{"function": "\n\ndef start(self):\n    'Start watching the directory for changes.'\n    with self._inotify_fd_lock:\n        if (self._inotify_fd < 0):\n            return\n        self._inotify_poll.register(self._inotify_fd, select.POLLIN)\n        for directory in self._directories:\n            self._add_watch_for_path(directory)\n", "label": 0}
{"function": "\n\ndef test_invalid_base_fields(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.ForeignKey('testapp.Author'), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E005')\n    assert ('Base field for list must be' in errors[0].msg)\n", "label": 0}
{"function": "\n\n@contextlib.contextmanager\ndef _assert_warns_context_manager(warning_class=None, warnings_test=None):\n    '\\n    Builds a context manager for testing code that should throw a warning.\\n    This will look for a given class, call a custom test, or both.\\n\\n    Args:\\n        warning_class - a class or subclass of Warning. If not None, then\\n            the context manager will raise an AssertionError if the block\\n            does not throw at least one warning of that type.\\n        warnings_test - a function which takes a list of warnings caught,\\n            and makes a number of assertions about the result. If the function\\n            returns without an exception, the context manager will consider\\n            this a successful assertion.\\n    '\n    with warnings.catch_warnings(record=True) as caught:\n        warnings.resetwarnings()\n        if warning_class:\n            warnings.simplefilter('ignore')\n            warnings.simplefilter('always', category=warning_class)\n        else:\n            warnings.simplefilter('always')\n        (yield)\n        assert_gt(len(caught), 0, 'expected at least one warning to be thrown')\n        if warnings_test:\n            warnings_test(caught)\n", "label": 0}
{"function": "\n\ndef _AtNonLeaf(self, attr_value, path):\n    'Called when at a non-leaf value. Should recurse and yield values.'\n    try:\n        if isinstance(attr_value, collections.Mapping):\n            sub_obj = attr_value.get(path[1])\n            if (len(path) > 2):\n                sub_obj = self.Expand(sub_obj, path[2:])\n            if isinstance(sub_obj, basestring):\n                (yield sub_obj)\n            elif isinstance(sub_obj, collections.Mapping):\n                for (k, v) in sub_obj.items():\n                    (yield {\n                        k: v,\n                    })\n            else:\n                for value in sub_obj:\n                    (yield value)\n        else:\n            for sub_obj in attr_value:\n                for value in self.Expand(sub_obj, path[1:]):\n                    (yield value)\n    except TypeError:\n        for value in self.Expand(attr_value, path[1:]):\n            (yield value)\n", "label": 1}
{"function": "\n\ndef longRunHighGrayLevelEmphasis(self, P_glrl, ivector, jvector, sumP_glrl, meanFlag=True):\n    try:\n        lrhgle = (numpy.sum(numpy.sum(((P_glrl * (ivector ** 2)[:, None, None]) * (jvector ** 2)[None, :, None]), 0), 0) / sumP_glrl[None, None, :])\n    except ZeroDivisionError:\n        lrhgle = 0\n    if meanFlag:\n        return lrhgle.mean()\n    else:\n        return lrhgle\n", "label": 0}
{"function": "\n\ndef clean_votes(self, value):\n    assert (value > 0), 'Must be greater than 0.'\n    assert (value < 51), 'Must be less than 51.'\n    return value\n", "label": 0}
{"function": "\n\ndef host_to_ip(host):\n    '\\n    Returns the IP address of a given hostname\\n    '\n    try:\n        (family, socktype, proto, canonname, sockaddr) = socket.getaddrinfo(host, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0]\n        if (family == socket.AF_INET):\n            (ip, port) = sockaddr\n        elif (family == socket.AF_INET6):\n            (ip, port, flow_info, scope_id) = sockaddr\n    except Exception:\n        ip = None\n    return ip\n", "label": 0}
{"function": "\n\ndef test_gzip():\n    res = app.get('/', extra_environ=dict(HTTP_ACCEPT_ENCODING='gzip'))\n    assert (int(res.header('content-length')) == len(res.body))\n    assert (res.body != b'this is a test')\n    actual = gzip.GzipFile(fileobj=six.BytesIO(res.body)).read()\n    assert (actual == b'this is a test')\n", "label": 0}
{"function": "\n\ndef to_ctype(self, parakeet_type):\n    if isinstance(parakeet_type, (NoneT, ScalarT)):\n        return type_mappings.to_ctype(parakeet_type)\n    elif isinstance(parakeet_type, TupleT):\n        return self.struct_type_from_fields(parakeet_type.elt_types)\n    elif isinstance(parakeet_type, PtrT):\n        return self.ptr_struct_type(parakeet_type.elt_type)\n    elif isinstance(parakeet_type, ArrayT):\n        elt_t = parakeet_type.elt_type\n        rank = parakeet_type.rank\n        return self.array_struct_type(elt_t, rank)\n    elif isinstance(parakeet_type, SliceT):\n        return self.slice_struct_type()\n    elif isinstance(parakeet_type, ClosureT):\n        return self.struct_type_from_fields(parakeet_type.arg_types)\n    elif isinstance(parakeet_type, TypeValueT):\n        return 'int'\n    else:\n        assert False, (\"Don't know how to make C type for %s\" % parakeet_type)\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    TestCase.__init__(self, *args, **kwargs)\n    for attr in [x for x in dir(self) if x.startswith('test')]:\n        meth = getattr(self, attr)\n\n        def test_(self):\n            try:\n                meth()\n            except psutil.AccessDenied:\n                pass\n        setattr(self, attr, types.MethodType(test_, self))\n", "label": 0}
{"function": "\n\ndef SaveTemporaryFile(fp):\n    'Store incoming database file in a temporary directory.'\n    loc = data_store.DB.Location()\n    if (not os.path.exists(loc)):\n        return False\n    if (not os.path.isdir(loc)):\n        return False\n    filecopy_len_str = fp.read(sutils.SIZE_PACKER.size)\n    filecopy_len = sutils.SIZE_PACKER.unpack(filecopy_len_str)[0]\n    filecopy = rdf_data_server.DataServerFileCopy(fp.read(filecopy_len))\n    rebdir = _CreateDirectory(loc, filecopy.rebalance_id)\n    filedir = utils.JoinPath(rebdir, filecopy.directory)\n    try:\n        os.makedirs(filedir)\n    except OSError:\n        pass\n    filepath = utils.JoinPath(filedir, filecopy.filename)\n    logging.info('Writing to file %s', filepath)\n    with open(filepath, 'wb') as wp:\n        decompressor = zlib.decompressobj()\n        while True:\n            block_len_str = fp.read(sutils.SIZE_PACKER.size)\n            block_len = sutils.SIZE_PACKER.unpack(block_len_str)[0]\n            if (not block_len):\n                break\n            block = fp.read(block_len)\n            to_decompress = (decompressor.unconsumed_tail + block)\n            while to_decompress:\n                decompressed = decompressor.decompress(to_decompress)\n                if decompressed:\n                    wp.write(decompressed)\n                    to_decompress = decompressor.unconsumed_tail\n                else:\n                    to_decompress = ''\n        remaining = decompressor.flush()\n        if remaining:\n            wp.write(remaining)\n    if (os.path.getsize(filepath) != filecopy.size):\n        logging.error('Size of file %s is not %d', filepath, filecopy.size)\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_can_update_status_via_trigger_on_participant_balance(self):\n    self.db.run(\"UPDATE participants SET balance=10, status_of_1_0_payout='pending-application' WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 10)\n    assert (alice.status_of_1_0_payout == 'pending-application')\n    self.db.run(\"UPDATE participants SET balance=0 WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 0)\n    assert (alice.status_of_1_0_payout == 'completed')\n", "label": 0}
{"function": "\n\ndef test_unknown_apps_are_ignored(self):\n    'Unknown engines get ignored.'\n    self.create_appversion('firefox', '33.0a1')\n    self.create_appversion('thunderbird', '33.0a1')\n    data = {\n        'engines': {\n            'firefox': '>=33.0a1',\n            'thunderbird': '>=33.0a1',\n            'node': '>=0.10',\n        },\n    }\n    apps = self.parse(data)['apps']\n    engines = [app.appdata.short for app in apps]\n    assert (sorted(engines) == ['firefox', 'thunderbird'])\n", "label": 0}
{"function": "\n\ndef _trending_for_month(metric=None):\n    this_month_date = month_for_date(datetime.date.today())\n    previous_month_date = get_previous_month(this_month_date)\n    previous_month_year_date = get_previous_year(this_month_date)\n    data = {\n        'month': 0,\n        'previous_month': 0,\n        'previous_month_year': 0,\n    }\n    try:\n        month = MetricMonth.objects.get(metric=metric, created=this_month_date)\n        data['month'] = month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)\n        data['previous_month'] = previous_month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)\n        data['previous_month_year'] = previous_month_year.num\n    except ObjectDoesNotExist:\n        pass\n    return data\n", "label": 0}
{"function": "\n\ndef report_to_ci_server(self, project):\n    for report in self.reports:\n        test_name = report['test']\n        test_failed = (report['success'] is not True)\n        with test_proxy_for(project).and_test_name(('Integrationtest.%s' % test_name)) as test:\n            if test_failed:\n                test.fails(report['exception'])\n", "label": 0}
{"function": "\n\ndef application(environ, start_response):\n    '\\n    The main WSGI application. Dispatch the current request to\\n    the functions from above and store the regular expression\\n    captures in the WSGI environment as  `oic.url_args` so that\\n    the functions from above can access the url placeholders.\\n\\n    If nothing matches call the `not_found` function.\\n\\n    :param environ: The HTTP application environment\\n    :param start_response: The application to run when the handling of the\\n        request is done\\n    :return: The response as a list of lines\\n    '\n    global OAS\n    path = environ.get('PATH_INFO', '').lstrip('/')\n    logger = logging.getLogger('oicServer')\n    if (path == 'robots.txt'):\n        return static(environ, start_response, 'static/robots.txt')\n    environ['oic.oas'] = OAS\n    if path.startswith('static/'):\n        return static(environ, start_response, path)\n    for (regex, callback) in URLS:\n        match = re.search(regex, path)\n        if (match is not None):\n            try:\n                environ['oic.url_args'] = match.groups()[0]\n            except IndexError:\n                environ['oic.url_args'] = path\n            logger.info(('callback: %s' % callback))\n            try:\n                return callback(environ, start_response, logger)\n            except Exception as err:\n                print(('%s' % err))\n                message = traceback.format_exception(*sys.exc_info())\n                print(message)\n                logger.exception(('%s' % err))\n                resp = ServiceError(('%s' % err))\n                return resp(environ, start_response)\n    LOGGER.debug(('unknown side: %s' % path))\n    resp = NotFound(\"Couldn't find the side you asked for!\")\n    return resp(environ, start_response)\n", "label": 0}
{"function": "\n\ndef decode(self, file):\n    fStart = file.tell()\n    identifier = None\n    try:\n        identifier = self.iEIEncoder.decode(file)\n    except UDHInformationElementIdentifierUnknownError:\n        pass\n    length = self.int8Encoder.decode(file)\n    data = None\n    if (identifier in self.dataEncoders):\n        data = self.dataEncoders[identifier].decode(file)\n    elif (length > 0):\n        data = self.read(file, length)\n    parsed = (file.tell() - fStart)\n    if (parsed != (length + 2)):\n        raise UDHParseError(('Invalid length: expected %d, parsed %d' % ((length + 2), parsed)))\n    if (identifier is None):\n        return None\n    return gsm_types.InformationElement(identifier, data)\n", "label": 0}
{"function": "\n\ndef test_func_adds_roots(self):\n\n    def add_roots(doc):\n        doc.add_root(AnotherModelInTestFunction())\n        doc.add_root(SomeModelInTestFunction())\n    handler = FunctionHandler(add_roots)\n    doc = Document()\n    handler.modify_document(doc)\n    if handler.failed:\n        raise RuntimeError(handler.error)\n    assert (len(doc.roots) == 2)\n", "label": 0}
{"function": "\n\ndef listen(self):\n    'Listen to incoming clients until\\n        self._running is set to False\\n        '\n    l = self.listener\n    self._running = True\n    try:\n        while self._running:\n            log.debug('Accept connection')\n            c = l.accept()\n            try:\n                action = c.recv()\n            except EOFError:\n                c.close()\n                continue\n            if isinstance(action, basestring):\n                args = ()\n                kwargs = {\n                    \n                }\n            else:\n                args = action.get('args', ())\n                kwargs = (action.get('kwargs') or {\n                    \n                })\n                action = action.get('action')\n            log.info(('Dispatch action \"%s\"' % action))\n            method = getattr(self, ('dispatch_%s' % action), None)\n            if method:\n                try:\n                    result = method(*args, **kwargs)\n                except Exception as err:\n                    log.exception(err)\n                    c.send({\n                        'error': True,\n                        'message': ('Exception in action %s - %s' % (action, err)),\n                    })\n                else:\n                    c.send({\n                        'error': False,\n                        'message': 'ok',\n                        'result': result,\n                    })\n            else:\n                log.warn(('No action %s' % action))\n                c.send({\n                    'error': True,\n                    'message': ('No action %s' % action),\n                })\n            c.close()\n    finally:\n        self._listener = None\n        l.close()\n    log.info('Exiting event loop')\n", "label": 1}
{"function": "\n\ndef convert_fragment(self, fragment, fd):\n    mdat = None\n    try:\n        f4v = F4V(fd, raw_payload=True)\n        for box in f4v:\n            if (box.type == 'mdat'):\n                mdat = box.payload.data\n                break\n    except F4VError as err:\n        self.logger.error('Failed to parse fragment {0}-{1}: {2}', fragment.segment, fragment.fragment, err)\n        return\n    if (not mdat):\n        self.logger.error('No MDAT box found in fragment {0}-{1}', fragment.segment, fragment.fragment)\n        return\n    try:\n        for chunk in self.concater.iter_chunks(buf=mdat, skip_header=True):\n            self.reader.buffer.write(chunk)\n            if self.closed:\n                break\n        else:\n            self.logger.debug('Download of fragment {0}-{1} complete', fragment.segment, fragment.fragment)\n    except IOError as err:\n        if ('Unknown tag type' in str(err)):\n            self.logger.error('Unknown tag type found, this stream is probably encrypted')\n            self.close()\n            return\n        self.logger.error('Error reading fragment {0}-{1}: {2}', fragment.segment, fragment.fragment, err)\n", "label": 1}
{"function": "\n\ndef test_os_stat(self):\n    'Test sizing os.stat and os.statvfs objects.\\n        '\n    try:\n        stat = os.stat(__file__)\n    except Exception:\n        pass\n    else:\n        stat_size = asizeof.asizeof(stat)\n        self.assertTrue((stat_size > 0), stat_size)\n        refs = asizeof.named_refs(stat)\n        ref_names = set([name for (name, _) in refs])\n        self.assertTrue((set(['st_mode', 'st_size', 'st_mtime']) <= ref_names), ref_names)\n    try:\n        stat = os.statvfs(__file__)\n    except Exception:\n        pass\n    else:\n        stat_size = asizeof.asizeof(stat)\n        self.assertTrue((stat_size > 0), stat_size)\n        refs = asizeof.named_refs(stat)\n        ref_names = set([name for (name, _) in refs])\n        self.assertTrue((set(['f_bsize', 'f_blocks']) <= ref_names), ref_names)\n", "label": 0}
{"function": "\n\ndef _unshorten_lnxlu(self, uri):\n    try:\n        r = requests.get(uri, headers=self._headers, timeout=self._timeout)\n        html = r.text\n        code = re.findall('/\\\\?click\\\\=(.*)\\\\.\"', html)\n        if (len(code) > 0):\n            payload = {\n                'click': code[0],\n            }\n            r = requests.get('http://lnx.lu/', params=payload, headers=self._headers, timeout=self._timeout)\n            return (r.url, r.status_code)\n        else:\n            return (uri, 'No click variable found')\n    except Exception as e:\n        return (uri, str(e))\n", "label": 0}
{"function": "\n\ndef test_encode_fields():\n    msg = APIEncodeMsg()\n    msg.enc = six.b('').join([six.int2byte(x) for x in range(0, 255)])\n    assert (msg.to_dict()['data']['enc'] == six.text_type('AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWprbG1ub3BxcnN0dXZ3eHl6e3x9fn+AgYKDhIWGh4iJiouMjY6PkJGSk5SVlpeYmZqbnJ2en6ChoqOkpaanqKmqq6ytrq+wsbKztLW2t7i5uru8vb6/wMHCw8TFxsfIycrLzM3Oz9DR0tPU1dbX2Nna29zd3t/g4eLj5OXm5+jp6uvs7e7v8PHy8/T19vf4+fr7/P3+'))\n    assert (msg.to_json() == '{\"data\": {\"enc\": \"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWprbG1ub3BxcnN0dXZ3eHl6e3x9fn+AgYKDhIWGh4iJiouMjY6PkJGSk5SVlpeYmZqbnJ2en6ChoqOkpaanqKmqq6ytrq+wsbKztLW2t7i5uru8vb6/wMHCw8TFxsfIycrLzM3Oz9DR0tPU1dbX2Nna29zd3t/g4eLj5OXm5+jp6uvs7e7v8PHy8/T19vf4+fr7/P3+\"}, \"type\": null}')\n    msg2 = APIEncodeMsg(data=msg.to_json())\n    assert (msg.to_dict() == msg2.to_dict())\n    assert (msg.to_json() == msg2.to_json())\n    assert (msg2.enc == msg.enc)\n    msg3 = APIEncodeMsg()\n    msg3.enc = six.u('xxxx')\n    assert (msg3.to_dict() == {\n        'data': {\n            'enc': 'eHh4eA==',\n        },\n        'type': None,\n    })\n    msg3.enc = six.b('xxxx')\n    assert (msg3.to_dict() == {\n        'data': {\n            'enc': 'eHh4eA==',\n        },\n        'type': None,\n    })\n    msg4 = APIEncodeMsg()\n    msg4.from_dict(msg.to_dict())\n    assert (msg4.to_dict() == msg.to_dict())\n", "label": 1}
{"function": "\n\ndef process_get_results_metadata(self, seqid, iprot, oprot):\n    args = get_results_metadata_args()\n    args.read(iprot)\n    iprot.readMessageEnd()\n    result = get_results_metadata_result()\n    try:\n        result.success = self._handler.get_results_metadata(args.handle)\n        msg_type = TMessageType.REPLY\n    except (TTransport.TTransportException, KeyboardInterrupt, SystemExit):\n        raise\n    except QueryNotFoundException as error:\n        msg_type = TMessageType.REPLY\n        result.error = error\n    except Exception as ex:\n        msg_type = TMessageType.EXCEPTION\n        logging.exception(ex)\n        result = TApplicationException(TApplicationException.INTERNAL_ERROR, 'Internal error')\n    oprot.writeMessageBegin('get_results_metadata', msg_type, seqid)\n    result.write(oprot)\n    oprot.writeMessageEnd()\n    oprot.trans.flush()\n", "label": 0}
{"function": "\n\n@add_method(Return)\ndef handle(self, ch):\n    msg = ch.message\n    msg.rx_channel = ch\n    if ch.on_return:\n        try:\n            ch.on_return(msg)\n        except Exception:\n            logger.error('ERROR in on_return() callback', exc_info=True)\n", "label": 0}
{"function": "\n\ndef shutdown(self):\n    if self.stopped:\n        return\n    self.stopped = True\n    try:\n        for handle in self.map_handles.values():\n            if (handle is not None):\n                handle.close()\n        for handle in self.file_handles.values():\n            if (handle is not None):\n                handle.close()\n    finally:\n        with self.lock:\n            if (self.create_lock_file is True):\n                os.remove(self.lock_file)\n    self.inited = False\n", "label": 0}
{"function": "\n\ndef __dump_xml(self, filename):\n    self.log.info('Dumping final status as XML: %s', filename)\n    root = etree.Element('FinalStatus')\n    if self.last_sec:\n        for (label, kpiset) in iteritems(self.last_sec[DataPoint.CUMULATIVE]):\n            root.append(self.__get_xml_summary(label, kpiset))\n    with open(get_full_path(filename), 'wb') as fhd:\n        tree = etree.ElementTree(root)\n        tree.write(fhd, pretty_print=True, encoding='UTF-8', xml_declaration=True)\n", "label": 0}
{"function": "\n\ndef backward_cpu(self, xs, gys):\n    assert (len(xs) == self.n_in)\n    assert (len(gys) == self.n_out)\n    return tuple((np.zeros_like(xs).astype(np.float32) for _ in six.moves.range(self.n_in)))\n", "label": 0}
{"function": "\n\ndef update(self):\n    ' Update loop for sensors->perception->control(->vehicle). '\n    try:\n        self.read_time()\n    except Exception as ex:\n        self.data.has_time = False\n        logging.exception(('CORE:\\tError in update loop (TIME) - %s' % ex))\n    try:\n        self.read_GPS()\n    except Exception as ex:\n        self.data.has_GPS = False\n        logging.exception(('CORE:\\tError in update loop (GPS) - %s' % ex))\n    try:\n        self.read_compass()\n    except Exception as ex:\n        self.data.has_compass = False\n        logging.exception(('CORE:\\tError in update loop (COMPASS) - %s' % ex))\n    try:\n        self.read_magnetometer()\n    except Exception as ex:\n        self.data.has_magnetometer = False\n        logging.exception(('CORE:\\tError in update loop (MAGNETOMETER) - %s' % ex))\n    try:\n        self.read_accelerometer()\n    except Exception as ex:\n        self.data.has_accelerometer = False\n        logging.exception(('CORE:\\tError in update loop (ACCELEROMETER) - %s' % ex))\n    try:\n        self.read_gyro()\n    except Exception as ex:\n        self.data.has_gyro = False\n        logging.exception(('CORE:\\tError in update loop (GYROSCOPE) - %s' % ex))\n    try:\n        self.read_temperature()\n    except Exception as ex:\n        self.data.has_temperature = False\n        logging.exception(('CORE:\\tError in update loop (TEMPERATURE) - %s' % ex))\n    try:\n        self._perception_unit.update(self.data)\n    except Exception as ex:\n        logging.exception(('CORE:\\tError in update loop (PERCEPTION) - %s' % ex))\n    try:\n        self._navigation_unit.update()\n    except Exception as ex:\n        logging.exception(('CORE:\\tError in update loop (NAVIGATION) - %s' % ex))\n", "label": 1}
{"function": "\n\ndef ssq_error(correct, estimate, mask):\n    'Compute the sum-squared-error for an image, where the estimate is\\n    multiplied by a scalar which minimizes the error. Sums over all pixels\\n    where mask is True. If the inputs are color, each color channel can be\\n    rescaled independently.'\n    assert (correct.ndim == 2)\n    if (np.sum(((estimate ** 2) * mask)) > 1e-05):\n        alpha = (np.sum(((correct * estimate) * mask)) / np.sum(((estimate ** 2) * mask)))\n    else:\n        alpha = 0.0\n    return np.sum((mask * ((correct - (alpha * estimate)) ** 2)))\n", "label": 0}
{"function": "\n\ndef _send_mail(self, handler, trap, is_duplicate):\n    if (is_duplicate and (not handler['mail_on_duplicate'])):\n        return\n    mail = handler['mail']\n    if (not mail):\n        return\n    recipients = handler['mail'].get('recipients')\n    if (not recipients):\n        return\n    subject = (handler['mail']['subject'] % {\n        'trap_oid': trap.oid,\n        'trap_name': ObjectId(trap.oid).name,\n        'ipaddress': trap.host,\n        'hostname': self.resolver.hostname_or_ip(trap.host),\n    })\n    ctxt = dict(trap=trap, dest_host=self.hostname)\n    try:\n        stats.incr('mail_sent_attempted', 1)\n        send_trap_email(recipients, 'trapperkeeper', subject, self.template_env, ctxt)\n        stats.incr('mail_sent_successful', 1)\n    except socket.error as err:\n        stats.incr('mail_sent_failed', 1)\n        logging.warning('Failed to send e-mail for trap: %s', err)\n", "label": 0}
{"function": "\n\ndef test_build_graph(self, huang_darwiche_nodes):\n    bbn = build_bbn(huang_darwiche_nodes)\n    nodes = dict([(node.name, node) for node in bbn.nodes])\n    assert (nodes['f_a'].parents == [])\n    assert (nodes['f_b'].parents == [nodes['f_a']])\n    assert (nodes['f_c'].parents == [nodes['f_a']])\n    assert (nodes['f_d'].parents == [nodes['f_b']])\n    assert (nodes['f_e'].parents == [nodes['f_c']])\n    assert (nodes['f_f'].parents == [nodes['f_d'], nodes['f_e']])\n    assert (nodes['f_g'].parents == [nodes['f_c']])\n    assert (nodes['f_h'].parents == [nodes['f_e'], nodes['f_g']])\n", "label": 1}
{"function": "\n\ndef _faster_to_representation(self, instance):\n    'Modified to_representation with optimizations.\\n\\n        1) Returns a plain old dict as opposed to OrderedDict.\\n            (Constructing ordered dict is ~100x slower than `{}`.)\\n        2) Ensure we use a cached list of fields\\n            (this optimization exists in DRF 3.2 but not 3.1)\\n\\n        Arguments:\\n            instance: a model instance or data object\\n        Returns:\\n            Dict of primitive datatypes.\\n        '\n    ret = {\n        \n    }\n    fields = self._readable_fields\n    for field in fields:\n        try:\n            attribute = field.get_attribute(instance)\n        except SkipField:\n            continue\n        if (attribute is None):\n            ret[field.field_name] = None\n        else:\n            ret[field.field_name] = field.to_representation(attribute)\n    return ret\n", "label": 0}
{"function": "\n\ndef store_catch_412_HTTPError(entity):\n    'Returns the stored Entity if the function succeeds or None if the 412 is caught.'\n    try:\n        return syn.store(entity)\n    except SynapseHTTPError as err:\n        if (err.response.status_code == 412):\n            return None\n        raise\n", "label": 0}
{"function": "\n\ndef unlink(self, name):\n    symlinks = self.read_bootstrap().get('symlinks', {\n        \n    })\n    removed_targets = set()\n    found = False\n    for (source_glob, target) in symlinks.items():\n        if self._islinkkey(source_glob, name):\n            found = True\n            for (source, target) in self.expandtargets(source_glob, target):\n                self._remove_link_target(source, target)\n                removed_targets.add(target)\n                shutil.move(source, target)\n                print(tty.progress('Moved {0} -> {1}'.format(collapseuser(source), collapseuser(target))))\n    if (not found):\n        raise StowError('No symlink found with name: {0}'.format(name))\n    try:\n        os.rmdir(os.path.join(self.symlink_dir, name))\n    except OSError as e:\n        if (e.errno != errno.ENOTEMPTY):\n            raise e\n    self.remove_symlink(name)\n    self._update_target_cache((set(self._cached_targets()) - removed_targets))\n", "label": 0}
{"function": "\n\ndef _make_table(self, table, fields):\n    'Set up the schema of the database. `fields` is a mapping\\n        from field names to `Type`s. Columns are added if necessary.\\n        '\n    with self.transaction() as tx:\n        rows = tx.query(('PRAGMA table_info(%s)' % table))\n    current_fields = set([row[1] for row in rows])\n    field_names = set(fields.keys())\n    if current_fields.issuperset(field_names):\n        return\n    if (not current_fields):\n        columns = []\n        for (name, typ) in fields.items():\n            columns.append('{0} {1}'.format(name, typ.sql))\n        setup_sql = 'CREATE TABLE {0} ({1});\\n'.format(table, ', '.join(columns))\n    else:\n        setup_sql = ''\n        for (name, typ) in fields.items():\n            if (name in current_fields):\n                continue\n            setup_sql += 'ALTER TABLE {0} ADD COLUMN {1} {2};\\n'.format(table, name, typ.sql)\n    with self.transaction() as tx:\n        tx.script(setup_sql)\n", "label": 0}
{"function": "\n\ndef undo(self, workflow_dict):\n    LOG.info('Running undo...')\n    try:\n        if (('databaseinfra' not in workflow_dict) and ('hosts' not in workflow_dict)):\n            LOG.info('We could not find a databaseinfra inside the workflow_dict')\n            return False\n        if (len(workflow_dict['hosts']) == 1):\n            return True\n        databaseinfraattr = DatabaseInfraAttr.objects.filter(databaseinfra=workflow_dict['databaseinfra'])\n        cs_credentials = get_credentials_for(environment=workflow_dict['environment'], credential_type=CredentialType.CLOUDSTACK)\n        networkapi_credentials = get_credentials_for(environment=workflow_dict['environment'], credential_type=CredentialType.NETWORKAPI)\n        cs_provider = CloudStackProvider(credentials=cs_credentials, networkapi_credentials=networkapi_credentials)\n        networkapi_equipment_id = workflow_dict.get('networkapi_equipment_id')\n        for infra_attr in databaseinfraattr:\n            networkapi_equipment_id = infra_attr.networkapi_equipment_id\n            networkapi_ip_id = infra_attr.networkapi_ip_id\n            if networkapi_ip_id:\n                LOG.info(('Removing network api IP for %s' % networkapi_ip_id))\n                if (not cs_provider.remove_networkapi_ip(equipment_id=networkapi_equipment_id, ip_id=networkapi_ip_id)):\n                    return False\n            LOG.info(('Removing secondary_ip for %s' % infra_attr.cs_ip_id))\n            if (not cs_provider.remove_secondary_ips(infra_attr.cs_ip_id)):\n                return False\n            LOG.info('Secondary ip deleted!')\n            infra_attr.delete()\n            LOG.info('Databaseinfraattr deleted!')\n        if networkapi_equipment_id:\n            cs_provider.remove_networkapi_equipment(equipment_id=networkapi_equipment_id)\n        return True\n    except Exception:\n        traceback = full_stack()\n        workflow_dict['exceptions']['error_codes'].append(DBAAS_0010)\n        workflow_dict['exceptions']['traceback'].append(traceback)\n        return False\n", "label": 1}
{"function": "\n\ndef start(self, environment=None, user=None):\n    'Mark this result started.'\n    envs = [environment]\n    try:\n        latest = self.results.get(is_latest=True, tester=user, environment=environment)\n        if (latest.status == Result.STATUS.skipped):\n            envs = self.environments.all()\n    except ObjectDoesNotExist:\n        pass\n    for env in envs:\n        Result.objects.create(runcaseversion=self, tester=user, environment=env, status=Result.STATUS.started, user=user)\n", "label": 0}
{"function": "\n\n@pytest.mark.timeout(10)\n@pytest.mark.parametrize('ssh_command', [None, 'fake_ssh_command'])\ndef test_sshkey(tmpdir, ssh_command):\n    tmpdir = str(tmpdir)\n    test_path = os.path.dirname(os.path.realpath(__file__))\n    with open(os.path.join(test_path, 'private_key')) as k:\n        private_key = k.read()\n    abs_source = os.path.join(tmpdir, 'deploy~git')\n    dest = os.path.join(tmpdir, 'dest-dir')\n    with mock.patch('subprocess.check_call') as submock:\n        with mock.patch('git.Repo') as gitmock:\n\n            def check_environ(*args):\n                if ssh_command:\n                    with open(os.environ['GIT_SSH']) as gitfile:\n                        assert (ssh_command in gitfile.read())\n            submock.check_call.side_effect = check_environ\n            url = 'git@example.org:squadron/test-repo.git'\n            version = 'a057eb0faaa8'\n            with open(abs_source, 'w') as gfile:\n                gfile.write(get_git_file(url, '@version', 'filename', '--depth=1'))\n            dest = os.path.join(tmpdir, 'dest-dir2')\n            if ssh_command:\n                os.environ['GIT_SSH'] = ssh_command\n            finalfile = ext_git(abs_source, dest, {\n                'version': version,\n            }, get_loader(), {\n                'filename': (lambda : private_key),\n            })\n            expected_sub_calls = [mock.call('git clone --depth=1 -- {} {} '.format(url, finalfile).split())]\n            expected_git_calls = [mock.call(finalfile), mock.call().git.checkout(version)]\n            assert (expected_sub_calls == submock.mock_calls)\n            assert (expected_git_calls == gitmock.mock_calls)\n            if ssh_command:\n                assert ('GIT_SSH' in os.environ)\n                assert (os.environ['GIT_SSH'] == ssh_command)\n            else:\n                assert ('GIT_SSH' not in os.environ)\n", "label": 1}
{"function": "\n\ndef result_fail(self, environment=None, comment='', stepnumber=None, bug='', user=None):\n    'Create a failed result for this case.'\n    result = Result.objects.create(runcaseversion=self, tester=user, environment=environment, status=Result.STATUS.failed, comment=comment, user=user)\n    if (stepnumber is not None):\n        try:\n            step = self.caseversion.steps.get(number=stepnumber)\n        except CaseStep.DoesNotExist:\n            pass\n        else:\n            stepresult = StepResult(result=result, step=step)\n            stepresult.status = StepResult.STATUS.failed\n            stepresult.bug_url = bug\n            stepresult.save(user=user)\n    self.save(force_update=True, user=user)\n", "label": 0}
{"function": "\n\ndef ValidateTree(self, queries, additional_visitors=(), max_alter_rows=100000, allowed_engines=('InnoDB',), fail_fast=False):\n    'Validate a parse tree.\\n\\n    Args:\\n      tokens: pyparsing parse tree\\n\\n    Returns:\\n      Whether the tree validated\\n    '\n    visitors = ([ShardSetChecker(self._schema), AlterChecker(self._schema, max_alter_rows=max_alter_rows), CreateDatabaseChecker(self._schema), DropDatabaseChecker(self._schema), CreateTableChecker(self._schema, allowed_engines=allowed_engines), DropTableChecker(self._schema), ReplaceChecker(self._schema), ColumnChecker(self._schema)] + list(additional_visitors))\n    for query in queries:\n        self._CheckCancelled()\n        assert (query.getName() == 'query'), ('Invalid second-level token: %s' % query.getName())\n        logging.debug('Visiting: %s', query)\n        for visitor in visitors:\n            visitor.visit([query])\n            if (fail_fast and (visitor.Errors() or visitor.Warnings())):\n                self._Finalize(visitors)\n                return False\n        if self._callback:\n            self._callback(self._loc)\n    self._Finalize(visitors)\n    return ((not self._errors) and (not self._warnings))\n", "label": 1}
{"function": "\n\ndef get(self, *args):\n    self.preflight()\n    self.set_header('Content-Type', 'application/json')\n    m = self.settings.get('manager')\n    pname = ('%s.%s' % (args[0], args[1]))\n    if (not self.api_key.can_read(pname)):\n        raise HTTPError(403)\n    try:\n        info = m.info(pname)\n    except ProcessError:\n        self.set_status(404)\n        return self.write({\n            'error': 'not_found',\n        })\n    self.write(info)\n", "label": 0}
{"function": "\n\ndef send_test(self, test=True):\n    if (not test):\n        try:\n            del self._data['test']\n        except KeyError:\n            pass\n    else:\n        self._data['test'] = 1\n    return self\n", "label": 0}
{"function": "\n\ndef print_stats(stats):\n    console_logger.info((((' %-' + str(STATS_NAME_WIDTH)) + 's %7s %12s %7s %7s %7s  | %7s %7s') % ('Name', '# reqs', '# fails', 'Avg', 'Min', 'Max', 'Median', 'req/s')))\n    console_logger.info(('-' * (80 + STATS_NAME_WIDTH)))\n    total_rps = 0\n    total_reqs = 0\n    total_failures = 0\n    for key in sorted(stats.iterkeys()):\n        r = stats[key]\n        total_rps += r.current_rps\n        total_reqs += r.num_requests\n        total_failures += r.num_failures\n        console_logger.info(r)\n    console_logger.info(('-' * (80 + STATS_NAME_WIDTH)))\n    try:\n        fail_percent = ((total_failures / float(total_reqs)) * 100)\n    except ZeroDivisionError:\n        fail_percent = 0\n    console_logger.info((((' %-' + str(STATS_NAME_WIDTH)) + 's %7d %12s %42.2f') % ('Total', total_reqs, ('%d(%.2f%%)' % (total_failures, fail_percent)), total_rps)))\n    console_logger.info('')\n", "label": 0}
{"function": "\n\ndef __getattr__(self, name):\n    t = type(self)\n    try:\n        i = t.name_to_idx[name]\n    except KeyError:\n        raise AttributeError(name)\n    f = t.fields[i]\n    if (i < len(self.data)):\n        v = self.data[i]\n    else:\n        v = ''\n    if (len(f) >= 3):\n        if (v == ''):\n            return None\n        return f[2](v)\n    else:\n        return v\n", "label": 0}
{"function": "\n\ndef _test_sync_state_helper(self, known_net_ids, active_net_ids):\n    active_networks = set((mock.Mock(id=netid) for netid in active_net_ids))\n    with mock.patch(DHCP_PLUGIN) as plug:\n        mock_plugin = mock.Mock()\n        mock_plugin.get_active_networks_info.return_value = active_networks\n        plug.return_value = mock_plugin\n        dhcp = dhcp_agent.DhcpAgent(HOSTNAME)\n        attrs_to_mock = dict([(a, mock.DEFAULT) for a in ['disable_dhcp_helper', 'cache', 'safe_configure_dhcp_for_network']])\n        with mock.patch.multiple(dhcp, **attrs_to_mock) as mocks:\n            mocks['cache'].get_network_ids.return_value = known_net_ids\n            dhcp.sync_state()\n            diff = (set(known_net_ids) - set(active_net_ids))\n            exp_disable = [mock.call(net_id) for net_id in diff]\n            mocks['cache'].assert_has_calls([mock.call.get_network_ids()])\n            mocks['disable_dhcp_helper'].assert_has_calls(exp_disable)\n", "label": 0}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (node in self.nodes_seen):\n        self.nodes_seen.discard(node)\n        self.process_node(fgraph, node)\n    if (not isinstance(node, string_types)):\n        assert node.inputs\n    if isinstance(new_r, graph.Constant):\n        self.process_constant(fgraph, new_r)\n", "label": 0}
{"function": "\n\ndef _initialize_trunk_interfaces_to_none(self, switch_ip):\n    'Initialize all nexus interfaces to trunk allowed none.'\n    try:\n        switch_ifs = self._mdriver._get_switch_interfaces(switch_ip)\n        if (not switch_ifs):\n            LOG.debug('Skipping switch %s which has no configured interfaces', switch_ip)\n            return\n        self._driver.initialize_all_switch_interfaces(switch_ifs)\n    except Exception:\n        with excutils.save_and_reraise_exception():\n            LOG.warning(_LW('Unable to initialize interfaces to switch %(switch_ip)s'), {\n                'switch_ip': switch_ip,\n            })\n            self._mdriver.register_switch_as_inactive(switch_ip, 'replay init_interface')\n    for (switch_ip, intf_type, port, is_native, ch_grp) in switch_ifs:\n        try:\n            reserved = nxos_db.get_reserved_port_binding(switch_ip, self._mdriver.format_interface_name(intf_type, port))\n        except excep.NexusPortBindingNotFound:\n            continue\n        if (reserved[0].channel_group != ch_grp):\n            self._mdriver._change_baremetal_interfaces(switch_ip, intf_type, port, reserved[0].channel_group, ch_grp)\n    if self._mdriver.is_replay_enabled():\n        return\n    try:\n        mgr = self._driver.nxos_connect(switch_ip)\n        self._driver._close_session(mgr, switch_ip)\n    except Exception:\n        LOG.warning(_LW('Failed to release connection after initialize interfaces for switch %(switch_ip)s'), {\n            'switch_ip': switch_ip,\n        })\n", "label": 1}
{"function": "\n\ndef test_current_time(self):\n    self.skipTest('time.xmlrpc.com is unreliable')\n    server = xmlrpclib.ServerProxy('http://time.xmlrpc.com/RPC2')\n    try:\n        t0 = server.currentTime.getCurrentTime()\n    except socket.error as e:\n        self.skipTest(('network error: %s' % e))\n        return\n    t1 = xmlrpclib.DateTime()\n    dt0 = xmlrpclib._datetime_type(t0.value)\n    dt1 = xmlrpclib._datetime_type(t1.value)\n    if (dt0 > dt1):\n        delta = (dt0 - dt1)\n    else:\n        delta = (dt1 - dt0)\n    self.assertTrue((delta.days <= 1))\n", "label": 0}
{"function": "\n\ndef handle(self, request, data):\n    volume_type_id = self.initial['id']\n    try:\n        cinder.volume_type_update(request, volume_type_id, data['name'], data['description'])\n        message = _('Successfully updated volume type.')\n        messages.success(request, message)\n        return True\n    except Exception as ex:\n        redirect = reverse('horizon:admin:volumes:index')\n        if (ex.code == 409):\n            error_message = _('New name conflicts with another volume type.')\n        else:\n            error_message = _('Unable to update volume type.')\n        exceptions.handle(request, error_message, redirect=redirect)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, position):\n    (i, j) = self._get_indexes(position)\n    if isinstance(i, slice):\n        if (j is None):\n            return Table(self.data[position])\n        else:\n            return Table((cells[j] for cells in self.data[i]))\n    else:\n        try:\n            row = self.data[i]\n        except IndexError:\n            msg = 'no row at index %r of %d-row table'\n            raise IndexError((msg % (position, len(self))))\n        if (j is None):\n            return row\n        else:\n            return row[j]\n", "label": 0}
{"function": "\n\ndef test_exists_many_with_none_keys(self):\n    try:\n        TestExistsMany.client.exists_many(None, {\n            \n        })\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'Keys should be specified as a list or tuple.')\n", "label": 0}
{"function": "\n\ndef _handle_message(self, opcode, data):\n    if self.client_terminated:\n        return\n    if (opcode == 1):\n        try:\n            decoded = data.decode('utf-8')\n        except UnicodeDecodeError:\n            self._abort()\n            return\n        self._run_callback(self.handler.on_message, decoded)\n    elif (opcode == 2):\n        self._run_callback(self.handler.on_message, data)\n    elif (opcode == 8):\n        self.client_terminated = True\n        if (len(data) >= 2):\n            self.handler.close_code = struct.unpack('>H', data[:2])[0]\n        if (len(data) > 2):\n            self.handler.close_reason = to_unicode(data[2:])\n        self.close()\n    elif (opcode == 9):\n        self._write_frame(True, 10, data)\n    elif (opcode == 10):\n        self._run_callback(self.handler.on_pong, data)\n    else:\n        self._abort()\n", "label": 1}
{"function": "\n\ndef mergeStatementsSequence(self, statement_sequence):\n    assert (statement_sequence.parent is self)\n    old_statements = list(self.getStatements())\n    assert (statement_sequence in old_statements), (statement_sequence, self)\n    merge_index = old_statements.index(statement_sequence)\n    new_statements = ((tuple(old_statements[:merge_index]) + statement_sequence.getStatements()) + tuple(old_statements[(merge_index + 1):]))\n    self.setChild('statements', new_statements)\n", "label": 0}
{"function": "\n\ndef _handle_message(self, message, buffer, record_fn):\n    'Build a response calling record_fn on all the TlsRecords in message\\n\\n        message: bytes to parse as TlsRecords\\n        record_fn: one of on_tls_request, on_tls_response to handle the record\\n        Returns tuple containing the bytes to send for all the records handled and any remaining unparsed data\\n        '\n    out = ''\n    message = (buffer.buffer + message)\n    buffer.buffer = ''\n    remaining = message\n    while remaining:\n        record = None\n        try:\n            (record, remaining) = tls.parse_tls(remaining, throw_on_incomplete=True)\n        except tls.types.TlsNotEnoughDataError:\n            if buffer.should_buffer:\n                buffer.buffer = remaining\n                if (len(buffer.buffer) >= buffer.MAX_BUFFER):\n                    buffer.buffer = ''\n            return out\n        if (not record):\n            return out\n        record_bytes = record_fn(record)\n        if record_bytes:\n            out += record_bytes\n        if (record.content_type == record.CONTENT_TYPE.CHANGE_CIPHER_SPEC):\n            buffer.should_buffer = False\n    return out\n", "label": 1}
{"function": "\n\n@classmethod\n@quickcache(['domain', 'app_id'])\ndef _app_data(cls, domain, app_id):\n    defaults = MaltAppData(AMPLIFIES_NOT_SET, AMPLIFIES_NOT_SET, 15, 3, False)\n    if (not app_id):\n        return defaults\n    try:\n        app = get_app(domain, app_id)\n    except Http404:\n        logger.debug(('App not found %s' % app_id))\n        return defaults\n    return MaltAppData(getattr(app, 'amplifies_workers', AMPLIFIES_NOT_SET), getattr(app, 'amplifies_project', AMPLIFIES_NOT_SET), getattr(app, 'minimum_use_threshold', 15), getattr(app, 'experienced_threshold', 3), app.is_deleted())\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _aggregator_counter_kind(combine_fn):\n    \"Returns the counter aggregation kind for the combine_fn passed in.\\n\\n    Args:\\n      combine_fn: The combining function used in an Aggregator.\\n\\n    Returns:\\n      The aggregation_kind (to use in a Counter) that matches combine_fn.\\n\\n    Raises:\\n      ValueError if the combine_fn doesn't map to any supported\\n      aggregation kind.\\n    \"\n    combine_kind_map = {\n        sum: Counter.SUM,\n        max: Counter.MAX,\n        min: Counter.MIN,\n        combiners.Mean: Counter.MEAN,\n    }\n    try:\n        return combine_kind_map[combine_fn]\n    except KeyError:\n        try:\n            return combine_kind_map[combine_fn.__class__]\n        except KeyError:\n            raise ValueError(('combine_fn %r (class %r) does not map to a supported aggregation kind' % (combine_fn, combine_fn.__class__)))\n", "label": 0}
{"function": "\n\ndef run(self):\n    '\\n        Kill any open Redshift sessions for the given database.\\n        '\n    connection = self.output().connect()\n    query = \"select pg_terminate_backend(process) from STV_SESSIONS where db_name=%s and user_name != 'rdsdb' and process != pg_backend_pid()\"\n    cursor = connection.cursor()\n    logger.info('Killing all open Redshift sessions for database: %s', self.database)\n    try:\n        cursor.execute(query, (self.database,))\n        cursor.close()\n        connection.commit()\n    except psycopg2.DatabaseError as e:\n        if (e.message and ('EOF' in e.message)):\n            connection.close()\n            logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n            time.sleep(self.connection_reset_wait_seconds)\n            logger.info('Reconnecting to Redshift')\n            connection = self.output().connect()\n        else:\n            raise\n    try:\n        self.output().touch(connection)\n        connection.commit()\n    finally:\n        connection.close()\n    logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n", "label": 0}
{"function": "\n\n@AccountRequired\n@PostOnly\ndef unfollow(request, user_id):\n    followed = get_object_or_404(User, pk=user_id)\n    try:\n        follow_instance = UserToUserFollow.objects.get(follower=request.user, followed=followed)\n        follow_instance.is_following = False\n        follow_instance.stopped_following = datetime.datetime.now()\n        follow_instance.save()\n    except UserToUserFollow.DoesNotExist:\n        pass\n    cache.bust(followed)\n    if request.is_ajax():\n        button = render_inclusiontag(request, 'follow_button followed', 'users_tags', {\n            'followed': followed,\n        })\n        return json_response({\n            'button': button,\n        })\n    else:\n        return redirect(followed)\n", "label": 0}
{"function": "\n\ndef configure_formatter(self, config):\n    'Configure a formatter from a dictionary.'\n    if ('()' in config):\n        factory = config['()']\n        try:\n            result = self.configure_custom(config)\n        except TypeError as te:\n            if (\"'format'\" not in str(te)):\n                raise\n            config['fmt'] = config.pop('format')\n            config['()'] = factory\n            result = self.configure_custom(config)\n    else:\n        fmt = config.get('format', None)\n        dfmt = config.get('datefmt', None)\n        result = logging.Formatter(fmt, dfmt)\n    return result\n", "label": 0}
{"function": "\n\ndef _on_auth(self, user):\n    if (not user):\n        raise tornado.web.HTTPError(403, 'Google auth failed')\n    access_token = user['access_token']\n    try:\n        response = httpclient.HTTPClient().fetch('https://www.googleapis.com/plus/v1/people/me', headers={\n            'Authorization': ('Bearer %s' % access_token),\n        })\n    except Exception as e:\n        raise tornado.web.HTTPError(403, ('Google auth failed: %s' % e))\n    email = json.loads(response.body.decode('utf-8'))['emails'][0]['value']\n    if (not re.match(self.application.options.auth, email)):\n        message = \"Access denied to '{email}'. Please use another account or ask your admin to add your email to flower --auth.\".format(email=email)\n        raise tornado.web.HTTPError(403, message)\n    self.set_secure_cookie('user', str(email))\n    next = self.get_argument('next', '/')\n    self.redirect(next)\n", "label": 0}
{"function": "\n\ndef handle_entityref(self, ref):\n    if (not self.elementstack):\n        return\n    if (ref in ('lt', 'gt', 'quot', 'amp', 'apos')):\n        text = ('&%s;' % ref)\n    elif (ref in self.entities):\n        text = self.entities[ref]\n        if (text.startswith('&#') and text.endswith(';')):\n            return self.handle_entityref(text)\n    else:\n        try:\n            name2codepoint[ref]\n        except KeyError:\n            text = ('&%s;' % ref)\n        else:\n            text = chr(name2codepoint[ref]).encode('utf-8')\n    self.elementstack[(- 1)][2].append(text)\n", "label": 1}
{"function": "\n\ndef clean_email(self):\n    value = self.cleaned_data['email']\n    if (UNIQUE_EMAIL or EMAIL_AUTHENTICATION):\n        try:\n            User.objects.get(email__iexact=value)\n        except User.DoesNotExist:\n            return value\n        raise forms.ValidationError(_('A user is registered with this e-mail address.'))\n    return value\n", "label": 0}
{"function": "\n\ndef get_contents_if_file(contents_or_file_name):\n    'Get the contents of a file.\\n\\n    If the value passed in is a file name or file URI, return the\\n    contents. If not, or there is an error reading the file contents,\\n    return the value passed in as the contents.\\n\\n    For example, a workflow definition will be returned if either the\\n    workflow definition file name, or file URI are passed in, or the\\n    actual workflow definition itself is passed in.\\n    '\n    try:\n        if parse.urlparse(contents_or_file_name).scheme:\n            definition_url = contents_or_file_name\n        else:\n            path = os.path.abspath(contents_or_file_name)\n            definition_url = parse.urljoin('file:', request.pathname2url(path))\n        return request.urlopen(definition_url).read().decode('utf8')\n    except Exception:\n        return contents_or_file_name\n", "label": 0}
{"function": "\n\ndef get_module_path(module_name):\n    try:\n        module = import_module(module_name)\n    except ImportError as e:\n        raise ImproperlyConfigured(('Error importing HIDE_IN_STACKTRACES: %s' % (e,)))\n    else:\n        source_path = inspect.getsourcefile(module)\n        if source_path.endswith('__init__.py'):\n            source_path = os.path.dirname(source_path)\n        return os.path.realpath(source_path)\n", "label": 0}
{"function": "\n\ndef _read_bytes_from_socket(self, msglen):\n    ' Read bytes from the socket. '\n    chunks = []\n    bytes_recd = 0\n    while (bytes_recd < msglen):\n        if self.stop.is_set():\n            raise InterruptLoop('Stopped while reading from socket')\n        try:\n            chunk = self.socket.recv(min((msglen - bytes_recd), 2048))\n            if (chunk == b''):\n                raise socket.error('socket connection broken')\n            chunks.append(chunk)\n            bytes_recd += len(chunk)\n        except socket.timeout:\n            continue\n        except ssl.SSLError as exc:\n            if _is_ssl_timeout(exc):\n                continue\n            raise\n    return b''.join(chunks)\n", "label": 0}
{"function": "\n\ndef test_many_to_one(self, session, objects, calls):\n    users = session.query(models.User).all()\n    users[0].addresses\n    assert (len(calls) == 1)\n    call = calls[0]\n    assert (call.objects == (models.User, 'User:1', 'addresses'))\n    assert ('users[0].addresses' in ''.join(call.frame[4]))\n", "label": 0}
{"function": "\n\n@register.tag\ndef shardtype(parser, token):\n    try:\n        (tag_name, shard_type) = token.split_contents()\n    except ValueError:\n        raise template.TemplateSyntaxError(('%r tag requires a single argument' % token.contents.split()[0]))\n    if (not ((shard_type[0] == shard_type[(- 1)]) and (shard_type[0] in ('\"', \"'\")))):\n        raise template.TemplateSyntaxError((\"%r tag's argument should be in quotes\" % tag_name))\n    shard_type = shard_type[1:(- 1)]\n    nodelist = parser.parse(('end{0}'.format(tag_name),))\n    parser.delete_first_token()\n    return EmailShardTypeNode(shard_type, nodelist)\n", "label": 0}
{"function": "\n\ndef _parse_changelog(self, pkg_name):\n    with open('ChangeLog.md') as f:\n        lineiter = iter(f)\n        for line in lineiter:\n            match = re.search(('^%s\\\\s+(.*)' % pkg_name), line.strip())\n            if (match is None):\n                continue\n            length = len(match.group(1))\n            version = match.group(1).strip()\n            if (lineiter.next().count('-') != len(match.group(0))):\n                continue\n            while 1:\n                change_info = lineiter.next().strip()\n                if change_info:\n                    break\n            match = re.search('released on (\\\\w+\\\\s+\\\\d+\\\\w+\\\\s+\\\\d+)', change_info)\n            if (match is None):\n                continue\n            datestr = match.group(1)\n            return (version, self._parse_date(datestr))\n", "label": 0}
{"function": "\n\ndef _run(self):\n    result = StatusCheckResult(check=self)\n    auth = None\n    if (self.username or self.password):\n        auth = (self.username, self.password)\n    try:\n        resp = requests.get(self.endpoint, timeout=self.timeout, verify=self.verify_ssl_certificate, auth=auth, headers={\n            'User-Agent': settings.HTTP_USER_AGENT,\n        })\n    except requests.RequestException as e:\n        result.error = ('Request error occurred: %s' % (e.message,))\n        result.succeeded = False\n    else:\n        if (self.status_code and (resp.status_code != int(self.status_code))):\n            result.error = ('Wrong code: got %s (expected %s)' % (resp.status_code, int(self.status_code)))\n            result.succeeded = False\n            result.raw_data = resp.content\n        elif self.text_match:\n            if (not re.search(self.text_match, resp.content)):\n                result.error = ('Failed to find match regex /%s/ in response body' % self.text_match)\n                result.raw_data = resp.content\n                result.succeeded = False\n            else:\n                result.succeeded = True\n        else:\n            result.succeeded = True\n    return result\n", "label": 1}
{"function": "\n\n@authorization_required(is_admin=True)\n@threaded\ndef put(self, uid):\n    try:\n        user = Users.get(id=uid)\n    except DoesNotExist:\n        raise HTTPError(404)\n    try:\n        user.login = self.json.get('login', user.login)\n        user.email = self.json.get('email', user.email)\n        user.is_admin = bool(self.json.get('is_admin', user.is_admin))\n        user.password = self.json.get('password', user.password)\n        if (not all((isinstance(user.login, text_type), isinstance(user.email, text_type), (LOGIN_EXP.match(str(user.login)) is not None), (user.password and (len(user.password) > 3)), (EMAIL_EXP.match(str(user.email)) is not None)))):\n            raise HTTPError(400)\n    except:\n        raise HTTPError(400)\n    user.save()\n    self.response({\n        'id': user.id,\n        'login': user.login,\n        'email': user.email,\n        'is_admin': user.is_admin,\n    })\n", "label": 0}
{"function": "\n\n@pytest.fixture(scope='module')\ndef smtp_servers(request):\n    try:\n        from .local_smtp_severs import SERVERS\n    except ImportError:\n        from .smtp_servers import SERVERS\n    return dict([(k, SMTPTestParams(**v)) for (k, v) in SERVERS.items()])\n", "label": 0}
{"function": "\n\ndef _version_str_to_list(version):\n    'convert a version string to a list of ints\\n\\n    non-int segments are excluded\\n    '\n    v = []\n    for part in version.split('.'):\n        try:\n            v.append(int(part))\n        except ValueError:\n            pass\n    return v\n", "label": 0}
{"function": "\n\n@classmethod\ndef award_points(cls, user, addon, status, **kwargs):\n    'Awards points to user based on an event and the queue.\\n\\n        `event` is one of the `REVIEWED_` keys in constants.\\n        `status` is one of the `STATUS_` keys in constants.\\n\\n        '\n    event = cls.get_event(addon, status, **kwargs)\n    score = amo.REVIEWED_SCORES.get(event)\n    try:\n        vq = ViewQueue.objects.get(addon_slug=addon.slug)\n        if (vq.waiting_time_days > amo.REVIEWED_OVERDUE_LIMIT):\n            days_over = (vq.waiting_time_days - amo.REVIEWED_OVERDUE_LIMIT)\n            bonus = (days_over * amo.REVIEWED_OVERDUE_BONUS)\n            score = (score + bonus)\n    except ViewQueue.DoesNotExist:\n        pass\n    if score:\n        cls.objects.create(user=user, addon=addon, score=score, note_key=event)\n        cls.get_key(invalidate=True)\n        user_log.info(('Awarding %s points to user %s for \"%s\" for addon %s' % (score, user, amo.REVIEWED_CHOICES[event], addon.id)).encode('utf-8'))\n    return score\n", "label": 0}
{"function": "\n\ndef test_DELETE(self):\n    res = self.app.delete('/crud.json?ref.id=1', expect_errors=False)\n    print('Received:', res.body)\n    result = json.loads(res.text)\n    print(result)\n    assert (result['data']['id'] == 1)\n    assert (result['data']['name'] == u('test'))\n    assert (result['message'] == 'delete')\n", "label": 0}
{"function": "\n\n@login_required\n@permission_required('workshops.add_todoitem', raise_exception=True)\ndef todos_add(request, event_ident):\n    'Add a standard TodoItems for a specific event.'\n    try:\n        event = Event.get_by_ident(event_ident)\n    except Event.DoesNotExist:\n        raise Http404('Event matching query does not exist.')\n    dt = datetime.datetime\n    timedelta = datetime.timedelta\n    initial = []\n    base = dt.now()\n    if ((not event.start) or (not event.end)):\n        initial = [{\n            'title': 'Set date with host',\n            'due': (dt.now() + timedelta(days=30)),\n            'event': event,\n        }]\n    formset = TodoFormSet(queryset=TodoItem.objects.none(), initial=(initial + [{\n        'title': 'Set up a workshop website',\n        'due': (base + timedelta(days=7)),\n        'event': event,\n    }, {\n        'title': 'Find instructor #1',\n        'due': (base + timedelta(days=14)),\n        'event': event,\n    }, {\n        'title': 'Find instructor #2',\n        'due': (base + timedelta(days=14)),\n        'event': event,\n    }, {\n        'title': 'Follow up that instructors have booked travel',\n        'due': (base + timedelta(days=21)),\n        'event': event,\n    }, {\n        'title': 'Set up pre-workshop survey',\n        'due': ((event.start - timedelta(days=7)) if event.start else ''),\n        'event': event,\n    }, {\n        'title': 'Make sure instructors are set with materials',\n        'due': ((event.start - timedelta(days=1)) if event.start else ''),\n        'event': event,\n    }, {\n        'title': 'Submit invoice',\n        'due': ((event.end + timedelta(days=2)) if event.end else ''),\n        'event': event,\n    }, {\n        'title': 'Make sure instructors are reimbursed',\n        'due': ((event.end + timedelta(days=7)) if event.end else ''),\n        'event': event,\n    }, {\n        'title': 'Get attendee list',\n        'due': ((event.end + timedelta(days=7)) if event.end else ''),\n        'event': event,\n    }]))\n    if (request.method == 'POST'):\n        formset = TodoFormSet(request.POST)\n        if formset.is_valid():\n            formset.save()\n            messages.success(request, 'Successfully added a bunch of TODOs.', extra_tags='todos')\n            return redirect(reverse(event_details, args=(event.get_ident(),)))\n        else:\n            messages.error(request, 'Fix errors below.')\n    context = {\n        'title': 'Add standard TODOs to the event',\n        'formset': formset,\n        'helper': bootstrap_helper_inline_formsets,\n        'event': event,\n    }\n    return render(request, 'workshops/todos_add.html', context)\n", "label": 1}
{"function": "\n\ndef connect_to_nsqd(self, conn):\n    if (not self.is_running):\n        return\n    if (conn in self.conns):\n        self.logger.debug(('[%s] already connected' % conn))\n        return\n    if (conn in self.pending):\n        self.logger.debug(('[%s] already pending' % conn))\n        return\n    self.logger.debug(('[%s] connecting...' % conn))\n    conn.on_response.connect(self.handle_response)\n    conn.on_error.connect(self.handle_error)\n    conn.on_finish.connect(self.handle_finish)\n    conn.on_requeue.connect(self.handle_requeue)\n    conn.on_auth.connect(self.handle_auth)\n    if self.max_concurrency:\n        conn.on_message.connect(self.queue_message)\n    else:\n        conn.on_message.connect(self.handle_message)\n    self.pending.add(conn)\n    try:\n        conn.connect()\n        conn.identify()\n        if (conn.max_ready_count < self.max_in_flight):\n            msg = ' '.join(['[%s] max RDY count %d < reader max in flight %d,', 'truncation possible'])\n            self.logger.warning((msg % (conn, conn.max_ready_count, self.max_in_flight)))\n        conn.subscribe(self.topic, self.channel)\n        self.send_ready(conn, 1)\n    except NSQException as error:\n        self.logger.warn(('[%s] connection failed (%r)' % (conn, error)))\n        self.handle_connection_failure(conn)\n        return\n    finally:\n        self.pending.remove(conn)\n    if (not self.is_running):\n        conn.close_stream()\n        return\n    self.logger.info(('[%s] connection successful' % conn))\n    self.handle_connection_success(conn)\n", "label": 1}
{"function": "\n\ndef find_by(self, finder, selector, original_find=None, original_query=None):\n    elements = None\n    end_time = (time.time() + self.wait_time)\n    func_name = getattr(getattr(finder, _meth_func), _func_name)\n    find_by = (original_find or func_name[(func_name.rfind('_by_') + 4):])\n    query = (original_query or selector)\n    while (time.time() < end_time):\n        try:\n            elements = finder(selector)\n            if (not isinstance(elements, list)):\n                elements = [elements]\n        except NoSuchElementException:\n            pass\n        if elements:\n            return ElementList([self.element_class(element, self) for element in elements], find_by=find_by, query=query)\n    return ElementList([], find_by=find_by, query=query)\n", "label": 1}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_version():\n        self.set_version(x.version())\n", "label": 0}
{"function": "\n\ndef __call__(self, driver):\n    try:\n        if isinstance(self.frame_locator, tuple):\n            driver.switch_to.frame(_find_element(driver, self.frame_locator))\n        else:\n            driver.switch_to.frame(self.frame_locator)\n        return True\n    except NoSuchFrameException:\n        return False\n", "label": 0}
{"function": "\n\ndef get_version():\n    INIT = os.path.abspath(os.path.join(HERE, '../psutil/__init__.py'))\n    with open(INIT, 'r') as f:\n        for line in f:\n            if line.startswith('__version__'):\n                ret = eval(line.strip().split(' = ')[1])\n                assert (ret.count('.') == 2), ret\n                for num in ret.split('.'):\n                    assert num.isdigit(), ret\n                return ret\n        else:\n            raise ValueError(\"couldn't find version string\")\n", "label": 0}
{"function": "\n\ndef test_search_products(self):\n    p = ProductFactory(title='Product One', slug='product')\n    doc = DocumentFactory(title='cookies', locale='en-US', category=10, products=[p])\n    RevisionFactory(document=doc, is_approved=True)\n    self.refresh()\n    response = self.client.get(reverse('search.advanced'), {\n        'a': '1',\n        'product': 'product',\n        'q': 'cookies',\n        'w': '1',\n    })\n    assert (\"We couldn't find any results for\" not in response.content)\n    eq_(200, response.status_code)\n    assert ('Product One' in response.content)\n", "label": 0}
{"function": "\n\ndef valid(self, order=None):\n    '\\n        Can do complex validation about whether or not this option is valid.\\n        For example, may check to see if the recipient is in an allowed country\\n        or location.\\n        '\n    if order:\n        try:\n            for item in order.orderitem_set.all():\n                p = item.product\n                price = self.carrier.price(p)\n        except ProductShippingPriceException:\n            return False\n    elif self.cart:\n        try:\n            price = self.cost()\n        except ProductShippingPriceException:\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef test_compute_centroid_quantile(self, empty_tdigest, example_centroids):\n    empty_tdigest.C = example_centroids\n    empty_tdigest.n = 4\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 1.1)]) == (((1 / 2.0) + 0) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 0.5)]) == (((1 / 2.0) + 1) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[0.1]) == (((1 / 2.0) + 2) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[1.5]) == (((1 / 2.0) + 3) / 4))\n", "label": 0}
{"function": "\n\ndef cycle(self):\n    '\\n        Perform full thermostat cycle and return state.\\n        '\n    units = cherrypy.config['units']\n    retry_count = cherrypy.config['retry_count']\n    retry_delay = cherrypy.config['retry_delay']\n    envcontroller = cherrypy.config['envcontroller']\n    thermometer = cherrypy.config['thermometer']\n    thermostat = cherrypy.config['thermostat']\n    (current_heat, current_cool) = envcontroller.get_power_levels()\n    for i in range(retry_count):\n        try:\n            current_temp = thermometer.get_temperature(units=units)\n            break\n        except braubuddy.thermometer.ReadError as err:\n            cherrypy.log.error(err.message)\n            time.sleep(retry_delay)\n    else:\n        cherrypy.request.app.log.error('Unable to collect temperature after {0} tries'.format(retry_count))\n        return False\n    (required_heat, required_cool) = thermostat.get_required_state(current_temp, current_heat, current_cool, units=units)\n    envcontroller.set_heater_level(required_heat)\n    envcontroller.set_cooler_level(required_cool)\n    target = thermostat.target\n    for (name, output) in cherrypy.request.app.config['outputs'].iteritems():\n        try:\n            output.publish_status(target, current_temp, current_heat, current_cool)\n        except braubuddy.output.OutputError as err:\n            cherrypy.log.error(err.message)\n    return True\n", "label": 0}
{"function": "\n\ndef _generate_image_id(self):\n    while True:\n        image_id = hashlib.sha256(str(random.getrandbits(128)).encode('utf8')).hexdigest()\n        try:\n            int(image_id[0:10])\n        except ValueError:\n            return image_id\n", "label": 0}
{"function": "\n\n@classmethod\ndef _is_socket(cls, stream):\n    'Check if the given stream is a socket.'\n    try:\n        fd = stream.fileno()\n    except ValueError:\n        return False\n    sock = socket.fromfd(fd, socket.AF_INET, socket.SOCK_RAW)\n    try:\n        sock.getsockopt(socket.SOL_SOCKET, socket.SO_TYPE)\n    except socket.error as ex:\n        if (ex.args[0] != errno.ENOTSOCK):\n            return True\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef test_profile_exception(self):\n\n    def f():\n        raise ValueError\n\n    def main():\n        try:\n            f()\n        except ValueError:\n            pass\n    with tracebin.record(profile=True) as recorder:\n        main()\n    [_, call, _] = recorder.calls\n    assert (call.func_name == 'main')\n    [subcall] = call.subcalls\n    assert (subcall.func_name == 'f')\n", "label": 0}
{"function": "\n\ndef test_should_override_by_env(self, settings_dict_to_override):\n\n    def mock_env_side_effect(k, d=None):\n        return ('simple from env' if (k == 'SIMPLE_STRING') else d)\n    with patch('os.environ.get', side_effect=mock_env_side_effect):\n        override_settings_by_env(settings_dict_to_override)\n    assert (settings_dict_to_override['SIMPLE_STRING'] == 'simple from env')\n    assert (settings_dict_to_override['SIMPLE_INTEGER'] == 1)\n", "label": 0}
{"function": "\n\ndef test_get_unique_constraints(self):\n    constraints = self.person_tools._get_unique_constraints()\n    assert (len(constraints) == 1)\n    assert (constraints[0] == 'person_name')\n", "label": 0}
{"function": "\n\ndef do(self, workflow_dict):\n    try:\n        mongodbkey = ''.join((random.choice(string.hexdigits) for i in range(50)))\n        workflow_dict['replicasetname'] = ('RepicaSet_' + workflow_dict['databaseinfra'].name)\n        statsd_credentials = get_credentials_for(environment=workflow_dict['environment'], credential_type=CredentialType.STATSD)\n        (statsd_host, statsd_port) = statsd_credentials.endpoint.split(':')\n        mongodb_password = get_credentials_for(environment=workflow_dict['environment'], credential_type=CredentialType.MONGODB).password\n        for (index, instance) in enumerate(workflow_dict['instances']):\n            host = instance.hostname\n            LOG.info('Getting vm credentials...')\n            host_csattr = CsHostAttr.objects.get(host=host)\n            LOG.info('Cheking host ssh...')\n            host_ready = check_ssh(server=host.address, username=host_csattr.vm_user, password=host_csattr.vm_password, wait=5, interval=10)\n            if (not host_ready):\n                LOG.warn(('Host %s is not ready...' % host))\n                return False\n            if instance.is_arbiter:\n                contextdict = {\n                    'HOST': workflow_dict['hosts'][index].hostname.split('.')[0],\n                    'DATABASENAME': workflow_dict['name'],\n                    'ENGINE': 'mongodb',\n                    'STATSD_HOST': statsd_host,\n                    'STATSD_PORT': statsd_port,\n                    'IS_HA': workflow_dict['databaseinfra'].plan.is_ha,\n                }\n                databaserule = 'ARBITER'\n            else:\n                host_nfsattr = HostAttr.objects.get(host=host)\n                contextdict = {\n                    'EXPORTPATH': host_nfsattr.nfsaas_path,\n                    'HOST': workflow_dict['hosts'][index].hostname.split('.')[0],\n                    'DATABASENAME': workflow_dict['name'],\n                    'ENGINE': 'mongodb',\n                    'DBPASSWORD': mongodb_password,\n                    'STATSD_HOST': statsd_host,\n                    'STATSD_PORT': statsd_port,\n                    'IS_HA': workflow_dict['databaseinfra'].plan.is_ha,\n                }\n                if (index == 0):\n                    databaserule = 'PRIMARY'\n                else:\n                    databaserule = 'SECONDARY'\n            if (len(workflow_dict['hosts']) > 1):\n                LOG.info(('Updating contexdict for %s' % host))\n                contextdict.update({\n                    'REPLICASETNAME': workflow_dict['replicasetname'],\n                    'HOST01': workflow_dict['hosts'][0],\n                    'HOST02': workflow_dict['hosts'][1],\n                    'HOST03': workflow_dict['hosts'][2],\n                    'MONGODBKEY': mongodbkey,\n                    'DATABASERULE': databaserule,\n                    'HOST': workflow_dict['hosts'][index].hostname.split('.')[0],\n                })\n            else:\n                contextdict.update({\n                    'DATABASERULE': databaserule,\n                })\n            planattr = PlanAttr.objects.get(plan=workflow_dict['plan'])\n            scripts = (planattr.initialization_script, planattr.configuration_script, planattr.start_database_script)\n            for script in scripts:\n                LOG.info(('Executing script on %s' % host))\n                script = build_context_script(contextdict, script)\n                return_code = exec_remote_command(server=host.address, username=host_csattr.vm_user, password=host_csattr.vm_password, command=script)\n                if (return_code != 0):\n                    return False\n        if (len(workflow_dict['hosts']) > 1):\n            scripts_to_run = planattr.start_replication_script\n            contextdict.update({\n                'DBPASSWORD': mongodb_password,\n                'DATABASERULE': 'PRIMARY',\n            })\n            scripts_to_run = build_context_script(contextdict, scripts_to_run)\n            host = workflow_dict['hosts'][0]\n            host_csattr = CsHostAttr.objects.get(host=host)\n            return_code = exec_remote_command(server=host.address, username=host_csattr.vm_user, password=host_csattr.vm_password, command=scripts_to_run)\n            if (return_code != 0):\n                return False\n        return True\n    except Exception:\n        traceback = full_stack()\n        workflow_dict['exceptions']['error_codes'].append(DBAAS_0014)\n        workflow_dict['exceptions']['traceback'].append(traceback)\n        return False\n", "label": 1}
{"function": "\n\ndef test_neg_list_trim_policy_is_string(self):\n    '\\n        Invoke list_trim() with policy is string\\n        '\n    key = ('test', 'demo', 1)\n    try:\n        self.as_connection.list_trim(key, 'contact_no', 0, 1, {\n            \n        }, '')\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'policy must be a dict')\n", "label": 0}
{"function": "\n\ndef _send_email(self, check):\n    check_name = check['check']\n    hostname = check['hostname']\n    if (check['status'] is True):\n        status = 'UP'\n    else:\n        status = 'DOWN'\n    subject = ('[stalker] %s on %s is %s' % (check_name, hostname, status))\n    message = ('From: %s\\n        To: %s\\n        Subject: %s\\n\\n        %s\\n        ' % (self.from_addr, self.recipients, subject, check))\n    try:\n        conn = smtplib.SMTP(self.smtp_host, self.smtp_port)\n        conn.ehlo()\n        conn.sendmail(self.from_addr, self.recipients, message)\n        conn.close()\n        self.logger.info(('Email sent for: %s' % check))\n        return True\n    except Exception:\n        self.logger.exception('Email notification error.')\n        return False\n", "label": 0}
{"function": "\n\ndef _do_login(backend, user, social_user):\n    user.backend = '{0}.{1}'.format(backend.__module__, backend.__class__.__name__)\n    login(backend.strategy.request, user)\n    if backend.setting('SESSION_EXPIRATION', False):\n        expiration = social_user.expiration_datetime()\n        if expiration:\n            try:\n                backend.strategy.request.session.set_expiry((expiration.seconds + (expiration.days * 86400)))\n            except OverflowError:\n                backend.strategy.request.session.set_expiry(None)\n", "label": 0}
{"function": "\n\ndef windows_shell(chan):\n    import threading\n    stdout.write('*** Emulating terminal on Windows; press F6 or Ctrl+Z then enter to send EOF,\\r\\nor at the end of the execution.\\r\\n')\n    stdout.flush()\n    out_lock = threading.RLock()\n\n    def write(recv, std):\n        while True:\n            data = recv(256)\n            if (not data):\n                if std:\n                    with out_lock:\n                        stdout.write('\\r\\n*** EOF reached; (press F6 or ^Z then enter to end)\\r\\n')\n                        stdout.flush()\n                break\n            stream = [stderr_bytes, stdout_bytes][std]\n            with out_lock:\n                stream.write(data)\n                stream.flush()\n    threading.Thread(target=write, args=(chan.recv, True)).start()\n    threading.Thread(target=write, args=(chan.recv_stderr, False)).start()\n    try:\n        while True:\n            d = stdin_bytes.read(1)\n            if (not d):\n                chan.shutdown_write()\n                break\n            try:\n                chan.send(d)\n            except socket.error:\n                break\n    except EOFError:\n        pass\n", "label": 0}
{"function": "\n\ndef test_exists(self):\n    block_val = predicates.existsPredicate(self.latlong1)\n    assert (block_val == ('1',))\n    block_val = predicates.existsPredicate((0, 0))\n    assert (block_val == ('0',))\n", "label": 0}
{"function": "\n\ndef get_current(self):\n    \"\\n        Returns the current ``Site`` based on the SITE_ID in the\\n        project's settings. The ``Site`` object is cached the first\\n        time it's retrieved from the database.\\n        \"\n    from django.conf import settings\n    try:\n        sid = settings.SITE_ID\n    except AttributeError:\n        from django.core.exceptions import ImproperlyConfigured\n        raise ImproperlyConfigured('You\\'re using the Django \"sites framework\" without having set the SITE_ID setting. Create a site in your database and set the SITE_ID setting to fix this error.')\n    try:\n        current_site = SITE_CACHE[sid]\n    except KeyError:\n        current_site = self.get(pk=sid)\n        SITE_CACHE[sid] = current_site\n    return current_site\n", "label": 0}
{"function": "\n\ndef test_set_serializer_default_config(tmpdir):\n    my_vcr = vcr.VCR(serializer='json')\n    with my_vcr.use_cassette(str(tmpdir.join('test.json'))):\n        assert (my_vcr.serializer == 'json')\n        urlopen('http://httpbin.org/get')\n    with open(str(tmpdir.join('test.json'))) as f:\n        assert json.loads(f.read())\n", "label": 0}
{"function": "\n\ndef _extract_sections(self):\n    '\\n        Here is an example of what a section header looks like in the\\n        html of a Google Document:\\n\\n        <h3 class=\"c1\"><a name=\"h.699ffpepx6zs\"></a><span>Hello World\\n        </span></h3>\\n\\n        We split the content of the Google Document up using a regular\\n        expression that matches the above header. re.split is a pretty\\n        cool function if you haven\\'t tried it before. It puts the\\n        matching groups into the list as well as the content between\\n        the matches. Check it out here:\\n\\n        http://docs.python.org/library/re.html#re.split\\n\\n        One big thing we do in this method is replace the ugly section\\n        id that Google creates with a nicely slugified version of the\\n        section title. This makes for pretty urls.\\n        '\n    self._sections = []\n    header = '<h(?P<level>\\\\d) class=\"[^\"]+\"><a name=\"(?P<id>[^\"]+)\"></a><span>(?P<title>[^<]+)</span></h\\\\d>'\n    l = re.split(header, self._content)\n    l.pop(0)\n    while l:\n        section = Section(level=(int(l.pop(0)) - 2), id=l.pop(0), title=l.pop(0).decode('utf8'), content=l.pop(0))\n        section['id'] = slugify(section['title'])\n        if (section['level'] >= 1):\n            self._sections.append(section)\n", "label": 0}
{"function": "\n\ndef startTag(self, namespace, name, attrs):\n    assert ((namespace is None) or isinstance(namespace, string_types)), type(namespace)\n    assert isinstance(name, string_types), type(name)\n    assert all(((((namespace is None) or isinstance(namespace, string_types)) and isinstance(name, string_types) and isinstance(value, string_types)) for ((namespace, name), value) in attrs.items()))\n    return {\n        'type': 'StartTag',\n        'name': text_type(name),\n        'namespace': to_text(namespace),\n        'data': dict((((to_text(namespace, False), to_text(name)), to_text(value, False)) for ((namespace, name), value) in attrs.items())),\n    }\n", "label": 0}
{"function": "\n\n@shared_task()\ndef unpublish_object(content_type_pk, obj_pk):\n    \"\\n    Unbuild all views related to a object and then sync to S3.\\n\\n    Accepts primary keys to retrieve a model object that\\n    inherits bakery's BuildableModel class.\\n    \"\n    ct = ContentType.objects.get_for_id(content_type_pk)\n    obj = ct.get_object_for_this_type(pk=obj_pk)\n    try:\n        logger.info(('unpublish_object task has received %s' % obj))\n        obj.unbuild()\n        if (not getattr(settings, 'ALLOW_BAKERY_AUTO_PUBLISHING', True)):\n            logger.info('Not running publish command because ALLOW_BAKERY_AUTO_PUBLISHING is False')\n        else:\n            management.call_command('publish')\n    except Exception:\n        logger.error('Task Error: unpublish_object', exc_info=True)\n", "label": 0}
{"function": "\n\ndef stderr_to_parser_error(parse_args, *args, **kwargs):\n    if (isinstance(sys.stderr, StdIOBuffer) or isinstance(sys.stdout, StdIOBuffer)):\n        return parse_args(*args, **kwargs)\n    old_stdout = sys.stdout\n    old_stderr = sys.stderr\n    sys.stdout = StdIOBuffer()\n    sys.stderr = StdIOBuffer()\n    try:\n        try:\n            result = parse_args(*args, **kwargs)\n            for key in list(vars(result)):\n                if (getattr(result, key) is sys.stdout):\n                    setattr(result, key, old_stdout)\n                if (getattr(result, key) is sys.stderr):\n                    setattr(result, key, old_stderr)\n            return result\n        except SystemExit:\n            code = sys.exc_info()[1].code\n            stdout = sys.stdout.getvalue()\n            stderr = sys.stderr.getvalue()\n            raise ArgumentParserError('SystemExit', stdout, stderr, code)\n    finally:\n        sys.stdout = old_stdout\n        sys.stderr = old_stderr\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    try:\n        message = record.getMessage()\n        assert isinstance(message, basestring_type)\n        record.message = _safe_unicode(message)\n    except Exception as e:\n        record.message = ('Bad message (%r): %r' % (e, record.__dict__))\n    record.asctime = self.formatTime(record, self.datefmt)\n    if (record.levelno in self._colors):\n        record.color = self._colors[record.levelno]\n        record.end_color = self._normal\n    else:\n        record.color = record.end_color = ''\n    formatted = (self._fmt % record.__dict__)\n    if record.exc_info:\n        if (not record.exc_text):\n            record.exc_text = self.formatException(record.exc_info)\n    if record.exc_text:\n        lines = [formatted.rstrip()]\n        lines.extend((_safe_unicode(ln) for ln in record.exc_text.split('\\n')))\n        formatted = '\\n'.join(lines)\n    return formatted.replace('\\n', '\\n    ')\n", "label": 1}
{"function": "\n\ndef it_adds_func_names_to_all(self):\n    base = 'uber'\n    namespace = {\n        '__all__': [],\n    }\n    generate_generic_calls(base, namespace)\n    base_funcs = (m.split('.', 1)[1] for m in METHODS if m.startswith(base))\n    assert (sorted(namespace['__all__']) == list(sorted(base_funcs)))\n", "label": 0}
{"function": "\n\ndef test_logsoftmax_grad_3():\n    npr.seed(5)\n    for ii in xrange(NUM_TRIALS):\n        np_X = npr.randn(5, 6)\n        np_T = npr.randint(0, 10, np_X.shape)\n        X = kayak.Parameter(np_X)\n        T = kayak.Targets(np_T)\n        Y = kayak.LogSoftMax(X)\n        Z = kayak.MatSum(kayak.LogMultinomialLoss(Y, T))\n        assert (kayak.util.checkgrad(X, Z) < MAX_GRAD_DIFF)\n", "label": 0}
{"function": "\n\ndef do43x(self, msg, problem):\n    if (not self.afterConnect):\n        newNick = self._getNextNick()\n        assert (newNick != self.nick)\n        log.info('Got %s: %s %s.  Trying %s.', msg.command, self.nick, problem, newNick)\n        self.sendMsg(ircmsgs.nick(newNick))\n", "label": 0}
{"function": "\n\ndef handle_noargs(self, *args, **options):\n    r = get_r()\n    try:\n        keys = r.r.smembers(r._metric_slugs_key)\n        slugs = set((s.split(':')[1] for s in keys))\n        r.r.srem(r._metric_slugs_key, *keys)\n        r.r.sadd(r._metric_slugs_key, *slugs)\n        p = '\\nMetrics: Converted {0} Keys to {1} Slugs\\n'\n        self.stdout.write(p.format(len(keys), len(slugs)))\n    except (IndexError, ResponseError):\n        pass\n    try:\n        keys = r.r.smembers(r._gauge_slugs_key)\n        slugs = set((s.split(':')[1] for s in keys))\n        r.r.srem(r._gauge_slugs_key, *keys)\n        r.r.sadd(r._gauge_slugs_key, *slugs)\n        p = 'Gauges: Converted {0} Keys to {1} Slugs\\n'\n        self.stdout.write(p.format(len(keys), len(slugs)))\n    except (IndexError, ResponseError):\n        pass\n    i = 0\n    categories = r.categories()\n    for category in categories:\n        try:\n            k = r._category_key(category)\n            data = r.r.get(k)\n            if data:\n                data = json.loads(data)\n                r.r.delete(k)\n                r.r.sadd(k, *set(data))\n                i += 1\n        except ResponseError:\n            pass\n    if (i > 0):\n        p = 'Converted {0} Categories from JSON -> Redis Sets\\n'\n        self.stdout.write(p.format(i))\n", "label": 1}
{"function": "\n\ndef __init__(self, env, name=None, version=None):\n    'Create a new page object or retrieves an existing page.\\n\\n        :param env: an `Environment` object.\\n        :param name: the page name or a `Resource` object.\\n        :param version: the page version. The value takes precedence over the\\n                        `Resource` version when both are specified.\\n        '\n    self.env = env\n    if version:\n        try:\n            version = int(version)\n        except ValueError:\n            version = None\n    if isinstance(name, Resource):\n        resource = name\n        name = resource.id\n        if ((version is None) and (resource.version is not None)):\n            try:\n                version = int(resource.version)\n            except ValueError:\n                version = None\n    self.name = name\n    self._resource_version = version\n    if name:\n        self._fetch(name, version)\n    else:\n        self.version = 0\n        self.text = self.comment = self.author = ''\n        self.time = None\n        self.readonly = 0\n    self.old_text = self.text\n    self.old_readonly = self.readonly\n", "label": 1}
{"function": "\n\ndef __call__(self, value):\n    if (value is None):\n        return None\n    try:\n        p = value.split(':', 2)\n        _60 = Duration._60\n        _unsigned = Duration._unsigned\n        if (len(p) == 1):\n            result = _unsigned(p[0])\n        if (len(p) == 2):\n            result = ((60 * _unsigned(p[0])) + _60(p[1]))\n        if (len(p) == 3):\n            result = (((3600 * _unsigned(p[0])) + (60 * _60(p[1]))) + _60(p[2]))\n    except ValueError:\n        raise ValueError('Invalid duration value: %s', value)\n    return result\n", "label": 0}
{"function": "\n\n@patch('pypdfocr.pypdfocr_tesseract.subprocess.call')\n@patch('pypdfocr.pypdfocr_tesseract.PyTesseract._is_version_uptodate')\n@patch('pypdfocr.pypdfocr_tesseract.os.name')\n@patch('pypdfocr.pypdfocr_tesseract.os.path.exists')\ndef test_tesseract_fail(self, mock_os_path_exists, mock_os_name, mock_uptodate, mock_subprocess_call, capsys):\n    '\\n            Get all the checks past and make sure we report the case where tesseract returns a non-zero status\\n        '\n    mock_os_name.__str__.return_value = 'nt'\n    p = P.PyTesseract({\n        \n    })\n    assert ('tesseract.exe' in p.binary)\n    mock_os_path_exists.return_value = True\n    mock_uptodate.return_value = (True, '')\n    mock_subprocess_call.return_value = (- 1)\n    with pytest.raises(SystemExit):\n        p.make_hocr_from_pnm('blah.tiff')\n    (out, err) = capsys.readouterr()\n    assert (p.msgs['TS_FAILED'] in out)\n", "label": 0}
{"function": "\n\ndef _safe_eval(self, code, event):\n    '\\n        Try to evaluate the given code on the given frame. If failure occurs, returns some ugly string with exception.\\n        '\n    try:\n        return eval(code, (event.globals if self.globals else {\n            \n        }), event.locals)\n    except Exception as exc:\n        return '{internal-failure}FAILED EVAL: {internal-detail}{!r}'.format(exc, **self.event_colors)\n", "label": 0}
{"function": "\n\ndef test_render_pdf(client, test_handwriting):\n    image = client.render_pdf({\n        'handwriting_id': test_handwriting['id'],\n        'text': 'Hello world!',\n    })\n    assert image\n    assert isinstance(image, bytes)\n", "label": 0}
{"function": "\n\ndef terminal(self, ret):\n    if (ret is not None):\n        assert os.path.isabs(ret), ret\n        assert os.path.exists(ret), ret\n", "label": 0}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_content():\n        self.set_content(x.content())\n    if x.has_statuscode():\n        self.set_statuscode(x.statuscode())\n    for i in xrange(x.header_size()):\n        self.add_header().CopyFrom(x.header(i))\n    if x.has_contentwastruncated():\n        self.set_contentwastruncated(x.contentwastruncated())\n    if x.has_externalbytessent():\n        self.set_externalbytessent(x.externalbytessent())\n    if x.has_externalbytesreceived():\n        self.set_externalbytesreceived(x.externalbytesreceived())\n    if x.has_finalurl():\n        self.set_finalurl(x.finalurl())\n", "label": 1}
{"function": "\n\ndef get(self, key_path, default=None):\n    try:\n        value = self._data\n        for k in key_path.split('.'):\n            value = value[k]\n        return value\n    except KeyError:\n        if (default is not None):\n            return default\n        else:\n            raise ConfigKeyError(key_path)\n", "label": 0}
{"function": "\n\n@editor_open(__file__)\ndef test_extended_selection(editor):\n    QTest.qWait(1000)\n    mode = get_mode(editor)\n    TextHelper(editor).goto_line(0, 10)\n    QTest.qWait(1000)\n    try:\n        event = QtGui.QMouseEvent(QtCore.QEvent.MouseButtonDblClick, QtCore.QPointF(0, 0), QtCore.Qt.LeftButton, QtCore.Qt.LeftButton, QtCore.Qt.ControlModifier)\n    except TypeError:\n        event = QtGui.QMouseEvent(QtCore.QEvent.MouseButtonDblClick, QtCore.QPoint(0, 0), QtCore.Qt.LeftButton, QtCore.Qt.LeftButton, QtCore.Qt.ControlModifier)\n    mode._on_double_click(event)\n    assert (editor.textCursor().selectedText() == 'pyqode.qt')\n    QTest.qWait(1000)\n", "label": 0}
{"function": "\n\ndef test_field_checks(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.CharField(), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E004')\n    assert ('Base field for list has errors' in errors[0].msg)\n    assert ('max_length' in errors[0].msg)\n", "label": 0}
{"function": "\n\ndef test_initialises_to_none(self):\n    r = result.SerfResult()\n    assert (r.head is None)\n    assert (r.body is None)\n", "label": 0}
{"function": "\n\ndef _safe_int(r, default=0):\n    if (r is None):\n        return default\n    try:\n        return int(r)\n    except ValueError:\n        return default\n", "label": 0}
{"function": "\n\ndef createVM(request_id, hostname, recipe, updateProgress):\n    configParser = configparser.RawConfigParser()\n    configFilePath = '/opt/chef-tools/createvm/createvm.config'\n    configParser.read(configFilePath)\n    subdomain = configParser.get('cocreate_config', 'subdomain')\n    progress = 0\n    validHostname = checkHostname(hostname)\n    if (validHostname == False):\n        return (None, None, 'Invalid hostname', progress)\n    fqdn = ((hostname + '.') + subdomain)\n    validRecipe = checkRecipe(configParser, recipe)\n    if (validRecipe == False):\n        return (None, None, 'Unsupported template', progress)\n    updateProgress(request_id, progress, 'Beginning VM template cloning')\n    try:\n        cloneVM(configParser, hostname)\n    except subprocess.CalledProcessError:\n        print('A cloning error occurred')\n        return (None, None, 'VM cloning failed', progress)\n    progress = 33\n    updateProgress(request_id, progress, 'Waiting for new VM IP address')\n    ipAddress = None\n    try:\n        ipAddress = getIP(configParser, hostname)\n    except:\n        print('Could not get IP Address')\n        return (None, None, 'Could not obtain VM IP address', progress)\n    progress = 67\n    updateProgress(request_id, progress, 'Beginning VM bootstrap')\n    try:\n        bootstrapVM(configParser, ipAddress, hostname, recipe)\n    except subprocess.CalledProcessError:\n        print('An error occurred during bootstrap')\n        return (None, None, 'VM bootstrap failed', progress)\n    url = ((('http://' + fqdn) + '/') + recipe)\n    progress = 100\n    updateProgress(request_id, progress, 'VM creation complete', url)\n    return (ipAddress, fqdn, None, progress)\n", "label": 0}
{"function": "\n\ndef opt_rulers_parser(value):\n    try:\n        converted = json.loads(value)\n        if isinstance(converted, list):\n            return converted\n        else:\n            raise ValueError\n    except ValueError:\n        raise\n    except TypeError:\n        raise ValueError\n", "label": 0}
{"function": "\n\ndef terminate(self):\n    ' Method should be called to terminate the client before the reactor\\n            is stopped.\\n\\n            @return:            Deferred which fires as soon as the client is\\n                                ready to stop the reactor.\\n            @rtype:             twisted.internet.defer.Deferred\\n        '\n    for call in self._deathCandidates.itervalues():\n        call.cancel()\n    self._deathCandidates = {\n        \n    }\n    for connection in self._connections.copy():\n        connection.destroy()\n    assert (len(self._connections) == 0)\n    Endpoint.terminate(self)\n", "label": 0}
{"function": "\n\ndef main():\n    import sys\n    import json\n    import os\n    import boto3.session\n    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')\n    if (not os.path.isdir(data_dir)):\n        os.makedirs(data_dir)\n    session = boto3.session.Session()\n    loader = session._loader\n    builder = ResourceIndexBuilder()\n    for resource_name in session.get_available_resources():\n        api_version = loader.determine_latest_version(resource_name, 'resources-1')\n        model = loader.load_service_model(resource_name, 'resources-1', api_version)\n        index = builder.build_index(model)\n        output_file = os.path.join(data_dir, resource_name, api_version, 'completions-1.json')\n        if (not os.path.isdir(os.path.dirname(output_file))):\n            os.makedirs(os.path.dirname(output_file))\n        with open(output_file, 'w') as f:\n            f.write(json.dumps(index, indent=2))\n", "label": 0}
{"function": "\n\ndef test_getset_metadata(self):\n    m = meta.Metadata()\n    md = m.get_metadata('files/one')\n    assert (md == 'r--------')\n    d = self.tmpdir()\n    p = os.path.join(d, 'test')\n    utils.touch(p)\n    assert (m.get_metadata(p) != md)\n    m.set_metadata(p, md)\n    assert (m.get_metadata(p) == md)\n", "label": 0}
{"function": "\n\ndef wait_till_stopped(self, conf, container_id, timeout=10, message=None, waiting=True):\n    'Wait till a container is stopped'\n    stopped = False\n    inspection = None\n    for _ in until(timeout=timeout, action=message):\n        try:\n            inspection = conf.harpoon.docker_context.inspect_container(container_id)\n            if (not isinstance(inspection, dict)):\n                log.error('Weird response from inspecting the container\\tresponse=%s', inspection)\n            elif (not inspection['State']['Running']):\n                stopped = True\n                conf.container_id = None\n                break\n            else:\n                break\n        except (socket.timeout, ValueError):\n            log.warning('Failed to inspect the container\\tcontainer_id=%s', container_id)\n        except DockerAPIError as error:\n            if (error.response.status_code != 404):\n                raise\n            else:\n                break\n    if (not inspection):\n        log.warning('Failed to inspect the container!')\n        stopped = True\n        exit_code = 1\n    else:\n        exit_code = inspection['State']['ExitCode']\n    return (stopped, exit_code)\n", "label": 1}
{"function": "\n\ndef __init__(self, *args):\n    if (len(args) == 1):\n        (start, stop, step) = (0, args[0], 1)\n    elif (len(args) == 2):\n        (start, stop, step) = (args[0], args[1], 1)\n    elif (len(args) == 3):\n        (start, stop, step) = args\n    else:\n        raise TypeError('range() requires 1-3 int arguments')\n    try:\n        (start, stop, step) = (int(start), int(stop), int(step))\n    except ValueError:\n        raise TypeError('an integer is required')\n    if (step == 0):\n        raise ValueError('range() arg 3 must not be zero')\n    elif (step < 0):\n        stop = min(stop, start)\n    else:\n        stop = max(stop, start)\n    self._start = start\n    self._stop = stop\n    self._step = step\n    self._len = (((stop - start) // step) + bool(((stop - start) % step)))\n", "label": 0}
{"function": "\n\ndef doCapNew(self, msg):\n    if (len(msg.args) != 3):\n        log.warning('Bad CAP NEW from server: %r', msg)\n        return\n    caps = msg.args[2].split()\n    assert caps, 'Empty list of capabilities'\n    self._addCapabilities(msg.args[2])\n    if ((not self.sasl_authenticated) and ('sasl' in self.state.capabilities_ls)):\n        self.resetSasl()\n        s = self.state.capabilities_ls['sasl']\n        if (s is not None):\n            self.filterSaslMechanisms(set(s.split(',')))\n    common_supported_unrequested_capabilities = (set(self.state.capabilities_ls) & (self.REQUEST_CAPABILITIES - self.state.capabilities_ack))\n    if common_supported_unrequested_capabilities:\n        caps = ' '.join(sorted(common_supported_unrequested_capabilities))\n        self.sendMsg(ircmsgs.IrcMsg(command='CAP', args=('REQ', caps)))\n", "label": 0}
{"function": "\n\n@classmethod\ndef detect_worktree(cls, binary='git', subdir=None):\n    \"Detect the git working tree above cwd and return it; else, return None.\\n\\n    :param string binary: The path to the git binary to use, 'git' by default.\\n    :param string subdir: The path to start searching for a git repo.\\n    :returns: path to the directory where the git working tree is rooted.\\n    :rtype: string\\n    \"\n    cmd = [binary, 'rev-parse', '--show-toplevel']\n    try:\n        if subdir:\n            with pushd(subdir):\n                (process, out) = cls._invoke(cmd)\n        else:\n            (process, out) = cls._invoke(cmd)\n        cls._check_result(cmd, process.returncode, raise_type=Scm.ScmException)\n    except Scm.ScmException:\n        return None\n    return cls._cleanse(out)\n", "label": 0}
{"function": "\n\ndef value(self, key, value=1):\n    'Set value of a counter by counter key'\n    if isinstance(key, six.string_types):\n        key = (key,)\n    assert isinstance(key, tuple), 'event key type error'\n    if (key not in self.counters):\n        self.counters[key] = self.cls()\n    self.counters[key].value(value)\n    return self\n", "label": 0}
{"function": "\n\n@transaction.atomic\ndef post(self, request, *args, **kwargs):\n    referer_url = request.GET.get('referer', '')\n    nextpage = request.POST.get('next')\n    form = DomainRegistrationForm(request.POST)\n    context = self.get_context_data(form=form)\n    if form.is_valid():\n        reqs_today = RegistrationRequest.get_requests_today()\n        max_req = settings.DOMAIN_MAX_REGISTRATION_REQUESTS_PER_DAY\n        if (reqs_today >= max_req):\n            context.update({\n                'current_page': {\n                    'page_name': _('Oops!'),\n                },\n                'error_msg': (_('Number of domains requested today exceeds limit (%d) - contact Dimagi') % max_req),\n                'show_homepage_link': 1,\n            })\n            return render(request, 'error.html', context)\n        try:\n            domain_name = request_new_domain(request, form, is_new_user=self.is_new_user)\n        except NameUnavailableException:\n            context.update({\n                'current_page': {\n                    'page_name': _('Oops!'),\n                },\n                'error_msg': _('Project name already taken - please try another'),\n                'show_homepage_link': 1,\n            })\n            return render(request, 'error.html', context)\n        if self.is_new_user:\n            context.update({\n                'requested_domain': domain_name,\n                'track_domain_registration': True,\n                'current_page': {\n                    'page_name': _('Confirm Account'),\n                },\n            })\n            return render(request, 'registration/confirmation_sent.html', context)\n        else:\n            if nextpage:\n                return HttpResponseRedirect(nextpage)\n            if referer_url:\n                return redirect(referer_url)\n            return HttpResponseRedirect(reverse('domain_homepage', args=[domain_name]))\n    return self.render_to_response(context)\n", "label": 0}
{"function": "\n\ndef slurp(self):\n    '\\n        :returns: item_list - list of Redshift Policies.\\n        :returns: exception_map - A dict where the keys are a tuple containing the\\n            location of the exception and the value is the actual exception\\n\\n        '\n    self.prep_for_slurp()\n    from security_monkey.common.sts_connect import connect\n    item_list = []\n    exception_map = {\n        \n    }\n    for account in self.accounts:\n        for region in regions():\n            app.logger.debug('Checking {}/{}/{}'.format(self.index, account, region.name))\n            try:\n                redshift = connect(account, 'redshift', region=region)\n                all_clusters = []\n                marker = None\n                while True:\n                    response = self.wrap_aws_rate_limited_call(redshift.describe_clusters, marker=marker)\n                    all_clusters.extend(response['DescribeClustersResponse']['DescribeClustersResult']['Clusters'])\n                    if (response['DescribeClustersResponse']['DescribeClustersResult']['Marker'] is not None):\n                        marker = response['DescribeClustersResponse']['DescribeClustersResult']['Marker']\n                    else:\n                        break\n            except Exception as e:\n                if (region.name not in TROUBLE_REGIONS):\n                    exc = BotoConnectionIssue(str(e), 'redshift', account, region.name)\n                    self.slurp_exception((self.index, account, region.name), exc, exception_map)\n                continue\n            app.logger.debug('Found {} {}'.format(len(all_clusters), Redshift.i_am_plural))\n            for cluster in all_clusters:\n                cluster_id = cluster['ClusterIdentifier']\n                if self.check_ignore_list(cluster_id):\n                    continue\n                item = RedshiftCluster(region=region.name, account=account, name=cluster_id, config=dict(cluster))\n                item_list.append(item)\n    return (item_list, exception_map)\n", "label": 1}
{"function": "\n\ndef generate_models(self, skip_invalid=False, table_names=None):\n    database = self.introspect(table_names=table_names)\n    models = {\n        \n    }\n\n    class BaseModel(Model):\n\n        class Meta():\n            database = self.metadata.database\n\n    def _create_model(table, models):\n        for foreign_key in database.foreign_keys[table]:\n            dest = foreign_key.dest_table\n            if ((dest not in models) and (dest != table)):\n                _create_model(dest, models)\n        primary_keys = []\n        columns = database.columns[table]\n        for (db_column, column) in columns.items():\n            if column.primary_key:\n                primary_keys.append(column.name)\n        multi_column_indexes = database.multi_column_indexes(table)\n        column_indexes = database.column_indexes(table)\n\n        class Meta():\n            indexes = multi_column_indexes\n        composite_key = False\n        if (len(primary_keys) == 0):\n            primary_keys = columns.keys()\n        if (len(primary_keys) > 1):\n            Meta.primary_key = CompositeKey(*[field.name for (col, field) in columns.items() if (col in primary_keys)])\n            composite_key = True\n        attrs = {\n            'Meta': Meta,\n        }\n        for (db_column, column) in columns.items():\n            FieldClass = column.field_class\n            if (FieldClass is UnknownField):\n                FieldClass = BareField\n            params = {\n                'db_column': db_column,\n                'null': column.nullable,\n            }\n            if (column.primary_key and composite_key):\n                if (FieldClass is PrimaryKeyField):\n                    FieldClass = IntegerField\n                params['primary_key'] = False\n            elif (column.primary_key and (FieldClass is not PrimaryKeyField)):\n                params['primary_key'] = True\n            if column.is_foreign_key():\n                if column.is_self_referential_fk():\n                    params['rel_model'] = 'self'\n                else:\n                    dest_table = column.foreign_key.dest_table\n                    params['rel_model'] = models[dest_table]\n                if column.to_field:\n                    params['to_field'] = column.to_field\n                params['related_name'] = ('%s_%s_rel' % (table, db_column))\n            if ((db_column in column_indexes) and (not column.is_primary_key())):\n                if column_indexes[db_column]:\n                    params['unique'] = True\n                elif (not column.is_foreign_key()):\n                    params['index'] = True\n            attrs[column.name] = FieldClass(**params)\n        try:\n            models[table] = type(str(table), (BaseModel,), attrs)\n        except ValueError:\n            if (not skip_invalid):\n                raise\n    for (table, model) in sorted(database.model_names.items()):\n        if (table not in models):\n            _create_model(table, models)\n    return models\n", "label": 0}
{"function": "\n\ndef unquote(string):\n    if (not string):\n        return b''\n    res = string.split(b'%')\n    if (len(res) != 1):\n        string = res[0]\n        for item in res[1:]:\n            try:\n                string += (bytes([int(item[:2], 16)]) + item[2:])\n            except ValueError:\n                string += (b'%' + item)\n    return string\n", "label": 0}
{"function": "\n\ndef create(self, req, body):\n    'Create or import keypair.\\n\\n        Sending name will generate a key and return private_key\\n        and fingerprint.\\n\\n        You can send a public_key to add an existing ssh key\\n\\n        params: keypair object with:\\n            name (required) - string\\n            public_key (optional) - string\\n        '\n    context = req.environ['nova.context']\n    authorize(context, action='create')\n    try:\n        params = body['keypair']\n        name = params['name']\n    except KeyError:\n        msg = _('Invalid request body')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        if ('public_key' in params):\n            keypair = self.api.import_key_pair(context, context.user_id, name, params['public_key'])\n            keypair = self._filter_keypair(keypair, user_id=True)\n        else:\n            (keypair, private_key) = self.api.create_key_pair(context, context.user_id, name)\n            keypair = self._filter_keypair(keypair, user_id=True)\n            keypair['private_key'] = private_key\n        return {\n            'keypair': keypair,\n        }\n    except exception.KeypairLimitExceeded:\n        msg = _('Quota exceeded, too many key pairs.')\n        raise webob.exc.HTTPForbidden(explanation=msg)\n    except exception.InvalidKeypair as exc:\n        raise webob.exc.HTTPBadRequest(explanation=exc.format_message())\n    except exception.KeyPairExists as exc:\n        raise webob.exc.HTTPConflict(explanation=exc.format_message())\n", "label": 0}
{"function": "\n\ndef convertPyClassOrFunctionDefinitionToForaFunctionExpression(self, classOrFunctionDefinition, objectIdToObjectDefinition):\n    pyAst = self.convertClassOrFunctionDefinitionToNativePyAst(classOrFunctionDefinition, objectIdToObjectDefinition)\n    assert (pyAst is not None)\n    sourcePath = objectIdToObjectDefinition[classOrFunctionDefinition.sourceFileId].path\n    tr = None\n    if isinstance(classOrFunctionDefinition, TypeDescription.FunctionDefinition):\n        if (isinstance(pyAst, ForaNative.PythonAstStatement) and pyAst.isFunctionDef()):\n            tr = self.nativeConverter.convertPythonAstFunctionDefToForaOrParseError(pyAst.asFunctionDef, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]))\n        else:\n            assert pyAst.isLambda()\n            tr = self.nativeConverter.convertPythonAstLambdaToForaOrParseError(pyAst.asLambda, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]))\n    elif isinstance(classOrFunctionDefinition, TypeDescription.ClassDefinition):\n        objectIdToFreeVar = {v: k for (k, v) in classOrFunctionDefinition.freeVariableMemberAccessChainsToId.iteritems()}\n        baseClasses = [objectIdToFreeVar[baseId].split('.') for baseId in classOrFunctionDefinition.baseClassIds]\n        tr = self.nativeConverter.convertPythonAstClassDefToForaOrParseError(pyAst.asClassDef, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]), baseClasses)\n    else:\n        assert False\n    if isinstance(tr, ForaNative.PythonToForaConversionError):\n        raise convertNativePythonToForaConversionError(tr, sourcePath)\n    return tr\n", "label": 1}
{"function": "\n\ndef get_users(self):\n    node = utils.get_repo_node(self.env, self.gitolite_admin_reponame, 'keydir')\n    assert node.isdir, ('Node %s at /keydir/ is not a directory' % node)\n    for child in node.get_entries():\n        name = child.get_name()\n        assert name.endswith('.pub'), ('Node %s' % name)\n        name = name[:(- 4)]\n        (yield name)\n", "label": 0}
{"function": "\n\ndef get_profile(self):\n    '\\n        Returns site-specific profile for this user. Raises\\n        SiteProfileNotAvailable if this site does not allow profiles.\\n        '\n    warnings.warn('The use of AUTH_PROFILE_MODULE to define user profiles has been deprecated.', PendingDeprecationWarning)\n    if (not hasattr(self, '_profile_cache')):\n        from django.conf import settings\n        if (not getattr(settings, 'AUTH_PROFILE_MODULE', False)):\n            raise SiteProfileNotAvailable('You need to set AUTH_PROFILE_MODULE in your project settings')\n        try:\n            (app_label, model_name) = settings.AUTH_PROFILE_MODULE.split('.')\n        except ValueError:\n            raise SiteProfileNotAvailable('app_label and model_name should be separated by a dot in the AUTH_PROFILE_MODULE setting')\n        try:\n            model = models.get_model(app_label, model_name)\n            if (model is None):\n                raise SiteProfileNotAvailable('Unable to load the profile model, check AUTH_PROFILE_MODULE in your project settings')\n            self._profile_cache = model._default_manager.using(self._state.db).get(user__id__exact=self.id)\n            self._profile_cache.user = self\n        except (ImportError, ImproperlyConfigured):\n            raise SiteProfileNotAvailable\n    return self._profile_cache\n", "label": 0}
{"function": "\n\ndef run_is_false(self, action):\n    try:\n        value = self._lookup(action)\n    except AssertionError:\n        pass\n    else:\n        self.assertIn(value, ('', None, False, 0))\n", "label": 0}
{"function": "\n\ndef test_http_default_port():\n    with InstalledApp(wsgi_app.simple_app, host=HOST, port=80) as app:\n        http = httplib2.Http()\n        (resp, content) = http.request('http://some_hopefully_nonexistant_domain/')\n        assert (content == b'WSGI intercept successful!\\n')\n        assert app.success()\n", "label": 0}
{"function": "\n\ndef test_equality():\n    (a, b, c) = (Identity(3), eye(3), ImmutableMatrix(eye(3)))\n    for x in [a, b, c]:\n        for y in [a, b, c]:\n            assert x.equals(y)\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **kwargs):\n    valid_domains = ['@stud.ntnu.no', '@ntnu.no']\n    for campain_id in args:\n        try:\n            campain = Campaign.objects.filter(pk=int(campain_id))\n        except Campaign.DoesNotExist:\n            raise CommandError(('Campaign ID %s does not exits' % campain_id))\n        signatures_qs = Signature.objects.filter(campaign__pk=int(campain_id))\n        for domain in valid_domains:\n            signatures_qs = signatures_qs.exclude(email__endswith=domain)\n        for signature in signatures_qs:\n            signature.delete()\n", "label": 0}
{"function": "\n\ndef get(self, build_id):\n    build = Build.query.options(joinedload('project', innerjoin=True), joinedload('author'), joinedload('source').joinedload('revision'), subqueryload_all('stats')).get(build_id)\n    if (build is None):\n        return ('', 404)\n    try:\n        most_recent_run = Build.query.filter((Build.project == build.project), (Build.date_created < build.date_created), (Build.status == Status.finished), (Build.id != build.id)).join(Source, (Build.source_id == Source.id)).filter(*build_type.get_any_commit_build_filters()).options(contains_eager('source').joinedload('revision'), joinedload('author')).order_by(Build.date_created.desc())[0]\n    except IndexError:\n        most_recent_run = None\n    jobs = list(Job.query.filter((Job.build_id == build.id)))\n    if (not jobs):\n        test_failures = []\n        num_test_failures = 0\n    else:\n        test_failures = TestCase.query.options(joinedload('job', innerjoin=True)).filter(TestCase.job_id.in_([j.id for j in jobs]), (TestCase.result == Result.failed)).order_by(TestCase.name.asc())\n        num_test_failures = test_failures.count()\n        test_failures = test_failures[:25]\n    failures_by_job = defaultdict(list)\n    for failure in test_failures:\n        failures_by_job[failure.job].append(failure)\n    failure_origins = find_failure_origins(build, test_failures)\n    for test_failure in test_failures:\n        test_failure.origin = failure_origins.get(test_failure)\n    if (most_recent_run and (build.status == Status.finished)):\n        changed_tests = find_changed_tests(build, most_recent_run)\n    else:\n        changed_tests = []\n    seen_by = list(User.query.join(BuildSeen, (BuildSeen.user_id == User.id)).filter((BuildSeen.build_id == build.id)))\n    extended_serializers = {\n        TestCase: TestCaseWithOriginCrumbler(),\n    }\n    event_list = list(Event.query.filter((Event.item_id == build.id)).order_by(Event.date_created.desc()))\n    context = self.serialize(build)\n    context.update({\n        'jobs': jobs,\n        'seenBy': seen_by,\n        'events': event_list,\n        'failures': get_failure_reasons(build),\n        'testFailures': {\n            'total': num_test_failures,\n            'tests': self.serialize(test_failures, extended_serializers),\n        },\n        'testChanges': self.serialize(changed_tests, extended_serializers),\n        'parents': self.serialize(get_parents_last_builds(build)),\n    })\n    return self.respond(context)\n", "label": 1}
{"function": "\n\ndef __call__(self, item, context=None):\n    array_value = self._array_expression(item, context)\n    if (not isinstance(array_value, list)):\n        return None\n    index_value = self._index_expression(item, context)\n    if (not isinstance(index_value, int)):\n        return None\n    try:\n        return array_value[index_value]\n    except IndexError:\n        return None\n", "label": 0}
{"function": "\n\ndef _parse_settings_bond_6(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond6.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '6',\n    }\n    for binding in ['miimon', 'downdelay', 'updelay']:\n        if (binding in opts):\n            try:\n                int(opts[binding])\n                bond.update({\n                    binding: opts[binding],\n                })\n            except ValueError:\n                _raise_error_iface(iface, binding, ['integer'])\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({\n                binding: bond_def[binding],\n            })\n    if ('use_carrier' in opts):\n        if (opts['use_carrier'] in _CONFIG_TRUE):\n            bond.update({\n                'use_carrier': '1',\n            })\n        elif (opts['use_carrier'] in _CONFIG_FALSE):\n            bond.update({\n                'use_carrier': '0',\n            })\n        else:\n            valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({\n            'use_carrier': bond_def['use_carrier'],\n        })\n    return bond\n", "label": 0}
{"function": "\n\ndef _verify_tombstones(self, tx_objs, policy):\n    for (o_name, diskfiles) in tx_objs.items():\n        try:\n            self._open_tx_diskfile(o_name, policy)\n            self.fail('DiskFileDeleted expected')\n        except DiskFileDeleted as exc:\n            tx_delete_time = exc.timestamp\n        try:\n            self._open_rx_diskfile(o_name, policy)\n            self.fail('DiskFileDeleted expected')\n        except DiskFileDeleted as exc:\n            rx_delete_time = exc.timestamp\n        self.assertEqual(tx_delete_time, rx_delete_time)\n", "label": 0}
{"function": "\n\ndef _get_axis_class(axis_type, range_input):\n    if (axis_type is None):\n        return None\n    elif (axis_type == 'linear'):\n        return LinearAxis\n    elif (axis_type == 'log'):\n        return LogAxis\n    elif (axis_type == 'datetime'):\n        return DatetimeAxis\n    elif (axis_type == 'auto'):\n        if isinstance(range_input, FactorRange):\n            return CategoricalAxis\n        elif isinstance(range_input, Range1d):\n            try:\n                Datetime.validate(Datetime(), range_input.start)\n                return DatetimeAxis\n            except ValueError:\n                pass\n        return LinearAxis\n    else:\n        raise ValueError((\"Unrecognized axis_type: '%r'\" % axis_type))\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_param_type(cls, param):\n    try:\n        param.aliases\n    except AttributeError:\n        return LABEL_POS\n    else:\n        return LABEL_OPT\n", "label": 0}
{"function": "\n\ndef test_basic_properties(self):\n    data = ['a', 'b', 'c', 'b', 'b', 'c', 'a', 'c', 'd']\n    cat_comp = CategoricalComponent(data)\n    np.testing.assert_equal(cat_comp.labels, data)\n    np.testing.assert_equal(cat_comp.codes, [0, 1, 2, 1, 1, 2, 0, 2, 3])\n    np.testing.assert_equal(cat_comp.categories, ['a', 'b', 'c', 'd'])\n    with warnings.catch_warnings(record=True) as w:\n        cat_comp.data\n    assert (len(w) == 1)\n    assert (str(w[0].message) == \"The 'data' attribute is deprecated. Use 'codes' instead to access the underlying index of the categories\")\n", "label": 0}
{"function": "\n\ndef test_create_with_foreign_key(self):\n    'Create a table with a foreign key constraint'\n    inmap = self.std_map()\n    inmap['schema public'].update({\n        'table t1': {\n            'columns': [{\n                'c11': {\n                    'type': 'integer',\n                },\n            }, {\n                'c12': {\n                    'type': 'text',\n                },\n            }],\n        },\n        'table t2': {\n            'columns': [{\n                'c21': {\n                    'type': 'integer',\n                },\n            }, {\n                'c22': {\n                    'type': 'text',\n                },\n            }, {\n                'c23': {\n                    'type': 'integer',\n                },\n            }],\n            'foreign_keys': {\n                't2_c23_fkey': {\n                    'columns': ['c23'],\n                    'references': {\n                        'columns': ['c11'],\n                        'table': 't1',\n                    },\n                },\n            },\n        },\n    })\n    sql = self.to_sql(inmap)\n    crt1 = 0\n    crt2 = 1\n    if ('t1' in sql[1]):\n        crt1 = 1\n        crt2 = 0\n    assert (fix_indent(sql[crt1]) == 'CREATE TABLE t1 (c11 integer, c12 text)')\n    assert (fix_indent(sql[crt2]) == 'CREATE TABLE t2 (c21 integer, c22 text, c23 integer)')\n    assert (fix_indent(sql[2]) == 'ALTER TABLE t2 ADD CONSTRAINT t2_c23_fkey FOREIGN KEY (c23) REFERENCES t1 (c11)')\n", "label": 0}
{"function": "\n\n@staticmethod\ndef specialize(args, gtl_map, loop_id):\n    \"Given ``args``, instances of some :class:`fusion.Arg` superclass,\\n        create and return specialized :class:`fusion.Arg` objects.\\n\\n        :param args: either a single :class:`host.Arg` object or an iterator\\n                     (accepted: list, tuple) of :class:`host.Arg` objects.\\n        :gtl_map: a dict associating global maps' names to local maps' c_names.\\n        :param loop_id: indicates the position of the args` loop in the loop\\n                        chain\\n        \"\n\n    def convert(arg, gtl_map, loop_id):\n        maps = as_tuple(arg.map, Map)\n        c_local_maps = ([None] * len(maps))\n        for (i, map) in enumerate(maps):\n            c_local_maps[i] = ([None] * len(map))\n            for (j, m) in enumerate(map):\n                c_local_maps[i][j] = gtl_map[('%s%d_%d' % (m.name, i, j))]\n        _arg = Arg(arg.data, arg.map, arg.idx, arg.access, arg._flatten)\n        _arg._loop_position = loop_id\n        _arg.position = arg.position\n        _arg.indirect_position = arg.indirect_position\n        _arg._c_local_maps = c_local_maps\n        return _arg\n    try:\n        return [convert(arg, gtl_map, loop_id) for arg in args]\n    except TypeError:\n        return convert(args, gtl_map, loop_id)\n", "label": 0}
{"function": "\n\ndef test_disk_usage(self):\n    usage = psutil.disk_usage(os.getcwd())\n    assert (usage.total > 0), usage\n    assert (usage.used > 0), usage\n    assert (usage.free > 0), usage\n    assert (usage.total > usage.used), usage\n    assert (usage.total > usage.free), usage\n    assert (0 <= usage.percent <= 100), usage.percent\n    fname = tempfile.mktemp()\n    try:\n        psutil.disk_usage(fname)\n    except OSError:\n        err = sys.exc_info()[1]\n        if (err.args[0] != errno.ENOENT):\n            raise\n    else:\n        self.fail('OSError not raised')\n", "label": 1}
{"function": "\n\ndef _make_window(window_dict):\n    '\\n    Creates a new class for that window and registers it at this module.\\n    '\n    cls_name = ('%sWindow' % camel_case(str(window_dict['name'])))\n    bases = (Window,)\n    attrs = {\n        '__module__': sys.modules[__name__],\n        'name': str(window_dict['name']),\n        'inv_type': str(window_dict['id']),\n        'inv_data': window_dict,\n    }\n\n    def make_slot_method(index, size=1):\n        if (size == 1):\n            return (lambda self: self.slots[index])\n        else:\n            return (lambda self: self.slots[index:(index + size)])\n    for slots in window_dict.get('slots', []):\n        index = slots['index']\n        size = slots.get('size', 1)\n        attr_name = snake_case(str(slots['name']))\n        attr_name += ('_slot' if (size == 1) else '_slots')\n        slots_method = make_slot_method(index, size)\n        slots_method.__name__ = attr_name\n        attrs[attr_name] = property(slots_method)\n    for (i, prop_name) in enumerate(window_dict.get('properties', [])):\n\n        def make_prop_method(i):\n            return (lambda self: self.properties[i])\n        prop_method = make_prop_method(i)\n        prop_name = snake_case(str(prop_name))\n        prop_method.__name__ = prop_name\n        attrs[prop_name] = property(prop_method)\n    cls = type(cls_name, bases, attrs)\n    assert (not hasattr(sys.modules[__name__], cls_name)), ('Window \"%s\" already registered at %s' % (cls_name, __name__))\n    setattr(sys.modules[__name__], cls_name, cls)\n    return cls\n", "label": 0}
{"function": "\n\ndef _connect(self):\n    '[Internal]'\n    password = None\n    if (len(self.server_list[0]) > 2):\n        password = self.server_list[0][2]\n    try:\n        self.connect(self.server_list[0][0], self.server_list[0][1], self._nickname, password, ircname=self._realname)\n    except ServerConnectionError:\n        pass\n", "label": 0}
{"function": "\n\ndef bn_relu_conv_backward(dout, cache, dres_ref=None):\n    '\\n    Backward pass for the conv-batchnorm-relu convenience layer.\\n    '\n    assert ((dres_ref is None) or (len(dres_ref) == 0))\n    (conv_cache, batchnorm_cache, relu_cache) = cache\n    if (dres_ref is not None):\n        dres_ref.append(dout)\n    (dx, dw, db) = conv_backward_fast(dout, conv_cache)\n    dout = relu_backward(dout, relu_cache)\n    (dout, dgamma, dbeta) = spatial_batchnorm_backward(dout, batchnorm_cache)\n    return (dx, dw, db, dgamma, dbeta)\n", "label": 0}
{"function": "\n\ndef value_ds_to_numpy(ds, count):\n    ret = None\n    try:\n        values = ds[(count * (- 1)):]\n        ret = numpy.array([float(value) for value in values])\n    except IndexError:\n        pass\n    except TypeError:\n        pass\n    return ret\n", "label": 0}
{"function": "\n\ndef test_bad_querysets(self):\n    with pytest.raises(ValueError) as excinfo:\n        Author.objects.all().order_by('name').iter_smart_chunks()\n    assert ('ordering' in str(excinfo.value))\n    with pytest.raises(ValueError) as excinfo:\n        Author.objects.all()[:5].iter_smart_chunks()\n    assert ('sliced QuerySet' in str(excinfo.value))\n    with pytest.raises(ValueError) as excinfo:\n        NameAuthor.objects.all().iter_smart_chunks()\n    assert ('non-integer primary key' in str(excinfo.value))\n", "label": 0}
{"function": "\n\ndef test_retrieve(client):\n    response = client.get('/widgets/1')\n    assert (response.status_code == 200)\n    assert (helpers.get_data(response) == {\n        'id': '1',\n        'name': 'Foo',\n        'description': 'foo widget',\n    })\n", "label": 0}
{"function": "\n\ndef test_server(args, prepare_func=None, family=socket.AF_INET):\n    try:\n        try:\n            sock = socket.socket(family=family)\n            sock.settimeout(args.timeout)\n            sock.connect((args.host, args.port))\n        except socket.error as e:\n            print('Unable to connect to {0}:{1}: {2}'.format(args.host, args.port, e))\n            return False\n        (remote_addr, remote_port) = sock.getpeername()[:2]\n        print('Connected to: {0}:{1}'.format(remote_addr, remote_port))\n        if (prepare_func is not None):\n            prepare_func(sock)\n            print('Pre-TLS stage completed, continuing with handshake')\n        return handle_ssl(sock, args)\n    except (Failure, socket.error) as e:\n        print(('Unable to check for vulnerability: ' + str(e)))\n        return False\n    finally:\n        if sock:\n            sock.close()\n", "label": 0}
{"function": "\n\ndef test_fold_stdev(self):\n    '\\n        Tests the standard deviations of counters is properly computed.\\n        '\n    now = 10\n    metrics = [Timer('k', 10), Timer('k', 15), Timer('j', 7.9), Timer('j', 8)]\n    result = Timer.fold(metrics, now)\n    assert (('timers.k.stdev', Timer._stdev([10, 15], 12.5), now) == self._get_metric('timers.k.stdev', result))\n    assert (('timers.j.stdev', Timer._stdev([7.9, 8], 7.95), now) == self._get_metric('timers.j.stdev', result))\n", "label": 0}
{"function": "\n\ndef exchange(self, pdu, timeout):\n    if ((not isinstance(pdu, Symmetry)) or (self.cfg.get('symm-log') is True)):\n        log.debug('SEND {0}'.format(pdu))\n    data = (pdu.to_string() if pdu else None)\n    try:\n        data = self.mac.exchange(data, timeout)\n        if (data is None):\n            return None\n    except nfc.clf.DigitalProtocolError as error:\n        log.debug('{0!r}'.format(error))\n        return None\n    pdu = ProtocolDataUnit.from_string(data)\n    if ((not isinstance(pdu, Symmetry)) or (self.cfg.get('symm-log') is True)):\n        log.debug('RECV {0}'.format(pdu))\n    return pdu\n", "label": 1}
{"function": "\n\ndef test_mreg_interpolation(self):\n    park_id = NREL.park_id['tehachapi']\n    windpark = NREL().get_windpark(park_id, 3, 2004)\n    target = windpark.get_target()\n    timestep = 600\n    measurements = target.get_measurements()[300:500]\n    (damaged, indices) = MARDestroyer().destroy(measurements, percentage=0.5)\n    before_misses = MissingDataFinder().find(damaged, timestep)\n    neighbors = windpark.get_turbines()[:(- 1)]\n    reg = 'knn'\n    regargs = {\n        'n': 10,\n        'variant': 'uniform',\n    }\n    nseries = [t.get_measurements()[300:500] for t in neighbors]\n    t_hat = MRegInterpolation().interpolate(damaged, timestep=timestep, neighbor_series=nseries, reg=reg, regargs=regargs)\n    after_misses = MissingDataFinder().find(t_hat, timestep)\n    assert (len(after_misses) < 1)\n", "label": 0}
{"function": "\n\ndef __init__(self, thread):\n    self.ident = 0\n    try:\n        self.ident = thread.ident\n    except AttributeError:\n        pass\n    if (not self.ident):\n        for (tid, athread) in threading._active.items():\n            if (athread is thread):\n                self.ident = tid\n                break\n    try:\n        self.name = thread.name\n    except AttributeError:\n        self.name = thread.getName()\n    try:\n        self.daemon = thread.daemon\n    except AttributeError:\n        self.daemon = thread.isDaemon()\n", "label": 0}
{"function": "\n\ndef __init__(self, obj, *args, **kwds):\n    assert ((len(args) % 2) == 0)\n    attrs = kwds.get('attrs', {\n        \n    })\n    for (k, v) in zip(args[::2], args[1::2]):\n        attrs[k] = v\n    self.attrs = attrs\n    self.obj = obj\n", "label": 0}
{"function": "\n\ndef indication(self, pdu):\n    if _debug:\n        deferred(ConsoleServer._debug, 'Indication %r', pdu)\n    try:\n        sys.stdout.write(pdu.pduData)\n    except Exception as err:\n        ConsoleServer._exception('Indication sys.stdout.write exception: %r', err)\n", "label": 0}
{"function": "\n\ndef test_werkzeug_upload():\n    try:\n        import werkzeug\n    except ImportError:\n        return\n    storage = app_storage()\n    object_name = 'my-txt-hello.txt'\n    filepath = (CWD + '/data/hello.txt')\n    file = None\n    with open(filepath, 'rb') as fp:\n        file = werkzeug.datastructures.FileStorage(fp)\n        file.filename = object_name\n        o = storage.upload(file, overwrite=True)\n        assert isinstance(o, Object)\n        assert (o.name == object_name)\n", "label": 0}
{"function": "\n\ndef test_match_unrolled(self):\n    ' tests that inference with scan matches result using unrolled loops '\n    unrolled_e_step = E_Step(h_new_coeff_schedule=self.h_new_coeff_schedule)\n    unrolled_e_step.register_model(self.model)\n    V = T.matrix()\n    scan_result = self.e_step.infer(V)\n    unrolled_result = unrolled_e_step.infer(V)\n    outputs = []\n    for key in scan_result:\n        outputs.append(scan_result[key])\n        outputs.append(unrolled_result[key])\n    f = function([V], outputs)\n    outputs = f(self.X)\n    assert ((len(outputs) % 2) == 0)\n    for i in xrange(0, len(outputs), 2):\n        assert np.allclose(outputs[i], outputs[(i + 1)])\n", "label": 0}
{"function": "\n\ndef handle(self, request, data):\n    try:\n        if (data['cipher'] == ''):\n            data['cipher'] = None\n        volume_type = cinder.volume_encryption_type_create(request, data['volume_type_id'], data)\n        messages.success(request, (_('Successfully created encryption for volume type: %s') % data['name']))\n        return volume_type\n    except Exception:\n        redirect = reverse('horizon:admin:volumes:index')\n        exceptions.handle(request, _('Unable to create encrypted volume type.'), redirect=redirect)\n", "label": 0}
{"function": "\n\ndef _setup_extensions(self):\n    self.extensions = []\n    if try_murmur3:\n        self.extensions.append(murmur3_ext)\n    if try_libev:\n        self.extensions.append(libev_ext)\n    if try_cython:\n        try:\n            from Cython.Build import cythonize\n            cython_candidates = ['cluster', 'concurrent', 'connection', 'cqltypes', 'metadata', 'pool', 'protocol', 'query', 'util']\n            compile_args = ([] if is_windows else ['-Wno-unused-function'])\n            self.extensions.extend(cythonize([Extension(('cassandra.%s' % m), [('cassandra/%s.py' % m)], extra_compile_args=compile_args) for m in cython_candidates], nthreads=build_concurrency, exclude_failures=True))\n            self.extensions.extend(cythonize(NoPatchExtension('*', ['cassandra/*.pyx'], extra_compile_args=compile_args), nthreads=build_concurrency))\n        except Exception:\n            sys.stderr.write('Failed to cythonize one or more modules. These will not be compiled as extensions (optional).\\n')\n", "label": 0}
{"function": "\n\ndef test_delete_removes_watches(self):\n    t = ThreadFactory()\n    NewPostEvent.notify('me@me.com', t)\n    assert NewPostEvent.is_notifying('me@me.com', t)\n    t.delete()\n    assert (not NewPostEvent.is_notifying('me@me.com', t))\n", "label": 0}
{"function": "\n\ndef _emit(self, cond_br, target=None, reg=None, absolute=False, link=REGISTERS['null']):\n    if (target is None):\n        imm = 0\n    elif isinstance(target, Label):\n        target.pinned = False\n        self.asm._add_backpatch_item(target.name)\n        imm = 0\n    elif isinstance(target, int):\n        imm = target\n    else:\n        raise AssembleError('Invalid branch target: {}'.format(target))\n    if reg:\n        if ((not (reg.spec & _REG_AR)) or (reg.name not in GENERAL_PURPOSE_REGISTERS)):\n            raise AssembleError('Must be general purpose regfile A register {}'.format(reg))\n        assert (reg.addr < 32)\n        raddr_a = reg.addr\n        use_reg = True\n    else:\n        raddr_a = 0\n        use_reg = False\n    (waddr_add, waddr_mul, write_swap, pack) = self._encode_write_operands(link)\n    if pack:\n        raise AssembleError('Packing is not available for link register')\n    insn = BranchInsn(sig=15, cond_br=cond_br, rel=(not absolute), reg=use_reg, raddr_a=raddr_a, ws=write_swap, waddr_add=waddr_add, waddr_mul=waddr_mul, immediate=imm)\n    self.asm._emit(insn)\n", "label": 1}
{"function": "\n\ndef save(self, filename=None, directory=None):\n    \"Save the DOT source to file.\\n\\n        Args:\\n            filename: Filename for saving the source (defaults to name + '.gv')\\n            directory: (Sub)directory for source saving and rendering.\\n        Returns:\\n            The (possibly relative) path of the saved source file.\\n        \"\n    if (filename is not None):\n        self.filename = filename\n    if (directory is not None):\n        self.directory = directory\n    filepath = self.filepath\n    tools.mkdirs(filepath)\n    data = text_type(self.source)\n    with io.open(filepath, 'w', encoding=self.encoding) as fd:\n        fd.write(data)\n    return filepath\n", "label": 0}
{"function": "\n\ndef __init__(self, sliceDB, seqDB, annotationType=None, itemClass=AnnotationSeq, itemSliceClass=AnnotationSlice, itemAttrDict=None, sliceAttrDict=None, maxCache=None, autoGC=True, checkFirstID=True, **kwargs):\n    'sliceDB must map identifier to a sliceInfo object;\\n        sliceInfo must have attributes: id, start, stop, orientation;\\n        seqDB must map sequence ID to a sliceable sequence object;\\n        sliceAttrDict gives optional dict of item attributes that\\n        should be mapped to sliceDB item attributes.\\n        maxCache specfies the maximum number of annotation objects\\n        to keep in the cache.'\n    if autoGC:\n        self._weakValueDict = classutil.RecentValueDictionary(autoGC)\n    else:\n        self._weakValueDict = {\n            \n        }\n    self.autoGC = autoGC\n    if (sliceAttrDict is None):\n        sliceAttrDict = {\n            \n        }\n    if (sliceDB is not None):\n        self.sliceDB = sliceDB\n    else:\n        self.sliceDB = classutil.get_shelve_or_dict(**kwargs)\n    self.seqDB = seqDB\n    self.annotationType = annotationType\n    self.itemClass = itemClass\n    self.itemSliceClass = itemSliceClass\n    self.sliceAttrDict = sliceAttrDict\n    if (maxCache is not None):\n        self.maxCache = maxCache\n    if checkFirstID:\n        try:\n            k = iter(self).next()\n            self.get_annot_obj(k, self.sliceDB[k])\n        except KeyError:\n            raise KeyError(('cannot create annotation object %s; sequence database %s may not be correct' % (k, repr(seqDB))))\n        except StopIteration:\n            pass\n", "label": 1}
{"function": "\n\ndef get_field_history_user(self, instance):\n    try:\n        return instance._field_history_user\n    except AttributeError:\n        try:\n            if self.thread.request.user.is_authenticated():\n                return self.thread.request.user\n            return None\n        except AttributeError:\n            return None\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    ploader = PluginLoaderMock()\n    self.plug = ChatPlugin(ploader, {\n        \n    })\n    assert (ploader.provides_ident == 'Chat')\n    assert isinstance(ploader.provides_obj, ChatCore)\n", "label": 0}
{"function": "\n\ndef test_min_length(self):\n    h = Hashids(min_length=25)\n    assert (h.encode(7452, 2967, 21401) == 'pO3K69b86jzc6krI416enr2B5')\n    assert (h.encode(1, 2, 3) == 'gyOwl4B97bo2fXhVaDR0Znjrq')\n    assert (h.encode(6097) == 'Nz7x3VXyMYerRmWeOBQn6LlRG')\n    assert (h.encode(99, 25) == 'k91nqP3RBe3lKfDaLJrvy8XjV')\n", "label": 0}
{"function": "\n\ndef test_root_name(webapp):\n    (status, content) = make_request(webapp, ('%s/name' % webapp.server.http.base))\n    assert (status == 200)\n    assert (content == b'Earth')\n", "label": 0}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    buildutil.linux.BuildEnvironment.add_arguments(parser)\n    args = parser.parse_args()\n    retval = (- 1)\n    env = buildutil.linux.BuildEnvironment(args)\n    env.cmake_flags = '-DMESSAGE=\"Hello, World!\"'\n    try:\n        env.git_clean()\n        env.run_cmake()\n        env.run_make()\n        env.make_archive(['Hello'], 'output.zip')\n        retval = 0\n    except buildutil.common.Error as e:\n        ((print >> sys.stderr), ('Caught buildutil error: %s' % e.error_message))\n        retval = e.error_code\n    except IOError as e:\n        ((print >> sys.stderr), ('Caught IOError for file %s: %s' % (e.filename, e.strerror)))\n        retval = (- 1)\n    return retval\n", "label": 0}
{"function": "\n\ndef _deserialize(self, value, attr, data):\n    if (not self.truthy):\n        return bool(value)\n    else:\n        try:\n            if (value in self.truthy):\n                return True\n            elif (value in self.falsy):\n                return False\n        except TypeError:\n            pass\n    self.fail('invalid')\n", "label": 0}
{"function": "\n\ndef test_array_CONMIN(self):\n    try:\n        from pyopt_driver.pyopt_driver import pyOptDriver\n    except ImportError:\n        raise SkipTest('this test requires pyOpt to be installed')\n    self.top = ArrayOpt()\n    set_as_top(self.top)\n    try:\n        self.top.driver.optimizer = 'CONMIN'\n    except ValueError:\n        raise SkipTest('CONMIN not present on this system')\n    self.top.driver.title = 'Little Test'\n    optdict = {\n        \n    }\n    self.top.driver.options = optdict\n    self.top.run()\n    assert_rel_error(self, self.top.paraboloid.x[0], 7.175775, 0.01)\n    assert_rel_error(self, self.top.paraboloid.x[1], (- 7.824225), 0.01)\n", "label": 0}
{"function": "\n\ndef test_registry_uris_param_v2(self):\n    spec = CommonSpec()\n    spec.set_params(registry_uris=['http://registry.example.com:5000/v2'], user=TEST_USER)\n    registry = spec.registry_uris.value[0]\n    assert (registry.uri == 'http://registry.example.com:5000')\n    assert (registry.docker_uri == 'registry.example.com:5000')\n    assert (registry.version == 'v2')\n", "label": 0}
{"function": "\n\ndef test_number_to_string(self):\n    ' Numbers are turned into strings.\\n        '\n    cleaner = Cleaners()\n    in_int = 85\n    in_float = 82.12\n    in_string = 'big frame, small spirit!'\n    in_list = ['hands', 'by', 'the', 'halyards']\n    in_none = None\n    assert (cleaner.number_to_string(in_int) == str(in_int))\n    assert (cleaner.number_to_string(in_float) == str(in_float))\n    assert (cleaner.number_to_string(in_string) == in_string)\n    assert (cleaner.number_to_string(in_list) == in_list)\n    assert (cleaner.number_to_string(in_none) is None)\n", "label": 0}
{"function": "\n\ndef __call__(self, action, *args, **kwargs):\n    module_name = ('%s.%s' % (self.package, action.replace('-', '_')))\n    cwd = kwargs.get('cwd')\n    if cwd:\n        self.chdir(cwd)\n    if kwargs.get('raises'):\n        with pytest.raises(qisys.error.Error) as error:\n            qisys.script.run_action(module_name, args)\n        return str(error.value)\n    if kwargs.get('retcode'):\n        try:\n            qisys.script.run_action(module_name, args)\n        except SystemExit as e:\n            return e.code\n        return 0\n    else:\n        return qisys.script.run_action(module_name, args)\n", "label": 0}
{"function": "\n\ndef connection_made(self, transport):\n    \"\\n        asyncio.Protocol member - called upon when there is a new socket\\n        connection. This creates a new responder (as determined by the member\\n        'responder_type') and stores in a list. Incoming data from this\\n        connection will always call\\n        on_data to the last element of this list.\\n\\n        Parameters\\n        ----------\\n        transport : asyncio.Transport\\n            The Transport handling the socket communication\\n        \"\n    self.transport = transport\n    self.responders = [self.make_responder(self)]\n    try:\n        good_func = callable(self.responders[0].on_data)\n    except AttributeError:\n        good_func = False\n    if (not good_func):\n        err_str = \"Provided responder MUST implement an 'on_data' method\"\n        raise TypeError(err_str)\n    log_info = (id(self), self.remote_hostname, self.remote_port)\n    log.info(('%d connection from %s:%s' % log_info))\n", "label": 0}
{"function": "\n\ndef __init__(self, request, subscription):\n    '\\n\\n        :param request: The WAMP request ID of this request.\\n        :type request: int\\n        :param subscription: The subscription ID for the subscription to unsubscribe from.\\n        :type subscription: int\\n        '\n    assert (type(request) in six.integer_types)\n    assert (type(subscription) in six.integer_types)\n    Message.__init__(self)\n    self.request = request\n    self.subscription = subscription\n", "label": 0}
{"function": "\n\ndef password_forgotten_view(request):\n    '\\n  forgotten password view and submit.\\n  includes return_url\\n  '\n    from helios_auth.view_utils import render_template\n    from helios_auth.models import User\n    if (request.method == 'GET'):\n        return render_template(request, 'password/forgot', {\n            'return_url': request.GET.get('return_url', ''),\n        })\n    else:\n        username = request.POST['username']\n        return_url = request.POST['return_url']\n        try:\n            user = User.get_by_type_and_id('password', username)\n        except User.DoesNotExist:\n            return render_template(request, 'password/forgot', {\n                'return_url': request.GET.get('return_url', ''),\n                'error': 'no such username',\n            })\n        body = ('\\n\\nThis is a password reminder:\\n\\nYour username: %s\\nYour password: %s\\n\\n--\\n%s\\n' % (user.user_id, user.info['password'], settings.SITE_TITLE))\n        send_mail('password reminder', body, settings.SERVER_EMAIL, [('%s <%s>' % (user.info['name'], user.info['email']))], fail_silently=False)\n        return HttpResponseRedirect(return_url)\n", "label": 0}
{"function": "\n\ndef vim_choice(prompt, default, choices):\n    default = (choices.index(default) + 1)\n    choices_str = '\\n'.join([('&%s' % choice) for choice in choices])\n    try:\n        choice = int(vim.eval(('confirm(\"%s\", \"%s\", %s)' % (prompt, choices_str, default))))\n    except KeyboardInterrupt:\n        return None\n    if (choice == 0):\n        return None\n    return choices[(choice - 1)]\n", "label": 0}
{"function": "\n\ndef get_submitted_exams(course, student):\n    try:\n        exams = Exam.objects.filter(course=course).order_by('exam_num')\n    except Exam.DoesNotExist:\n        exams = None\n    try:\n        submitted_exams = ExamSubmission.objects.filter(exam__course=course, student=student)\n    except ExamSubmission.DoesNotExist:\n        submitted_exams = None\n    if (len(exams) != len(submitted_exams)):\n        for exam in exams:\n            found_exam = False\n            for submitted_exam in submitted_exams:\n                if (exam.id == submitted_exam.exam_id):\n                    found_exam = True\n            if (not found_exam):\n                submission = ExamSubmission.objects.create(student=student, exam=exam)\n                submission.save()\n        submitted_exams = ExamSubmission.objects.filter(exam__course=course, student=student)\n    return submitted_exams\n", "label": 1}
{"function": "\n\ndef testExceptions(self):\n    request = roots.Request()\n    try:\n        request.write(b'blah')\n    except NotImplementedError:\n        pass\n    else:\n        self.fail()\n    try:\n        request.finish()\n    except NotImplementedError:\n        pass\n    else:\n        self.fail()\n", "label": 0}
{"function": "\n\ndef execute(self, cmd):\n    if (len(cmd) > 237):\n        print_error('Your command must be at most 237 characters long. Longer strings might crash the server.')\n        return\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    sock.bind(('0.0.0.0', 9999))\n    sock.settimeout(2)\n    packet = ((((b'\\x0c\\x153\\x00' + os.urandom(4)) + (b'\\x00' * 38)) + struct.pack('<H', len(cmd))) + cmd).ljust(512, b'\\x00')\n    try:\n        sock.sendto(packet, (self.target, 9999))\n    except socket.error:\n        return ''\n    while True:\n        try:\n            (data, addr) = sock.recvfrom(512)\n        except socket.timeout:\n            sock.close()\n            raise\n        if ((len(data) == 512) and (data[1] == '\\x16')):\n            break\n    length = struct.unpack('<H', data[14:16])[0]\n    output = data[16:(16 + length)]\n    sock.close()\n    return output\n", "label": 0}
{"function": "\n\ndef get_request_params(what, request=None, **kwargs):\n    'Returns requested argument value'\n    args = {\n        'app_name': 1,\n        'model_name': 2,\n        'pk': 4,\n    }\n    try:\n        return kwargs.get(what, (request.path.split('/')[args[what]] if request else None))\n    except IndexError:\n        pass\n", "label": 0}
{"function": "\n\ndef test_options(self):\n    app = self.build_app()\n    app.config['RAPTOR_TRIGGER'] = 'konami-code'\n    app.config['RAPTOR_DELAY'] = 5000\n    init_app(app)\n    response = app.test_client().get('/')\n    assert ('enterOn: \"konami-code\"' in response.data)\n    assert ('delayTime: 5000' in response.data)\n", "label": 0}
{"function": "\n\ndef plot(figure_or_data, show_link=True, link_text='Export to plot.ly', validate=True, output_type='file', include_plotlyjs=True, filename='temp-plot.html', auto_open=True):\n    \" Create a plotly graph locally as an HTML document or string.\\n\\n    Example:\\n    ```\\n    from plotly.offline import plot\\n    import plotly.graph_objs as go\\n\\n    plot([go.Scatter(x=[1, 2, 3], y=[3, 2, 6])], filename='my-graph.html')\\n    ```\\n    More examples below.\\n\\n    figure_or_data -- a plotly.graph_objs.Figure or plotly.graph_objs.Data or\\n                      dict or list that describes a Plotly graph.\\n                      See https://plot.ly/python/ for examples of\\n                      graph descriptions.\\n\\n    Keyword arguments:\\n    show_link (default=True) -- display a link in the bottom-right corner of\\n        of the chart that will export the chart to Plotly Cloud or\\n        Plotly Enterprise\\n    link_text (default='Export to plot.ly') -- the text of export link\\n    validate (default=True) -- validate that all of the keys in the figure\\n        are valid? omit if your version of plotly.js has become outdated\\n        with your version of graph_reference.json or if you need to include\\n        extra, unnecessary keys in your figure.\\n    output_type ('file' | 'div' - default 'file') -- if 'file', then\\n        the graph is saved as a standalone HTML file and `plot`\\n        returns None.\\n        If 'div', then `plot` returns a string that just contains the\\n        HTML <div> that contains the graph and the script to generate the\\n        graph.\\n        Use 'file' if you want to save and view a single graph at a time\\n        in a standalone HTML file.\\n        Use 'div' if you are embedding these graphs in an HTML file with\\n        other graphs or HTML markup, like a HTML report or an website.\\n    include_plotlyjs (default=True) -- If True, include the plotly.js\\n        source code in the output file or string.\\n        Set as False if your HTML file already contains a copy of the plotly.js\\n        library.\\n    filename (default='temp-plot.html') -- The local filename to save the\\n        outputted chart to. If the filename already exists, it will be\\n        overwritten. This argument only applies if `output_type` is 'file'.\\n    auto_open (default=True) -- If True, open the saved file in a\\n        web browser after saving.\\n        This argument only applies if `output_type` is 'file'.\\n    \"\n    if (output_type not in ['div', 'file']):\n        raise ValueError(((\"`output_type` argument must be 'div' or 'file'. You supplied `\" + output_type) + '``'))\n    if ((not filename.endswith('.html')) and (output_type == 'file')):\n        warnings.warn((('Your filename `' + filename) + \"` didn't end with .html. Adding .html to the end of your file.\"))\n        filename += '.html'\n    (plot_html, plotdivid, width, height) = _plot_html(figure_or_data, show_link, link_text, validate, '100%', '100%', global_requirejs=False)\n    resize_script = ''\n    if ((width == '100%') or (height == '100%')):\n        resize_script = '<script type=\"text/javascript\">window.removeEventListener(\"resize\");window.addEventListener(\"resize\", function(){{Plotly.Plots.resize(document.getElementById(\"{id}\"));}});</script>'.format(id=plotdivid)\n    if (output_type == 'file'):\n        with open(filename, 'w') as f:\n            if include_plotlyjs:\n                plotly_js_script = ''.join(['<script type=\"text/javascript\">', get_plotlyjs(), '</script>'])\n            else:\n                plotly_js_script = ''\n            f.write(''.join(['<html>', '<head><meta charset=\"utf-8\" /></head>', '<body>', plotly_js_script, plot_html, resize_script, '</body>', '</html>']))\n        url = ('file://' + os.path.abspath(filename))\n        if auto_open:\n            webbrowser.open(url)\n        return url\n    elif (output_type == 'div'):\n        if include_plotlyjs:\n            return ''.join(['<div>', '<script type=\"text/javascript\">', get_plotlyjs(), '</script>', plot_html, '</div>'])\n        else:\n            return plot_html\n", "label": 1}
{"function": "\n\ndef deploy_file(self, file_name, calc_md5=True, calc_sha1=True, parameters={\n    \n}):\n    '\\n        Upload the given file to this path\\n        '\n    if calc_md5:\n        md5 = md5sum(file_name)\n    if calc_sha1:\n        sha1 = sha1sum(file_name)\n    target = self\n    if self.is_dir():\n        target = (self / pathlib.Path(file_name).name)\n    with open(file_name, 'rb') as fobj:\n        target.deploy(fobj, md5, sha1, parameters)\n", "label": 0}
{"function": "\n\ndef gradient(self, x, Y):\n    '\\n        Computes the gradient of the Polynomial kernel wrt. to the left argument, i.e.\\n        \\nabla_x k(x,y)=\\nabla_x (1+x^Ty)^d=d(1+x^Ty)^(d-1) y\\n        \\n        x - single sample on right hand side (1D vector)\\n        Y - samples on left hand side (2D matrix)\\n        '\n    assert (len(x.shape) == 1)\n    assert (len(Y.shape) == 2)\n    assert (len(x) == Y.shape[1])\n    return ((self.degree * pow((1 + x.dot(Y.T)), self.degree)) * Y)\n", "label": 0}
{"function": "\n\n@permission_required('core.manage_shop')\ndef export_dispatcher(request):\n    'Dispatches to the first export or to the add form.\\n    '\n    try:\n        export = Export.objects.all()[0]\n    except IndexError:\n        return HttpResponseRedirect(reverse('lfs_export_add_export'))\n    else:\n        return HttpResponseRedirect(reverse('lfs_export', kwargs={\n            'export_id': export.id,\n        }))\n", "label": 0}
{"function": "\n\ndef _describe_table(self, connection, table, charset=None, full_name=None):\n    'Run DESCRIBE for a ``Table`` and return processed rows.'\n    if (full_name is None):\n        full_name = self.identifier_preparer.format_table(table)\n    st = ('DESCRIBE %s' % full_name)\n    (rp, rows) = (None, None)\n    try:\n        try:\n            rp = connection.execution_options(skip_user_error_events=True).execute(st)\n        except exc.DBAPIError as e:\n            if (self._extract_error_code(e.orig) == 1146):\n                raise exc.NoSuchTableError(full_name)\n            else:\n                raise\n        rows = self._compat_fetchall(rp, charset=charset)\n    finally:\n        if rp:\n            rp.close()\n    return rows\n", "label": 0}
{"function": "\n\ndef get_model(app_label, model_name):\n    '\\n    Given an app label and a model name, returns the corresponding model class.\\n    '\n    try:\n        return apps.get_model(app_label, model_name)\n    except AppRegistryNotReady:\n        if (apps.apps_ready and (not apps.models_ready)):\n            app_config = apps.get_app_config(app_label)\n            import_module(('%s.%s' % (app_config.name, MODELS_MODULE_NAME)))\n            return apps.get_registered_model(app_label, model_name)\n        else:\n            raise\n", "label": 0}
{"function": "\n\n@contextfunction\ndef htform(context, form):\n    'Set time zone'\n    request = context['request']\n    user = None\n    if request.user.username:\n        try:\n            user = request.user.profile\n        except Exception:\n            pass\n    default_timezone = settings.HARDTREE_SERVER_DEFAULT_TIMEZONE\n    try:\n        conf = ModuleSetting.get('default_timezone')[0]\n        default_timezone = conf.value\n    except:\n        pass\n    try:\n        conf = ModuleSetting.get('default_timezone', user=user)[0]\n        default_timezone = conf.value\n    except Exception:\n        default_timezone = getattr(settings, 'HARDTREE_SERVER_TIMEZONE')[default_timezone][0]\n    all_timezones = getattr(settings, 'HARDTREE_SERVER_TIMEZONE', [(1, '(GMT-11:00) International Date Line West')])\n    title = all_timezones[int(default_timezone)][1]\n    GMT = title[4:10]\n    sign = GMT[0:1]\n    hours = int(GMT[1:3])\n    mins = int(GMT[4:6])\n    if (not form.errors):\n        for field in form:\n            try:\n                date = datetime.strptime(str(field.form.initial[field.name]), '%Y-%m-%d %H:%M:%S')\n                if date:\n                    if (sign == '-'):\n                        field.form.initial[field.name] = (date - timedelta(hours=hours, minutes=mins))\n                    else:\n                        field.form.initial[field.name] = (date + timedelta(hours=hours, minutes=mins))\n            except:\n                pass\n    return form\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(UserSettingsForm, self).__init__(*args, **kwargs)\n\n    def get_language_display_name(code, desc):\n        try:\n            desc = translation.get_language_info(code)['name_local']\n            desc = string.capwords(desc)\n        except KeyError:\n            pass\n        return ('%s (%s)' % (desc, code))\n    languages = [(k, get_language_display_name(k, v)) for (k, v) in settings.LANGUAGES]\n    self.fields['language'].choices = languages\n    timezones = []\n    language = translation.get_language()\n    current_locale = translation.to_locale(language)\n    babel_locale = babel.Locale.parse(current_locale)\n    for (tz, offset) in self._sorted_zones():\n        try:\n            utc_offset = (_('UTC %(hour)s:%(min)s') % {\n                'hour': offset[:3],\n                'min': offset[3:],\n            })\n        except Exception:\n            utc_offset = ''\n        if (tz == 'UTC'):\n            tz_name = _('UTC')\n        elif (tz == 'GMT'):\n            tz_name = _('GMT')\n        else:\n            tz_label = babel.dates.get_timezone_location(tz, locale=babel_locale)\n            tz_name = (_('%(offset)s: %(label)s') % {\n                'offset': utc_offset,\n                'label': tz_label,\n            })\n        timezones.append((tz, tz_name))\n    self.fields['timezone'].choices = timezones\n", "label": 0}
{"function": "\n\ndef test_helper_label():\n    f1 = [h for h in link_helper if (h[0] is helper)][0]\n    assert (helper_label(f1) == 'test helper')\n", "label": 0}
{"function": "\n\ndef test_vector_norm():\n    x = np.arange(30).reshape((5, 6))\n    s = symbol('x', discover(x))\n    assert eq(compute(s.vnorm(), x), np.linalg.norm(x))\n    assert eq(compute(s.vnorm(ord=1), x), np.linalg.norm(x.flatten(), ord=1))\n    assert eq(compute(s.vnorm(ord=4, axis=0), x), np.linalg.norm(x, ord=4, axis=0))\n    expr = s.vnorm(ord=4, axis=0, keepdims=True)\n    assert (expr.shape == compute(expr, x).shape)\n", "label": 0}
{"function": "\n\ndef assert_no_warnings(func, *args, **kw):\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        result = func(*args, **kw)\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            w = [e for e in w if (e.category is not np.VisibleDeprecationWarning)]\n        if (len(w) > 0):\n            raise AssertionError(('Got warnings when calling %s: %s' % (func.__name__, w)))\n    return result\n", "label": 0}
{"function": "\n\ndef get_custom_exports(self):\n\n    def _rewrap(export):\n        try:\n            return {\n                'form': FormExportSchema,\n                'case': CaseExportSchema,\n            }[export.type].wrap(export._doc)\n        except KeyError:\n            return export\n    for custom in list(self.custom_export_ids):\n        custom_export = self._get_custom(custom)\n        if custom_export:\n            (yield _rewrap(custom_export))\n", "label": 0}
{"function": "\n\ndef do_list(self, arg):\n    'list [arg]: lists last command issued\\n\\n        no arg -> list most recent command\\n        arg is integer -> list one history item, by index\\n        a..b, a:b, a:, ..b -> list spans from a (or start) to b (or end)\\n        arg is string -> list all commands matching string search\\n        arg is /enclosed in forward-slashes/ -> regular expression search\\n        '\n    try:\n        history = self.history.span((arg or '-1'))\n    except IndexError:\n        history = self.history.search(arg)\n    for hi in history:\n        self.poutput(hi.pr())\n", "label": 0}
{"function": "\n\n@Cache\ndef GetTypeManager(self):\n    ' Get dynamic type manager '\n    dynTypeMgr = None\n    if self.hostSystem:\n        try:\n            dynTypeMgr = self.hostSystem.RetrieveDynamicTypeManager()\n        except vmodl.fault.MethodNotFound as err:\n            pass\n    if (not dynTypeMgr):\n        cmdlineTypesMoId = 'ha-dynamic-type-manager'\n        dynTypeMgr = vmodl.reflect.DynamicTypeManager(cmdlineTypesMoId, self.stub)\n    return dynTypeMgr\n", "label": 0}
{"function": "\n\ndef compute_files(hive_holder, output, settings):\n    ' given the current hive_holder, compute the files that have to be saved in disk\\n    return: {BlockCellName: StrOrBytes to be saved in disk}\\n    param output: something that supports info, warn, error\\n    '\n    new_files = {\n        \n    }\n    for (block_cell_name, (cell, content)) in hive_holder.resources.iteritems():\n        if isinstance(cell, VirtualCell):\n            try:\n                target = cell.evaluate(settings)\n            except ConfigurationFileError as e:\n                output.error(('Error evaluating virtual %s: %s' % (block_cell_name, e.message)))\n                continue\n            content = hive_holder[target.block_name][target.cell_name].content\n            new_files[block_cell_name] = content.load.load\n        elif content.blob_updated:\n            new_files[block_cell_name] = content.load.load\n    return new_files\n", "label": 0}
{"function": "\n\ndef _update_project_members(self, request, data, project_id):\n    users_to_add = 0\n    try:\n        available_roles = api.keystone.role_list(request)\n        member_step = self.get_step(PROJECT_USER_MEMBER_SLUG)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_to_add += len(role_list)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_added = 0\n            for user in role_list:\n                api.keystone.add_tenant_user_role(request, project=project_id, user=user, role=role.id)\n                users_added += 1\n            users_to_add -= users_added\n    except Exception:\n        if PROJECT_GROUP_ENABLED:\n            group_msg = _(', add project groups')\n        else:\n            group_msg = ''\n        exceptions.handle(request, (_('Failed to add %(users_to_add)s project members%(group_msg)s and set project quotas.') % {\n            'users_to_add': users_to_add,\n            'group_msg': group_msg,\n        }))\n", "label": 0}
{"function": "\n\ndef _execute_multi_task(self, gen, executor, task):\n    if task.unordered:\n        results_gen = self._execute_multi_gen_task(gen, executor, task)\n        return gen.send(results_gen)\n    future_tasks = [executor.submit(t) for t in task.tasks]\n    while True:\n        if (not task.wait(executor, future_tasks, self.engine.pool_timeout)):\n            self.engine.update_gui()\n        else:\n            break\n    if task.skip_errors:\n        results = []\n        for f in future_tasks:\n            try:\n                results.append(f.result())\n            except Exception:\n                pass\n    else:\n        try:\n            results = [f.result() for f in future_tasks]\n        except Exception:\n            return gen.throw(*sys.exc_info())\n    return gen.send(results)\n", "label": 1}
{"function": "\n\ndef nested_update(d, u):\n    for (k, v) in u.iteritems():\n        if isinstance(v, collections.Mapping):\n            r = nested_update(d.get(k, {\n                \n            }), v)\n            d[k] = r\n        elif isinstance(v, collections.Iterable):\n            try:\n                d[k].extend(u[k])\n            except KeyError:\n                d[k] = u[k]\n        else:\n            d[k] = u[k]\n    return d\n", "label": 0}
{"function": "\n\ndef test_bind_from_metadata(self):\n    (users, User) = (self.tables.users, self.classes.User)\n    mapper(User, users)\n    session = create_session()\n    session.execute(users.insert(), dict(name='Johnny'))\n    assert (len(session.query(User).filter_by(name='Johnny').all()) == 1)\n    session.execute(users.delete())\n    assert (len(session.query(User).filter_by(name='Johnny').all()) == 0)\n    session.close()\n", "label": 0}
{"function": "\n\ndef cleanup(self):\n    'Destroys benchmark environment.'\n    ctxlst = (self._visited or self._get_sorted_context_lst())\n    for ctx in ctxlst[::(- 1)]:\n        try:\n            ctx.cleanup()\n        except Exception as e:\n            LOG.error(('Context %s failed during cleanup.' % ctx.get_name()))\n            LOG.exception(e)\n", "label": 0}
{"function": "\n\n@register.simple_tag(takes_context=True)\ndef render_paginator(context, page, page_var='page', hashtag=''):\n    query_dict = context['request'].GET.copy()\n    try:\n        del query_dict[page_var]\n    except KeyError:\n        pass\n    extra_query = ''\n    if query_dict:\n        extra_query = ('&%s' % query_dict.urlencode())\n    if hashtag:\n        hashtag = ('#%s' % hashtag)\n    new_context = {\n        'page': page,\n        'page_var': page_var,\n        'hashtag': hashtag,\n        'extra_query': extra_query,\n    }\n    if isinstance(page, Page):\n        template = 'spirit/utils/paginator/_paginator.html'\n    else:\n        template = 'spirit/utils/paginator/_yt_paginator.html'\n    return render_to_string(template, new_context)\n", "label": 0}
{"function": "\n\ndef slugize(slug):\n    'Convert a string to a string for an URL.\\n    '\n    assert (name_pattern.match(slug) is not None)\n    slug = slug.lower()\n    for c in (' ', ',', '.', '_'):\n        slug = slug.replace(c, '-')\n    while ('--' in slug):\n        slug = slug.replace('--', '-')\n    slug = slug.strip('-')\n    return slug\n", "label": 0}
{"function": "\n\ndef test_connect_checkout_handler_always_gets_info(self):\n    'test [ticket:3497]'\n    (dbapi, p) = self._queuepool_dbapi_fixture(pool_size=2, max_overflow=2)\n    c1 = p.connect()\n    c2 = p.connect()\n    c1.close()\n    c2.close()\n    dbapi.shutdown(True)\n    bad = p.connect()\n    p._invalidate(bad)\n    bad.close()\n    assert p._invalidate_time\n\n    @event.listens_for(p, 'connect')\n    def connect(conn, conn_rec):\n        conn_rec.info['x'] = True\n\n    @event.listens_for(p, 'checkout')\n    def checkout(conn, conn_rec, conn_f):\n        assert ('x' in conn_rec.info)\n    assert_raises(Exception, p.connect)\n    p._pool.queue = collections.deque([c for c in p._pool.queue if (c.connection is not None)])\n    dbapi.shutdown(False)\n    c = p.connect()\n    c.close()\n", "label": 0}
{"function": "\n\ndef encodeFiles(array):\n    IDfile = readIDfile()\n    dictionary = readDictionaryFile()\n    for thisfile in array:\n        try:\n            input = pickle.load(open(thisfile))\n        except ValueError:\n            logging.warn(((('unable to unpicked ' + thisfile) + '... dangerously just ') + 'skipping, some texts may be lost'))\n            continue\n        except:\n            logging.warn((('Some problem: fix if ' + thisfile) + ' should be unpicklable'))\n            continue\n        for level in input.levels:\n            input.encode(level, IDfile, dictionary)\n", "label": 0}
{"function": "\n\ndef LoadServerCertificate(self, server_certificate=None, ca_certificate=None):\n    'Loads and verifies the server certificate.'\n    try:\n        server_cert = X509.load_cert_string(str(server_certificate))\n        ca_cert = X509.load_cert_string(str(ca_certificate))\n        if (server_cert.verify(ca_cert.get_pubkey()) != 1):\n            self.server_name = None\n            raise IOError('Server cert is invalid.')\n        server_cert_serial = server_cert.get_serial_number()\n        if (server_cert_serial < config_lib.CONFIG['Client.server_serial_number']):\n            raise IOError('Server cert is too old.')\n        elif (server_cert_serial > config_lib.CONFIG['Client.server_serial_number']):\n            logging.info('Server serial number updated to %s', server_cert_serial)\n            config_lib.CONFIG.Set('Client.server_serial_number', server_cert_serial)\n            config_lib.CONFIG.Write()\n    except X509.X509Error:\n        raise IOError('Server cert is invalid.')\n    self.server_name = self.pub_key_cache.GetCNFromCert(server_cert)\n    self.server_certificate = server_certificate\n    self.ca_certificate = ca_certificate\n    self.pub_key_cache.Put(self.server_name, self.pub_key_cache.PubKeyFromCert(server_cert))\n", "label": 0}
{"function": "\n\ndef print_ctypes_struct(struct, name='', ident=0, hexa=False):\n    if isinstance(struct, _ctypes._Pointer):\n        if (ctypes.cast(struct, ctypes.c_void_p).value is None):\n            print('{0} -> NULL'.format(name))\n            return\n        return print_ctypes_struct(struct[0], (name + '<deref>'), hexa=hexa)\n    if (not hasattr(struct, '_fields_')):\n        value = struct\n        if hasattr(struct, 'value'):\n            value = struct.value\n        if isinstance(value, basestring):\n            value = repr(value)\n        if hexa:\n            try:\n                print('{0} -> {1}'.format(name, hex(value)))\n                return\n            except TypeError:\n                pass\n        print('{0} -> {1}'.format(name, value))\n        return\n    for (fname, ftype) in struct._fields_:\n        value = getattr(struct, fname)\n        print_ctypes_struct(value, '{0}.{1}'.format(name, fname), hexa=hexa)\n", "label": 1}
{"function": "\n\ndef test_column_optimizations_with_bcolz_and_rewrite():\n    try:\n        import bcolz\n    except ImportError:\n        return\n    bc = bcolz.ctable([[1, 2, 3], [10, 20, 30]], names=['a', 'b'])\n    func = (lambda x: x)\n    for cols in [None, 'abc', ['abc']]:\n        dsk2 = merge(dict(((('x', i), (dataframe_from_ctable, bc, slice(0, 2), cols, {\n            \n        })) for i in [1, 2, 3])), dict(((('y', i), (getitem, ('x', i), (list, ['a', 'b']))) for i in [1, 2, 3])))\n        expected = dict(((('y', i), (dataframe_from_ctable, bc, slice(0, 2), (list, ['a', 'b']), {\n            \n        })) for i in [1, 2, 3]))\n        result = dd.optimize(dsk2, [('y', i) for i in [1, 2, 3]])\n        assert (result == expected)\n", "label": 1}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser(description='Validate and reformat JSONfiles.')\n    parser.add_argument('files', metavar='FILES', nargs='+')\n    parser.add_argument('-f', '--formatting', choices=['check', 'fix'], help='check or fix formatting of JSON files')\n    args = parser.parse_args()\n    exit_status = 0\n    for path in args.files:\n        try:\n            _process_file(path, args.formatting, verbose=True)\n        except ValueError as err:\n            print(('%s\\n%s' % (path, _indent_note(str(err)))))\n            exit_status = 1\n    return exit_status\n", "label": 0}
{"function": "\n\n@pytest.mark.browser\ndef test_can_be_added_with_new_image_and_link(admin_user, live_server, browser):\n    fancypage = factories.FancyPageFactory()\n    im = Image.new('RGB', (320, 240), 'red')\n    (__, filename) = tempfile.mkstemp(suffix='.jpg', dir=TEMP_MEDIA_ROOT)\n    im.save(filename, 'JPEG')\n    image_filename = os.path.basename(filename)\n    ImageAsset.objects.create(name='test image', image=image_filename, creator=admin_user)\n    second_page = factories.FancyPageFactory(node__name='Another page', node__slug='another-page')\n    browser.visit((live_server.url + fancypage.get_absolute_url()))\n    find_and_click_by_css(browser, '#editor-handle')\n    find_and_click_by_css(browser, 'div[class=block-add-control]>a')\n    find_and_click_by_css(browser, \"a[href='#content']\")\n    find_and_click_by_css(browser, 'button[data-block-code=image]')\n    wait_for_editor_reload()\n    default_image_text = 'Add An Image'\n    if (not browser.is_text_present(default_image_text, 2)):\n        raise AssertionError('Could not find image block on page')\n    if (not browser.is_element_present_by_css('.edit-button', 2)):\n        raise AssertionError('Could not find edit button for block')\n    find_and_click_by_css(browser, '.edit-button')\n    wait_for_editor_reload()\n    find_and_click_by_css(browser, 'a[data-behaviours=load-asset-modal]')\n    wait_for_editor_reload()\n    with browser.get_iframe(0) as iframe:\n        wait_for_editor_reload()\n        find_and_click_by_css(iframe, 'li[data-behaviours=selectable-asset]')\n    wait_for_editor_reload()\n    find_and_click_by_css(browser, '.glyphicon-share')\n    find_and_click_by_css(browser, 'a[href*={}]'.format(second_page.slug))\n    find_and_click_by_css(browser, 'button[type=submit]')\n    wait_for_editor_reload()\n    shows_image = browser.is_element_present_by_css(\"img[src$='{}']\".format(image_filename))\n    if (not shows_image):\n        raise AssertionError('Image not added to image block')\n    link_exists = browser.is_element_present_by_css('div.block-wrapper>a[href*={}]'.format(second_page.slug))\n    if (not link_exists):\n        raise AssertionError('image block is not wrapped in link to other page')\n", "label": 0}
{"function": "\n\ndef _query_metadata_proxy(self, machine):\n    url = ('http://%(host)s:%(port)s' % {\n        'host': dhcp.METADATA_DEFAULT_IP,\n        'port': dhcp.METADATA_PORT,\n    })\n    cmd = ('curl', '--max-time', METADATA_REQUEST_TIMEOUT, '-D-', url)\n    i = 0\n    CONNECTION_REFUSED_TIMEOUT = (METADATA_REQUEST_TIMEOUT // 2)\n    while (i <= CONNECTION_REFUSED_TIMEOUT):\n        try:\n            raw_headers = machine.execute(cmd)\n            break\n        except RuntimeError as e:\n            if ('Connection refused' in str(e)):\n                time.sleep(METADATA_REQUEST_SLEEP)\n                i += METADATA_REQUEST_SLEEP\n            else:\n                self.fail(('metadata proxy unreachable on %s before timeout' % url))\n    if (i > CONNECTION_REFUSED_TIMEOUT):\n        self.fail('Timed out waiting metadata proxy to become available')\n    return raw_headers.splitlines()[0]\n", "label": 0}
{"function": "\n\ndef update(self, value, timestamp=None):\n    if (timestamp is None):\n        timestamp = now()\n    self.rescale_if_necessary()\n    with self.lock:\n        try:\n            priority = (self.weight(timestamp) / random.random())\n        except (OverflowError, ZeroDivisionError):\n            priority = sys.float_info.max\n        if (len(self.values) < self.reservoir_size):\n            heapq.heappush(self.values, (priority, value))\n        else:\n            heapq.heappushpop(self.values, (priority, value))\n", "label": 0}
{"function": "\n\ndef get(self, resource):\n    '\\n        Get a resource into the cache,\\n\\n        :param resource: A :class:`Resource` instance.\\n        :return: The pathname of the resource in the cache.\\n        '\n    (prefix, path) = resource.finder.get_cache_info(resource)\n    if (prefix is None):\n        result = path\n    else:\n        result = os.path.join(self.base, self.prefix_to_dir(prefix), path)\n        dirname = os.path.dirname(result)\n        if (not os.path.isdir(dirname)):\n            os.makedirs(dirname)\n        if (not os.path.exists(result)):\n            stale = True\n        else:\n            stale = self.is_stale(resource, path)\n        if stale:\n            with open(result, 'wb') as f:\n                f.write(resource.bytes)\n    return result\n", "label": 0}
{"function": "\n\ndef _digest(self, alg, password, salt=None):\n    \"\\n        Helper method to perform the password digest.\\n\\n        :param alg: The hash algorithm to use.\\n        :type alg: str - 'sha512' | 'bcrypt'\\n        :param password: The password to digest.\\n        :type password: str\\n        :param salt: The salt to use. In the case of bcrypt,\\n                     when storing the password, pass None;\\n                     when testing the password, pass the hashed value.\\n        :type salt: None or str\\n        :returns: The hashed value as a string.\\n        \"\n    cur_config = config.getConfig()\n    if (alg == 'sha512'):\n        return hashlib.sha512((password + salt).encode('utf8')).hexdigest()\n    elif (alg == 'bcrypt'):\n        try:\n            import bcrypt\n        except ImportError:\n            raise Exception('Bcrypt module is not installed. See girder.local.cfg.')\n        password = password.encode('utf8')\n        if (salt is None):\n            rounds = int(cur_config['auth']['bcrypt_rounds'])\n            return bcrypt.hashpw(password, bcrypt.gensalt(rounds))\n        else:\n            if isinstance(salt, six.text_type):\n                salt = salt.encode('utf8')\n            return bcrypt.hashpw(password, salt)\n    else:\n        raise Exception(('Unsupported hash algorithm: %s' % alg))\n", "label": 0}
{"function": "\n\ndef get_image_parent(image_id):\n    if (image_id in images_cache):\n        return images_cache[image_id]\n    image_json = store.image_json_path(image_id)\n    parent_id = None\n    try:\n        info = store.get_json(image_json)\n        if (info['id'] != image_id):\n            warning(('image_id != json image_id for image_id: ' + image_id))\n        parent_id = info.get('parent')\n    except exceptions.FileNotFoundError:\n        warning('graph is broken for image_id: {0}'.format(image_id))\n    images_cache[image_id] = parent_id\n    return parent_id\n", "label": 0}
{"function": "\n\ndef read_data(self, offset, length):\n    end = (offset + length)\n    eof = self['size']\n    if (end > eof):\n        end = eof\n        length = (end - offset)\n        if (length <= 0):\n            return ''\n    first_block = (offset / self.blocksize)\n    last_block = (end / self.blocksize)\n    output = StringIO()\n    for n_block in range(first_block, (last_block + 1)):\n        block_offset = (n_block * self.blocksize)\n        fragment_offset = 0\n        if (n_block == first_block):\n            fragment_offset = (offset - block_offset)\n        fragment_end = self.blocksize\n        if (n_block == last_block):\n            fragment_end = (end - block_offset)\n        block_data = self.read_block(n_block)\n        fragment = block_data[fragment_offset:fragment_end]\n        assert (len(fragment) == (fragment_end - fragment_offset))\n        output.write(fragment)\n    output = output.getvalue()\n    assert (len(output) == length)\n    return output\n", "label": 1}
{"function": "\n\ndef test_as_image(self):\n    filename = 'testnet.png'\n    self.net.as_image(filename=filename)\n    file_exists = (filename in os.listdir('.'))\n    if file_exists:\n        os.remove(('./' + filename))\n    assert file_exists, 'Can create image file.'\n", "label": 0}
{"function": "\n\n@classmethod\ndef create_user(cls, name, email, password, email_verified=True):\n    'Create (and save) a new user with the given password and\\n        email address.\\n        '\n    now = datetime.datetime.utcnow()\n    try:\n        (email_name, domain_part) = email.strip().split('@', 1)\n    except ValueError:\n        pass\n    else:\n        email = '@'.join([email_name.lower(), domain_part.lower()])\n    user = User(name=name, email=email, date_joined=now)\n    if (not password):\n        password = generate_password()\n    user.set_password(password)\n    if (not email_verified):\n        user.mark_email_for_activation()\n    else:\n        user.is_email_activated = True\n    user.save()\n    return user\n", "label": 0}
{"function": "\n\ndef test_set_reuse_addr(self):\n    if (HAS_UNIX_SOCKETS and (self.family == socket.AF_UNIX)):\n        self.skipTest('Not applicable to AF_UNIX sockets.')\n    sock = socket.socket(self.family)\n    try:\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    except socket.error:\n        unittest.skip('SO_REUSEADDR not supported on this platform')\n    else:\n        s = asyncore.dispatcher(socket.socket(self.family))\n        self.assertFalse(s.socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR))\n        s.socket.close()\n        s.create_socket(self.family)\n        s.set_reuse_addr()\n        self.assertTrue(s.socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR))\n    finally:\n        sock.close()\n", "label": 0}
{"function": "\n\ndef test_login_logout_save():\n    (auth, app, user) = _get_flask_app()\n    auth.session = Session()\n    client = app.test_client()\n    auth.session.saved = 0\n    resp = client.get('/protected/')\n    print(resp.data)\n    assert (auth.session_key not in auth.session)\n    assert (auth.redirect_key in auth.session)\n    assert (auth.session.saved == 1)\n    data = {\n        'login': user.login,\n        'password': 'foobar',\n        '_csrf_token': auth.get_csrf_token(),\n    }\n    client.post(auth.url_sign_in, data=data)\n    assert (auth.session_key in auth.session)\n    assert (auth.session.saved == 4)\n    data = {\n        '_csrf_token': auth.get_csrf_token(),\n    }\n    client.post(auth.url_sign_out, data=data)\n    assert (auth.session_key not in auth.session)\n    assert (auth.session.saved == 5)\n", "label": 1}
{"function": "\n\ndef AllowPort(self, vm, port):\n    'Opens a port on the firewall.\\n\\n    Args:\\n      vm: The BaseVirtualMachine object to open the port for.\\n      port: The local port to open.\\n    '\n    if vm.is_static:\n        return\n    entry = (port, vm.group_id)\n    if (entry in self.firewall_set):\n        return\n    with self._lock:\n        if (entry in self.firewall_set):\n            return\n        authorize_cmd = (util.AWS_PREFIX + ['ec2', 'authorize-security-group-ingress', ('--region=%s' % vm.region), ('--group-id=%s' % vm.group_id), ('--port=%s' % port), '--cidr=0.0.0.0/0'])\n        util.IssueRetryableCommand((authorize_cmd + ['--protocol=tcp']))\n        util.IssueRetryableCommand((authorize_cmd + ['--protocol=udp']))\n        self.firewall_set.add(entry)\n", "label": 0}
{"function": "\n\ndef test_simple():\n    for (simple_type, expected) in [('int', lib._CFFI_PRIM_INT), ('signed int', lib._CFFI_PRIM_INT), ('  long  ', lib._CFFI_PRIM_LONG), ('long int', lib._CFFI_PRIM_LONG), ('unsigned short', lib._CFFI_PRIM_USHORT), ('long double', lib._CFFI_PRIM_LONGDOUBLE)]:\n        assert (parse(simple_type) == ['->', Prim(expected)])\n", "label": 0}
{"function": "\n\ndef test_help_list(self, capfd):\n    'Test if list help text gets printed'\n    (out, err) = run_ofSM('list --help', capfd=capfd)\n    assert out.startswith('usage: ofStateManager.py list [-h]')\n    assert (err == '')\n", "label": 0}
{"function": "\n\ndef hgopen(args, cwd=None):\n    'Call Git with arguments.'\n    returncode = None\n    output = None\n    cmd = ([_hg_path] + args)\n    env = environ.copy()\n    env['LC_ALL'] = 'en_US'\n    if (_PLATFORM == 'windows'):\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        process = subprocess.Popen(cmd, startupinfo=startupinfo, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, stdin=subprocess.PIPE, cwd=cwd, shell=False, env=env)\n    else:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, stdin=subprocess.PIPE, cwd=cwd, shell=False, env=env)\n    output = process.communicate()\n    returncode = process.returncode\n    assert (returncode == 0), ('Runtime Error: %s\\n%s' % (output[0].rstrip(), str(cmd)))\n    return output[0]\n", "label": 0}
{"function": "\n\ndef _ensure_path(self):\n    result = self.client.ensure_path(self.path)\n    self.assured_path = True\n    if (result is True):\n        (data, _) = self.client.get(self.path)\n        try:\n            leases = int(data.decode('utf-8'))\n        except (ValueError, TypeError):\n            pass\n        else:\n            if (leases != self.max_leases):\n                raise ValueError(('Inconsistent max leases: %s, expected: %s' % (leases, self.max_leases)))\n    else:\n        self.client.set(self.path, str(self.max_leases).encode('utf-8'))\n", "label": 0}
{"function": "\n\ndef test_eval_size_zero(self, TrainSplit, nn):\n    (X, y) = (np.random.random((100, 10)), np.repeat([0, 1, 2, 3], 25))\n    (X_train, X_valid, y_train, y_valid) = TrainSplit(0.0)(X, y, nn)\n    assert (len(X_train) == len(X))\n    assert (len(y_train) == len(y))\n    assert (len(X_valid) == 0)\n    assert (len(y_valid) == 0)\n", "label": 0}
{"function": "\n\ndef test_last(self):\n    expected = (False, False, True)\n    actual = tuple((self.ctx.last for i in self.ctx))\n    assert (expected == actual), 'last is only true on the last iteration'\n", "label": 0}
{"function": "\n\ndef process_clean(self, seqid, iprot, oprot):\n    args = clean_args()\n    args.read(iprot)\n    iprot.readMessageEnd()\n    result = clean_result()\n    try:\n        self._handler.clean(args.log_context)\n        msg_type = TMessageType.REPLY\n    except (TTransport.TTransportException, KeyboardInterrupt, SystemExit):\n        raise\n    except Exception as ex:\n        msg_type = TMessageType.EXCEPTION\n        logging.exception(ex)\n        result = TApplicationException(TApplicationException.INTERNAL_ERROR, 'Internal error')\n    oprot.writeMessageBegin('clean', msg_type, seqid)\n    result.write(oprot)\n    oprot.writeMessageEnd()\n    oprot.trans.flush()\n", "label": 0}
{"function": "\n\ndef to_python(self, value):\n    try:\n        if (value is not None):\n            return json.loads(value)\n    except TypeError:\n        raise ValidationError(_('String type is required.'))\n    except ValueError:\n        raise ValidationError(_('Enter a valid value.'))\n", "label": 0}
{"function": "\n\ndef compile_template(func):\n    spec = inspect.getargspec(func)\n    assert (len(spec.args) == len((spec.defaults or []))), 'All template args should have AST classes'\n    compiler = TemplateCompiler(zipdict(spec.args, (spec.defaults or [])))\n    template = map(compiler.visit, get_body_ast(func))\n    if ((len(template) == 1) and isinstance(template[0], ast.Expr)):\n        return template[0].value\n    return template\n", "label": 0}
{"function": "\n\ndef match_or_trust(self, host, der_encoded_certificate):\n    base64_encoded_certificate = b64encode(der_encoded_certificate)\n    if isfile(self.path):\n        with open(self.path) as f_in:\n            for line in f_in:\n                (known_host, _, known_cert) = line.strip().partition(':')\n                known_cert = known_cert.encode('utf-8')\n                if (host == known_host):\n                    return (base64_encoded_certificate == known_cert)\n    try:\n        makedirs(dirname(self.path))\n    except OSError:\n        pass\n    f_out = os_open(self.path, ((O_CREAT | O_APPEND) | O_WRONLY), 384)\n    if isinstance(host, bytes):\n        os_write(f_out, host)\n    else:\n        os_write(f_out, host.encode('utf-8'))\n    os_write(f_out, b':')\n    os_write(f_out, base64_encoded_certificate)\n    os_write(f_out, b'\\n')\n    os_close(f_out)\n    return True\n", "label": 0}
{"function": "\n\ndef process_request(self, req):\n    req.perm.assert_permission('TICKET_BATCH_MODIFY')\n    comment = req.args.get('batchmod_value_comment', '')\n    action = req.args.get('action')\n    try:\n        new_values = self._get_new_ticket_values(req)\n    except TracError as e:\n        new_values = None\n        add_warning(req, tag_('The changes could not be saved: %(message)s', message=to_unicode(e)))\n    if (new_values is not None):\n        selected_tickets = self._get_selected_tickets(req)\n        self._save_ticket_changes(req, selected_tickets, new_values, comment, action)\n    req.redirect(req.session['query_href'])\n", "label": 0}
{"function": "\n\ndef get_fqhostname():\n    '\\n    Returns the fully qualified hostname\\n    '\n    l = []\n    l.append(socket.getfqdn())\n    try:\n        addrinfo = socket.getaddrinfo(socket.gethostname(), 0, socket.AF_UNSPEC, socket.SOCK_STREAM, socket.SOL_TCP, socket.AI_CANONNAME)\n        for info in addrinfo:\n            if (len(info) >= 4):\n                l.append(info[3])\n    except socket.gaierror:\n        pass\n    l = _sort_hostnames(l)\n    if (len(l) > 0):\n        return l[0]\n    return None\n", "label": 0}
{"function": "\n\ndef _plot_html(figure_or_data, show_link, link_text, validate, default_width, default_height, global_requirejs):\n    figure = tools.return_figure_from_figure_or_data(figure_or_data, validate)\n    width = figure.get('layout', {\n        \n    }).get('width', default_width)\n    height = figure.get('layout', {\n        \n    }).get('height', default_height)\n    try:\n        float(width)\n    except (ValueError, TypeError):\n        pass\n    else:\n        width = (str(width) + 'px')\n    try:\n        float(width)\n    except (ValueError, TypeError):\n        pass\n    else:\n        width = (str(width) + 'px')\n    plotdivid = uuid.uuid4()\n    jdata = json.dumps(figure.get('data', []), cls=utils.PlotlyJSONEncoder)\n    jlayout = json.dumps(figure.get('layout', {\n        \n    }), cls=utils.PlotlyJSONEncoder)\n    config = {\n        \n    }\n    config['showLink'] = show_link\n    config['linkText'] = link_text\n    jconfig = json.dumps(config)\n    plotly_platform_url = plotly.plotly.get_config().get('plotly_domain', 'https://plot.ly')\n    if ((plotly_platform_url != 'https://plot.ly') and (link_text == 'Export to plot.ly')):\n        link_domain = plotly_platform_url.replace('https://', '').replace('http://', '')\n        link_text = link_text.replace('plot.ly', link_domain)\n    script = 'Plotly.newPlot(\"{id}\", {data}, {layout}, {config})'.format(id=plotdivid, data=jdata, layout=jlayout, config=jconfig)\n    optional_line1 = ('require([\"plotly\"], function(Plotly) {{ ' if global_requirejs else '')\n    optional_line2 = ('}});' if global_requirejs else '')\n    plotly_html_div = (((((('<div id=\"{id}\" style=\"height: {height}; width: {width};\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">' + optional_line1) + 'window.PLOTLYENV=window.PLOTLYENV || {{}};window.PLOTLYENV.BASE_URL=\"') + plotly_platform_url) + '\";{script}') + optional_line2) + '</script>').format(id=plotdivid, script=script, height=height, width=width)\n    return (plotly_html_div, plotdivid, width, height)\n", "label": 1}
{"function": "\n\ndef _build_doc(self):\n    '\\n        Raises\\n        ------\\n        ValueError\\n            * If a URL that lxml cannot parse is passed.\\n\\n        Exception\\n            * Any other ``Exception`` thrown. For example, trying to parse a\\n              URL that is syntactically correct on a machine with no internet\\n              connection will fail.\\n\\n        See Also\\n        --------\\n        pandas.io.html._HtmlFrameParser._build_doc\\n        '\n    from lxml.html import parse, fromstring, HTMLParser\n    from lxml.etree import XMLSyntaxError\n    parser = HTMLParser(recover=False, encoding=self.encoding)\n    try:\n        r = parse(self.io, parser=parser)\n        try:\n            r = r.getroot()\n        except AttributeError:\n            pass\n    except (UnicodeDecodeError, IOError):\n        if (not _is_url(self.io)):\n            r = fromstring(self.io, parser=parser)\n            try:\n                r = r.getroot()\n            except AttributeError:\n                pass\n        else:\n            scheme = parse_url(self.io).scheme\n            if (scheme not in _valid_schemes):\n                msg = ('%r is not a valid url scheme, valid schemes are %s' % (scheme, _valid_schemes))\n                raise ValueError(msg)\n            else:\n                raise\n    else:\n        if (not hasattr(r, 'text_content')):\n            raise XMLSyntaxError('no text parsed from document', 0, 0, 0)\n    return r\n", "label": 1}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if (not self.pk):\n        try:\n            self.position = (self.siblings(include_self=True).only('position').order_by('-position')[0].position + 1)\n        except IndexError:\n            if self.parent:\n                self.position = (self.parent.position + 1)\n            else:\n                try:\n                    self.position = (Forum.objects.only('position').order_by('-position')[0].position + 1)\n                except IndexError:\n                    self.position = 1\n        if (self.position != 1):\n            Forum.objects.update_position(self.position)\n        if self.parent:\n            self.level = (self.parent.level + 1)\n    super(Forum, self).save(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef fsync_dir(dirpath):\n    '\\n    Sync directory entries to disk.\\n\\n    :param dirpath: Path to the directory to be synced.\\n    '\n    dirfd = None\n    try:\n        dirfd = os.open(dirpath, (os.O_DIRECTORY | os.O_RDONLY))\n        fsync(dirfd)\n    except OSError as err:\n        if (err.errno == errno.ENOTDIR):\n            raise\n        logging.warning(_('Unable to perform fsync() on directory %(dir)s: %(err)s'), {\n            'dir': dirpath,\n            'err': os.strerror(err.errno),\n        })\n    finally:\n        if dirfd:\n            os.close(dirfd)\n", "label": 0}
{"function": "\n\ndef test_model_time_res_uniform(self):\n    override = '\\n            time:\\n                resolution: 24\\n        '\n    model = create_and_run_model(override)\n    assert (len(model.data.time_res_series) == 4)\n    assert (str(model.results.solver.termination_condition) == 'optimal')\n    sol = model.solution\n    assert (sol['e'].loc[dict(c='power', y='ccgt')].sum(dim=['x', 't']) == 1320)\n", "label": 0}
{"function": "\n\ndef download_set(set_id, get_filename, size_label=None):\n    \"\\n    Download the set with 'set_id' to the current directory.\\n\\n    @param set_id: str, id of the photo set\\n    @param get_filename: Function, function that creates a filename for the photo\\n    @param size_label: str|None, size to download (or None for largest available)\\n    \"\n    suffix = (' ({})'.format(size_label) if size_label else '')\n    pset = Flickr.Photoset(id=set_id)\n    photos = pset.getPhotos()\n    pagenum = 2\n    while True:\n        try:\n            page = pset.getPhotos(page=pagenum)\n            photos.extend(page)\n            pagenum += 1\n        except FlickrAPIError as ex:\n            if (ex.code == 1):\n                break\n            raise\n    dirname = pset.title.replace(os.sep, '_')\n    if (not os.path.exists(dirname)):\n        os.mkdir(dirname)\n    for photo in photos:\n        fname = get_full_path(dirname, get_filename(pset, photo, suffix))\n        if os.path.exists(fname):\n            print('Skipping {0}, as it exists already'.format(fname))\n            continue\n        print('Saving: {0}'.format(fname))\n        photo.save(fname, size_label)\n        info = photo.getInfo()\n        taken = parser.parse(info['taken'])\n        taken_unix = time.mktime(taken.timetuple())\n        os.utime(fname, (taken_unix, taken_unix))\n", "label": 1}
{"function": "\n\n@patch('sys.stdout', new_callable=StringIO)\n@patch('paasta_tools.cli.cmds.check.get_pipeline_config')\n@patch('paasta_tools.cli.cmds.check.get_marathon_steps')\ndef test_marathon_deployments_deploy_but_not_marathon(mock_get_marathon_steps, mock_get_pipeline_config, mock_stdout):\n    mock_get_pipeline_config.return_value = [{\n        'instancename': 'itest',\n    }, {\n        'instancename': 'performance-check',\n    }, {\n        'instancename': 'push-to-registry',\n    }, {\n        'instancename': 'hab.canary',\n        'trigger_next_step_manually': True,\n    }, {\n        'instancename': 'hab.main',\n    }, {\n        'instancename': 'hab.EXTRA',\n    }]\n    mock_get_marathon_steps.return_value = ['hab.canary', 'hab.main']\n    actual = deployments_check(service='fake_service', soa_dir='/fake/service')\n    assert (actual is False)\n    assert ('EXTRA' in mock_stdout.getvalue())\n", "label": 0}
{"function": "\n\ndef test_multivariate_log_rank_is_identital_to_log_rank_for_n_equals_2():\n    N = 200\n    T1 = np.random.exponential(5, size=N)\n    T2 = np.random.exponential(5, size=N)\n    C1 = np.random.binomial(2, 0.9, size=N)\n    C2 = np.random.binomial(2, 0.9, size=N)\n    result = stats.logrank_test(T1, T2, C1, C2, alpha=0.95)\n    T = np.r_[(T1, T2)]\n    C = np.r_[(C1, C2)]\n    G = np.array((([1] * 200) + ([2] * 200)))\n    result_m = stats.multivariate_logrank_test(T, G, C, alpha=0.95)\n    assert (result.is_significant == result_m.is_significant)\n    assert (result.p_value == result_m.p_value)\n", "label": 0}
{"function": "\n\ndef refreshTextboxes(self, reset=False):\n    ' Update the information of the textboxes that give information about the measurements\\n        '\n    self.aortaTextBox.setText('0')\n    self.paTextBox.setText('0')\n    self.ratioTextBox.setText('0')\n    self.ratioTextBox.setStyleSheet(' QLineEdit { background-color: white; color: black}')\n    volumeId = self.volumeSelector.currentNodeId\n    if (volumeId not in self.logic.currentVolumesLoaded):\n        return\n    if volumeId:\n        self.logic.changeColor(volumeId, self.logic.defaultColor)\n    aorta = None\n    pa = None\n    if (not reset):\n        (rulerAorta, newAorta) = self.logic.getRulerNodeForVolumeAndStructure(self.volumeSelector.currentNodeId, self.logic.AORTA, createIfNotExist=False)\n        (rulerPA, newPA) = self.logic.getRulerNodeForVolumeAndStructure(self.volumeSelector.currentNodeId, self.logic.PA, createIfNotExist=False)\n        if rulerAorta:\n            aorta = rulerAorta.GetDistanceMeasurement()\n            self.aortaTextBox.setText(str(aorta))\n        if rulerPA:\n            pa = rulerPA.GetDistanceMeasurement()\n            self.paTextBox.setText(str(pa))\n        if ((aorta is not None) and (aorta != 0)):\n            try:\n                ratio = (pa / aorta)\n                self.ratioTextBox.setText(str(ratio))\n                if (ratio > 1):\n                    self.ratioTextBox.setStyleSheet(' QLineEdit { background-color: rgb(255, 0, 0); color: white}')\n                    self.logic.changeColor(volumeId, self.logic.defaultWarningColor)\n            except Exception:\n                Util.print_last_exception()\n", "label": 1}
{"function": "\n\ndef check_pid(pidfile):\n    if (pidfile and os.path.exists(pidfile)):\n        try:\n            pid = int(open(pidfile).read().strip())\n            os.kill(pid, 0)\n            return pid\n        except BaseException:\n            return 0\n    return 0\n", "label": 0}
{"function": "\n\n@skipUnlessDBFeature('has_SnapToGrid_function')\ndef test_snap_to_grid(self):\n    for bad_args in ((), range(3), range(5)):\n        with self.assertRaises(ValueError):\n            Country.objects.annotate(snap=functions.SnapToGrid('mpoly', *bad_args))\n    for bad_args in (('1.0',), (1.0, None), tuple(map(six.text_type, range(4)))):\n        with self.assertRaises(TypeError):\n            Country.objects.annotate(snap=functions.SnapToGrid('mpoly', *bad_args))\n    wkt = 'MULTIPOLYGON(((12.41580 43.95795,12.45055 43.97972,12.45389 43.98167,12.46250 43.98472,12.47167 43.98694,12.49278 43.98917,12.50555 43.98861,12.51000 43.98694,12.51028 43.98277,12.51167 43.94333,12.51056 43.93916,12.49639 43.92333,12.49500 43.91472,12.48778 43.90583,12.47444 43.89722,12.46472 43.89555,12.45917 43.89611,12.41639 43.90472,12.41222 43.90610,12.40782 43.91366,12.40389 43.92667,12.40500 43.94833,12.40889 43.95499,12.41580 43.95795)))'\n    Country.objects.create(name='San Marino', mpoly=fromstr(wkt))\n    tol = 1e-09\n    ref = fromstr('MULTIPOLYGON(((12.4 44,12.5 44,12.5 43.9,12.4 43.9,12.4 44)))')\n    self.assertTrue(ref.equals_exact(Country.objects.annotate(snap=functions.SnapToGrid('mpoly', 0.1)).get(name='San Marino').snap, tol))\n    ref = fromstr('MULTIPOLYGON(((12.4 43.93,12.45 43.93,12.5 43.93,12.45 43.93,12.4 43.93)))')\n    self.assertTrue(ref.equals_exact(Country.objects.annotate(snap=functions.SnapToGrid('mpoly', 0.05, 0.23)).get(name='San Marino').snap, tol))\n    ref = fromstr('MULTIPOLYGON(((12.4 43.87,12.45 43.87,12.45 44.1,12.5 44.1,12.5 43.87,12.45 43.87,12.4 43.87)))')\n    self.assertTrue(ref.equals_exact(Country.objects.annotate(snap=functions.SnapToGrid('mpoly', 0.05, 0.23, 0.5, 0.17)).get(name='San Marino').snap, tol))\n", "label": 0}
{"function": "\n\n@permission_required('core.manage_shop')\ndef send_rating_mails(request):\n    'Send rating mails for given orders.\\n    '\n    if (request.method == 'POST'):\n        ctype = ContentType.objects.get_for_model(Product)\n        site = ('http://%s' % Site.objects.get(id=settings.SITE_ID))\n        shop = lfs.core.utils.get_default_shop()\n        subject = _('Please rate your products on ')\n        subject += shop.name\n        from_email = shop.from_email\n        orders_sent = []\n        for order in lfs.marketing.utils.get_orders():\n            try:\n                OrderRatingMail.objects.get(order=order)\n            except OrderRatingMail.DoesNotExist:\n                pass\n            else:\n                continue\n            orders_sent.append(order)\n            if request.POST.get('test'):\n                to = shop.get_notification_emails()\n                bcc = []\n            else:\n                to = [order.customer_email]\n                if request.POST.get('bcc'):\n                    bcc = shop.get_notification_emails()\n                else:\n                    bcc = []\n                OrderRatingMail.objects.create(order=order)\n            text = render_to_string('lfs/reviews/rating_mail.txt', {\n                'order': order,\n                'content_type_id': ctype.id,\n                'site': site,\n            })\n            mail = EmailMultiAlternatives(subject=subject, body=text, from_email=from_email, to=to, bcc=bcc)\n            order_items = []\n            for order_item in order.items.all():\n                product = order_item.product\n                if product.is_variant():\n                    product = product.parent\n                order_items.append({\n                    'product_id': product.id,\n                    'product_name': product.name,\n                })\n            html = render_to_string('lfs/reviews/rating_mail.html', {\n                'order': order,\n                'order_items': order_items,\n                'content_type_id': ctype.id,\n                'site': site,\n            })\n            mail.attach_alternative(html, 'text/html')\n            mail.send()\n        return render_to_response('manage/marketing/rating_mails.html', RequestContext(request, {\n            'display_orders_sent': True,\n            'orders_sent': orders_sent,\n        }))\n", "label": 1}
{"function": "\n\ndef test_flow_control_shrinks_in_response_to_settings(self, frame_factory):\n    '\\n        Acknowledging SETTINGS_INITIAL_WINDOW_SIZE shrinks the flow control\\n        window.\\n        '\n    c = h2.connection.H2Connection()\n    c.send_headers(1, self.example_request_headers)\n    assert (c.local_flow_control_window(1) == 65535)\n    f = frame_factory.build_settings_frame(settings={\n        h2.settings.INITIAL_WINDOW_SIZE: 1280,\n    })\n    c.receive_data(f.serialize())\n    assert (c.local_flow_control_window(1) == 1280)\n", "label": 0}
{"function": "\n\ndef __init__(self, bits, **kwds):\n    assert isinstance(bits, int)\n    assert ((bits > 0) and ((bits % 4) == 0))\n    super(BoundedLongIntegerProperty, self).__init__(**kwds)\n    self._bits = bits\n", "label": 0}
{"function": "\n\ndef _EncodeString(self, string):\n    'Encodes a string in the preferred encoding.\\n\\n    Returns:\\n      A byte string containing the encoded string.\\n    '\n    try:\n        encoded_string = string.encode(self._preferred_encoding, errors=self._encode_errors)\n    except UnicodeEncodeError:\n        if (self._encode_errors == 'strict'):\n            logging.error('Unable to properly write output due to encoding error. Switching to error tolerant encoding which can result in non Basic Latin (C0) characters to be replaced with \"?\" or \"\\\\ufffd\".')\n            self._encode_errors = 'replace'\n        encoded_string = string.encode(self._preferred_encoding, errors=self._encode_errors)\n    return encoded_string\n", "label": 0}
{"function": "\n\ndef run_tests():\n    for each_size in testlib.sizes:\n        print(('Testing with %s graphs' % each_size))\n        suite = unittest.TestSuite()\n        testlib.use_size = each_size\n        for each_module in test_modules():\n            try:\n                suite.addTests(unittest.TestLoader().loadTestsFromName(each_module))\n            except ImportError as ie:\n                log.exception(ie)\n                continue\n        tr = unittest.TextTestRunner(verbosity=2)\n        result = tr.run(suite)\n        del suite\n", "label": 0}
{"function": "\n\ndef show_timezone_quickpanel(callback, selected_item):\n    global s\n    show_quick_panel = sublime.active_window().show_quick_panel\n    if ST2:\n        show_quick_panel(pytz.all_timezones, callback)\n    else:\n        try:\n            selected_index = pytz.all_timezones.index(selected_item)\n        except ValueError:\n            selected_index = 0\n        show_quick_panel(pytz.all_timezones, callback, selected_index=selected_index)\n", "label": 0}
{"function": "\n\ndef _validate_python(self, value, state):\n    try:\n        if (len(value) < self.minLength):\n            raise Invalid(self.message('tooShort', state, minLength=self.minLength), value, state)\n    except TypeError:\n        raise Invalid(self.message('invalid', state), value, state)\n", "label": 0}
{"function": "\n\ndef test_create_different_index_multiple_times_same_bin(self):\n    '\\n            Invoke createindex() with multiple times on same bin with different\\nname\\n        '\n    policy = {\n        \n    }\n    retobj = TestIndex.client.index_integer_create('test', 'demo', 'age', 'age_index', policy)\n    if (retobj == 0):\n        retobj = TestIndex.client.index_integer_create('test', 'demo', 'age', 'age_index1', policy)\n        assert (retobj == 0)\n        TestIndex.client.index_remove('test', 'age_index', policy)\n        TestIndex.client.index_remove('test', 'age_index1', policy)\n    else:\n        assert (True is False)\n", "label": 0}
{"function": "\n\ndef unpack_args(self, arg, kwarg_name, kwargs):\n    try:\n        new_args = kwargs[kwarg_name]\n        if (not isinstance(new_args, list)):\n            new_args = [new_args]\n        for i in new_args:\n            if (not isinstance(i, str)):\n                raise MesonException('html_args values must be strings.')\n    except KeyError:\n        return []\n    if (len(new_args) > 0):\n        return [(arg + '@@'.join(new_args))]\n    return []\n", "label": 0}
{"function": "\n\ndef invokeCli(argv, username='', password='', useApiUrl=False):\n    '\\n    Invoke the Girder Python client CLI with a set of arguments.\\n    '\n    if useApiUrl:\n        apiUrl = ('http://localhost:%s/api/v1' % os.environ['GIRDER_PORT'])\n        argsList = (['girder-client', '--api-url', apiUrl, '--username', username, '--password', password] + list(argv))\n    else:\n        argsList = (['girder-client', '--port', os.environ['GIRDER_PORT'], '--username', username, '--password', password] + list(argv))\n    exitVal = 0\n    with mock.patch.object(sys, 'argv', argsList), mock.patch('sys.exit', side_effect=SysExitException) as exit, captureOutput() as output:\n        try:\n            girder_client.cli.main()\n        except SysExitException:\n            args = exit.mock_calls[0][1]\n            exitVal = (args[0] if len(args) else 0)\n    return {\n        'exitVal': exitVal,\n        'stdout': output[0],\n        'stderr': output[1],\n    }\n", "label": 0}
{"function": "\n\ndef test_variable_access_before_setup(self):\n    prob = Problem(root=ExampleGroup())\n    try:\n        prob['G2.C1.x'] = 5.0\n    except AttributeError as err:\n        msg = \"'unknowns' has not been initialized, setup() must be called before 'G2.C1.x' can be accessed\"\n        self.assertEqual(text_type(err), msg)\n    else:\n        self.fail('Exception expected')\n    try:\n        prob.run()\n    except RuntimeError as err:\n        msg = 'setup() must be called before running the model.'\n        self.assertEqual(text_type(err), msg)\n    else:\n        self.fail('Exception expected')\n", "label": 0}
{"function": "\n\ndef __init__(self, env_spec, hidden_sizes=(32,), state_include_action=True, hidden_nonlinearity=NL.tanh):\n    '\\n        :param env_spec: A spec for the env.\\n        :param hidden_sizes: list of sizes for the fully connected hidden layers\\n        :param hidden_nonlinearity: nonlinearity used for each hidden layer\\n        :return:\\n        '\n    assert isinstance(env_spec.action_space, Discrete)\n    Serializable.quick_init(self, locals())\n    super(CategoricalGRUPolicy, self).__init__(env_spec)\n    assert (len(hidden_sizes) == 1)\n    if state_include_action:\n        input_shape = ((env_spec.observation_space.flat_dim + env_spec.action_space.flat_dim),)\n    else:\n        input_shape = (env_spec.observation_space.flat_dim,)\n    prob_network = GRUNetwork(input_shape=input_shape, output_dim=env_spec.action_space.n, hidden_dim=hidden_sizes[0], hidden_nonlinearity=hidden_nonlinearity, output_nonlinearity=NL.softmax)\n    self._prob_network = prob_network\n    self._state_include_action = state_include_action\n    self._f_step_prob = ext.compile_function([prob_network.step_input_layer.input_var, prob_network.step_prev_hidden_layer.input_var], L.get_output([prob_network.step_output_layer, prob_network.step_hidden_layer]))\n    self._prev_action = None\n    self._prev_hidden = None\n    self._hidden_sizes = hidden_sizes\n    self._dist = RecurrentCategorical()\n    self.reset()\n    LasagnePowered.__init__(self, [prob_network.output_layer])\n", "label": 0}
{"function": "\n\ndef _register(self, dp):\n    assert (dp.id is not None)\n    self.dps[dp.id] = dp\n    if (dp.id not in self.port_state):\n        self.port_state[dp.id] = PortState()\n        for port in dp.ports.values():\n            self.port_state[dp.id].add(port.port_no, port)\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    '\\n        A custom save that publishes or unpublishes the object where\\n        appropriate.\\n\\n        Save with keyword argument obj.save(publish=False) to skip the process.\\n        '\n    from bakery import tasks\n    from django.contrib.contenttypes.models import ContentType\n    if (not kwargs.pop('publish', True)):\n        super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n    else:\n        try:\n            preexisting = self.__class__.objects.get(pk=self.pk)\n        except self.__class__.DoesNotExist:\n            preexisting = None\n        if (not preexisting):\n            if self.get_publication_status():\n                action = 'publish'\n            else:\n                action = None\n        elif ((not self.get_publication_status()) and preexisting.get_publication_status()):\n            action = 'unpublish'\n        elif self.get_publication_status():\n            action = 'publish'\n        else:\n            action = None\n        with transaction.atomic():\n            super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n        ct = ContentType.objects.get_for_model(self.__class__)\n        if (action == 'publish'):\n            tasks.publish_object.delay(ct.pk, self.pk)\n        elif (action == 'unpublish'):\n            tasks.unpublish_object.delay(ct.pk, self.pk)\n", "label": 1}
{"function": "\n\ndef testRunNotebooks(self):\n    notebook_dir = path.join('tests', 'input')\n    for notebook_path in glob(path.join(notebook_dir, '*.ipynb')):\n        notebook_file = path.basename(notebook_path)\n        print(notebook_file)\n        expected_file = path.join('tests', 'expected', notebook_file)\n        notebook = ''\n        with open(notebook_path) as notebook_file:\n            notebook = notebook_file.read()\n        try:\n            notebook = reads(notebook, 3)\n        except (TypeError, NBFormatError):\n            notebook = reads(notebook, 'json')\n        runner = NotebookRunner(notebook, working_dir=notebook_dir)\n        runner.run_notebook(True)\n        expected = ''\n        with open(expected_file) as notebook_file:\n            expected = notebook_file.read()\n        try:\n            expected = reads(expected, 3)\n        except (TypeError, NBFormatError):\n            expected = reads(expected, 'json')\n        self.assert_notebooks_equal(expected, runner.nb)\n", "label": 0}
{"function": "\n\ndef write_image(results, output_filename=None):\n    print(('Gathered %s results that represents a mandelbrot image (using %s chunks that are computed jointly by %s workers).' % (len(results), CHUNK_COUNT, WORKERS)))\n    if (not output_filename):\n        return\n    try:\n        from PIL import Image\n    except ImportError as e:\n        raise RuntimeError(('Pillow is required to write image files: %s' % e))\n    color_max = 0\n    for (_point, color) in results:\n        color_max = max(color, color_max)\n    img = Image.new('L', IMAGE_SIZE, 'black')\n    pixels = img.load()\n    for ((x, y), color) in results:\n        if (color_max == 0):\n            color = 0\n        else:\n            color = int(((float(color) / color_max) * 255.0))\n        pixels[(x, y)] = color\n    img.save(output_filename)\n", "label": 0}
{"function": "\n\ndef get_param_as_date(self, name, format_string='%Y-%m-%d', required=False, store=None):\n    'Return the value of a query string parameter as a date.\\n\\n        Args:\\n            name (str): Parameter name, case-sensitive (e.g., \\'ids\\').\\n            format_string (str): String used to parse the param value into a\\n                date.\\n                Any format recognized by strptime() is supported.\\n                (default ``\"%Y-%m-%d\"``)\\n            required (bool, optional): Set to ``True`` to raise\\n                ``HTTPBadRequest`` instead of returning ``None`` when the\\n                parameter is not found (default ``False``).\\n            store (dict, optional): A ``dict``-like object in which to place\\n                the value of the param, but only if the param is found (default\\n                ``None``).\\n        Returns:\\n            datetime.date: The value of the param if it is found and can be\\n                converted to a ``date`` according to the supplied format\\n                string. If the param is not found, returns ``None`` unless\\n                required is ``True``.\\n\\n        Raises:\\n            HTTPBadRequest: A required param is missing from the request.\\n            HTTPInvalidParam: A transform function raised an instance of\\n                ``ValueError``.\\n        '\n    param_value = self.get_param(name, required=required)\n    if (param_value is None):\n        return None\n    try:\n        date = strptime(param_value, format_string).date()\n    except ValueError:\n        msg = 'The date value does not match the required format'\n        raise HTTPInvalidParam(msg, name)\n    if (store is not None):\n        store[name] = date\n    return date\n", "label": 0}
{"function": "\n\ndef test_selector_auto_multi(self):\n    rule = AutoLineDisableRule()\n    assert rule.regex_match_any('# flake8: disable=E101, E102', ['E101'])\n    assert rule.regex_match_any('# flake8: disable=E101, E102', ['E102'])\n    assert (not rule.regex_match_any('# flake8: disable=E10', ['E102']))\n", "label": 0}
{"function": "\n\ndef Validate(self, value, unused_key=None):\n    'Validates a subnet.'\n    if (value is None):\n        raise validation.MissingAttribute('subnet must be specified')\n    if (not isinstance(value, basestring)):\n        raise validation.ValidationError((\"subnet must be a string, not '%r'\" % type(value)))\n    try:\n        ipaddr.IPNetwork(value)\n    except ValueError:\n        raise validation.ValidationError(('%s is not a valid IPv4 or IPv6 subnet' % value))\n    parts = value.split('/')\n    if ((len(parts) == 2) and (not re.match('^[0-9]+$', parts[1]))):\n        raise validation.ValidationError(('Prefix length of subnet %s must be an integer (quad-dotted masks are not supported)' % value))\n    return value\n", "label": 0}
{"function": "\n\ndef test_initial_run(self):\n\n    class MyComp(Component):\n        x = Float(0.0, iotype='in', low=(- 10), high=10)\n        xx = Float(0.0, iotype='in', low=(- 10), high=10)\n        f_x = Float(iotype='out')\n        y = Float(iotype='out')\n\n        def execute(self):\n            if (self.xx != 1.0):\n                self.raise_exception('Lazy', RuntimeError)\n            self.f_x = (2.0 * self.x)\n            self.y = self.x\n\n    @add_delegate(HasParameters)\n    class SpecialDriver(Driver):\n        implements(IHasParameters)\n\n        def execute(self):\n            self.set_parameters([1.0])\n    top = set_as_top(Assembly())\n    top.add('comp', MyComp())\n    try:\n        from pyopt_driver.pyopt_driver import pyOptDriver\n    except ImportError:\n        raise SkipTest('this test requires pyOpt to be installed')\n    try:\n        top.driver.optimizer = 'CONMIN'\n    except ValueError:\n        raise SkipTest('CONMIN not present on this system')\n    top.driver.title = 'Little Test'\n    optdict = {\n        \n    }\n    top.driver.options = optdict\n    top.driver.pyopt_diff = True\n    top.add('driver', pyOptDriver())\n    top.add('subdriver', SpecialDriver())\n    top.driver.workflow.add('subdriver')\n    top.subdriver.workflow.add('comp')\n    top.subdriver.add_parameter('comp.xx')\n    top.driver.add_parameter('comp.x')\n    top.driver.add_constraint('comp.y > 1.0')\n    top.driver.add_objective('comp.f_x')\n    top.run()\n", "label": 0}
{"function": "\n\ndef test_ssl_dsdtestprovider_badssl_com():\n    total_failed_tests = 0\n    total_passed_tests = 0\n    myfile = open((THIS_DIR + '/fixtures/badssl.yaml'))\n    test_json = yaml.load(myfile)\n    for test in test_json['tests']:\n        test_obj = Charcoal(test=test, host='dsdtestprovider.badssl.com')\n        total_failed_tests += test_obj.failed\n        total_passed_tests += test_obj.passed\n    LOG.debug(total_failed_tests)\n    assert (total_failed_tests > 0)\n", "label": 0}
{"function": "\n\ndef test_gdb(board_id=None):\n    result = GdbTestResult()\n    with MbedBoard.chooseBoard(board_id=board_id) as board:\n        memory_map = board.target.getMemoryMap()\n        ram_regions = [region for region in memory_map if (region.type == 'ram')]\n        ram_region = ram_regions[0]\n        rom_region = memory_map.getBootMemory()\n        target_type = board.getTargetType()\n        binary_file = os.path.join(parentdir, 'binaries', board.getTestBinary())\n        if (board_id is None):\n            board_id = board.getUniqueID()\n        test_clock = 10000000\n        test_port = 3334\n        error_on_invalid_access = True\n        ignore_hw_bkpt_result = (1 if (ram_region.start >= 536870912) else 0)\n        if (target_type == 'nrf51'):\n            test_clock = 1000000\n            error_on_invalid_access = False\n        board.flash.flashBinary(binary_file, rom_region.start)\n        board.uninit(False)\n    test_params = {\n        \n    }\n    test_params['rom_start'] = rom_region.start\n    test_params['rom_length'] = rom_region.length\n    test_params['ram_start'] = ram_region.start\n    test_params['ram_length'] = ram_region.length\n    test_params['invalid_start'] = 4294901760\n    test_params['invalid_length'] = 4096\n    test_params['expect_error_on_invalid_access'] = error_on_invalid_access\n    test_params['ignore_hw_bkpt_result'] = ignore_hw_bkpt_result\n    with open(TEST_PARAM_FILE, 'wb') as f:\n        f.write(json.dumps(test_params))\n    gdb = [PYTHON_GDB, '--command=gdb_script.py']\n    with open('output.txt', 'wb') as f:\n        program = Popen(gdb, stdin=PIPE, stdout=f, stderr=STDOUT)\n        args = [('-p=%i' % test_port), ('-f=%i' % test_clock), ('-b=%s' % board_id)]\n        server = GDBServerTool()\n        server.run(args)\n        program.wait()\n    with open(TEST_RESULT_FILE, 'rb') as f:\n        test_result = json.loads(f.read())\n    if set(TEST_RESULT_KEYS).issubset(test_result):\n        print('----------------Test Results----------------')\n        print(('HW breakpoint count: %s' % test_result['breakpoint_count']))\n        print(('Watchpoint count: %s' % test_result['watchpoint_count']))\n        print(('Average instruction step time: %s' % test_result['step_time_si']))\n        print(('Average single step time: %s' % test_result['step_time_s']))\n        print(('Average over step time: %s' % test_result['step_time_n']))\n        print(('Failure count: %i' % test_result['fail_count']))\n        result.passed = (test_result['fail_count'] == 0)\n    else:\n        result.passed = False\n    os.remove(TEST_RESULT_FILE)\n    os.remove(TEST_PARAM_FILE)\n    return result\n", "label": 0}
{"function": "\n\ndef test_cache(self):\n    mocked_repo = MagicMock()\n    mocked_commit = MagicMock()\n    mocked_repo.lookup_reference().resolve().target = 'head'\n    mocked_repo.walk.return_value = [mocked_commit]\n    mocked_commit.commit_time = 1411135000\n    mocked_commit.hex = '1111111111'\n    cache = CommitCache(mocked_repo)\n    cache.update()\n    cache['2014-09-20'] = Commit(1, 1, '1111111111')\n    assert (sorted(cache.keys()) == ['2014-09-19', '2014-09-20'])\n    asserted_time = datetime.fromtimestamp(mocked_commit.commit_time)\n    asserted_time = '{}-{}-{}'.format(asserted_time.hour, asserted_time.minute, asserted_time.second)\n    assert (repr(cache['2014-09-19']) == ('[%s-1111111111]' % asserted_time))\n    del cache['2014-09-20']\n    for commit_date in cache:\n        assert (commit_date == '2014-09-19')\n    mocked_repo.lookup_reference.has_calls([call('HEAD')])\n    mocked_repo.walk.assert_called_once_with('head', GIT_SORT_TIME)\n    assert (mocked_repo.lookup_reference().resolve.call_count == 2)\n", "label": 0}
{"function": "\n\ndef get_theme(self, matcher_info):\n    if (matcher_info == 'in'):\n        return self.theme\n    else:\n        match = self.local_themes[matcher_info]\n        try:\n            return match['theme']\n        except KeyError:\n            match['theme'] = Theme(theme_config=match['config'], main_theme_config=self.theme_config, **self.theme_kwargs)\n            return match['theme']\n", "label": 0}
{"function": "\n\ndef candidates(self, items, artist, album, va_likely):\n    'Returns a list of AlbumInfo objects for discogs search results\\n        matching an album and artist (if not various).\\n        '\n    if (not self.discogs_client):\n        return\n    if va_likely:\n        query = album\n    else:\n        query = ('%s %s' % (artist, album))\n    try:\n        return self.get_albums(query)\n    except DiscogsAPIError as e:\n        self._log.debug('API Error: {0} (query: {1})', e, query)\n        if (e.status_code == 401):\n            self.reset_auth()\n            return self.candidates(items, artist, album, va_likely)\n        else:\n            return []\n    except CONNECTION_ERRORS:\n        self._log.debug('Connection error in album search', exc_info=True)\n        return []\n", "label": 0}
{"function": "\n\ndef test_record_exchange_result_doesnt_restore_balance_on_success(self):\n    alice = self.make_participant('alice', balance=50, last_paypal_result='')\n    ba = ExchangeRoute.from_network(alice, 'paypal')\n    e_id = record_exchange(self.db, ba, D('-43.98'), D('1.60'), alice, 'pre')\n    assert (alice.balance == D('4.42'))\n    record_exchange_result(self.db, e_id, 'succeeded', None, alice)\n    alice = Participant.from_username('alice')\n    assert (alice.balance == D('4.42'))\n", "label": 0}
{"function": "\n\ndef parse_json_value_from_request(request):\n    \"Parse a JSON value from the body of a twisted HTTP request.\\n\\n    :param request: the twisted HTTP request.\\n    :returns: The JSON value.\\n    :raises\\n        SynapseError if the request body couldn't be decoded as JSON.\\n    \"\n    try:\n        content_bytes = request.content.read()\n    except:\n        raise SynapseError(400, 'Error reading JSON content.')\n    try:\n        content = simplejson.loads(content_bytes)\n    except simplejson.JSONDecodeError:\n        raise SynapseError(400, 'Content not JSON.', errcode=Codes.NOT_JSON)\n    return content\n", "label": 0}
{"function": "\n\ndef to_json(self, user):\n    ret = super(GitHubNodeSettings, self).to_json(user)\n    user_settings = user.get_addon('github')\n    ret.update({\n        'user_has_auth': (user_settings and user_settings.has_auth),\n        'is_registration': self.owner.is_registration,\n    })\n    if (self.user_settings and self.user_settings.has_auth):\n        valid_credentials = False\n        owner = self.user_settings.owner\n        connection = GitHubClient(external_account=self.external_account)\n        valid_credentials = True\n        try:\n            repos = itertools.chain.from_iterable((connection.repos(), connection.my_org_repos()))\n            repo_names = ['{0} / {1}'.format(repo.owner.login, repo.name) for repo in repos]\n        except GitHubError:\n            repo_names = []\n            valid_credentials = False\n        if (owner == user):\n            ret.update({\n                'repo_names': repo_names,\n            })\n        ret.update({\n            'node_has_auth': True,\n            'github_user': (self.user or ''),\n            'github_repo': (self.repo or ''),\n            'github_repo_full_name': ('{0} / {1}'.format(self.user, self.repo) if (self.user and self.repo) else ''),\n            'auth_osf_name': owner.fullname,\n            'auth_osf_url': owner.url,\n            'auth_osf_id': owner._id,\n            'github_user_name': self.external_account.display_name,\n            'github_user_url': self.external_account.profile_url,\n            'is_owner': (owner == user),\n            'valid_credentials': valid_credentials,\n            'addons_url': web_url_for('user_addons'),\n            'files_url': self.owner.web_url_for('collect_file_trees'),\n        })\n    return ret\n", "label": 1}
{"function": "\n\ndef emit(events, stream=None, Dumper=Dumper, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None):\n    '\\n    Emit YAML parsing events into a stream.\\n    If stream is None, return the produced string instead.\\n    '\n    getvalue = None\n    if (stream is None):\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        stream = StringIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break)\n    for event in events:\n        dumper.emit(event)\n    if getvalue:\n        return getvalue()\n", "label": 0}
{"function": "\n\ndef suspend_job(job_execution_id):\n    ctx = context.ctx()\n    job_execution = conductor.job_execution_get(ctx, job_execution_id)\n    if (job_execution.info['status'] not in edp.JOB_STATUSES_SUSPENDIBLE):\n        raise e.SuspendingFailed(_('Suspending operation can not be performed on status: {status}')).format(status=job_execution.info['status'])\n    cluster = conductor.cluster_get(ctx, job_execution.cluster_id)\n    engine = get_job_engine(cluster, job_execution)\n    job_execution = conductor.job_execution_update(ctx, job_execution_id, {\n        'info': {\n            'status': edp.JOB_STATUS_TOBESUSPENDED,\n        },\n    })\n    try:\n        job_info = engine.suspend_job(job_execution)\n    except Exception as ex:\n        job_info = None\n        conductor.job_execution_update(ctx, job_execution_id, {\n            'info': {\n                'status': edp.JOB_STATUS_SUSPEND_FAILED,\n            },\n        })\n        raise e.SuspendingFailed(_('Error during suspending of job execution: {error}')).format(error=ex)\n    if (job_info is not None):\n        job_execution = _write_job_status(job_execution, job_info)\n        LOG.info(_LI('Job execution was suspended successfully'))\n        return job_execution\n    conductor.job_execution_update(ctx, job_execution_id, {\n        'info': {\n            'status': edp.JOB_STATUS_SUSPEND_FAILED,\n        },\n    })\n    raise e.SuspendingFailed(_('Failed to suspend job execution{jid}')).format(jid=job_execution_id)\n", "label": 0}
{"function": "\n\ndef _can_connect(self):\n    connection = httplib.HTTPConnection(self._host, self._port)\n    with contextlib.closing(connection):\n        try:\n            connection.connect()\n        except socket.error:\n            return False\n        else:\n            return True\n", "label": 0}
{"function": "\n\ndef __init__(self, default, *items):\n    DiagramItem.__init__(self, 'g')\n    assert (default < len(items))\n    self.default = default\n    self.items = [wrapString(item) for item in items]\n    self.width = ((ARC_RADIUS * 4) + max((item.width for item in self.items)))\n    self.up = 0\n    self.down = 0\n    self.yAdvance = self.items[self.default].yAdvance\n    for (i, item) in enumerate(self.items):\n        if (i < default):\n            self.up += max(ARC_RADIUS, ((item.up + item.down) + VERTICAL_SEPARATION))\n        elif (i == default):\n            self.up += max(ARC_RADIUS, item.up)\n            self.down += max(ARC_RADIUS, item.down)\n        else:\n            assert (i > default)\n            self.down += max(ARC_RADIUS, ((VERTICAL_SEPARATION + item.up) + item.down))\n    if DEBUG:\n        self.attrs['data-updown'] = '{0} {1}'.format(self.up, self.down)\n        self.attrs['data-type'] = 'choice'\n", "label": 1}
{"function": "\n\n@cached_property\ndef api_version(self):\n    metas = [x for x in self.parsed.findall('.//meta') if (x.get('name', '').lower() == 'api-version')]\n    if metas:\n        try:\n            return int(metas[0].get('value', None))\n        except (TypeError, ValueError):\n            pass\n    return None\n", "label": 0}
{"function": "\n\n@common.check_cells_enabled\ndef create(self, req, body):\n    'Create a child cell entry.'\n    context = req.environ['nova.context']\n    authorize(context)\n    authorize(context, action='create')\n    nova_context.require_admin_context(context)\n    if ('cell' not in body):\n        msg = _('No cell information in request')\n        raise exc.HTTPBadRequest(explanation=msg)\n    cell = body['cell']\n    if ('name' not in cell):\n        msg = _('No cell name in request')\n        raise exc.HTTPBadRequest(explanation=msg)\n    self._validate_cell_name(cell['name'])\n    self._normalize_cell(cell)\n    try:\n        cell = self.cells_rpcapi.cell_create(context, cell)\n    except exception.CellsUpdateUnsupported as e:\n        raise exc.HTTPForbidden(explanation=e.format_message())\n    return dict(cell=_scrub_cell(cell))\n", "label": 0}
{"function": "\n\ndef _dump_json(data):\n    options = getattr(settings, 'JSON_OPTIONS', {\n        \n    })\n    if ('cls' in options):\n        if isinstance(options['cls'], six.string_types):\n            options['cls'] = import_string(options['cls'])\n    else:\n        try:\n            use_django = getattr(settings, 'JSON_USE_DJANGO_SERIALIZER')\n        except AttributeError:\n            use_django = True\n        else:\n            warnings.warn(\"JSON_USE_DJANGO_SERIALIZER is deprecated and will be removed. Please use JSON_OPTIONS['cls'] instead.\", DeprecationWarning)\n        if use_django:\n            options['cls'] = DjangoJSONEncoder\n    return json.dumps(data, **options)\n", "label": 0}
{"function": "\n\n@gen.engine\ndef Transform(self, client, device_photo, callback):\n    from device import Device\n    from user_photo import UserPhoto\n    device_id = device_photo.device_id\n    if (device_id not in MoveDevicePhoto._device_to_user_cache):\n        query_expr = ('device.device_id={t}', {\n            't': device_id,\n        })\n        devices = (yield gen.Task(Device.IndexQuery, client, query_expr, None))\n        assert (len(devices) == 1)\n        MoveDevicePhoto._device_to_user_cache[device_id] = devices[0].user_id\n    user_id = MoveDevicePhoto._device_to_user_cache[device_id]\n    existing = (yield gen.Task(UserPhoto.Query, client, user_id, device_photo.photo_id, None, must_exist=False))\n    if (existing is None):\n        logging.info('Creating user photo for photo %s, device %s, user %s', device_photo.photo_id, device_id, user_id)\n        user_photo = UserPhoto.CreateFromKeywords(photo_id=device_photo.photo_id, user_id=user_id, asset_keys=device_photo.asset_keys)\n    else:\n        logging.info('Photo %s, device %s, user %s already has user photo', device_photo.photo_id, device_id, user_id)\n        user_photo = None\n    if (user_photo is not None):\n        self._LogUpdate(user_photo)\n    if (Version._mutate_items and (user_photo is not None)):\n        (yield gen.Task(user_photo.Update, client))\n    callback(device_photo)\n", "label": 0}
{"function": "\n\n@wrap_exception()\n@wrap_instance_fault\ndef attach_interface(self, context, instance, network_id, port_id, requested_ip):\n    'Use hotplug to add an network adapter to an instance.'\n    bind_host_id = self.driver.network_binding_host_id(context, instance)\n    network_info = self.network_api.allocate_port_for_instance(context, instance, port_id, network_id, requested_ip, bind_host_id=bind_host_id)\n    if (len(network_info) != 1):\n        LOG.error(_LE('allocate_port_for_instance returned %(ports)s ports'), {\n            'ports': len(network_info),\n        })\n        raise exception.InterfaceAttachFailed(instance_uuid=instance.uuid)\n    image_meta = objects.ImageMeta.from_instance(instance)\n    try:\n        self.driver.attach_interface(instance, image_meta, network_info[0])\n    except exception.NovaException as ex:\n        port_id = network_info[0].get('id')\n        LOG.warning(_LW('attach interface failed , try to deallocate port %(port_id)s, reason: %(msg)s'), {\n            'port_id': port_id,\n            'msg': ex,\n        }, instance=instance)\n        try:\n            self.network_api.deallocate_port_for_instance(context, instance, port_id)\n        except Exception:\n            LOG.warning(_LW('deallocate port %(port_id)s failed'), {\n                'port_id': port_id,\n            }, instance=instance)\n        raise exception.InterfaceAttachFailed(instance_uuid=instance.uuid)\n    return network_info[0]\n", "label": 0}
{"function": "\n\ndef delete_mistyped_role():\n    '\\n    Delete \" system_admin\" role which was fat fingered.\\n    '\n    role_name = ' system_admin'\n    assert role_name.startswith(' ')\n    try:\n        role_db = Role.get_by_name(role_name)\n    except:\n        return\n    if (not role_db):\n        return\n    try:\n        Role.delete(role_db)\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef send_exception_to_sentry(self, exc_info):\n    'Send an exception to Sentry if enabled.\\n\\n        :param tuple exc_info: exception information as returned from\\n            :func:`sys.exc_info`\\n\\n        '\n    if (not self.sentry_client):\n        return\n    message = dict(self.active_message)\n    try:\n        duration = (math.ceil((time.time() - self.delivery_time)) * 1000)\n    except TypeError:\n        duration = 0\n    kwargs = {\n        'logger': 'rejected.processs',\n        'modules': self.get_module_data(),\n        'extra': {\n            'consumer_name': self.consumer_name,\n            'connection': self.connection_name,\n            'env': self.strip_uri_passwords(dict(os.environ)),\n            'message': message,\n        },\n        'time_spent': duration,\n    }\n    LOGGER.debug('Sending exception to sentry: %r', kwargs)\n    self.sentry_client.captureException(exc_info, **kwargs)\n", "label": 0}
{"function": "\n\ndef upload_file(self, upload_url, upload_auth_token, file_name, content_length, content_type, content_sha1, file_infos, data_stream):\n    assert (upload_url == upload_auth_token)\n    url_match = re.match('https://upload.example.com/([^/]*)/([^/]*)', upload_url)\n    if (url_match is None):\n        raise BadUploadUrl(upload_url)\n    if (len(self.upload_errors) != 0):\n        raise self.upload_errors.pop(0)\n    (bucket_id, upload_id) = url_match.groups()\n    bucket = self._get_bucket_by_id(bucket_id)\n    response = bucket.upload_file(upload_id, upload_auth_token, file_name, content_length, content_type, content_sha1, file_infos, data_stream)\n    file_id = response['fileId']\n    self.file_id_to_bucket_id[file_id] = bucket_id\n    return response\n", "label": 0}
{"function": "\n\n@public\ndef gcd(f, g=None, *gens, **args):\n    '\\n    Compute GCD of ``f`` and ``g``.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy import gcd\\n    >>> from sympy.abc import x\\n\\n    >>> gcd(x**2 - 1, x**2 - 3*x + 2)\\n    x - 1\\n\\n    '\n    if hasattr(f, '__iter__'):\n        if (g is not None):\n            gens = ((g,) + gens)\n        return gcd_list(f, *gens, **args)\n    elif (g is None):\n        raise TypeError('gcd() takes 2 arguments or a sequence of arguments')\n    options.allowed_flags(args, ['polys'])\n    try:\n        ((F, G), opt) = parallel_poly_from_expr((f, g), *gens, **args)\n    except PolificationFailed as exc:\n        (domain, (a, b)) = construct_domain(exc.exprs)\n        try:\n            return domain.to_sympy(domain.gcd(a, b))\n        except NotImplementedError:\n            raise ComputationFailed('gcd', 2, exc)\n    result = F.gcd(G)\n    if (not opt.polys):\n        return result.as_expr()\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef _showmodule(module_name):\n    module = sys.modules[module_name]\n    if (not hasattr(module, '__file__')):\n        raise ValueError('cannot display module {0} (no __file__)'.format(module_name))\n    if module.__file__.endswith('.py'):\n        fname = module.__file__\n    else:\n        if (not module.__file__.endswith('.pyc')):\n            raise ValueError('cannot display module file {0} for {1}'.format(module.__file__, module_name))\n        fname = module.__file__[:(- 1)]\n    if (not os.path.exists(fname)):\n        raise ValueError('could not find file {0} for {1}'.format(fname, module_name))\n    (leaf_count, branch_count) = _get_samples_by_line(fname)\n    lines = []\n    with open(fname) as f:\n        for (i, line) in enumerate(f):\n            lines.append(((i + 1), cgi.escape(line), leaf_count[(i + 1)], branch_count[(i + 1)]))\n    return lines\n", "label": 0}
{"function": "\n\n@classmethod\ndef parser(cls, buf, offset):\n    (length, event, xid) = struct.unpack_from(ofproto.OFP_FLOW_UPDATE_ABBREV_PACK_STR, buf, offset)\n    assert (cls.cls_flow_update_length == length)\n    assert (cls.cls_flow_update_event == event)\n    return cls(length, event, xid)\n", "label": 0}
{"function": "\n\ndef test_get_magics():\n    kernel = get_kernel()\n    d = Dummy(kernel)\n    line = d.get_magics('line')\n    cell = d.get_magics('cell')\n    assert ('dummy' in line)\n    assert ('spam' in cell)\n    assert ('eggs' in line)\n", "label": 0}
{"function": "\n\ndef test_wrapping_long_options_strings(runner):\n\n    @click.group()\n    def cli():\n        'Top level command\\n        '\n\n    @cli.group()\n    def a_very_long():\n        'Second level\\n        '\n\n    @a_very_long.command()\n    @click.argument('first')\n    @click.argument('second')\n    @click.argument('third')\n    @click.argument('fourth')\n    @click.argument('fifth')\n    @click.argument('sixth')\n    def command():\n        'A command.\\n        '\n    result = runner.invoke(cli, ['a_very_long', 'command', '--help'], terminal_width=54)\n    assert (not result.exception)\n    assert (result.output.splitlines() == ['Usage: cli a_very_long command [OPTIONS] FIRST SECOND', '                               THIRD FOURTH FIFTH', '                               SIXTH', '', '  A command.', '', 'Options:', '  --help  Show this message and exit.'])\n", "label": 0}
{"function": "\n\ndef test_null(self):\n    if _debug:\n        TestInteger._debug('test_null')\n    obj = Null()\n    assert (obj.value == ())\n    with self.assertRaises(TypeError):\n        Null('some string')\n    with self.assertRaises(TypeError):\n        Null(1.0)\n", "label": 0}
{"function": "\n\ndef test0(self):\n    x = zvector()\n    rng = numpy.random.RandomState(23)\n    xval = numpy.asarray(list((numpy.complex(rng.randn(), rng.randn()) for i in xrange(10))))\n    assert numpy.all((xval.real == theano.function([x], real(x))(xval)))\n    assert numpy.all((xval.imag == theano.function([x], imag(x))(xval)))\n", "label": 0}
{"function": "\n\ndef test_responsive_legacy_chart_starts_at_correct_size(output_file_url, selenium):\n    values = dict(apples=[2, 3, 7, 5, 26, 221, 44, 233, 254, 25, 2, 67, 10, 11], oranges=[22, 43, 10, 25, 26, 101, 114, 203, 194, 215, 201, 227, 139, 160])\n    area = Area(values, title='Area Chart', responsive=True)\n    save(area)\n    selenium.set_window_size(width=1000, height=600)\n    selenium.get(output_file_url)\n    canvas = selenium.find_element_by_tag_name('canvas')\n    wait_for_canvas_resize(canvas, selenium)\n    assert (canvas.size['width'] > 900)\n    assert (canvas.size['width'] < 1000)\n", "label": 0}
{"function": "\n\ndef testStatisticExtra(self):\n    timer = StatisticExtra(g=0.1)\n    for i in range(10):\n        with timer.time():\n            Tasklet.sleep(0.1)\n    self.assertEquals(10, timer.count)\n    self.assertAlmostEqual(0.1, timer.avg, places=1)\n    timer = StatisticExtra(g=0.1)\n    for i in range(11):\n        with timer.time():\n            Tasklet.sleep(0.2)\n    self.assertEquals(11, timer.count)\n    self.assertAlmostEqual(0.2, timer.avg, places=1)\n", "label": 0}
{"function": "\n\ndef test_ssl_sha256_badssl_com():\n    total_failed_tests = 0\n    total_passed_tests = 0\n    myfile = open((THIS_DIR + '/fixtures/badssl.yaml'))\n    test_json = yaml.load(myfile)\n    for test in test_json['tests']:\n        test_obj = Charcoal(test=test, host='sha256.badssl.com')\n        total_failed_tests += test_obj.failed\n        total_passed_tests += test_obj.passed\n    LOG.debug(total_failed_tests)\n    assert (total_failed_tests == 0)\n", "label": 0}
{"function": "\n\ndef test_user_and_group_title(self):\n    u_title = 'User Title'\n    g_title = 'Group Title'\n    t = Title(name=u_title)\n    t.save()\n    t.users.add(self.user)\n    t = Title(name=g_title)\n    t.save()\n    t.groups.add(self.group)\n    titles = [k.name for k in karma_titles(self.user)]\n    eq_(2, len(titles))\n    assert (u_title in titles)\n    assert (g_title in titles)\n", "label": 0}
{"function": "\n\ndef run(self):\n    ' Compute current version and update debian version accordingly '\n    version = self.distribution.get_version()\n    if self.build_number:\n        version = ('%s-%s' % (version, self.build_number))\n    try:\n        update_debianization(version)\n    except Exception:\n        import traceback\n        traceback.print_exc()\n        raise\n", "label": 0}
{"function": "\n\n@expose(help='Change directory to site webroot')\ndef cd(self):\n    if (not self.app.pargs.site_name):\n        try:\n            while (not self.app.pargs.site_name):\n                self.app.pargs.site_name = input('Enter site name : ').strip()\n        except IOError as e:\n            Log.error(self, 'Unable to read input, please try again')\n    self.app.pargs.site_name = self.app.pargs.site_name.strip()\n    (ee_domain, ee_www_domain) = ValidateDomain(self.app.pargs.site_name)\n    if (not check_domain_exists(self, ee_domain)):\n        Log.error(self, 'site {0} does not exist'.format(ee_domain))\n    ee_site_webroot = getSiteInfo(self, ee_domain).site_path\n    EEFileUtils.chdir(self, ee_site_webroot)\n    try:\n        subprocess.call(['bash'])\n    except OSError as e:\n        Log.debug(self, '{0}{1}'.format(e.errno, e.strerror))\n        Log.error(self, 'unable to change directory')\n", "label": 0}
{"function": "\n\ndef populate(self):\n    'Populates a new cache.\\n\\n        '\n    if self.exists:\n        raise CacheAlreadyExistsException(('location: %s' % self.cache_uri))\n    self._populate_setup()\n    self.graph.open(self.cache_uri, create=True)\n    with closing(self.graph):\n        with self._download_metadata_archive() as metadata_archive:\n            for fact in self._iter_metadata_triples(metadata_archive):\n                self.graph.add(fact)\n", "label": 0}
{"function": "\n\ndef create_cloud(oname, words, maxsize=120, fontname='Lobster'):\n    'Creates a word cloud (when pytagcloud is installed)\\n\\n    Parameters\\n    ----------\\n    oname : output filename\\n    words : list of (value,str)\\n    maxsize : int, optional\\n        Size of maximum word. The best setting for this parameter will often\\n        require some manual tuning for each input.\\n    fontname : str, optional\\n        Font to use.\\n    '\n    try:\n        from pytagcloud import create_tag_image, make_tags\n    except ImportError:\n        if (not warned_of_error):\n            print('Could not import pytagcloud. Skipping cloud generation')\n        return\n    words = [(w, int((v * 10000))) for (v, w) in words]\n    tags = make_tags(words, maxsize=maxsize)\n    create_tag_image(tags, oname, size=(1800, 1200), fontname=fontname)\n", "label": 0}
{"function": "\n\ndef connectionLost(self, reason):\n    if (self.bodyDecoder is not None):\n        try:\n            try:\n                self.bodyDecoder.noMoreData()\n            except PotentialDataLoss:\n                self.response._bodyDataFinished(Failure())\n            except _DataLoss:\n                self.response._bodyDataFinished(Failure(ResponseFailed([reason, Failure()], self.response)))\n            else:\n                self.response._bodyDataFinished()\n        except:\n            log.err()\n    elif (self.state != DONE):\n        if self._everReceivedData:\n            exceptionClass = ResponseFailed\n        else:\n            exceptionClass = ResponseNeverReceived\n        self._responseDeferred.errback(Failure(exceptionClass([reason])))\n        del self._responseDeferred\n", "label": 1}
{"function": "\n\ndef __init__(self, generator, name=None, storage=None, cachefile_backend=None, cachefile_strategy=None):\n    '\\n        :param generator: The object responsible for generating a new image.\\n        :param name: The filename\\n        :param storage: A Django storage object that will be used to save the\\n            file.\\n        :param cachefile_backend: The object responsible for managing the\\n            state of the file.\\n        :param cachefile_strategy: The object responsible for handling events\\n            for this file.\\n\\n        '\n    self.generator = generator\n    if (not name):\n        try:\n            name = generator.cachefile_name\n        except AttributeError:\n            fn = get_by_qname(settings.IMAGEKIT_CACHEFILE_NAMER, 'namer')\n            name = fn(generator)\n    self.name = name\n    storage = (storage or getattr(generator, 'cachefile_storage', None) or get_singleton(settings.IMAGEKIT_DEFAULT_FILE_STORAGE, 'file storage backend'))\n    self.cachefile_backend = (cachefile_backend or getattr(generator, 'cachefile_backend', None) or get_singleton(settings.IMAGEKIT_DEFAULT_CACHEFILE_BACKEND, 'cache file backend'))\n    self.cachefile_strategy = (cachefile_strategy or getattr(generator, 'cachefile_strategy', None) or get_singleton(settings.IMAGEKIT_DEFAULT_CACHEFILE_STRATEGY, 'cache file strategy'))\n    super(ImageCacheFile, self).__init__(storage=storage)\n", "label": 1}
{"function": "\n\ndef assert_lte(lval, rval, message=None):\n    'Assert that lval is less than or equal to rval'\n    if message:\n        assert (lval <= rval), message\n    else:\n        assert (lval <= rval), ('assertion failed: %r <= %r' % (lval, rval))\n", "label": 0}
{"function": "\n\ndef _edit_running_config(self, conf_str, snippet):\n    conn = self._get_connection()\n    LOG.info(_LI('Config generated for [%(device)s] %(snip)s is:%(conf)s caller:%(caller)s'), {\n        'device': self.hosting_device['id'],\n        'snip': snippet,\n        'conf': conf_str,\n        'caller': self.caller_name(),\n    })\n    try:\n        rpc_obj = conn.edit_config(target='running', config=conf_str)\n        self._check_response(rpc_obj, snippet, conf_str=conf_str)\n    except Exception as e:\n        if re.search('REMOVE_|DELETE_', snippet):\n            LOG.warning(_LW('Pass exception for %s'), snippet)\n            pass\n        elif isinstance(e, ncclient.operations.rpc.RPCError):\n            e_tag = e.tag\n            e_type = e.type\n            params = {\n                'snippet': snippet,\n                'type': e_type,\n                'tag': e_tag,\n                'dev_id': self.hosting_device['id'],\n                'ip': self._host_ip,\n                'confstr': conf_str,\n            }\n            raise cfg_exc.CSR1kvConfigException(**params)\n", "label": 0}
{"function": "\n\ndef all_broker_ids(self):\n    brokers_path = self.path_for_brokers()\n    try:\n        topic_node_children = self._zk_children(brokers_path)\n    except zookeeper.NoNodeException:\n        log.error(\"Couldn't find brokers entry {0}\".format(brokers_path))\n        return []\n    return sorted((int(broker_id) for broker_id in topic_node_children))\n", "label": 0}
{"function": "\n\ndef test_profiler():\n    with prof:\n        out = get(dsk, 'e')\n    assert (out == 6)\n    prof_data = sorted(prof.results, key=(lambda d: d.key))\n    keys = [i.key for i in prof_data]\n    assert (keys == ['c', 'd', 'e'])\n    tasks = [i.task for i in prof_data]\n    assert (tasks == [(add, 'a', 'b'), (mul, 'a', 'b'), (mul, 'c', 'd')])\n    prof.clear()\n    assert (prof.results == [])\n", "label": 0}
{"function": "\n\ndef assert_raises_and_contains(expected_exception_class, strings, callable_obj, *args, **kwargs):\n    'Assert an exception is raised by passing in a callable and its\\n    arguments and that the string representation of the exception\\n    contains the case-insensitive list of passed in strings.\\n\\n    Args\\n        strings -- can be a string or an iterable of strings\\n    '\n    try:\n        callable_obj(*args, **kwargs)\n    except expected_exception_class as e:\n        message = str(e).lower()\n        try:\n            is_string = isinstance(strings, basestring)\n        except NameError:\n            is_string = isinstance(strings, str)\n        if is_string:\n            strings = [strings]\n        for string in strings:\n            assert_in(string.lower(), message)\n    else:\n        assert_not_reached(('No exception was raised (expected %s)' % expected_exception_class))\n", "label": 0}
{"function": "\n\ndef calc_gradient(self, indep_list, unknown_list, mode='auto', return_format='array', dv_scale=None, cn_scale=None, sparsity=None):\n    \" Returns the gradient for the system that is specified in\\n        self.root. This function is used by the optimizer but also can be\\n        used for testing derivatives on your model.\\n\\n        Args\\n        ----\\n        indep_list : iter of strings\\n            Iterator of independent variable names that derivatives are to\\n            be calculated with respect to. All params must have a IndepVarComp.\\n\\n        unknown_list : iter of strings\\n            Iterator of output or state names that derivatives are to\\n            be calculated for. All must be valid unknowns in OpenMDAO.\\n\\n        mode : string, optional\\n            Deriviative direction, can be 'fwd', 'rev', 'fd', or 'auto'.\\n            Default is 'auto', which uses mode specified on the linear solver\\n            in root.\\n\\n        return_format : string, optional\\n            Format for the derivatives, can be 'array' or 'dict'.\\n\\n        dv_scale : dict, optional\\n            Dictionary of driver-defined scale factors on the design variables.\\n\\n        cn_scale : dict, optional\\n            Dictionary of driver-defined scale factors on the constraints.\\n\\n        sparsity : dict, optional\\n            Dictionary that gives the relevant design variables for each\\n            constraint. This option is only supported in the `dict` return\\n            format.\\n\\n        Returns\\n        -------\\n        ndarray or dict\\n            Jacobian of unknowns with respect to params.\\n        \"\n    if (mode not in ['auto', 'fwd', 'rev', 'fd']):\n        msg = \"mode must be 'auto', 'fwd', 'rev', or 'fd'\"\n        raise ValueError(msg)\n    if (return_format not in ['array', 'dict']):\n        msg = \"return_format must be 'array' or 'dict'\"\n        raise ValueError(msg)\n    with self.root._dircontext:\n        if ((mode == 'fd') or self.root.fd_options['force_fd']):\n            return self._calc_gradient_fd(indep_list, unknown_list, return_format, dv_scale=dv_scale, cn_scale=cn_scale, sparsity=sparsity)\n        else:\n            return self._calc_gradient_ln_solver(indep_list, unknown_list, return_format, mode, dv_scale=dv_scale, cn_scale=cn_scale, sparsity=sparsity)\n", "label": 0}
{"function": "\n\ndef _async_connect(self, *args):\n    \"Internal use only; use 'connect' with 'yield' instead.\\n\\n        Asynchronous version of socket connect method.\\n        \"\n\n    def _connect(self, *args):\n        err = self._rsock.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n        if err:\n            self._notifier.unregister(self)\n            self._write_task = None\n            (coro, self._write_coro) = (self._write_coro, None)\n            coro.throw(socket.error(err))\n        elif self._certfile:\n\n            def _ssl_handshake(self):\n                try:\n                    self._rsock.do_handshake()\n                except ssl.SSLError as err:\n                    if ((err.args[0] == ssl.SSL_ERROR_WANT_READ) or (err.args[0] == ssl.SSL_ERROR_WANT_WRITE)):\n                        pass\n                    else:\n                        self._read_task = self._write_task = None\n                        (coro, self._write_coro) = (self._write_coro, None)\n                        self._read_coro = None\n                        self.close()\n                        coro.throw(*sys.exc_info())\n                except:\n                    self._read_task = self._write_task = None\n                    (coro, self._write_coro) = (self._write_coro, None)\n                    self._read_coro = None\n                    self.close()\n                    coro.throw(*sys.exc_info())\n                else:\n                    self._notifier.clear(self)\n                    self._read_task = self._write_task = None\n                    (coro, self._write_coro) = (self._write_coro, None)\n                    self._read_coro = None\n                    coro._proceed_(0)\n            try:\n                self._rsock = ssl.wrap_socket(self._rsock, ca_certs=self._certfile, cert_reqs=ssl.CERT_REQUIRED, server_side=False, do_handshake_on_connect=False)\n            except:\n                (coro, self._write_coro) = (self._write_coro, None)\n                self.close()\n                coro.throw(*sys.exc_info())\n            else:\n                self._read_task = self._write_task = functools.partial(_ssl_handshake, self)\n                self._read_coro = self._write_coro\n                self._notifier.add(self, _AsyncPoller._Read)\n                self._write_task()\n        else:\n            self._write_task = None\n            (coro, self._write_coro) = (self._write_coro, None)\n            self._notifier.clear(self, _AsyncPoller._Write)\n            coro._proceed_(0)\n    self._write_task = functools.partial(_connect, self, *args)\n    self._write_coro = AsynCoro.cur_coro()\n    self._write_coro._await_()\n    self._notifier.add(self, _AsyncPoller._Write)\n    try:\n        self._rsock.connect(*args)\n    except socket.error as e:\n        if ((e.args[0] == EINPROGRESS) or (e.args[0] == EWOULDBLOCK)):\n            pass\n        else:\n            raise\n", "label": 0}
{"function": "\n\ndef _result(self, response, json=False, binary=False):\n    assert (not (json and binary))\n    self._raise_for_status(response)\n    if json:\n        return response.json()\n    if binary:\n        return response.content\n    return response.text\n", "label": 0}
{"function": "\n\n@classmethod\n@unguarded\ndef validate_url(cls, url):\n    '\\n        Return a boolean indicating whether the URL has a protocol and hostname.\\n        If a port is specified, ensure it is an integer.\\n\\n        Arguments:\\n            url (str): The URL to check.\\n\\n        Returns:\\n            Boolean indicating whether the URL has a protocol and hostname.\\n        '\n    result = urlparse.urlsplit(url)\n    if ((not result.scheme) or (not result.netloc)):\n        return False\n    try:\n        if (result.port is not None):\n            int(result.port)\n        elif result.netloc.endswith(':'):\n            return False\n    except ValueError:\n        return False\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef test_reflect_entity_overrides():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    b = Symbol('b')\n    m = Symbol('m')\n    l = Line((0, b), slope=m)\n    p = Point(x, y)\n    r = p.reflect(l)\n    c = Circle((x, y), 3)\n    cr = c.reflect(l)\n    assert (cr == Circle(r, (- 3)))\n    assert (c.area == (- cr.area))\n    pent = RegularPolygon((1, 2), 1, 5)\n    l = Line((0, pi), slope=sqrt(2))\n    rpent = pent.reflect(l)\n    assert (rpent.center == pent.center.reflect(l))\n    assert (str([w.n(3) for w in rpent.vertices]) == '[Point2D(-0.586, 4.27), Point2D(-1.69, 4.66), Point2D(-2.41, 3.73), Point2D(-1.74, 2.76), Point2D(-0.616, 3.10)]')\n    assert pent.area.equals((- rpent.area))\n", "label": 0}
{"function": "\n\ndef test_dispatch():\n\n    class App(morepath.App):\n        pass\n\n    class Foo(object):\n        pass\n\n    class Bar(object):\n        pass\n\n    class Other(object):\n        pass\n\n    @reg.dispatch('obj')\n    def f(obj):\n        return 'fallback'\n\n    @App.function(f, obj=Foo)\n    def f_foo(obj):\n        return 'foo'\n\n    @App.function(f, obj=Bar)\n    def f_bar(obj):\n        return 'bar'\n    a = App()\n    lookup = a.lookup\n    assert (f(Foo(), lookup=lookup) == 'foo')\n    assert (f(Bar(), lookup=lookup) == 'bar')\n    assert (f(Other(), lookup=lookup) == 'fallback')\n", "label": 0}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(UpdateProjectMembersAction, self).__init__(request, *args, **kwargs)\n    err_msg = _('Unable to retrieve user list. Please try again later.')\n    domain_id = self.initial.get('domain_id', None)\n    project_id = ''\n    if ('project_id' in self.initial):\n        project_id = self.initial['project_id']\n    try:\n        default_role = keystone.get_default_role(self.request)\n        if (default_role is None):\n            default = getattr(settings, 'OPENSTACK_KEYSTONE_DEFAULT_ROLE', None)\n            msg = (_('Could not find default role \"%s\" in Keystone') % default)\n            raise exceptions.NotFound(msg)\n    except Exception:\n        exceptions.handle(self.request, err_msg, redirect=reverse(INDEX_URL))\n    default_role_name = self.get_default_role_field_name()\n    self.fields[default_role_name] = forms.CharField(required=False)\n    self.fields[default_role_name].initial = default_role.id\n    all_users = []\n    try:\n        all_users = api.keystone.user_list(request, domain=domain_id)\n    except Exception:\n        exceptions.handle(request, err_msg)\n    users_list = [(user.id, user.name) for user in all_users]\n    role_list = []\n    try:\n        role_list = api.keystone.role_list(request)\n    except Exception:\n        exceptions.handle(request, err_msg, redirect=reverse(INDEX_URL))\n    for role in role_list:\n        field_name = self.get_member_field_name(role.id)\n        label = role.name\n        self.fields[field_name] = forms.MultipleChoiceField(required=False, label=label)\n        self.fields[field_name].choices = users_list\n        self.fields[field_name].initial = []\n    if project_id:\n        try:\n            users_roles = api.keystone.get_project_users_roles(request, project_id)\n        except Exception:\n            exceptions.handle(request, err_msg, redirect=reverse(INDEX_URL))\n        for user_id in users_roles:\n            roles_ids = users_roles[user_id]\n            for role_id in roles_ids:\n                field_name = self.get_member_field_name(role_id)\n                self.fields[field_name].initial.append(user_id)\n", "label": 1}
{"function": "\n\ndef test_no_objects(self):\n    Author.objects.all().delete()\n    seen = [author.id for author in Author.objects.iter_smart()]\n    assert (seen == [])\n", "label": 0}
{"function": "\n\ndef test_list_extend_meta_type_integer(self):\n    '\\n        Invoke list_extend() with metadata input is of type integer\\n        '\n    key = ('test', 'demo', 1)\n    try:\n        TestListExtend.client.list_extend(key, 'contact_no', [85], 888)\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'Metadata should be of type dictionary')\n", "label": 0}
{"function": "\n\ndef load_meta_file(file_path):\n    if (not os.path.isfile(file_path)):\n        raise Exception(('File \"%s\" does not exist.' % file_path))\n    (file_name, file_ext) = os.path.splitext(file_path)\n    if (file_ext not in ALLOWED_EXTS):\n        raise Exception(('Unsupported meta type %s, file %s. Allowed: %s' % (file_ext, file_path, ALLOWED_EXTS)))\n    with open(file_path, 'r') as f:\n        return PARSER_FUNCS[file_ext](f)\n", "label": 0}
{"function": "\n\ndef __init__(self, vm_spec):\n    'Initialize a CloudStack virtual machine.\\n\\n    Args:\\n      vm_spec: virtual_machine.BaseVirtualMachineSpec object of the vm.\\n    '\n    super(CloudStackVirtualMachine, self).__init__(vm_spec)\n    self.network = cloudstack_network.CloudStackNetwork.GetNetwork(self)\n    self.cs = util.CsClient(FLAGS.CS_API_URL, FLAGS.CS_API_KEY, FLAGS.CS_API_SECRET)\n    self.project_id = None\n    if FLAGS.project:\n        project = self.cs.get_project(FLAGS.project)\n        assert project, 'Project not found'\n        self.project_id = project['id']\n    zone = self.cs.get_zone(self.zone)\n    assert zone, 'Zone not found'\n    self.zone_id = zone['id']\n    self.user_name = self.DEFAULT_USER_NAME\n    self.image = (self.image or self.DEFAULT_IMAGE)\n    self.disk_counter = 0\n", "label": 0}
{"function": "\n\ndef test_Matrix_printing():\n    mat = Matrix([(x * y), Piecewise(((2 + x), (y > 0)), (y, True)), sin(z)])\n    A = MatrixSymbol('A', 3, 1)\n    assert (ccode(mat, A) == 'A[0] = x*y;\\nif (y > 0) {\\n   A[1] = x + 2;\\n}\\nelse {\\n   A[1] = y;\\n}\\nA[2] = sin(z);')\n    expr = ((Piecewise(((2 * A[(2, 0)]), (x > 0)), (A[(2, 0)], True)) + sin(A[(1, 0)])) + A[(0, 0)])\n    assert (ccode(expr) == '((x > 0) ? (\\n   2*A[2]\\n)\\n: (\\n   A[2]\\n)) + sin(A[1]) + A[0]')\n    q = MatrixSymbol('q', 5, 1)\n    M = MatrixSymbol('M', 3, 3)\n    m = Matrix([[sin(q[(1, 0)]), 0, cos(q[(2, 0)])], [(q[(1, 0)] + q[(2, 0)]), q[(3, 0)], 5], [((2 * q[(4, 0)]) / q[(1, 0)]), (sqrt(q[(0, 0)]) + 4), 0]])\n    assert (ccode(m, M) == 'M[0] = sin(q[1]);\\nM[1] = 0;\\nM[2] = cos(q[2]);\\nM[3] = q[1] + q[2];\\nM[4] = q[3];\\nM[5] = 5;\\nM[6] = 2*q[4]*1.0/q[1];\\nM[7] = 4 + sqrt(q[0]);\\nM[8] = 0;')\n", "label": 0}
{"function": "\n\ndef pseudo_raw_input(self, prompt):\n    \"copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout\"\n    if self.use_rawinput:\n        try:\n            line = raw_input(prompt)\n        except EOFError:\n            line = 'EOF'\n    else:\n        self.stdout.write(prompt)\n        self.stdout.flush()\n        line = self.stdin.readline()\n        if (not len(line)):\n            line = 'EOF'\n        elif (line[(- 1)] == '\\n'):\n            line = line[:(- 1)]\n    return line\n", "label": 0}
{"function": "\n\ndef get_stack(message=None, stack_first_frame=1, max_frames=15):\n    try:\n        stack = inspect.stack()\n        frame_num = stack_first_frame\n        context = []\n        while ((len(stack) > frame_num) and (frame_num < (max_frames + stack_first_frame))):\n            exec_line = ('%s:%s:%s' % (stack[frame_num][1], stack[frame_num][2], stack[frame_num][3]))\n            context.insert(0, exec_line)\n            if (exec_line.endswith('_control_flow') or exec_line.endswith('load_ion') or exec_line.endswith('spawn_process') or exec_line.endswith(':main') or exec_line.endswith(':dispatch_request')):\n                break\n            frame_num += 1\n        stack_str = '\\n '.join(context)\n        if message:\n            stack_str = ((message + '\\n') + stack_str)\n        return stack_str\n    except Exception as ex:\n        stack_str = ('ERROR: ' + str(ex))\n        if message:\n            stack_str = ((message + '\\n') + stack_str)\n        return stack_str\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    if (self.length is None):\n        length = '*'\n    else:\n        length = self.length\n    if (self.start is None):\n        assert (self.stop is None)\n        return ('bytes */%s' % length)\n    stop = (self.stop - 1)\n    return ('bytes %s-%s/%s' % (self.start, stop, length))\n", "label": 0}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(SelectPluginAction, self).__init__(request, *args, **kwargs)\n    try:\n        plugins = saharaclient.plugin_list(request)\n    except Exception:\n        plugins = []\n        exceptions.handle(request, _('Unable to fetch plugin list.'))\n    plugin_choices = [(plugin.name, plugin.title) for plugin in plugins]\n    self.fields['plugin_name'] = forms.ChoiceField(label=_('Plugin name'), choices=plugin_choices, widget=forms.Select(attrs={\n        'class': 'plugin_name_choice',\n    }))\n    for plugin in plugins:\n        field_name = (plugin.name + '_version')\n        choice_field = forms.ChoiceField(label=_('Version'), choices=[(version, version) for version in plugin.versions], widget=forms.Select(attrs={\n            'class': (('plugin_version_choice ' + field_name) + '_choice'),\n        }))\n        self.fields[field_name] = choice_field\n", "label": 0}
{"function": "\n\ndef __iter__(self):\n    with self.source.open('rb') as f:\n        try:\n            while True:\n                (yield tuple(pickle.load(f)))\n        except EOFError:\n            pass\n", "label": 0}
{"function": "\n\ndef test_asint_set(self):\n    dset = dictset({\n        'foo': '1',\n    })\n    assert (dset.asint('foo', _set=True) == 1)\n    assert (dset.foo == 1)\n", "label": 0}
{"function": "\n\ndef run(self):\n    super(FakeDeletionThread, self).run()\n    receiver = NailgunReceiver\n    kwargs = {\n        'task_uuid': self.task_uuid,\n        'nodes': self.data['args']['nodes'],\n        'status': 'ready',\n    }\n    nodes_to_restore = copy.deepcopy(self.data['args'].get('nodes_to_restore', []))\n    resp_method = getattr(receiver, self.respond_to)\n    try:\n        resp_method(**kwargs)\n        db().commit()\n    except Exception:\n        db().rollback()\n        raise\n    recover_nodes = self.params.get('recover_nodes', True)\n    recover_offline_nodes = self.params.get('recover_offline_nodes', True)\n    if (not recover_nodes):\n        db().commit()\n        return\n    for node_data in nodes_to_restore:\n        is_offline = (('online' in node_data) and (not node_data['online']))\n        if (is_offline and (not recover_offline_nodes)):\n            continue\n        node_data['status'] = 'discover'\n        objects.Node.create(node_data)\n    db().commit()\n", "label": 0}
{"function": "\n\ndef add_jumpbox(host):\n    if jb.cluster.locate(host):\n        return\n    print('Connecting to jumpbox:', host)\n    try:\n        t = ssh.connection_worker(host, None, init_data['auth'])\n        retries = 3\n        while ((not t.is_authenticated()) and (retries > 0)):\n            print(('Failed to authenticate to Jumpbox (%s)' % host))\n            jb_user = raw_input(('Enter username for [%s]: ' % host))\n            jb_passwd = getpass.getpass(('Enter password for %s@%s: ' % (jb_user, host)))\n            reauth = AuthManager(jb_user, auth_file=None, include_agent=False, include_userkeys=False, default_password=jb_passwd)\n            t = ssh.connection_worker(host, None, reauth)\n            retries -= 1\n    except Exception as e:\n        print(host, repr(e))\n    finally:\n        jb.cluster.connections[host] = t\n", "label": 0}
{"function": "\n\ndef _finish_query_lookupd(self, response, lookupd_url):\n    if response.error:\n        logger.warning('[%s] lookupd %s query error: %s', self.name, lookupd_url, response.error)\n        return\n    try:\n        lookup_data = json.loads(response.body)\n    except ValueError:\n        logger.warning('[%s] lookupd %s failed to parse JSON: %r', self.name, lookupd_url, response.body)\n        return\n    if (lookup_data['status_code'] != 200):\n        logger.warning('[%s] lookupd %s responded with %d', self.name, lookupd_url, lookup_data['status_code'])\n        return\n    for producer in lookup_data['data']['producers']:\n        address = producer.get('broadcast_address', producer.get('address'))\n        assert address\n        self.connect_to_nsqd(address, producer['tcp_port'])\n", "label": 0}
{"function": "\n\ndef update_record(self, record, name, type, data, extra=None):\n    '\\n        Update an existing record.\\n\\n        :param record: Record to update.\\n        :type  record: :class:`Record`\\n\\n        :param name: FQDN of the new record, for example \"www.example.com\".\\n        :type  name: ``str``\\n\\n        :param type: DNS record type (A, AAAA, ...).\\n        :type  type: :class:`RecordType`\\n\\n        :param data: Data for the record (depends on the record type).\\n        :type  data: ``str``\\n\\n        :param extra: (optional) Extra attributes (driver specific).\\n        :type  extra: ``dict``\\n\\n        :rtype: :class:`Record`\\n        '\n    action = ('%s/servers/%s/zones/%s' % (self.api_root, self.ex_server, record.zone.id))\n    if ((extra is None) or (extra.get('ttl', None) is None)):\n        raise ValueError('PowerDNS requires a ttl value for every record')\n    updated_record = {\n        'content': data,\n        'disabled': False,\n        'name': name,\n        'ttl': extra['ttl'],\n        'type': type,\n    }\n    payload = {\n        'rrsets': [{\n            'name': record.name,\n            'type': record.type,\n            'changetype': 'DELETE',\n        }, {\n            'name': name,\n            'type': type,\n            'changetype': 'REPLACE',\n            'records': [updated_record],\n        }],\n    }\n    try:\n        self.connection.request(action=action, data=json.dumps(payload), method='PATCH')\n    except BaseHTTPError:\n        e = sys.exc_info()[1]\n        if ((e.code == httplib.UNPROCESSABLE_ENTITY) and e.message.startswith('Could not find domain')):\n            raise ZoneDoesNotExistError(zone_id=record.zone.id, driver=self, value=e.message)\n        raise e\n    return Record(id=None, name=name, data=data, type=type, zone=record.zone, driver=self, ttl=extra['ttl'])\n", "label": 0}
{"function": "\n\n@xmlrpc_func(returns='string[]', args=['string'])\ndef pingback_extensions_get_pingbacks(target):\n    \"\\n    pingback.extensions.getPingbacks(url) => '[url, url, ...]'\\n\\n    Returns an array of URLs that link to the specified url.\\n\\n    See: http://www.aquarionics.com/misc/archives/blogite/0198.html\\n    \"\n    site = Site.objects.get_current()\n    (scheme, netloc, path, query, fragment) = urlsplit(target)\n    if (netloc != site.domain):\n        return TARGET_DOES_NOT_EXIST\n    try:\n        (view, args, kwargs) = resolve(path)\n    except Resolver404:\n        return TARGET_DOES_NOT_EXIST\n    try:\n        entry = Entry.published.get(slug=kwargs['slug'], publication_date__year=kwargs['year'], publication_date__month=kwargs['month'], publication_date__day=kwargs['day'])\n    except (KeyError, Entry.DoesNotExist):\n        return TARGET_IS_NOT_PINGABLE\n    return [pingback.user_url for pingback in entry.pingbacks]\n", "label": 0}
{"function": "\n\ndef test_astar_undirected3(self):\n    XG4 = nx.Graph()\n    XG4.add_edges_from([[0, 1, {\n        'weight': 2,\n    }], [1, 2, {\n        'weight': 2,\n    }], [2, 3, {\n        'weight': 1,\n    }], [3, 4, {\n        'weight': 1,\n    }], [4, 5, {\n        'weight': 1,\n    }], [5, 6, {\n        'weight': 1,\n    }], [6, 7, {\n        'weight': 1,\n    }], [7, 0, {\n        'weight': 1,\n    }]])\n    assert (nx.astar_path(XG4, 0, 2) == [0, 1, 2])\n    assert (nx.astar_path_length(XG4, 0, 2) == 4)\n", "label": 0}
{"function": "\n\ndef profile_head_single(benchmark):\n    import gc\n    results = []\n    gc.collect()\n    try:\n        from ctypes import cdll, CDLL\n        cdll.LoadLibrary('libc.so.6')\n        libc = CDLL('libc.so.6')\n        libc.malloc_trim(0)\n    except:\n        pass\n    N = (args.hrepeats + args.burnin)\n    results = []\n    try:\n        for i in range(N):\n            gc.disable()\n            d = dict()\n            try:\n                d = benchmark.run()\n            except KeyboardInterrupt:\n                raise\n            except Exception as e:\n                err = ''\n                try:\n                    err = d.get('traceback')\n                    if (err is None):\n                        err = str(e)\n                except:\n                    pass\n                print(('%s died with:\\n%s\\nSkipping...\\n' % (benchmark.name, err)))\n            results.append(d.get('timing', np.nan))\n            gc.enable()\n            gc.collect()\n    finally:\n        gc.enable()\n    if results:\n        results = results[args.burnin:]\n    sys.stdout.write('.')\n    sys.stdout.flush()\n    return Series(results, name=benchmark.name)\n", "label": 1}
{"function": "\n\ndef test_cpu_stats_interrupts(self):\n    with open('/proc/stat', 'rb') as f:\n        for line in f:\n            if line.startswith(b'intr'):\n                interrupts = int(line.split()[1])\n                break\n        else:\n            raise ValueError(\"couldn't find line\")\n    self.assertAlmostEqual(psutil.cpu_stats().interrupts, interrupts, delta=1000)\n", "label": 0}
{"function": "\n\ndef replace(self, new):\n    'Replaces this node with a new one in the parent.'\n    assert (self.parent is not None), str(self)\n    assert (new is not None)\n    if (not isinstance(new, list)):\n        new = [new]\n    l_children = []\n    found = False\n    for ch in self.parent.children:\n        if (ch is self):\n            assert (not found), (self.parent.children, self, new)\n            if (new is not None):\n                l_children.extend(new)\n            found = True\n        else:\n            l_children.append(ch)\n    assert found, (self.children, self, new)\n    self.parent.changed()\n    self.parent.children = l_children\n    for x in new:\n        x.parent = self.parent\n    self.parent = None\n", "label": 1}
{"function": "\n\ndef close(self, rollback=False):\n    try:\n        if (not self.isJsonFile):\n            self.conn.close()\n        self.__dict__.clear()\n    except Exception as ex:\n        self.__dict__.clear()\n        raise\n", "label": 0}
{"function": "\n\ndef _writeFile(self, directory, file_name, contents):\n    path = os.path.abspath(os.path.join(directory, file_name))\n    path_dir = os.path.dirname(path)\n    if (not os.path.exists(path_dir)):\n        if path_dir.startswith(directory):\n            os.makedirs(path_dir)\n        else:\n            raise ValueError\n    with open(path, 'wt') as f:\n        f.write(contents)\n", "label": 0}
{"function": "\n\ndef test_scrape_handler(self):\n    valid = {\n        'url': 'www.stuff.com',\n        'crawlid': 'abc124',\n        'appid': 'testapp',\n        'spiderid': 'link',\n        'priority': 5,\n    }\n    handler = ScraperHandler()\n    handler.extract = tldextract.TLDExtract()\n    handler.redis_conn = MagicMock()\n    handler.redis_conn.zadd = MagicMock(side_effect=AssertionError('added'))\n    try:\n        handler.handle(valid)\n        self.fail('Action not called')\n    except AssertionError as e:\n        self.assertEquals('added', e.message)\n    handler.redis_conn.zadd = MagicMock()\n    handler.redis_conn.set = MagicMock(side_effect=AssertionError('expires'))\n    valid['expires'] = 124242\n    try:\n        handler.handle(valid)\n        self.fail('Expires not called')\n    except AssertionError as e:\n        self.assertEquals('expires', e.message)\n", "label": 0}
{"function": "\n\ndef test_ssl_very_badssl_com():\n    total_failed_tests = 0\n    total_passed_tests = 0\n    myfile = open((THIS_DIR + '/fixtures/badssl.yaml'))\n    test_json = yaml.load(myfile)\n    for test in test_json['tests']:\n        test_obj = Charcoal(test=test, host='very.badssl.com')\n        total_failed_tests += test_obj.failed\n        total_passed_tests += test_obj.passed\n    LOG.debug(total_failed_tests)\n    assert (total_failed_tests == 0)\n", "label": 0}
{"function": "\n\ndef main():\n    global imap_account\n    global routes\n    if (conf is None):\n        return 1\n    logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n    for routing in routings:\n        (methods, regex, app) = routing[:3]\n        if isinstance(methods, six.string_types):\n            methods = (methods,)\n        vars = (routing[3] if (len(routing) >= 4) else {\n            \n        })\n        routes.append((methods, re.compile(regex), app, vars))\n        log.info('Route {} openned'.format(regex[1:(- 1)]))\n    try:\n        imap_account = imap_cli.connect(**conf)\n        httpd = simple_server.make_server('127.0.0.1', 8000, router)\n        log.info('Serving on http://127.0.0.1:8000')\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        log.info('Interupt by user, exiting')\n    return 0\n", "label": 0}
{"function": "\n\ndef prepend(line, path):\n    \"\\n    Appends *line* to the _beginning_ of the file at the given *path*.\\n\\n    If *line* doesn't end in a newline one will be appended to the end of it.\\n    \"\n    if isinstance(line, str):\n        line = line.encode('utf-8')\n    if (not line.endswith(b'\\n')):\n        line += b'\\n'\n    temp = tempfile.NamedTemporaryFile('wb')\n    temp_name = temp.name\n    temp.close()\n    with open(temp_name, 'wb') as temp:\n        temp.write(line)\n        with open(path, 'rb') as r:\n            temp.write(r.read())\n    shutil.move(temp_name, path)\n", "label": 0}
{"function": "\n\ndef test_revoke_roles_with_proper_timeout_policy_value(self):\n    policy = {\n        'timeout': 50,\n    }\n    user = 'example-test'\n    roles = ['read-write', 'sys-admin']\n    status = self.client.admin_revoke_roles(user, roles, policy)\n    time.sleep(2)\n    assert (status == 0)\n    user_details = self.client.admin_query_user(user)\n    assert (user_details == ['read'])\n", "label": 0}
{"function": "\n\ndef _validate_spacecount(self, spacecount):\n    try:\n        spacecount = int(spacecount)\n        if (spacecount < 2):\n            raise ValueError\n    except ValueError:\n        raise DataError('--spacecount must be an integer greater than 1.')\n    return spacecount\n", "label": 0}
{"function": "\n\ndef check_for_update(self):\n    ' Returns text to put in the title bar '\n    try:\n        latest = 'https://api.github.com/repos/Hyphen-ated/RebirthItemTracker/releases/latest'\n        github_info_json = urllib2.urlopen(latest).read()\n        info = json.loads(github_info_json)\n        latest_version = info['name']\n        title_text = (' v' + self.tracker_version)\n        if (latest_version != self.tracker_version):\n            title_text += ' (new version available)'\n        return title_text\n    except Exception as e:\n        self.log.debug(('Failed to find update info: ' + e.message))\n    return ''\n", "label": 0}
{"function": "\n\ndef test_slice():\n    inds = [0, slice(2), slice(1, 3), slice(None, None, 2), [1, 2, 3], (0, 1), (0, slice(1, 3)), (slice(0, 3), slice(3, 1, (- 1))), (0, [1, 2])]\n    for s in inds:\n        assert (compute(a[s], ax) == ax[s]).all()\n", "label": 0}
{"function": "\n\ndef authorize_account(self, realm_url, account_id, application_key):\n    assert (realm_url == 'http://production.example.com')\n    if (application_key != 'good-app-key'):\n        raise InvalidAuthToken(('invalid application key: %s' % (application_key,)), 'bad_auth_token')\n    self.authorized_accounts.add(account_id)\n    return dict(accountId=account_id, authorizationToken=('AUTH:' + account_id), apiUrl=self.API_URL, downloadUrl=self.DOWNLOAD_URL, minimumPartSize=self.MIN_PART_SIZE)\n", "label": 0}
{"function": "\n\ndef __call__(self, request, *args, **kwargs):\n    '\\n        In case of the wrapped view being determined as a class (that it has\\n        a dispatch attribute) we return a new instance of the class with all\\n        view arguments passed to the dispatch method. The case of a view\\n        function things are much simpler, we just call the view function with\\n        the view arguments.\\n\\n        For debugging purposes we insert some additional information that is\\n        useful for view classes in the raised exception.\\n        '\n    try:\n        if isclass(self.view):\n            view = self.view()\n            if hasattr(view, 'dispatch'):\n                return view.dispatch(request, *args, **kwargs)\n        else:\n            view = self.view\n        if callable(view):\n            return view(request, *args, **kwargs)\n    except (Http404, PermissionDenied, SystemExit):\n        raise\n    except Exception as ex:\n        try:\n            (cls, e, trace) = sys.exc_info()\n            msg = ('%s in %s.%s: %s' % (cls.__name__, self.view.__module__, self.view.__name__, e))\n        except Exception:\n            raise ex\n        else:\n            raise UtkikException(msg).with_traceback(trace)\n    raise ImproperlyConfigured(('%s.%s does not define a view function or class view.' % (self.view.__module__, self.view.__name__)))\n", "label": 1}
{"function": "\n\ndef _safeio(self, callback):\n    if self.closed:\n        raise ValueError('I/O operation on closed file')\n    for i in range(self.retries):\n        try:\n            if (not self.stream):\n                self.stream = self._reconnect()\n            return callback(self.stream)\n        except (EOFError, IOError, OSError, socket.error):\n            if (i >= (self.retries - 1)):\n                raise\n            if self.stream:\n                self.stream.close()\n            self.stream = None\n            time.sleep(0.5)\n", "label": 0}
{"function": "\n\ndef __contains__(self, name):\n    if ('/' in name):\n        try:\n            self[name]\n            return True\n        except KeyError:\n            return False\n    else:\n        return (name in self._obj)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(NoArgsCommand, self).__init__(*args, **kwargs)\n    self.copied_files = []\n    self.symlinked_files = []\n    self.unmodified_files = []\n    self.storage = get_storage_class(settings.STATICFILES_STORAGE)()\n    try:\n        self.storage.path('')\n    except NotImplementedError:\n        self.local = False\n    else:\n        self.local = True\n    os.stat_float_times(False)\n", "label": 0}
{"function": "\n\ndef main(args=None):\n    try:\n        if (args is None):\n            args = sys.argv[1:]\n        MonascaShell().main(args)\n    except Exception as e:\n        if (('--debug' in args) or ('-d' in args)):\n            raise\n        else:\n            print(e, file=sys.stderr)\n        sys.exit(1)\n", "label": 0}
{"function": "\n\ndef run(self):\n    arguments = self.arguments\n    wrong_sorted_files = False\n    arguments['check'] = True\n    for path in self.distribution_files():\n        for python_file in glob.iglob(os.path.join(path, '*.py')):\n            try:\n                incorrectly_sorted = SortImports(python_file, **arguments).incorrectly_sorted\n                if incorrectly_sorted:\n                    wrong_sorted_files = True\n            except IOError as e:\n                print('WARNING: Unable to parse file {0} due to {1}'.format(file_name, e))\n    if wrong_sorted_files:\n        exit(1)\n", "label": 0}
{"function": "\n\ndef get_setting(self, name, default=None):\n    v = settings.get(name)\n    if (v == None):\n        try:\n            return sublime.active_window().active_view().settings().get(name, default)\n        except AttributeError:\n            return default\n    else:\n        return v\n", "label": 0}
{"function": "\n\ndef test_join_option_types():\n    a = symbol('a', 'var * {x: ?int}')\n    b = symbol('b', 'var * {x: int}')\n    assert (join(a, b, 'x').dshape == dshape('var * {x: int}'))\n    assert (join(b, a, 'x').dshape == dshape('var * {x: int}'))\n", "label": 0}
{"function": "\n\ndef paginate(request, queryset, results_per_page=20):\n    paginator = Paginator(queryset, results_per_page)\n    try:\n        page = paginator.page(int(request.GET.get('page', 1)))\n    except InvalidPage:\n        raise Http404('Sorry, that page of results does not exist.')\n    except ValueError:\n        raise PermissionDenied()\n    return (page, paginator)\n", "label": 0}
{"function": "\n\ndef download_file(url, chunk_size=(100 * 1024)):\n    ' Helper method to download a file displaying a progress bar '\n    print('Fetching:', url)\n    file_content = None\n    progressbar = None\n    if (sys.version_info.major <= 2):\n        from rplibs.progressbar import FileTransferSpeed, ETA, ProgressBar, Percentage\n        from rplibs.progressbar import Bar\n        widgets = ['\\tDownloading: ', FileTransferSpeed(), ' ', Bar(), Percentage(), '   ', ETA()]\n        file_content = []\n        bytes_read = 0\n        try:\n            usock = urllib.request.urlopen(url)\n            file_size = int(usock.headers.get('Content-Length', 10000000000.0))\n            print('File size is', round((file_size / (1024 ** 2)), 2), 'MB')\n            progressbar = ProgressBar(widgets=widgets, maxval=file_size).start()\n            while True:\n                data = usock.read(chunk_size)\n                file_content.append(data)\n                bytes_read += len(data)\n                progressbar.update(bytes_read)\n                if (not data):\n                    break\n            usock.close()\n        except Exception:\n            print('ERROR: Could not fetch', url, '!', file=sys.stderr)\n            raise\n    else:\n        print('Downloading .. (progressbar disabled due to python 3 build)')\n        try:\n            usock = urllib.request.urlopen(url)\n            file_content = []\n            while True:\n                data = usock.read(chunk_size)\n                file_content.append(data)\n                if (not data):\n                    break\n            usock.close()\n        except Exception:\n            print('ERROR: Could not fetch', url, '!', file=sys.stderr)\n            raise\n    if progressbar:\n        progressbar.finish()\n    return binary_type().join(file_content)\n", "label": 1}
{"function": "\n\ndef rangeRingsFromList(centerFC, rangeList, distanceUnits, numRadials, outputRingFeatures, outputRadialFeatures, sr):\n    ' Make range ring features from a center, and list of distances '\n    try:\n        if ((sr == '#') or (sr == '') or (sr == None)):\n            sr = srDefault\n        rm = RingMaker(centerFC, rangeList, distanceUnits, sr)\n        numCenterPoints = arcpy.GetCount_management(centerFC).getOutput(0)\n        numRingsPerCenter = len(rangeList)\n        totalNumRings = (int(numCenterPoints) * int(numRingsPerCenter))\n        totalNumRadials = (int(numCenterPoints) * int(numRadials))\n        arcpy.AddMessage((((((('Making rings ' + str(totalNumRings)) + ' (') + str(numRingsPerCenter)) + ' for ') + str(numCenterPoints)) + ' centers)...'))\n        rm.makeRingsFromDistances()\n        outRings = rm.saveRingsAsFeatures(outputRingFeatures)\n        arcpy.AddMessage((((((('Making radials ' + str(totalNumRadials)) + ' (') + str(numRadials)) + ' for ') + str(numCenterPoints)) + ' centers)...'))\n        rm.makeRadials(numRadials)\n        outRadials = rm.saveRadialsAsFeatures(outputRadialFeatures)\n        return [outRings, outRadials]\n    except arcpy.ExecuteError:\n        msgs = arcpy.GetMessages()\n        arcpy.AddError(msgs)\n        print(msgs)\n    except:\n        tb = sys.exc_info()[2]\n        tbinfo = traceback.format_tb(tb)[0]\n        pymsg = ((('PYTHON ERRORS:\\nTraceback info:\\n' + tbinfo) + '\\nError Info:\\n') + str(sys.exc_info()[1]))\n        msgs = (('ArcPy ERRORS:\\n' + arcpy.GetMessages()) + '\\n')\n        arcpy.AddError(pymsg)\n        arcpy.AddError(msgs)\n        print((pymsg + '\\n'))\n        print(msgs)\n", "label": 0}
{"function": "\n\ndef execute(self):\n    context = Gaffer.Context.current()\n    fileName = self['fileName'].getValue()\n    directory = os.path.dirname(fileName)\n    if directory:\n        try:\n            os.makedirs(directory)\n        except OSError:\n            if (not os.path.isdir(directory)):\n                raise\n    text = self.__processText(context)\n    with file(fileName, self['mode'].getValue()) as f:\n        f.write(text)\n", "label": 0}
{"function": "\n\n@blueprint.route('/bundle/<bundle_name>/asset/<int:asset_index>_v<asset_hash>.<bundle_extension>')\ndef render_asset(bundle_name, bundle_extension, asset_index, asset_hash):\n    ' Render a single source from an asset.\\n\\n    Args:\\n        bundle_name: name of the bundle to render\\n        bundle_extension: file extension for the bundle\\n        asset_index: index of the asset in `Bundle.assets`\\n        asset_hash: calculated hash from asset content\\n    '\n    compressor = current_app.extensions['compressor']\n    try:\n        bundle = compressor.get_bundle(bundle_name)\n    except CompressorException:\n        abort(404)\n    try:\n        asset = bundle.assets[asset_index]\n    except IndexError:\n        abort(404)\n    if (asset.hash != asset_hash):\n        abort(404)\n    if (bundle.extension != bundle_extension):\n        abort(404)\n    content = asset.content\n    return Response(content, mimetype=bundle.mimetype)\n", "label": 0}
{"function": "\n\ndef test_compute():\n    a = (delayed(1) + 5)\n    b = (a + 1)\n    c = (a + 2)\n    assert (compute(b, c) == (7, 8))\n    assert (compute(b) == (7,))\n    assert (compute([a, b], c) == ([6, 7], 8))\n", "label": 0}
{"function": "\n\ndef compute(self):\n    port_object = None\n    if self.has_input('SetInputConnection0'):\n        ic = self.get_input('SetInputConnection0')\n        if hasattr(ic, 'vtkInstance'):\n            ic = ic.vtkInstance\n        producer = ic.GetProducer()\n        try:\n            port_object = producer.GetOutput()\n        except AttributeError:\n            raise ModuleError(self, 'expected a module that supports GetOutput')\n    elif self.has_input('SetInput'):\n        port_object = self.get_input('SetInput')\n        if hasattr(port_object, 'vtkInstance'):\n            port_object = port_object.vtkInstance\n    if port_object:\n        self.auto_set_results(port_object)\n", "label": 0}
{"function": "\n\n@common.check_cells_enabled\ndef sync_instances(self, req, body):\n    'Tell all cells to sync instance info.'\n    context = req.environ['nova.context']\n    authorize(context)\n    authorize(context, action='sync_instances')\n    project_id = body.pop('project_id', None)\n    deleted = body.pop('deleted', False)\n    updated_since = body.pop('updated_since', None)\n    if body:\n        msg = _(\"Only 'updated_since', 'project_id' and 'deleted' are understood.\")\n        raise exc.HTTPBadRequest(explanation=msg)\n    if isinstance(deleted, six.string_types):\n        try:\n            deleted = strutils.bool_from_string(deleted, strict=True)\n        except ValueError as err:\n            raise exc.HTTPBadRequest(explanation=six.text_type(err))\n    if updated_since:\n        try:\n            timeutils.parse_isotime(updated_since)\n        except ValueError:\n            msg = _('Invalid changes-since value')\n            raise exc.HTTPBadRequest(explanation=msg)\n    self.cells_rpcapi.sync_instances(context, project_id=project_id, updated_since=updated_since, deleted=deleted)\n", "label": 0}
{"function": "\n\ndef __init__(self, name, *fields, **kwargs):\n    self.name = name\n    self.fields = fields\n    assert (kwargs.get('join') is not 'and'), 'And joiner is not implemented!'\n    self.query_joiner = 'or'\n    self.search_in_field = kwargs.get('search_in_field')\n    self.search_in_values = kwargs.get('search_in_values')\n    assert self.search_in_field, 'Please specify search_in kwargs.'\n    assert self.search_in_values, 'Please specify a search_in_values kwarg.'\n    if (self.query_joiner not in ['and', 'or']):\n        raise TypeError(('Unknown query joiner: %s' % self.query_joiner))\n", "label": 0}
{"function": "\n\n@classmethod\ndef connection_pool(cls, **kwargs):\n    pool_key = cls.key_for_kwargs(kwargs)\n    if (pool_key in cls.pools):\n        return cls.pools[pool_key]\n    location = kwargs.get('location', None)\n    if (not location):\n        raise ImproperlyConfigured('no `location` key on connection kwargs')\n    params = {\n        'connection_class': Connection,\n        'db': kwargs.get('db', 0),\n        'password': kwargs.get('password', None),\n    }\n    if location.startswith('unix:'):\n        params['connection_class'] = UnixDomainSocketConnection\n        params['path'] = location[5:]\n    else:\n        try:\n            (params['host'], params['port']) = location.split(':')\n            params['port'] = int(params['port'])\n        except ValueError:\n            raise ImproperlyConfigured('Invalid `location` key syntax on connection kwargs')\n    cls.pools[pool_key] = RedisConnectionPool(**params)\n    return cls.pools[pool_key]\n", "label": 0}
{"function": "\n\ndef _maxbansParser(s):\n    if (':' in s):\n        modes = ''\n        limits = []\n        pairs = s.split(',')\n        for pair in pairs:\n            (mode, limit) = pair.split(':', 1)\n            modes += mode\n            limits += ((int(limit),) * len(mode))\n        d = dict(list(zip(modes, limits)))\n        assert ('b' in d)\n        return d['b']\n    else:\n        return int(s)\n", "label": 0}
{"function": "\n\ndef auth(self, irc, msg, args, url):\n    '<url>\\n\\n            Check the GPG signature at the <url> and authenticates you if\\n            the key used is associated to a user.'\n    self._expire_tokens()\n    content = utils.web.getUrl(url)\n    if (minisix.PY3 and isinstance(content, bytes)):\n        content = content.decode()\n    match = self._auth_re.search(content)\n    if (not match):\n        irc.error(_('Signature or token not found.'), Raise=True)\n    data = match.group(0)\n    token = match.group(1)\n    if (token not in self._tokens):\n        irc.error(_('Unknown token. It may have expired before you submit it.'), Raise=True)\n    if (self._tokens[token][0] != msg.prefix):\n        irc.error(_('Your hostname/nick changed in the process. Authentication aborted.'), Raise=True)\n    verified = gpg.keyring.verify(data)\n    if (verified and verified.valid):\n        keyid = verified.pubkey_fingerprint[(- 16):]\n        (prefix, expiry) = self._tokens.pop(token)\n        found = False\n        for (id, user) in ircdb.users.items():\n            if (keyid in [x[(- len(keyid)):] for x in user.gpgkeys]):\n                try:\n                    user.addAuth(msg.prefix)\n                except ValueError:\n                    irc.error(_(\"Your secure flag is true and your hostmask doesn't match any of your known hostmasks.\"), Raise=True)\n                ircdb.users.setUser(user, flush=False)\n                irc.reply((_('You are now authenticated as %s.') % user.name))\n                return\n        irc.error(_('Unknown GPG key.'), Raise=True)\n    else:\n        irc.error(_('Signature could not be verified. Make sure this is a valid GPG signature and the URL is valid.'))\n", "label": 1}
{"function": "\n\ndef test_single_draw_call_on_create(self):\n    d = Data(x=np.random.random(((2,) * self.ndim)))\n    dc = DataCollection([d])\n    app = GlueApplication(dc)\n    try:\n        from glue.viewers.common.qt.mpl_widget import MplCanvas\n        draw = MplCanvas.draw\n        MplCanvas.draw = MagicMock()\n        app.new_data_viewer(self.widget_cls, data=d)\n        selfs = [c[0][0] for c in MplCanvas.draw.call_arg_list]\n        assert (len(set(selfs)) == len(selfs))\n    finally:\n        MplCanvas.draw = draw\n", "label": 0}
{"function": "\n\ndef update_thumbs(self, nameList):\n    invalid = (set(self.thumbList) - set(nameList))\n    if (len(invalid) > 0):\n        with self.thmblock:\n            for thumbkey in invalid:\n                self.thumbList.remove(thumbkey)\n                del self.thumbDict[thumbkey]\n                self._tkf_highlight.discard(thumbkey)\n        self.reorder_thumbs()\n", "label": 0}
{"function": "\n\ndef __init__(self, short=None, long=None, argcount=0, value=False):\n    assert (argcount in (0, 1))\n    (self.short, self.long) = (short, long)\n    (self.argcount, self.value) = (argcount, value)\n    self.value = (None if ((value is False) and argcount) else value)\n", "label": 0}
{"function": "\n\ndef test_nottest_function_decorator(testdir):\n    testdir.makepyfile('\\n        import nose.tools\\n        @nose.tools.nottest\\n        def test_prefix():\\n            pass\\n        ')\n    reprec = testdir.inline_run()\n    assert (not reprec.getfailedcollections())\n    calls = reprec.getreports('pytest_runtest_logreport')\n    assert (not calls)\n", "label": 0}
{"function": "\n\ndef closeUnusedFiles(transport):\n    import os, sys\n    notouch = transport.protectedFileNumList()\n    for each in [sys.stdin, sys.stderr, sys.stdout]:\n        try:\n            notouch.append(each.fileno())\n        except AttributeError:\n            pass\n    for fdnum in range(3, 255):\n        if (fdnum not in notouch):\n            try:\n                os.close(fdnum)\n            except OSError:\n                pass\n", "label": 0}
{"function": "\n\ndef get_context(self, bot, update, **kwargs):\n    queryset = self.get_queryset()\n    if (not self.slug_field):\n        raise AttributeError(('Generic detail view %s must be called with a slug.' % self.__class__.__name__))\n    slug_field = self.get_slug_field(**kwargs)\n    slug = self.get_slug(**kwargs)\n    if slug:\n        try:\n            object = queryset.get(**{\n                slug_field: slug,\n            })\n        except FieldError:\n            raise FieldError(('Field %s not in valid. Review slug_field' % slug_field))\n        except ObjectDoesNotExist:\n            object = None\n    else:\n        object = None\n    context = {\n        'context_object_name': object,\n    }\n    if self.context_object_name:\n        context[self.context_object_name] = object\n    return context\n", "label": 0}
{"function": "\n\ndef test_do_bounce_when_apps_to_kill(self):\n    fake_bounce_func_return = {\n        'create_app': False,\n        'tasks_to_drain': [],\n    }\n    fake_bounce_func = mock.create_autospec(bounce_lib.brutal_bounce, return_value=fake_bounce_func_return)\n    fake_config = {\n        'instances': 5,\n    }\n    fake_new_app_running = True\n    fake_happy_new_tasks = ['fake_one', 'fake_two', 'fake_three']\n    fake_old_app_live_happy_tasks = {\n        'fake_app_to_kill_1': set(),\n    }\n    fake_old_app_live_unhappy_tasks = {\n        'fake_app_to_kill_1': set(),\n    }\n    fake_old_app_draining_tasks = {\n        'fake_app_to_kill_1': set(),\n    }\n    fake_service = 'fake_service'\n    fake_serviceinstance = 'fake_service.fake_instance'\n    self.fake_cluster = 'fake_cluster'\n    fake_instance = 'fake_instance'\n    fake_bounce_method = 'fake_bounce_method'\n    fake_drain_method = mock.Mock()\n    fake_marathon_jobid = 'fake.marathon.jobid'\n    fake_client = mock.create_autospec(marathon.MarathonClient)\n    expected_new_task_count = (fake_config['instances'] - len(fake_happy_new_tasks))\n    with contextlib.nested(mock.patch('paasta_tools.setup_marathon_job._log', autospec=True), mock.patch('paasta_tools.setup_marathon_job.bounce_lib.create_marathon_app', autospec=True), mock.patch('paasta_tools.setup_marathon_job.bounce_lib.kill_old_ids', autospec=True)) as (mock_log, mock_create_marathon_app, mock_kill_old_ids):\n        setup_marathon_job.do_bounce(bounce_func=fake_bounce_func, drain_method=fake_drain_method, config=fake_config, new_app_running=fake_new_app_running, happy_new_tasks=fake_happy_new_tasks, old_app_live_happy_tasks=fake_old_app_live_happy_tasks, old_app_live_unhappy_tasks=fake_old_app_live_unhappy_tasks, old_app_draining_tasks=fake_old_app_draining_tasks, service=fake_service, bounce_method=fake_bounce_method, serviceinstance=fake_serviceinstance, cluster=self.fake_cluster, instance=fake_instance, marathon_jobid=fake_marathon_jobid, client=fake_client, soa_dir='fake_soa_dir')\n        assert (mock_log.call_count == 3)\n        first_logged_line = mock_log.mock_calls[0][2]['line']\n        assert (('%s new tasks' % expected_new_task_count) in first_logged_line)\n        second_logged_line = mock_log.mock_calls[1][2]['line']\n        assert (('removing old unused apps with app_ids: %s' % 'fake_app_to_kill_1') in second_logged_line)\n        assert (mock_create_marathon_app.call_count == 0)\n        assert (fake_client.kill_task.call_count == len(fake_bounce_func_return['tasks_to_drain']))\n        assert (mock_kill_old_ids.call_count == 1)\n        third_logged_line = mock_log.mock_calls[2][2]['line']\n        assert (('%s bounce on %s finish' % (fake_bounce_method, fake_serviceinstance)) in third_logged_line)\n        assert (('Now running %s' % fake_marathon_jobid) in third_logged_line)\n", "label": 1}
{"function": "\n\ndef test_find_entity(self):\n    band = self.Band(name='The Furious')\n    self.session.add(band)\n    singer = self.Person(name='Paul Insane')\n    self.BandMember(band=band, person=singer, role='singer')\n    guitarist = self.Person(name='John Crazy')\n    self.BandMember(band=band, person=guitarist, role='guitar')\n    self.session.commit()\n    sing_data = dict(band_id=band.id, person_id=singer.id, role='singer')\n    guitar_data = dict(band_id=band.id, person_id=guitarist.id, role='guitar')\n    assert (utils.find_entity(band.members, self.BandMember, sing_data) is singer.band_role)\n    assert (utils.find_entity(band.members, self.BandMember, guitar_data) is guitarist.band_role)\n", "label": 0}
{"function": "\n\ndef test_multiple_numbers(self):\n    h = Hashids()\n    assert (h.encode(683, 94108, 123, 5) == 'vJvi7On9cXGtD')\n    assert (h.encode(1, 2, 3) == 'o2fXhV')\n    assert (h.encode(2, 4, 6) == 'xGhmsW')\n    assert (h.encode(99, 25) == '3lKfD')\n", "label": 0}
{"function": "\n\ndef test_pickle_unpickle_with_reoptimization():\n    mode = theano.config.mode\n    if (mode in ['DEBUG_MODE', 'DebugMode']):\n        mode = 'FAST_RUN'\n    x1 = T.fmatrix('x1')\n    x2 = T.fmatrix('x2')\n    x3 = theano.shared(numpy.ones((10, 10), dtype=floatX))\n    x4 = theano.shared(numpy.ones((10, 10), dtype=floatX))\n    y = T.sum((T.sum((T.sum(((x1 ** 2) + x2)) + x3)) + x4))\n    updates = OrderedDict()\n    updates[x3] = (x3 + 1)\n    updates[x4] = (x4 + 1)\n    f = theano.function([x1, x2], y, updates=updates, mode=mode)\n    string_pkl = pickle.dumps(f, (- 1))\n    in1 = numpy.ones((10, 10), dtype=floatX)\n    in2 = numpy.ones((10, 10), dtype=floatX)\n    default = theano.config.reoptimize_unpickled_function\n    try:\n        theano.config.reoptimize_unpickled_function = True\n        f_ = pickle.loads(string_pkl)\n        assert (f(in1, in2) == f_(in1, in2))\n    finally:\n        theano.config.reoptimize_unpickled_function = default\n", "label": 0}
{"function": "\n\ndef assert_within_tolerance(lval, rval, tolerance, message=None):\n    'Assert that the difference between the two values, as a fraction of the left value, is smaller than the tolerance specified.\\n    That is, abs(float(lval) - float(rval)) / float(lval) < tolerance'\n    real_message = (message or ('%r !~= %r' % (lval, rval)))\n    assert ((abs((float(lval) - float(rval))) / float(lval)) < tolerance), real_message\n", "label": 0}
{"function": "\n\ndef test_include_1():\n    sub_ffi = FFI()\n    sub_ffi.cdef('static const int k2 = 121212;')\n    sub_ffi.include(original_ffi)\n    assert ('macro FOOBAR' in original_ffi._parser._declarations)\n    assert ('macro FOOBAZ' in original_ffi._parser._declarations)\n    sub_ffi.set_source('re_python_pysrc', None)\n    sub_ffi.emit_python_code(str(tmpdir.join('_re_include_1.py')))\n    if (sys.version_info[:2] >= (3, 3)):\n        import importlib\n        importlib.invalidate_caches()\n    from _re_include_1 import ffi\n    assert (ffi.integer_const('FOOBAR') == (- 42))\n    assert (ffi.integer_const('FOOBAZ') == (- 43))\n    assert (ffi.integer_const('k2') == 121212)\n    lib = ffi.dlopen(extmod)\n    assert (lib.FOOBAR == (- 42))\n    assert (lib.FOOBAZ == (- 43))\n    assert (lib.k2 == 121212)\n    p = ffi.new('bar_t *', [5, b'foobar'])\n    assert (p.a[4] == ord('a'))\n", "label": 1}
{"function": "\n\ndef _do_job(self, hook_call):\n\n    @contextlib.contextmanager\n    def create_temp_dir():\n        working_dir = tempfile.mktemp()\n        shutil.copytree(self.working_dir, working_dir)\n        (yield working_dir)\n        shutil.rmtree(working_dir)\n    with SessionScope(self.db):\n        with mock.patch.object(Build, 'set_status'), mock.patch.object(DeployKey, 'ensure') as ensure_deploy_key_mock, mock.patch.object(Job, 'get_cache_id', return_value='qwerty'), mock.patch('kozmic.builds.tasks.create_temp_dir', create_temp_dir):\n            kozmic.builds.tasks.do_job(hook_call_id=hook_call.id)\n    self.db.session.rollback()\n    if hook_call.hook.project.is_public:\n        assert (not ensure_deploy_key_mock.called)\n    return Job.query.filter_by(hook_call=hook_call).first()\n", "label": 0}
{"function": "\n\ndef test_Distinct():\n    t = Symbol('t', 'var * {name: string, amount: int32}')\n    r = distinct(t['name'])\n    print(r.dshape)\n    assert (r.dshape == dshape('var * string'))\n    assert (r._name == 'name')\n    r = t.distinct()\n    assert (r.dshape == t.dshape)\n", "label": 0}
{"function": "\n\n@classmethod\ndef dbapi(cls):\n    try:\n        from pysqlite2 import dbapi2 as sqlite\n    except ImportError as e:\n        try:\n            from sqlite3 import dbapi2 as sqlite\n        except ImportError:\n            raise e\n    return sqlite\n", "label": 0}
{"function": "\n\ndef process_request(self, request):\n    if request.user.is_authenticated():\n        try:\n            profile = util.get_pybb_profile(request.user)\n        except ObjectDoesNotExist:\n            from pybb.signals import user_saved\n            user_saved(request.user, created=True)\n            profile = util.get_pybb_profile(request.user)\n        if (not profile.language):\n            profile.language = translation.get_language_from_request(request)\n            profile.save()\n        request.session['django_language'] = profile.language\n        translation.activate(profile.language)\n        request.LANGUAGE_CODE = translation.get_language()\n", "label": 0}
{"function": "\n\ndef populate(self):\n    self.page = get_page_draft(self.request.current_page)\n    if (not self.page):\n        return\n    if get_cms_setting('PERMISSION'):\n        has_global_current_page_change_permission = has_page_change_permission(self.request)\n    else:\n        has_global_current_page_change_permission = False\n    can_change = (self.request.current_page and self.request.current_page.has_change_permission(self.request))\n    if (has_global_current_page_change_permission or can_change):\n        try:\n            mypageextension = MyPageExtension.objects.get(extended_object_id=self.page.id)\n        except MyPageExtension.DoesNotExist:\n            mypageextension = None\n        try:\n            if mypageextension:\n                url = admin_reverse('extensionapp_mypageextension_change', args=(mypageextension.pk,))\n            else:\n                url = (admin_reverse('extensionapp_mypageextension_add') + ('?extended_object=%s' % self.page.pk))\n        except NoReverseMatch:\n            pass\n        else:\n            not_edit_mode = (not self.toolbar.edit_mode)\n            current_page_menu = self.toolbar.get_or_create_menu('page')\n            current_page_menu.add_modal_item(_('Page Extension'), url=url, disabled=not_edit_mode)\n", "label": 1}
{"function": "\n\ndef _get_services():\n    cookie = get_cookie(config.ADMIN_USER, config.USERS[config.ADMIN_USER]['password'])\n    about = requests.get((config.GATEWAY_URL + '/manage/about/'), headers={\n        'Accept': 'application/json',\n    }, cookies={\n        'auth_pubtkt': cookie,\n    })\n    if (about.status_code > 399):\n        return []\n    try:\n        return about.json()['service']['services']\n    except ValueError:\n        raise Exception(('Cannot get services from %s' % (config.GATEWAY_URL + '/manage/about/')))\n", "label": 0}
{"function": "\n\ndef _updateFromPlug(self):\n    plug = self.getPlug()\n    if (plug is not None):\n        with self.getContext():\n            try:\n                value = plug.getValue()\n            except:\n                value = None\n        if (value is not None):\n            with Gaffer.BlockedConnection(self.__valueChangedConnection):\n                self.__numericWidget.setValue(value)\n        self.__numericWidget.setErrored((value is None))\n        animated = Gaffer.Animation.isAnimated(plug)\n        widgetAnimated = (GafferUI._Variant.fromVariant(self.__numericWidget._qtWidget().property('gafferAnimated')) or False)\n        if (widgetAnimated != animated):\n            self.__numericWidget._qtWidget().setProperty('gafferAnimated', GafferUI._Variant.toVariant(bool(animated)))\n            self.__numericWidget._repolish()\n    self.__numericWidget.setEditable(self._editable(canEditAnimation=True))\n", "label": 0}
{"function": "\n\ndef register_metric(self, metric):\n    with self._lock:\n        if (metric.metric_name in self.metrics):\n            raise ValueError(('A metric named \"%s\" already exists, cannot register another one.' % metric.metric_name))\n        self.metrics[metric.metric_name] = metric\n        for reporter in self._reporters:\n            reporter.metric_change(metric)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef format_percent(ratio, denom=1, unit=False):\n    try:\n        ratio /= float(denom)\n    except ZeroDivisionError:\n        ratio = 0\n    if (round(ratio, 2) >= 1):\n        precision = 0\n    elif (round(ratio, 2) >= 0.1):\n        precision = 1\n    else:\n        precision = 2\n    string = (('{:.' + str(precision)) + 'f}').format((ratio * 100))\n    if unit:\n        return (string + '%')\n    else:\n        return string\n", "label": 0}
{"function": "\n\ndef update(self, req, id, body):\n    context = req.environ['nova.context']\n    authorize(context)\n    try:\n        utils.check_string_length(id, 'quota_class_name', min_length=1, max_length=255)\n    except exception.InvalidInput as e:\n        raise webob.exc.HTTPBadRequest(explanation=e.format_message())\n    quota_class = id\n    bad_keys = []\n    if (not self.is_valid_body(body, 'quota_class_set')):\n        msg = _('quota_class_set not specified')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    quota_class_set = body['quota_class_set']\n    for key in quota_class_set.keys():\n        if (key not in self.supported_quotas):\n            bad_keys.append(key)\n            continue\n        try:\n            body['quota_class_set'][key] = utils.validate_integer(body['quota_class_set'][key], key, max_value=db.MAX_INT)\n        except exception.InvalidInput as e:\n            raise webob.exc.HTTPBadRequest(explanation=e.format_message())\n    if bad_keys:\n        msg = (_('Bad key(s) %s in quota_set') % ','.join(bad_keys))\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        nova.context.require_admin_context(context)\n    except exception.AdminRequired:\n        raise webob.exc.HTTPForbidden()\n    for (key, value) in quota_class_set.items():\n        try:\n            db.quota_class_update(context, quota_class, key, value)\n        except exception.QuotaClassNotFound:\n            db.quota_class_create(context, quota_class, key, value)\n    values = QUOTAS.get_class_quotas(context, quota_class)\n    return self._format_quota_set(None, values)\n", "label": 1}
{"function": "\n\ndef test_output_skip(capsys, dst):\n    render(dst)\n    (out, err) = capsys.readouterr()\n    render(dst, quiet=False, skip=True)\n    (out, err) = capsys.readouterr()\n    print(out)\n    assert re.search('conflict[^\\\\s]+  config\\\\.py', out)\n    assert re.search('skip[^\\\\s]+  config\\\\.py', out)\n    assert re.search('identical[^\\\\s]+  setup\\\\.py', out)\n    assert re.search('identical[^\\\\s]+  doc/images/nslogo\\\\.gif', out)\n", "label": 0}
{"function": "\n\ndef __call__(self, parser, namespace, value, option_string=None):\n    try:\n        assert (self.dest == 'spam'), ('dest: %s' % self.dest)\n        assert (option_string == '-s'), ('flag: %s' % option_string)\n        expected_ns = NS(spam=0.25)\n        if (value in [0.125, 0.625]):\n            expected_ns.badger = 2\n        elif (value in [2.0]):\n            expected_ns.badger = 84\n        else:\n            raise AssertionError(('value: %s' % value))\n        assert (expected_ns == namespace), ('expected %s, got %s' % (expected_ns, namespace))\n    except AssertionError:\n        e = sys.exc_info()[1]\n        raise ArgumentParserError(('opt_action failed: %s' % e))\n    setattr(namespace, 'spam', value)\n", "label": 0}
{"function": "\n\ndef __init__(self, machinesInCluster, ownAddress, testId, testDir, timeout=None):\n    self.machinesInCluster = machinesInCluster\n    self.ownAddress = ownAddress\n    assert (self.ownAddress in self.machinesInCluster)\n    self.testId = testId\n    self.testDir = testDir\n    self.timeout = timeout\n    assert os.path.exists(self.testDir), (\"test directory '%s' does not exist\" % self.testDir)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    for (name, value) in self._iter_contents():\n        if (key == name):\n            break\n    else:\n        raise KeyError(('Folder entry %s not found' % repr(key)))\n    if (value == '/'):\n        qname = quote(name)\n        child_ls = self.sub_tree[(qname + '.ls')]\n        try:\n            child_sub = self.sub_tree[(qname + '.sub')]\n        except KeyError:\n            child_sub = self.sub_tree.new_tree((qname + '.sub'))\n            self.storage._autocommit()\n        return StorageDir(name, child_ls, child_sub, ((self.path + name) + '/'), self.storage, self)\n    else:\n        inode = self.storage.get_inode(value)\n        return StorageFile(name, inode, self)\n", "label": 0}
{"function": "\n\ndef get_image_service(image_href, client=None, version=1, context=None):\n    'Get image service instance to download the image.\\n\\n    :param image_href: String containing href to get image service for.\\n    :param client: Glance client to be used for download, used only if\\n        image_href is Glance href.\\n    :param version: Version of Glance API to use, used only if image_href is\\n        Glance href.\\n    :param context: request context, used only if image_href is Glance href.\\n    :raises: exception.ImageRefValidationFailed if no image service can\\n        handle specified href.\\n    :returns: Instance of an image service class that is able to download\\n        specified image.\\n    '\n    scheme = urlparse.urlparse(image_href).scheme.lower()\n    try:\n        cls = protocol_mapping[(scheme or 'glance')]\n    except KeyError:\n        raise exception.ImageRefValidationFailed(image_href=image_href, reason=(_('Image download protocol %s is not supported.') % scheme))\n    if (cls == GlanceImageService):\n        return cls(client, version, context)\n    return cls()\n", "label": 0}
{"function": "\n\n@pytest.mark.integration\n@pytest.mark.django_db(transaction=True)\ndef test_variant_discounts(product_variant):\n    product = product_variant.product\n    low_discount = Sale.objects.create(type=Sale.FIXED, value=5)\n    low_discount.products.add(product)\n    discount = Sale.objects.create(type=Sale.FIXED, value=8)\n    discount.products.add(product)\n    high_discount = Sale.objects.create(type=Sale.FIXED, value=50)\n    high_discount.products.add(product)\n    final_price = product_variant.get_price_per_item(discounts=Sale.objects.all())\n    assert (final_price.gross == 0)\n    applied_discount = final_price.history.right\n    assert isinstance(applied_discount, FixedDiscount)\n    assert (applied_discount.amount.gross == 50)\n", "label": 0}
{"function": "\n\ndef main(argv=None):\n    if (argv is not None):\n        saved_argv = sys.argv[:]\n        sys.argv[1:] = argv\n    if (sys.platform == 'darwin'):\n        import setuptools\n        Plist = dict(CFBundleName='nbopen', CFBundleShortVersionString='0.2', CFBundleVersion='0.2', CFBundleIdentifier='org.jupyter.nbopen', CFBundleDevelopmentRegion='English', CFBundleDocumentTypes=[dict(CFBundleTypeExtensions=['ipynb'], CFBundleTypeName='IPython Notebook', CFBundleTypeIconFile='nbopen', CFBundleTypeRole='Editor')])\n        extra_options = dict(app=['nbopen.py'], options={\n            'py2app': {\n                'argv_emulation': True,\n                'packages': ['nbopen'],\n                'alias': True,\n                'plist': Plist,\n                'iconfile': 'nbopen.icns',\n            },\n        }, setup_requires=['py2app'])\n    else:\n        extra_options = {\n            \n        }\n    try:\n        with open('README.rst') as f:\n            readme = f.read()\n        setup(name='nbopen', version='0.3', description='Open a notebook from the command line in the best available server', long_description=readme, author='Thomas Kluyver', author_email='thomas@kluyver.me.uk', url='https://github.com/takluyver/nbopen', py_modules=['nbopen'], scripts=['nbopen'], classifiers=['Framework :: IPython', 'License :: OSI Approved :: BSD License', 'Programming Language :: Python :: 2', 'Programming Language :: Python :: 3'], **extra_options)\n    finally:\n        if (argv is not None):\n            sys.argv = saved_argv\n", "label": 0}
{"function": "\n\ndef test_execute_mutations():\n    query = '\\n    mutation M{\\n      first: changeNumber {\\n        result\\n      },\\n      second: changeNumber {\\n        result\\n      }\\n      third: changeNumber(to: 5) {\\n        result\\n      }\\n    }\\n    '\n    expected = {\n        'first': {\n            'result': '1',\n        },\n        'second': {\n            'result': '2',\n        },\n        'third': {\n            'result': '5',\n        },\n    }\n    result = schema.execute(query, root=object())\n    assert (not result.errors)\n    assert (result.data == expected)\n", "label": 0}
{"function": "\n\ndef domain_recipients_valid(domain_recipients=[]):\n    'Confirm that the first email recipient @smtp_server_domain could correspond to a valid project (i.e., it a new project or an int) and return it'\n    result = None\n    try:\n        if ((domain_recipients[0] in action_mailboxes.keys()) or (domain_recipients[0] in pass_through_mailboxes) or (default_mailbox is not None)):\n            result = domain_recipients[0]\n    except IndexError:\n        pass\n    return result\n", "label": 0}
{"function": "\n\ndef test_list_insert_bin_is_none(self):\n    '\\n        Invoke list_insert() with bin is none\\n        '\n    key = ('test', 'demo', 1)\n    try:\n        TestListInsert.client.list_insert(key, None, 2, 'str')\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'Bin name should be of type string')\n", "label": 0}
{"function": "\n\ndef _LoadArtifactsFromDatastore(self, source_urns=None, token=None, overwrite_if_exists=True):\n    'Load artifacts from the data store.'\n    if (token is None):\n        token = access_control.ACLToken(username='GRRArtifactRegistry', reason='Managing Artifacts.')\n    loaded_artifacts = []\n    for artifact_coll_urn in (source_urns or []):\n        with aff4.FACTORY.Create(artifact_coll_urn, aff4_type='RDFValueCollection', token=token, mode='rw') as artifact_coll:\n            for artifact_value in artifact_coll:\n                self.RegisterArtifact(artifact_value, source=('datastore:%s' % artifact_coll_urn), overwrite_if_exists=overwrite_if_exists)\n                loaded_artifacts.append(artifact_value)\n                logging.debug('Loaded artifact %s from %s', artifact_value.name, artifact_coll_urn)\n    revalidate = True\n    while revalidate:\n        revalidate = False\n        for artifact_obj in loaded_artifacts[:]:\n            try:\n                artifact_obj.Validate()\n            except ArtifactDefinitionError as e:\n                logging.error('Artifact %s did not validate: %s', artifact_obj.name, e)\n                artifact_obj.error_message = utils.SmartStr(e)\n                loaded_artifacts.remove(artifact_obj)\n                revalidate = True\n", "label": 1}
{"function": "\n\ndef kmeans_init(X, Y, n_labels, n_hidden_states, latent_node_features=False):\n    all_feats = []\n    for (x, y) in zip(X, Y):\n        if (len(x) == 3):\n            (features, edges, n_hidden) = x\n        elif (len(x) == 4):\n            (features, edges, _, n_hidden) = x\n        else:\n            raise ValueError('Something is fishy!')\n        n_visible = features.shape[0]\n        if latent_node_features:\n            n_visible -= n_hidden\n        if (np.max(edges) != ((n_hidden + n_visible) - 1)):\n            raise ValueError(\"Edges don't add up\")\n        labels_one_hot = np.zeros((n_visible, n_labels), dtype=np.int)\n        y = y.ravel()\n        gx = np.ogrid[:n_visible]\n        labels_one_hot[(gx, y)] = 1\n        graph = sparse.coo_matrix((np.ones(edges.shape[0]), edges.T), ((n_visible + n_hidden), (n_visible + n_hidden)))\n        graph = (graph + graph.T)[(- n_hidden):, :n_visible]\n        neighbors = (graph * labels_one_hot.reshape(n_visible, (- 1)))\n        neighbors /= np.maximum(neighbors.sum(axis=1)[:, np.newaxis], 1)\n        all_feats.append(neighbors)\n    all_feats_stacked = np.vstack(all_feats)\n    try:\n        km = KMeans(n_clusters=n_hidden_states)\n    except TypeError:\n        km = KMeans(k=n_hidden_states)\n    km.fit(all_feats_stacked)\n    H = []\n    for (y, feats) in zip(Y, all_feats):\n        H.append(np.hstack([y, (km.predict(feats) + n_labels)]))\n    return H\n", "label": 1}
{"function": "\n\ndef test_filter_updates(self):\n    self.log_creates(10)\n    self.log_updates(10)\n    doc = self.get_pq()\n    assert (len(doc('.item')) == 20)\n    doc = self.get_pq(action='updates')\n    assert (len(doc('.item')) == 10)\n", "label": 0}
{"function": "\n\ndef list(region=None, key=None, keyid=None, profile=None):\n    '\\n    List all buckets owned by the authenticated sender of the request.\\n\\n    Returns list of buckets\\n\\n    CLI Example:\\n\\n    .. code-block:: yaml\\n\\n        Owner: {...}\\n        Buckets:\\n          - {...}\\n          - {...}\\n\\n    '\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        buckets = conn.list_buckets()\n        if (not bool(buckets.get('Buckets'))):\n            log.warning('No buckets found')\n        del buckets['ResponseMetadata']\n        return buckets\n    except ClientError as e:\n        return {\n            'error': __utils__['boto3.get_error'](e),\n        }\n", "label": 0}
{"function": "\n\ndef send_status(test_result, status_url, repo_token, pending=False):\n    if (status_url and repo_token):\n        commit_id = status_url.rstrip('/').split('/')[(- 1)]\n        log_url = (cfg.CONF.worker.log_url_prefix + commit_id)\n        headers = {\n            'Authorization': ('token ' + repo_token),\n            'Content-Type': 'application/json',\n        }\n        if pending:\n            data = {\n                'state': 'pending',\n                'description': 'Solum says: Testing in progress',\n                'target_url': log_url,\n            }\n        elif (test_result == 0):\n            data = {\n                'state': 'success',\n                'description': 'Solum says: Tests passed',\n                'target_url': log_url,\n            }\n        else:\n            data = {\n                'state': 'failure',\n                'description': 'Solum says: Tests failed',\n                'target_url': log_url,\n            }\n        try:\n            conn = get_http_connection()\n            (resp, _) = conn.request(status_url, 'POST', headers=headers, body=json.dumps(data))\n            if (resp['status'] != '201'):\n                LOG.debug(('Failed to send back status. Error code %s,status_url %s, repo_token %s' % (resp['status'], status_url, repo_token)))\n        except (httplib2.HttpLib2Error, socket.error) as ex:\n            LOG.warning(('Error in sending status, status url: %s, repo token: %s, error: %s' % (status_url, repo_token, ex)))\n    else:\n        LOG.debug('No url or token available to send back status')\n", "label": 0}
{"function": "\n\ndef get_container(self, container_name):\n    try:\n        response = self.connection.request(('/%s' % container_name), method='HEAD')\n        if (response.status == httplib.NOT_FOUND):\n            raise ContainerDoesNotExistError(value=None, driver=self, container_name=container_name)\n    except InvalidCredsError:\n        pass\n    return Container(name=container_name, extra=None, driver=self)\n", "label": 0}
{"function": "\n\ndef do_migration(records):\n    database['boxnodesettings'].update({\n        'user_settings': {\n            '$type': 2,\n        },\n    }, {\n        '$rename': {\n            'user_settings': 'foreign_user_settings',\n        },\n    }, multi=True)\n    for user_addon in records:\n        user = user_addon.owner\n        old_account = user_addon.oauth_settings\n        logger.info('Record found for user {}'.format(user._id))\n        try:\n            account = ExternalAccount(provider='box', provider_name='Box', display_name=old_account.username, oauth_key=old_account.access_token, refresh_token=old_account.refresh_token, provider_id=old_account.user_id, expires_at=old_account.expires_at)\n            account.save()\n        except KeyExistsException:\n            account = ExternalAccount.find_one((Q('provider', 'eq', 'box') & Q('provider_id', 'eq', old_account.user_id)))\n            assert (account is not None)\n        user.external_accounts.append(account)\n        user.save()\n        user_addon.oauth_settings = None\n        user_addon.save()\n        logger.info('Added external account {0} to user {1}'.format(account._id, user._id))\n    for node in BoxNodeSettings.find():\n        if (node.foreign_user_settings is None):\n            continue\n        logger.info('Migrating user_settings for box {}'.format(node._id))\n        node.user_settings = node.foreign_user_settings\n        node.save()\n", "label": 0}
{"function": "\n\n@db.validates('email', include_removes=True)\ndef validate_email(self, key, email, is_remove):\n    if is_remove:\n        raise ValueError('Not allowed to remove eamil')\n    assert validate_email(email)\n    assert (sanitize_string(email, extra_chars='.@') == email)\n    return email\n", "label": 0}
{"function": "\n\ndef cast(self, expr, t):\n    if (expr.type == t):\n        return expr\n    assert isinstance(t, ScalarT), (\"Can't cast %s : %s to non-scalar type %s\" % (expr, expr.type, t))\n    return self.assign_name(Cast(expr, type=t), ('cast_%s' % t))\n", "label": 0}
{"function": "\n\ndef test_validate_invite_code_valid(self, user):\n    form = RegisterForm(username='newusername', email='new2@test.test', password='example', confirm='example', invite_code='bad')\n    assert (form.validate() is False)\n    assert ('Invite Code not recognized.' in form.invite_code.errors)\n", "label": 0}
{"function": "\n\ndef test_hub(db):\n    hub = orm.Hub(server=orm.Server(ip='1.2.3.4', port=1234, base_url='/hubtest/'))\n    db.add(hub)\n    db.commit()\n    assert (hub.server.ip == '1.2.3.4')\n    (hub.server.port == 1234)\n    assert (hub.api_url == 'http://1.2.3.4:1234/hubtest/api')\n", "label": 0}
{"function": "\n\ndef test_certonly_bad_args(self):\n    try:\n        self._call(['-a', 'bad_auth', 'certonly'])\n        assert False, 'Exception should have been raised'\n    except errors.PluginSelectionError as e:\n        self.assertTrue(('The requested bad_auth plugin does not appear' in e.message))\n", "label": 0}
{"function": "\n\n@login_required()\ndef submit_r_assignment_answer(request, course_id, assignment_id):\n    if request.is_ajax():\n        if (request.method == 'POST'):\n            question_id = int(request.POST['question_id'])\n            answer = request.POST['answer']\n            course = Course.objects.get(id=course_id)\n            assignment = Assignment.objects.get(assignment_id=assignment_id)\n            student = Student.objects.get(user=request.user)\n            try:\n                question = ResponseQuestion.objects.get(assignment=assignment, question_id=question_id)\n            except ResponseQuestion.DoesNotExist:\n                response_data = {\n                    'status': 'failed',\n                    'message': 'cannot find question',\n                }\n                return HttpResponse(json.dumps(response_data), content_type='application/json')\n            try:\n                submission = ResponseSubmission.objects.get(student=student, question_id=question_id)\n            except ResponseSubmission.DoesNotExist:\n                submission = ResponseSubmission.objects.create(student=student, question_id=question_id)\n            submission.answer = answer\n            submission.save()\n            response_data = {\n                'status': 'success',\n                'message': 'submitted',\n            }\n    return HttpResponse(json.dumps(response_data), content_type='application/json')\n", "label": 0}
{"function": "\n\ndef safely_reserve_a_username(cursor, gen_usernames=gen_random_usernames, reserve=insert_into_participants):\n    'Safely reserve a username.\\n\\n    :param cursor: a :py:class:`psycopg2.cursor` managed as a :py:mod:`postgres`\\n        transaction\\n    :param gen_usernames: a generator of usernames to try\\n    :param reserve: a function that takes the cursor and does the SQL\\n        stuff\\n    :database: one ``INSERT`` on average\\n    :returns: a 12-hex-digit unicode\\n    :raises: :py:class:`FailedToReserveUsername` if no acceptable username is found\\n        within 100 attempts, or :py:class:`RanOutOfUsernameAttempts` if the username\\n        generator runs out first\\n\\n    The returned value is guaranteed to have been reserved in the database.\\n\\n    '\n    cursor.execute('SAVEPOINT safely_reserve_a_username')\n    seatbelt = 0\n    for username in gen_usernames():\n        seatbelt += 1\n        if (seatbelt > 100):\n            raise FailedToReserveUsername\n        try:\n            check = reserve(cursor, username)\n        except IntegrityError:\n            cursor.execute('ROLLBACK TO safely_reserve_a_username')\n            continue\n        else:\n            assert (check == username)\n            break\n    else:\n        raise RanOutOfUsernameAttempts\n    cursor.execute('RELEASE safely_reserve_a_username')\n    return username\n", "label": 0}
{"function": "\n\ndef execute(self, object, *multiparams, **params):\n    'Executes the a SQL statement construct and returns a\\n        :class:`.ResultProxy`.\\n\\n        :param object: The statement to be executed.  May be\\n         one of:\\n\\n         * a plain string\\n         * any :class:`.ClauseElement` construct that is also\\n           a subclass of :class:`.Executable`, such as a\\n           :func:`~.expression.select` construct\\n         * a :class:`.FunctionElement`, such as that generated\\n           by :attr:`.func`, will be automatically wrapped in\\n           a SELECT statement, which is then executed.\\n         * a :class:`.DDLElement` object\\n         * a :class:`.DefaultGenerator` object\\n         * a :class:`.Compiled` object\\n\\n        :param \\\\*multiparams/\\\\**params: represent bound parameter\\n         values to be used in the execution.   Typically,\\n         the format is either a collection of one or more\\n         dictionaries passed to \\\\*multiparams::\\n\\n             conn.execute(\\n                 table.insert(),\\n                 {\"id\":1, \"value\":\"v1\"},\\n                 {\"id\":2, \"value\":\"v2\"}\\n             )\\n\\n         ...or individual key/values interpreted by \\\\**params::\\n\\n             conn.execute(\\n                 table.insert(), id=1, value=\"v1\"\\n             )\\n\\n         In the case that a plain SQL string is passed, and the underlying\\n         DBAPI accepts positional bind parameters, a collection of tuples\\n         or individual values in \\\\*multiparams may be passed::\\n\\n             conn.execute(\\n                 \"INSERT INTO table (id, value) VALUES (?, ?)\",\\n                 (1, \"v1\"), (2, \"v2\")\\n             )\\n\\n             conn.execute(\\n                 \"INSERT INTO table (id, value) VALUES (?, ?)\",\\n                 1, \"v1\"\\n             )\\n\\n         Note above, the usage of a question mark \"?\" or other\\n         symbol is contingent upon the \"paramstyle\" accepted by the DBAPI\\n         in use, which may be any of \"qmark\", \"named\", \"pyformat\", \"format\",\\n         \"numeric\".   See `pep-249 <http://www.python.org/dev/peps/pep-0249/>`_\\n         for details on paramstyle.\\n\\n         To execute a textual SQL statement which uses bound parameters in a\\n         DBAPI-agnostic way, use the :func:`~.expression.text` construct.\\n\\n        '\n    if isinstance(object, util.string_types[0]):\n        return self._execute_text(object, multiparams, params)\n    try:\n        meth = object._execute_on_connection\n    except AttributeError:\n        raise exc.InvalidRequestError(('Unexecutable object type: %s' % type(object)))\n    else:\n        return meth(self, multiparams, params)\n", "label": 0}
{"function": "\n\ndef test_msgpack_object_stream_decode(self, rpc):\n    rpc.handshake()\n    result = rpc.call('members')\n    assert (result.head == {\n        b'Error': b'',\n        b'Seq': 1,\n    })\n    assert (b'Members' in result.body.keys())\n", "label": 0}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01, gamma=10, cof=4000):\n    ''\n    samplerate = self.audio.samplerate\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for y in super(Y4, self).walk(N, freq_base, freq_max, start, end, each=False, combine=False, gamma=gamma, cof=cof):\n        y = np.maximum(0, y)\n        if (not combine):\n            if each:\n                for v in y:\n                    (yield v)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\ndef send_data(sock, data, address=None):\n    if (address is None):\n        _rep = []\n    else:\n        _rep = list(address)\n    for a in data:\n        if isinstance(a, str):\n            _rep.append(a.encode('utf-8'))\n        else:\n            _rep.append(a)\n    while True:\n        try:\n            sock.send_multipart(_rep, zmq.NOBLOCK)\n        except zmq.ZMQError as e:\n            if (e.errno == errno.EAGAIN):\n                gethub().do_write(sock)\n                continue\n            elif (e.errno == errno.EINTR):\n                continue\n            else:\n                raise\n        else:\n            break\n", "label": 1}
{"function": "\n\ndef remove_user_permissions(self, email, perm):\n    ' Revokes a capability from the specified user.\\n\\n    Args:\\n      email: A str containing the e-mail address of the user who we wish to\\n        remove a permission from.\\n      perm: A str containing the name of the permission to remove from the user.\\n    Returns:\\n      True if the permission was removed from the user, and False otherwise.\\n    '\n    try:\n        caps_list = self.get_user_capabilities(email)\n        uas = self.get_uaserver()\n        if (perm in caps_list):\n            caps_list.remove(perm)\n        else:\n            return True\n        ret = uas.set_capabilities(email, self.USER_CAPABILITIES_DELIMITER.join(caps_list), GLOBAL_SECRET_KEY)\n        if (ret == 'true'):\n            self.cache['user_caps'][email] = caps_list\n            return True\n        else:\n            logging.error('remove_user_permissions returned: {0}'.format(ret))\n            return False\n    except Exception as err:\n        logging.exception(err)\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef validate_int(input, min_value, max_value):\n    try:\n        number = int(input)\n    except ValueError:\n        raise UsageError('Input must be an integer')\n    if (min_value <= number <= max_value):\n        return number\n    else:\n        raise UsageError('Input must be between {} and {}'.format(min_value, max_value))\n", "label": 0}
{"function": "\n\ndef test_quote():\n    literals = [[1, 2, 3], (add, 1, 2), [1, [2, 3]], (add, 1, (add, 2, 3))]\n    for l in literals:\n        assert (core.get({\n            'x': quote(l),\n        }, 'x') == l)\n", "label": 0}
{"function": "\n\ndef test_renderer_override(app, client):\n    configure_app(app, overrides={\n        'renderers': OrderedDict([('text/html', dummy_renderer)]),\n    }, setup=True)\n    res = client.get('/hello')\n    assert (res.content_type == 'text/html')\n    assert (res.body == b'<p>Hello world</p>')\n", "label": 0}
{"function": "\n\ndef test_chunked(self):\n    chunked_start = 'HTTP/1.1 200 OK\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\na\\r\\nhello worl\\r\\n1\\r\\nd\\r\\n'\n    sock = FakeSocket((chunked_start + '0\\r\\n'))\n    resp = client.HTTPResponse(sock, method='GET')\n    resp.begin()\n    self.assertEqual(resp.read(), b'hello world')\n    resp.close()\n    for x in ('', 'foo\\r\\n'):\n        sock = FakeSocket((chunked_start + x))\n        resp = client.HTTPResponse(sock, method='GET')\n        resp.begin()\n        try:\n            resp.read()\n        except client.IncompleteRead as i:\n            self.assertEqual(i.partial, b'hello world')\n            self.assertEqual(repr(i), 'IncompleteRead(11 bytes read)')\n            self.assertEqual(str(i), 'IncompleteRead(11 bytes read)')\n        else:\n            self.fail('IncompleteRead expected')\n        finally:\n            resp.close()\n", "label": 0}
{"function": "\n\ndef log(self, message, args=None, level=logging.INFO, sublogger=None):\n    if (args is None):\n        args = []\n    module_name = inspect.getmodule(inspect.stack()[1][0]).__name__\n    logger_name = module_name\n    if sublogger:\n        logger_name = '{module_name}:{sublogger}'.format(module_name=module_name, sublogger=sublogger)\n    logger.log(level, message, *args)\n    with io.open(self.log_path, 'a', encoding='utf-8') as log_file:\n        log_file.write(six.text_type('{date}\\t{level}\\t{module}\\t{message}\\n'.format(date=datetime.datetime.utcnow().isoformat(), level=logging.getLevelName(level), module=logger_name, message=(message % args).replace('\\n', '\\\\n'))))\n    if ((level >= logging.INFO) and (not self.quiet)):\n        print(('[%s %s] %s' % (logging.getLevelName(level), self.issue, (message % args))))\n", "label": 0}
{"function": "\n\ndef test_cache_invalidate_on_save(self):\n    manager = global_preferences_registry.manager()\n    model_instance = manager.create_db_pref(section=None, name='no_section', value=False)\n    with self.assertNumQueries(0):\n        assert (not manager['no_section'])\n        manager['no_section']\n    model_instance.value = True\n    model_instance.save()\n    with self.assertNumQueries(0):\n        assert manager['no_section']\n        manager['no_section']\n", "label": 0}
{"function": "\n\ndef test_initialization_legacy(self, NeuralNet):\n    input = Mock(__name__='InputLayer', __bases__=(InputLayer,))\n    (hidden1, hidden2, output) = [Mock(__name__='MockLayer', __bases__=(Layer,)) for i in range(3)]\n    nn = NeuralNet(layers=[('input', input), ('hidden1', hidden1), ('hidden2', hidden2), ('output', output)], input_shape=(10, 10), hidden1_some='param')\n    out = nn.initialize_layers(nn.layers)\n    input.assert_called_with(name='input', shape=(10, 10))\n    assert (nn.layers_['input'] is input.return_value)\n    hidden1.assert_called_with(incoming=input.return_value, name='hidden1', some='param')\n    assert (nn.layers_['hidden1'] is hidden1.return_value)\n    hidden2.assert_called_with(incoming=hidden1.return_value, name='hidden2')\n    assert (nn.layers_['hidden2'] is hidden2.return_value)\n    output.assert_called_with(incoming=hidden2.return_value, name='output')\n    assert (out is nn.layers_['output'])\n", "label": 0}
{"function": "\n\n@ValidateLarchPlugin\ndef _more(fname, pagelength=32, _larch=None, **kws):\n    \"list file contents:\\n    > more('file.txt')\\nby default, the file is shown 32 lines at a time.\\nYou can specify the number of lines to show at a time\\nwith the  pagelength option:\\n    > more('file.txt', pagelength=10)\\n    \"\n    output = _larch.writer.write\n    if (not os.path.exists(fname)):\n        output((\"File '%s' not found.\\n\" % fname))\n        return\n    elif (not os.path.isfile(fname)):\n        output((\"'%s' not a file.\\n\" % fname))\n        return\n    try:\n        text = open(fname, 'r').readlines()\n    except IOError:\n        output(('cannot open file: %s\\n' % fname))\n        return\n    show_more(text, filename=fname, _larch=_larch, pagelength=pagelength, **kws)\n", "label": 0}
{"function": "\n\ndef test_slice_start_clamps_to_max(self):\n    wrapper = paginate._ElasticsearchWrapper(FakeQuery([1, 2, 3, 4, 5, 6]))\n    wrapper.max_results = 5\n    assert (wrapper[6:10] == [])\n    assert (len(wrapper) == 5)\n", "label": 0}
{"function": "\n\ndef test_indexSearch(self, dataFrame):\n    datasearch = DataSearch('Test', dataFrame=dataFrame)\n    filterString = 'indexSearch([0])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 1)\n    filterString = 'indexSearch([0, 2])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 2)\n    filterString = 'indexSearch([0, 1, 2])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 3)\n    filterString = 'indexSearch([99])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 0)\n", "label": 1}
{"function": "\n\ndef lineReceived(self, line):\n    line = line.strip()\n    self.log.debug('[sref:%s] Received line: %s', self.sessionRef, line)\n    (cmd, arg, line) = self.parseline(line)\n    if (self.sessionLineCallback is not None):\n        return self.sessionLineCallback(cmd, arg, line)\n    if (not line):\n        return self.sendData()\n    if ((cmd is None) or (cmd not in self.findCommands())):\n        return self.default(line)\n    funcName = ('do_' + cmd)\n    try:\n        func = getattr(self, funcName)\n    except AttributeError:\n        return self.default(line)\n    self.log.debug('[sref:%s] Running %s with arg:%s', self.sessionRef, funcName, arg)\n    return func(arg)\n", "label": 0}
{"function": "\n\ndef pre_build_check():\n    '\\n    Try to verify build tools\\n    '\n    if os.environ.get('CASS_DRIVER_NO_PRE_BUILD_CHECK'):\n        return True\n    try:\n        from distutils.ccompiler import new_compiler\n        from distutils.sysconfig import customize_compiler\n        from distutils.dist import Distribution\n        be = build_ext(Distribution())\n        be.initialize_options()\n        be.finalize_options()\n        have_python_include = any((os.path.isfile(os.path.join(p, 'Python.h')) for p in be.include_dirs))\n        if (not have_python_include):\n            sys.stderr.write((\"Did not find 'Python.h' in %s.\\n\" % (be.include_dirs,)))\n            return False\n        compiler = new_compiler(compiler=be.compiler)\n        customize_compiler(compiler)\n        executables = []\n        if (compiler.compiler_type in ('unix', 'cygwin')):\n            executables = [compiler.executables[exe][0] for exe in ('compiler_so', 'linker_so')]\n        elif (compiler.compiler_type == 'nt'):\n            executables = [getattr(compiler, exe) for exe in ('cc', 'linker')]\n        if executables:\n            from distutils.spawn import find_executable\n            for exe in executables:\n                if (not find_executable(exe)):\n                    sys.stderr.write(('Failed to find %s for compiler type %s.\\n' % (exe, compiler.compiler_type)))\n                    return False\n    except Exception as exc:\n        sys.stderr.write(('%s\\n' % str(exc)))\n        sys.stderr.write('Failed pre-build check. Attempting anyway.\\n')\n    return True\n", "label": 1}
{"function": "\n\ndef doNick(self, msg):\n    'Handles NICK messages.'\n    if (msg.nick == self.nick):\n        newNick = msg.args[0]\n        self.nick = newNick\n        (nick, user, domain) = ircutils.splitHostmask(msg.prefix)\n        self.prefix = ircutils.joinHostmask(self.nick, user, domain)\n    elif conf.supybot.followIdentificationThroughNickChanges():\n        try:\n            id = ircdb.users.getUserId(msg.prefix)\n            u = ircdb.users.getUser(id)\n        except KeyError:\n            return\n        if u.auth:\n            (_, user, host) = ircutils.splitHostmask(msg.prefix)\n            newhostmask = ircutils.joinHostmask(msg.args[0], user, host)\n            for (i, (when, authmask)) in enumerate(u.auth[:]):\n                if ircutils.strEqual(msg.prefix, authmask):\n                    log.info('Following identification for %s: %s -> %s', u.name, authmask, newhostmask)\n                    u.auth[i] = (u.auth[i][0], newhostmask)\n                    ircdb.users.setUser(u)\n", "label": 0}
{"function": "\n\ndef acquire(self, blocking=True):\n    \"Must be used with 'yield' as 'yield cv.acquire()'.\\n        \"\n    coro = AsynCoro.cur_coro()\n    if (self._owner == coro):\n        self._depth += 1\n        raise StopIteration(True)\n    if ((not blocking) and (self._owner is not None)):\n        raise StopIteration(False)\n    while (self._owner is not None):\n        self._waitlist.append(coro)\n        (yield coro._await_())\n    assert (self._depth == 0)\n    self._owner = coro\n    self._depth = 1\n    raise StopIteration(True)\n", "label": 0}
{"function": "\n\ndef Configure(self, *args, **kw):\n    nargs = [self]\n    if args:\n        nargs = (nargs + self.subst_list(args)[0])\n    nkw = self.subst_kw(kw)\n    nkw['_depth'] = (kw.get('_depth', 0) + 1)\n    try:\n        nkw['custom_tests'] = self.subst_kw(nkw['custom_tests'])\n    except KeyError:\n        pass\n    return SCons.SConf.SConf(*nargs, **nkw)\n", "label": 0}
{"function": "\n\ndef display_help(self, parser=None):\n    if (not parser):\n        parser = self.parser\n    for opt_str in parser.options_to_hide_from_help:\n        try:\n            parser.remove_option(opt_str)\n        except ValueError:\n            environment.error((\"Option %s added for hiding, but it's not in parser...?\" % opt_str))\n    name = self.name\n    print_(('\\n%s' % name))\n    print_(('-' * len(name)))\n    parser.print_help()\n    print_()\n    print_(self.__doc__)\n    print_()\n", "label": 0}
{"function": "\n\ndef __iter__(self):\n    protocol = self.protocol\n    source = write_source_from_arg(self.source)\n    with source.open('wb') as f:\n        it = iter(self.table)\n        hdr = next(it)\n        if self.write_header:\n            pickle.dump(hdr, f, protocol)\n        (yield tuple(hdr))\n        for row in it:\n            pickle.dump(row, f, protocol)\n            (yield tuple(row))\n", "label": 0}
{"function": "\n\ndef testOutsiderNeedsOldAndNew(self):\n    (dt, db, cu) = self.init()\n    dep = parseDep('soname: ELF32/libtest.so.1(flag)')\n    reqTrv1 = self.reqTrove('test-req', dep, version='1.0-1-1')\n    prvTrv1 = self.prvTrove('test-prov', dep, version='1.0-1-1')\n    prvTrv2 = self.prvTrove('test-prov2', dep, version='1.0-1-1')\n    troveInfo = db.addTrove(prvTrv1)\n    db.addTroveDone(troveInfo)\n    troveInfo = db.addTrove(reqTrv1)\n    db.addTroveDone(troveInfo)\n    db.commit()\n    jobInfo = self.createJobInfo(db, (prvTrv1, None), (None, prvTrv2))\n    (broken, byErase, order) = self.check(*jobInfo, findOrdering=True)\n    assert ((not broken) and (not byErase))\n    assert (len(order) == 1)\n", "label": 0}
{"function": "\n\ndef _verbose(self, msg, *args, **kwargs):\n    if self.context.verbosity:\n        skip_sub_command = kwargs.get('skip_sub_command', False)\n        with self.context.io_manager.with_debug(skip_sub_command=skip_sub_command) as fp:\n            try:\n                fp.write(('VERBOSE %.02f ' % (time.time() - self.context.original_begin)))\n                fp.write((msg % args))\n            except TypeError as err:\n                raise TypeError(('%s: %r %r' % (err, msg, args)))\n            fp.write('\\n')\n            fp.flush()\n", "label": 0}
{"function": "\n\n@float32_floatX\ndef test_correctness():\n    '\\n    Test the forward pass Op against theano graph implementation\\n    '\n    rng = np.random.RandomState([2012, 7, 19])\n    batch_size_list = [1, 5]\n    channels = 16\n    rows_list = [2, 24]\n    pool_rows_list = [2, 3]\n    for batch_size in batch_size_list:\n        for (rows, pool_rows) in zip(rows_list, pool_rows_list):\n            cols = rows\n            pool_cols = pool_rows\n            zv = rng.randn(channels, rows, cols, batch_size).astype(config.floatX)\n            z = T.tensor4()\n            (p, h) = prob_max_pool_c01b(z, (pool_rows, pool_cols))\n            func = function([z], [p, h], mode=mode_with_gpu)\n            (p_op, h_op) = func(zv)\n            (p, h) = max_pool_c01b(z, (pool_rows, pool_cols))\n            func = function([z], [p, h], mode=mode_without_gpu)\n            (p_th, h_th) = func(zv)\n            assert np.allclose(p_op, p_th)\n            assert np.allclose(h_op, h_th)\n", "label": 0}
{"function": "\n\ndef send_action(name, action, *args, **kwargs):\n    \"\\n    Send an action to a listener\\n    the listener must have a 'dispatch_{action}' method\\n    or this will raise a ChalmersError\\n    \"\n    addr = get_addr(name)\n    try:\n        c = Client(addr, family=EventDispatcher.FAMILY)\n    except (socket.error, WindowsError):\n        raise errors.ConnectionError(('Could not connect to chalmers program %s' % name))\n    try:\n        c.send({\n            'action': action,\n            'args': args,\n            'kwargs': kwargs,\n        })\n        res = c.recv()\n        if res.get('error'):\n            raise errors.ChalmersError(res.get('message', 'Unknown error'))\n        return res.get('result')\n    finally:\n        c.close()\n", "label": 0}
{"function": "\n\ndef process(self):\n    'Execute the best-match processor for the given media type.'\n    proc = None\n    ct = self.content_type.value\n    try:\n        proc = self.processors[ct]\n    except KeyError:\n        toptype = ct.split('/', 1)[0]\n        try:\n            proc = self.processors[toptype]\n        except KeyError:\n            pass\n    if (proc is None):\n        self.default_proc()\n    else:\n        proc(self)\n", "label": 0}
{"function": "\n\n@permission_required('core.manage_shop')\ndef manage_export(request, export_id, template_name='manage/export/export.html'):\n    'The main view to display exports.\\n    '\n    export = Export.objects.get(pk=export_id)\n    categories = []\n    for category in Category.objects.filter(parent=None):\n        options = []\n        try:\n            category_option = CategoryOption.objects.get(export=export, category=category)\n        except CategoryOption.DoesNotExist:\n            variants_option = None\n        else:\n            variants_option = category_option.variants_option\n        for option in CATEGORY_VARIANTS_CHOICES:\n            options.append({\n                'name': option[1],\n                'value': option[0],\n                'selected': (option[0] == variants_option),\n            })\n        (checked, klass) = _get_category_state(export, category)\n        categories.append({\n            'id': category.id,\n            'name': category.name,\n            'checked': checked,\n            'klass': klass,\n            'options': options,\n        })\n    data_form = ExportDataForm(instance=export)\n    return render_to_response(template_name, RequestContext(request, {\n        'categories': categories,\n        'export_id': export_id,\n        'slug': export.slug,\n        'selectable_exports_inline': selectable_exports_inline(request, export_id),\n        'export_data_inline': export_data_inline(request, export_id, data_form),\n    }))\n", "label": 0}
{"function": "\n\ndef _verify_data_file(self, fp):\n    \"\\n        Verify the metadata's name value matches what we think the object is\\n        named.\\n\\n        :raises DiskFileCollision: if the metadata stored name does not match\\n                                   the referenced name of the file\\n        :raises DiskFileNotExist: if the object has expired\\n        :raises DiskFileQuarantined: if data inconsistencies were detected\\n                                     between the metadata and the file-system\\n                                     metadata\\n        \"\n    try:\n        mname = self._metadata['name']\n    except KeyError:\n        raise self._quarantine(self._name, 'missing name metadata')\n    else:\n        if (mname != self._name):\n            raise DiskFileCollision('Client path does not match path stored in object metadata')\n    try:\n        x_delete_at = int(self._metadata['X-Delete-At'])\n    except KeyError:\n        pass\n    except ValueError:\n        raise self._quarantine(self._name, ('bad metadata x-delete-at value %s' % self._metadata['X-Delete-At']))\n    else:\n        if (x_delete_at <= time.time()):\n            raise DiskFileNotExist('Expired')\n    try:\n        metadata_size = int(self._metadata['Content-Length'])\n    except KeyError:\n        raise self._quarantine(self._name, 'missing content-length in metadata')\n    except ValueError:\n        raise self._quarantine(self._name, ('bad metadata content-length value %s' % self._metadata['Content-Length']))\n    try:\n        fp.seek(0, 2)\n        obj_size = fp.tell()\n        fp.seek(0, 0)\n    except OSError as err:\n        raise self._quarantine(self._name, ('not stat-able: %s' % err))\n    if (obj_size != metadata_size):\n        raise self._quarantine(self._name, ('metadata content-length %s does not match actual object size %s' % (metadata_size, obj_size)))\n    return fp\n", "label": 1}
{"function": "\n\ndef __init__(self, init_momentum, nesterov_momentum=False):\n    assert (init_momentum >= 0.0)\n    assert (init_momentum < 1.0)\n    self.momentum = sharedX(init_momentum, 'momentum')\n    self.nesterov_momentum = nesterov_momentum\n", "label": 0}
{"function": "\n\ndef __init__(self, ofc, fid='UNKNOWN', org='UNKNOWN', curdef=None, lang='ENG', debug=False):\n    self.ofc = ofc\n    self.fid = fid\n    self.org = org\n    self.curdef = curdef\n    self.lang = lang\n    self.debug = debug\n    self.bankid = 'UNKNOWN'\n    self.accttype = 'UNKNOWN'\n    self.acctid = 'UNKNOWN'\n    self.balance = 'UNKNOWN'\n    self.start_date = 'UNKNOWN'\n    self.end_date = 'UNKNOWN'\n    self.parsed_ofc = None\n    self.acct_types = {\n        '0': 'CHECKING',\n        '1': 'SAVINGS',\n        '2': 'CREDITCARD',\n        '3': 'MONEYMRKT',\n        '4': 'CREDITLINE',\n        '5': 'UNKNOWN',\n        '6': 'UNKNOWN',\n        '7': 'UNKNOWN',\n    }\n    self.txn_types = {\n        '0': 'CREDIT',\n        '1': 'DEBIT',\n        '2': 'INT',\n        '3': 'DIV',\n        '4': 'SRVCHG',\n        '5': 'DEP',\n        '6': 'ATM',\n        '7': 'XFER',\n        '8': 'CHECK',\n        '9': 'PAYMENT',\n        '10': 'CASH',\n        '11': 'DIRECTDEP',\n        '12': 'OTHER',\n    }\n    if self.debug:\n        sys.stderr.write('Parsing document.\\n')\n    parser = ofxtools.OfcParser(debug=debug)\n    self.parsed_ofc = parser.parse(self.ofc)\n    if self.debug:\n        sys.stderr.write('Extracting document properties.\\n')\n    try:\n        self.bankid = self.parsed_ofc['document']['OFC']['ACCTSTMT']['ACCTFROM']['BANKID']\n        acct_code = self.parsed_ofc['document']['OFC']['ACCTSTMT']['ACCTFROM']['ACCTTYPE']\n        self.accttype = self.acct_types.get(acct_code, 'UNKNOWN')\n        self.acctid = self.parsed_ofc['document']['OFC']['ACCTSTMT']['ACCTFROM']['ACCTID']\n    except KeyError:\n        self.bankid = self.parsed_ofc['document']['OFC']['ACCTSTMT']['ACCTFROM']['ACCOUNT']['BANKID']\n        acct_code = self.parsed_ofc['document']['OFC']['ACCTSTMT']['ACCTFROM']['ACCOUNT']['ACCTTYPE']\n        self.accttype = self.acct_types.get(acct_code, 'UNKNOWN')\n        self.acctid = self.parsed_ofc['document']['OFC']['ACCTSTMT']['ACCTFROM']['ACCOUNT']['ACCTID']\n    self.balance = self.parsed_ofc['document']['OFC']['ACCTSTMT']['STMTRS']['LEDGER']\n    self.start_date = self.parsed_ofc['document']['OFC']['ACCTSTMT']['STMTRS']['DTSTART']\n    self.end_date = self.parsed_ofc['document']['OFC']['ACCTSTMT']['STMTRS']['DTEND']\n", "label": 0}
{"function": "\n\n@gen_test(timeout=60)\ndef test_max_time_ms_getmore(self):\n    (yield self.collection.insert(({\n        \n    } for _ in range(200))))\n    try:\n        cursor = self.collection.find().max_time_ms(100000)\n        (yield cursor.fetch_next)\n        cursor.next_object()\n        (yield self.enable_timeout())\n        with self.assertRaises(ExecutionTimeout):\n            while (yield cursor.fetch_next):\n                cursor.next_object()\n        (yield cursor.close())\n        (yield self.disable_timeout())\n        cursor = self.collection.find().max_time_ms(100000)\n        (yield cursor.fetch_next)\n        cursor.next_object()\n        (yield self.enable_timeout())\n        with self.assertRaises(ExecutionTimeout):\n            (yield cursor.to_list(None))\n        (yield cursor.close())\n    finally:\n        (yield self.disable_timeout())\n        (yield self.collection.remove())\n", "label": 0}
{"function": "\n\ndef clean_caseversions(self):\n    '\\n        Make sure all the ids for the cases are valid and populate\\n        self.cleaned_data with the real objects.\\n        '\n    caseversions = dict(((unicode(x.id), x) for x in model.CaseVersion.objects.filter(pk__in=self.cleaned_data['caseversions'])))\n    try:\n        return [caseversions[x] for x in self.cleaned_data['caseversions']]\n    except KeyError as e:\n        raise ValidationError('Not a valid caseversion for this tag.')\n", "label": 0}
{"function": "\n\ndef test_update_archived_single(db, tmpdir):\n    if (not db.startswith('mysql')):\n        db = os.path.join(str(tmpdir), db)\n    jip.db.init(db)\n    j = jip.db.Job()\n    jip.db.save(j)\n    assert (not jip.db.get(j.id).archived)\n    jip.db.update_archived(j, True)\n    assert jip.db.get(j.id).archived\n", "label": 0}
{"function": "\n\n@app.route('/v1/roles', methods=['GET'])\n@authnz.require_auth\ndef get_iam_roles_list():\n    try:\n        roles = [x.name for x in iam.roles.all()]\n    except ClientError:\n        return (jsonify({\n            'error': 'Unable to roles.',\n        }), 500)\n    return jsonify({\n        'roles': roles,\n    })\n", "label": 0}
{"function": "\n\ndef get_context_data(self, **kwargs):\n    context = super(InputStockView, self).get_context_data(**kwargs)\n    try:\n        sql_location = SQLLocation.objects.get(domain=self.domain, site_code=kwargs.get('site_code'))\n    except SQLLocation.DoesNotExist:\n        raise Http404()\n    InputStockFormSet = formset_factory(InputStockForm, extra=0)\n    initial_data = []\n    for product in sql_location.products.order_by('name'):\n        try:\n            stock_state = StockState.objects.get(case_id=sql_location.supply_point_id, product_id=product.product_id)\n            stock_on_hand = stock_state.stock_on_hand\n            monthly_consumption = stock_state.get_monthly_consumption()\n        except StockState.DoesNotExist:\n            stock_on_hand = 0\n            monthly_consumption = 0\n        initial_data.append({\n            'product_id': product.product_id,\n            'product': product.name,\n            'stock_on_hand': int(stock_on_hand),\n            'monthly_consumption': (round(monthly_consumption) if monthly_consumption else 0),\n            'default_consumption': get_default_monthly_consumption(self.domain, product.product_id, sql_location.location_type.name, sql_location.supply_point_id),\n            'units': product.units,\n        })\n    context['formset'] = InputStockFormSet(initial=initial_data)\n    return context\n", "label": 0}
{"function": "\n\ndef ensure_dir_exists(dir):\n    try:\n        os.makedirs(dir)\n    except FileExistsError:\n        return\n    except Exception as e:\n        sublime.error_message((((\"Could not create directory '\" + dir) + \"'. Please make sure you have the correct permissions: \") + str(e)))\n", "label": 0}
{"function": "\n\ndef test_fenced_code(t):\n    content = \"\\nPlain:\\n\\n```\\npip install clay\\n```\\n\\nHighlighted:\\n\\n```python\\nprint('hi')\\n```\\n\"\n    expected = '\\n<p>Plain:</p>\\n<pre><code>pip install clay\\n</code></pre>\\n\\n\\n<p>Highlighted:</p>\\n<pre><code class=\"language-python\"><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">&#39;hi&#39;</span><span class=\"p\">)</span>\\n</code></pre>\\n'\n    path = get_source_path('test.md')\n    create_file(path, content)\n    resp = t.get('/test.md')\n    assert (resp.status_code == HTTP_OK)\n    assert (resp.mimetype == 'text/html')\n    print(resp.data)\n    assert (resp.data.strip() == expected.strip())\n", "label": 0}
{"function": "\n\ndef check_char_lookup(self, lookup):\n    lname = ('field__' + lookup)\n    mymodel = CharListModel.objects.create(field=['mouldy', 'rotten'])\n    mouldy = CharListModel.objects.filter(**{\n        lname: 'mouldy',\n    })\n    assert (mouldy.count() == 1)\n    assert (mouldy[0] == mymodel)\n    rotten = CharListModel.objects.filter(**{\n        lname: 'rotten',\n    })\n    assert (rotten.count() == 1)\n    assert (rotten[0] == mymodel)\n    clean = CharListModel.objects.filter(**{\n        lname: 'clean',\n    })\n    assert (clean.count() == 0)\n    with pytest.raises(ValueError):\n        list(CharListModel.objects.filter(**{\n            lname: ['a', 'b'],\n        }))\n    both = CharListModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) & Q(**{\n        lname: 'rotten',\n    })))\n    assert (both.count() == 1)\n    assert (both[0] == mymodel)\n    either = CharListModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) | Q(**{\n        lname: 'clean',\n    })))\n    assert (either.count() == 1)\n    not_clean = CharListModel.objects.exclude(**{\n        lname: 'clean',\n    })\n    assert (not_clean.count() == 1)\n    not_mouldy = CharListModel.objects.exclude(**{\n        lname: 'mouldy',\n    })\n    assert (not_mouldy.count() == 0)\n", "label": 1}
{"function": "\n\ndef build_bond(iface, **settings):\n    \"\\n    Create a bond script in /etc/modprobe.d with the passed settings\\n    and load the bonding kernel module.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' ip.build_bond bond0 mode=balance-alb\\n    \"\n    rh_major = __grains__['osrelease'][:1]\n    opts = _parse_settings_bond(settings, iface)\n    try:\n        template = JINJA.get_template('conf.jinja')\n    except jinja2.exceptions.TemplateNotFound:\n        log.error('Could not load template conf.jinja')\n        return ''\n    data = template.render({\n        'name': iface,\n        'bonding': opts,\n    })\n    _write_file_iface(iface, data, _RH_NETWORK_CONF_FILES, '{0}.conf'.format(iface))\n    path = os.path.join(_RH_NETWORK_CONF_FILES, '{0}.conf'.format(iface))\n    if (rh_major == '5'):\n        __salt__['cmd.run']('sed -i -e \"/^alias\\\\s{0}.*/d\" /etc/modprobe.conf'.format(iface), python_shell=False)\n        __salt__['cmd.run']('sed -i -e \"/^options\\\\s{0}.*/d\" /etc/modprobe.conf'.format(iface), python_shell=False)\n        __salt__['file.append']('/etc/modprobe.conf', path)\n    __salt__['kmod.load']('bonding')\n    if settings['test']:\n        return _read_temp(data)\n    return _read_file(path)\n", "label": 0}
{"function": "\n\ndef get_localized_at(self, dtime):\n    'return localized events which are scheduled at `dtime`\\n\\n        :type dtime: datetime.datetime\\n        '\n    assert (dtime.tzinfo is not None)\n    dtime = aux.to_unix_time(dtime)\n    sql_s = 'SELECT item, recs_loc.href, dtstart, dtend, ref, etag, dtype, events.calendar FROM recs_loc JOIN events ON recs_loc.href = events.href AND recs_loc.calendar = events.calendar WHERE (dtstart <= ? AND dtend >= ? ) AND events.calendar in ({0});'\n    stuple = (dtime, dtime)\n    result = self.sql_ex(sql_s.format(self._select_calendars), stuple)\n    for (item, href, start, end, ref, etag, dtype, calendar) in result:\n        start = pytz.UTC.localize(datetime.utcfromtimestamp(start))\n        end = pytz.UTC.localize(datetime.utcfromtimestamp(end))\n        (yield self.construct_event(item, href, start, end, ref, etag, calendar, dtype))\n", "label": 0}
{"function": "\n\ndef enqueue(self, pdu):\n    self.log('enqueue {pdu.name} PDU'.format(pdu=pdu))\n    if (not (pdu.type in connection_mode_pdu_types)):\n        self.err('non connection mode pdu on data link connection')\n        pdu = FrameReject.from_pdu(pdu, flags='W', dlc=self)\n        self.close()\n        self.send_queue.append(pdu)\n        return\n    if self.state.CLOSED:\n        pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=1)\n        self.send_queue.append(pdu)\n    if self.state.LISTEN:\n        if isinstance(pdu, Connect):\n            if (super(DataLinkConnection, self).enqueue(pdu) == False):\n                log.warn('full backlog on listening socket')\n                pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=32)\n                self.send_queue.append(pdu)\n                return False\n            return True\n    if self.state.CONNECT:\n        if (isinstance(pdu, ConnectionComplete) or isinstance(pdu, DisconnectedMode)):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.DISCONNECT:\n        if isinstance(pdu, DisconnectedMode):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.ESTABLISHED:\n        return self._enqueue_state_established(pdu)\n", "label": 1}
{"function": "\n\n@fixture()\ndef bind(request, manager, watcher):\n    server = UDPServer(0).register(manager)\n    assert watcher.wait('ready')\n    (host, port) = (server.host, server.port)\n    server.fire(close())\n    assert watcher.wait('closed')\n    server.unregister()\n    assert watcher.wait('unregistered')\n    return (host, port)\n", "label": 0}
{"function": "\n\ndef import_steps(self, caseversion, step_data):\n    '\\n        Add the steps to this case version.\\n\\n        Keyword arguments:\\n\\n        * caseversion -- the CaseVersion object that step_data applies to\\n        * step_data -- a dictionary containing the steps for the case\\n\\n        Instruction is a required field for a step, but expected is optional.\\n\\n        '\n    for (step_num, new_step) in enumerate(step_data):\n        try:\n            casestep = CaseStep.objects.create(caseversion=caseversion, number=(step_num + 1), instruction=new_step['instruction'], expected=new_step.get('expected', ''))\n        except KeyError:\n            raise ValueError(ImportResult.SKIP_STEP_NO_INSTRUCTION)\n", "label": 0}
{"function": "\n\ndef test_anno_upload(self):\n    'Upload all different kinds of annotations and retrieve'\n    for anntype in [1, 2, 3, 4, 5, 6]:\n        annoid = 0\n        f = H5AnnotationFile(anntype, annoid)\n        putid1 = putAnnotation(p, f)\n        newid = (putid1 + 1)\n        f = H5AnnotationFile(anntype, newid)\n        putid2 = putAnnotation(p, f)\n        p.annoid = putid1\n        assert (putid1 == getId(p))\n        p.annoid = putid2\n        assert (putid2 == getId(p))\n", "label": 0}
{"function": "\n\ndef __call__(self, event):\n    '\\n        Write event to file.\\n\\n        @param event: An event.\\n        @type event: L{dict}\\n        '\n    text = self.formatEvent(event)\n    if (text is None):\n        text = ''\n    if ('log_failure' in event):\n        try:\n            traceback = event['log_failure'].getTraceback()\n        except Exception:\n            traceback = '(UNABLE TO OBTAIN TRACEBACK FROM EVENT)\\n'\n        text = '\\n'.join((text, traceback))\n    if (self._encoding is not None):\n        text = text.encode(self._encoding)\n    if text:\n        self._outFile.write(text)\n        self._outFile.flush()\n", "label": 0}
{"function": "\n\ndef gpg_encrypt(config, filename, passphrase, content):\n    stderr = tempfile.TemporaryFile()\n    target = open((filename + '.tmp'), 'w')\n    with nested(stderr, target):\n        gnupg = GnuPG(['--armor', '--cipher-algo', config.get('cipher')])\n        gnupg.run(['--symmetric'], create_fhs=['stdin', 'passphrase'], attach_fhs={\n            'stdout': target,\n            'stderr': stderr,\n        })\n        gnupg.handles['passphrase'].write(passphrase)\n        gnupg.handles['passphrase'].close()\n        gnupg.handles['stdin'].write(content)\n        gnupg.handles['stdin'].close()\n    try:\n        gnupg.wait()\n    except IOError as exc:\n        print(exc, file=sys.stderr)\n        os.remove((filename + '.tmp'))\n    else:\n        os.rename((filename + '.tmp'), filename)\n", "label": 0}
{"function": "\n\ndef metapackage_check(self, path, pkg):\n    if pkg.is_metapackage():\n        try:\n            metapackage.validate_metapackage(path, pkg)\n        except metapackage.InvalidMetapackage as e:\n            warning('Invalid metapackage:')\n            warning(('  %s\\n' % str(e)))\n            error(fmt((\"Refusing to release invalid metapackage '@|%s@{rf}@!', metapackage requirements:\\n  @|%s\" % (pkg.name, metapackage.DEFINITION_URL))), exit=True)\n", "label": 0}
{"function": "\n\ndef add_image_info_cb(self, viewer, channel, info):\n    save_thumb = self.settings.get('cache_thumbs', False)\n    chname = channel.name\n    thumbkey = self.get_thumb_key(chname, info.name, info.path)\n    thumbpath = self.get_thumbpath(info.path)\n    with self.thmblock:\n        try:\n            bnch = self.thumbDict[thumbkey]\n            if (bnch.thumbpath == thumbpath):\n                return\n        except KeyError:\n            pass\n    image = None\n    if ((thumbpath is not None) and os.path.exists(thumbpath)):\n        save_thumb = False\n        try:\n            image = self.fv.load_image(thumbpath)\n        except Exception as e:\n            pass\n    try:\n        if (image is None):\n            image = info.image_loader(info.path)\n        image.set(name=info.name)\n        self.fv.gui_do(self._make_thumb, chname, image, info.name, info.path, thumbkey, info.image_future, save_thumb=save_thumb, thumbpath=thumbpath)\n    except Exception as e:\n        self.logger.error((\"Error generating thumbnail for '%s': %s\" % (info.path, str(e))))\n", "label": 1}
{"function": "\n\ndef _inject_context_manager_func(func, start_line, end_line, name):\n    \"\\n    injects a context manager into the given function\\n\\n    e.g given:\\n\\n        x = 5\\n        def foo():\\n            print x\\n            print '1'\\n            print '2'\\n            print '3'\\n        inject_context_manager_func(foo, 0, 2, 'cm')\\n\\n    foo will now have the definition:\\n\\n        def foo():\\n            with silk_profile('cm'):\\n                print x\\n                print '1'\\n                print '2'\\n            print '3'\\n\\n    closures, globals & locals are honoured\\n\\n    @param func: object of type<function> or type<instancemethod>\\n    @param start_line: line at which to inject 'with' statement. line num. is relative to the func, not the module.\\n    @param end_line: line at which to exit the context\\n    @param name: name of the profiler\\n    \"\n    source = _get_source_lines(func)\n    start_line += 1\n    end_line += 1\n    ws = _get_ws(source[start_line])\n    for i in range(start_line, end_line):\n        try:\n            source[i] = ('  ' + source[i])\n        except IndexError:\n            raise IndexError(('Function %s does not have line %d' % (func.__name__, i)))\n    source.insert(start_line, (ws + 'from silk.profiling.profiler import silk_profile\\n'))\n    source.insert((start_line + 1), (ws + (\"with silk_profile('%s', _dynamic=True):\\n\" % name)))\n    return _new_func_from_source(source, func)\n", "label": 0}
{"function": "\n\ndef get_themes_wp():\n    r = urllib2.Request('http://themes.svn.wordpress.org/')\n    content = urllib2.urlopen(r)\n    tree = html.fromstring(content.read())\n    el_list = tree.find('body').find('ul').findall('li')\n    themes = []\n    for el in el_list:\n        themes.append(el.text_content())\n    with open(('%s/wp_themes.txt.new' % CMS_EXPLORER_DIR), 'w+') as file:\n        for theme in themes:\n            file.write(('wp-content/themes/%s\\n' % theme.encode('ascii', 'ignore')))\n    print('WP themes list updated!')\n", "label": 0}
{"function": "\n\ndef update_text_object(self, event):\n    ' Handles the user typing text into the combo box text entry field.\\n        '\n    if (self._no_enum_update == 0):\n        value = self.control.GetValue()\n        try:\n            value = self.mapping[value]\n        except:\n            try:\n                value = self.factory.evaluate(value)\n            except Exception as excp:\n                self.error(excp)\n                return\n        self._no_enum_update += 1\n        try:\n            self.value = value\n            self.control.SetBackgroundColour(OKColor)\n            self.control.Refresh()\n        except:\n            pass\n        self._no_enum_update -= 1\n", "label": 0}
{"function": "\n\ndef incidence_matrix(G, nodelist=None, edgelist=None, oriented=False, weight=None):\n    'Return incidence matrix of G.\\n\\n    The incidence matrix assigns each row to a node and each column to an edge.\\n    For a standard incidence matrix a 1 appears wherever a row\\'s node is\\n    incident on the column\\'s edge.  For an oriented incidence matrix each\\n    edge is assigned an orientation (arbitrarily for undirected and aligning to\\n    direction for directed).  A -1 appears for the tail of an edge and 1\\n    for the head of the edge.  The elements are zero otherwise.\\n\\n    Parameters\\n    ----------\\n    G : graph\\n       A NetworkX graph\\n\\n    nodelist : list, optional   (default= all nodes in G)\\n       The rows are ordered according to the nodes in nodelist.\\n       If nodelist is None, then the ordering is produced by G.nodes().\\n\\n    edgelist : list, optional (default= all edges in G)\\n       The columns are ordered according to the edges in edgelist.\\n       If edgelist is None, then the ordering is produced by G.edges().\\n\\n    oriented: bool, optional (default=False)\\n       If True, matrix elements are +1 or -1 for the head or tail node\\n       respectively of each edge.  If False, +1 occurs at both nodes.\\n\\n    weight : string or None, optional (default=None)\\n       The edge data key used to provide each value in the matrix.\\n       If None, then each edge has weight 1.  Edge weights, if used,\\n       should be positive so that the orientation can provide the sign.\\n\\n    Returns\\n    -------\\n    A : SciPy sparse matrix\\n      The incidence matrix of G.\\n\\n    Notes\\n    -----\\n    For MultiGraph/MultiDiGraph, the edges in edgelist should be\\n    (u,v,key) 3-tuples.\\n\\n    \"Networks are the best discrete model for so many problems in\\n    applied mathematics\" [1]_.\\n\\n    References\\n    ----------\\n    .. [1] Gil Strang, Network applications: A = incidence matrix,\\n       http://academicearth.org/lectures/network-applications-incidence-matrix\\n    '\n    import scipy.sparse\n    if (nodelist is None):\n        nodelist = list(G)\n    if (edgelist is None):\n        if G.is_multigraph():\n            edgelist = list(G.edges(keys=True))\n        else:\n            edgelist = list(G.edges())\n    A = scipy.sparse.lil_matrix((len(nodelist), len(edgelist)))\n    node_index = dict(((node, i) for (i, node) in enumerate(nodelist)))\n    for (ei, e) in enumerate(edgelist):\n        (u, v) = e[:2]\n        if (u == v):\n            continue\n        try:\n            ui = node_index[u]\n            vi = node_index[v]\n        except KeyError:\n            raise NetworkXError('node %s or %s in edgelist but not in nodelist\"%(u,v)')\n        if (weight is None):\n            wt = 1\n        elif G.is_multigraph():\n            ekey = e[2]\n            wt = G[u][v][ekey].get(weight, 1)\n        else:\n            wt = G[u][v].get(weight, 1)\n        if oriented:\n            A[(ui, ei)] = (- wt)\n            A[(vi, ei)] = wt\n        else:\n            A[(ui, ei)] = wt\n            A[(vi, ei)] = wt\n    return A.asformat('csc')\n", "label": 1}
{"function": "\n\ndef test_custom_handling(activation_strategy, logger):\n\n    class MyTestHandler(logbook.TestHandler):\n\n        def handle(self, record):\n            if (record.extra.get('flag') != 'testing'):\n                return False\n            return logbook.TestHandler.handle(self, record)\n    assert (logbook.Handler.__class__ == logbook.handlers._HandlerType)\n\n    class MyLogger(logbook.Logger):\n\n        def process_record(self, record):\n            logbook.Logger.process_record(self, record)\n            record.extra['flag'] = 'testing'\n    log = MyLogger()\n    handler = MyTestHandler()\n    with capturing_stderr_context() as captured:\n        with activation_strategy(handler):\n            log.warn('From my logger')\n            logger.warn('From another logger')\n        assert handler.has_warning('From my logger')\n        assert ('From another logger' in captured.getvalue())\n", "label": 0}
{"function": "\n\ndef update_model(self, form, model):\n    '\\n            Update model helper\\n\\n            :param form:\\n                Form instance\\n            :param model:\\n                Model instance to update\\n        '\n    try:\n        form.populate_obj(model)\n        self._on_model_change(form, model, False)\n        model.save()\n    except Exception as ex:\n        if (not self.handle_view_exception(ex)):\n            flash(gettext('Failed to update record. %(error)s', error=format_error(ex)), 'error')\n            log.exception('Failed to update record.')\n        return False\n    else:\n        self.after_model_change(form, model, False)\n    return True\n", "label": 0}
{"function": "\n\ndef getPassword(keyDesc):\n    if (not _keyutils):\n        return None\n    _setupSession()\n    try:\n        keyId = _keyutils.request_key(keyDesc, _keyring)\n    except _keyutils.Error as err:\n        if (err.args[0] != _keyutils.EKEYREVOKED):\n            raise\n        return None\n    if (keyId is not None):\n        return _keyutils.read_key(keyId)\n    return None\n", "label": 0}
{"function": "\n\ndef test_formdata_file_upload(simple_app):\n    app_client = simple_app.app.test_client()\n    resp = app_client.post('/v1.0/test-formData-file-upload', data={\n        'formData': (BytesIO(b'file contents'), 'filename.txt'),\n    })\n    assert (resp.status_code == 200)\n    response = json.loads(resp.data.decode())\n    assert (response == {\n        'filename.txt': 'file contents',\n    })\n", "label": 0}
{"function": "\n\ndef test_fuse_getitem():\n    pairs = [((getarray, (getarray, 'x', slice(1000, 2000)), slice(15, 20)), (getarray, 'x', slice(1015, 1020))), ((getitem, (getarray, 'x', (slice(1000, 2000), slice(100, 200))), (slice(15, 20), slice(50, 60))), (getarray, 'x', (slice(1015, 1020), slice(150, 160)))), ((getarray, (getarray, 'x', slice(1000, 2000)), 10), (getarray, 'x', 1010)), ((getitem, (getarray, 'x', (slice(1000, 2000), 10)), (slice(15, 20),)), (getarray, 'x', (slice(1015, 1020), 10))), ((getarray, (getarray, 'x', (10, slice(1000, 2000))), (slice(15, 20),)), (getarray, 'x', (10, slice(1015, 1020)))), ((getarray, (getarray, 'x', (slice(1000, 2000), slice(100, 200))), (slice(None, None), slice(50, 60))), (getarray, 'x', (slice(1000, 2000), slice(150, 160)))), ((getarray, (getarray, 'x', (None, slice(None, None))), (slice(None, None), 5)), (getarray, 'x', (None, 5))), ((getarray, (getarray, 'x', (slice(1000, 2000), slice(10, 20))), (slice(5, 10),)), (getarray, 'x', (slice(1005, 1010), slice(10, 20)))), ((getitem, (getitem, 'x', (slice(1000, 2000),)), (slice(5, 10), slice(10, 20))), (getitem, 'x', (slice(1005, 1010), slice(10, 20))))]\n    for (inp, expected) in pairs:\n        result = rewrite_rules.rewrite(inp)\n        assert (result == expected)\n", "label": 0}
{"function": "\n\ndef update_te_inst(self, instance_data):\n    '\\n        Update a tracked entity instance with the given data\\n\\n        :param instance_data: Tracked entity instance data. Must include its ID,\\n                     organisation unit and tracked entity type\\n        '\n    try:\n        te_inst_id = instance_data.pop('Instance')\n        ou_id = instance_data.pop('Org unit')\n    except KeyError as err:\n        raise KeyError(('Mandatory attribute missing from tracked entity instance data: %s' % err))\n    request_data = {\n        'trackedEntityInstance': te_inst_id,\n        'orgUnit': ou_id,\n        'attributes': self._data_to_attributes(instance_data),\n    }\n    response = self._request.put(('trackedEntityInstances/' + te_inst_id), request_data)\n    if (response['status'] != 'SUCCESS'):\n        logger.error('Failed to update instance of tracked entity \"%s\". DHIS2 API error: %s', te_inst_id, response)\n", "label": 0}
{"function": "\n\ndef onEnterNode(self, node):\n    for variable in node.getClosureVariables():\n        assert (not variable.isModuleVariable())\n        current = node\n        while (current is not variable.getOwner()):\n            if current.isParentVariableProvider():\n                if (variable not in current.getClosureVariables()):\n                    current.addClosureVariable(variable)\n            assert (current.getParentVariableProvider() is not current)\n            current = current.getParentVariableProvider()\n            assert (current is not None), variable\n", "label": 1}
{"function": "\n\ndef test_group(self):\n    'Test the per-group switch.'\n    group = Group.objects.create(name='foo')\n    user = User.objects.create(username='bar')\n    user.groups.add(group)\n    flag = Flag.objects.create(name='myflag')\n    flag.groups.add(group)\n    request = get()\n    request.user = user\n    response = process_request(request, views.flag_in_view)\n    self.assertEqual(b'on', response.content)\n    assert ('dwf_myflag' not in response.cookies)\n    request.user = User(username='someone_else')\n    request.user.save()\n    response = process_request(request, views.flag_in_view)\n    self.assertEqual(b'off', response.content)\n    assert ('dwf_myflag' not in response.cookies)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    try:\n        entry = self._cache[key]\n        cache_counter.inc_hits(self._cache_name)\n    except KeyError:\n        cache_counter.inc_misses(self._cache_name)\n        raise\n    if self._reset_expiry_on_get:\n        entry.time = self._clock.time_msec()\n    return entry.value\n", "label": 0}
{"function": "\n\ndef test_log_message(self):\n    logger = logging.getLogger('django.server')\n    original_handlers = logger.handlers\n    logger.handlers = [logging.NullHandler()]\n    try:\n        request = WSGIRequest(RequestFactory().get('/').environ)\n        request.makefile = (lambda *args, **kwargs: BytesIO())\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        level_status_codes = {\n            'info': [200, 301, 304],\n            'warning': [400, 403, 404],\n            'error': [500, 503],\n        }\n\n        def _log_level_code(level, status_code):\n            with patch_logger('django.server', level) as messages:\n                handler.log_message('GET %s %s', 'A', str(status_code))\n            return messages\n        for (level, status_codes) in level_status_codes.items():\n            for status_code in status_codes:\n                messages = _log_level_code(level, status_code)\n                self.assertIn(('GET A %d' % status_code), messages[0])\n                for wrong_level in level_status_codes.keys():\n                    if (wrong_level != level):\n                        messages = _log_level_code(wrong_level, status_code)\n                        self.assertEqual(len(messages), 0)\n    finally:\n        logger.handlers = original_handlers\n", "label": 0}
{"function": "\n\ndef getCovMatrix(self, x=None, z=None, mode=None):\n    self.checkInputGetCovMatrix(x, z, mode)\n    c = np.exp(self.hyp[0])\n    sf2 = np.exp((2.0 * self.hyp[1]))\n    ord = self.para[0]\n    if (np.abs((ord - np.round(ord))) < 1e-08):\n        ord = int(round(ord))\n    assert (ord >= 1.0)\n    ord = int(ord)\n    if (mode == 'self_test'):\n        (nn, D) = z.shape\n        A = np.reshape(np.sum((z * z), 1), (nn, 1))\n    elif (mode == 'train'):\n        (n, D) = x.shape\n        A = (np.dot(x, x.T) + (np.eye(n) * 1e-10))\n    elif (mode == 'cross'):\n        A = np.dot(x, z.T)\n    A = (sf2 * ((c + A) ** ord))\n    return A\n", "label": 0}
{"function": "\n\ndef test_404_tried_urls_have_names(self):\n    '\\n        Verifies that the list of URLs that come back from a Resolver404\\n        exception contains a list in the right format for printing out in\\n        the DEBUG 404 page with both the patterns and URL names, if available.\\n        '\n    urls = 'regressiontests.urlpatterns_reverse.named_urls'\n    url_types_names = [[{\n        'type': RegexURLPattern,\n        'name': 'named-url1',\n    }], [{\n        'type': RegexURLPattern,\n        'name': 'named-url2',\n    }], [{\n        'type': RegexURLPattern,\n        'name': None,\n    }], [{\n        'type': RegexURLResolver,\n    }, {\n        'type': RegexURLPattern,\n        'name': 'named-url3',\n    }], [{\n        'type': RegexURLResolver,\n    }, {\n        'type': RegexURLPattern,\n        'name': 'named-url4',\n    }], [{\n        'type': RegexURLResolver,\n    }, {\n        'type': RegexURLPattern,\n        'name': None,\n    }], [{\n        'type': RegexURLResolver,\n    }, {\n        'type': RegexURLResolver,\n    }]]\n    try:\n        resolve('/included/non-existent-url', urlconf=urls)\n        self.fail('resolve did not raise a 404')\n    except Resolver404 as e:\n        self.assertTrue(('tried' in e.args[0]))\n        tried = e.args[0]['tried']\n        self.assertEqual(len(e.args[0]['tried']), len(url_types_names), ('Wrong number of tried URLs returned.  Expected %s, got %s.' % (len(url_types_names), len(e.args[0]['tried']))))\n        for (tried, expected) in zip(e.args[0]['tried'], url_types_names):\n            for (t, e) in zip(tried, expected):\n                self.assertTrue(isinstance(t, e['type']), (str('%s is not an instance of %s') % (t, e['type'])))\n                if ('name' in e):\n                    if (not e['name']):\n                        self.assertTrue((t.name is None), ('Expected no URL name but found %s.' % t.name))\n                    else:\n                        self.assertEqual(t.name, e['name'], ('Wrong URL name.  Expected \"%s\", got \"%s\".' % (e['name'], t.name)))\n", "label": 0}
{"function": "\n\ndef test_upload_image(client, mocker):\n    today = datetime.date.today()\n    mocker.patch('requests.get')\n    mocker.patch('redwind.tasks.create_queue')\n    rv = client.post('/save_new', data={\n        'photo': (open('tests/image.jpg', 'rb'), 'image.jpg', 'image/jpeg'),\n        'post_type': 'photo',\n        'content': 'High score',\n        'action': 'publish_quietly',\n    })\n    assert (rv.status_code == 302)\n    assert (rv.location == 'http://example.com/{}/{:02d}/high-score'.format(today.year, today.month))\n    permalink = rv.location\n    rv = client.get(permalink)\n    assert (rv.status_code == 200)\n    content = rv.get_data(as_text=True)\n    assert ('High score' in content)\n    assert ('<img' in content)\n    rv = client.get((permalink + '/files/image.jpg'))\n    assert (rv.status_code == 200)\n", "label": 0}
{"function": "\n\ndef _get_text(self, encoding):\n    lines = self.editor.toPlainText().splitlines()\n    if self.clean_trailing_whitespaces:\n        lines = [l.rstrip() for l in lines]\n    try:\n        last_line = lines[(- 1)]\n    except IndexError:\n        pass\n    else:\n        while (last_line == ''):\n            try:\n                lines.pop()\n                last_line = lines[(- 1)]\n            except IndexError:\n                last_line = None\n    text = (self._eol.join(lines) + self._eol)\n    return text.encode(encoding)\n", "label": 0}
{"function": "\n\ndef OnOk(self, hwnd):\n    \"When OK is pressed, if this isn't a progress dialog then simply\\n        gather the results and return. If this is a progress dialog then\\n        start a thread to handle progress via the progress iterator.\\n        \"\n\n    def progress_thread(iterator, cancelled):\n        'Handle the progress side of the dialog by iterating over a supplied\\n            iterator(presumably a generator) sending generated values as messages\\n            to the progress box -- these might be percentages or files processed\\n            or whatever.\\n\\n            If the user cancels, an event will be fired which is detected here and\\n            the iteration broken. Likewise an exception will be logged to the usual\\n            places and a suitable message sent.\\n            '\n        try:\n            for message in iterator:\n                if (wrapped(win32event.WaitForSingleObject, cancelled, 0) != win32event.WAIT_TIMEOUT):\n                    self._progress_complete('User cancelled')\n                    break\n                else:\n                    self._progress_message(message)\n        except:\n            info_dialog('An error occurred: please contact the Helpdesk', traceback.format_exc(), hwnd)\n            self._progress_complete('An error occurred')\n        else:\n            self._progress_complete('Complete')\n    self.results = []\n    for (i, (field, default_value, callback)) in enumerate(self.fields):\n        value = self._get_item((self.IDC_FIELD_BASE + i))\n        if isinstance(default_value, datetime.date):\n            try:\n                if value:\n                    value = datetime.datetime.strptime(value, '%d %b %Y').date()\n                else:\n                    value = None\n            except ValueError:\n                win32api.MessageBox(hwnd, ('Dates must look like:\\n%s' % datetime.date.today().strftime('%d %b %Y').lstrip('0')), 'Invalid Date')\n                return\n        self.results.append(value)\n    if self.progress_callback:\n        self._set_item(self._progress_id, 'Working...')\n        for i in range(len(self.fields)):\n            self._enable((self.IDC_FIELD_BASE + i), False)\n        self._enable(win32con.IDOK, False)\n        wrapped(win32gui.SetFocus, wrapped(win32gui.GetDlgItem, hwnd, win32con.IDCANCEL))\n        progress_iterator = self.progress_callback(*self.results)\n        self.progress_callback = None\n        self.progress_thread = threading.Thread(target=progress_thread, args=(progress_iterator, self.progress_cancelled))\n        self.progress_thread.setDaemon(True)\n        self.progress_thread.start()\n    else:\n        wrapped(win32gui.EndDialog, hwnd, win32con.IDOK)\n", "label": 0}
{"function": "\n\ndef mon_status(conn, logger, hostname, args, silent=False):\n    '\\n    run ``ceph daemon mon.`hostname` mon_status`` on the remote end and provide\\n    not only the output, but be able to return a boolean status of what is\\n    going on.\\n    ``False`` represents a monitor that is not doing OK even if it is up and\\n    running, while ``True`` would mean the monitor is up and running correctly.\\n    '\n    mon = ('mon.%s' % hostname)\n    try:\n        out = mon_status_check(conn, logger, hostname, args)\n        if (not out):\n            logger.warning(('monitor: %s, might not be running yet' % mon))\n            return False\n        if (not silent):\n            logger.debug(('*' * 80))\n            logger.debug(('status for monitor: %s' % mon))\n            for line in json.dumps(out, indent=2, sort_keys=True).split('\\n'):\n                logger.debug(line)\n            logger.debug(('*' * 80))\n        if (out['rank'] >= 0):\n            logger.info(('monitor: %s is running' % mon))\n            return True\n        if ((out['rank'] == (- 1)) and out['state']):\n            logger.info(('monitor: %s is currently at the state of %s' % (mon, out['state'])))\n            return True\n        logger.info(('monitor: %s is not running' % mon))\n        return False\n    except RuntimeError:\n        logger.info(('monitor: %s is not running' % mon))\n        return False\n", "label": 1}
{"function": "\n\ndef execute(self, cluster, commands):\n    pipes = {\n        \n    }\n    pool = self.get_pool(commands)\n    for (db_num, command_list) in commands.iteritems():\n        pipes[db_num] = cluster[db_num].get_pipeline()\n        for command in command_list:\n            pipes[db_num].add(command.clone())\n    for (db_num, pipe) in pipes.iteritems():\n        pool.add(db_num, pipe.execute, (), {\n            \n        })\n    db_result_map = pool.join()\n    results = defaultdict(list)\n    for (db_num, db_results) in db_result_map.iteritems():\n        assert (len(db_results) == 1)\n        db_results = db_results[0]\n        if isinstance(db_results, Exception):\n            for command in commands[db_num]:\n                results[command].append(db_results)\n            continue\n        for (command, result) in db_results.iteritems():\n            results[command].append(result)\n    return results\n", "label": 1}
{"function": "\n\ndef _buildSequenceUnpacking(provider, node, source_ref):\n    kind = getKind(node)\n    if (kind == 'List'):\n        return buildListUnpacking(provider, node.elts, source_ref)\n    elif (kind == 'Tuple'):\n        return _buildTupleUnpacking(provider, node.elts, source_ref)\n    elif (kind == 'Set'):\n        return _buildSetUnpacking(provider, node.elts, source_ref)\n    else:\n        assert False, kind\n", "label": 0}
{"function": "\n\n@classmethod\ndef normalize_email(cls, email):\n    '\\n        Normalize the address by lowercasing the domain part of the email\\n        address.\\n        '\n    email = (email or '')\n    try:\n        (email_name, domain_part) = email.strip().rsplit('@', 1)\n    except ValueError:\n        pass\n    else:\n        email = '@'.join([email_name, domain_part.lower()])\n    return email\n", "label": 0}
{"function": "\n\ndef test_reject_overlarge_conn_window_settings(self, frame_factory):\n    '\\n        SETTINGS frames cannot change the size of the connection flow control\\n        window.\\n        '\n    c = h2.connection.H2Connection()\n    c.initiate_connection()\n    increment = (((2 ** 31) - 1) - c.outbound_flow_control_window)\n    f = frame_factory.build_window_update_frame(stream_id=0, increment=increment)\n    c.receive_data(f.serialize())\n    f = frame_factory.build_settings_frame(settings={\n        h2.settings.INITIAL_WINDOW_SIZE: (self.DEFAULT_FLOW_WINDOW + 1),\n    })\n    c.clear_outbound_data_buffer()\n    events = c.receive_data(f.serialize())\n    assert (len(events) == 1)\n    assert isinstance(events[0], h2.events.RemoteSettingsChanged)\n    expected_frame = frame_factory.build_settings_frame(settings={\n        \n    }, ack=True)\n    assert (c.data_to_send() == expected_frame.serialize())\n", "label": 0}
{"function": "\n\ndef load_gray(random_seed=123522):\n    data_path = os.path.join(os.path.split(__file__)[0], 'data')\n    if (not os.path.exists(data_path)):\n        os.makedirs(data_path)\n    dataset = 'train.zip'\n    data_file = os.path.join(data_path, dataset)\n    if os.path.isfile(data_file):\n        dataset = data_file\n    if (not os.path.isfile(data_file)):\n        try:\n            import urllib\n            urllib.urlretrieve('http://google.com')\n        except AttributeError:\n            import urllib.request as urllib\n        url = 'https://dl.dropboxusercontent.com/u/15378192/train.zip'\n        print(('Downloading data from %s' % url))\n        urllib.urlretrieve(url, data_file)\n    data_dir = os.path.join(data_path, 'cvd')\n    if (not os.path.exists(data_dir)):\n        os.makedirs(data_dir)\n        zf = zipfile.ZipFile(data_file)\n        zf.extractall(data_dir)\n    data_file = os.path.join(data_path, 'cvd_gray.npy')\n    label_file = os.path.join(data_path, 'cvd_gray_labels.npy')\n    if (not os.path.exists(data_file)):\n        print('... loading data')\n        cat_matches = []\n        dog_matches = []\n        for (root, dirname, filenames) in os.walk(data_dir):\n            for filename in fnmatch.filter(filenames, 'cat*'):\n                cat_matches.append(os.path.join(root, filename))\n            for filename in fnmatch.filter(filenames, 'dog*'):\n                dog_matches.append(os.path.join(root, filename))\n        sort_key = (lambda x: int(x.split('.')[(- 2)]))\n        cat_matches = sorted(cat_matches, key=sort_key)\n        dog_matches = sorted(dog_matches, key=sort_key)\n\n        def square_and_gray(X):\n            gray_consts = np.array([[0.299], [0.587], [0.144]])\n            return imresize(X, (48, 48)).dot(gray_consts).squeeze()\n        X_cat = np.asarray([square_and_gray(mpimg.imread(f)) for f in cat_matches])\n        y_cat = np.zeros((len(X_cat),))\n        X_dog = np.asarray([square_and_gray(mpimg.imread(f)) for f in dog_matches])\n        y_dog = np.ones((len(X_dog),))\n        X = np.concatenate((X_cat, X_dog), axis=0).astype('float32')\n        y = np.concatenate((y_cat, y_dog), axis=0).astype('int32')\n        np.save(data_file, X)\n        np.save(label_file, y)\n    else:\n        X = np.load(data_file)\n        y = np.load(label_file)\n    random_state = np.random.RandomState(random_seed)\n    idx = random_state.permutation(len(X))\n    X_s = X[idx].reshape(len(X), (- 1))\n    y_s = y[idx]\n    train_x = X_s[:20000]\n    valid_x = X_s[20000:22500]\n    test_x = X_s[22500:]\n    train_y = y_s[:20000]\n    valid_y = y_s[20000:22500]\n    test_y = y_s[22500:]\n    test_x = test_x.astype('float32')\n    test_y = test_y.astype('int32')\n    valid_x = valid_x.astype('float32')\n    valid_y = valid_y.astype('int32')\n    train_x = train_x.astype('float32')\n    train_y = train_y.astype('int32')\n    rval = [(train_x, train_y), (valid_x, valid_y), (test_x, test_y)]\n    return rval\n", "label": 1}
{"function": "\n\ndef _setup_environment(environ):\n    if platform.system().upper().startswith('CYGWIN'):\n        try:\n            import ctypes\n        except ImportError as e:\n            from django.core.exceptions import ImproperlyConfigured\n            raise ImproperlyConfigured(('Error loading ctypes: %s; the Oracle backend requires ctypes to operate correctly under Cygwin.' % e))\n        kernel32 = ctypes.CDLL('kernel32')\n        for (name, value) in environ:\n            kernel32.SetEnvironmentVariableA(name, value)\n    else:\n        os.environ.update(environ)\n", "label": 0}
{"function": "\n\ndef test_spike_terms():\n    rules = {\n        'threshold_ref': 5,\n        'spike_height': 2,\n        'timeframe': datetime.timedelta(minutes=10),\n        'spike_type': 'both',\n        'use_count_query': False,\n        'timestamp_field': 'ts',\n        'query_key': 'username',\n        'use_term_query': True,\n    }\n    terms1 = {\n        ts_to_dt('2014-01-01T00:01:00Z'): [{\n            'key': 'userA',\n            'doc_count': 10,\n        }, {\n            'key': 'userB',\n            'doc_count': 5,\n        }],\n    }\n    terms2 = {\n        ts_to_dt('2014-01-01T00:10:00Z'): [{\n            'key': 'userA',\n            'doc_count': 22,\n        }, {\n            'key': 'userB',\n            'doc_count': 5,\n        }],\n    }\n    terms3 = {\n        ts_to_dt('2014-01-01T00:25:00Z'): [{\n            'key': 'userA',\n            'doc_count': 25,\n        }, {\n            'key': 'userB',\n            'doc_count': 27,\n        }],\n    }\n    terms4 = {\n        ts_to_dt('2014-01-01T00:27:00Z'): [{\n            'key': 'userA',\n            'doc_count': 10,\n        }, {\n            'key': 'userB',\n            'doc_count': 12,\n        }, {\n            'key': 'userC',\n            'doc_count': 100,\n        }],\n    }\n    terms5 = {\n        ts_to_dt('2014-01-01T00:30:00Z'): [{\n            'key': 'userD',\n            'doc_count': 100,\n        }, {\n            'key': 'userC',\n            'doc_count': 100,\n        }],\n    }\n    rule = SpikeRule(rules)\n    rule.add_terms_data(terms1)\n    assert (len(rule.matches) == 0)\n    rule.add_terms_data(terms2)\n    assert (len(rule.matches) == 0)\n    rule.add_terms_data(terms3)\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0].get('username') == 'userB')\n    rules.pop('threshold_ref')\n    rules['threshold_cur'] = 50\n    rule = SpikeRule(rules)\n    rule.add_terms_data(terms1)\n    rule.add_terms_data(terms2)\n    rule.add_terms_data(terms3)\n    rule.add_terms_data(terms4)\n    assert (len(rule.matches) == 0)\n    rules['alert_on_new_data'] = True\n    rule = SpikeRule(rules)\n    rule.add_terms_data(terms1)\n    rule.add_terms_data(terms2)\n    rule.add_terms_data(terms3)\n    rule.add_terms_data(terms4)\n    assert (len(rule.matches) == 1)\n    rule.matches = []\n    rule.add_terms_data(terms5)\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0]['username'] == 'userD')\n", "label": 1}
{"function": "\n\ndef load_ctfs(saved_data, volume_property):\n    ' Given the saved data produced via `save_ctfs`, this sets the\\n    state of the passed volume_property appropriately.\\n\\n    It returns the new color transfer function and piecewise function.\\n    '\n    rgb = saved_data['rgb']\n    a = saved_data['alpha']\n    new_ctf = True\n    ctf = volume_property.rgb_transfer_function\n    if isinstance(ctf, ColorTransferFunction):\n        new_ctf = False\n        ctf.remove_all_points()\n    else:\n        ctf = ColorTransferFunction()\n    nc = len(rgb)\n    for i in range(nc):\n        ctf.add_rgb_point(rgb[i][0], *rgb[i][1:])\n    if new_ctf:\n        volume_property.set_color(ctf)\n    try:\n        ctf.range = saved_data['range']\n    except Exception:\n        pass\n    na = len(a)\n    new_otf = True\n    otf = volume_property.get_scalar_opacity()\n    if isinstance(otf, PiecewiseFunction):\n        new_otf = False\n        otf.remove_all_points()\n    else:\n        otf = PiecewiseFunction()\n    for i in range(na):\n        otf.add_point(a[i][0], a[i][1])\n    if new_otf:\n        volume_property.set_scalar_opacity(otf)\n    return (ctf, otf)\n", "label": 1}
{"function": "\n\ndef _validate_ip(af, ip):\n    '\\n    Common logic for IPv4\\n    '\n    if ((af == socket.AF_INET6) and (not socket.has_ipv6)):\n        raise RuntimeError('IPv6 not supported')\n    try:\n        socket.inet_pton(af, ip)\n    except AttributeError:\n        if (af == socket.AF_INET6):\n            raise RuntimeError('socket.inet_pton not available')\n        try:\n            socket.inet_aton(ip)\n        except socket.error:\n            return False\n    except (socket.error, TypeError):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef load(s):\n    try:\n        yml_dict = yaml.load(s, yaml_loader)\n    except yaml.YAMLError as exc:\n        msg = 'An error occurred during YAML parsing.'\n        if hasattr(exc, 'problem_mark'):\n            msg += (' Error position: (%s:%s)' % ((exc.problem_mark.line + 1), (exc.problem_mark.column + 1)))\n        raise ValueError(msg)\n    if ((not isinstance(yml_dict, dict)) and (not isinstance(yml_dict, list))):\n        raise ValueError('The source is not a YAML mapping or list.')\n    if (isinstance(yml_dict, dict) and (len(yml_dict) < 1)):\n        raise ValueError('Could not find any element in your YAML mapping.')\n    return yml_dict\n", "label": 0}
{"function": "\n\n@decorator\ndef retry(call, tries, errors=Exception, timeout=0):\n    if isinstance(errors, list):\n        errors = tuple(errors)\n    for attempt in xrange(tries):\n        try:\n            return call()\n        except errors:\n            if ((attempt + 1) == tries):\n                raise\n            else:\n                timeout_value = (timeout(attempt) if callable(timeout) else timeout)\n                if (timeout_value > 0):\n                    time.sleep(timeout_value)\n", "label": 0}
{"function": "\n\ndef test_default_tag(temp_pkg):\n    subprocess.check_call([sys.executable, 'setup.py', 'bdist_wheel'], cwd=str(temp_pkg))\n    dist_dir = temp_pkg.join('dist')\n    assert dist_dir.check(dir=1)\n    wheels = dist_dir.listdir()\n    assert (len(wheels) == 1)\n    assert wheels[0].basename.startswith(('Test-1.0-py%s-' % (sys.version[0],)))\n    assert (wheels[0].ext == '.whl')\n", "label": 0}
{"function": "\n\ndef _hostmaskPatternEqual(pattern, hostmask):\n    try:\n        return (_patternCache[pattern](hostmask) is not None)\n    except KeyError:\n        fd = minisix.io.StringIO()\n        for c in pattern:\n            if (c == '*'):\n                fd.write('.*')\n            elif (c == '?'):\n                fd.write('.')\n            elif (c in '[{'):\n                fd.write('[[{]')\n            elif (c in '}]'):\n                fd.write('[}\\\\]]')\n            elif (c in '|\\\\'):\n                fd.write('[|\\\\\\\\]')\n            elif (c in '^~'):\n                fd.write('[~^]')\n            else:\n                fd.write(re.escape(c))\n        fd.write('$')\n        f = re.compile(fd.getvalue(), re.I).match\n        _patternCache[pattern] = f\n        return (f(hostmask) is not None)\n", "label": 1}
{"function": "\n\ndef __init__(self, host, port, base_prefix='', tail_prefix='', default_sample_rate=1, sample_rate_factor=1, logger=None):\n    self._host = host\n    self._port = port\n    self._base_prefix = base_prefix\n    self.set_prefix(tail_prefix)\n    self._default_sample_rate = default_sample_rate\n    self._sample_rate_factor = sample_rate_factor\n    self.random = random\n    self.logger = logger\n    addr_info = None\n    try:\n        addr_info = socket.getaddrinfo(host, port, socket.AF_INET)\n        self._sock_family = socket.AF_INET\n    except socket.gaierror:\n        try:\n            addr_info = socket.getaddrinfo(host, port, socket.AF_INET6)\n            self._sock_family = socket.AF_INET6\n        except socket.gaierror:\n            self._sock_family = socket.AF_INET\n    if (addr_info is not None):\n        sockaddr = addr_info[0][(- 1)]\n        self._target = ((host,) + sockaddr[1:])\n    else:\n        self._target = (host, port)\n", "label": 0}
{"function": "\n\ndef get_by_index(src_gpu, ind):\n    '\\n    Get values in a GPUArray by index.\\n\\n    Parameters\\n    ----------\\n    src_gpu : pycuda.gpuarray.GPUArray\\n        GPUArray instance from which to extract values.\\n    ind : pycuda.gpuarray.GPUArray or numpy.ndarray\\n        Array of element indices to set. Must have an integer dtype.\\n\\n    Returns\\n    -------\\n    res_gpu : pycuda.gpuarray.GPUArray\\n        GPUArray with length of `ind` and dtype of `src_gpu` containing\\n        selected values.\\n\\n    Examples\\n    --------\\n    >>> import pycuda.gpuarray as gpuarray\\n    >>> import pycuda.autoinit\\n    >>> import numpy as np\\n    >>> import misc\\n    >>> src = np.random.rand(5).astype(np.float32)\\n    >>> src_gpu = gpuarray.to_gpu(src)\\n    >>> ind = gpuarray.to_gpu(np.array([0, 2, 4]))\\n    >>> res_gpu = misc.get_by_index(src_gpu, ind)\\n    >>> np.allclose(res_gpu.get(), src[[0, 2, 4]])\\n    True\\n\\n    Notes\\n    -----\\n    Only supports 1D index arrays.\\n\\n    May not be efficient for certain index patterns because of lack of inability\\n    to coalesce memory operations.\\n    '\n    assert (len(np.shape(ind)) == 1)\n    assert issubclass(ind.dtype.type, numbers.Integral)\n    N = len(ind)\n    if (not isinstance(ind, gpuarray.GPUArray)):\n        ind = gpuarray.to_gpu(ind)\n    dest_gpu = gpuarray.empty(N, dtype=src_gpu.dtype)\n    if (N == 0):\n        return dest_gpu\n    try:\n        func = get_by_index.cache[(src_gpu.dtype, ind.dtype)]\n    except KeyError:\n        data_ctype = tools.dtype_to_ctype(src_gpu.dtype)\n        ind_ctype = tools.dtype_to_ctype(ind.dtype)\n        v = '{data_ctype} *dest, {ind_ctype} *ind, {data_ctype} *src'.format(data_ctype=data_ctype, ind_ctype=ind_ctype)\n        func = elementwise.ElementwiseKernel(v, 'dest[i] = src[ind[i]]')\n        get_by_index.cache[(src_gpu.dtype, ind.dtype)] = func\n    func(dest_gpu, ind, src_gpu, range=slice(0, N, 1))\n    return dest_gpu\n", "label": 0}
{"function": "\n\ndef __init__(self, dataset, subset_iterator, preprocessor=None, fit_preprocessor=False, which_set=None, return_dict=True):\n    self.dataset = dataset\n    self.subset_iterator = list(subset_iterator)\n    dataset_iterator = dataset.iterator(mode='sequential', num_batches=1, data_specs=dataset.data_specs, return_tuple=True)\n    self._data = dataset_iterator.next()\n    self.preprocessor = preprocessor\n    self.fit_preprocessor = fit_preprocessor\n    self.which_set = which_set\n    if (which_set is not None):\n        which_set = np.atleast_1d(which_set)\n        assert len(which_set)\n        for label in which_set:\n            if (label not in ['train', 'valid', 'test']):\n                raise ValueError(\"Unrecognized subset '{}'\".format(label))\n        self.which_set = which_set\n    self.return_dict = return_dict\n", "label": 0}
{"function": "\n\n@staticmethod\ndef canMatch(l1, l2):\n    if ((l1 is None) and (l2 is None)):\n        return True\n    elif ((l1 is None) or (l2 is None)):\n        return False\n    try:\n        l1.intersect(l2)\n        return True\n    except EmptyLabelSet:\n        return False\n", "label": 0}
{"function": "\n\ndef test_pipeline(self, runner, tmpdir):\n    po_file = build_po_string('#: foo/foo.py:5\\nmsgid \"Foo bar baz\"\\nmsgstr \"\"\\n')\n    fn = tmpdir.join('messages.po')\n    fn.write(po_file)\n    assert po_file.endswith('msgstr \"\"\\n')\n    result = runner.invoke(cli, ('translate', '-p', 'shouty', str(fn)))\n    assert (result.exit_code == 0)\n    last_line = fn.read().splitlines()[(- 1)]\n    assert (last_line == 'msgstr \"FOO BAR BAZ\"')\n", "label": 0}
{"function": "\n\ndef test_auth(self):\n    if (self.driver.connection._auth_version == '2.0'):\n        return\n    OpenStackMockHttp.type = 'UNAUTHORIZED'\n    try:\n        self.driver = self.create_driver()\n        self.driver.list_nodes()\n    except InvalidCredsError:\n        e = sys.exc_info()[1]\n        self.assertEqual(True, isinstance(e, InvalidCredsError))\n    else:\n        self.fail('test should have thrown')\n", "label": 0}
{"function": "\n\ndef rs_series_reversion(p, x, n, y):\n    \"\\n    Reversion of a series.\\n\\n    ``p`` is a series with ``O(x**n)`` of the form `p = a*x + f(x)`\\n    where `a` is a number different from 0.\\n\\n    `f(x) = sum( a\\\\_k*x\\\\_k, k in range(2, n))`\\n\\n      a_k : Can depend polynomially on other variables, not indicated.\\n      x : Variable with name x.\\n      y : Variable with name y.\\n\\n    Solve `p = y`, that is, given `a*x + f(x) - y = 0`,\\n    find the solution x = r(y) up to O(y**n)\\n\\n    Algorithm:\\n\\n    If `r\\\\_i` is the solution at order i, then:\\n    `a*r\\\\_i + f(r\\\\_i) - y = O(y**(i + 1))`\\n\\n    and if r_(i + 1) is the solution at order i + 1, then:\\n    `a*r\\\\_(i + 1) + f(r\\\\_(i + 1)) - y = O(y**(i + 2))`\\n\\n    We have, r_(i + 1) = r_i + e, such that,\\n    `a*e + f(r\\\\_i) = O(y**(i + 2))`\\n    or `e = -f(r\\\\_i)/a`\\n\\n    So we use the recursion relation:\\n    `r\\\\_(i + 1) = r\\\\_i - f(r\\\\_i)/a`\\n    with the boundary condition: `r\\\\_1 = y`\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_series_reversion, rs_trunc\\n    >>> R, x, y, a, b = ring('x, y, a, b', QQ)\\n    >>> p = x - x**2 - 2*b*x**2 + 2*a*b*x**2\\n    >>> p1 = rs_series_reversion(p, x, 3, y); p1\\n    -2*y**2*a*b + 2*y**2*b + y**2 + y\\n    >>> rs_trunc(p.compose(x, p1), y, 3)\\n    y\\n    \"\n    if rs_is_puiseux(p, x):\n        raise NotImplementedError\n    R = p.ring\n    nx = R.gens.index(x)\n    y = R(y)\n    ny = R.gens.index(y)\n    if _has_constant_term(p, x):\n        raise ValueError('p must not contain a constant term in the series variable')\n    a = _coefficient_t(p, (nx, 1))\n    zm = R.zero_monom\n    assert ((zm in a) and (len(a) == 1))\n    a = a[zm]\n    r = (y / a)\n    for i in range(2, n):\n        sp = rs_subs(p, {\n            x: r,\n        }, y, (i + 1))\n        sp = (_coefficient_t(sp, (ny, i)) * (y ** i))\n        r -= (sp / a)\n    return r\n", "label": 0}
{"function": "\n\ndef __init__(self, config_files=None, refresh=False, private=False, config_key=None, config_defaults=None, cloud=None):\n    if (config_files is None):\n        config_files = []\n    config = os_client_config.config.OpenStackConfig(config_files=(os_client_config.config.CONFIG_FILES + config_files))\n    self.extra_config = config.get_extra_config(config_key, config_defaults)\n    if (cloud is None):\n        self.clouds = [shade.OpenStackCloud(cloud_config=cloud_config) for cloud_config in config.get_all_clouds()]\n    else:\n        try:\n            self.clouds = [shade.OpenStackCloud(cloud_config=config.get_one_cloud(cloud))]\n        except os_client_config.exceptions.OpenStackConfigException as e:\n            raise shade.OpenStackCloudException(e)\n    if private:\n        for cloud in self.clouds:\n            cloud.private = True\n    if refresh:\n        for cloud in self.clouds:\n            cloud._cache.invalidate()\n", "label": 1}
{"function": "\n\ndef decode(message, pblite, ignore_first_item=False):\n    \"Decode pblite to Protocol Buffer message.\\n\\n    This method is permissive of decoding errors and will log them as warnings\\n    and continue decoding where possible.\\n\\n    The first element of the outer pblite list must often be ignored using the\\n    ignore_first_item parameter because it contains an abbreviation of the name\\n    of the protobuf message (eg.  cscmrp for ClientSendChatMessageResponseP)\\n    that's not part of the protobuf.\\n\\n    Args:\\n        message: protocol buffer message instance to decode into.\\n        pblite: list representing a pblite-serialized message.\\n        ignore_first_item: If True, ignore the item at index 0 in the pblite\\n            list, making the item at index 1 correspond to field 1 in the\\n            message.\\n    \"\n    if (not isinstance(pblite, list)):\n        logger.warning('Ignoring invalid message: expected list, got %r', type(pblite))\n        return\n    if ignore_first_item:\n        pblite = pblite[1:]\n    if ((len(pblite) > 0) and isinstance(pblite[(- 1)], dict)):\n        extra_fields = {int(field_number): value for (field_number, value) in pblite[(- 1)].items()}\n        pblite = pblite[:(- 1)]\n    else:\n        extra_fields = {\n            \n        }\n    fields_values = itertools.chain(enumerate(pblite, start=1), extra_fields.items())\n    for (field_number, value) in fields_values:\n        if (value is None):\n            continue\n        try:\n            field = message.DESCRIPTOR.fields_by_number[field_number]\n        except KeyError:\n            if (value not in [[], '', 0]):\n                logger.debug('Message %r contains unknown field %s with value %r', message.__class__.__name__, field_number, value)\n            continue\n        if (field.label == FieldDescriptor.LABEL_REPEATED):\n            _decode_repeated_field(message, field, value)\n        else:\n            _decode_field(message, field, value)\n", "label": 1}
{"function": "\n\ndef test_ignore_table_fields(self):\n    c1 = Column('A', format='L', array=[True, False])\n    c2 = Column('B', format='X', array=[[0], [1]])\n    c3 = Column('C', format='4I', dim='(2, 2)', array=[[0, 1, 2, 3], [4, 5, 6, 7]])\n    c4 = Column('B', format='X', array=[[1], [0]])\n    c5 = Column('C', format='4I', dim='(2, 2)', array=[[1, 2, 3, 4], [5, 6, 7, 8]])\n    ta = BinTableHDU.from_columns([c1, c2, c3])\n    tb = BinTableHDU.from_columns([c1, c4, c5])\n    diff = TableDataDiff(ta.data, tb.data, ignore_fields=['B', 'C'])\n    assert diff.identical\n    assert (len(diff.common_columns) == 1)\n    assert (diff.common_column_names == set(['a']))\n    assert (diff.diff_ratio == 0)\n    assert (diff.diff_total == 0)\n", "label": 0}
{"function": "\n\ndef get_recipients(self):\n    user = self.user\n    if (not user):\n        try:\n            user = self.request.user\n        except KeyError:\n            if notification_settings.FAIL_SILENT:\n                return False\n            raise BackendError('No user or request object given. Please give at least one of them, or override get_recipients')\n    return user\n", "label": 0}
{"function": "\n\n@patch('nefertari.view.BaseView._run_init_actions')\ndef test_init(self, run):\n    request = Mock(content_type='application/json', json={\n        'param1.foo': 'val1',\n        'param3': 'val3',\n    }, method='POST', accept=[''])\n    request.params.mixed.return_value = {\n        'param2.foo': 'val2',\n    }\n    view = DummyBaseView(context={\n        'foo': 'bar',\n    }, request=request)\n    run.assert_called_once_with()\n    assert (request.override_renderer == 'nefertari_json')\n    assert (list(sorted(view._params.keys())) == ['param1', 'param2', 'param3'])\n    assert (view._params['param1'] == {\n        'foo': 'val1',\n    })\n    assert (view._params['param2'] == {\n        'foo': 'val2',\n    })\n    assert (view._params['param3'] == 'val3')\n    assert (view.request == request)\n    assert (view.context == {\n        'foo': 'bar',\n    })\n    assert (view._before_calls == {\n        \n    })\n    assert (view._after_calls == {\n        \n    })\n", "label": 1}
{"function": "\n\ndef test_update_time_on_text():\n    (temporary_folder, folder) = helper.create_working_folder()\n    (temporary_folder_destination, folder_destination) = helper.create_working_folder()\n    origin = ('%s/text.txt' % folder)\n    shutil.copyfile(helper.get_file('text.txt'), origin)\n    text = Text(origin)\n    metadata = text.get_metadata()\n    reset_hash_db()\n    status = elodie.update_time(text, origin, '2000-01-01 12:00:00')\n    restore_hash_db()\n    text_processed = Text(origin)\n    metadata_processed = text_processed.get_metadata()\n    shutil.rmtree(folder)\n    shutil.rmtree(folder_destination)\n    assert (status == True), status\n    assert (metadata['date_taken'] != metadata_processed['date_taken'])\n    assert (metadata_processed['date_taken'] == helper.time_convert((2000, 1, 1, 12, 0, 0, 5, 1, 0))), metadata_processed['date_taken']\n", "label": 0}
{"function": "\n\ndef evacuate_host(request, host, target=None, on_shared_storage=False):\n    hypervisors = novaclient(request).hypervisors.search(host, True)\n    response = []\n    err_code = None\n    for hypervisor in hypervisors:\n        hyper = Hypervisor(hypervisor)\n        for server in hyper.servers:\n            try:\n                novaclient(request).servers.evacuate(server['uuid'], target, on_shared_storage)\n            except nova_exceptions.ClientException as err:\n                err_code = err.code\n                msg = _('Name: %(name)s ID: %(uuid)s')\n                msg = (msg % {\n                    'name': server['name'],\n                    'uuid': server['uuid'],\n                })\n                response.append(msg)\n    if err_code:\n        msg = (_('Failed to evacuate instances: %s') % ', '.join(response))\n        raise nova_exceptions.ClientException(err_code, msg)\n    return True\n", "label": 0}
{"function": "\n\ndef test_issubclass(self):\n    code = 'class Foo:pass\\nclass Bar(Foo): pass\\n'\n    assert (evalpy((code + 'issubclass(Bar, Foo)')) == 'true')\n    assert (evalpy((code + 'issubclass(Foo, Bar)')) == 'false')\n    assert (evalpy((code + 'issubclass(Bar, object)')) == 'true')\n", "label": 0}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    for i in xrange(x.application_key_size()):\n        self.add_application_key(x.application_key(i))\n    if x.has_tag():\n        self.set_tag(x.tag())\n", "label": 0}
{"function": "\n\ndef add_proof(self, lhs, rhs):\n    'Adds a proof obligation to show the rhs is the representation of the lhs'\n    assert isinstance(lhs, Gen)\n    assert (lhs.prove == False)\n    assert isinstance(rhs, Gen)\n    assert (rhs.prove == True)\n    assert (self == lhs.zkp == rhs.zkp)\n    self.proofs += [(lhs, rhs)]\n", "label": 0}
{"function": "\n\ndef seed(self, seed=None):\n    '\\n        Re-initialize each random stream.\\n\\n        Parameters\\n        ----------\\n        seed : None or integer in range 0 to 2**30\\n            Each random stream will be assigned a unique state that depends\\n            deterministically on this value.\\n\\n        Returns\\n        -------\\n        None\\n\\n        '\n    if (seed is None):\n        seed = self.default_instance_seed\n    self.set_rstate(seed)\n    for (old_r, new_r, size, nstreams) in self.state_updates:\n        if (nstreams is None):\n            nstreams = self.n_streams(size)\n        rstates = self.get_substream_rstates(nstreams, new_r.owner.outputs[1].dtype)\n        assert (old_r.get_value(borrow=True, return_internal_type=True).shape == rstates.shape)\n        assert (rstates.dtype == old_r.dtype)\n        old_r.set_value(rstates, borrow=True)\n", "label": 0}
{"function": "\n\ndef __init__(self, value, locale):\n    assert isinstance(value, (date, datetime, time))\n    if (isinstance(value, (datetime, time)) and (value.tzinfo is None)):\n        value = value.replace(tzinfo=UTC)\n    self.value = value\n    self.locale = Locale.parse(locale)\n", "label": 0}
{"function": "\n\n@classmethod\ndef _with_json_dict(cls, jsondict):\n    ' Overridden to use a factory if called when \"resourceType\" is\\n        defined in the JSON but does not match the receiver\\'s resource_name.\\n        '\n    if (not isinstance(jsondict, dict)):\n        raise Exception('Cannot use this method with anything but a JSON dictionary, got {}'.format(jsondict))\n    res_type = jsondict.get('resourceType')\n    if (res_type and (res_type != cls.resource_name)):\n        return fhirelementfactory.FHIRElementFactory.instantiate(res_type, jsondict)\n    return super(FHIRAbstractResource, cls)._with_json_dict(jsondict)\n", "label": 0}
{"function": "\n\ndef test_url_constants():\n    from rosdep2.rep3 import REP3_TARGETS_URL\n    for (url_name, url) in [('REP3_TARGETS_URL', REP3_TARGETS_URL)]:\n        try:\n            f = urlopen(url)\n            f.read()\n            f.close()\n        except:\n            assert False, ('URL [%s][%s] failed to download' % (url_name, url))\n", "label": 0}
{"function": "\n\ndef findMode_Mult_faster(Nvec, Wmat):\n    ' Find modes to multinomial with given parameters\\n        vectorized to solve many problems simultaneously\\n\\n      Args\\n      ------\\n      Nvec : 1D array, size P\\n             contains non-negative integers\\n      Wmat : 2D array, size P x K\\n             each row is valid probability vector of length K\\n             Wmat[p] has non-negative entries, sums to one\\n\\n      Returns\\n      -------\\n      Nmat : 2D sparse array, size P x K\\n          Nmat[p,:] = argmax_{n} \\\\log p_{mult}( Nmat[p] | Nvec[p], Wmat[p] ) \\n  '\n    Nvec = np.asarray(Nvec, dtype=np.int64)\n    if (Nvec.ndim < 1):\n        Nvec = Nvec[np.newaxis]\n    Wmat = np.asarray(Wmat, dtype=np.float64)\n    if (Wmat.ndim < 2):\n        Wmat = Wmat[np.newaxis, :]\n    Nmat = np.zeros(Wmat.shape, dtype=np.int64)\n    np.floor((Nvec[:, np.newaxis] * Wmat), out=Nmat)\n    activeRows = np.flatnonzero((Nmat.sum(axis=1) < Nvec))\n    if (len(activeRows) > 0):\n        f = ((Nvec[:, np.newaxis] * Wmat) - Nmat)\n        q = ((1 - f) / Wmat)\n        q = np.take(q, activeRows, axis=0)\n    while (len(activeRows) > 0):\n        minIDs = np.argmin(q, axis=1)\n        ids = np.ravel_multi_index([activeRows, minIDs], Nmat.shape)\n        Nmat.ravel()[ids] += 1\n        stillActiveMask = (np.take(Nmat, activeRows, axis=0).sum(axis=1) < np.take(Nvec, activeRows))\n        activeRows = activeRows[stillActiveMask]\n        ids = ids[stillActiveMask]\n        minIDs = minIDs[stillActiveMask]\n        q = np.take(q, np.flatnonzero(stillActiveMask), axis=0)\n        qids = np.ravel_multi_index([np.arange(len(minIDs)), minIDs], q.shape)\n        q.ravel()[qids] += (1.0 / Wmat.ravel()[ids])\n    assert np.all((np.abs((Nmat.sum(axis=1) - Nvec)) < 0.0001))\n    return Nmat\n", "label": 0}
{"function": "\n\ndef test_as_coefficients_dict():\n    check = [S(1), x, y, (x * y), 1]\n    assert ([Add((3 * x), (2 * x), y, 3).as_coefficients_dict()[i] for i in check] == [3, 5, 1, 0, 3])\n    assert ([((3 * x) * y).as_coefficients_dict()[i] for i in check] == [0, 0, 0, 3, 0])\n    assert (((3.0 * x) * y).as_coefficients_dict()[((3.0 * x) * y)] == 1)\n", "label": 0}
{"function": "\n\ndef run(self):\n    try:\n        process = subprocess.Popen(['esformatter'], bufsize=(160 * len(self.code)), stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, startupinfo=getStartupInfo())\n        if ST2:\n            (stdout, stderr) = process.communicate(self.code)\n            self.result = re.sub('(\\\\r|\\\\r\\\\n|\\\\n)\\\\Z', '', stdout).decode('utf-8')\n        else:\n            (stdout, stderr) = process.communicate(self.code)\n            self.result = re.sub('(\\\\r|\\\\r\\\\n|\\\\n)\\\\Z', '', str(stdout, encoding='utf-8'))\n        if stderr:\n            self.result = False\n            if ST2:\n                self.error = str(stderr.decode('utf-8'))\n            else:\n                self.error = str(stderr, encoding='utf-8')\n    except Exception as e:\n        self.result = False\n        self.error = str(e)\n", "label": 0}
{"function": "\n\ndef parse_vexrc(inp, environ):\n    'Iterator yielding key/value pairs from given stream.\\n\\n    yields tuples of heading, key, value.\\n    '\n    heading = None\n    errors = []\n    with inp:\n        for (line_number, line) in enumerate(inp):\n            line = line.decode('utf-8')\n            if (not line.strip()):\n                continue\n            extracted_heading = extract_heading(line)\n            if (extracted_heading is not None):\n                heading = extracted_heading\n                continue\n            kv_tuple = extract_key_value(line, environ)\n            if (kv_tuple is None):\n                errors.append((line_number, line))\n                continue\n            try:\n                (yield (heading, kv_tuple[0], kv_tuple[1]))\n            except GeneratorExit:\n                break\n    if errors:\n        raise InvalidConfigError(inp.name, errors)\n", "label": 0}
{"function": "\n\ndef main(args=None):\n    'Run the main command-line interface for beets. Includes top-level\\n    exception handlers that print friendly error messages.\\n    '\n    try:\n        _raw_main(args)\n    except UserError as exc:\n        message = (exc.args[0] if exc.args else None)\n        log.error('error: {0}', message)\n        sys.exit(1)\n    except util.HumanReadableException as exc:\n        exc.log(log)\n        sys.exit(1)\n    except library.FileOperationError as exc:\n        log.debug('{}', traceback.format_exc())\n        log.error('{}', exc)\n        sys.exit(1)\n    except confit.ConfigError as exc:\n        log.error('configuration error: {0}', exc)\n        sys.exit(1)\n    except db_query.InvalidQueryError as exc:\n        log.error('invalid query: {0}', exc)\n        sys.exit(1)\n    except IOError as exc:\n        if (exc.errno == errno.EPIPE):\n            pass\n        else:\n            raise\n    except KeyboardInterrupt:\n        log.debug('{}', traceback.format_exc())\n", "label": 1}
{"function": "\n\ndef test_merge_tiny_output_opt(tiffs):\n    outputname = str(tiffs.join('merged.tif'))\n    inputs = [str(x) for x in tiffs.listdir()]\n    inputs.sort()\n    runner = CliRunner()\n    result = runner.invoke(merge, (inputs + ['-o', outputname]))\n    assert (result.exit_code == 0)\n    with rasterio.open(outputname) as src:\n        data = src.read()\n        assert (data[0][0:2, 1] == 120).all()\n        assert (data[0][0:2, 2:4] == 90).all()\n        assert (data[0][2][1] == 60)\n        assert (data[0][3][0] == 40)\n", "label": 0}
{"function": "\n\ndef execute(self, *tasks):\n    '\\n        Execute one or more ``tasks`` in sequence.\\n\\n        :param tasks:\\n            An all-purpose iterable of \"tasks to execute\", each member of which\\n            may take one of the following forms:\\n\\n            **A string** naming a task from the Executor\\'s `.Collection`. This\\n            name may contain dotted syntax appropriate for calling namespaced\\n            tasks, e.g. ``subcollection.taskname``. Such tasks are executed\\n            without arguments.\\n\\n            **A two-tuple** whose first element is a task name string (as\\n            above) and whose second element is a dict suitable for use as\\n            ``**kwargs`` when calling the named task. E.g.::\\n\\n                [\\n                    (\\'task1\\', {}),\\n                    (\\'task2\\', {\\'arg1\\': \\'val1\\'}),\\n                    ...\\n                ]\\n\\n            is equivalent, roughly, to::\\n\\n                task1()\\n                task2(arg1=\\'val1\\')\\n\\n            **A `.ParserContext`** instance, whose ``.name`` attribute is used\\n            as the task name and whose ``.as_kwargs`` attribute is used as the\\n            task kwargs (again following the above specifications).\\n\\n            .. note::\\n                When called without any arguments at all (i.e. when ``*tasks``\\n                is empty), the default task from ``self.collection`` is used\\n                instead, if defined.\\n\\n        :returns:\\n            A dict mapping task objects to their return values.\\n\\n            This dict may include pre- and post-tasks if any were executed. For\\n            example, in a collection with a ``build`` task depending on another\\n            task named ``setup``, executing ``build`` will result in a dict\\n            with two keys, one for ``build`` and one for ``setup``.\\n        '\n    debug('Examining top level tasks {0!r}'.format([x for x in tasks]))\n    calls = self.normalize(tasks)\n    debug('Tasks (now Calls) with kwargs: {0!r}'.format(calls))\n    direct = list(calls)\n    config = self.config.clone()\n    expanded = self.expand_calls(calls, config)\n    try:\n        dedupe = config.tasks.dedupe\n    except AttributeError:\n        dedupe = True\n    calls = (self.dedupe(expanded) if dedupe else expanded)\n    results = {\n        \n    }\n    for call in calls:\n        autoprint = ((call in direct) and call.autoprint)\n        args = call.args\n        debug('Executing {0!r}'.format(call))\n        if call.contextualized:\n            args = ((call.context,) + args)\n        result = call.task(*args, **call.kwargs)\n        if autoprint:\n            print(result)\n        results[call.task] = result\n    return results\n", "label": 1}
{"function": "\n\ndef start(name):\n    '\\n    Start a named virtual machine\\n    '\n    ret = {\n        \n    }\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    data = vm_info(name, quiet=True)\n    if (not data):\n        __jid_event__.fire_event({\n            'message': 'Failed to find VM {0} to start'.format(name),\n        }, 'progress')\n        return 'fail'\n    host = next(six.iterkeys(data))\n    if (data[host][name]['state'] == 'running'):\n        print('VM {0} is already running'.format(name))\n        return 'bad state'\n    try:\n        cmd_ret = client.cmd_iter(host, 'virt.start', [name], timeout=600)\n    except SaltClientError as client_error:\n        return 'Virtual machine {0} not started: {1}'.format(name, client_error)\n    for comp in cmd_ret:\n        ret.update(comp)\n    __jid_event__.fire_event({\n        'message': 'Started VM {0}'.format(name),\n    }, 'progress')\n    return 'good'\n", "label": 0}
{"function": "\n\ndef ami(account):\n    myCredAccount = CredAccountAws()\n    if (not ('x509Cert' in account)):\n        printer.out('x509Cert in ami account not found', printer.ERROR)\n        return\n    if (not ('x509PrivateKey' in account)):\n        printer.out('x509PrivateKey in ami account not found', printer.ERROR)\n        return\n    if (not ('accessKey' in account)):\n        printer.out('accessKey in ami account not found', printer.ERROR)\n        return\n    if (not ('secretAccessKey' in account)):\n        printer.out('secretAccessKey in ami account not found', printer.ERROR)\n        return\n    if (not ('accountNumber' in account)):\n        printer.out('accountNumber for ami account not found', printer.ERROR)\n        return\n    if (not ('name' in account)):\n        printer.out('name for ami account not found', printer.ERROR)\n        return\n    myCredAccount.accountNumber = account['accountNumber']\n    myCredAccount.secretAccessKeyID = account['secretAccessKey']\n    myCredAccount.accessKeyID = account['accessKey']\n    myCredAccount.name = account['name']\n    myCertificates = certificates()\n    myCredAccount.certificates = myCertificates\n    try:\n        myCertificate = certificate()\n        with open(account['x509Cert'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'x509'\n        myCertificate.name = ntpath.basename(account['x509Cert'])\n        myCertificates.add_certificate(myCertificate)\n        myCertificate = certificate()\n        with open(account['x509PrivateKey'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'ec2PrivateKey'\n        myCertificate.name = ntpath.basename(account['x509PrivateKey'])\n        myCertificates.add_certificate(myCertificate)\n        if ('keyPairPrivateKey' in account):\n            myCertificate = certificate()\n            with open(account['keyPairPrivateKey'], 'r') as myfile:\n                myCertificate.certStr = myfile.read()\n            myCertificate.type_ = 'ec2KeyPairPrivateKey'\n            myCertificate.name = ntpath.basename(account['keyPairPrivateKey'])\n            myCertificates.add_certificate(myCertificate)\n            myCredAccount.keyPairName = os.path.splitext(myCertificate.name)[0]\n    except IOError as e:\n        printer.out(('File error: ' + str(e)), printer.ERROR)\n        return\n    return myCredAccount\n", "label": 1}
{"function": "\n\ndef test_graph_equality_attributes(self):\n    '\\n        Graph equality test. This one checks node equality. \\n        '\n    gr = graph()\n    gr.add_nodes([0, 1, 2])\n    gr.add_edge((0, 1))\n    gr.add_node_attribute(1, ('a', 'x'))\n    gr.add_node_attribute(2, ('b', 'y'))\n    gr.add_edge_attribute((0, 1), ('c', 'z'))\n    gr2 = deepcopy(gr)\n    gr3 = deepcopy(gr)\n    gr3.del_edge((0, 1))\n    gr3.add_edge((0, 1))\n    gr4 = deepcopy(gr)\n    gr4.del_edge((0, 1))\n    gr4.add_edge((0, 1))\n    gr4.add_edge_attribute((0, 1), ('d', 'k'))\n    gr5 = deepcopy(gr)\n    gr5.del_node(2)\n    gr5.add_node(2)\n    gr5.add_node_attribute(0, ('d', 'k'))\n    assert (gr == gr2)\n    assert (gr2 == gr)\n    assert (gr != gr3)\n    assert (gr3 != gr)\n    assert (gr != gr4)\n    assert (gr4 != gr)\n    assert (gr != gr5)\n    assert (gr5 != gr)\n", "label": 1}
{"function": "\n\ndef write(self, data):\n    if (self.paused or ((not self.iAmStreaming) and (not self.outstandingPull))):\n        self._buffer.append(data)\n    elif (self.consumer is not None):\n        assert (not self._buffer), 'Writing fresh data to consumer before my buffer is empty!'\n        bytesSent = self._writeSomeData(data)\n        self.outstandingPull = False\n        if (not (bytesSent == len(data))):\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer.append(data[bytesSent:])\n    if ((self.producer is not None) and self.producerIsStreaming):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (bytesBuffered >= self.bufferSize):\n            self.producer.pauseProducing()\n            self.producerPaused = True\n", "label": 1}
{"function": "\n\ndef from_iso_format(string):\n    '\\n    Parse a string representing the date and time in ISO 8601 format,\\n    YYYY-MM-DDTHH:MM:SS.mmmmmm or, if microsecond is 0, YYYY-MM-DDTHH:MM:SS\\n\\n    If utcoffset() does not return None, a 6-character string is appended,\\n    giving the UTC offset in (signed) hours and minutes:\\n    YYYY-MM-DDTHH:MM:SS.mmmmmm+HH:MM or, if microsecond is 0\\n    YYYY-MM-DDTHH:MM:SS+HH:MM\\n    '\n    patterns = [('\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2}.\\\\d{0,6}$', '%Y-%m-%d %H:%M:%S.%f'), ('\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2}$', '%Y-%m-%d %H:%M:%S')]\n    result = None\n    for (regex_pattern, strptime_pattern) in patterns:\n        match = re.match(regex_pattern, string)\n        if match:\n            result = datetime.datetime.strptime(string, strptime_pattern)\n            break\n        elif (len(string) >= 6):\n            timezone = string[(- 6):]\n            if re.match('[+-]\\\\d{2}:\\\\d{2}$', timezone):\n                match = re.match(regex_pattern, string[:(- 6)])\n                if match:\n                    result = datetime.datetime.strptime(string[:(- 6)], strptime_pattern)\n                    offset_hours = int(timezone[0:3])\n                    offset_minutes = int(timezone[4:6])\n                    if (offset_hours < 0):\n                        offset_minutes *= (- 1)\n                    offset_minutes += (offset_hours * 60)\n                    result = result.replace(tzinfo=TimezoneInfo(offset_minutes))\n                    break\n    if (not result):\n        raise ValueError('String not in ISO 8601 format')\n    assert result, string\n    return result\n", "label": 1}
{"function": "\n\n@patch('os.getcwd', return_value='{}/tests/testsrc/someuser/src/some_package'.format(base))\n@setup_test\ndef test_sync_package_and_clean(mock_os):\n    args = argparse.Namespace()\n    args.package = ['some_package/test.py']\n    sync(args)\n    assert (os.path.isdir('./tests/testsrc/someuser/.envies/someenv/some_package') == True)\n    assert (os.path.isfile('./tests/testsrc/someuser/.virtualenvs/someenv/lib/python2.7/site-packages/some_package/test.py') == True)\n    args.all = False\n    clean(args)\n    assert (os.path.isdir('./tests/testsrc/someuser/.envies/someenv/some_package') == False)\n    assert (os.path.isfile('./tests/testsrc/someuser/.virtualenvs/someenv/lib/python2.7/site-packages/some_package/test.py') == False)\n", "label": 0}
{"function": "\n\ndef write_fasta(ofile, s, chunk=60, id=None, reformatter=None):\n    'Trivial FASTA output'\n    if (id is None):\n        try:\n            id = str(s.id)\n        except AttributeError:\n            id = new_seq_id()\n    ofile.write((('>' + id) + '\\n'))\n    seq = str(s)\n    if (reformatter is not None):\n        seq = reformatter(seq)\n    end = len(seq)\n    pos = 0\n    while 1:\n        ofile.write((seq[pos:(pos + chunk)] + '\\n'))\n        pos += chunk\n        if (pos >= end):\n            break\n    return id\n", "label": 0}
{"function": "\n\n@classmethod\ndef _pop_params(cls, kwargs):\n    '\\n        Pop entries from the `kwargs` passed to cls.__new__ based on the values\\n        in `cls.params`.\\n\\n        Parameters\\n        ----------\\n        kwargs : dict\\n            The kwargs passed to cls.__new__.\\n\\n        Returns\\n        -------\\n        params : list[(str, object)]\\n            A list of string, value pairs containing the entries in cls.params.\\n\\n        Raises\\n        ------\\n        TypeError\\n            Raised if any parameter values are not passed or not hashable.\\n        '\n    param_values = []\n    for key in cls.params:\n        try:\n            value = kwargs.pop(key)\n            hash(key)\n            param_values.append(value)\n        except KeyError:\n            raise TypeError('{typename} expected a keyword parameter {name!r}.'.format(typename=cls.__name__, name=key))\n        except TypeError:\n            raise TypeError('{typename} expected a hashable value for parameter {name!r}, but got {value!r} instead.'.format(typename=cls.__name__, name=key, value=value))\n    return tuple(zip(cls.params, param_values))\n", "label": 0}
{"function": "\n\ndef _check_skip_installed(self, req_to_install, finder):\n    \"Check if req_to_install should be skipped.\\n\\n        This will check if the req is installed, and whether we should upgrade\\n        or reinstall it, taking into account all the relevant user options.\\n\\n        After calling this req_to_install will only have satisfied_by set to\\n        None if the req_to_install is to be upgraded/reinstalled etc. Any\\n        other value will be a dist recording the current thing installed that\\n        satisfies the requirement.\\n\\n        Note that for vcs urls and the like we can't assess skipping in this\\n        routine - we simply identify that we need to pull the thing down,\\n        then later on it is pulled down and introspected to assess upgrade/\\n        reinstalls etc.\\n\\n        :return: A text reason for why it was skipped, or None.\\n        \"\n    req_to_install.check_if_exists()\n    if req_to_install.satisfied_by:\n        skip_reason = 'satisfied (use --upgrade to upgrade)'\n        if self.upgrade:\n            best_installed = False\n            if (not (self.force_reinstall or req_to_install.link)):\n                try:\n                    finder.find_requirement(req_to_install, self.upgrade)\n                except BestVersionAlreadyInstalled:\n                    skip_reason = 'up-to-date'\n                    best_installed = True\n                except DistributionNotFound:\n                    pass\n            if (not best_installed):\n                if (not (self.use_user_site and (not dist_in_usersite(req_to_install.satisfied_by)))):\n                    req_to_install.conflicts_with = req_to_install.satisfied_by\n                req_to_install.satisfied_by = None\n        return skip_reason\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef get_recipient_info(self, message, contact_cache):\n    recipient_id = message.couch_recipient\n    if (recipient_id in contact_cache):\n        return contact_cache[recipient_id]\n    doc = None\n    if (recipient_id not in [None, '']):\n        try:\n            if (message.couch_recipient_doc_type == 'CommCareCase'):\n                doc = CommCareCase.get(recipient_id)\n            else:\n                doc = CouchUser.get_by_user_id(recipient_id)\n        except Exception:\n            pass\n    if doc:\n        doc_info = get_doc_info(doc.to_json(), self.domain)\n    else:\n        doc_info = None\n    contact_cache[recipient_id] = doc_info\n    return doc_info\n", "label": 0}
{"function": "\n\ndef copy_file(self, path, prefixed_path, source_storage, **options):\n    '\\n        Attempt to copy ``path`` with storage\\n        '\n    if (prefixed_path in self.copied_files):\n        return self.log((\"Skipping '%s' (already copied earlier)\" % path))\n    if (not self.delete_file(path, prefixed_path, source_storage, **options)):\n        return\n    source_path = source_storage.path(path)\n    if options['dry_run']:\n        self.log((\"Pretending to copy '%s'\" % source_path), level=1)\n    else:\n        self.log((\"Copying '%s'\" % source_path), level=1)\n        if self.local:\n            full_path = self.storage.path(prefixed_path)\n            try:\n                os.makedirs(os.path.dirname(full_path))\n            except OSError:\n                pass\n            shutil.copy2(source_path, full_path)\n        else:\n            source_file = source_storage.open(path)\n            self.storage.save(prefixed_path, source_file)\n    if (not (prefixed_path in self.copied_files)):\n        self.copied_files.append(prefixed_path)\n", "label": 0}
{"function": "\n\ndef _check_criterion(self, criterion_k, criterion_v, payload_lookup):\n    if ('type' not in criterion_v):\n        return False\n    criteria_operator = criterion_v['type']\n    criteria_pattern = criterion_v.get('pattern', None)\n    try:\n        criteria_pattern = self._render_criteria_pattern(criteria_pattern=criteria_pattern)\n    except Exception:\n        LOG.exception(('Failed to render pattern value \"%s\" for key \"%s\"' % (criteria_pattern, criterion_k)), extra=self._base_logger_context)\n        return False\n    try:\n        matches = payload_lookup.get_value(criterion_k)\n        if matches:\n            payload_value = (matches[0] if (len(matches) > 0) else matches)\n        else:\n            payload_value = None\n    except:\n        LOG.exception('Failed transforming criteria key %s', criterion_k, extra=self._base_logger_context)\n        return False\n    op_func = criteria_operators.get_operator(criteria_operator)\n    try:\n        result = op_func(value=payload_value, criteria_pattern=criteria_pattern)\n    except:\n        LOG.exception('There might be a problem with critera in rule %s.', self.rule, extra=self._base_logger_context)\n        return False\n    return (result, payload_value, criteria_pattern)\n", "label": 0}
{"function": "\n\ndef cleanup(self, _warn=False):\n    if (self.name and (not self._closed)):\n        try:\n            self._rmtree(self.name)\n        except (TypeError, AttributeError) as ex:\n            if ('None' not in str(ex)):\n                raise\n            print_('ERROR: {!r} while cleaning up {!r}'.format(ex, self), file=_sys.stderr)\n            return\n        self._closed = True\n        if _warn:\n            self._warn('Implicitly cleaning up {!r}'.format(self), Warning)\n", "label": 0}
{"function": "\n\ndef gather_results(self, count):\n    objects = []\n    lock = threading.Lock()\n\n    def target():\n        o = self.injector.get(self.cls)\n        with lock:\n            objects.append(o)\n    threads = [threading.Thread(target=target) for i in range(count)]\n    for t in threads:\n        t.start()\n    self.event.set()\n    for t in threads:\n        t.join()\n    return objects\n", "label": 0}
{"function": "\n\ndef _test_pyfile(self, pyfile, want=True, comparator=None, expectexit=False):\n    l = locals()\n    l['print'] = (lambda x: None)\n    with open(pyfile) as fp:\n        pycode = compile(fp.read(), pyfile, 'exec')\n    try:\n        result = six.exec_(pycode, globals(), l)\n        got = locals()['return_value']\n        if (not comparator):\n            comparator = self.assertEqual\n        comparator(want, got)\n    except SystemExit as e:\n        if (not expectexit):\n            raise e\n", "label": 0}
{"function": "\n\ndef validate_link(self, link, bookmark=False):\n    'Checks if the given link can get correct data.'\n    url_parts = list(urlparse.urlparse(link))\n    url_parts[0] = url_parts[1] = ''\n    if (bookmark and url_parts[2].startswith(PATH_PREFIX)):\n        return False\n    full_path = urlparse.urlunparse(url_parts)\n    try:\n        self.get_json(full_path, path_prefix='')\n        return True\n    except Exception:\n        return False\n", "label": 0}
{"function": "\n\ndef dispy_provisional_result(result, timeout=MsgTimeout):\n    \"Sends provisional result of computation back to the client.\\n\\n    In some cases, such as optimizations, computations may send\\n    current (best) result to the client and continue computation (for\\n    next iteration) so that the client may decide to terminate\\n    computations based on the results or alter computations if\\n    necessary. The computations can use this function in such cases\\n    with the current result of computation as argument.\\n\\n    'timeout' is seconds for socket connection/messages; i.e., if\\n    there is no I/O on socket (to client), this call fails. Default\\n    value for it is MsgTimeout (5) seconds.\\n\\n    Returns 0 if result was delivered to client.\\n    \"\n    dispy_job_reply = __dispy_job_info.job_reply\n    dispy_job_reply.status = DispyJob.ProvisionalResult\n    dispy_job_reply.result = result\n    dispy_job_reply.end_time = time.time()\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock = AsyncSocket(sock, blocking=True, keyfile=__dispy_job_keyfile, certfile=__dispy_job_certfile)\n    sock.settimeout(timeout)\n    try:\n        sock.connect(__dispy_job_info.reply_addr)\n        sock.send_msg(('JOB_REPLY:' + serialize(dispy_job_reply)))\n        ack = sock.recv_msg()\n        assert (ack == 'ACK')\n    except:\n        print((\"Couldn't send provisional results %s:\\n%s\" % (str(result), traceback.format_exc())))\n        return (- 1)\n    else:\n        return 0\n    finally:\n        sock.close()\n", "label": 0}
{"function": "\n\n@tornado.gen.coroutine\ndef _connect(self):\n    '\\n        Try to connect for the rest of time!\\n        '\n    while True:\n        if self._closing:\n            break\n        try:\n            self._stream = (yield self._tcp_client.connect(self.host, self.port))\n            self._connecting_future.set_result(True)\n            break\n        except Exception as e:\n            (yield tornado.gen.sleep(1))\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize(['method', 'npartitions', 'freq', 'closed', 'label'], list(product(['count', 'mean', 'ohlc'], [2, 5], ['30T', 'h', 'd', 'w', 'M'], ['right', 'left'], ['right', 'left'])))\ndef test_series_resample(method, npartitions, freq, closed, label):\n    index = pd.date_range('1-1-2000', '2-15-2000', freq='h')\n    index = index.union(pd.date_range('4-15-2000', '5-15-2000', freq='h'))\n    df = pd.Series(range(len(index)), index=index)\n    ds = dd.from_pandas(df, npartitions=npartitions)\n    result = resample(ds, freq, how=method, closed=closed, label=label)\n    divisions = result.divisions\n    result = result.compute()\n    expected = resample(df, freq, how=method, closed=closed, label=label)\n    if (method != 'ohlc'):\n        tm.assert_series_equal(result, expected, check_dtype=False)\n    else:\n        tm.assert_frame_equal(result, expected, check_dtype=False)\n    assert (expected.index[0] == divisions[0])\n    assert (expected.index[(- 1)] == divisions[(- 1)])\n", "label": 0}
{"function": "\n\ndef test_caching(self):\n    changed = False\n\n    class TestLoader(loaders.BaseLoader):\n\n        def get_source(self, environment, template):\n            return ('foo', None, (lambda : (not changed)))\n    env = Environment(loader=TestLoader(), cache_size=(- 1))\n    tmpl = env.get_template('template')\n    assert (tmpl is env.get_template('template'))\n    changed = True\n    assert (tmpl is not env.get_template('template'))\n    changed = False\n    env = Environment(loader=TestLoader(), cache_size=0)\n    assert (env.get_template('template') is not env.get_template('template'))\n    env = Environment(loader=TestLoader(), cache_size=2)\n    t1 = env.get_template('one')\n    t2 = env.get_template('two')\n    assert (t2 is env.get_template('two'))\n    assert (t1 is env.get_template('one'))\n    t3 = env.get_template('three')\n    assert ('one' in env.cache)\n    assert ('two' not in env.cache)\n    assert ('three' in env.cache)\n", "label": 1}
{"function": "\n\ndef test_borg_cmd(self):\n\n    class MockArgs():\n        remote_path = 'borg'\n        umask = 63\n    assert (self.repository.borg_cmd(None, testing=True) == [sys.executable, '-m', 'borg.archiver', 'serve'])\n    args = MockArgs()\n    assert (self.repository.borg_cmd(args, testing=False) == ['borg', 'serve', '--umask=077', '--info'])\n    args.remote_path = 'borg-0.28.2'\n    assert (self.repository.borg_cmd(args, testing=False) == ['borg-0.28.2', 'serve', '--umask=077', '--info'])\n", "label": 0}
{"function": "\n\ndef __init__(self, cgroups, kill_process_fn, hardtimelimit, softtimelimit, walltimelimit, pid_to_kill, cores, callbackFn=(lambda reason: None)):\n    super(_TimelimitThread, self).__init__()\n    if (hardtimelimit or softtimelimit):\n        assert (CPUACCT in cgroups)\n    assert (walltimelimit is not None)\n    if cores:\n        self.cpuCount = len(cores)\n    else:\n        try:\n            self.cpuCount = multiprocessing.cpu_count()\n        except NotImplementedError:\n            self.cpuCount = 1\n    self.daemon = True\n    self.cgroups = cgroups\n    self.timelimit = (hardtimelimit or ((((60 * 60) * 24) * 365) * 100))\n    self.softtimelimit = (softtimelimit or ((((60 * 60) * 24) * 365) * 100))\n    self.latestKillTime = (util.read_monotonic_time() + walltimelimit)\n    self.pid_to_kill = pid_to_kill\n    self.callback = callbackFn\n    self.kill_process = kill_process_fn\n    self.finished = threading.Event()\n", "label": 1}
{"function": "\n\ndef doAuthenticate(self, msg):\n    if (not self.authenticate_decoder):\n        self.authenticate_decoder = ircutils.AuthenticateDecoder()\n    self.authenticate_decoder.feed(msg)\n    if (not self.authenticate_decoder.ready):\n        return\n    string = self.authenticate_decoder.get()\n    self.authenticate_decoder = None\n    mechanism = self.sasl_current_mechanism\n    if (mechanism == 'ecdsa-nist256p-challenge'):\n        if (string == b''):\n            self.sendSaslString(self.sasl_username.encode('utf-8'))\n            return\n        try:\n            with open(self.sasl_ecdsa_key) as fd:\n                private_key = SigningKey.from_pem(fd.read())\n            authstring = private_key.sign(base64.b64decode(msg.args[0].encode()))\n            self.sendSaslString(authstring)\n        except (BadDigestError, OSError, ValueError):\n            self.sendMsg(ircmsgs.IrcMsg(command='AUTHENTICATE', args=('*',)))\n            self.tryNextSaslMechanism()\n    elif (mechanism == 'external'):\n        self.sendSaslString(b'')\n    elif (mechanism == 'plain'):\n        authstring = b'\\x00'.join([self.sasl_username.encode('utf-8'), self.sasl_username.encode('utf-8'), self.sasl_password.encode('utf-8')])\n        self.sendSaslString(authstring)\n", "label": 1}
{"function": "\n\ndef add_block_proposal(self, p):\n    assert isinstance(p, BlockProposal)\n    if self.has_blockproposal(p.blockhash):\n        self.log('known block_proposal')\n        return\n    assert p.signing_lockset.has_quorum\n    assert (p.signing_lockset.height == (p.height - 1))\n    for v in p.signing_lockset:\n        self.add_vote(v)\n    self.block_candidates[p.blockhash] = p\n", "label": 0}
{"function": "\n\ndef render_image(self, rgbobj, dst_x, dst_y):\n    for ax in self.figure.axes:\n        if (not (ax in (self.ax_img, self.ax_util))):\n            if hasattr(ax, 'lines'):\n                for line in ax.lines:\n                    try:\n                        line._transformed_path.invalidate()\n                    except AttributeError:\n                        pass\n    if self.in_axes:\n        self.render_image2(rgbobj, dst_x, dst_y)\n    else:\n        self.render_image1(rgbobj, dst_x, dst_y)\n    self.ax_util.cla()\n    if self.t_['show_pan_position']:\n        self.ax_util.add_line(self.cross1)\n        self.ax_util.add_line(self.cross2)\n    if self.message:\n        self.draw_message(self.message)\n    self.figure.canvas.draw()\n", "label": 1}
{"function": "\n\ndef dates(self):\n    'returns the years and months for which there are posts'\n    if o_settings.CACHE_ENABLED:\n        key = get_key('posts_dates')\n        cached = cache.get(key)\n        if cached:\n            return cached\n    posts = self.published()\n    dates = OrderedDict()\n    for post in posts:\n        key = post.created.strftime('%Y_%m')\n        try:\n            dates[key][1] = (dates[key][1] + 1)\n        except KeyError:\n            dates[key] = [post.created, 1]\n    if o_settings.CACHE_ENABLED:\n        cache.set(key, dates, o_settings.CACHE_TIMEOUT)\n    return dates\n", "label": 0}
{"function": "\n\n@classmethod\ndef _exec_ivy(cls, ivy, confs, ivyxml, args, jvm_options, executor, workunit_name, workunit_factory):\n    ivy = (ivy or Bootstrapper.default_ivy())\n    ivy_args = ['-ivy', ivyxml]\n    ivy_args.append('-confs')\n    ivy_args.extend(confs)\n    ivy_args.extend(args)\n    ivy_jvm_options = list(jvm_options)\n    ivy_jvm_options.append('-Dsun.io.useCanonCaches=false')\n    runner = ivy.runner(jvm_options=ivy_jvm_options, args=ivy_args, executor=executor)\n    try:\n        with ivy.resolution_lock:\n            result = execute_runner(runner, workunit_factory=workunit_factory, workunit_name=workunit_name)\n        if (result != 0):\n            raise IvyUtils.IvyError('Ivy returned {result}. cmd={cmd}'.format(result=result, cmd=runner.cmd))\n    except runner.executor.Error as e:\n        raise IvyUtils.IvyError(e)\n", "label": 0}
{"function": "\n\ndef run(self, working_dir, loader_pattern='test*.py', tests_dir='tests', **kwargs):\n    assert os.path.exists(working_dir), 'working_dir must exist'\n    with self.chdir(working_dir):\n        p = os.path.join(os.getcwd(), tests_dir)\n        patt = loader_pattern\n        if (kwargs.get('active_file_only') is True):\n            patt = os.path.basename(self.window.active_view().file_name())\n            if patt.endswith(('.cmd-test', '.cmd-test-solo')):\n                patt = 'test_all_cmds.py'\n        suite = unittest.TestLoader().discover(p, pattern=patt)\n        file_regex = '^\\\\s*File\\\\s*\"([^.].*?)\",\\\\s*line\\\\s*(\\\\d+),.*$'\n        display = OutputPanel('vintageous.tests', file_regex=file_regex, word_wrap=True)\n        display.show()\n        runner = unittest.TextTestRunner(stream=display, verbosity=1)\n\n        def run_and_display():\n            runner.run(suite)\n            self.window.run_command('reset_vintageous')\n        Thread(target=run_and_display).start()\n", "label": 0}
{"function": "\n\ndef deserialize(self, raw_value):\n    try:\n        if isinstance(self._value_option, BaseOption):\n            deserialized_dict = {\n                \n            }\n            for (k, v) in raw_value.items():\n                deserialized_dict.update({\n                    k: self._value_option.deserialize(v),\n                })\n            value = deserialized_dict\n        else:\n            value = raw_value\n    except DeserializationError:\n        raise DeserializationError('Unable to deserialize \"{}\" into dict for \"{}\"!'.format(raw_value, self.name), raw_value, self.name)\n    else:\n        return value\n", "label": 0}
{"function": "\n\ndef test_extractors_from_department_and_password(self):\n    department = DepartmentFactory()\n    department.save()\n    password = str(uuid.uuid4())\n    (extractor, envs) = Extractor.from_department_and_password(department=department, password=password)\n    assert (department == extractor.departments[0])\n    assert (extractor.check_password(password) is True)\n    assert (password in envs)\n", "label": 0}
{"function": "\n\n@setup_cache\ndef test_stats(cache):\n    for value in range(100):\n        cache[value] = value\n    assert (cache.stats(enable=True) == (0, 0))\n    for value in range(100):\n        cache[value]\n    for value in range(100, 110):\n        cache.get(value)\n    assert (cache.stats(reset=True) == (100, 10))\n    assert (cache.stats(enable=False) == (0, 0))\n    for value in range(100):\n        cache[value]\n    for value in range(100, 110):\n        cache.get(value)\n    assert (cache.stats() == (0, 0))\n    assert (len(cache.check()) == 0)\n", "label": 1}
{"function": "\n\ndef do(self, workflow_dict):\n    try:\n        flipper_crdentials = get_credentials_for(workflow_dict['source_environment'], CredentialType.FLIPPER)\n        flipper_vip = flipper_crdentials.get_parameter_by_name('vip')\n        for host in workflow_dict['target_hosts']:\n            cs_host_attr = CS_HostAttr.objects.get(host=host)\n            source_host = workflow_dict['source_hosts'][0]\n            nf_host_attr = NF_HostAttr.objects.get(host=source_host)\n            script = test_bash_script_error()\n            script += build_mount_snapshot_volume_script()\n            script += build_remove_deprecated_files_script()\n            script += build_permission_script()\n            script += build_start_database_script()\n            script += build_flipper_script()\n            context_dict = {\n                'EXPORT_PATH': nf_host_attr.nfsaas_path,\n                'SNAPSHOPT_NAME': workflow_dict['snapshot_name'],\n                'VIP_FLIPPER': flipper_vip,\n                'IPWRITE': workflow_dict['target_secondary_ips'][0].ip,\n                'HOST01': workflow_dict['target_hosts'][0],\n                'HOST02': workflow_dict['target_hosts'][1],\n            }\n            script = build_context_script(context_dict, script)\n            output = {\n                \n            }\n            LOG.info(script)\n            return_code = exec_remote_command(server=host.address, username=cs_host_attr.vm_user, password=cs_host_attr.vm_password, command=script, output=output)\n            LOG.info(output)\n            if (return_code != 0):\n                raise Exception(str(output))\n        return True\n    except Exception:\n        traceback = full_stack()\n        workflow_dict['exceptions']['error_codes'].append(DBAAS_0020)\n        workflow_dict['exceptions']['traceback'].append(traceback)\n        return False\n", "label": 0}
{"function": "\n\ndef process_request(self, request):\n    if (':' in request.get_host()):\n        (domain, port) = request.get_host().split(':')\n        if (int(port) not in (80, 443)):\n            domain = request.get_host()\n    else:\n        domain = request.get_host().split(':')[0]\n    domain = domain.lower()\n    cache_key = ('Site:domain:%s' % domain)\n    site = cache.get(cache_key)\n    if site:\n        SITE_ID.value = site\n    else:\n        try:\n            site = Site.objects.get(domain=domain)\n        except Site.DoesNotExist:\n            site = None\n        if (not site):\n            if domain.startswith('www.'):\n                fallback_domain = domain[4:]\n            else:\n                fallback_domain = ('www.' + domain)\n            try:\n                site = Site.objects.get(domain=fallback_domain)\n            except Site.DoesNotExist:\n                site = None\n        if ((not site) and getattr(settings, 'CREATE_SITES_AUTOMATICALLY', True)):\n            site = Site(domain=domain, name=domain)\n            site.save()\n        if site:\n            SITE_ID.value = site.pk\n        else:\n            SITE_ID.value = _default_site_id\n        cache.set(cache_key, SITE_ID.value, (5 * 60))\n", "label": 1}
{"function": "\n\ndef _load(self):\n    ' Read the metadata from the binary package and store them in the\\n        instance.\\n\\n        :return: the metadata dictionary\\n\\n        '\n    with qisys.sh.TempDir() as work_dir:\n        pkg = portage.xpak.tbz2(self.path)\n        pkg.decompose(work_dir, cleanup=0)\n        (arch, arch_variant) = _get_pkg_arch(work_dir)\n        with open(os.path.join(work_dir, 'PF'), 'r') as fpf:\n            pf = fpf.readline().strip()\n        (name, version, revision) = portage.versions.pkgsplit(pf)\n        dependency = dict()\n        for (dep, dep_filename) in _DEPENDENCY.iteritems():\n            dep_path = os.path.join(work_dir, dep_filename)\n            if (not os.path.exists(dep_path)):\n                dependency[dep] = list()\n                continue\n            with open(dep_path, 'r') as fdep:\n                dependency[dep] = fdep.read().strip().split()\n    dependency['all'] = list()\n    for dep_list in _DEPENDENCY.keys():\n        dependency['all'].extend(dependency[dep_list])\n    for (dep, dep_list) in dependency.iteritems():\n        dependency[dep] = list(set(dep_list))\n    metadata = {\n        'name': name,\n        'version': version,\n        'revision': revision,\n        'arch': arch,\n        'arch_variant': arch_variant,\n        'dependencies': dependency,\n    }\n    self.metadata = metadata\n", "label": 0}
{"function": "\n\ndef to_python(self, data):\n    '\\n        Checks that the file-upload field data contains a valid image (GIF, JPG,\\n        PNG, possibly others -- whatever the Python Imaging Library supports).\\n        '\n    f = super(ImageField, self).to_python(data)\n    if (f is None):\n        return None\n    from PIL import Image\n    if hasattr(data, 'temporary_file_path'):\n        file = data.temporary_file_path()\n    elif hasattr(data, 'read'):\n        file = BytesIO(data.read())\n    else:\n        file = BytesIO(data['content'])\n    try:\n        image = Image.open(file)\n        image.verify()\n        f.image = image\n        f.content_type = Image.MIME.get(image.format)\n    except Exception:\n        six.reraise(ValidationError, ValidationError(self.error_messages['invalid_image'], code='invalid_image'), sys.exc_info()[2])\n    if (hasattr(f, 'seek') and callable(f.seek)):\n        f.seek(0)\n    return f\n", "label": 0}
{"function": "\n\ndef test_repeat_get_url_interval(url_prefix):\n    session = RateLimitRequests(url_interval=1)\n    t = datetime.now()\n    resp1 = session.get((url_prefix + '/cookies'))\n    session.get((url_prefix + '/cookies/set?a=b'))\n    resp2 = session.get((url_prefix + '/cookies'))\n    assert ((datetime.now() - t) < timedelta(seconds=1))\n    assert (resp1 == resp2)\n    assert (resp1.json()['cookies'] == resp2.json()['cookies'])\n    time.sleep(1)\n    resp3 = session.get((url_prefix + '/cookies'))\n    assert (resp1 != resp3)\n    assert (resp1.json()['cookies'] != resp3.json()['cookies'])\n", "label": 0}
{"function": "\n\ndef edit_comment(self, new_comment):\n    'Edit comment of wiki page version in-place.'\n    if (not self.exists):\n        raise TracError(_('Cannot edit comment of non-existent page'))\n    old_comment = self.comment\n    with self.env.db_transaction as db:\n        db('UPDATE wiki SET comment=%s WHERE name=%s AND version=%s', (new_comment, self.name, self.version))\n    self.comment = new_comment\n    self.env.log.info('Changed comment on page %s version %s to %s', self.name, self.version, new_comment)\n    for listener in WikiSystem(self.env).change_listeners:\n        if hasattr(listener, 'wiki_page_comment_modified'):\n            listener.wiki_page_comment_modified(self, old_comment)\n", "label": 0}
{"function": "\n\ndef _create_test_db(self, verbosity, autoclobber, keepdb=False):\n    '\\n        Internal implementation - creates the test db tables.\\n        '\n    suffix = self.sql_table_creation_suffix()\n    test_database_name = self._get_test_db_name()\n    qn = self.connection.ops.quote_name\n    with self._nodb_connection.cursor() as cursor:\n        try:\n            cursor.execute(('CREATE DATABASE %s %s' % (qn(test_database_name), suffix)))\n        except Exception as e:\n            if keepdb:\n                return test_database_name\n            sys.stderr.write(('Got an error creating the test database: %s\\n' % e))\n            if (not autoclobber):\n                confirm = input((\"Type 'yes' if you would like to try deleting the test database '%s', or 'no' to cancel: \" % test_database_name))\n            if (autoclobber or (confirm == 'yes')):\n                try:\n                    if (verbosity >= 1):\n                        print(('Destroying old test database for alias %s...' % (self._get_database_display_str(verbosity, test_database_name),)))\n                    cursor.execute(('DROP DATABASE %s' % qn(test_database_name)))\n                    cursor.execute(('CREATE DATABASE %s %s' % (qn(test_database_name), suffix)))\n                except Exception as e:\n                    sys.stderr.write(('Got an error recreating the test database: %s\\n' % e))\n                    sys.exit(2)\n            else:\n                print('Tests cancelled.')\n                sys.exit(1)\n    return test_database_name\n", "label": 1}
{"function": "\n\ndef set_value(self, pymux, cli, value):\n    '\\n        Take a string, and return an integer. Raise SetOptionError when the\\n        given text does not parse to a positive integer.\\n        '\n    try:\n        value = int(value)\n        if (value < 0):\n            raise ValueError\n    except ValueError:\n        raise SetOptionError('Expecting an integer.')\n    else:\n        setattr(pymux, self.attribute_name, value)\n", "label": 0}
{"function": "\n\ndef get_variable_name(self, abbreviated_xpath):\n    '\\n        If the abbreviated_xpath has been renamed in\\n        self.variable_names_json return that new name, otherwise\\n        return the original abbreviated_xpath.\\n        '\n    if (not hasattr(self, '_keys')):\n        self._keys = self.get_keys()\n    if (not hasattr(self, '_headers')):\n        self._headers = self.get_headers()\n    assert (abbreviated_xpath in self._keys), abbreviated_xpath\n    i = self._keys.index(abbreviated_xpath)\n    header = self._headers[i]\n    if (not hasattr(self, '_variable_names')):\n        self._variable_names = ColumnRename.get_dict()\n        assert (type(self._variable_names) == dict)\n    if ((header in self._variable_names) and self._variable_names[header]):\n        return self._variable_names[header]\n    return header\n", "label": 1}
{"function": "\n\ndef get_plan(self, plan_name):\n    'Retrieves the Heat templates and environment file\\n\\n        Retrieves the files from the container matching plan_name.\\n\\n        :param plan_name: The name of the plan to retrieve files for.\\n        :type plan_name: str\\n        :rtype dict\\n        '\n    try:\n        return self.plan_store.get(plan_name)\n    except swiftexceptions.ClientException as ce:\n        LOG.exception('Swift error retrieving plan.')\n        if (ce.http_status == 404):\n            six.raise_from(exception.PlanDoesNotExistError(name=plan_name), ce)\n    except Exception:\n        LOG.exception('Error retrieving plan.')\n        raise\n", "label": 0}
{"function": "\n\ndef export_to_csv(self, model_name):\n    self.header('Exporting models ...')\n    today = datetime.datetime.today()\n    model = get_model('calaccess_campaign_browser', model_name)\n    fieldnames = ([f.name for f in model._meta.fields] + ['committee_name', 'filer_name', 'filer_id', 'filer_id_raw'])\n    relation_names = ([f.name for f in model._meta.fields] + ['committee__name', 'committee__filer__name', 'committee__filer__id', 'committee__filer__filer_id_raw'])\n    filename = '{}-{}-{}-{}.csv'.format(today.year, today.month, today.day, model_name.lower())\n    filepath = os.path.join(self.data_dir, filename)\n    self.header('  Exporting {} model ...'.format(model_name.capitalize()))\n    with open(filepath, 'wb') as csvfile:\n        writer = csv.writer(csvfile, delimiter='\\t')\n        writer.writerow(fieldnames)\n        if (model_name != 'summary'):\n            for cycle in Cycle.objects.all():\n                self.log('    Looking at cycle {} ...'.format(cycle.name))\n                rows = model.objects.filter(cycle=cycle).exclude(is_duplicate=True).values_list(*relation_names)\n                if (not rows):\n                    self.failure('      No data for {}'.format(cycle.name))\n                else:\n                    rows = self.encoded(rows)\n                    writer.writerows(rows)\n                    self.success('      Added {} {} data'.format(cycle.name, model_name))\n        else:\n            rows = self.encoded(model.objects.values_list())\n            writer.writerows(rows)\n    self.success('  Exported {}!'.format(model_name.capitalize()))\n", "label": 0}
{"function": "\n\ndef data(self):\n    if (not hasattr(self, '_data')):\n        request = URLRequest(self.url)\n        if (self.env and self.env.cache):\n            headers = self.env.cache.get(('url', 'headers', self.url))\n            if headers:\n                (etag, lmod) = headers\n                if etag:\n                    request.add_header('If-None-Match', etag)\n                if lmod:\n                    request.add_header('If-Modified-Since', lmod)\n        try:\n            response = urlopen(request)\n        except HTTPError as e:\n            if (e.code != 304):\n                raise\n            self._data = self.env.cache.get(('url', 'contents', self.url))\n        else:\n            with contextlib.closing(response):\n                self._data = response.read()\n            if (self.env and self.env.cache):\n                self.env.cache.set(('url', 'headers', self.url), (response.headers.get('ETag'), response.headers.get('Last-Modified')))\n                self.env.cache.set(('url', 'contents', self.url), self._data)\n    return self._data\n", "label": 1}
{"function": "\n\ndef find_diverge_commits(self, first_branch, second_branch):\n    \"\\n        Take two branches and find diverge commits.\\n\\n             2--3--4--5\\n            /\\n        1--+              Return:\\n            \\\\               - common parent: 1\\n             6              - first list of commits: (2, 3, 4, 5)\\n                            - second list of commits: (6)\\n\\n        :param first_branch: first branch to look for common parent\\n        :type first_branch: `pygit2.Branch`\\n        :param second_branch: second branch to look for common parent\\n        :type second_branch: `pygit2.Branch`\\n        :returns: a namedtuple with common parent, a list of first's branch\\n        commits and another list with second's branch commits\\n        :rtype: DivergeCommits (namedtuple)\\n        \"\n    common_parent = None\n    first_commits = CommitsList()\n    second_commits = CommitsList()\n    walker = self.walk_branches(GIT_SORT_TOPOLOGICAL, first_branch, second_branch)\n    for (first_commit, second_commit) in walker:\n        if ((first_commit in second_commits) or (second_commit in first_commits)):\n            break\n        if (first_commit not in first_commits):\n            first_commits.append(first_commit)\n        if (second_commit not in second_commits):\n            second_commits.append(second_commit)\n        if (second_commit.hex == first_commit.hex):\n            break\n    try:\n        index = second_commits.index(first_commit)\n    except ValueError:\n        pass\n    else:\n        second_commits = second_commits[:index]\n        common_parent = first_commit\n    try:\n        index = first_commits.index(second_commit)\n    except ValueError:\n        pass\n    else:\n        first_commits = first_commits[:index]\n        common_parent = second_commit\n    return DivergeCommits(common_parent, first_commits, second_commits)\n", "label": 1}
{"function": "\n\ndef call_for_each_element(data, function, args=[], data_type='sequential'):\n    if (data_type == 'plain'):\n        return function(data, *args)\n    elif (data_type == 'sequential'):\n        assert list_of_lists(data)\n        return [function(d, *args) for d in data]\n    elif (data_type == 'token'):\n        assert (type(data) == dict)\n    return {token: function(contexts, *args) for (token, contexts) in data.items()}\n", "label": 1}
{"function": "\n\ndef headers_received(self, start_line, headers):\n    if (start_line.code != 101):\n        return super(WebSocketClientConnection, self).headers_received(start_line, headers)\n    self.headers = headers\n    assert (self.headers['Upgrade'].lower() == 'websocket')\n    assert (self.headers['Connection'].lower() == 'upgrade')\n    accept = WebSocketProtocol13.compute_accept_value(self.key)\n    assert (self.headers['Sec-Websocket-Accept'] == accept)\n    self.protocol = WebSocketProtocol13(self, mask_outgoing=True)\n    self.protocol._receive_frame()\n    if (self._timeout is not None):\n        self.io_loop.remove_timeout(self._timeout)\n        self._timeout = None\n    self.stream = self.connection.detach()\n    self.stream.set_close_callback(self.on_connection_close)\n    self.final_callback = None\n    self.connect_future.set_result(self)\n", "label": 0}
{"function": "\n\n@classmethod\n@postonly\n@multiplayer_service\n@jsonify\ndef client_leave(cls):\n    remote_addr = get_remote_addr(request)\n    params = request.params\n    try:\n        session_id = params['session']\n        player_id = params['client']\n        hmac = params['hmac']\n    except KeyError:\n        response.status_int = 400\n        return {\n            'ok': False,\n            'msg': 'Missing session information.',\n        }\n    calculated_hmac = _calculate_client_hmac(cls.secret, remote_addr, session_id, player_id)\n    if (hmac != calculated_hmac):\n        response.status_int = 400\n        return {\n            'ok': False,\n            'msg': 'Invalid server information.',\n        }\n    sessions = cls.sessions\n    try:\n        session = sessions[session_id]\n    except KeyError:\n        response.status_int = 404\n        return {\n            'ok': False,\n            'msg': 'Unknown session.',\n        }\n    with cls.lock:\n        if session.has_player(player_id):\n            request_ip = get_remote_addr(request)\n            stored_ip = session.get_player_ip(player_id)\n            if ((stored_ip is not None) and (request_ip != stored_ip)):\n                response.status_int = 401\n                return {\n                    'ok': False,\n                }\n            LOG.info('Player %s left session %s', player_id, session_id)\n            session.remove_player(player_id)\n            cls._clean_empty_sessions()\n    return {\n        'ok': True,\n    }\n", "label": 0}
{"function": "\n\ndef test_hi_level_transaction(self):\n    factory = pyorient.OrientDB('localhost', 2424)\n    factory.get_message(pyorient.CONNECT).prepare(('root', 'root')).send().fetch_response()\n    db_name = 'test_transactions'\n    exists = factory.get_message(pyorient.DB_EXIST).prepare([db_name, pyorient.STORAGE_TYPE_MEMORY]).send().fetch_response()\n    print(('Before %r' % exists))\n    try:\n        factory.get_message(pyorient.DB_DROP).prepare([db_name]).send().fetch_response()\n        assert True\n    except pyorient.PyOrientCommandException as e:\n        print(str(e))\n    finally:\n        factory.get_message(pyorient.DB_CREATE).prepare((db_name, pyorient.DB_TYPE_GRAPH, pyorient.STORAGE_TYPE_MEMORY)).send().fetch_response()\n    msg = factory.get_message(pyorient.DB_OPEN)\n    cluster_info = msg.prepare((db_name, 'admin', 'admin', pyorient.DB_TYPE_GRAPH, '')).send().fetch_response()\n    rec = {\n        'alloggio': 'baita',\n        'lavoro': 'no',\n        'vacanza': 'lago',\n    }\n    rec_position = factory.get_message(pyorient.RECORD_CREATE).prepare((3, rec)).send().fetch_response()\n    rec3 = {\n        'alloggio': 'albergo',\n        'lavoro': 'ufficio',\n        'vacanza': 'montagna',\n    }\n    update_success = factory.get_message(pyorient.RECORD_UPDATE).prepare((3, rec_position._rid, rec3, rec_position._version))\n    rec1 = {\n        'alloggio': 'casa',\n        'lavoro': 'ufficio',\n        'vacanza': 'mare',\n    }\n    rec_position1 = factory.get_message(pyorient.RECORD_CREATE).prepare(((- 1), rec1))\n    rec2 = {\n        'alloggio': 'baita',\n        'lavoro': 'no',\n        'vacanza': 'lago',\n    }\n    rec_position2 = factory.get_message(pyorient.RECORD_CREATE).prepare(((- 1), rec2))\n    rec = {\n        'alloggio': 'baita',\n        'lavoro': 'no',\n        'vacanza': 'lago',\n    }\n    rec_position = factory.get_message(pyorient.RECORD_CREATE).prepare((3, rec)).send().fetch_response()\n    delete_msg = factory.get_message(pyorient.RECORD_DELETE)\n    delete_msg.prepare((3, rec_position._rid))\n    tx = factory.get_message(pyorient.TX_COMMIT)\n    tx.begin()\n    tx.attach(rec_position1)\n    tx.attach(rec_position1)\n    tx.attach(rec_position2)\n    tx.attach(update_success)\n    tx.attach(delete_msg)\n    res = tx.commit()\n    for (k, v) in res.items():\n        print(((k + ' -> ') + v.vacanza))\n    assert (len(res) == 4)\n    assert (res['#3:0'].vacanza == 'montagna')\n    assert (res['#3:2'].vacanza == 'mare')\n    assert (res['#3:3'].vacanza == 'mare')\n    assert (res['#3:4'].vacanza == 'lago')\n    sid = factory.get_message(pyorient.CONNECT).prepare(('root', 'root')).send().fetch_response()\n    factory.get_message(pyorient.DB_DROP).prepare([db_name, pyorient.STORAGE_TYPE_MEMORY]).send().fetch_response()\n", "label": 1}
{"function": "\n\ndef crawl(name, mapping):\n    try:\n        result = _crawl(name, mapping)\n        if isinstance(result, _Dict):\n            if getattr(result, 'default', False):\n                result = result.default\n            else:\n                result = None\n        return result\n    except (KeyError, TypeError):\n        return None\n", "label": 0}
{"function": "\n\ndef start_vm(workflow_dict):\n    try:\n        environment = workflow_dict['environment']\n        cs_credentials = get_credentials_for(environment=environment, credential_type=CredentialType.CLOUDSTACK)\n        cs_provider = CloudStackProvider(credentials=cs_credentials)\n        host = workflow_dict['host']\n        host_csattr = HostAttr.objects.get(host=host)\n        started = cs_provider.start_virtual_machine(vm_id=host_csattr.vm_id)\n        if (not started):\n            raise Exception('Could not start host {}'.format(host))\n        host_ready = check_ssh(server=host.address, username=host_csattr.vm_user, password=host_csattr.vm_password, retries=50, wait=20, interval=30)\n        if (not host_ready):\n            error = ('Host %s is not ready...' % host)\n            LOG.warn(error)\n            raise Exception(error)\n        return True\n    except Exception:\n        traceback = full_stack()\n        workflow_dict['exceptions']['error_codes'].append(DBAAS_0015)\n        workflow_dict['exceptions']['traceback'].append(traceback)\n        return False\n", "label": 0}
{"function": "\n\ndef test_listen_targets_per_subclass(self):\n    'test that listen() called on a subclass remains specific to\\n        that subclass.'\n    canary = []\n\n    def listen_one(*args):\n        canary.append('listen_one')\n\n    def listen_two(*args):\n        canary.append('listen_two')\n\n    def listen_three(*args):\n        canary.append('listen_three')\n    event.listen(pool.Pool, 'connect', listen_one)\n    event.listen(pool.QueuePool, 'connect', listen_two)\n    event.listen(pool.SingletonThreadPool, 'connect', listen_three)\n    p1 = pool.QueuePool(creator=MockDBAPI().connect)\n    p2 = pool.SingletonThreadPool(creator=MockDBAPI().connect)\n    assert (listen_one in p1.dispatch.connect)\n    assert (listen_two in p1.dispatch.connect)\n    assert (listen_three not in p1.dispatch.connect)\n    assert (listen_one in p2.dispatch.connect)\n    assert (listen_two not in p2.dispatch.connect)\n    assert (listen_three in p2.dispatch.connect)\n    p1.connect()\n    eq_(canary, ['listen_one', 'listen_two'])\n    p2.connect()\n    eq_(canary, ['listen_one', 'listen_two', 'listen_one', 'listen_three'])\n", "label": 0}
{"function": "\n\n@expose('/mkdir/', methods=('GET', 'POST'))\n@expose('/mkdir/<path:path>', methods=('GET', 'POST'))\ndef mkdir(self, path=None):\n    '\\n            Directory creation view method\\n\\n            :param path:\\n                Optional directory path. If not provided, will use the base directory\\n        '\n    (base_path, directory, path) = self._normalize_path(path)\n    dir_url = self._get_dir_url('.index', path)\n    if (not self.can_mkdir):\n        flash(gettext('Directory creation is disabled.'), 'error')\n        return redirect(dir_url)\n    if (not self.is_accessible_path(path)):\n        flash(gettext('Permission denied.'), 'error')\n        return redirect(self._get_dir_url('.index'))\n    form = self.name_form()\n    if self.validate_form(form):\n        try:\n            os.mkdir(op.join(directory, form.name.data))\n            self.on_mkdir(directory, form.name.data)\n            return redirect(dir_url)\n        except Exception as ex:\n            flash(gettext('Failed to create directory: %(error)s', error=ex), 'error')\n    else:\n        helpers.flash_errors(form, message='Failed to create directory: %(error)s')\n    return self.render(self.mkdir_template, form=form, dir_url=dir_url)\n", "label": 0}
{"function": "\n\ndef _validate_int_format(self, name, val):\n    if (val == ''):\n        return\n    try:\n        assert (type(val) in (str, unicode))\n        assert val.isdigit()\n    except AssertionError:\n        raise Exception(('Invalid value for %s!  Must be an integer format string.' % name))\n", "label": 0}
{"function": "\n\ndef login(self):\n    '\\n        Connect to the server using hte login (with credentials) or connect (without).\\n        If login() fails, attempt to perform a fallback method using base64 encoded password and\\n        raw SMTP commands\\n        '\n    self.client = self._get_client()\n    if (self.username and self.password):\n        try:\n            self.client.login(self.username.encode('utf-8'), self.password.encode('utf-8'))\n        except (smtplib.SMTPException, smtplib.SMTPAuthenticationError) as e:\n            self.client.docmd('AUTH LOGIN', base64.b64encode(self.username))\n            self.client.docmd(base64.b64encode(self.password), '')\n    else:\n        self.client.connect()\n", "label": 0}
{"function": "\n\ndef getexecutable(name, cache={\n    \n}):\n    try:\n        return cache[name]\n    except KeyError:\n        executable = py.path.local.sysfind(name)\n        if executable:\n            if (name == 'jython'):\n                import subprocess\n                popen = subprocess.Popen([str(executable), '--version'], universal_newlines=True, stderr=subprocess.PIPE)\n                (out, err) = popen.communicate()\n                if ((not err) or ('2.5' not in err)):\n                    executable = None\n                if ('2.5.2' in err):\n                    executable = None\n        cache[name] = executable\n        return executable\n", "label": 0}
{"function": "\n\ndef test_multi_column_join():\n    a = Symbol('a', 'var * {x: int, y: int, z: int}')\n    b = Symbol('b', 'var * {w: int, x: int, y: int}')\n    j = join(a, b, ['x', 'y'])\n    assert (set(j.fields) == set('wxyz'))\n    assert (j.on_left == j.on_right == ['x', 'y'])\n    assert hash(j)\n    assert (j.fields == ['x', 'y', 'z', 'w'])\n", "label": 0}
{"function": "\n\ndef __init__(self, method):\n    '\\n        :param method: One of SSLv2_METHOD, SSLv3_METHOD, SSLv23_METHOD, or\\n            TLSv1_METHOD.\\n        '\n    if (not isinstance(method, integer_types)):\n        raise TypeError('method must be an integer')\n    try:\n        method_func = self._methods[method]\n    except KeyError:\n        raise ValueError('No such protocol')\n    method_obj = method_func()\n    if (method_obj == _ffi.NULL):\n        _raise_current_error()\n    context = _lib.SSL_CTX_new(method_obj)\n    if (context == _ffi.NULL):\n        _raise_current_error()\n    context = _ffi.gc(context, _lib.SSL_CTX_free)\n    self._context = context\n    self._passphrase_helper = None\n    self._passphrase_callback = None\n    self._passphrase_userdata = None\n    self._verify_helper = None\n    self._verify_callback = None\n    self._info_callback = None\n    self._tlsext_servername_callback = None\n    self._app_data = None\n    self._npn_advertise_helper = None\n    self._npn_advertise_callback = None\n    self._npn_select_helper = None\n    self._npn_select_callback = None\n    self._alpn_select_helper = None\n    self._alpn_select_callback = None\n    self.set_mode(_lib.SSL_MODE_ENABLE_PARTIAL_WRITE)\n", "label": 0}
{"function": "\n\ndef test_failover(self):\n    nthreads = 10\n    client = connected(self.mock_client(localThresholdMS=0.001))\n    wait_until((lambda : (len(client.nodes) == 3)), 'connect to all mongoses')\n    client.kill_host('a:1')\n    passed = []\n\n    def f():\n        try:\n            client.db.command('ismaster')\n        except AutoReconnect:\n            client.db.command('ismaster')\n        passed.append(True)\n    threads = [threading.Thread(target=f) for _ in range(nthreads)]\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n    self.assertEqual(nthreads, len(passed))\n    self.assertEqual(2, len(client.nodes))\n", "label": 0}
{"function": "\n\ndef discover(self, start_dir, pattern='test*.py', top_level_dir=None):\n    \"Find and return all test modules from the specified start\\n        directory, recursing into subdirectories to find them and return all\\n        tests found within them. Only test files that match the pattern will\\n        be loaded. (Using shell style pattern matching.)\\n\\n        All test modules must be importable from the top level of the project.\\n        If the start directory is not the top level directory then the top\\n        level directory must be specified separately.\\n\\n        If a test package name (directory with '__init__.py') matches the\\n        pattern then the package will be checked for a 'load_tests' function. If\\n        this exists then it will be called with loader, tests, pattern.\\n\\n        If load_tests exists then discovery does  *not* recurse into the package,\\n        load_tests is responsible for loading all tests in the package.\\n\\n        The pattern is deliberately not stored as a loader attribute so that\\n        packages can continue discovery themselves. top_level_dir is stored so\\n        load_tests does not need to pass this argument in to loader.discover().\\n        \"\n    set_implicit_top = False\n    if ((top_level_dir is None) and (self._top_level_dir is not None)):\n        top_level_dir = self._top_level_dir\n    elif (top_level_dir is None):\n        set_implicit_top = True\n        top_level_dir = start_dir\n    top_level_dir = os.path.abspath(top_level_dir)\n    if (not (top_level_dir in sys.path)):\n        sys.path.insert(0, top_level_dir)\n    self._top_level_dir = top_level_dir\n    is_not_importable = False\n    if os.path.isdir(os.path.abspath(start_dir)):\n        start_dir = os.path.abspath(start_dir)\n        if (start_dir != top_level_dir):\n            is_not_importable = (not os.path.isfile(os.path.join(start_dir, '__init__.py')))\n    else:\n        try:\n            __import__(start_dir)\n        except ImportError:\n            is_not_importable = True\n        else:\n            the_module = sys.modules[start_dir]\n            top_part = start_dir.split('.')[0]\n            start_dir = os.path.abspath(os.path.dirname(the_module.__file__))\n            if set_implicit_top:\n                self._top_level_dir = self._get_directory_containing_module(top_part)\n                sys.path.remove(top_level_dir)\n    if is_not_importable:\n        raise ImportError(('Start directory is not importable: %r' % start_dir))\n    tests = list(self._find_tests(start_dir, pattern))\n    return self.suiteClass(tests)\n", "label": 1}
{"function": "\n\n@pytest.yield_fixture\ndef sql(url):\n    ds = dshape('var * {A: string, B: int64}')\n    try:\n        t = data((url % next(names)), dshape=ds)\n    except sa.exc.OperationalError as e:\n        pytest.skip(str(e))\n    else:\n        assert (t.dshape == ds)\n        t = data(odo([('a', 1), ('b', 2)], t))\n        try:\n            (yield t)\n        finally:\n            drop(t)\n", "label": 0}
{"function": "\n\ndef attempt(self, explain):\n    \"Attempts to execute the goal's tasks in installed order.\\n\\n    :param bool explain: If ``True`` then the goal plan will be explained instead of being\\n                         executed.\\n    \"\n    goal_workdir = os.path.join(self._context.options.for_global_scope().pants_workdir, self._goal.name)\n    with self._context.new_workunit(name=self._goal.name, labels=[WorkUnitLabel.GOAL]):\n        for (name, task_type) in reversed(self._tasktypes_by_name.items()):\n            task_workdir = os.path.join(goal_workdir, name)\n            task = task_type(self._context, task_workdir)\n            log_config = WorkUnit.LogConfig(level=task.get_options().level, colors=task.get_options().colors)\n            with self._context.new_workunit(name=name, labels=[WorkUnitLabel.TASK], log_config=log_config):\n                if explain:\n                    self._context.log.debug('Skipping execution of {} in explain mode'.format(name))\n                else:\n                    task.execute()\n        if explain:\n            reversed_tasktypes_by_name = reversed(self._tasktypes_by_name.items())\n            goal_to_task = ', '.join(('{}->{}'.format(name, task_type.__name__) for (name, task_type) in reversed_tasktypes_by_name))\n            print('{goal} [{goal_to_task}]'.format(goal=self._goal.name, goal_to_task=goal_to_task))\n", "label": 0}
{"function": "\n\ndef test_postgres_search_path_parsing(self):\n    url = 'postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn?currentSchema=otherschema'\n    url = dj_database_url.parse(url)\n    assert (url['ENGINE'] == 'django.db.backends.postgresql_psycopg2')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 5431)\n    assert (url['OPTIONS']['options'] == '-c search_path=otherschema')\n", "label": 1}
{"function": "\n\ndef sslcli(argv):\n    b'sslcli [-?D]\\nProvides an interactive CLI for managing SSL certificates and CA operations.\\n\\nOptions:\\n   -?        = This help text.\\n   -D        = Debug on.\\n    '\n    import os\n    import getopt\n    try:\n        (optlist, args) = getopt.getopt(argv[1:], b'?hD')\n    except getopt.GetoptError:\n        print(sslcli.__doc__)\n        return\n    for (opt, val) in optlist:\n        if (opt in (b'-?', b'-h')):\n            print(sslcli.__doc__)\n            return\n        elif (opt == b'-D'):\n            from pycopia import autodebug\n    io = CLI.ConsoleIO()\n    ui = CLI.UserInterface(io)\n    ui._env[b'PS1'] = b'SSL> '\n    cmd = SSLCommands(ui)\n    parser = CLI.CommandParser(cmd, historyfile=os.path.expandvars(b'$HOME/.hist_sslcli'))\n    parser.interact()\n", "label": 0}
{"function": "\n\ndef test_attribute_2(self):\n    'Test parsing a float nan attribute'\n    import math\n    xml = '<attribute name=\"missing_value\" type=\"float\" value=\"NaN\"/>'\n    element = ET.fromstring(xml)\n    expected = {\n        'missing_value': [float('NaN')],\n    }\n    actual = self.types.handle_attribute(element)\n    assert (expected.keys() == actual.keys())\n    assert math.isnan(actual['missing_value'][0])\n    assert math.isnan(expected['missing_value'][0])\n", "label": 0}
{"function": "\n\ndef equals(self, other):\n    if (other == 0):\n        return (self.coeff == 0)\n    other = sympify(other)\n    if (not isinstance(other, TensExpr)):\n        assert (not self.components)\n        return (S.One == other)\n\n    def _get_compar_comp(self):\n        t = self.canon_bp()\n        r = (t.coeff, tuple(t.components), tuple(sorted(t.free)), tuple(sorted(t.dum)))\n        return r\n    return (_get_compar_comp(self) == _get_compar_comp(other))\n", "label": 0}
{"function": "\n\ndef assert_lines(cmd, num_lines):\n    ' Assert stdout contains the expected number of lines\\n\\n    :param cmd: program and arguments\\n    :type cmd: [str]\\n    :param num_lines: expected number of lines for stdout\\n    :type num_lines: int\\n    :rtype: None\\n    '\n    (returncode, stdout, stderr) = exec_command(cmd)\n    assert (returncode == 0)\n    assert (stderr == b'')\n    assert ((len(stdout.decode('utf-8').split('\\n')) - 1) == num_lines)\n", "label": 0}
{"function": "\n\ndef c_code(self, node, name, inp, out_, sub):\n    (out,) = out_\n    out_shape = len(inp)\n    out_num = numpy.dtype(node.outputs[0].dtype).num\n    out_dtype = node.outputs[0].type.dtype_specs()[1]\n    if (len(inp) > 0):\n        assert (self.dtype == node.inputs[0].dtype)\n        out_num = ('PyArray_TYPE(%s)' % inp[0])\n    ret = ('\\n        npy_intp dims[1];\\n        dims[0] = %(out_shape)s;\\n        if(!%(out)s || PyArray_DIMS(%(out)s)[0] != %(out_shape)s){\\n            Py_XDECREF(%(out)s);\\n            %(out)s = (PyArrayObject*)PyArray_EMPTY(1, dims, %(out_num)s, 0);\\n        }\\n        ' % locals())\n    for (idx, i) in enumerate(inp):\n        ret += ('\\n            *((%(out_dtype)s *)PyArray_GETPTR1(%(out)s, %(idx)s)) = *((%(out_dtype)s *) PyArray_DATA(%(i)s));\\n            ' % locals())\n    return ret\n", "label": 0}
{"function": "\n\ndef load_color(random_seed=123522):\n    data_path = os.path.join(os.path.split(__file__)[0], 'data')\n    if (not os.path.exists(data_path)):\n        os.makedirs(data_path)\n    dataset = 'train.zip'\n    data_file = os.path.join(data_path, dataset)\n    if os.path.isfile(data_file):\n        dataset = data_file\n    if (not os.path.isfile(data_file)):\n        try:\n            import urllib\n            urllib.urlretrieve('http://google.com')\n        except AttributeError:\n            import urllib.request as urllib\n        url = 'https://dl.dropboxusercontent.com/u/15378192/train.zip'\n        print(('Downloading data from %s' % url))\n        urllib.urlretrieve(url, data_file)\n    data_dir = os.path.join(data_path, 'cvd')\n    if (not os.path.exists(data_dir)):\n        os.makedirs(data_dir)\n        zf = zipfile.ZipFile(data_file)\n        zf.extractall(data_dir)\n    data_file = os.path.join(data_path, 'cvd_color.hdf5')\n    label_file = os.path.join(data_path, 'cvd_color_labels.npy')\n    if (not os.path.exists(data_file)):\n        print('... loading data')\n        cat_matches = []\n        dog_matches = []\n        for (root, dirname, filenames) in os.walk(data_dir):\n            for filename in fnmatch.filter(filenames, 'cat*'):\n                cat_matches.append(os.path.join(root, filename))\n            for filename in fnmatch.filter(filenames, 'dog*'):\n                dog_matches.append(os.path.join(root, filename))\n        sort_key = (lambda x: int(x.split('.')[(- 2)]))\n        cat_matches = sorted(cat_matches, key=sort_key)\n        dog_matches = sorted(dog_matches, key=sort_key)\n\n        def square(X):\n            resize_shape = (64, 64)\n            slice_size = (48, 48)\n            slice_left = ((resize_shape[0] - slice_size[0]) / 2)\n            slice_upper = ((resize_shape[1] - slice_size[1]) / 2)\n            return imresize(X, resize_shape, interp='nearest')[slice_left:(slice_left + slice_size[0]), slice_upper:(slice_upper + slice_size[1])].transpose(2, 0, 1).astype('float32')\n        matches = (cat_matches + dog_matches)\n        matches = np.array(matches)\n        random_state = np.random.RandomState(random_seed)\n        idx = random_state.permutation(len(matches))\n        c = ([0] * len(cat_matches))\n        d = ([1] * len(dog_matches))\n        y = np.array((c + d)).astype('int32')\n        matches = matches[idx]\n        y = y[idx]\n        compression_filter = tables.Filters(complevel=5, complib='blosc')\n        h5_file = tables.openFile(data_file, mode='w')\n        example = square(mpimg.imread(matches[0]))\n        image_storage = h5_file.createEArray(h5_file.root, 'images', tables.Float32Atom(), shape=((0,) + example.shape), filters=compression_filter)\n        for (n, f) in enumerate(matches):\n            print(('Processing image %i of %i' % (n, len(matches))))\n            X = square(mpimg.imread(f)).astype('float32')\n            image_storage.append(X[None])\n        h5_file.close()\n        np.save(label_file, y)\n    h5_file = tables.openFile(data_file, mode='r')\n    X_s = h5_file.root.images\n    y_s = np.load(label_file)\n    return (X_s, y_s)\n", "label": 1}
{"function": "\n\ndef _autodiscover(self):\n    'Discovers modules to register from ``settings.INSTALLED_APPS``.\\n\\n        This makes sure that the appropriate modules get imported to register\\n        themselves with Horizon.\\n        '\n    if (not getattr(self, '_registerable_class', None)):\n        raise ImproperlyConfigured('You must set a \"_registerable_class\" property in order to use autodiscovery.')\n    for mod_name in ('dashboard', 'panel'):\n        for app in settings.INSTALLED_APPS:\n            mod = import_module(app)\n            try:\n                before_import_registry = copy.copy(self._registry)\n                import_module(('%s.%s' % (app, mod_name)))\n            except Exception:\n                self._registry = before_import_registry\n                if module_has_submodule(mod, mod_name):\n                    raise\n", "label": 0}
{"function": "\n\ndef __get__(self, instance, cls=None):\n    if (instance is None):\n        return self\n    try:\n        return getattr(instance, self.cache_attr)\n    except AttributeError:\n        rel_obj = None\n        f = self.model._meta.get_field(self.ct_field)\n        ct_id = getattr(instance, f.get_attname(), None)\n        if (ct_id is not None):\n            ct = self.get_content_type(id=ct_id, using=instance._state.db)\n            try:\n                rel_obj = ct.get_object_for_this_type(pk=getattr(instance, self.fk_field))\n            except ObjectDoesNotExist:\n                pass\n        setattr(instance, self.cache_attr, rel_obj)\n        return rel_obj\n", "label": 0}
{"function": "\n\ndef write_head_pos(fname, pos):\n    'Write MaxFilter-formatted head position parameters\\n\\n    Parameters\\n    ----------\\n    fname : str\\n        The filename to write.\\n    pos : array, shape (N, 10)\\n        The position and quaternion parameters from cHPI fitting.\\n\\n    See Also\\n    --------\\n    read_head_pos\\n    head_pos_to_trans_rot_t\\n\\n    Notes\\n    -----\\n    .. versionadded:: 0.12\\n    '\n    _check_fname(fname, overwrite=True)\n    pos = np.array(pos, np.float64)\n    if ((pos.ndim != 2) or (pos.shape[1] != 10)):\n        raise ValueError('pos must be a 2D array of shape (N, 10)')\n    with open(fname, 'wb') as fid:\n        fid.write(' Time       q1       q2       q3       q4       q5       q6       g-value  error    velocity\\n'.encode('ASCII'))\n        for p in pos:\n            fmts = (['% 9.3f'] + (['% 8.5f'] * 9))\n            fid.write((((' ' + ' '.join(fmts)) + '\\n') % tuple(p)).encode('ASCII'))\n", "label": 0}
{"function": "\n\n@then('the tag expression selects model elements with')\ndef step_given_named_model_elements_with_tags(context):\n    '\\n    .. code-block:: gherkin\\n\\n        Then the tag expression select model elements with:\\n            | tag expression | selected?    |\\n            |  @foo          | S1, S3       |\\n            | -@foo          | S0, S2, S3   |\\n    '\n    assert context.model_elements, 'REQUIRE: context attribute'\n    assert context.table, 'REQUIRE: context.table'\n    context.table.require_columns(['tag expression', 'selected?'])\n    for (row_index, row) in enumerate(context.table.rows):\n        tag_expression_text = row['tag expression']\n        tag_expression = convert_tag_expression(tag_expression_text)\n        expected_selected_names = convert_comma_list(row['selected?'])\n        actual_selected = []\n        for model_element in context.model_elements:\n            if tag_expression.check(model_element.tags):\n                actual_selected.append(model_element.name)\n        assert_that(actual_selected, equal_to(expected_selected_names), ('tag_expression=%s (row=%s)' % (tag_expression_text, row_index)))\n", "label": 0}
{"function": "\n\ndef generateFile(problem, filename=None, content=None, correct=False):\n    '\\n    Uses Problem.solution to generate a problem file. The correct\\n    argument controls whether the generated file is correct or not.\\n    '\n    p = Problem(problem)\n    filename = (filename or p.filename())\n    with open(filename, 'w') as f:\n        if correct:\n            f.write('print({})'.format(p.solution))\n        elif content:\n            f.write(content)\n", "label": 0}
{"function": "\n\ndef set_system_date_time(years=None, months=None, days=None, hours=None, minutes=None, seconds=None):\n    \"\\n    Set the system date and time. Each argument is an element of the date, but\\n    not required. If an element is not passed, the current system value for that\\n    element will be used. For example, if you don't pass the year, the current\\n    system year will be used. (Used by set_system_date and set_system_time)\\n\\n    :param int years: Years digit, ie: 2015\\n    :param int months: Months digit: 1 - 12\\n    :param int days: Days digit: 1 - 31\\n    :param int hours: Hours digit: 0 - 23\\n    :param int minutes: Minutes digit: 0 - 59\\n    :param int seconds: Seconds digit: 0 - 59\\n\\n    :return: True if successful. Otherwise False.\\n    :rtype: bool\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' system.set_system_date_ time 2015 5 12 11 37 53\\n    \"\n    try:\n        date_time = win32api.GetLocalTime()\n    except win32api.error as exc:\n        (number, context, message) = exc\n        log.error('Failed to get local time')\n        log.error('nbr: {0}'.format(number))\n        log.error('ctx: {0}'.format(context))\n        log.error('msg: {0}'.format(message))\n        return False\n    if (not years):\n        years = date_time[0]\n    if (not months):\n        months = date_time[1]\n    if (not days):\n        days = date_time[3]\n    if (not hours):\n        hours = date_time[4]\n    if (not minutes):\n        minutes = date_time[5]\n    if (not seconds):\n        seconds = date_time[6]\n    time_tuple = (years, months, days, hours, minutes, seconds, 0)\n    try:\n        win32api.SetLocalTime(time_tuple)\n    except win32api.error as exc:\n        (number, context, message) = exc\n        log.error('Failed to set local time')\n        log.error('nbr: {0}'.format(number))\n        log.error('ctx: {0}'.format(context))\n        log.error('msg: {0}'.format(message))\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef rs_atan(p, x, prec):\n    \"\\n    The arctangent of a series\\n\\n    Return the series expansion of the atan of ``p``, about 0.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_atan\\n    >>> R, x, y = ring('x, y', QQ)\\n    >>> rs_atan(x + x*y, x, 4)\\n    -1/3*x**3*y**3 - x**3*y**2 - x**3*y - 1/3*x**3 + x*y + x\\n\\n    See Also\\n    ========\\n\\n    atan\\n    \"\n    if rs_is_puiseux(p, x):\n        return rs_puiseux(rs_atan, p, x, prec)\n    R = p.ring\n    const = 0\n    if _has_constant_term(p, x):\n        zm = R.zero_monom\n        c = p[zm]\n        if (R.domain is EX):\n            c_expr = c.as_expr()\n            const = atan(c_expr)\n        elif isinstance(c, PolyElement):\n            try:\n                c_expr = c.as_expr()\n                const = R(atan(c_expr))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        else:\n            try:\n                const = R(atan(c))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n    dp = p.diff(x)\n    p1 = (rs_square(p, x, prec) + R(1))\n    p1 = rs_series_inversion(p1, x, (prec - 1))\n    p1 = rs_mul(dp, p1, x, (prec - 1))\n    return (rs_integrate(p1, x) + const)\n", "label": 0}
{"function": "\n\ndef sync_ldap_groups(self, ldap_groups):\n    '\\n        Synchronize LDAP groups with local group database.\\n        '\n    attributes = getattr(settings, 'LDAP_SYNC_GROUP_ATTRIBUTES', None)\n    groupname_field = 'name'\n    if (groupname_field not in attributes.values()):\n        error_msg = (\"LDAP_SYNC_GROUP_ATTRIBUTES must contain the group name field '%s'\" % groupname_field)\n        raise ImproperlyConfigured(error_msg)\n    for (cname, attrs) in ldap_groups:\n        try:\n            items = attrs.items()\n        except AttributeError:\n            continue\n        group_attr = {\n            \n        }\n        for (name, attr) in items:\n            group_attr[attributes[name]] = attr[0].decode('utf-8')\n        try:\n            groupname = group_attr[groupname_field]\n            group_attr[groupname_field] = groupname.lower()\n        except KeyError:\n            logger.warning((\"Group is missing a required attribute '%s'\" % groupname_field))\n            continue\n        kwargs = {\n            (groupname_field + '__iexact'): groupname,\n            'defaults': group_attr,\n        }\n        try:\n            (group, created) = Group.objects.get_or_create(**kwargs)\n        except IntegrityError as e:\n            logger.error(('Error creating group %s' % e))\n        else:\n            if created:\n                logger.debug(('Created group %s' % groupname))\n    logger.info('Groups are synchronized')\n", "label": 1}
{"function": "\n\ndef _extra_config(user_defined_config, base_dir):\n    'Discover new items in any extra directories and add the new values.\\n\\n    :param user_defined_config: ``dict``\\n    :param base_dir: ``str``\\n    '\n    for (root_dir, _, files) in os.walk(base_dir):\n        for name in files:\n            if name.endswith(('.yml', '.yaml')):\n                with open(os.path.join(root_dir, name), 'rb') as f:\n                    _merge_dict(user_defined_config, (yaml.safe_load(f.read()) or {\n                        \n                    }))\n", "label": 0}
{"function": "\n\ndef test_process_commit():\n    (m, ctl, config) = init()\n    config['numprocesses'] = 0\n    m.load(config, start=False)\n    cmd = TestCommand('commit', ['dummy'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    state = m._get_locked_state('dummy')\n    assert (len(state.running) == 0)\n    assert (state.numprocesses == 0)\n    assert (len(state.running_out) == 1)\n    assert (m.pids() == [1])\n    m.stop()\n    m.run()\n    assert (cmd.result['pid'] == 1)\n", "label": 0}
{"function": "\n\ndef bind_params(self, I, E, O, alpha):\n    assert (I.dtype == E.dtype)\n    if (not self.initialized):\n        self.initialized = True\n        self.autotune(I, E, O)\n    if ((O.dtype.type is not np.float32) or self.determ_size):\n        updat_temp = self.lib.scratch_buffer(self.output_size)\n        image_temp = self.lib.scratch_buffer_offset(self.image_size)\n        delta_temp = self.lib.scratch_buffer_offset(self.delta_size)\n        self.convert_args = [updat_temp, 'f4', O, self.determ_shape]\n    else:\n        updat_temp = O.gpudata\n        image_temp = self.lib.scratch_buffer(self.image_size)\n        delta_temp = self.lib.scratch_buffer_offset(self.delta_size)\n        self.convert_args = False\n    self.image_args[2:5] = (self.lib.stream, image_temp, I.gpudata)\n    self.delta_args[2:5] = (self.lib.stream, delta_temp, E.gpudata)\n    if self.zero:\n        self.zero_args = [updat_temp, 0, O.size, self.lib.stream]\n    self.kernel[3:8] = (self.lib.stream, updat_temp, image_temp, delta_temp, alpha)\n", "label": 0}
{"function": "\n\ndef clear_mode(self, mode, value=None):\n    'Clear mode on the channel.\\n\\n        Arguments:\\n\\n            mode -- The mode (a single-character string).\\n\\n            value -- Value\\n        '\n    try:\n        if (mode == 'o'):\n            del self.operdict[value]\n        elif (mode == 'v'):\n            del self.voiceddict[value]\n        else:\n            del self.modes[mode]\n    except KeyError:\n        pass\n", "label": 0}
{"function": "\n\ndef test_with_existing_data(self, annotate):\n    model = MagicMock(__metadata__={\n        'one': 1,\n        'two': 2,\n    })\n    annotate(model, {\n        'one': '11',\n    })\n    assert (model.__metadata__['one'] == '11')\n    assert (model.__metadata__['two'] == 2)\n    assert (annotate(model) == {\n        'one': '11',\n        'two': 2,\n    })\n", "label": 0}
{"function": "\n\ndef validate(self, object, name, value):\n    ' Validates that the value is a valid font descriptor string.\\n        '\n    try:\n        point_size = family = style = weight = underline = ''\n        facename = ['']\n        for word in value.split():\n            lword = word.lower()\n            if (lword in font_families):\n                family = (' ' + lword)\n            elif (lword in font_styles):\n                style = (' ' + lword)\n            elif (lword in font_weights):\n                weight = (' ' + lword)\n            elif (lword == 'underline'):\n                underline = (' ' + lword)\n            elif (lword not in font_noise):\n                try:\n                    int(lword)\n                    point_size = (lword + ' pt')\n                except:\n                    facename.append(word)\n        fontstr = ('%s%s%s%s%s%s' % (point_size, family, style, weight, underline, ' '.join(facename))).strip()\n        return fontstr\n    except Exception:\n        pass\n    raise TraitError(object, name, 'a font descriptor string', repr(value))\n", "label": 1}
{"function": "\n\ndef read_events(self, timeout=None):\n    timeout_ms = 2147483647\n    if (timeout is not None):\n        timeout_ms = int((timeout * 1000))\n        if ((timeout_ms < 0) or (timeout_ms >= 2147483647)):\n            raise ValueError('Timeout value out of range')\n    try:\n        events = []\n        (rc, num, key, _) = win32file.GetQueuedCompletionStatus(self.__cphandle, timeout_ms)\n        if (rc == 0):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled and (not watch._removed)):\n                    events.extend(process_events(watch, num))\n        elif (rc == 5):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled):\n                    close_watch(watch)\n                    del self.__key_to_watch[key]\n                    events.append(FSEvent(watch, FSEvent.DeleteSelf))\n        return events\n    except pywintypes.error as e:\n        raise FSMonitorWindowsError(*e.args)\n", "label": 1}
{"function": "\n\n@property\ndef select_options(self):\n    start_year = getattr(settings, 'START_YEAR', 2008)\n    years = [dict(val=unicode(y), text=y) for y in range(start_year, (datetime.utcnow().year + 1))]\n    years.reverse()\n    months = [dict(val=('%02d' % m), text=calendar.month_name[m]) for m in range(1, 13)]\n    now = datetime.now()\n    three_month_earlier = (now - relativedelta(months=3))\n    first_friday = rrule(DAILY, dtstart=datetime(three_month_earlier.year, three_month_earlier.month, 1), until=now, byweekday=FR)[0]\n    if (first_friday.day != 1):\n        first_friday = (first_friday - relativedelta(days=7))\n    fridays = rrule(WEEKLY, dtstart=first_friday, until=now, byweekday=FR)\n    weeks = []\n    value = None\n    text = None\n    for (idx, val) in enumerate(fridays):\n        try:\n            value = '{0}|{1}'.format(val.strftime('%Y-%m-%d'), fridays[(idx + 1)].strftime('%Y-%m-%d'))\n            text = '{0} - {1}'.format(ews_date_format(val), ews_date_format((fridays[(idx + 1)] - relativedelta(days=1))))\n        except IndexError:\n            next_thursday = (val + relativedelta(days=6))\n            value = '{0}|{1}'.format(val.strftime('%Y-%m-%d'), next_thursday.strftime('%Y-%m-%d'))\n            text = '{0} - {1}'.format(ews_date_format(val), ews_date_format(next_thursday))\n        finally:\n            weeks.append(dict(val=value, text=text))\n    return [{\n        'text': 'Week (Friday - Thursday)',\n        'val': 2,\n        'firstOptions': weeks,\n        'secondOptions': [],\n    }, {\n        'text': 'Month',\n        'val': 1,\n        'firstOptions': months,\n        'secondOptions': years,\n    }]\n", "label": 0}
{"function": "\n\ndef test_mixedset_cache_miss(self, backend, base_set, base_set2):\n    ms = op2.MixedSet([base_set, base_set2])\n    ms2 = op2.MixedSet([base_set2, base_set])\n    assert (ms is not ms2)\n    assert (ms != ms2)\n    assert (not (ms == ms2))\n    ms3 = op2.MixedSet([base_set, base_set2])\n    assert (ms is ms3)\n    assert (not (ms != ms3))\n    assert (ms == ms3)\n", "label": 0}
{"function": "\n\ndef image_style(image, style):\n    ' Return the url of different image style\\n    Construct appropriately if not exist\\n    See 9cms-crop.odt\\n\\n    Available styles\\n     - thumbnail: create a thumbnail restricted to the smaller dimension\\n     - thumbnail-upscale: create a thumbnail that is upscaled if smaller\\n     - thumbnail-crop: create a thumbnail that is cropped to the exact dimension\\n\\n    :param image: ImageFieldFile\\n    :param style: Specify style to return image\\n    :return: image url of specified style\\n    '\n    if (not image):\n        return image\n    url = image.url\n    url_path = '/'.join(url.split('/')[:(- 1)])\n    img_path_file_name = str(image.file)\n    img_file_name = os.path.basename(img_path_file_name)\n    style_url_path = '/'.join((url_path, style))\n    style_url = '/'.join((style_url_path, img_file_name))\n    style_path = os.path.join(os.path.dirname(img_path_file_name), style)\n    style_path_file_name = os.path.join(style_path, img_file_name)\n    style_def = settings.IMAGE_STYLES[style]\n    if (not os.path.exists(style_path_file_name)):\n        if (not os.path.exists(style_path)):\n            os.makedirs(style_path)\n        by = chr(120)\n        plus = chr(43)\n        try:\n            source_size_str = check_output(['identify', img_path_file_name]).decode()\n        except CalledProcessError:\n            return url\n        source_size_str = source_size_str[len(img_path_file_name):].split(' ')[2]\n        source_size_array = source_size_str.split(by)\n        source_size_x = int(source_size_array[0])\n        source_size_y = int(source_size_array[1])\n        target_size_x = style_def['size'][0]\n        target_size_y = style_def['size'][1]\n        target_size_str = ((str(target_size_x) + by) + str(target_size_y))\n        if (style_def['type'] == 'thumbnail'):\n            if ((target_size_x > source_size_x) and (target_size_y > source_size_y)):\n                target_size_str = source_size_str\n            call(['convert', img_path_file_name, '-thumbnail', target_size_str, '-antialias', style_path_file_name])\n        elif (style_def['type'] == 'thumbnail-upscale'):\n            call(['convert', img_path_file_name, '-thumbnail', target_size_str, '-antialias', style_path_file_name])\n        elif (style_def['type'] == 'thumbnail-crop'):\n            source_ratio = (float(source_size_x) / float(source_size_y))\n            target_ratio = (float(target_size_x) / float(target_size_y))\n            if (source_ratio > target_ratio):\n                crop_target_size_x = (source_size_y * target_ratio)\n                crop_target_size_y = source_size_y\n                offset = ((source_size_x - crop_target_size_x) / 2)\n                crop_size_str = ((((((str(crop_target_size_x) + by) + str(crop_target_size_y)) + plus) + str(offset)) + plus) + '0')\n            else:\n                crop_target_size_x = source_size_x\n                crop_target_size_y = (source_size_x / target_ratio)\n                offset = ((source_size_y - crop_target_size_y) / 2)\n                crop_size_str = ((((((str(crop_target_size_x) + by) + str(crop_target_size_y)) + plus) + '0') + plus) + str(offset))\n            call(['convert', img_path_file_name, '-crop', crop_size_str, style_path_file_name])\n            call(['convert', style_path_file_name, '-thumbnail', target_size_str, '-antialias', style_path_file_name])\n    return style_url\n", "label": 1}
{"function": "\n\ndef getTempPath(file_name=False):\n    tmp_path = '/tmp'\n    os_name = Tools.getOsName()\n    if (os_name == 'windows'):\n        tmp_path = os.environ['tmp']\n    tmp_path = os.path.join(tmp_path, 'Deviot')\n    if file_name:\n        tmp_path = os.path.join(tmp_path, file_name)\n    try:\n        os.makedirs(tmp_path)\n    except OSError as exc:\n        if (exc.errno != errno.EEXIST):\n            raise exc\n        pass\n    return tmp_path\n", "label": 0}
{"function": "\n\ndef getas(self, convert, name, default=None, valuetype=None):\n    'Converts the value of user-data parameter from a string into a\\n        specific value type.\\n\\n        :param convert: Converter function to use (string to value-type).\\n        :param name:    Variable name to use.\\n        :param default: Default value, used if parameter is not found.\\n        :param valuetype: Value type(s), needed if convert != valuetype()\\n        :return: Converted textual value (type: valuetype)\\n        :return: Default value, if parameter is unknown.\\n        :raises ValueError: If type conversion fails.\\n        '\n    if (valuetype is None):\n        valuetype = convert\n    value = self.get(name, Unknown)\n    if (value is Unknown):\n        return default\n    elif isinstance(value, valuetype):\n        return value\n    else:\n        assert callable(convert)\n        return convert(value)\n", "label": 0}
{"function": "\n\ndef test_augmented_assignment(self, space):\n    self.assert_compiles(space, 'i = 0; i += 1', '\\n        LOAD_CONST 0\\n        STORE_DEREF 0\\n        DISCARD_TOP\\n\\n        LOAD_DEREF 0\\n        LOAD_CONST 1\\n        SEND 2 1\\n        STORE_DEREF 0\\n\\n        RETURN\\n        ')\n    bc = self.assert_compiles(space, 'self.x.y += 1', '\\n        LOAD_SELF\\n        SEND 0 0\\n        DUP_TOP\\n        SEND 1 0\\n        LOAD_CONST 2\\n        SEND 3 1\\n        SEND 4 1\\n\\n        RETURN\\n        ')\n    assert (space.symbol_w(bc.consts_w[0]) == 'x')\n    assert (space.symbol_w(bc.consts_w[1]) == 'y')\n    assert (space.symbol_w(bc.consts_w[3]) == '+')\n    assert (space.symbol_w(bc.consts_w[4]) == 'y=')\n    self.assert_compiles(space, '@a += 2', '\\n        LOAD_SELF\\n        DUP_TOP\\n        LOAD_INSTANCE_VAR 0\\n        LOAD_CONST 1\\n        SEND 2 1\\n        STORE_INSTANCE_VAR 0\\n\\n        RETURN\\n        ')\n", "label": 0}
{"function": "\n\ndef shell_exec_monitor(self, Command, PluginInfo):\n    CommandInfo = self.StartCommand(Command, Command)\n    (Target, CanRun) = self.CanRunCommand(CommandInfo)\n    if (not CanRun):\n        Message = ('The command was already run for target: ' + str(Target))\n        return Message\n    logging.info('')\n    logging.info('Executing :\\n\\n%s\\n\\n', Command)\n    logging.info('')\n    logging.info(('------> Execution Start Date/Time: ' + self.timer.get_start_date_time_as_str('Command')))\n    logging.info('')\n    Output = ''\n    Cancelled = False\n    try:\n        proc = self.create_subprocess(Command)\n        while True:\n            line = proc.stdout.readline()\n            if (not line):\n                break\n            logging.warn(line.strip())\n            Output += line\n    except KeyboardInterrupt:\n        os.killpg(proc.pid, signal.SIGINT)\n        (outdata, errdata) = proc.communicate()\n        logging.warn(outdata)\n        Output += outdata\n        try:\n            os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n        except OSError:\n            pass\n        Cancelled = True\n        Output += self.error_handler.UserAbort('Command', Output)\n    finally:\n        self.FinishCommand(CommandInfo, Cancelled, PluginInfo)\n    return scrub_output(Output)\n", "label": 0}
{"function": "\n\ndef get_or_create(self, question_text, choice_texts):\n    try:\n        question = Question.objects.filter(question_text=question_text).first()\n    except Question.DoesNotExist:\n        question = None\n    if question:\n        choices = map((lambda c: c.choice_text), question.choices.order_by('choice_text'))\n        if (choices == sorted(choice_texts)):\n            return (question, False)\n    return (self.create_question(question_text, choice_texts), True)\n", "label": 0}
{"function": "\n\ndef _after_flush(app, changes):\n    bytype = {\n        \n    }\n    for change in changes:\n        update = (change[1] in ('update', 'insert'))\n        if hasattr(change[0].__class__, __searchable__):\n            bytype.setdefault(change[0].__class__.__name__, []).append((update, change[0]))\n    for (model, values) in bytype.iteritems():\n        index = whoosh_index(app, values[0][1].__class__)\n        with index.writer() as writer:\n            primary_field = values[0][1].pure_whoosh.primary_key_name\n            searchable = values[0][1].__searchable__\n            for (update, v) in values:\n                if update:\n                    attrs = {\n                        \n                    }\n                    for key in searchable:\n                        try:\n                            attrs[key] = unicode(getattr(v, key))\n                        except AttributeError:\n                            raise AttributeError('{0} does not have {1} field {2}'.format(model, __searchable__, key))\n                    attrs[primary_field] = unicode(getattr(v, primary_field))\n                    writer.update_document(**attrs)\n                else:\n                    writer.delete_by_term(primary_field, unicode(getattr(v, primary_field)))\n", "label": 1}
{"function": "\n\ndef ssl_settings(host, config_file, env=os.environ):\n    \"\\n    Function wcich generates SSL setting for cassandra.Cluster\\n\\n    Params:\\n    * host .........: hostname of Cassandra node.\\n    * env ..........: environment variables. SSL factory will use, if passed,\\n                      SSL_CERTFILE and SSL_VALIDATE variables.\\n    * config_file ..: path to cqlsh config file (usually ~/.cqlshrc).\\n                      SSL factory will use, if set, certfile and validate\\n                      options in [ssl] section, as well as host to certfile\\n                      mapping in [certfiles] section.\\n\\n    [certfiles] section is optional, 'validate' setting in [ssl] section is\\n    optional too. If validation is enabled then SSL certfile must be provided\\n    either in the config file or as an environment variable.\\n    Environment variables override any options set in cqlsh config file.\\n    \"\n    configs = ConfigParser.SafeConfigParser()\n    configs.read(config_file)\n\n    def get_option(section, option):\n        try:\n            return configs.get(section, option)\n        except ConfigParser.Error:\n            return None\n    ssl_validate = env.get('SSL_VALIDATE')\n    if (ssl_validate is None):\n        ssl_validate = get_option('ssl', 'validate')\n    ssl_validate = ((ssl_validate is None) or (ssl_validate.lower() != 'false'))\n    ssl_certfile = env.get('SSL_CERTFILE')\n    if (ssl_certfile is None):\n        ssl_certfile = get_option('certfiles', host)\n    if (ssl_certfile is None):\n        ssl_certfile = get_option('ssl', 'certfile')\n    if (ssl_validate and (ssl_certfile is None)):\n        sys.exit((\"Validation is enabled; SSL transport factory requires a valid certfile to be specified. Please provide path to the certfile in [ssl] section as 'certfile' option in %s (or use [certfiles] section) or set SSL_CERTFILE environment variable.\" % (config_file,)))\n    if (not (ssl_certfile is None)):\n        ssl_certfile = os.path.expanduser(ssl_certfile)\n    userkey = get_option('ssl', 'userkey')\n    if userkey:\n        userkey = os.path.expanduser(userkey)\n    usercert = get_option('ssl', 'usercert')\n    if usercert:\n        usercert = os.path.expanduser(usercert)\n    return dict(ca_certs=ssl_certfile, cert_reqs=(ssl.CERT_REQUIRED if ssl_validate else ssl.CERT_NONE), ssl_version=ssl.PROTOCOL_TLSv1, keyfile=userkey, certfile=usercert)\n", "label": 1}
{"function": "\n\ndef poll(self, event, timeout):\n    if (event == 'recv'):\n        with self.recv_ready:\n            if (len(self.recv_queue) == 0):\n                self.recv_ready.wait(timeout)\n            if (len(self.recv_queue) > 0):\n                return self.recv_queue[0].type\n            return None\n    if (event == 'send'):\n        with self.send_ready:\n            if (len(self.send_queue) >= self.send_buf):\n                self.send_ready.wait(timeout)\n            return (len(self.send_queue) < self.send_buf)\n", "label": 0}
{"function": "\n\ndef reset(self, **traits):\n    'Creates the dataset afresh or resets existing data source.'\n    self.set(trait_change_notify=False, **traits)\n    points = self.points\n    scalars = self.scalars\n    (x, y, z) = (self.x, self.y, self.z)\n    points = np.c_[(x.ravel(), y.ravel(), z.ravel())].ravel()\n    points.shape = ((- 1), 3)\n    self.set(points=points, trait_change_notify=False)\n    triangles = self.triangles\n    assert (triangles.shape[1] == 3), 'The shape of the triangles array must be (X, 3)'\n    assert (triangles.max() < len(points)), 'The triangles indices must be smaller that the number of points'\n    assert (triangles.min() >= 0), 'The triangles indices must be positive or null'\n    if (self.dataset is None):\n        pd = tvtk.PolyData()\n    else:\n        pd = self.dataset\n    pd.set(points=points)\n    pd.set(polys=triangles)\n    if ((not ('scalars' in traits)) and (scalars is not None) and (scalars.shape != x.shape)):\n        scalars = z\n    if ((scalars is not None) and (len(scalars) > 0)):\n        if (not scalars.flags.contiguous):\n            scalars = scalars.copy()\n            self.set(scalars=scalars, trait_change_notify=False)\n        assert (x.shape == scalars.shape)\n        pd.point_data.scalars = scalars.ravel()\n        pd.point_data.scalars.name = 'scalars'\n    self.dataset = pd\n", "label": 1}
{"function": "\n\ndef parse_cluster_position(_cluster_position):\n    try:\n        if isinstance(_cluster_position, str):\n            pass\n        elif isinstance(_cluster_position, int):\n            _cluster_position = str(_cluster_position)\n        elif isinstance(_cluster_position, bytes):\n            _cluster_position = _cluster_position.decode('utf-8')\n        elif isinstance(_cluster_position, OrientRecordLink):\n            _cluster_position = _cluster_position.get()\n        (_cluster, _position) = _cluster_position.split(':')\n    except (AttributeError, ValueError):\n        _position = _cluster_position\n    return _position\n", "label": 0}
{"function": "\n\ndef import_submodules(module):\n    'Import all submodules and make them available in a dict.'\n    submodules = {\n        \n    }\n    for (loader, name, ispkg) in pkgutil.iter_modules(module.__path__, (module.__name__ + '.')):\n        try:\n            submodule = importlib.import_module(name)\n        except ImportError as e:\n            logging.warning(('Error importing %s' % name))\n            logging.exception(e)\n        else:\n            (parent, child) = name.rsplit('.', 1)\n            submodules[child] = submodule\n    return submodules\n", "label": 0}
{"function": "\n\ndef test_read_write(self):\n    record_type = FCGI_DATA\n    request_id = randint(1, 65535)\n    data = binary_data()\n    record = Record(record_type, data, request_id)\n    conn = Connection(self.sock)\n    conn.write_record(record)\n    self.sock.flip()\n    record = conn.read_record()\n    assert (record.type == record_type)\n    assert (record.content == data)\n    assert (record.request_id == request_id)\n    assert (conn.read_record() is None)\n", "label": 0}
{"function": "\n\ndef test_nested_block(self, space):\n    bc = self.assert_compiles(space, '\\n        sums = []\\n        [].each do |x|\\n            [].each do |y|\\n                sums << x + y\\n            end\\n        end\\n        ', '\\n        BUILD_ARRAY 0\\n        STORE_DEREF 0\\n        DISCARD_TOP\\n\\n        BUILD_ARRAY 0\\n        LOAD_CONST 0\\n        LOAD_CLOSURE 0\\n        BUILD_BLOCK 1\\n        SEND_BLOCK 1 1\\n\\n        RETURN\\n        ')\n    assert (bc.freevars == [])\n    assert (bc.cellvars == ['sums'])\n    self.assert_compiled(bc.consts_w[0], '\\n        BUILD_ARRAY 0\\n        LOAD_CONST 0\\n        LOAD_CLOSURE 0\\n        LOAD_CLOSURE 1\\n        BUILD_BLOCK 2\\n        SEND_BLOCK 1 1\\n        RETURN\\n        ')\n    assert (bc.consts_w[0].freevars == ['sums'])\n    assert (bc.consts_w[0].cellvars == ['x'])\n    self.assert_compiled(bc.consts_w[0].consts_w[0], '\\n        LOAD_DEREF 1\\n        LOAD_DEREF 2\\n        LOAD_DEREF 0\\n        SEND 0 1\\n        SEND 1 1\\n        RETURN\\n        ')\n    assert (bc.consts_w[0].consts_w[0].freevars == ['sums', 'x'])\n    assert (bc.consts_w[0].consts_w[0].cellvars == ['y'])\n", "label": 0}
{"function": "\n\n@classmethod\ndef validate(cls, data):\n    try:\n        su.schema_validate(data, cls.SCHEMA)\n    except su.ValidationError as e:\n        cls_name = reflection.get_class_name(cls, fully_qualified=False)\n        excp.raise_with_cause(excp.InvalidFormat, ('%s message response data not of the expected format: %s' % (cls_name, e.message)), cause=e)\n    else:\n        failures = []\n        if ('failures' in data):\n            failures.extend(six.itervalues(data['failures']))\n        result = data.get('result')\n        if (result is not None):\n            (result_data_type, result_data) = result\n            if (result_data_type == 'failure'):\n                failures.append(result_data)\n        for fail_data in failures:\n            ft.Failure.validate(fail_data)\n", "label": 0}
{"function": "\n\ndef isDBPort(host, port, db, timeout=10):\n    if (host == JSONFILE_HOSTNAME):\n        return True\n    t = 2\n    while (t < timeout):\n        try:\n            conn = urllib.request.urlopen('http://{0}:{1}/{2}/status'.format(host, (port or '80'), db))\n            return True\n        except HTTPError:\n            return False\n        except URLError:\n            return False\n        except socket.timeout:\n            t = (t + 2)\n    return False\n", "label": 0}
{"function": "\n\ndef assert_raises_message(except_cls, msg, callable_, *args, **kwargs):\n    try:\n        callable_(*args, **kwargs)\n        assert False, 'Callable did not raise an exception'\n    except except_cls as e:\n        assert re.search(msg, util.text_type(e), re.UNICODE), ('%r !~ %s' % (msg, e))\n        print(util.text_type(e).encode('utf-8'))\n", "label": 0}
{"function": "\n\ndef test_mysql_database_url_with_sslca_options(self):\n    os.environ['DATABASE_URL'] = 'mysql://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:3306/d8r82722r2kuvn?ssl-ca=rds-combined-ca-bundle.pem'\n    url = dj_database_url.config()\n    assert (url['ENGINE'] == 'django.db.backends.mysql')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 3306)\n    assert (url['OPTIONS'] == {\n        'ssl': {\n            'ca': 'rds-combined-ca-bundle.pem',\n        },\n    })\n    os.environ['DATABASE_URL'] = 'mysql://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:3306/d8r82722r2kuvn?'\n    url = dj_database_url.config()\n    assert ('OPTIONS' not in url)\n", "label": 1}
{"function": "\n\ndef __call__(self, task, stm_queue=None, solver_id=None):\n    try:\n        cnf_path = task.support_paths.get('cnf-g')\n        if (cnf_path is None):\n            (fd, cnf_path) = tempfile.mkstemp(suffix='.cnf')\n            task.support_paths['cnf-g'] = cnf_path\n            with contextlib.closing(os.fdopen(fd)) as cnf_file:\n                with open(task.path, 'rb') as asp_file:\n                    borg.domains.asp.run_lp2sat(self._domain.binaries_path, asp_file, cnf_file)\n    except borg.domains.asp.LP2SAT_FailedException:\n        return borg.solver_io.EmptySolver(None)\n    else:\n        with borg.get_domain('sat').task_from_path(cnf_path) as sat_task:\n            return self._sat_factory(sat_task)\n", "label": 0}
{"function": "\n\ndef test_flatline_query_key():\n    rules = {\n        'timeframe': datetime.timedelta(seconds=30),\n        'threshold': 1,\n        'use_query_key': True,\n        'query_key': 'qk',\n        'timestamp_field': '@timestamp',\n    }\n    rule = FlatlineRule(rules)\n    rule.add_data(hits(1, qk='key1'))\n    rule.add_data(hits(1, qk='key2'))\n    rule.add_data(hits(1, qk='key3'))\n    assert (rule.matches == [])\n    rule.garbage_collect(ts_to_dt('2014-09-26T12:00:11Z'))\n    assert (rule.matches == [])\n    rule.add_data([create_event(ts_to_dt('2014-09-26T12:00:20Z'), qk='key3')])\n    timestamp = '2014-09-26T12:00:45Z'\n    rule.garbage_collect(ts_to_dt(timestamp))\n    assert (len(rule.matches) == 2)\n    assert (set(['key1', 'key2']) == set([m['key'] for m in rule.matches if (m['@timestamp'] == timestamp)]))\n    timestamp = '2014-09-26T12:01:20Z'\n    rule.garbage_collect(ts_to_dt(timestamp))\n    assert (len(rule.matches) == 3)\n    assert (set(['key3']) == set([m['key'] for m in rule.matches if (m['@timestamp'] == timestamp)]))\n", "label": 0}
{"function": "\n\ndef parse_data(infile):\n    'Parse data from `infile`.'\n    blocks = re.compile(' '.join(([('=' * 9)] * 8)))\n    dashes = re.compile('^-{79}$')\n    title = re.compile('^Timings for (.*)$')\n    row = re.compile((' '.join((['(.{9})'] * 7)) + ' (.{8,9})'))\n    lines = infile.readlines()\n    data = co.OrderedDict()\n    index = 0\n    while (index < len(lines)):\n        line = lines[index]\n        if blocks.match(line):\n            try:\n                name = title.match(lines[(index + 1)]).group(1)\n            except:\n                index += 1\n                continue\n            data[name] = {\n                \n            }\n            assert dashes.match(lines[(index + 2)])\n            cols = parse_row(row, lines[(index + 3)])\n            assert blocks.match(lines[(index + 4)])\n            get_row = parse_row(row, lines[(index + 5)])\n            assert (get_row[0] == 'get')\n            set_row = parse_row(row, lines[(index + 6)])\n            assert (set_row[0] == 'set')\n            delete_row = parse_row(row, lines[(index + 7)])\n            assert (delete_row[0] == 'delete')\n            assert blocks.match(lines[(index + 9)])\n            data[name]['get'] = dict(zip(cols, get_row))\n            data[name]['set'] = dict(zip(cols, set_row))\n            data[name]['delete'] = dict(zip(cols, delete_row))\n            index += 10\n        else:\n            index += 1\n    return data\n", "label": 1}
{"function": "\n\ndef test_dist(self):\n    data = [0, 1, 1, 2, 3, 3, 3, 4, 6, 6, 6, 7, 7, 8]\n    random.shuffle(data)\n    frange = accumulate.FloatRange(1, 7, 6)\n    assert (frange.end == 7.0)\n    self.assertEqual(frange.get_bin(6), (6.0, 7.0))\n    acc = accumulate.NumericAccumulator()\n    acc.consume_all(data)\n    self.assertEqual(acc, sorted(data))\n    dist = acc.get_dist(frange)\n    expected_dist = {\n        None: 4,\n        (1.0, 2.0): 2,\n        (2.0, 3.0): 1,\n        (3.0, 4.0): 3,\n        (4.0, 5.0): 1,\n        (5.0, 6.0): 0,\n        (6.0, 7.0): 3,\n    }\n    for k in expected_dist:\n        self.assertEqual(dist[k], expected_dist[k])\n    assert set(dist).issubset(set(expected_dist))\n", "label": 0}
{"function": "\n\ndef run(self, _tokens=1, **kwargs):\n    restart_limit = self.restart_limit\n    errors = (self.connection.connection_errors + self.connection.channel_errors)\n    while (not self.should_stop):\n        try:\n            if restart_limit.can_consume(_tokens):\n                for _ in self.consume(limit=None, **kwargs):\n                    pass\n            else:\n                sleep(restart_limit.expected_time(_tokens))\n        except errors:\n            warn(W_CONN_LOST, exc_info=1)\n", "label": 0}
{"function": "\n\ndef _find_exe_in_registry(self):\n    try:\n        from _winreg import OpenKey, QueryValue, HKEY_LOCAL_MACHINE, HKEY_CURRENT_USER\n    except ImportError:\n        from winreg import OpenKey, QueryValue, HKEY_LOCAL_MACHINE, HKEY_CURRENT_USER\n    import shlex\n    keys = ('SOFTWARE\\\\Classes\\\\FirefoxHTML\\\\shell\\\\open\\\\command', 'SOFTWARE\\\\Classes\\\\Applications\\\\firefox.exe\\\\shell\\\\open\\\\command')\n    command = ''\n    for path in keys:\n        try:\n            key = OpenKey(HKEY_LOCAL_MACHINE, path)\n            command = QueryValue(key, '')\n            break\n        except OSError:\n            try:\n                key = OpenKey(HKEY_CURRENT_USER, path)\n                command = QueryValue(key, '')\n                break\n            except OSError:\n                pass\n    else:\n        return ''\n    if (not command):\n        return ''\n    return shlex.split(command)[0]\n", "label": 0}
{"function": "\n\ndef update_database_info(self):\n    ' Queries the AppController for information about what datastore is used\\n    to implement support for the Google App Engine Datastore API, placing this\\n    info in the Datastore for later viewing.\\n\\n    This update is only performed if there is no data in the Datastore about the\\n    current location of the head node, as this is unlikely to dynamically change\\n    at this time.\\n\\n    Returns:\\n      A dict containing the name of the database used (a str), as well as the\\n      number of replicas for each piece of data (an int).\\n    '\n    dashboard_root = self.get_by_id(DashboardDataRoot, self.ROOT_KEYNAME)\n    if (dashboard_root and (dashboard_root.table is not None) and (dashboard_root.replication is not None)):\n        return {\n            'table': dashboard_root.table,\n            'replication': dashboard_root.replication,\n        }\n    try:\n        acc = self.helper.get_appcontroller_client()\n        db_info = acc.get_database_information()\n        if (dashboard_root is None):\n            dashboard_root = DashboardDataRoot(id=self.ROOT_KEYNAME)\n        dashboard_root.table = db_info['table']\n        dashboard_root.replication = int(db_info['replication'])\n        dashboard_root.put()\n        return {\n            'table': dashboard_root.table,\n            'replication': dashboard_root.replication,\n        }\n    except Exception as err:\n        logging.exception(err)\n        return {\n            'table': 'unknown',\n            'replication': 0,\n        }\n", "label": 0}
{"function": "\n\ndef process_explain(self, seqid, iprot, oprot):\n    args = explain_args()\n    args.read(iprot)\n    iprot.readMessageEnd()\n    result = explain_result()\n    try:\n        result.success = self._handler.explain(args.query)\n        msg_type = TMessageType.REPLY\n    except (TTransport.TTransportException, KeyboardInterrupt, SystemExit):\n        raise\n    except BeeswaxException as error:\n        msg_type = TMessageType.REPLY\n        result.error = error\n    except Exception as ex:\n        msg_type = TMessageType.EXCEPTION\n        logging.exception(ex)\n        result = TApplicationException(TApplicationException.INTERNAL_ERROR, 'Internal error')\n    oprot.writeMessageBegin('explain', msg_type, seqid)\n    result.write(oprot)\n    oprot.writeMessageEnd()\n    oprot.trans.flush()\n", "label": 0}
{"function": "\n\ndef run(self):\n    if (not have_bonjour):\n        logging.warn('Cannot run bonjour server!  Maybe some packages need to be installed?')\n        return\n    sdRef = pybonjour.DNSServiceRegister(name=self.name, regtype=self.regtype, port=self.port, callBack=self.register_callback)\n    try:\n        try:\n            while True:\n                ready = select.select([sdRef], [], [])\n                if (sdRef in ready[0]):\n                    pybonjour.DNSServiceProcessResult(sdRef)\n        except KeyboardInterrupt:\n            pass\n    finally:\n        sdRef.close()\n", "label": 0}
{"function": "\n\n@utt.AttemptManyTimes(n_attempts=3, n_req_successes=1)\ndef test_run_nnet():\n    for n_in in (1024, 2048, 4096):\n        for n_hid in (1024, 2048, 4096):\n            utt.seed_rng()\n            (rval_cpu, tc) = run_nnet(False, n_in=n_in, n_hid=n_hid)\n            utt.seed_rng()\n            (rval_gpu, tg) = run_nnet(True, n_in=n_in, n_hid=n_hid)\n            (abs_diff, rel_diff) = theano.gradient.numeric_grad.abs_rel_err(rval_gpu, rval_cpu)\n            max_abs_diff = abs_diff.max()\n            rtol = 0.0001\n            if ((n_in * n_hid) >= (2048 * 4096)):\n                rtol = 0.0007\n            assert numpy.allclose(rval_cpu, rval_gpu, rtol=rtol, atol=1e-06), ('max_abs_diff, max_rel_diff, n_in, n_hid', max_abs_diff, rel_diff.max(), n_in, n_hid)\n", "label": 0}
{"function": "\n\ndef test_notch_filters():\n    'Test notch filters\\n    '\n    sfreq = 487.0\n    sig_len_secs = 20\n    t = (np.arange(0, int((sig_len_secs * sfreq))) / sfreq)\n    freqs = np.arange(60, 241, 60)\n    a = rng.randn(int((sig_len_secs * sfreq)))\n    orig_power = np.sqrt(np.mean((a ** 2)))\n    a += np.sum([np.sin((((2 * np.pi) * f) * t)) for f in freqs], axis=0)\n    assert_raises(ValueError, notch_filter, a, sfreq, None, 'fft')\n    assert_raises(ValueError, notch_filter, a, sfreq, None, 'iir')\n    methods = ['spectrum_fit', 'spectrum_fit', 'fft', 'fft', 'iir']\n    filter_lengths = [None, None, None, 8192, None]\n    line_freqs = [None, freqs, freqs, freqs, freqs]\n    tols = [2, 1, 1, 1]\n    for (meth, lf, fl, tol) in zip(methods, line_freqs, filter_lengths, tols):\n        with catch_logging() as log_file:\n            b = notch_filter(a, sfreq, lf, filter_length=fl, method=meth, verbose='INFO')\n        if (lf is None):\n            out = log_file.getvalue().split('\\n')[:(- 1)]\n            if ((len(out) != 2) and (len(out) != 3)):\n                raise ValueError('Detected frequencies not logged properly')\n            out = np.fromstring(out[(- 1)], sep=', ')\n            assert_array_almost_equal(out, freqs)\n        new_power = np.sqrt((sum_squared(b) / b.size))\n        assert_almost_equal(new_power, orig_power, tol)\n", "label": 0}
{"function": "\n\ndef __exit__(self, exc_type, exc_value, exc_traceback):\n    'Auto called on transaction (with statement) exit.\\n\\n        Commits the transaction unless there has been an exception,\\n        in which case it rolls back the transaction. Restores the\\n        old commit mode to the database connection.\\n\\n        This implicitly returns None which causes any exception\\n        to be re-raised.\\n        '\n    if (exc_type is None):\n        self.__commit()\n    else:\n        self.__rollback()\n    self.tile_remove_list = None\n    self.tile_create_list = None\n    self.db.restore_commit_mode(self.previous_commit_mode)\n    if (self.tr_stack is not None):\n        tr = self.tr_stack.pop()\n        assert (tr is self), 'Unexpected value on transaction stack.'\n", "label": 0}
{"function": "\n\ndef __init__(self, op_tree, ad):\n    '\\n        op_tree: the op_tree at this grad_node\\n        ad: the autodiff object with global op_tree, next_error and dicts\\n        '\n    assert (op_tree is not None)\n    self.op_tree = op_tree\n    self.grad_op_tree = None\n    self.ad = ad\n    self.left = None\n    self.right = None\n    if isinstance(op_tree, Tensor):\n        if (op_tree._original_base not in ad.map_tensor_grad_node):\n            ad.map_tensor_grad_node[op_tree._original_base] = self\n    elif (type(op_tree) == OpTreeNode):\n        if (op_tree[1] is not None):\n            if (isinstance(op_tree[1], Tensor) and (op_tree[1]._original_base in ad.map_tensor_grad_node)):\n                self.left = ad.map_tensor_grad_node[op_tree[1]._original_base]\n            else:\n                self.left = GradNode(op_tree[1], ad)\n        if (op_tree[2] is not None):\n            if (isinstance(op_tree[2], Tensor) and (op_tree[2]._original_base in ad.map_tensor_grad_node)):\n                self.right = ad.map_tensor_grad_node[op_tree[2]._original_base]\n            else:\n                self.right = GradNode(op_tree[2], ad)\n", "label": 1}
{"function": "\n\ndef test_accounting_commit_fails_delete(self):\n    User = self.classes.User\n    sess = create_session(autocommit=True)\n    fail = False\n\n    def fail_fn(*arg, **kw):\n        if fail:\n            raise Exception('commit fails')\n    event.listen(sess, 'after_flush_postexec', fail_fn)\n    u1 = User(name='ed')\n    sess.add(u1)\n    sess.flush()\n    sess.delete(u1)\n    fail = True\n    assert_raises(Exception, sess.flush)\n    fail = False\n    assert (u1 in sess)\n    assert (u1 not in sess.deleted)\n    sess.delete(u1)\n    sess.flush()\n    assert (u1 not in sess)\n    eq_(sess.query(User.name).order_by(User.name).all(), [])\n", "label": 0}
{"function": "\n\ndef request_new_domain(request, form, is_new_user=True):\n    now = datetime.utcnow()\n    current_user = CouchUser.from_django_user(request.user)\n    dom_req = RegistrationRequest()\n    if is_new_user:\n        dom_req.request_time = now\n        dom_req.request_ip = get_ip(request)\n        dom_req.activation_guid = uuid.uuid1().hex\n    name = name_to_url(form.cleaned_data['hr_name'], 'project')\n    with CriticalSection(['request_domain_name_{}'.format(name)]):\n        name = Domain.generate_name(name)\n        new_domain = Domain(name=name, hr_name=form.cleaned_data['hr_name'], is_active=False, date_created=datetime.utcnow(), creating_user=current_user.username, secure_submissions=True)\n        if form.cleaned_data.get('domain_timezone'):\n            new_domain.default_timezone = form.cleaned_data['domain_timezone']\n        if (not is_new_user):\n            new_domain.is_active = True\n        new_domain.save(**get_safe_write_kwargs())\n    if (not new_domain.name):\n        new_domain.name = new_domain._id\n        new_domain.save()\n    if is_new_user:\n        create_30_day_advanced_trial(new_domain)\n    UserRole.init_domain_with_presets(new_domain.name)\n    dom_req.domain = new_domain.name\n    if request.user.is_authenticated():\n        if (not current_user):\n            current_user = WebUser()\n            current_user.sync_from_django_user(request.user)\n            current_user.save()\n        current_user.add_domain_membership(new_domain.name, is_admin=True)\n        current_user.save()\n        dom_req.requesting_user_username = request.user.username\n        dom_req.new_user_username = request.user.username\n    if is_new_user:\n        dom_req.save()\n        send_domain_registration_email(request.user.email, dom_req.domain, dom_req.activation_guid, request.user.get_full_name())\n    else:\n        send_global_domain_registration_email(request.user, new_domain.name)\n    send_new_request_update_email(request.user, get_ip(request), new_domain.name, is_new_user=is_new_user)\n    meta = get_meta(request)\n    track_created_new_project_space_on_hubspot.delay(current_user, request.COOKIES, meta)\n    return new_domain.name\n", "label": 1}
{"function": "\n\ndef begin(self):\n    history = []\n    unlocked = {\n        \n    }\n    if self.data_filename:\n        try:\n            data_file = open(self.data_filename, 'rb')\n        except IOError:\n            log.debug('Failed to read achievement data from %s', self.data_filename)\n        else:\n            data = AchievementData.load(data_file)\n            data_file.close()\n            log.info('Loaded achievement data from %s', self.data_filename)\n            history = data.pop('history', history)\n            history.append(data)\n            del history[:(- 10)]\n            unlocked = data.get('achievements.unlocked', unlocked)\n    self.data.setdefault('history', history)\n    self.data.setdefault('achievements.unlocked', unlocked)\n    self.data.setdefault('achievements.new', [])\n    self.data.setdefault('result.tests', [])\n    self.data.setdefault('result.string', '')\n    self.data.setdefault('result.errors', [])\n    self.data.setdefault('result.failures', [])\n    self.data.setdefault('time.start', datetime.now())\n", "label": 0}
{"function": "\n\ndef put_versioning(Bucket, Status, MFADelete=None, MFA=None, region=None, key=None, keyid=None, profile=None):\n    '\\n    Given a valid config, update the versioning configuration for a bucket.\\n\\n    Returns {updated: true} if versioning configuration was updated and returns\\n    {updated: False} if versioning configuration was not updated.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt myminion boto_s3_bucket.put_versioning my_bucket Enabled\\n\\n    '\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        VersioningConfiguration = {\n            'Status': Status,\n        }\n        if (MFADelete is not None):\n            VersioningConfiguration['MFADelete'] = MFADelete\n        kwargs = {\n            \n        }\n        if (MFA is not None):\n            kwargs['MFA'] = MFA\n        conn.put_bucket_versioning(Bucket=Bucket, VersioningConfiguration=VersioningConfiguration, **kwargs)\n        return {\n            'updated': True,\n            'name': Bucket,\n        }\n    except ClientError as e:\n        return {\n            'updated': False,\n            'error': __utils__['boto3.get_error'](e),\n        }\n", "label": 0}
{"function": "\n\ndef do_action(self):\n    '\\n        Snapshot indices in `index_list.indices`, with options passed.\\n        '\n    if (not self.skip_repo_fs_check):\n        test_repo_fs(self.client, self.repository)\n    if snapshot_running(self.client):\n        raise SnapshotInProgress('Snapshot already in progress.')\n    try:\n        self.client.snapshot.create(repository=self.repository, snapshot=self.name, body=self.body, wait_for_completion=self.wait_for_completion)\n        if self.wait_for_completion:\n            self.report_state()\n        else:\n            self.loggit.warn('\"wait_for_completion\" set to {0}. Remember to check for successful completion manually.'.format(self.wait_for_completion))\n    except Exception as e:\n        report_failure(e)\n", "label": 0}
{"function": "\n\ndef get_cookie_session_id(self):\n    'Extract the current session ID from the cookie.\\n\\n        Return None if not set or invalid.\\n        '\n    if ('Cookie' not in self.headers):\n        return None\n    cookie = cookies.SimpleCookie()\n    try:\n        cookie.load(self.headers['Cookie'])\n    except cookies.CookieError:\n        return None\n    morsel = cookie.get(SESSION_KEY)\n    if (morsel is None):\n        return None\n    session_id = cookie[SESSION_KEY].value\n    if self.server.sessions.is_valid(session_id):\n        return session_id\n    return None\n", "label": 0}
{"function": "\n\ndef make_node(self, *inputs):\n    assert (self.nout == 1)\n    assert (len(inputs) == 2)\n    _inputs = [gpu_contiguous(as_cuda_ndarray_variable(i)) for i in inputs]\n    if ((self.nin > 0) and (len(_inputs) != self.nin)):\n        raise TypeError('Wrong argument count', (self.nin, len(_inputs)))\n    for i in _inputs[1:]:\n        if (i.type.ndim != inputs[0].type.ndim):\n            raise TypeError('different ranks among inputs')\n    if any([any(i.type.broadcastable) for i in inputs]):\n        raise Exception(\"pycuda don't support broadcasted dimensions\")\n    otype = CudaNdarrayType(broadcastable=([False] * _inputs[0].type.ndim))\n    out_node = Apply(self, _inputs, [otype() for o in xrange(self.nout)])\n    return out_node\n", "label": 1}
{"function": "\n\ndef fuzzy_load(name, merge_inherited=True):\n    localedata._cache_lock.acquire()\n    try:\n        data = localedata._cache.get(name)\n        if (not data):\n            if ((name == 'root') or (not merge_inherited)):\n                data = {\n                    \n                }\n            else:\n                parts = name.split('_')\n                if (len(parts) == 1):\n                    parent = 'root'\n                else:\n                    parent = '_'.join(parts[:(- 1)])\n                data = fuzzy_load(parent).copy()\n            filename = os.path.join(localedata._dirname, ('%s.dat' % name))\n            try:\n                fileobj = open(filename, 'rb')\n                try:\n                    if ((name != 'root') and merge_inherited):\n                        localedata.merge(data, pickle.load(fileobj))\n                    else:\n                        data = pickle.load(fileobj)\n                    localedata._cache[name] = data\n                finally:\n                    fileobj.close()\n            except IOError:\n                pass\n        return data\n    finally:\n        localedata._cache_lock.release()\n", "label": 1}
{"function": "\n\ndef populate_network_service_policy_id_choices(self, request, context):\n    policies = []\n    try:\n        policies = client.networkservicepolicy_list(request, tenant_id=request.user.tenant_id)\n        for p in policies:\n            p.set_id_as_name_if_empty()\n        policies = [(p.id, ((p.name + ':') + p.id)) for p in policies]\n        policies.insert(0, ('None', 'No Network Service Policy'))\n    except Exception as e:\n        msg = (_('Unable to retrieve service policies. %s).') % str(e))\n        exceptions.handle(request, msg)\n    return policies\n", "label": 0}
{"function": "\n\ndef test_integer_int(self):\n    if _debug:\n        TestInteger._debug('test_integer_int')\n    obj = Integer(1)\n    assert (obj.value == 1)\n    assert (str(obj) == 'Integer(1)')\n    obj = Integer((- 1))\n    assert (obj.value == (- 1))\n    assert (str(obj) == 'Integer(-1)')\n", "label": 0}
{"function": "\n\ndef get_local_ip():\n    try:\n        interfaces = socket.gethostbyname_ex(socket.gethostname())[(- 1)]\n    except socket.gaierror:\n        return\n    for ip in interfaces:\n        if ip.startswith('192.'):\n            return ip\n", "label": 0}
{"function": "\n\ndef test_objects_max_size_bounds_chunk_size(self):\n    smart = iter(Author.objects.iter_smart_chunks(chunk_max=5, chunk_size=1000))\n    seen = []\n    for chunk in smart:\n        ids = [author.id for author in chunk]\n        assert (len(ids) <= 5)\n        seen.extend(ids)\n    all_ids = list(Author.objects.order_by('id').values_list('id', flat=True))\n    assert (seen == all_ids)\n", "label": 0}
{"function": "\n\ndef rename(src, dst):\n    '\\n    On Windows, os.rename() will fail with a WindowsError exception if a file\\n    exists at the destination path. This function checks for this error and if\\n    found, it deletes the destination path first.\\n    '\n    try:\n        os.rename(src, dst)\n    except OSError as exc:\n        if (exc.errno != errno.EEXIST):\n            raise\n        try:\n            os.remove(dst)\n        except OSError as exc:\n            if (exc.errno != errno.ENOENT):\n                raise MinionError('Error: Unable to remove {0}: {1}'.format(dst, exc.strerror))\n        os.rename(src, dst)\n", "label": 0}
{"function": "\n\ndef get_new_connection(self, conn_params):\n    connection = Database.connect(**conn_params)\n    options = self.settings_dict['OPTIONS']\n    try:\n        self.isolation_level = options['isolation_level']\n    except KeyError:\n        self.isolation_level = connection.isolation_level\n    else:\n        if (self.isolation_level != connection.isolation_level):\n            connection.set_session(isolation_level=self.isolation_level)\n    return connection\n", "label": 0}
{"function": "\n\ndef OnAuthTumblr(self):\n    self.User = self.le_mail.text().trimmed()\n    self.Password = self.le_password.text()\n    self.Blog = self.le_url.text().trimmed()\n    self.error = None\n    if (not ((self.User.isEmpty() | self.Password.isEmpty()) | self.Blog.isEmpty())):\n        self.api = Api(self.Blog, self.User, self.Password)\n        try:\n            self.auth = self.api.auth_check()\n            if QtGui.QSystemTrayIcon.isSystemTrayAvailable():\n                self.hide()\n                tray = TumblrTray(self)\n            else:\n                dashboard = Dashboard(self)\n                self.hide()\n                dashboard.show()\n            if (self.rememberme.checkState() == 2):\n                file = open((QtCore.QDir().homePath() + '/.opentumblr'), 'w')\n                file.write(self.le_mail.text())\n                file.write(self.le_url.text())\n        except TumblrAuthError:\n            self.error = errors['403']\n        except urllib2.HTTPError:\n            self.error = errors['404']\n        except urllib2.URLError:\n            self.error = errors['urlopen']\n        finally:\n            if (self.error != None):\n                QtGui.QMessageBox.warning(self, 'Error', ('Occurrio un error: \\n' + self.error), QtGui.QMessageBox.Ok)\n    else:\n        QtGui.QMessageBox.warning(self, 'Error', 'Todos los Campos son necesarios', QtGui.QMessageBox.Ok)\n", "label": 1}
{"function": "\n\ndef warn(self):\n    if self.urgent:\n        type = 'import'\n    else:\n        type = 'use'\n    message = ('%s %s: %s' % (type, self.name, self.info))\n    try:\n        import warnings\n        if self.urgent:\n            level = 4\n        else:\n            level = 3\n        warnings.warn(message, RuntimeWarning, level)\n    except ImportError:\n        print(message)\n", "label": 0}
{"function": "\n\ndef optimizeJumps(irdata):\n    instrs = irdata.flat_instructions\n    jump_instrs = [ins for ins in instrs if isinstance(ins, ir.LazyJumpBase)]\n    while 1:\n        done = True\n        (posd, _) = _calcMinimumPositions(instrs)\n        for ins in jump_instrs:\n            if ((ins.min < ins.max) and ins.widenIfNecessary(irdata.labels, posd)):\n                done = False\n        if done:\n            break\n    for ins in jump_instrs:\n        assert (ins.min <= ins.max)\n        ins.max = ins.min\n", "label": 1}
{"function": "\n\ndef handle(self):\n    message_format = '%s     %3d%%     [ %d / %d ]'\n    last_status_message_len = 0\n    status_message = ''\n    message_sent = False\n    self.server.binwalk.status.running = True\n    while True:\n        time.sleep(0.1)\n        try:\n            self.request.send(binwalk.core.compat.str2bytes(('\\x08' * last_status_message_len)))\n            self.request.send(binwalk.core.compat.str2bytes((' ' * last_status_message_len)))\n            self.request.send(binwalk.core.compat.str2bytes(('\\x08' * last_status_message_len)))\n            if self.server.binwalk.status.shutdown:\n                self.server.binwalk.status.finished = True\n                break\n            if (self.server.binwalk.status.total != 0):\n                percentage = ((float(self.server.binwalk.status.completed) / float(self.server.binwalk.status.total)) * 100)\n                status_message = (message_format % (self.server.binwalk.status.fp.path, percentage, self.server.binwalk.status.completed, self.server.binwalk.status.total))\n            elif (not message_sent):\n                status_message = 'No status information available at this time!'\n            else:\n                continue\n            last_status_message_len = len(status_message)\n            self.request.send(binwalk.core.compat.str2bytes(status_message))\n            message_sent = True\n        except IOError as e:\n            if (e.errno == errno.EPIPE):\n                break\n        except Exception as e:\n            binwalk.core.common.debug((('StatusRequestHandler exception: ' + str(e)) + '\\n'))\n        except KeyboardInterrupt as e:\n            raise e\n    self.server.binwalk.status.running = False\n    return\n", "label": 1}
{"function": "\n\n@memoized\ndef get_pillow_process_info(self, pillow_name):\n    process_info = [info for info in self.get_all_process_info() if (pillow_name in info.name)]\n    try:\n        return process_info[0]\n    except IndexError:\n        raise SupervisorException('Process not found for pillow: {}'.format(pillow_name))\n", "label": 0}
{"function": "\n\ndef reproject(source, destination, src_transform=None, src_crs=None, src_nodata=None, dst_transform=None, dst_crs=None, dst_nodata=None, resampling=Resampling.nearest, **kwargs):\n    \"\\n    Reproject a source raster to a destination raster.\\n\\n    If the source and destination are ndarrays, coordinate reference\\n    system definitions and affine transformation parameters are required\\n    for reprojection.\\n\\n    If the source and destination are rasterio Bands, shorthand for\\n    bands of datasets on disk, the coordinate reference systems and\\n    transforms will be read from the appropriate datasets.\\n\\n    Parameters\\n    ------------\\n    source: ndarray or rasterio Band\\n        Source raster.\\n    destination: ndarray or rasterio Band\\n        Target raster.\\n    src_transform: affine transform object, optional\\n        Source affine transformation.  Required if source and destination\\n        are ndarrays.  Will be derived from source if it is a rasterio Band.\\n    src_crs: dict, optional\\n        Source coordinate reference system, in rasterio dict format.\\n        Required if source and destination are ndarrays.\\n        Will be derived from source if it is a rasterio Band.\\n        Example: {'init': 'EPSG:4326'}\\n    src_nodata: int or float, optional\\n        The source nodata value.  Pixels with this value will not be used\\n        for interpolation.  If not set, it will be default to the\\n        nodata value of the source image if a masked ndarray or rasterio band,\\n        if available.  Must be provided if dst_nodata is not None.\\n    dst_transform: affine transform object, optional\\n        Target affine transformation.  Required if source and destination\\n        are ndarrays.  Will be derived from target if it is a rasterio Band.\\n    dst_crs: dict, optional\\n        Target coordinate reference system.  Required if source and destination\\n        are ndarrays.  Will be derived from target if it is a rasterio Band.\\n    dst_nodata: int or float, optional\\n        The nodata value used to initialize the destination; it will remain\\n        in all areas not covered by the reprojected source.  Defaults to the\\n        nodata value of the destination image (if set), the value of\\n        src_nodata, or 0 (GDAL default).\\n    resampling: int\\n        Resampling method to use.  One of the following:\\n            Resampling.nearest,\\n            Resampling.bilinear,\\n            Resampling.cubic,\\n            Resampling.cubic_spline,\\n            Resampling.lanczos,\\n            Resampling.average,\\n            Resampling.mode\\n    kwargs:  dict, optional\\n        Additional arguments passed to transformation function.\\n\\n    Returns\\n    ---------\\n    out: None\\n        Output is written to destination.\\n    \"\n    try:\n        Resampling(resampling)\n        if (resampling == 7):\n            raise ValueError\n    except ValueError:\n        raise ValueError('resampling must be one of: {0}'.format(', '.join(['Resampling.{0}'.format(k) for k in Resampling.__members__.keys() if (k != 'gauss')])))\n    if src_transform:\n        src_transform = guard_transform(src_transform).to_gdal()\n    if dst_transform:\n        dst_transform = guard_transform(dst_transform).to_gdal()\n    rasterio.env.setenv()\n    _reproject(source, destination, src_transform, src_crs, src_nodata, dst_transform, dst_crs, dst_nodata, resampling, **kwargs)\n", "label": 0}
{"function": "\n\ndef pickletest(self, it, stop=4, take=1, compare=None):\n    'Test that an iterator is the same after pickling, also when part-consumed'\n\n    def expand(it, i=0):\n        if (i > 10):\n            raise RuntimeError('infinite recursion encountered')\n        if isinstance(it, str):\n            return it\n        try:\n            l = list(islice(it, stop))\n        except TypeError:\n            return it\n        return [expand(e, (i + 1)) for e in l]\n    dump = pickle.dumps(it)\n    i2 = pickle.loads(dump)\n    self.assertEqual(type(it), type(i2))\n    (a, b) = (expand(it), expand(i2))\n    self.assertEqual(a, b)\n    if compare:\n        c = expand(compare)\n        self.assertEqual(a, c)\n    i3 = pickle.loads(dump)\n    took = 0\n    try:\n        for i in range(take):\n            next(i3)\n            took += 1\n    except StopIteration:\n        pass\n    dump = pickle.dumps(i3)\n    i4 = pickle.loads(dump)\n    (a, b) = (expand(i3), expand(i4))\n    self.assertEqual(a, b)\n    if compare:\n        c = expand(compare[took:])\n        self.assertEqual(a, c)\n", "label": 0}
{"function": "\n\n@classmethod\ndef load_dotenv(cls):\n    '\\n        Pulled from Honcho code with minor updates, reads local default\\n        environment variables from a .env file located in the project root\\n        or provided directory.\\n\\n        http://www.wellfireinteractive.com/blog/easier-12-factor-django/\\n        https://gist.github.com/bennylope/2999704\\n        '\n    dotenv = getattr(cls, 'DOTENV', None)\n    if (not dotenv):\n        return\n    try:\n        with open(dotenv, 'r') as f:\n            content = f.read()\n    except IOError as e:\n        raise ImproperlyConfigured(\"Couldn't read .env file with the path {}. Error: {}\".format(dotenv, e))\n    else:\n        for line in content.splitlines():\n            m1 = re.match('\\\\A([A-Za-z_0-9]+)=(.*)\\\\Z', line)\n            if (not m1):\n                continue\n            (key, val) = (m1.group(1), m1.group(2))\n            m2 = re.match(\"\\\\A'(.*)'\\\\Z\", val)\n            if m2:\n                val = m2.group(1)\n            m3 = re.match('\\\\A\"(.*)\"\\\\Z', val)\n            if m3:\n                val = re.sub('\\\\\\\\(.)', '\\\\1', m3.group(1))\n            os.environ.setdefault(key, val)\n        cls.DOTENV_LOADED = dotenv\n", "label": 1}
{"function": "\n\ndef clean(self):\n    username = self.cleaned_data.get('username')\n    password = self.cleaned_data.get('password')\n    message = ERROR_MESSAGE\n    if (username and password):\n        self.user_cache = authenticate(username=username, password=password)\n        if (self.user_cache is None):\n            if ('@' in username):\n                try:\n                    user = User.objects.get(email=username)\n                except (User.DoesNotExist, User.MultipleObjectsReturned):\n                    pass\n                else:\n                    if user.check_password(password):\n                        message = (_(\"Your e-mail address is not your username. Try '%s' instead.\") % user.username)\n            raise forms.ValidationError(message)\n        elif ((not self.user_cache.is_active) or (not self.user_cache.is_staff)):\n            raise forms.ValidationError(message)\n    self.check_for_test_cookie()\n    return self.cleaned_data\n", "label": 1}
{"function": "\n\n@never_cache\n@login_required\ndef delete(request, app_label, model_name, instance_id, delete_form=DeleteRequestForm):\n    instance_return = _get_instance(request, 'delete', app_label, model_name, instance_id)\n    if isinstance(instance_return, HttpResponseForbidden):\n        return instance_return\n    (model, instance_form, instance) = instance_return\n    cancel = _handle_cancel(request)\n    if cancel:\n        return cancel\n    if (request.method == 'POST'):\n        form = delete_form(request.POST)\n        if form.is_valid():\n            instance.delete()\n            msg = (_('Your %(model_name)s was deleted.') % {\n                'model_name': model._meta.verbose_name,\n            })\n            try:\n                request.user.message_set.create(message=msg)\n            except AttributeError:\n                messages.success(request, msg)\n            if request.is_ajax():\n                return success_delete(request)\n            return _handle_response(request, instance)\n    else:\n        form = delete_form()\n    template_context = {\n        'action': 'delete',\n        'action_url': request.get_full_path(),\n        'model_title': model._meta.verbose_name,\n        'form': form,\n    }\n    return render_to_response(_get_template(request, None, None), template_context, RequestContext(request))\n", "label": 0}
{"function": "\n\ndef ensure_ssh_key_added(key_files):\n    need_adding = set((os.path.abspath(os.path.expanduser(p)) for p in key_files))\n    with settings(hide('warnings', 'running', 'stdout', 'stderr'), warn_only=True):\n        res = local('ssh-add -l', capture=True)\n        if res.succeeded:\n            for line in res.splitlines():\n                m = SSH_KEY_LIST_RE.match(line)\n                if (not m):\n                    continue\n                path = os.path.abspath(os.path.expanduser(m.group('key_file')))\n                need_adding.discard(path)\n    with settings(hide('warnings', 'running', 'stdout', 'stderr')):\n        if need_adding:\n            key_string = ' '.join(need_adding)\n            start_ssh_agent = ('eval `ssh-agent` && echo $SSH_AUTH_SOCK && ssh-add %s' % key_string)\n            info_agent = local(start_ssh_agent, capture=True).splitlines()\n            os.environ['SSH_AGENT_PID'] = info_agent[0].split()[(- 1)]\n            os.environ['SSH_AUTH_SOCK'] = info_agent[1]\n            return False\n        else:\n            return True\n", "label": 0}
{"function": "\n\ndef dialect_of(data, **kwargs):\n    ' CSV dialect of a CSV file stored in SSH, HDFS, or a Directory. '\n    keys = set(['delimiter', 'doublequote', 'escapechar', 'lineterminator', 'quotechar', 'quoting', 'skipinitialspace', 'strict', 'has_header'])\n    if isinstance(data, (HDFS(CSV), SSH(CSV))):\n        with sample(data) as fn:\n            d = dialect_of(CSV(fn, **data.dialect))\n    elif isinstance(data, (HDFS(Directory(CSV)), SSH(Directory(CSV)))):\n        with sample(data) as fn:\n            d = dialect_of(CSV(fn, **data.kwargs))\n    elif isinstance(data, Directory(CSV)):\n        d = dialect_of(next(data))\n    else:\n        assert isinstance(data, CSV)\n        with open(data.path, 'r') as f:\n            text = f.read()\n        result = dict()\n        d = sniffer.sniff(text)\n        d = dict(((k, getattr(d, k)) for k in keys if hasattr(d, k)))\n        if (data.has_header is None):\n            d['has_header'] = sniffer.has_header(text)\n        else:\n            d['has_header'] = data.has_header\n        d.update(data.dialect)\n    d.update(kwargs)\n    d = dict(((k, v) for (k, v) in d.items() if (k in keys)))\n    return d\n", "label": 1}
{"function": "\n\ndef _on_calculate_verts(self):\n    self.u_interval = self.intervals[0]\n    self.u_set = list(self.u_interval.frange())\n    self.v_interval = self.intervals[1]\n    self.v_set = list(self.v_interval.frange())\n    self.bounds = [[S.Infinity, (- S.Infinity), 0], [S.Infinity, (- S.Infinity), 0], [S.Infinity, (- S.Infinity), 0]]\n    evaluate = self._get_evaluator()\n    self._calculating_verts_pos = 0.0\n    self._calculating_verts_len = float((self.u_interval.v_len * self.v_interval.v_len))\n    verts = list()\n    b = self.bounds\n    for u in self.u_set:\n        column = list()\n        for v in self.v_set:\n            try:\n                _e = evaluate(u, v)\n            except ZeroDivisionError:\n                _e = None\n            if (_e is not None):\n                for axis in range(3):\n                    b[axis][0] = min([b[axis][0], _e[axis]])\n                    b[axis][1] = max([b[axis][1], _e[axis]])\n            column.append(_e)\n            self._calculating_verts_pos += 1.0\n        verts.append(column)\n    for axis in range(3):\n        b[axis][2] = (b[axis][1] - b[axis][0])\n        if (b[axis][2] == 0.0):\n            b[axis][2] = 1.0\n    self.verts = verts\n    self.push_wireframe(self.draw_verts(False, False))\n    self.push_solid(self.draw_verts(False, True))\n", "label": 1}
{"function": "\n\ndef _read(obj):\n    'Try to read from a url, file or string.\\n\\n    Parameters\\n    ----------\\n    obj : str, unicode, or file-like\\n\\n    Returns\\n    -------\\n    raw_text : str\\n    '\n    if _is_url(obj):\n        with urlopen(obj) as url:\n            text = url.read()\n    elif hasattr(obj, 'read'):\n        text = obj.read()\n    elif isinstance(obj, char_types):\n        text = obj\n        try:\n            if os.path.isfile(text):\n                with open(text, 'rb') as f:\n                    return f.read()\n        except (TypeError, ValueError):\n            pass\n    else:\n        raise TypeError(('Cannot read object of type %r' % type(obj).__name__))\n    return text\n", "label": 0}
{"function": "\n\ndef findsubclass(self, node, during):\n    handlerinfo = ('%s.%s' % (self.getclassname(self), during))\n    log.debug('%s: %s', handlerinfo, self.getclassname(node))\n    log.debug('%s: [%s]', handlerinfo, nodeid(node))\n    try:\n        log.debug('%s: %s', handlerinfo, node)\n    except (UnicodeDecodeError, UnicodeEncodeError):\n        log.debug('%s: %r', handlerninfo, node)\n    log.debug('')\n    dispatchdict = self.dispatchdict\n    for baseclass in inspect.getmro(node.__class__):\n        result = dispatchdict.get(baseclass)\n        if (result is not None):\n            break\n    else:\n        self.log_unknown(node, during)\n        result = self\n    return result\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _ExtractMetadata(image, parse_metadata):\n    'Extract EXIF metadata from the image.\\n\\n    Note that this is a much simplified version of metadata extraction. After\\n    deployment applications have access to a more powerful parser that can\\n    parse hundreds of fields from images.\\n\\n    Args:\\n      image: PIL Image object.\\n      parse_metadata: bool, True if metadata parsing has been requested. If\\n        False the result will contain image dimensions.\\n    Returns:\\n      str, JSON encoded values with various metadata fields.\\n    '\n\n    def ExifTimeToUnixtime(exif_time):\n        'Convert time in EXIF to unix time.\\n\\n      Args:\\n        exif_time: str, the time from the EXIF block formated by EXIF standard.\\n          E.g., \"2011:02:20 10:23:12\", seconds are optional.\\n\\n      Returns:\\n        Integer, the time in unix fromat: seconds since the epoch.\\n      '\n        regexp = re.compile('^([0-9]{4}):([0-9]{1,2}):([0-9]{1,2}) ([0-9]{1,2}):([0-9]{1,2})(?::([0-9]{1,2}))?')\n        match = regexp.match(exif_time)\n        if (match is None):\n            return None\n        try:\n            date = datetime.datetime(*map(int, filter(None, match.groups())))\n        except ValueError:\n            logging.info('Invalid date in EXIF: %s', exif_time)\n            return None\n        return int(time.mktime(date.timetuple()))\n    metadata_dict = ((parse_metadata and ImagesServiceStub._GetExifFromImage(image)) or {\n        \n    })\n    (metadata_dict[256], metadata_dict[257]) = image.size\n    if (_EXIF_DATETIMEORIGINAL_TAG in metadata_dict):\n        date_ms = ExifTimeToUnixtime(metadata_dict[_EXIF_DATETIMEORIGINAL_TAG])\n        if date_ms:\n            metadata_dict[_EXIF_DATETIMEORIGINAL_TAG] = date_ms\n        else:\n            del metadata_dict[_EXIF_DATETIMEORIGINAL_TAG]\n    metadata = dict([(_EXIF_TAGS[k], v) for (k, v) in metadata_dict.iteritems() if (k in _EXIF_TAGS)])\n    return simplejson.dumps(metadata)\n", "label": 0}
{"function": "\n\ndef test_deploy_service_known_bounce(self):\n    fake_bounce = 'areallygoodbouncestrategy'\n    fake_drain_method_name = 'noop'\n    fake_name = 'how_many_strings'\n    fake_instance = 'will_i_need_to_think_of'\n    fake_id = marathon_tools.format_job_id(fake_name, fake_instance, 'git11111111', 'config11111111')\n    fake_config = {\n        'id': fake_id,\n        'instances': 2,\n    }\n    old_app_id = marathon_tools.format_job_id(fake_name, fake_instance, 'git22222222', 'config22222222')\n    old_task_to_drain = mock.Mock(id='old_task_to_drain', app_id=old_app_id)\n    old_task_is_draining = mock.Mock(id='old_task_is_draining', app_id=old_app_id)\n    old_task_dont_drain = mock.Mock(id='old_task_dont_drain', app_id=old_app_id)\n    old_app = mock.Mock(id=('/%s' % old_app_id), tasks=[old_task_to_drain, old_task_is_draining, old_task_dont_drain])\n    fake_client = mock.MagicMock(list_apps=mock.Mock(return_value=[old_app]), kill_given_tasks=mock.Mock(spec=(lambda task_ids, scale=False: None)))\n    fake_bounce_func = mock.create_autospec(bounce_lib.brutal_bounce, return_value={\n        'create_app': True,\n        'tasks_to_drain': [old_task_to_drain],\n    })\n    fake_drain_method = mock.Mock(is_draining=(lambda t: (t is old_task_is_draining)), is_safe_to_kill=(lambda t: True))\n    with contextlib.nested(mock.patch('paasta_tools.bounce_lib.get_bounce_method_func', return_value=fake_bounce_func, autospec=True), mock.patch('paasta_tools.bounce_lib.bounce_lock_zookeeper', autospec=True), mock.patch('paasta_tools.bounce_lib.get_happy_tasks', autospec=True, side_effect=(lambda x, _, __, ___, **kwargs: x.tasks)), mock.patch('paasta_tools.bounce_lib.kill_old_ids', autospec=True), mock.patch('paasta_tools.bounce_lib.create_marathon_app', autospec=True), mock.patch('paasta_tools.setup_marathon_job._log', autospec=True), mock.patch('paasta_tools.setup_marathon_job.load_system_paasta_config', autospec=True), mock.patch('paasta_tools.drain_lib.get_drain_method', return_value=fake_drain_method)) as (_, _, _, kill_old_ids_patch, create_marathon_app_patch, mock_log, mock_load_system_paasta_config, _):\n        mock_load_system_paasta_config.return_value.get_cluster = mock.Mock(return_value='fake_cluster')\n        result = setup_marathon_job.deploy_service(service=fake_name, instance=fake_instance, marathon_jobid=fake_id, config=fake_config, client=fake_client, bounce_method=fake_bounce, drain_method_name=fake_drain_method_name, drain_method_params={\n            \n        }, nerve_ns=fake_instance, bounce_health_params={\n            \n        }, soa_dir='fake_soa_dir')\n        assert (result[0] == 0), ('Expected successful result; got (%d, %s)' % result)\n        fake_client.list_apps.assert_called_once_with(embed_failures=True)\n        assert (fake_client.create_app.call_count == 0)\n        fake_bounce_func.assert_called_once_with(new_config=fake_config, new_app_running=False, happy_new_tasks=[], old_app_live_happy_tasks={\n            old_app.id: set([old_task_to_drain, old_task_dont_drain]),\n        }, old_app_live_unhappy_tasks={\n            old_app.id: set(),\n        })\n        assert (fake_drain_method.drain.call_count == 2)\n        fake_drain_method.drain.assert_any_call(old_task_is_draining)\n        fake_drain_method.drain.assert_any_call(old_task_to_drain)\n        assert (fake_client.kill_given_tasks.call_count == 1)\n        assert (set([old_task_to_drain.id, old_task_is_draining.id]) == set(fake_client.kill_given_tasks.call_args[1]['task_ids']))\n        assert (fake_client.kill_given_tasks.call_args[1]['scale'] is True)\n        create_marathon_app_patch.assert_called_once_with(fake_config['id'], fake_config, fake_client)\n        assert (kill_old_ids_patch.call_count == 0)\n        assert (mock_log.call_count == 5)\n", "label": 1}
{"function": "\n\ndef wait(self, timeout=None):\n    'Block until the instance is ready.\\n\\n        If this instance already holds a value / an exception, return immediatelly.\\n        Otherwise, block until another thread calls :meth:`set` or :meth:`set_exception` or\\n        until the optional timeout occurs.\\n\\n        When the *timeout* argument is present and not ``None``, it should be a\\n        floating point number specifying a timeout for the operation in seconds\\n        (or fractions thereof).\\n\\n        Return :attr:`value`.\\n        '\n    if (self._exception is not _NONE):\n        return self.value\n    else:\n        switch = greenlet.getcurrent().switch\n        self.rawlink(switch)\n        try:\n            timer = Timeout(timeout)\n            try:\n                result = self.hub.switch()\n                assert (result is self), ('Invalid switch into AsyncResult.wait(): %r' % (result,))\n            finally:\n                timer.cancel()\n        except Timeout as exc:\n            self.unlink(switch)\n            if (exc is not timer):\n                raise\n        except:\n            self.unlink(switch)\n            raise\n    return self.value\n", "label": 0}
{"function": "\n\ndef safe_create_instance(username, xml_file, media_files, uuid, request):\n    'Create an instance and catch exceptions.\\n\\n    :returns: A list [error, instance] where error is None if there was no\\n        error.\\n    '\n    error = instance = None\n    try:\n        instance = create_instance(username, xml_file, media_files, uuid=uuid, request=request)\n    except InstanceInvalidUserError:\n        error = OpenRosaResponseBadRequest(_('Username or ID required.'))\n    except InstanceEmptyError:\n        error = OpenRosaResponseBadRequest(_('Received empty submission. No instance was created'))\n    except FormInactiveError:\n        error = OpenRosaResponseNotAllowed(_('Form is not active'))\n    except XForm.DoesNotExist:\n        error = OpenRosaResponseNotFound(_('Form does not exist on this account'))\n    except ExpatError as e:\n        error = OpenRosaResponseBadRequest(_('Improperly formatted XML.'))\n    except DuplicateInstance:\n        response = OpenRosaResponse(_('Duplicate submission'))\n        response.status_code = 202\n        response['Location'] = request.build_absolute_uri(request.path)\n        error = response\n    except PermissionDenied as e:\n        error = OpenRosaResponseForbidden(e)\n    except InstanceMultipleNodeError as e:\n        error = OpenRosaResponseBadRequest(e)\n    except DjangoUnicodeDecodeError:\n        error = OpenRosaResponseBadRequest(_('File likely corrupted during transmission, please try later.'))\n    return [error, instance]\n", "label": 1}
{"function": "\n\ndef _async_recv_msg(self):\n    \"Internal use only; use 'recv_msg' with 'yield' instead.\\n\\n        Message is tagged with length of the payload (data). This\\n        method receives length of payload, then the payload and\\n        returns the payload.\\n        \"\n    n = struct.calcsize('>L')\n    try:\n        data = (yield self.recvall(n))\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            raise StopIteration('')\n        else:\n            raise\n    if (len(data) != n):\n        raise StopIteration('')\n    n = struct.unpack('>L', data)[0]\n    assert (n >= 0)\n    try:\n        data = (yield self.recvall(n))\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            raise StopIteration('')\n        else:\n            raise\n    if (len(data) != n):\n        raise StopIteration('')\n    raise StopIteration(data)\n", "label": 1}
{"function": "\n\n@patch('gonzo.tasks.release.list_releases')\ndef test_get_previous_release(list_releases):\n    releases = ['aaa', 'bbb', 'ccc', 'ddd']\n    list_releases.return_value = releases\n    assert (get_previous_release('ccc') == 'bbb')\n    assert (get_previous_release(None) is None)\n    assert (get_previous_release('aaa') is None)\n    assert (get_previous_release('xxx') is None)\n", "label": 0}
{"function": "\n\ndef test_delete_user_in_backends_by_username(self):\n    ' Delete a user previously registered user by username\\n        '\n    try:\n        self.msu.create_user('bootsy', 'collins', 'funk@mothership.com')\n    except NotImplementedError:\n        skip('user management not supported in this version of managesf')\n    self.logout()\n    self.login('bootsy', 'collins', config.GATEWAY_URL)\n    self.assertEqual('funk@mothership.com', self.gu.get_account('bootsy').get('email'))\n    if has_issue_tracker():\n        users = self.rm.active_users()\n        users = [u for u in users if (u[0] == 'bootsy')]\n        self.assertEqual(1, len(users))\n        user = users[0]\n        self.assertEqual('funk@mothership.com', user[1])\n    del_url = (config.GATEWAY_URL + 'manage/services_users/?username=bootsy')\n    auth_cookie = get_cookie(config.GATEWAY_HOST, 'user5', config.ADMIN_PASSWORD)\n    d = requests.delete(del_url, cookies={\n        'auth_pubtkt': auth_cookie,\n    })\n    self.assertEqual(401, int(d.status_code))\n    auth_cookie = config.USERS[config.ADMIN_USER]['auth_cookie']\n    d = requests.delete(del_url, cookies={\n        'auth_pubtkt': auth_cookie,\n    })\n    self.assertTrue((int(d.status_code) < 400), d.status_code)\n    self.assertEqual(False, self.gu.get_account('bootsy'))\n    if has_issue_tracker():\n        users = self.rm.active_users()\n        self.assertEqual(0, len([u for u in users if (u[0] == 'bootsy')]))\n", "label": 1}
{"function": "\n\ndef test_create():\n    with tmpfile('.hdf5') as fn:\n        ds = datashape.dshape('{x: int32, y: {z: 3 * int32}}')\n        f = create(h5py.File, dshape='{x: int32, y: {z: 3 * int32}}', path=fn)\n        try:\n            assert isinstance(f, h5py.File)\n            assert (f.filename == fn)\n            assert (discover(f) == ds)\n        finally:\n            f.close()\n", "label": 0}
{"function": "\n\ndef test_aes_gcm_byname():\n    aes = Cipher('aes-128-gcm')\n    assert aes.gcm\n    enc = aes.op(key=(b'A' * 16), iv=(b'A' * 16))\n    enc.update_associated(b'Hello')\n    ciphertext = enc.update(b'World!')\n    c2 = enc.finalize()\n    assert (c2 == b'')\n    tag = enc.get_tag(16)\n    assert (len(tag) == 16)\n    dec = aes.dec(key=(b'A' * 16), iv=(b'A' * 16))\n    dec.update_associated(b'Hello')\n    plaintext = dec.update(ciphertext)\n    dec.set_tag(tag)\n    dec.finalize()\n    assert (plaintext == b'World!')\n", "label": 0}
{"function": "\n\ndef set_options(self, **options):\n    try:\n        super_set_options = super(StorageOverrideMixin, self).set_options\n    except AttributeError:\n        pass\n    else:\n        super_set_options(**options)\n    storage_override = options.get('storage_override')\n    if storage_override:\n        cls = get_storage_class(storage_override)\n        self.storage = cls()\n    else:\n        self.storage = staticfiles_storage\n    try:\n        self.storage.path('')\n    except NotImplementedError:\n        self.local = False\n    else:\n        self.local = True\n", "label": 0}
{"function": "\n\ndef _create_x509_extension(self, handlers, extension):\n    if isinstance(extension.value, x509.UnrecognizedExtension):\n        obj = _txt2obj_gc(self, extension.oid.dotted_string)\n        value = _encode_asn1_str_gc(self, extension.value.value, len(extension.value.value))\n        return self._lib.X509_EXTENSION_create_by_OBJ(self._ffi.NULL, obj, (1 if extension.critical else 0), value)\n    else:\n        try:\n            encode = handlers[extension.oid]\n        except KeyError:\n            raise NotImplementedError('Extension not supported: {0}'.format(extension.oid))\n        ext_struct = encode(self, extension.value)\n        nid = self._lib.OBJ_txt2nid(extension.oid.dotted_string.encode('ascii'))\n        backend.openssl_assert((nid != self._lib.NID_undef))\n        x509_extension = self._lib.X509V3_EXT_i2d(nid, (1 if extension.critical else 0), ext_struct)\n        if ((x509_extension == self._ffi.NULL) and (extension.oid == x509.OID_CERTIFICATE_ISSUER)):\n            self._consume_errors()\n            pp = backend._ffi.new('unsigned char **')\n            r = self._lib.i2d_GENERAL_NAMES(ext_struct, pp)\n            backend.openssl_assert((r > 0))\n            pp = backend._ffi.gc(pp, (lambda pointer: backend._lib.OPENSSL_free(pointer[0])))\n            obj = _txt2obj_gc(self, extension.oid.dotted_string)\n            return self._lib.X509_EXTENSION_create_by_OBJ(self._ffi.NULL, obj, (1 if extension.critical else 0), _encode_asn1_str_gc(self, pp[0], r))\n        return x509_extension\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    for status in ('active', 'failed', 'invalidated'):\n        resp = self.client.post(self.path, data={\n            'status': status,\n        })\n        assert (resp.status_code == 200)\n        data = self.unserialize(resp)\n        assert (data['id'] == self.image.id.hex)\n        assert (data['status']['id'] == status)\n        db.session.expire(self.image)\n        image = SnapshotImage.query.get(self.image.id)\n        assert (image.status == SnapshotStatus[status])\n", "label": 0}
{"function": "\n\ndef _ss(data, c=None):\n    'Return sum of square deviations of sequence data.\\n\\n    If ``c`` is None, the mean is calculated in one pass, and the deviations\\n    from the mean are calculated in a second pass. Otherwise, deviations are\\n    calculated from ``c`` as given. Use the second case with care, as it can\\n    lead to garbage results.\\n    '\n    if (c is None):\n        c = mean(data)\n    ss = sum((((x - c) ** 2) for x in data))\n    ss -= ((sum(((x - c) for x in data)) ** 2) / len(data))\n    assert (not (ss < 0)), ('negative sum of square deviations: %f' % ss)\n    return ss\n", "label": 0}
{"function": "\n\ndef __init__(self, node):\n    'Create the basic structures to keep the attribute information.\\n\\n        Reads all the HDF5 attributes (if any) on disk for the node \"node\".\\n\\n        Parameters\\n        ----------\\n        node\\n            The parent node\\n\\n        '\n    if (not node._v_isopen):\n        raise ClosedNodeError('the node for attribute set is closed')\n    dict_ = self.__dict__\n    self._g_new(node)\n    dict_['_v__nodefile'] = node._v_file\n    dict_['_v__nodepath'] = node._v_pathname\n    dict_['_v_attrnames'] = self._g_list_attr(node)\n    dict_['_v_unimplemented'] = []\n    try:\n        format_version = node._v_file.format_version\n    except AttributeError:\n        parsed_version = None\n    else:\n        if (format_version == 'unknown'):\n            parsed_version = None\n        else:\n            parsed_version = tuple(map(int, format_version.split('.')))\n    dict_['_v__format_version'] = parsed_version\n    dict_['_v_attrnamessys'] = []\n    dict_['_v_attrnamesuser'] = []\n    for attr in self._v_attrnames:\n        self.__getattr__(attr)\n        if issysattrname(attr):\n            self._v_attrnamessys.append(attr)\n        else:\n            self._v_attrnamesuser.append(attr)\n    self._v_attrnames.sort()\n    self._v_attrnamessys.sort()\n    self._v_attrnamesuser.sort()\n", "label": 0}
{"function": "\n\ndef _do_ssl_handshake(self):\n    try:\n        sock_debug('Doing SSL handshake')\n        self._sock.do_handshake()\n    except ssl.SSLError as e:\n        sock_debug('Floobits: ssl.SSLError. This is expected sometimes.')\n        if (e.args[0] in [ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE]):\n            return False\n        self.stop()\n        editor.error_message(('Floobits SSL handshake error: %s' % str(e)))\n        sock_debug(('SSLError args: %s' % ''.join([str(a) for a in e.args])))\n    except Exception as e:\n        msg.error('Error in SSL handshake: ', str_e(e))\n    else:\n        sock_debug('Successful handshake')\n        self._needs_handshake = False\n        editor.status_message(('%s:%s: SSL handshake completed' % (self.host, self.port)))\n        return True\n    self.reconnect()\n    return False\n", "label": 0}
{"function": "\n\ndef run(self):\n    'Main thread for processing messages.'\n    self.OnStartup()\n    while True:\n        message = self._in_queue.get()\n        if (message is None):\n            break\n        try:\n            self.HandleMessage(message)\n        except Exception as e:\n            logging.warn('%s', e)\n            self.SendReply(rdf_flows.GrrStatus(status=rdf_flows.GrrStatus.ReturnedStatus.GENERIC_ERROR, error_message=utils.SmartUnicode(e)), request_id=message.request_id, response_id=1, session_id=message.session_id, task_id=message.task_id, message_type=rdf_flows.GrrMessage.Type.STATUS)\n            if flags.FLAGS.debug:\n                pdb.post_mortem()\n", "label": 0}
{"function": "\n\ndef post(self, user_id=None, name=None):\n    user = User.get(self.session, user_id, name)\n    if (not user):\n        return self.notfound()\n    if ((user.name != self.current_user.name) and (not self.current_user.user_admin)):\n        return self.forbidden()\n    form = PublicKeyForm(self.request.arguments)\n    if (not form.validate()):\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    try:\n        pubkey = public_key.add_public_key(self.session, user, form.data['public_key'])\n    except public_key.PublicKeyParseError:\n        form.public_key.errors.append('Key failed to parse and is invalid.')\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    except public_key.DuplicateKey:\n        form.public_key.errors.append('Key already in use. Public keys must be unique.')\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    AuditLog.log(self.session, self.current_user.id, 'add_public_key', 'Added public key: {}'.format(pubkey.fingerprint), on_user_id=user.id)\n    email_context = {\n        'actioner': self.current_user.name,\n        'changed_user': user.name,\n        'action': 'added',\n    }\n    send_email(self.session, [user.name], 'Public SSH key added', 'ssh_keys_changed', settings, email_context)\n    return self.redirect('/users/{}?refresh=yes'.format(user.name))\n", "label": 0}
{"function": "\n\ndef the_local_prediction_is(step, prediction):\n    if (isinstance(world.local_prediction, list) or isinstance(world.local_prediction, tuple)):\n        local_prediction = world.local_prediction[0]\n    elif isinstance(world.local_prediction, dict):\n        local_prediction = world.local_prediction['prediction']\n    else:\n        local_prediction = world.local_prediction\n    try:\n        local_model = world.local_model\n        if (not isinstance(world.local_model, LogisticRegression)):\n            if isinstance(local_model, MultiModel):\n                local_model = local_model.models[0]\n            if local_model.tree.regression:\n                local_prediction = round(float(local_prediction), 4)\n                prediction = round(float(prediction), 4)\n    except AttributeError:\n        local_model = world.local_ensemble.multi_model.models[0]\n        if local_model.tree.regression:\n            local_prediction = round(float(local_prediction), 4)\n            prediction = round(float(prediction), 4)\n    if (local_prediction == prediction):\n        assert True\n    else:\n        assert False, ('found: %s, expected %s' % (local_prediction, prediction))\n", "label": 1}
{"function": "\n\ndef test_tweet_ordering():\n    now = datetime.now(timezone.utc)\n    tweet_1 = Tweet('A', now)\n    tweet_2 = Tweet('B', (now + timedelta(hours=1)))\n    tweet_3 = Tweet('C', (now + timedelta(hours=2)))\n    tweet_4 = Tweet('D', (now + timedelta(hours=2)))\n    tweet_5 = Tweet('D', (now + timedelta(hours=2)))\n    source = Source('foo', 'bar')\n    with pytest.raises(TypeError):\n        (tweet_1 < source)\n    with pytest.raises(TypeError):\n        (tweet_1 <= source)\n    with pytest.raises(TypeError):\n        (tweet_1 > source)\n    with pytest.raises(TypeError):\n        (tweet_1 >= source)\n    assert (tweet_1 != source)\n    assert (tweet_1 < tweet_2)\n    assert (tweet_1 <= tweet_2)\n    assert (tweet_2 > tweet_1)\n    assert (tweet_2 >= tweet_1)\n    assert (tweet_3 != tweet_4)\n    assert (tweet_5 == tweet_4)\n    assert (tweet_5 >= tweet_4)\n    assert (tweet_5 <= tweet_4)\n    assert (not (tweet_3 <= tweet_4))\n    assert (not (tweet_3 >= tweet_4))\n", "label": 1}
{"function": "\n\ndef test_host_caching_for_docopts(self):\n    host_caching_docopts = self.__host_caching_docopts('--no-cache')\n    assert (Defaults.host_caching_for_docopts(host_caching_docopts) == 'None')\n    host_caching_docopts = self.__host_caching_docopts('--read-only-cache')\n    assert (Defaults.host_caching_for_docopts(host_caching_docopts) == 'ReadOnly')\n    host_caching_docopts = self.__host_caching_docopts('--read-write-cache')\n    assert (Defaults.host_caching_for_docopts(host_caching_docopts) == 'ReadWrite')\n", "label": 0}
{"function": "\n\ndef cli_put_container(context, path):\n    '\\n    Performs a PUT on the container.\\n\\n    See :py:mod:`swiftly.cli.put` for context usage information.\\n\\n    See :py:class:`CLIPut` for more information.\\n    '\n    path = path.rstrip('/')\n    if ('/' in path):\n        raise ReturnCode(('called cli_put_container with object %r' % path))\n    body = None\n    if context.input_:\n        if (context.input_ == '-'):\n            body = context.io_manager.get_stdin()\n        else:\n            body = open(context.input_, 'rb')\n    with context.client_manager.with_client() as client:\n        (status, reason, headers, contents) = client.put_container(path, headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n        if hasattr(contents, 'read'):\n            contents.read()\n    if ((status // 100) != 2):\n        raise ReturnCode(('putting container %r: %s %s' % (path, status, reason)))\n", "label": 0}
{"function": "\n\ndef _on_headers(self, data):\n    try:\n        data = native_str(data.decode('latin1'))\n        eol = data.find('\\r\\n')\n        start_line = data[:eol]\n        try:\n            (method, uri, version) = start_line.split(' ')\n        except ValueError:\n            raise _BadRequestException('Malformed HTTP request line')\n        if (not version.startswith('HTTP/')):\n            raise _BadRequestException('Malformed HTTP version in HTTP Request-Line')\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            raise _BadRequestException('Malformed HTTP headers')\n        if (self.address_family in (socket.AF_INET, socket.AF_INET6)):\n            remote_ip = self.address[0]\n        else:\n            remote_ip = '0.0.0.0'\n        self._request = HTTPRequest(connection=self, method=method, uri=uri, version=version, headers=headers, remote_ip=remote_ip, protocol=self.protocol)\n        content_length = headers.get('Content-Length')\n        if content_length:\n            content_length = int(content_length)\n            if (content_length > self.stream.max_buffer_size):\n                raise _BadRequestException('Content-Length too long')\n            if (headers.get('Expect') == '100-continue'):\n                self.stream.write(b'HTTP/1.1 100 (Continue)\\r\\n\\r\\n')\n            self.stream.read_bytes(content_length, self._on_request_body)\n            return\n        self.request_callback(self._request)\n    except _BadRequestException as e:\n        gen_log.info('Malformed HTTP request from %s: %s', self.address[0], e)\n        self.close()\n        return\n", "label": 1}
{"function": "\n\n@mock.patch('changes.config.queue.delay')\n@mock.patch.object(HistoricalImmutableStep, 'get_implementation')\n@responses.activate\ndef test_finished(self, get_implementation, queue_delay):\n    responses.add(responses.GET, SyncJobStepTest.ARTIFACTSTORE_REQUEST_RE, body='', status=404)\n    implementation = mock.Mock()\n    get_implementation.return_value = implementation\n\n    def mark_finished(step):\n        step.status = Status.finished\n        step.result = Result.failed\n    implementation.update_step.side_effect = mark_finished\n    project = self.create_project()\n    build = self.create_build(project=project)\n    job = self.create_job(build=build)\n    plan = self.create_plan(project)\n    self.create_step(plan, implementation='test', order=0)\n    self.create_job_plan(job, plan)\n    phase = self.create_jobphase(job)\n    step = self.create_jobstep(phase)\n    task = self.create_task(parent_id=job.id, task_id=step.id, task_name='sync_job_step', status=Status.finished)\n    db.session.add(TestCase(name='test', step_id=step.id, job_id=job.id, project_id=project.id, result=Result.failed))\n    db.session.add(FileCoverage(job=job, step=step, project=job.project, filename='foo.py', data='CCCUUUCCCUUNNN', lines_covered=6, lines_uncovered=5, diff_lines_covered=3, diff_lines_uncovered=2))\n    db.session.commit()\n    sync_job_step(step_id=step.id.hex, task_id=step.id.hex, parent_task_id=job.id.hex)\n    implementation.update_step.assert_called_once_with(step=step)\n    db.session.expire(step)\n    db.session.expire(task)\n    step = JobStep.query.get(step.id)\n    assert (step.status == Status.finished)\n    task = Task.query.get(task.id)\n    assert (task.status == Status.finished)\n    assert (len(queue_delay.mock_calls) == 0)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'tests_missing')).first()\n    assert (stat.value == 0)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'lines_covered')).first()\n    assert (stat.value == 6)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'lines_uncovered')).first()\n    assert (stat.value == 5)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'diff_lines_covered')).first()\n    assert (stat.value == 3)\n    stat = ItemStat.query.filter((ItemStat.item_id == step.id), (ItemStat.name == 'diff_lines_uncovered')).first()\n    assert (stat.value == 2)\n    assert FailureReason.query.filter((FailureReason.step_id == step.id), (FailureReason.reason == 'test_failures'))\n", "label": 1}
{"function": "\n\ndef polyhedra(self, wm):\n    'Iterates through the polyhedra that make up the closest volume to a certain vertex'\n    for (p, facerow) in enumerate(self.connected):\n        faces = facerow.indices\n        (pts, polys) = (_ptset(), _quadset())\n        if (len(faces) > 0):\n            poly = np.roll(self.polys[faces[0]], (- np.nonzero((self.polys[faces[0]] == p))[0][0]))\n            assert (pts[wm[p]] == 0)\n            assert (pts[self.pts[p]] == 1)\n            pts[wm[poly[[0, 1]]].mean(0)]\n            pts[self.pts[poly[[0, 1]]].mean(0)]\n            for face in faces:\n                poly = np.roll(self.polys[face], (- np.nonzero((self.polys[face] == p))[0][0]))\n                a = pts[wm[poly].mean(0)]\n                b = pts[self.pts[poly].mean(0)]\n                c = pts[wm[poly[[0, 2]]].mean(0)]\n                d = pts[self.pts[poly[[0, 2]]].mean(0)]\n                e = pts[wm[poly[[0, 1]]].mean(0)]\n                f = pts[self.pts[poly[[0, 1]]].mean(0)]\n                polys((0, c, a, e))\n                polys((1, f, b, d))\n                polys((1, d, c, 0))\n                polys((1, 0, e, f))\n                polys((f, e, a, b))\n                polys((d, b, a, c))\n        (yield (pts.points, np.array(list(polys.triangles))))\n", "label": 0}
{"function": "\n\ndef _apt_packages(to_install=None, pkg_list=None):\n    '\\n    Install packages available via apt-get.\\n    Note that ``to_install`` and ``pkg_list`` arguments cannot be used simultaneously.\\n\\n    :type to_install:  list\\n    :param to_install: A list of strings (ie, groups) present in the ``main.yaml``\\n                       config file that will be used to filter out the specific\\n                       packages to be installed.\\n\\n    :type pkg_list:  list\\n    :param pkg_list: An explicit list of packages to install. No other files,\\n                     flavors are considered.\\n    '\n    if ('minimal' not in env.flavor.short_name):\n        env.logger.info('Update the system')\n        with settings(warn_only=True):\n            env.safe_sudo('apt-get update')\n    if (to_install is not None):\n        config_file = get_config_file(env, 'packages.yaml')\n        if (('minimal' not in env.flavor.name) and ('minimal' not in env.flavor.short_name)):\n            env.flavor.apt_upgrade_system(env=env)\n        (packages, _) = _yaml_to_packages(config_file.base, to_install, config_file.dist)\n        packages = env.flavor.rewrite_config_items('packages', packages)\n    elif (pkg_list is not None):\n        env.logger.info('Will install specific packages: {0}'.format(pkg_list))\n        packages = pkg_list\n    else:\n        raise ValueError('Need a file with packages or a list of packages')\n    group_size = 30\n    i = 0\n    env.logger.info(('Installing %i packages' % len(packages)))\n    while (i < len(packages)):\n        env.logger.info('Package install progress: {0}/{1}'.format(i, len(packages)))\n        env.safe_sudo(('apt-get -y --force-yes install %s' % ' '.join(packages[i:(i + group_size)])))\n        i += group_size\n    env.safe_sudo('apt-get clean')\n", "label": 0}
{"function": "\n\n@expose('/delete/', methods=('POST',))\ndef delete(self):\n    '\\n            Delete view method\\n        '\n    form = self.delete_form()\n    path = form.path.data\n    if path:\n        return_url = self._get_dir_url('.index', op.dirname(path))\n    else:\n        return_url = self.get_url('.index')\n    if self.validate_form(form):\n        (base_path, full_path, path) = self._normalize_path(path)\n        if (not self.can_delete):\n            flash(gettext('Deletion is disabled.'), 'error')\n            return redirect(return_url)\n        if (not self.is_accessible_path(path)):\n            flash(gettext('Permission denied.'), 'error')\n            return redirect(self._get_dir_url('.index'))\n        if op.isdir(full_path):\n            if (not self.can_delete_dirs):\n                flash(gettext('Directory deletion is disabled.'), 'error')\n                return redirect(return_url)\n            try:\n                shutil.rmtree(full_path)\n                self.on_directory_delete(full_path, path)\n                flash(gettext('Directory \"%(path)s\" was successfully deleted.', path=path))\n            except Exception as ex:\n                flash(gettext('Failed to delete directory: %(error)s', error=ex), 'error')\n        else:\n            try:\n                os.remove(full_path)\n                self.on_file_delete(full_path, path)\n                flash(gettext('File \"%(name)s\" was successfully deleted.', name=path))\n            except Exception as ex:\n                flash(gettext('Failed to delete file: %(name)s', name=ex), 'error')\n    else:\n        helpers.flash_errors(form, message='Failed to delete file. %(error)s')\n    return redirect(return_url)\n", "label": 1}
{"function": "\n\n@RequireLogin()\ndef put(request, rule_id):\n    '\\n    Update existing rule based on rule_id.\\n    '\n    try:\n        in_json = json.loads(request.body)\n    except ValueError:\n        return HttpResponseBadRequest('invalid JSON')\n    dbc = db_model.connect()\n    try:\n        rule = dbc.rule.find_one({\n            '_id': ObjectId(rule_id),\n        })\n    except InvalidId:\n        return HttpResponseNotFound()\n    if (rule is None):\n        return HttpResponseNotFound()\n    else:\n        in_json['createdAt'] = rule['createdAt']\n        rule = rule_model.validate(in_json)\n        if (rule is None):\n            return HttpResponseBadRequest('invalid rule')\n        else:\n            rule['_id'] = ObjectId(rule_id)\n            rule['updatedAt'] = datetime.isoformat(datetime.now())\n            dbc.rule.save(rule)\n            r = JsonResponse({\n                'id': rule_id,\n            })\n            r['location'] = ('/api/rule/%s' % rule_id)\n            logger.info((\"rule '%s' updated by '%s'\" % (rule_id, request.user['username'])))\n            return r\n", "label": 0}
{"function": "\n\ndef the_local_prediction_confidence_is(step, confidence):\n    if (isinstance(world.local_prediction, list) or isinstance(world.local_prediction, tuple)):\n        local_confidence = world.local_prediction[1]\n    else:\n        local_confidence = world.local_prediction['confidence']\n    local_confidence = round(float(local_confidence), 4)\n    confidence = round(float(confidence), 4)\n    if (local_confidence == confidence):\n        assert True\n    else:\n        assert False, ('found: %s, expected %s' % (local_confidence, confidence))\n", "label": 0}
{"function": "\n\n@skipif(('GPy' not in sys.modules), 'this test requires hyperopt')\ndef test_gp():\n    searchspace = SearchSpace()\n    searchspace.add_float('x', (- 10), 10)\n    searchspace.add_float('y', 1, 10, warp='log')\n    searchspace.add_int('z', (- 10), 10)\n    searchspace.add_enum('w', ['opt1', 'opt2'])\n    history = [(searchspace.rvs(), np.random.random(), 'SUCCEEDED') for _ in range(4)]\n    params = GP().suggest(history, searchspace)\n    for (k, v) in iteritems(params):\n        assert (k in searchspace.variables)\n        if isinstance(searchspace[k], EnumVariable):\n            assert (v in searchspace[k].choices)\n        elif isinstance(searchspace[k], FloatVariable):\n            assert (searchspace[k].min <= v <= searchspace[k].max)\n        elif isinstance(searchspace[k], IntVariable):\n            assert (searchspace[k].min <= v <= searchspace[k].max)\n        else:\n            assert False\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    assert name.endswith('PropMap'), 'Please use convention: ___PropMap, e.g. ElectromagneticPropMap'\n    _properties = {\n        \n    }\n    for base in bases:\n        for baseProp in getattr(base, '_properties', {\n            \n        }):\n            _properties[baseProp] = base._properties[baseProp]\n    keys = [key for key in attrs]\n    for attr in keys:\n        if isinstance(attrs[attr], Property):\n            attrs[attr].name = attr\n            attrs[(attr + 'Map')] = attrs[attr]._getMapProperty()\n            attrs[(attr + 'Index')] = attrs[attr]._getIndexProperty()\n            _properties[attr] = attrs[attr]\n            attrs.pop(attr)\n    attrs['_properties'] = _properties\n    defaultInvProps = []\n    for p in _properties:\n        prop = _properties[p]\n        if prop.defaultInvProp:\n            defaultInvProps += [p]\n        if (prop.propertyLink is not None):\n            assert (prop.propertyLink[0] in _properties), (\"You can only link to things that exist: '%s' is trying to link to '%s'\" % (prop.name, prop.propertyLink[0]))\n    if (len(defaultInvProps) > 1):\n        raise Exception(('You have more than one default inversion property: %s' % defaultInvProps))\n    newClass = super(_PropMapMetaClass, cls).__new__(cls, name, bases, attrs)\n    newClass.PropModel = cls.createPropModelClass(newClass, name, _properties)\n    _PROPMAPCLASSREGISTRY[name] = newClass\n    return newClass\n", "label": 1}
{"function": "\n\ndef inner_optimization_result_test(self):\n    'Test for the correct result of a GridSearch optimization.'\n    fm = ExponentialSmoothing()\n    startingPercentage = 0.0\n    endPercentage = 100.0\n    self.timeSeries.normalize('second')\n    results = []\n    for smoothingFactor in [(alpha / 100.0) for alpha in xrange(1, 100)]:\n        fm.set_parameter('smoothingFactor', smoothingFactor)\n        resultTS = self.timeSeries.apply(fm)\n        error = SMAPE()\n        error.initialize(self.timeSeries, resultTS)\n        results.append([error, smoothingFactor])\n    bestManualResult = min(results, key=(lambda item: item[0].get_error(startingPercentage, endPercentage)))\n    gridSearch = GridSearch(SMAPE, precision=(- 2))\n    gridSearch._startingPercentage = startingPercentage\n    gridSearch._endPercentage = endPercentage\n    result = gridSearch.optimize_forecasting_method(self.timeSeries, fm)\n    bestManualAlpha = bestManualResult[1]\n    errorManualResult = bestManualResult[0].get_error()\n    bestGridSearchAlpha = result[1]['smoothingFactor']\n    errorGridSearchResult = result[0].get_error()\n    assert (str(errorManualResult)[:8] >= str(errorGridSearchResult)[:8])\n    assert (str(bestManualAlpha)[:5] == str(bestGridSearchAlpha)[:5])\n", "label": 0}
{"function": "\n\ndef test_mutable_array():\n    bool_vec = np.array([True, False])\n    int_vec = np.array([1, 2])\n    (typed, _) = specialize(f, [bool_vec, int_vec])\n    mutable_types = mutability_analysis.find_mutable_types(typed)\n    int_array_t = make_array_type(Int64, 1)\n    bool_array_t = make_array_type(Bool, 1)\n    assert (int_array_t not in mutable_types), (\"Didn't expect %s in mutable_types %s\" % (int_array_t, mutable_types))\n    assert (bool_array_t in mutable_types), ('Expected %s in mutable_types %s' % (bool_array_t, mutable_types))\n    lowered = lowering.apply(typed)\n    mutable_types = mutability_analysis.find_mutable_types(lowered)\n    ptr_bool_t = ptr_type(Bool)\n    ptr_int_t = ptr_type(Int64)\n    assert (ptr_int_t not in mutable_types), (\"Didn't expect %s in lowered mutable types %s\" % (ptr_int_t, mutable_types))\n    assert (ptr_bool_t in mutable_types), ('Expected %s in lowered mutable_types %s' % (ptr_bool_t, mutable_types))\n", "label": 0}
{"function": "\n\ndef test_new_term_with_terms():\n    rules = {\n        'fields': ['a'],\n        'timestamp_field': '@timestamp',\n        'es_host': 'example.com',\n        'es_port': 10,\n        'index': 'logstash',\n        'query_key': 'a',\n    }\n    mock_res = {\n        'aggregations': {\n            'filtered': {\n                'values': {\n                    'buckets': [{\n                        'key': 'key1',\n                        'doc_count': 1,\n                    }, {\n                        'key': 'key2',\n                        'doc_count': 5,\n                    }],\n                },\n            },\n        },\n    }\n    with mock.patch('elastalert.ruletypes.Elasticsearch') as mock_es:\n        mock_es.return_value = mock.Mock()\n        mock_es.return_value.search.return_value = mock_res\n        rule = NewTermsRule(rules)\n        assert (rule.es.search.call_count == 1)\n    terms = {\n        ts_now(): [{\n            'key': 'key1',\n            'doc_count': 1,\n        }, {\n            'key': 'key2',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (rule.matches == [])\n    terms = {\n        ts_now(): [{\n            'key': 'key3',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0]['new_field'] == 'a')\n    assert (rule.matches[0]['a'] == 'key3')\n    rule.matches = []\n    terms = {\n        ts_now(): [{\n            'key': 'key3',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (rule.matches == [])\n", "label": 0}
{"function": "\n\n@freeze_time('2012-01-14')\ndef test_addition():\n    now = datetime.datetime.now()\n    later = (now + datetime.timedelta(days=1))\n    other_later = (now + relativedelta(days=1))\n    assert utils.is_fake_datetime(later)\n    assert utils.is_fake_datetime(other_later)\n    today = datetime.date.today()\n    tomorrow = (today + datetime.timedelta(days=1))\n    other_tomorrow = (today + relativedelta(days=1))\n    assert utils.is_fake_date(tomorrow)\n    assert utils.is_fake_date(other_tomorrow)\n", "label": 0}
{"function": "\n\ndef test_remove_role():\n    db = SQLAlchemy('sqlite:///:memory:')\n    auth = authcode.Auth(SECRET_KEY, db=db, roles=True)\n    User = auth.User\n    Role = auth.Role\n    db.create_all()\n    user = User(login='meh', password='foobar')\n    db.session.add(user)\n    db.session.commit()\n    assert hasattr(auth, 'Role')\n    assert hasattr(User, 'roles')\n    user.add_role('admin')\n    db.session.commit()\n    assert user.has_role('admin')\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('foobar')\n    db.session.commit()\n    assert (db.query(Role).count() == 1)\n", "label": 1}
{"function": "\n\ndef test_virtual_memory(self):\n    mem = psutil.virtual_memory()\n    assert (mem.total > 0), mem\n    assert (mem.available > 0), mem\n    assert (0 <= mem.percent <= 100), mem\n    assert (mem.used > 0), mem\n    assert (mem.free >= 0), mem\n    for name in mem._fields:\n        if (name != 'total'):\n            value = getattr(mem, name)\n            if (not (value >= 0)):\n                self.fail(('%r < 0 (%s)' % (name, value)))\n            if (value > mem.total):\n                self.fail(('%r > total (total=%s, %s=%s)' % (name, mem.total, name, value)))\n", "label": 1}
{"function": "\n\ndef load_module(self, fullname):\n    try:\n        return self.sys.modules[fullname]\n    except KeyError:\n        pass\n    subname = fullname[len(self.prefix):]\n    actual = self.alias.format(subname)\n    try:\n        __import__(actual)\n    except ImportError:\n        raise ImportError(('No module named ' + repr(fullname)))\n    module = self.sys.modules[actual]\n    self.sys.modules[fullname] = module\n    if ('.' not in subname):\n        setattr(self.sys.modules[self.namespace], subname, module)\n    return module\n", "label": 0}
{"function": "\n\ndef static(environ, start_response, path):\n    logger.info(('[static]sending: %s' % (path,)))\n    try:\n        text = open(path).read()\n        if path.endswith('.ico'):\n            start_response('200 OK', [('Content-Type', 'image/x-icon')])\n        elif path.endswith('.html'):\n            start_response('200 OK', [('Content-Type', 'text/html')])\n        elif path.endswith('.json'):\n            start_response('200 OK', [('Content-Type', 'application/json')])\n        elif path.endswith('.txt'):\n            start_response('200 OK', [('Content-Type', 'text/plain')])\n        elif path.endswith('.css'):\n            start_response('200 OK', [('Content-Type', 'text/css')])\n        else:\n            start_response('200 OK', [('Content-Type', 'text/xml')])\n        return [text]\n    except IOError:\n        resp = NotFound()\n        return resp(environ, start_response)\n", "label": 0}
{"function": "\n\ndef update_room_topic(self):\n    'Get room topic\\n\\n        Return True if the room topic changed, False if not\\n        '\n    try:\n        response = self.client.api.get_room_topic(self.room_id)\n        if (('topic' in response) and (response['topic'] != self.topic)):\n            self.topic = response['topic']\n            return True\n        else:\n            return False\n    except MatrixRequestError:\n        return False\n", "label": 0}
{"function": "\n\ndef get_internal_wsgi_application():\n    \"\\n    Loads and returns the WSGI application as configured by the user in\\n    ``settings.WSGI_APPLICATION``. With the default ``startproject`` layout,\\n    this will be the ``application`` object in ``projectname/wsgi.py``.\\n\\n    This function, and the ``WSGI_APPLICATION`` setting itself, are only useful\\n    for Django's internal servers (runserver, runfcgi); external WSGI servers\\n    should just be configured to point to the correct application object\\n    directly.\\n\\n    If settings.WSGI_APPLICATION is not set (is ``None``), we just return\\n    whatever ``django.core.wsgi.get_wsgi_application`` returns.\\n\\n    \"\n    from django.conf import settings\n    app_path = getattr(settings, 'WSGI_APPLICATION')\n    if (app_path is None):\n        return get_wsgi_application()\n    (module_name, attr) = app_path.rsplit('.', 1)\n    try:\n        mod = import_module(module_name)\n    except ImportError as e:\n        raise ImproperlyConfigured((\"WSGI application '%s' could not be loaded; could not import module '%s': %s\" % (app_path, module_name, e)))\n    try:\n        app = getattr(mod, attr)\n    except AttributeError as e:\n        raise ImproperlyConfigured((\"WSGI application '%s' could not be loaded; can't find '%s' in module '%s': %s\" % (app_path, attr, module_name, e)))\n    return app\n", "label": 0}
{"function": "\n\n@staticmethod\ndef shutdown(dbpool, filename, archive_db_dir):\n    '\\n        Facilitates cleaning up filesystem objects after the last\\n        user of the connection pool goes away.\\n        '\n    try:\n        dbpool.close()\n    except Exception as e:\n        logging.info('SqlConnections: dbpool.close failed in shutdown: %s', e)\n    if ((filename != ':memory:') and os.path.exists(filename)):\n        if archive_db_dir:\n            logging.info(\"SqlConnections: Archiving db file '%s' to '%s'\", filename, archive_db_dir)\n            try:\n                os.renames(filename, archive_db_dir)\n            except Exception as e:\n                logging.info('SqlConnections: ERROR: Archive failed! %s', e)\n        else:\n            logging.info(\"SqlConnections: Removing db file '%s'\", filename)\n            try:\n                os.remove(filename)\n            except Exception as e:\n                logging.info('SqlConnections: ERROR: Could not remove db file! %s', e)\n", "label": 0}
{"function": "\n\ndef main():\n    (options, args) = parse_args()\n    names = sorted((name for name in resource_listdir('trac.wiki', 'default-pages') if (not name.startswith('.'))))\n    if args:\n        args = sorted((set(names) & set(map(os.path.basename, args))))\n    else:\n        args = names\n    if options.download:\n        download_default_pages(args, options.prefix)\n    env = EnvironmentStub(disable=['trac.mimeview.pygments.*'])\n    load_components(env)\n    with env.db_transaction:\n        for name in names:\n            wiki = WikiPage(env, name)\n            wiki.text = resource_string('trac.wiki', ('default-pages/' + name)).decode('utf-8')\n            if wiki.text:\n                wiki.save('trac', '')\n            else:\n                printout(('%s: Skipped empty page' % name))\n    req = Mock(href=Href('/'), abs_href=Href('http://localhost/'), perm=MockPerm())\n    for name in args:\n        wiki = WikiPage(env, name)\n        if (not wiki.exists):\n            continue\n        context = web_context(req, wiki.resource)\n        out = DummyIO()\n        DefaultWikiChecker(env, context, name).format(wiki.text, out)\n", "label": 1}
{"function": "\n\n@contextmanager\ndef WorkingDirectory(new_working_directory):\n    \"Changes the working directory for the duration of a 'with' call.\\n\\n  Args:\\n    new_working_directory: The directory to switch to before executing wrapped\\n      code. A None value indicates that no switching is necessary.\\n\\n  Yields:\\n    Once after working directory has been changed.\\n  \"\n    prev_working_directory = None\n    try:\n        prev_working_directory = os.getcwd()\n    except OSError:\n        pass\n    if new_working_directory:\n        os.chdir(new_working_directory)\n    try:\n        (yield)\n    finally:\n        if (new_working_directory and prev_working_directory):\n            os.chdir(prev_working_directory)\n", "label": 0}
{"function": "\n\ndef _importers():\n    global _IMPORTS\n    if _IMPORTS:\n        return\n    _IMPORTS = True\n    global _HAS_BS4, _HAS_LXML, _HAS_HTML5LIB\n    try:\n        import bs4\n        _HAS_BS4 = True\n    except ImportError:\n        pass\n    try:\n        import lxml\n        _HAS_LXML = True\n    except ImportError:\n        pass\n    try:\n        import html5lib\n        _HAS_HTML5LIB = True\n    except ImportError:\n        pass\n", "label": 0}
{"function": "\n\ndef remove_effect_handle(self, handle):\n    '\\n        Remove given handle\\n\\n        :param handle: handle to remove\\n        :type handle: EffectHandle\\n        '\n    assert (handle is not None)\n    for (key, value) in self.handles.items():\n        if (handle in value):\n            value.remove(handle)\n", "label": 0}
{"function": "\n\n@expose(help='Display Nginx configuration of example.com')\ndef show(self):\n    if (not self.app.pargs.site_name):\n        try:\n            while (not self.app.pargs.site_name):\n                self.app.pargs.site_name = input('Enter site name : ').strip()\n        except IOError as e:\n            Log.error(self, 'could not input site name')\n    self.app.pargs.site_name = self.app.pargs.site_name.strip()\n    (ee_domain, ee_www_domain) = ValidateDomain(self.app.pargs.site_name)\n    if (not check_domain_exists(self, ee_domain)):\n        Log.error(self, 'site {0} does not exist'.format(ee_domain))\n    if os.path.isfile('/etc/nginx/sites-available/{0}'.format(ee_domain)):\n        Log.info(self, 'Display NGINX configuration for {0}'.format(ee_domain))\n        f = open('/etc/nginx/sites-available/{0}'.format(ee_domain), encoding='utf-8', mode='r')\n        text = f.read()\n        Log.info(self, (Log.ENDC + text))\n        f.close()\n    else:\n        Log.error(self, 'nginx configuration file does not exists'.format(ee_domain))\n", "label": 0}
{"function": "\n\ndef fix_lib64(lib_dir):\n    \"\\n    Some platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y\\n    instead of lib/pythonX.Y.  If this is such a platform we'll just create a\\n    symlink so lib64 points to lib\\n    \"\n    if [p for p in distutils.sysconfig.get_config_vars().values() if (isinstance(p, basestring) and ('lib64' in p))]:\n        logger.debug('This system uses lib64; symlinking lib64 to lib')\n        assert (os.path.basename(lib_dir) == ('python%s' % sys.version[:3])), ('Unexpected python lib dir: %r' % lib_dir)\n        lib_parent = os.path.dirname(lib_dir)\n        assert (os.path.basename(lib_parent) == 'lib'), ('Unexpected parent dir: %r' % lib_parent)\n        copyfile(lib_parent, os.path.join(os.path.dirname(lib_parent), 'lib64'))\n", "label": 0}
{"function": "\n\ndef test_neg_list_trim_key_is_none(self):\n    '\\n        Invoke list_trim() with key is none\\n        '\n    try:\n        self.as_connection.list_trim(None, 'contact_no', 0, 2)\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'key is invalid')\n", "label": 0}
{"function": "\n\ndef test_logistic_regression_sample_weights():\n    (X, y) = make_classification(n_samples=20, n_features=5, n_informative=3, n_classes=2, random_state=0)\n    sample_weight = (y + 1)\n    for LR in [LogisticRegression, LogisticRegressionCV]:\n        for solver in ['lbfgs', 'liblinear']:\n            clf_sw_none = LR(solver=solver, fit_intercept=False)\n            clf_sw_none.fit(X, y)\n            clf_sw_ones = LR(solver=solver, fit_intercept=False)\n            clf_sw_ones.fit(X, y, sample_weight=np.ones(y.shape[0]))\n            assert_array_almost_equal(clf_sw_none.coef_, clf_sw_ones.coef_, decimal=4)\n        clf_sw_lbfgs = LR(solver='lbfgs', fit_intercept=False)\n        clf_sw_lbfgs.fit(X, y, sample_weight=sample_weight)\n        clf_sw_n = LR(solver='newton-cg', fit_intercept=False)\n        clf_sw_n.fit(X, y, sample_weight=sample_weight)\n        clf_sw_sag = LR(solver='sag', fit_intercept=False, tol=1e-10)\n        with ignore_warnings():\n            clf_sw_sag.fit(X, y, sample_weight=sample_weight)\n        clf_sw_liblinear = LR(solver='liblinear', fit_intercept=False)\n        clf_sw_liblinear.fit(X, y, sample_weight=sample_weight)\n        assert_array_almost_equal(clf_sw_lbfgs.coef_, clf_sw_n.coef_, decimal=4)\n        assert_array_almost_equal(clf_sw_lbfgs.coef_, clf_sw_sag.coef_, decimal=4)\n        assert_array_almost_equal(clf_sw_lbfgs.coef_, clf_sw_liblinear.coef_, decimal=4)\n        for solver in ['lbfgs', 'liblinear']:\n            clf_cw_12 = LR(solver=solver, fit_intercept=False, class_weight={\n                0: 1,\n                1: 2,\n            })\n            clf_cw_12.fit(X, y)\n            clf_sw_12 = LR(solver=solver, fit_intercept=False)\n            clf_sw_12.fit(X, y, sample_weight=sample_weight)\n            assert_array_almost_equal(clf_cw_12.coef_, clf_sw_12.coef_, decimal=4)\n    clf_cw = LogisticRegression(solver='liblinear', fit_intercept=False, class_weight={\n        0: 1,\n        1: 2,\n    }, penalty='l1')\n    clf_cw.fit(X, y)\n    clf_sw = LogisticRegression(solver='liblinear', fit_intercept=False, penalty='l1')\n    clf_sw.fit(X, y, sample_weight)\n    assert_array_almost_equal(clf_cw.coef_, clf_sw.coef_, decimal=4)\n    clf_cw = LogisticRegression(solver='liblinear', fit_intercept=False, class_weight={\n        0: 1,\n        1: 2,\n    }, penalty='l2', dual=True)\n    clf_cw.fit(X, y)\n    clf_sw = LogisticRegression(solver='liblinear', fit_intercept=False, penalty='l2', dual=True)\n    clf_sw.fit(X, y, sample_weight)\n    assert_array_almost_equal(clf_cw.coef_, clf_sw.coef_, decimal=4)\n", "label": 0}
{"function": "\n\ndef _PopulateX509(self):\n    with self._x509_init_lock:\n        if (self._x509 is None):\n            url = ('https://www.googleapis.com/service_accounts/v1/metadata/x509/%s' % urllib.unquote_plus(self._credentials.service_account_email))\n            response = urlfetch.fetch(url=url, validate_certificate=True, method=urlfetch.GET)\n            if (response.status_code != 200):\n                raise apiproxy_errors.ApplicationError(app_identity_service_pb.AppIdentityServiceError.UNKNOWN_ERROR, ('Unable to load X509 cert: %s Response code: %i, Content: %s' % (url, response.status_code, response.content)))\n            message = 'dummy'\n            (_, signature) = self._credentials.sign_blob(message)\n            for (signing_key, x509) in json.loads(response.content).items():\n                der = rsa.pem.load_pem(x509, 'CERTIFICATE')\n                (asn1_cert, _) = decoder.decode(der, asn1Spec=Certificate())\n                key_bitstring = asn1_cert['tbsCertificate']['subjectPublicKeyInfo']['subjectPublicKey']\n                key_bytearray = BitStringToByteString(key_bitstring)\n                public_key = rsa.PublicKey.load_pkcs1(key_bytearray, 'DER')\n                try:\n                    if rsa.pkcs1.verify(message, signature, public_key):\n                        self._x509 = x509\n                        self._signing_key = signing_key\n                        return\n                except rsa.pkcs1.VerificationError:\n                    pass\n            raise apiproxy_errors.ApplicationError(app_identity_service_pb.AppIdentityServiceError.UNKNOWN_ERROR, ('Unable to find matching X509 cert for private key: %s' % url))\n", "label": 0}
{"function": "\n\ndef DeclareLocks(self, level):\n    if (level == locking.LEVEL_NODEGROUP):\n        assert (not self.needed_locks[locking.LEVEL_NODEGROUP])\n        if self.req_target_uuids:\n            lock_groups = set(self.req_target_uuids)\n            instance_groups = self.cfg.GetInstanceNodeGroups(self.op.instance_uuid)\n            lock_groups.update(instance_groups)\n        else:\n            lock_groups = locking.ALL_SET\n        self.needed_locks[locking.LEVEL_NODEGROUP] = lock_groups\n    elif (level == locking.LEVEL_NODE):\n        if self.req_target_uuids:\n            self.recalculate_locks[locking.LEVEL_NODE] = constants.LOCKS_APPEND\n            self._LockInstancesNodes()\n            lock_groups = (frozenset(self.owned_locks(locking.LEVEL_NODEGROUP)) | self.cfg.GetInstanceNodeGroups(self.op.instance_uuid))\n            member_nodes = [node_uuid for group in lock_groups for node_uuid in self.cfg.GetNodeGroup(group).members]\n            self.needed_locks[locking.LEVEL_NODE].extend(member_nodes)\n        else:\n            self.needed_locks[locking.LEVEL_NODE] = locking.ALL_SET\n", "label": 1}
{"function": "\n\ndef test_admin_inheritor_documents(suite, administrator, docs):\n    jwt = _login(administrator)\n    headers = _auth_header(jwt)\n    docs = [{\n        'name': 'Added Document {0}'.format(x),\n        'date': datetime.utcnow().isoformat(),\n    } for x in range(101)]\n    inheritor_documents = 'http://localhost:5000/api/authorized-app/simple-documents'\n    confirmed_dangerous_delete = requests.delete(inheritor_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    post = requests.post(inheritor_documents, headers=headers, data=json.dumps(docs))\n    assert post.ok\n    get_all = requests.get((inheritor_documents + ';json'), headers=headers)\n    assert get_all.ok\n    confirmed_dangerous_delete = requests.delete(inheritor_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    get_empty = requests.get((inheritor_documents + ';json'), headers=headers)\n    assert get_empty.ok\n    _logout(jwt)\n", "label": 0}
{"function": "\n\ndef test_expand_header_single_threshold():\n    header_row = (((BASE_HEADERS + SAMBAMBA_HEADERS) + THRESHOLDS[:1]) + ['sampleName'])\n    header = sambamba.expand_header(header_row)\n    assert (header['readCount'] == 3)\n    assert (header['thresholds'] == {\n        10: 5,\n    })\n    assert (header['sampleName'] == 6)\n", "label": 0}
{"function": "\n\ndef _execute(self, cprefix, command):\n    if (not command):\n        raise iex.BadRequest(\"execute argument 'command' not present\")\n    if (not command.command):\n        raise iex.BadRequest('command not set')\n    cmd_res = IonObject('AgentCommandResult', command_id=command.command_id, command=command.command)\n    cmd_func = getattr(self, (cprefix + str(command.command)), None)\n    if cmd_func:\n        cmd_res.ts_execute = get_ion_ts()\n        try:\n            res = cmd_func(*command.args, **command.kwargs)\n            cmd_res.status = 0\n            cmd_res.result = res\n        except iex.IonException as ex:\n            cmd_res.status = getattr(ex, 'status_code', (- 1))\n            cmd_res.result = str(ex)\n            log.warn(('Agent command %s failed with trace=%s' % (command.command, traceback.format_exc())))\n    else:\n        log.info(('Agent command not supported: %s' % command.command))\n        ex = iex.NotFound(('Command not supported: %s' % command.command))\n        cmd_res.status = iex.NotFound.status_code\n        cmd_res.result = str(ex)\n    sub_type = ('%s.%s' % (command.command, cmd_res.status))\n    event_data = self._post_execute_event_hook(event_type=self.COMMAND_EVENT_TYPE, origin=self.resource_id, origin_type=self.ORIGIN_TYPE, sub_type=sub_type, command=str(command.command), result=str(cmd_res.result))\n    post_event = self._event_publisher.publish_event(**event_data)\n    return cmd_res\n", "label": 0}
{"function": "\n\ndef prepare_on_all_hosts(self, query, excluded_host):\n    '\\n        Prepare the given query on all hosts, excluding ``excluded_host``.\\n        Intended for internal use only.\\n        '\n    futures = []\n    for host in self._pools.keys():\n        if ((host != excluded_host) and host.is_up):\n            future = ResponseFuture(self, PrepareMessage(query=query), None, self.default_timeout)\n            try:\n                request_id = future._query(host)\n            except Exception:\n                log.exception('Error preparing query for host %s:', host)\n                continue\n            if (request_id is None):\n                log.debug('Failed to prepare query for host %s: %r', host, future._errors.get(host))\n                continue\n            futures.append((host, future))\n    for (host, future) in futures:\n        try:\n            future.result()\n        except Exception:\n            log.exception('Error preparing query for host %s:', host)\n", "label": 1}
{"function": "\n\ndef check_error_callback(test, function, arg, expected_error_count, expect_result):\n    'General test template for error_callback argument.\\n\\n    :param test: Test case instance.\\n    :param function: Either try_import or try_imports.\\n    :param arg: Name or names to import.\\n    :param expected_error_count: Expected number of calls to the callback.\\n    :param expect_result: Boolean for whether a module should\\n        ultimately be returned or not.\\n    '\n    cb_calls = []\n\n    def cb(e):\n        test.assertIsInstance(e, ImportError)\n        cb_calls.append(e)\n    try:\n        result = function(arg, error_callback=cb)\n    except ImportError:\n        test.assertFalse(expect_result)\n    else:\n        if expect_result:\n            test.assertThat(result, Not(Is(None)))\n        else:\n            test.assertThat(result, Is(None))\n    test.assertEquals(len(cb_calls), expected_error_count)\n", "label": 0}
{"function": "\n\ndef json(self, **kwargs):\n    'Returns the json-encoded content of a response, if any.\\n\\n        :param \\\\*\\\\*kwargs: Optional arguments that ``json.loads`` takes.\\n        '\n    if ((not self.encoding) and (len(self.content) > 3)):\n        encoding = guess_json_utf(self.content)\n        if (encoding is not None):\n            try:\n                return complexjson.loads(self.content.decode(encoding), **kwargs)\n            except UnicodeDecodeError:\n                pass\n    return complexjson.loads(self.text, **kwargs)\n", "label": 0}
{"function": "\n\ndef put(self, objects, force=False, commit=True):\n    if self.stopped:\n        return\n    self.init()\n    if (isinstance(objects, basestring) or (not iterable(objects))):\n        return self.put_one(objects, force, commit)\n    remains = []\n    for obj in objects:\n        result = self.put_one(obj, force, commit=False)\n        if (result is not None):\n            remains.append(result)\n    m = self.map_handles[WRITE_ENTRANCE]\n    if ((len(remains) > 0) and (m is not None)):\n        with self.lock:\n            m.flush()\n    return remains\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('auth_scopes', (('users:write',), ('users:read',), ('users:read', 'users:write')))\ndef test_getting_list_of_users_by_unauthorized_user_must_fail(flask_app_client, regular_user, auth_scopes):\n    with flask_app_client.login(regular_user, auth_scopes=auth_scopes):\n        response = flask_app_client.get('/api/v1/users/')\n    if ('users:read' in auth_scopes):\n        assert (response.status_code == 403)\n    else:\n        assert (response.status_code == 401)\n    assert (response.content_type == 'application/json')\n    assert (set(response.json.keys()) >= {'status', 'message'})\n", "label": 0}
{"function": "\n\ndef unique_messages():\n    basedir = None\n    if os.path.isdir(os.path.join('conf', 'locale')):\n        basedir = os.path.abspath(os.path.join('conf', 'locale'))\n    elif os.path.isdir('locale'):\n        basedir = os.path.abspath('locale')\n    else:\n        print('This script should be run from the Django Git tree or your project or app tree.')\n        sys.exit(1)\n    for (dirpath, dirnames, filenames) in os.walk(basedir):\n        for f in filenames:\n            if f.endswith('.po'):\n                sys.stderr.write(('processing file %s in %s\\n' % (f, dirpath)))\n                pf = os.path.splitext(os.path.join(dirpath, f))[0]\n                cmd = ('msguniq \"%s.po\"' % pf)\n                stdout = os.popen(cmd)\n                msg = stdout.read()\n                with open(('%s.po' % pf), 'w') as fp:\n                    fp.write(msg)\n", "label": 0}
{"function": "\n\n@memoized\ndef value(self, **kwargs):\n    operator = kwargs[self.operator_param_name]\n    operand = kwargs[self.operand_param_name]\n    if (operand == ''):\n        return None\n    try:\n        assert (operator in ['=', '!=', '<', '<=', '>', '>='])\n        assert (isinstance(operand, float) or isinstance(operand, int))\n    except AssertionError as e:\n        raise FilterValueException('Error parsing numeric filter parameters: {}'.format(e.message))\n    return {\n        'operator': operator,\n        'operand': operand,\n    }\n", "label": 0}
{"function": "\n\ndef cli_post(context, path, body=None):\n    '\\n    Performs a POST on the item (account, container, or object).\\n\\n    See :py:mod:`swiftly.cli.post` for context usage information.\\n\\n    See :py:class:`CLIPost` for more information.\\n\\n    :param context: The :py:class:`swiftly.cli.context.CLIContext` to\\n        use.\\n    :param path: The path to the item to issue a POST for.\\n    :param body: The body send along with the POST.\\n    '\n    path = (path.lstrip('/') if path else '')\n    (status, reason, headers, contents) = (0, 'Unknown', {\n        \n    }, '')\n    with context.client_manager.with_client() as client:\n        if (not path):\n            (status, reason, headers, contents) = client.post_account(headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n            if ((status // 100) != 2):\n                raise ReturnCode(('posting account: %s %s' % (status, reason)))\n        elif ('/' not in path.rstrip('/')):\n            path = path.rstrip('/')\n            (status, reason, headers, contents) = client.post_container(path, headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n            if ((status // 100) != 2):\n                raise ReturnCode(('posting container %r: %s %s' % (path, status, reason)))\n        else:\n            (status, reason, headers, contents) = client.post_object(*path.split('/', 1), headers=context.headers, query=context.query, cdn=context.cdn, body=body)\n            if ((status // 100) != 2):\n                raise ReturnCode(('posting object %r: %s %s' % (path, status, reason)))\n", "label": 0}
{"function": "\n\n@not_implemented_for('directed')\ndef generate_edgelist(G, delimiter=' ', data=True):\n    \"Generate a single line of the bipartite graph G in edge list format.\\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n       The graph is assumed to have node attribute `part` set to 0,1 representing\\n       the two graph parts\\n\\n    delimiter : string, optional\\n       Separator for node labels\\n\\n    data : bool or list of keys\\n       If False generate no edge data.  If True use a dictionary\\n       representation of edge data.  If a list of keys use a list of data\\n       values corresponding to the keys.\\n\\n    Returns\\n    -------\\n    lines : string\\n        Lines of data in adjlist format.\\n\\n    Examples\\n    --------\\n    >>> from networkx.algorithms import bipartite\\n    >>> G = nx.path_graph(4)\\n    >>> G.add_nodes_from([0,2], bipartite=0)\\n    >>> G.add_nodes_from([1,3], bipartite=1)\\n    >>> G[1][2]['weight'] = 3\\n    >>> G[2][3]['capacity'] = 12\\n    >>> for line in bipartite.generate_edgelist(G, data=False):\\n    ...     print(line)\\n    0 1\\n    2 1\\n    2 3\\n\\n    >>> for line in bipartite.generate_edgelist(G):\\n    ...     print(line)\\n    0 1 {}\\n    2 1 {'weight': 3}\\n    2 3 {'capacity': 12}\\n\\n    >>> for line in bipartite.generate_edgelist(G,data=['weight']):\\n    ...     print(line)\\n    0 1\\n    2 1 3\\n    2 3\\n    \"\n    try:\n        part0 = [n for (n, d) in G.node.items() if (d['bipartite'] == 0)]\n    except:\n        raise AttributeError('Missing node attribute `bipartite`')\n    if ((data is True) or (data is False)):\n        for n in part0:\n            for e in G.edges(n, data=data):\n                (yield delimiter.join(map(make_str, e)))\n    else:\n        for n in part0:\n            for (u, v, d) in G.edges(n, data=True):\n                e = [u, v]\n                try:\n                    e.extend((d[k] for k in data))\n                except KeyError:\n                    pass\n                (yield delimiter.join(map(make_str, e)))\n", "label": 1}
{"function": "\n\ndef test_user(self):\n    'Test the ability to retrieve a User.'\n    cassette_name = self.cassette_name('user')\n    with self.recorder.use_cassette(cassette_name):\n        sigmavirus24 = self.gh.user('sigmavirus24')\n    assert isinstance(sigmavirus24, github3.users.User)\n    try:\n        assert repr(sigmavirus24)\n    except UnicodeEncodeError:\n        self.fail('Regression caught. See PR #52. Names must be utf-8 encoded')\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('grid_type', ALL_GRID_TYPES)\ndef test_append_two_quantities_invalid(self, grid_type):\n    g = self.grid[grid_type]\n    g['density'] = []\n    g['energy'] = []\n    g['density'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    assert g['density'][0]\n    assert g['density'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['density'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert g['energy'][0]\n    assert g['energy'][1]\n    assert g['energy'][(- 1)]\n    assert g['energy'][(- 2)]\n    with pytest.raises(IndexError) as exc:\n        assert g['energy'][2]\n    assert (exc_msg(exc) == 'list index out of range')\n    with pytest.raises(ValueError) as exc:\n        g.n_dust\n    assert (exc_msg(exc) == 'Not all dust lists in the grid have the same size')\n", "label": 1}
{"function": "\n\ndef exhaust_iterator(iterator):\n    '\\n    Exhaust an iterator and return all data returned by it.\\n\\n    :type iterator: :class:`object` which implements iterator interface.\\n    :param iterator: An object which implements an iterator interface\\n                     or a File like object with read method.\\n\\n    :rtype ``str``\\n    :return Data returned by the iterator.\\n    '\n    data = b('')\n    try:\n        chunk = b(next(iterator))\n    except StopIteration:\n        chunk = b('')\n    while (len(chunk) > 0):\n        data += chunk\n        try:\n            chunk = b(next(iterator))\n        except StopIteration:\n            chunk = b('')\n    return data\n", "label": 0}
{"function": "\n\ndef get_program_logs(request):\n    logs = 'Logs for program {}:{} in server {}'.format(request.GET['group'], request.GET['program'], request.GET['server'])\n    stream = request.GET['stream']\n    assert (stream in {'stdout', 'stderr', 'applog'})\n    full_name = '{}:{}'.format(request.GET['group'], request.GET['program'])\n    if (stream == 'stdout'):\n        supervisor = _get_supervisor(request.GET['server'])\n        try:\n            (logs, _offeset, _overflow) = supervisor.supervisor.tailProcessStdoutLog(full_name, (- 100000), 100000)\n        finally:\n            supervisor('close')()\n    elif (stream == 'stderr'):\n        supervisor = _get_supervisor(request.GET['server'])\n        try:\n            (logs, _offeset, _overflow) = supervisor.supervisor.tailProcessStderrLog(full_name, (- 100000), 100000)\n        finally:\n            supervisor('close')()\n    elif (stream == 'applog'):\n        supervisor = _get_monhelper(request.GET['server'])\n        try:\n            (logs, _offeset, _overflow) = supervisor.tailApplicationLog(int(request.GET['pid']), (- 100000), 100000)\n        finally:\n            supervisor('close')()\n    else:\n        raise AssertionError\n    return HttpResponse(logs, content_type='text/plain')\n", "label": 0}
{"function": "\n\ndef update_deployment_parameters(self, plan_name, deployment_parameters):\n    'Update the deployment parameters\\n\\n        :param plan_name: The name of the plan and container name\\n        :type plan_name: str\\n        :param deployment_parameters: dictionary of deployment parameters\\n        :type deployment_parameters: dict\\n        '\n    plan = self.get_plan(plan_name)\n    deployment_params_file = 'environments/deployment_parameters.yaml'\n    if (not deployment_parameters.get('parameter_defaults')):\n        deployment_parameters = {\n            'parameter_defaults': deployment_parameters,\n        }\n    if (deployment_params_file in plan.files):\n        plan.files.pop(deployment_params_file)\n    (template, environment, files) = templates.process_plan_data(plan.files)\n    environment = templates.deep_update(environment, deployment_parameters)\n    try:\n        self.heatclient.stacks.validate(template=template, files=files, environment=environment, show_nested=True)\n    except heatexceptions.HTTPBadRequest as exc:\n        six.raise_from(exception.HeatValidationFailedError(msg=exc), exc)\n    env = yaml.safe_dump(deployment_parameters, default_flow_style=False)\n    plan.files[deployment_params_file] = {\n        'contents': env,\n        'meta': {\n            'file-type': 'temp-environment',\n        },\n    }\n    self.update_plan(plan_name, plan.files)\n", "label": 0}
{"function": "\n\ndef add_task(self, task, raise_error=False):\n    '\\n        Add task to the task queue.\\n        '\n    if self.parser_mode:\n        self.parser_result_queue.put((task, None))\n        return\n    if (self.task_queue is None):\n        raise SpiderMisuseError('You should configure task queue before adding tasks. Use `setup_queue` method.')\n    if ((task.priority is None) or (not task.priority_is_custom)):\n        task.priority = self.generate_task_priority()\n        task.priority_is_custom = False\n    else:\n        task.priority_is_custom = True\n    try:\n        if (not task.url.startswith(('http://', 'https://', 'ftp://', 'file://', 'feed://'))):\n            if (self.base_url is None):\n                msg = ('Could not resolve relative URL because base_url is not specified. Task: %s, URL: %s' % (task.name, task.url))\n                raise SpiderError(msg)\n            else:\n                warn('Class attribute `Spider::base_url` is deprecated. Use Task objects with absolute URLs')\n                task.url = urljoin(self.base_url, task.url)\n                if task.grab_config:\n                    task.grab_config['url'] = task.url\n    except Exception as ex:\n        self.stat.collect('task-with-invalid-url', task.url)\n        if raise_error:\n            raise\n        else:\n            logger.error('', exc_info=ex)\n            return False\n    self.task_queue.put(task, task.priority, schedule_time=task.schedule_time)\n    return True\n", "label": 1}
{"function": "\n\ndef commit_dirtiness_flags(self):\n    '\\n        Updates any dirtiness flags in the database.\\n        '\n    if self.domain:\n        flags_to_save = self.get_flags_to_save()\n        if should_create_flags_on_submission(self.domain):\n            assert settings.UNIT_TESTING\n            all_touched_ids = (set(flags_to_save.keys()) | self.get_clean_owner_ids())\n            to_update = {f.owner_id: f for f in OwnershipCleanlinessFlag.objects.filter(domain=self.domain, owner_id__in=list(all_touched_ids))}\n            for owner_id in all_touched_ids:\n                if (owner_id not in to_update):\n                    flag = OwnershipCleanlinessFlag(domain=self.domain, owner_id=owner_id, is_clean=True)\n                    if (owner_id in flags_to_save):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                    flag.save()\n                else:\n                    flag = to_update[owner_id]\n                    if ((owner_id in flags_to_save) and (flag.is_clean or (not flag.hint))):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                        flag.save()\n        else:\n            flags_to_update = OwnershipCleanlinessFlag.objects.filter(Q(domain=self.domain), Q(owner_id__in=flags_to_save.keys()), (Q(is_clean=True) | Q(hint__isnull=True)))\n            for flag in flags_to_update:\n                flag.is_clean = False\n                flag.hint = flags_to_save[flag.owner_id]\n                flag.save()\n", "label": 1}
{"function": "\n\ndef pagerank_numpy(G, alpha=0.85, personalization=None, weight='weight'):\n    'Return the PageRank of the nodes in the graph.\\n\\n    PageRank computes a ranking of the nodes in the graph G based on\\n    the structure of the incoming links. It was originally designed as\\n    an algorithm to rank web pages.\\n\\n    Parameters\\n    -----------\\n    G : graph\\n      A NetworkX graph\\n\\n    alpha : float, optional\\n      Damping parameter for PageRank, default=0.85\\n\\n    personalization: dict, optional\\n       The \"personalization vector\" consisting of a dictionary with a\\n       key for every graph node and nonzero personalization value for each node.\\n\\n    weight : key, optional\\n      Edge data key to use as weight.  If None weights are set to 1.\\n\\n    Returns\\n    -------\\n    pagerank : dictionary\\n       Dictionary of nodes with PageRank as value\\n\\n    Examples\\n    --------\\n    >>> G=nx.DiGraph(nx.path_graph(4))\\n    >>> pr=nx.pagerank_numpy(G,alpha=0.9)\\n\\n    Notes\\n    -----\\n    The eigenvector calculation uses NumPy\\'s interface to the LAPACK\\n    eigenvalue solvers.  This will be the fastest and most accurate\\n    for small graphs.\\n\\n    This implementation works with Multi(Di)Graphs.\\n\\n    See Also\\n    --------\\n    pagerank, pagerank_scipy, google_matrix\\n\\n    References\\n    ----------\\n    .. [1] A. Langville and C. Meyer,\\n       \"A survey of eigenvector methods of web information retrieval.\"\\n       http://citeseer.ist.psu.edu/713792.html\\n    .. [2] Page, Lawrence; Brin, Sergey; Motwani, Rajeev and Winograd, Terry,\\n       The PageRank citation ranking: Bringing order to the Web. 1999\\n       http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1999-66&format=pdf\\n    '\n    try:\n        import numpy as np\n    except ImportError:\n        raise ImportError('pagerank_numpy() requires NumPy: http://scipy.org/')\n    if (len(G) == 0):\n        return {\n            \n        }\n    if (personalization is None):\n        nodelist = G.nodes()\n    else:\n        nodelist = personalization.keys()\n    M = google_matrix(G, alpha, personalization=personalization, nodelist=nodelist, weight=weight)\n    (eigenvalues, eigenvectors) = np.linalg.eig(M.T)\n    ind = eigenvalues.argsort()\n    largest = np.array(eigenvectors[:, ind[(- 1)]]).flatten().real\n    norm = float(largest.sum())\n    centrality = dict(zip(nodelist, map(float, (largest / norm))))\n    return centrality\n", "label": 0}
{"function": "\n\ndef test_as_dict(self):\n    sproc = get_test_subprocess()\n    p = psutil.Process(sproc.pid)\n    d = p.as_dict()\n    try:\n        import json\n    except ImportError:\n        pass\n    else:\n        json.dumps(d)\n    d = p.as_dict(attrs=['exe', 'name'])\n    self.assertEqual(sorted(d.keys()), ['exe', 'name'])\n    p = psutil.Process(min(psutil.get_pid_list()))\n    d = p.as_dict(attrs=['get_connections'], ad_value='foo')\n    if (not isinstance(d['connections'], list)):\n        self.assertEqual(d['connections'], 'foo')\n", "label": 0}
{"function": "\n\ndef _compute_generator_info(self):\n    \"\\n        Compute the generator's state variables as the union of live variables\\n        at all yield points.\\n        \"\n    gi = self.generator_info\n    for yp in gi.get_yield_points():\n        live_vars = set(self.block_entry_vars[yp.block])\n        weak_live_vars = set()\n        stmts = iter(yp.block.body)\n        for stmt in stmts:\n            if isinstance(stmt, ir.Assign):\n                if (stmt.value is yp.inst):\n                    break\n                live_vars.add(stmt.target.name)\n            elif isinstance(stmt, ir.Del):\n                live_vars.remove(stmt.value)\n        else:\n            assert 0, \"couldn't find yield point\"\n        for stmt in stmts:\n            if isinstance(stmt, ir.Del):\n                name = stmt.value\n                if (name in live_vars):\n                    live_vars.remove(name)\n                    weak_live_vars.add(name)\n            else:\n                break\n        yp.live_vars = live_vars\n        yp.weak_live_vars = weak_live_vars\n    st = set()\n    for yp in gi.get_yield_points():\n        st |= yp.live_vars\n        st |= yp.weak_live_vars\n    gi.state_vars = sorted(st)\n", "label": 1}
{"function": "\n\ndef __init__(self, source, filename=None, lineno=(- 1), lookup='strict', xform=None):\n    'Create the code object, either from a string, or from an AST node.\\n        \\n        :param source: either a string containing the source code, or an AST\\n                       node\\n        :param filename: the (preferably absolute) name of the file containing\\n                         the code\\n        :param lineno: the number of the line on which the code was found\\n        :param lookup: the lookup class that defines how variables are looked\\n                       up in the context; can be either \"strict\" (the default),\\n                       \"lenient\", or a custom lookup class\\n        :param xform: the AST transformer that should be applied to the code;\\n                      if `None`, the appropriate transformation is chosen\\n                      depending on the mode\\n        '\n    if isinstance(source, str):\n        self.source = source\n        node = _parse(source, mode=self.mode)\n    else:\n        assert isinstance(source, _ast.AST), ('Expected string or AST node, but got %r' % source)\n        self.source = '?'\n        if (self.mode == 'eval'):\n            node = _ast.Expression()\n            node.body = source\n        else:\n            node = _ast.Module()\n            node.body = [source]\n    self.ast = node\n    self.code = _compile(node, self.source, mode=self.mode, filename=filename, lineno=lineno, xform=xform)\n    if (lookup is None):\n        lookup = LenientLookup\n    elif isinstance(lookup, str):\n        lookup = {\n            'lenient': LenientLookup,\n            'strict': StrictLookup,\n        }[lookup]\n    self._globals = lookup.globals\n", "label": 0}
{"function": "\n\ndef transform_Assign(self, stmt):\n    lhs = stmt.lhs\n    lhs_class = lhs.__class__\n    rhs = self.transform_expr(stmt.rhs)\n    if (lhs_class is Tuple):\n        for (i, _) in enumerate(lhs.type.elt_types):\n            lhs_i = self.tuple_proj(lhs, i)\n            rhs_i = self.tuple_proj(rhs, i)\n            assert (lhs_i.__class__ not in (ArrayView, Tuple))\n            self.assign(lhs_i, rhs_i)\n        return None\n    elif (lhs_class is Index):\n        lhs = self.transform_Index(lhs)\n        if (lhs.__class__ is ArrayView):\n            copy_loop = self.array_copy(src=rhs, dest=lhs, return_stmt=True)\n            copy_loop = self.transform_stmt(copy_loop)\n            return copy_loop\n    elif ((lhs_class is Var) and (stmt.rhs.__class__ in (Slice, Struct, ArrayView, Tuple))):\n        self.bindings[lhs.name] = rhs\n    return Assign(lhs, rhs)\n", "label": 1}
{"function": "\n\ndef pil_from_ndarray(ndarray):\n    '\\n    Converts an ndarray to a PIL image.\\n\\n    Parameters\\n    ----------\\n    ndarray : ndarray\\n        An ndarray containing an image.\\n\\n    Returns\\n    -------\\n    pil : PIL Image\\n        A PIL Image containing the image.\\n    '\n    try:\n        if ((ndarray.dtype == 'float32') or (ndarray.dtype == 'float64')):\n            assert (ndarray.min() >= 0.0)\n            assert (ndarray.max() <= 1.0)\n            ndarray = np.cast['uint8']((ndarray * 255))\n            if ((len(ndarray.shape) == 3) and (ndarray.shape[2] == 1)):\n                ndarray = ndarray[:, :, 0]\n        ensure_Image()\n        rval = Image.fromarray(ndarray)\n        return rval\n    except Exception as e:\n        logger.exception('original exception: ')\n        logger.exception(e)\n        logger.exception('ndarray.dtype: {0}'.format(ndarray.dtype))\n        logger.exception('ndarray.shape: {0}'.format(ndarray.shape))\n        raise\n    assert False\n", "label": 1}
{"function": "\n\ndef test_SeqExprOp():\n    form = SeqFormula((n ** 2), (n, 0, 10))\n    per = SeqPer((1, 2, 3), (m, 5, 10))\n    s = SeqExprOp(form, per)\n    assert (s.gen == ((n ** 2), (1, 2, 3)))\n    assert (s.interval == Interval(5, 10))\n    assert (s.start == 5)\n    assert (s.stop == 10)\n    assert (s.length == 6)\n    assert (s.variables == (n, m))\n", "label": 0}
{"function": "\n\ndef it_gathers_package_image_parts_after_unmarshalling(self):\n    package = Package.open(docx_path('having-images'))\n    image_parts = package.image_parts\n    assert (len(image_parts) == 3)\n    for image_part in image_parts:\n        assert isinstance(image_part, ImagePart)\n", "label": 0}
{"function": "\n\ndef cleanup(self):\n    self.logit('Cleaning up...')\n    for item in self.cleanup_items:\n        try:\n            item.delete()\n            self.logit(' - Deleting:', end=' ')\n            try:\n                self.logit(item.name)\n            except AttributeError:\n                self.logit(item)\n        except exc.NotFound:\n            pass\n        except Exception as e:\n            self.logit((\"Could not delete '%s': %s\" % (item, e)))\n", "label": 0}
{"function": "\n\ndef save(self, **kwargs):\n    if ('encode_attachments' not in kwargs):\n        kwargs['encode_attachments'] = False\n    RETRIES = 10\n    SLEEP = 0.5\n    tries = 0\n    while True:\n        try:\n            return super(XFormInstance, self).save(**kwargs)\n        except PreconditionFailed:\n            if (tries == 0):\n                logging.error(('doc %s got a precondition failed' % self._id))\n            if (tries < RETRIES):\n                tries += 1\n                time.sleep(SLEEP)\n            else:\n                raise\n", "label": 0}
{"function": "\n\ndef _new_actor(self, actor_type, actor_id=None, credentials=None):\n    \"Return a 'bare' actor of actor_type, raises an exception on failure.\"\n    if (credentials is not None):\n        sec = Security()\n        sec.set_principal(credentials)\n        sec.authenticate_principal()\n    else:\n        sec = None\n    (found, is_primitive, class_) = ActorStore(security=sec).lookup(actor_type)\n    if (not found):\n        _log.analyze(self.node.id, '+ NOT FOUND CREATE SHADOW ACTOR', {\n            'class': class_,\n        })\n        found = True\n        is_primitive = True\n        class_ = ShadowActor\n    if ((not found) or (not is_primitive)):\n        _log.error(('Requested actor %s is not available' % actor_type))\n        raise Exception('ERROR_NOT_FOUND')\n    try:\n        a = class_(actor_type, actor_id=actor_id)\n    except Exception as e:\n        _log.exception('')\n        _log.error((\"The actor %s(%s) can't be instantiated.\" % (actor_type, class_.__init__)))\n        raise e\n    try:\n        a.set_credentials(credentials, security=sec)\n        a._calvinsys = self.node.calvinsys()\n        a.check_requirements()\n    except Exception as e:\n        _log.exception('Catched new from state')\n        _log.analyze(self.node.id, '+ FAILED REQS CREATE SHADOW ACTOR', {\n            'class': class_,\n        })\n        a = ShadowActor(actor_type, actor_id=actor_id)\n        a.set_credentials(credentials, security=sec)\n        a._calvinsys = self.node.calvinsys()\n    return a\n", "label": 0}
{"function": "\n\ndef WordsTableCreate(maxDictionaryLength=1000000, maxMemoryStorage=20000000):\n    \"\\n    This function reads already-tokenized words from sys.stdin,\\n    and uses them to count all the words in the document.\\n\\n    The only real challenge here is that there may be more unique words than can fit\\n    in memory, so we need some kind of buffering on disk.\\n\\n    It does this by maintaining a dictionary in memory, and periodically \\n    writing the least used portions of that to disk; once it's read everything,\\n    it writes everything to disk, sorts the list of all words so that \\n    \"\n    database = open('.bookworm/texts/wordlist/raw.txt', 'w')\n    start_time = timeit.default_timer()\n    n = 1\n    keepThreshold = 2\n    wordcounts = dict()\n    for row in sys.stdin:\n        for item in row.split(' '):\n            n += 1\n            if ((n % 100000000) == 0):\n                elapsed = (timeit.default_timer() - start_time)\n                logging.info((((((str((float(len(wordcounts)) / 1000000)) + ' million distinct entries at ') + str((float(n) / 1000000000))) + ' billion words---') + str(elapsed)) + ' seconds since last print'))\n                start_time = timeit.default_timer()\n            item = item.rstrip('\\n')\n            try:\n                wordcounts[item] += 1\n            except KeyError:\n                wordcounts[item] = 1\n            while (len(wordcounts) > maxMemoryStorage):\n                logging.info((('exporting to disk at ' + str((float(len(wordcounts)) / 1000000))) + ' million words'))\n                wordcounts = exportToDisk(wordcounts, diskFile=database, keepThreshold=keepThreshold)\n                logging.info(((\"after export, it's \" + str((float(len(wordcounts)) / 1000000))) + ' million words'))\n                if (len(wordcounts) > (0.8 * float(maxMemoryStorage))):\n                    keepThreshold = (keepThreshold * 2)\n                    logging.info(('upping the keep threshold to ' + str(keepThreshold)))\n    nothing = exportToDisk(wordcounts, diskFile=database, keepThreshold=float('inf'))\n    database.close()\n    sortWordlist(maxDictionaryLength=maxDictionaryLength)\n", "label": 0}
{"function": "\n\ndef bayesdb_generator_metamodel(bdb, id):\n    'Return the metamodel of the generator with id `id`.'\n    sql = 'SELECT metamodel FROM bayesdb_generator WHERE id = ?'\n    cursor = bdb.sql_execute(sql, (id,))\n    try:\n        row = cursor.next()\n    except StopIteration:\n        raise ValueError(('No such generator: %s' % (repr(id),)))\n    else:\n        if (row[0] not in bdb.metamodels):\n            name = bayesdb_generator_name(bdb, id)\n            raise ValueError(('Metamodel of generator %s not registered: %s' % (repr(name), repr(row[0]))))\n        return bdb.metamodels[row[0]]\n", "label": 0}
{"function": "\n\ndef test_fermionoperator():\n    c = FermionOp('c')\n    d = FermionOp('d')\n    assert isinstance(c, FermionOp)\n    assert isinstance(Dagger(c), FermionOp)\n    assert c.is_annihilation\n    assert (not Dagger(c).is_annihilation)\n    assert (FermionOp('c') == FermionOp('c'))\n    assert (FermionOp('c') != FermionOp('d'))\n    assert (FermionOp('c', True) != FermionOp('c', False))\n    assert (AntiCommutator(c, Dagger(c)).doit() == 1)\n    assert (AntiCommutator(c, Dagger(d)).doit() == ((c * Dagger(d)) + (Dagger(d) * c)))\n", "label": 1}
{"function": "\n\ndef _fetch(self, model_cls, query=None, sort=None):\n    'Fetch the objects of type `model_cls` matching the given\\n        query. The query may be given as a string, string sequence, a\\n        Query object, or None (to fetch everything). `sort` is an\\n        `Sort` object.\\n        '\n    query = (query or TrueQuery())\n    sort = (sort or NullSort())\n    (where, subvals) = query.clause()\n    order_by = sort.order_clause()\n    sql = 'SELECT * FROM {0} WHERE {1} {2}'.format(model_cls._table, (where or '1'), ('ORDER BY {0}'.format(order_by) if order_by else ''))\n    with self.transaction() as tx:\n        rows = tx.query(sql, subvals)\n    return Results(model_cls, rows, self, (None if where else query), (sort if sort.is_slow() else None))\n", "label": 0}
{"function": "\n\ndef test_getting_user_info_by_authorized_user(flask_app_client, regular_user, admin_user):\n    with flask_app_client.login(admin_user, auth_scopes=('users:read',)):\n        response = flask_app_client.get(('/api/v1/users/%d' % regular_user.id))\n    assert (response.status_code == 200)\n    assert (response.content_type == 'application/json')\n    assert isinstance(response.json, dict)\n    assert (set(response.json.keys()) >= {'id', 'username'})\n    assert ('password' not in response.json.keys())\n", "label": 0}
{"function": "\n\ndef test_mapping(self):\n    doc_name = self.indexer.get_doctype_name()\n    assert doc_name\n    mapping_properties = self.indexer.get_mapping()[doc_name]['properties']\n    assert ('summary' not in mapping_properties)\n    assert ('boost' in mapping_properties)\n", "label": 0}
{"function": "\n\ndef test_percentile_algorithm_extremes():\n    ' Unsorted so it is expected to sort it for us. '\n    snapshot = PercentileSnapshot(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 800, 768, 657, 700, 867)\n    print('0.01', snapshot.percentile(0.01))\n    print('10th', snapshot.percentile(10))\n    print('Median', snapshot.percentile(50))\n    print('75th', snapshot.percentile(75))\n    print('90th', snapshot.percentile(90))\n    print('99th', snapshot.percentile(99))\n    print('99.5th', snapshot.percentile(99.5))\n    print('99.99', snapshot.percentile(99.99))\n    assert (snapshot.percentile(50) == 2)\n    assert (snapshot.percentile(10) == 2)\n    assert (snapshot.percentile(75) == 2)\n    if (snapshot.percentile(95) < 600):\n        msg = 'We expect 90th to be over 600 to show the extremes but got: {}'\n        pytest.fail(msg.format(snapshot.percentile(95)))\n    if (snapshot.percentile(99) < 600):\n        msg = 'We expect 99th to be over 600 to show the extremes but got: {}'\n        pytest.fail(msg.format(snapshot.percentile(99)))\n", "label": 0}
{"function": "\n\ndef convert(self, format=None, geom=None):\n    imgpath = self.image.path\n    ext = (format or os.path.splitext(imgpath)[(- 1)][1:])\n    ext = os.path.splitext(ext)[0]\n    driver = greenwich.driver_for_path(('base.%s' % ext))\n    if ((not geom) and imgpath.endswith(ext)):\n        return\n    memio = MemFileIO()\n    if geom:\n        with greenwich.Raster(imgpath) as r:\n            with r.clip(geom) as clipped:\n                clipped.save(memio, driver)\n    else:\n        driver.copy(imgpath, memio.name)\n    self.image.file = memio\n", "label": 0}
{"function": "\n\ndef array_view(self, data, shape, strides, offset, nelts):\n    'Helper function used by multiple array-related transformations'\n    data = self.assign_name(self.transform_expr(data), 'data_ptr')\n    data_t = data.type\n    assert isinstance(data_t, PtrT), ('Data field of array must be a pointer, got %s' % data_t)\n    shape = self.assign_name(self.transform_expr(shape), 'shape')\n    shape_t = shape.type\n    assert isinstance(shape_t, TupleT), ('Shape of array must be a tuple, got: %s' % shape_t)\n    strides = self.assign_name(self.transform_expr(strides), 'strides')\n    strides_t = strides.type\n    assert isinstance(strides_t, TupleT), ('Strides of array must be a tuple, got: %s' % strides_t)\n    rank = len(shape_t.elt_types)\n    strides_rank = len(strides_t.elt_types)\n    assert (rank == strides_rank), ('Shape and strides must be of same length, but got %d and %d' % (rank, strides_rank))\n    array_t = make_array_type(data_t.elt_type, rank)\n    return Struct([data, shape, strides, offset, nelts], type=array_t)\n", "label": 0}
{"function": "\n\ndef get_geometry(self, value):\n    '\\n        Retrieves the geometry, setting the default SRID from the given\\n        lookup parameters.\\n        '\n    if isinstance(value, (tuple, list)):\n        geom = value[0]\n    else:\n        geom = value\n    if isinstance(geom, SpatialBackend.Geometry):\n        pass\n    elif isinstance(geom, basestring):\n        try:\n            geom = SpatialBackend.Geometry(geom)\n        except SpatialBackend.GeometryException:\n            raise ValueError(('Could not create geometry from lookup value: %s' % str(value)))\n    else:\n        raise TypeError(('Cannot use parameter of `%s` type as lookup parameter.' % type(value)))\n    geom.srid = self.get_srid(geom)\n    return geom\n", "label": 0}
{"function": "\n\ndef _check_shuffle(self, func, ptr):\n    '\\n        Check a shuffle()-like function for 1D arrays.\\n        '\n    a = np.arange(20)\n    if (sys.version_info >= (3,)):\n        r = self._follow_cpython(ptr)\n        for i in range(3):\n            got = a.copy()\n            expected = a.copy()\n            func(got)\n            r.shuffle(expected)\n            self.assertTrue(np.all((got == expected)), (got, expected))\n    else:\n        for i in range(3):\n            b = a.copy()\n            func(b)\n            self.assertFalse(np.all((a == b)))\n            self.assertEqual(sorted(a), sorted(b))\n            a = b\n    b = a.copy()\n    func(memoryview(b))\n    self.assertFalse(np.all((a == b)))\n    self.assertEqual(sorted(a), sorted(b))\n    with self.assertTypingError():\n        func(memoryview(b'xyz'))\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    r = []\n    try:\n        for el in self:\n            c = el.get('class')\n            c = ((c and ('.' + '.'.join(c.split(' ')))) or '')\n            id = el.get('id')\n            id = ((id and ('#' + id)) or '')\n            r.append(('<%s%s%s>' % (el.tag, id, c)))\n        return (('[' + ', '.join(r)) + ']')\n    except AttributeError:\n        if PY3k:\n            return list.__repr__(self)\n        else:\n            for el in self:\n                if isinstance(el, unicode):\n                    r.append(el.encode('utf-8'))\n                else:\n                    r.append(el)\n            return repr(r)\n", "label": 1}
{"function": "\n\ndef load_content(content):\n    if ((content is None) or (content == '')):\n        return dict()\n    try:\n        data = yaml.safe_load(content)\n    except Exception:\n        data = json.loads(content)\n    return data\n", "label": 0}
{"function": "\n\ndef _configure_port_entries(self, port, vlan_id, device_id, host_id, vni, is_provider_vlan):\n    'Create a nexus switch entry.\\n\\n        if needed, create a VLAN in the appropriate switch or port and\\n        configure the appropriate interfaces for this VLAN.\\n\\n        Called during update postcommit port event.\\n        '\n    connections = self._get_active_port_connections(port, host_id)\n    vlan_already_created = []\n    starttime = time.time()\n    for (switch_ip, intf_type, nexus_port, is_native) in connections:\n        all_bindings = nxos_db.get_nexusvlan_binding(vlan_id, switch_ip)\n        previous_bindings = [row for row in all_bindings if (row.instance_id != device_id)]\n        if (previous_bindings and (switch_ip in vlan_already_created)):\n            duplicate_type = const.DUPLICATE_VLAN\n        else:\n            vlan_already_created.append(switch_ip)\n            duplicate_type = const.NO_DUPLICATE\n        port_starttime = time.time()\n        try:\n            self._configure_port_binding(is_provider_vlan, duplicate_type, is_native, switch_ip, vlan_id, intf_type, nexus_port, vni)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                self.driver.capture_and_print_timeshot(port_starttime, 'port_configerr', switch=switch_ip)\n                self.driver.capture_and_print_timeshot(starttime, 'configerr', switch=switch_ip)\n        self.driver.capture_and_print_timeshot(port_starttime, 'port_config', switch=switch_ip)\n    self.driver.capture_and_print_timeshot(starttime, 'config')\n", "label": 0}
{"function": "\n\ndef __init__(self, Events=None, **Options):\n    'Initializes the object.\\n\\n        :Parameters:\\n          Events\\n            An optional object with event handlers. See `Skype4Py.utils.EventHandlingBase`\\n            for more information on events.\\n          Options\\n            Additional options for low-level API handler. See the `Skype4Py.api`\\n            subpackage for supported options. Available options may depend on the\\n            current platform. Note that the current platform can be queried using\\n            `Skype4Py.platform` variable.\\n        '\n    self._Logger = logging.getLogger('Skype4Py.skype.Skype')\n    self._Logger.info('object created')\n    EventHandlingBase.__init__(self)\n    if Events:\n        self._SetEventHandlerObject(Events)\n    try:\n        self._Api = Options.pop('Api')\n        if Options:\n            raise TypeError('No options supported with custom API objects.')\n    except KeyError:\n        self._Api = SkypeAPI(Options)\n    self._Api.set_notifier(APINotifier(self))\n    Cached._CreateOwner(self)\n    self._Cache = True\n    self.ResetCache()\n    from api import DEFAULT_TIMEOUT\n    self._Timeout = DEFAULT_TIMEOUT\n    self._Convert = Conversion(self)\n    self._Client = Client(self)\n    self._Settings = Settings(self)\n    self._Profile = Profile(self)\n", "label": 0}
{"function": "\n\ndef build_table(self):\n    for domain in Domain.get_all():\n        malt_rows_to_save = []\n        logger.info('Building MALT for {}'.format(domain.name))\n        all_users_by_id = {user._id: user for user in domain.all_users()}\n        for monthspan in self.monthspan_list:\n            try:\n                malt_rows_to_save.extend(self._get_malt_row_dicts(domain.name, monthspan, all_users_by_id))\n            except Exception as ex:\n                logger.error('Failed to get rows for domain {name}. Exception is {ex}'.format(name=domain.name, ex=str(ex)), exc_info=True)\n        self._save_to_db(malt_rows_to_save, domain._id)\n", "label": 0}
{"function": "\n\ndef show(self, req, id):\n    s = self.os_helper.get_server(req, id)\n    flavor = self.os_helper.get_flavor(req, s['flavor']['id'])\n    res_tpl = templates.OpenStackResourceTemplate(flavor['id'], flavor['name'], flavor['vcpus'], flavor['ram'], flavor['disk'])\n    img_id = s['image']['id']\n    try:\n        image = self.os_helper.get_image(req, img_id)\n    except webob.exc.HTTPNotFound:\n        image = {\n            'id': img_id,\n            'name': (\"None (Image with ID '%s' not found)\" % img_id),\n        }\n    os_tpl = templates.OpenStackOSTemplate(image['id'], image['name'])\n    comp = compute.ComputeResource(title=s['name'], id=s['id'], cores=flavor['vcpus'], hostname=s['name'], memory=flavor['ram'], state=helpers.vm_state(s['status']), mixins=[os_tpl, res_tpl])\n    vols = self.os_helper.get_server_volumes_link(req, s['id'])\n    for v in vols:\n        st = storage.StorageResource(title='storage', id=v['volumeId'])\n        comp.add_link(storage_link.StorageLink(comp, st, deviceid=v['device']))\n    addresses = s.get('addresses', {\n        \n    })\n    if addresses:\n        for addr_set in addresses.values():\n            for addr in addr_set:\n                comp.add_link(_create_network_link(addr, comp))\n    return [comp]\n", "label": 0}
{"function": "\n\ndef assert_same(self, val1, val2):\n    try:\n        self.assertEqual(val1, val2)\n    except AssertionError:\n        if (val1 is pd.NaT):\n            self.assertTrue((val2 is pd.NaT))\n        elif np.isnan(val1):\n            self.assertTrue(np.isnan(val2))\n        else:\n            raise\n", "label": 0}
{"function": "\n\ndef __init__(self, maxresp=0, address='::', s_flg=0, qrv=2, qqic=0, num=0, srcs=None):\n    super(mldv2_query, self).__init__(maxresp, address)\n    self.s_flg = s_flg\n    self.qrv = qrv\n    self.qqic = qqic\n    self.num = num\n    srcs = (srcs or [])\n    assert isinstance(srcs, list)\n    for src in srcs:\n        assert isinstance(src, str)\n    self.srcs = srcs\n", "label": 0}
{"function": "\n\ndef test_users_mocked(self):\n    with mock.patch('psutil._pslinux.cext.users', return_value=[('giampaolo', 'pts/2', ':0', 1436573184.0, True)]) as m:\n        self.assertEqual(psutil.users()[0].host, 'localhost')\n        assert m.called\n    with mock.patch('psutil._pslinux.cext.users', return_value=[('giampaolo', 'pts/2', ':0.0', 1436573184.0, True)]) as m:\n        self.assertEqual(psutil.users()[0].host, 'localhost')\n        assert m.called\n    with mock.patch('psutil._pslinux.cext.users', return_value=[('giampaolo', 'pts/2', 'foo', 1436573184.0, True)]) as m:\n        self.assertEqual(psutil.users()[0].host, 'foo')\n        assert m.called\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    for (i, arg) in enumerate(self.macro.args):\n        try:\n            fe = self.fe_args[i]\n            context[arg] = fe.resolve(context)\n        except IndexError:\n            context[arg] = ''\n    for (name, default) in six.iteritems(self.macro.kwargs):\n        if (name in self.fe_kwargs):\n            context[name] = self.fe_kwargs[name].resolve(context)\n        else:\n            context[name] = FilterExpression(default, self.macro.parser).resolve(context)\n    return self.macro.nodelist.render(context)\n", "label": 0}
{"function": "\n\ndef load_middleware(self):\n    '\\n        Populate middleware lists from settings.MIDDLEWARE_CLASSES.\\n\\n        Must be called after the environment is fixed (see __call__ in subclasses).\\n        '\n    self._view_middleware = []\n    self._template_response_middleware = []\n    self._response_middleware = []\n    self._exception_middleware = []\n    request_middleware = []\n    for middleware_path in settings.MIDDLEWARE_CLASSES:\n        mw_class = import_string(middleware_path)\n        try:\n            mw_instance = mw_class()\n        except MiddlewareNotUsed as exc:\n            if settings.DEBUG:\n                if six.text_type(exc):\n                    logger.debug('MiddlewareNotUsed(%r): %s', middleware_path, exc)\n                else:\n                    logger.debug('MiddlewareNotUsed: %r', middleware_path)\n            continue\n        if hasattr(mw_instance, 'process_request'):\n            request_middleware.append(mw_instance.process_request)\n        if hasattr(mw_instance, 'process_view'):\n            self._view_middleware.append(mw_instance.process_view)\n        if hasattr(mw_instance, 'process_template_response'):\n            self._template_response_middleware.insert(0, mw_instance.process_template_response)\n        if hasattr(mw_instance, 'process_response'):\n            self._response_middleware.insert(0, mw_instance.process_response)\n        if hasattr(mw_instance, 'process_exception'):\n            self._exception_middleware.insert(0, mw_instance.process_exception)\n    self._request_middleware = request_middleware\n", "label": 1}
{"function": "\n\n@attr('matplotlib')\ndef test_simple_line():\n    (fig, ax) = plt.subplots()\n    ax.plot(D['x1'], D['y1'], label='simple')\n    renderer = run_fig(fig)\n    for (data_no, data_dict) in enumerate(renderer.plotly_fig['data']):\n        (equivalent, msg) = compare_dict(data_dict, SIMPLE_LINE['data'][data_no])\n        assert equivalent, msg\n    (equivalent, msg) = compare_dict(renderer.plotly_fig['layout'], SIMPLE_LINE['layout'])\n    assert equivalent, msg\n", "label": 0}
{"function": "\n\ndef salvage_broken_user_settings_document(document):\n    if ((not document['access_token']) or (not document['dropbox_id'])):\n        return False\n    if ((not document['owner']) or (not User.load(document['owner']).is_active)):\n        return False\n    if document['deleted']:\n        return False\n    if ((not document.get('dropbox_info')) or (not document['dropbox_info']['display_name'])):\n        logger.info('Attempting dropbox_info population for document (id:{0})'.format(document['_id']))\n        client = DropboxClient(document['access_token'])\n        document['dropbox_info'] = {\n            \n        }\n        try:\n            database['dropboxusersettings'].find_and_modify({\n                '_id': document['_id'],\n            }, {\n                '$set': {\n                    'dropbox_info': client.account_info(),\n                },\n            })\n        except Exception:\n            return True\n        else:\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef test_loader_channels():\n    (n_samples_trace, n_channels) = (1000, 10)\n    n_samples_waveforms = 20\n    traces = artificial_traces(n_samples_trace, n_channels)\n    loader = WaveformLoader(traces, n_samples_waveforms=n_samples_waveforms)\n    loader.traces = traces\n    channels = [2, 5, 7]\n    loader.channels = channels\n    assert (loader.channels == channels)\n    assert (loader[500].shape == (1, n_samples_waveforms, 3))\n    assert (loader[[500, 501, 600, 300]].shape == (4, n_samples_waveforms, 3))\n    assert (loader[3].shape == (1, n_samples_waveforms, 3))\n    assert (loader[995].shape == (1, n_samples_waveforms, 3))\n    with raises(NotImplementedError):\n        loader[500:510]\n", "label": 0}
{"function": "\n\ndef layer_test(layer_cls, kwargs={\n    \n}, input_shape=None, input_dtype=None, input_data=None, expected_output=None, expected_output_dtype=None):\n    'Test routine for a layer with a single input tensor\\n    and single output tensor.\\n    '\n    if (input_data is None):\n        assert input_shape\n        if (not input_dtype):\n            input_dtype = K.floatx()\n        input_data = (10 * np.random.random(input_shape)).astype(input_dtype)\n    elif (input_shape is None):\n        input_shape = input_data.shape\n    if (expected_output_dtype is None):\n        expected_output_dtype = input_dtype\n    layer = layer_cls(**kwargs)\n    weights = layer.get_weights()\n    layer.set_weights(weights)\n    if ('weights' in inspect.getargspec(layer_cls.__init__)):\n        kwargs['weights'] = weights\n        layer = layer_cls(**kwargs)\n    x = Input(shape=input_shape[1:], dtype=input_dtype)\n    y = layer(x)\n    assert (K.dtype(y) == expected_output_dtype)\n    model = Model(input=x, output=y)\n    model.compile('rmsprop', 'mse')\n    expected_output_shape = layer.get_output_shape_for(input_shape)\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    assert (expected_output_shape == actual_output_shape)\n    if (expected_output is not None):\n        assert_allclose(actual_output, expected_output, rtol=0.001)\n    model_config = model.get_config()\n    model = Model.from_config(model_config)\n    model.compile('rmsprop', 'mse')\n    layer_config = layer.get_config()\n    layer_config['batch_input_shape'] = input_shape\n    layer = layer.__class__.from_config(layer_config)\n    model = Sequential()\n    model.add(layer)\n    model.compile('rmsprop', 'mse')\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    assert (expected_output_shape == actual_output_shape)\n    if (expected_output is not None):\n        assert_allclose(actual_output, expected_output, rtol=0.001)\n    json_model = model.to_json()\n    model = model_from_json(json_model)\n    return actual_output\n", "label": 1}
{"function": "\n\ndef finalize(genomes, env):\n    'Provide symlinks back to reference genomes so tophat avoids generating FASTA genomes.\\n    '\n    genome_dir = os.path.join(env.data_files, 'genomes')\n    for (orgname, gid, manager) in genomes:\n        org_dir = os.path.join(genome_dir, orgname)\n        for aligner in ['bowtie', 'bowtie2']:\n            aligner_dir = os.path.join(org_dir, gid, aligner)\n            if env.safe_exists(aligner_dir):\n                with cd(aligner_dir):\n                    for ext in ['', '.fai']:\n                        orig_seq = os.path.join(os.pardir, 'seq', ('%s.fa%s' % (gid, ext)))\n                        if (env.safe_exists(orig_seq) and (not env.safe_exists(os.path.basename(orig_seq)))):\n                            env.safe_run(('ln -sf %s' % orig_seq))\n", "label": 0}
{"function": "\n\n@login_required\ndef profileupdaterequest_details(request, request_id):\n    update_request = get_object_or_404(ProfileUpdateRequest, pk=request_id)\n    person_selected = False\n    person = None\n    form = None\n    try:\n        person = Person.objects.get(email=update_request.email)\n    except Person.DoesNotExist:\n        try:\n            person = Person.objects.get(personal=update_request.personal, family=update_request.family)\n        except (Person.DoesNotExist, Person.MultipleObjectsReturned):\n            try:\n                form = PersonLookupForm(request.GET)\n                person = Person.objects.get(pk=int(request.GET['person_1']))\n                person_selected = True\n            except KeyError:\n                person = None\n                form = PersonLookupForm()\n            except (ValueError, Person.DoesNotExist):\n                person = None\n    if person:\n        person.has_instructor_badge = Award.objects.filter(badge__in=Badge.objects.instructor_badges(), person=person).exists()\n    try:\n        airport = Airport.objects.get(iata=update_request.airport_iata)\n    except Airport.DoesNotExist:\n        airport = None\n    context = {\n        'title': 'Instructor profile update request #{}'.format(update_request.pk),\n        'new': update_request,\n        'old': person,\n        'person_form': form,\n        'person_selected': person_selected,\n        'form_helper': bootstrap_helper_get,\n        'airport': airport,\n    }\n    return render(request, 'workshops/profileupdaterequest.html', context)\n", "label": 0}
{"function": "\n\ndef setup(self, maps, slices=None):\n    \"\\n            Sets up the maps and slices for the PropertyMap\\n\\n\\n            :param list maps: [('sigma', sigmaMap), ('mu', muMap), ...]\\n            :param list slices: [('sigma', slice(0,nP)), ('mu', [1,2,5,6]), ...]\\n\\n        \"\n    assert np.all([((type(m) is tuple) and (len(m) == 2) and (type(m[0]) is str) and (m[0] in self._properties) and isinstance(m[1], Maps.IdentityMap)) for m in maps]), ('Use signature: [%s]' % ', '.join([(\"('%s', %sMap)\" % (p, p)) for p in self._properties]))\n    if (slices is None):\n        slices = dict()\n    else:\n        assert np.all([((s in self._properties) and ((type(slices[s]) in [slice, list]) or isinstance(slices[s], np.ndarray))) for s in slices]), 'Slices must be for each property'\n    self.clearMaps()\n    nP = 0\n    for (name, mapping) in maps:\n        setattr(self, ('%sMap' % name), mapping)\n        setattr(self, ('%sIndex' % name), slices.get(name, slice(nP, (nP + mapping.nP))))\n        nP += mapping.nP\n    self.nP = nP\n", "label": 0}
{"function": "\n\ndef get_comment_app():\n    '\\n    Get the comment app (i.e. \"django.contrib.comments\") as defined in the settings\\n    '\n    comments_app = get_comment_app_name()\n    if (comments_app not in settings.INSTALLED_APPS):\n        raise ImproperlyConfigured(('The COMMENTS_APP (%r) must be in INSTALLED_APPS' % settings.COMMENTS_APP))\n    try:\n        package = __import__(comments_app, '', '', [''])\n    except ImportError:\n        raise ImproperlyConfigured('The COMMENTS_APP setting refers to a non-existing package.')\n    for attribute in REQUIRED_COMMENTS_APP_ATTRIBUTES:\n        if (not hasattr(package, attribute)):\n            raise ImproperlyConfigured(('The COMMENTS_APP package %r does not define the (required) %r function' % (package, attribute)))\n    return package\n", "label": 0}
{"function": "\n\ndef flush(self, debug=False):\n    '\\n        Pump until there is no more input or output.\\n\\n        Returns whether any data was moved.\\n        '\n    result = False\n    for x in range(1000):\n        if self.pump(debug):\n            result = True\n        else:\n            break\n    else:\n        assert 0, 'Too long'\n    return result\n", "label": 0}
{"function": "\n\n@SubredditController.register(Command('REFRESH'))\ndef refresh_content(self, order=None, name=None):\n    'Re-download all submissions and reset the page index'\n    order = (order or self.content.order)\n    name = (name or self.content.name)\n    if (order == 'ignore'):\n        order = None\n    with self.term.loader('Refreshing page'):\n        self.content = SubredditContent.from_name(self.reddit, name, self.term.loader, order=order)\n    if (not self.term.loader.exception):\n        self.nav = Navigator(self.content.get)\n", "label": 0}
{"function": "\n\ndef test_tb_option(self, testdir, option):\n    testdir.makepyfile('\\n            import pytest\\n            def g():\\n                raise IndexError\\n            def test_func():\\n                print (6*7)\\n                g()  # --calling--\\n        ')\n    for tbopt in ['long', 'short', 'no']:\n        print(('testing --tb=%s...' % tbopt))\n        result = testdir.runpytest(('--tb=%s' % tbopt))\n        s = result.stdout.str()\n        if (tbopt == 'long'):\n            assert ('print (6*7)' in s)\n        else:\n            assert ('print (6*7)' not in s)\n        if (tbopt != 'no'):\n            assert ('--calling--' in s)\n            assert ('IndexError' in s)\n        else:\n            assert ('FAILURES' not in s)\n            assert ('--calling--' not in s)\n            assert ('IndexError' not in s)\n", "label": 1}
{"function": "\n\ndef contests():\n    '\\n        Show the upcoming contests\\n    '\n    today = datetime.datetime.today()\n    today = datetime.datetime.strptime(str(today)[:(- 7)], '%Y-%m-%d %H:%M:%S')\n    start_date = today.date()\n    end_date = (start_date + datetime.timedelta(90))\n    url = 'https://contesttrackerapi.herokuapp.com/'\n    response = requests.get(url)\n    if (response.status_code == 200):\n        response = response.json()['result']\n    else:\n        return dict()\n    ongoing = response['ongoing']\n    upcoming = response['upcoming']\n    contests = []\n    cal = pdt.Calendar()\n    table = TABLE(_class='centered striped')\n    thead = THEAD(TR(TH('Contest Name'), TH('Site'), TH('Start'), TH('Duration/Ending'), TH('Link'), TH('Add Reminder')))\n    table.append(thead)\n    tbody = TBODY()\n    for i in ongoing:\n        if (i['Platform'] in ('TOPCODER', 'OTHER')):\n            continue\n        try:\n            endtime = datetime.datetime.strptime(i['EndTime'], '%a, %d %b %Y %H:%M')\n        except ValueError:\n            continue\n        tr = TR()\n        span = SPAN(_class='green tooltipped', data={\n            'position': 'right',\n            'delay': '50',\n            'tooltip': 'Live Contest',\n        }, _style=(((('cursor: pointer; ' + 'float:right; ') + 'height:10px; ') + 'width:10px; ') + 'border-radius: 50%;'))\n        tr.append(TD(i['Name'], span))\n        tr.append(TD(i['Platform'].capitalize()))\n        tr.append(TD('-'))\n        tr.append(TD(str(endtime).replace('-', '/'), _class='contest-end-time'))\n        tr.append(TD(A(I(_class='fa fa-external-link-square fa-lg'), _class='btn-floating btn-small green accent-4 tooltipped', _href=i['url'], data={\n            'position': 'left',\n            'tooltip': 'Contest Link',\n            'delay': '50',\n        }, _target='_blank')))\n        tr.append(TD(BUTTON(I(_class='fa fa-calendar-plus-o'), _class='btn-floating btn-small orange accent-4 tooltipped disabled', data={\n            'position': 'left',\n            'tooltip': 'Already started!',\n            'delay': '50',\n        })))\n        tbody.append(tr)\n    button_id = 1\n    for i in upcoming:\n        if (i['Platform'] in ('TOPCODER', 'OTHER')):\n            continue\n        start_time = datetime.datetime.strptime(i['StartTime'], '%a, %d %b %Y %H:%M')\n        tr = TR(_id=('contest-' + str(button_id)))\n        tr.append(TD(i['Name']))\n        tr.append(TD(i['Platform'].capitalize()))\n        tr.append(TD(str(start_time)))\n        duration = i['Duration']\n        duration = duration.replace(' days', 'd')\n        duration = duration.replace(' day', 'd')\n        tr.append(TD(duration))\n        tr.append(TD(A(I(_class='fa fa-external-link-square fa-lg'), _class='btn-floating btn-small green accent-4 tooltipped', _href=i['url'], data={\n            'position': 'left',\n            'tooltip': 'Contest Link',\n            'delay': '50',\n        }, _target='_blank')))\n        tr.append(TD(BUTTON(I(_class='fa fa-calendar-plus-o'), _class='btn-floating btn-small orange accent-4 tooltipped', data={\n            'position': 'left',\n            'tooltip': 'Set Reminder to Google Calendar',\n            'delay': '50',\n        }, _id=('set-reminder-' + str(button_id)))))\n        tbody.append(tr)\n        button_id += 1\n    table.append(tbody)\n    return dict(table=table, upcoming=upcoming)\n", "label": 0}
{"function": "\n\ndef sleep_while_copying(self, timeoutSec=3600):\n    Util.validate_type(timeoutSec, 'int')\n    step = 3\n    while (0 < timeoutSec):\n        try:\n            self.reload()\n        except saklient.errors.httpexception.HttpException:\n            pass\n        a = self.get_availability()\n        if (a == EAvailability.available):\n            return True\n        if (a != EAvailability.migrating):\n            timeoutSec = 0\n        timeoutSec -= step\n        if (0 < timeoutSec):\n            Util.sleep(step)\n    return False\n", "label": 0}
{"function": "\n\ndef test_salt(self):\n    h = Hashids(salt='Arbitrary string')\n    assert (h.decode('QWyf8yboH7KT2') == (683, 94108, 123, 5))\n    assert (h.decode('neHrCa') == (1, 2, 3))\n    assert (h.decode('LRCgf2') == (2, 4, 6))\n    assert (h.decode('JOMh1') == (99, 25))\n", "label": 0}
{"function": "\n\ndef _CheckAzureVersion():\n    'Checks the version of the installed Azure CLI.\\n\\n  Exits the process if the version cannot be checked, or if the installed CLI\\n  version does not match the expected version.\\n\\n  If the user provides the --azure_ignore_cli_version flag, a mismatched CLI\\n  version will result in a warning, but execution will continue.\\n  '\n    version_cmd = [AZURE_PATH, '-v']\n    try:\n        (stdout, _, retcode) = vm_util.IssueCommand(version_cmd)\n    except OSError:\n        err_msg = 'Unable to execute the Azure CLI. See the log file for more details. This failure may indicate that the Azure CLI is not installed. Execute the following command to install the Azure CLI:{sep}{install_cmd}{sep}See README.md for more information about how to set up PerfKit Benchmarker for Azure.'.format(install_cmd=_CLI_INSTALL_CMD, sep=os.linesep)\n        logging.debug(err_msg, exc_info=True)\n        sys.exit(err_msg)\n    if retcode:\n        logging.error('Failed to get the Azure CLI version. This failure may indicate that the installed version of the Azure CLI is out of date. Execute the following command to install the recommended version of the Azure CLI:%s%s', os.linesep, _CLI_INSTALL_CMD)\n        sys.exit(retcode)\n    version = stdout.strip()\n    if (version != azure.EXPECTED_CLI_VERSION):\n        err_msg = 'The version of the installed Azure CLI ({installed}) does not match the expected version ({expected}). This may result in incompatibilities that can cause commands to fail. To ensure compatibility, install the recommended version of the CLI by executing the following command:{sep}{install_cmd}'.format(expected=azure.EXPECTED_CLI_VERSION, install_cmd=_CLI_INSTALL_CMD, installed=version, sep=os.linesep)\n        if FLAGS.azure_ignore_cli_version:\n            logging.warning(err_msg)\n        else:\n            logging.error('%s%sTo bypass this CLI version check, run PerfKit Benchmarker with the --azure_ignore_cli_version flag.', err_msg, os.linesep)\n            sys.exit(1)\n", "label": 0}
{"function": "\n\ndef test_basic_authenticated_documents(suite, basic_user, docs):\n    jwt = _login(basic_user)\n    headers = _auth_header(jwt)\n    authenticated_documents = 'http://localhost:5000/api/authenticated-app/authenticated-documents'\n    confirmed_dangerous_delete = requests.delete(authenticated_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    dangerous_delete = requests.delete(authenticated_documents, headers=headers, params={\n        'delete_all': False,\n    })\n    assert (not dangerous_delete.ok)\n    post = requests.post(authenticated_documents, headers=headers, data=json.dumps(docs))\n    assert post.ok\n    get_all = requests.get((authenticated_documents + ';json'), headers=headers)\n    assert get_all.ok\n    confirmed_dangerous_delete = requests.delete(authenticated_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    _logout(jwt)\n", "label": 0}
{"function": "\n\n@contextmanager\ndef lock_file(path, maxdelay=0.1, lock_cls=LockFile, timeout=10.0):\n    'Cooperative file lock. Uses `lockfile.LockFile` polling under the hood.\\n\\n    `maxdelay` defines the interval between individual polls.\\n\\n    '\n    lock = lock_cls(path)\n    max_t = (time.time() + timeout)\n    while True:\n        if (time.time() >= max_t):\n            raise LockTimeout(('Timeout waiting to acquire lock for %s' % (path,)))\n        try:\n            lock.acquire(timeout=0)\n        except AlreadyLocked:\n            sleep(maxdelay)\n        else:\n            try:\n                (yield lock)\n                break\n            finally:\n                lock.release()\n", "label": 0}
{"function": "\n\ndef left_context(token_list, token, context_size, idx):\n    left_window = []\n    if (idx <= 0):\n        return ['<s>' for i in range(context_size)]\n    assert (token_list[idx] == token)\n    for i in range((idx - context_size), idx):\n        if (i < 0):\n            left_window.append('<s>')\n        else:\n            left_window.append(token_list[i])\n    return left_window\n", "label": 0}
{"function": "\n\ndef _on_process_finished(self, exit_code, exit_status):\n    if (self is None):\n        return\n    self._running = False\n    if (not self._user_stop):\n        if self._write_app_messages:\n            self._writer(self, ('\\nProcess finished with exit code %d' % self.exit_code), self._app_msg_col)\n    _logger().debug('process finished (exit_code=%r, exit_status=%r)', exit_code, exit_status)\n    try:\n        self.process_finished.emit(exit_code)\n    except TypeError:\n        pass\n    else:\n        self.setReadOnly(True)\n", "label": 0}
{"function": "\n\ndef run(self, sync):\n    app = sync.app\n    with app.db.getSession() as session:\n        for c in session.getPendingTopics():\n            sync.submitTask(SetTopicTask(c.key, self.priority))\n        for c in session.getPendingRebases():\n            sync.submitTask(RebaseChangeTask(c.key, self.priority))\n        for c in session.getPendingStatusChanges():\n            sync.submitTask(ChangeStatusTask(c.key, self.priority))\n        for c in session.getPendingStarred():\n            sync.submitTask(ChangeStarredTask(c.key, self.priority))\n        for c in session.getPendingCherryPicks():\n            sync.submitTask(SendCherryPickTask(c.key, self.priority))\n        for r in session.getPendingCommitMessages():\n            sync.submitTask(ChangeCommitMessageTask(r.key, self.priority))\n        for m in session.getPendingMessages():\n            sync.submitTask(UploadReviewTask(m.key, self.priority))\n", "label": 1}
{"function": "\n\ndef rs_tanh(p, x, prec):\n    \"\\n    Hyperbolic tangent of a series\\n\\n    Return the series expansion of the tanh of ``p``, about 0.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_tanh\\n    >>> R, x, y = ring('x, y', QQ)\\n    >>> rs_tanh(x + x*y, x, 4)\\n    -1/3*x**3*y**3 - x**3*y**2 - x**3*y - 1/3*x**3 + x*y + x\\n\\n    See Also\\n    ========\\n\\n    tanh\\n    \"\n    if rs_is_puiseux(p, x):\n        return rs_puiseux(rs_tanh, p, x, prec)\n    R = p.ring\n    const = 0\n    if _has_constant_term(p, x):\n        zm = R.zero_monom\n        c = p[zm]\n        if (R.domain is EX):\n            c_expr = c.as_expr()\n            const = tanh(c_expr)\n        elif isinstance(c, PolyElement):\n            try:\n                c_expr = c.as_expr()\n                const = R(tanh(c_expr))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        else:\n            try:\n                const = R(tanh(c))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        p1 = (p - c)\n        t1 = rs_tanh(p1, x, prec)\n        t = rs_series_inversion((1 + (const * t1)), x, prec)\n        return rs_mul((const + t1), t, x, prec)\n    if (R.ngens == 1):\n        return _tanh(p, x, prec)\n    else:\n        return rs_fun(p, _tanh, x, prec)\n", "label": 1}
{"function": "\n\ndef _coverall(self, fr, analysis):\n    try:\n        with open(fr.filename) as fp:\n            source = fp.readlines()\n    except IOError:\n        if (not self.config.ignore_errors):\n            raise\n    if self.config.strip_dirs:\n        filename = fr.filename\n        for dir in self.config.strip_dirs:\n            if filename.startswith(dir):\n                filename = filename.replace(dir, '').lstrip('/')\n                break\n    else:\n        filename = fr.relname\n    self.ret.append({\n        'name': filename,\n        'source': ''.join(source).rstrip(),\n        'coverage': list(self._coverage_list(source, analysis)),\n    })\n", "label": 0}
{"function": "\n\ndef _InvokeGitkitApi(self, method, params=None, need_service_account=True):\n    'Invokes Gitkit API, with optional access token for service account.\\n\\n    Args:\\n      method: string, the api method name.\\n      params: dict of optional parameters for the API.\\n      need_service_account: false if service account is not needed.\\n\\n    Raises:\\n      GitkitClientError: if the request is bad.\\n      GitkitServerError: if Gitkit can not handle the request.\\n\\n    Returns:\\n      API response as dict.\\n    '\n    body = (simplejson.dumps(params) if params else None)\n    req = urllib_request.Request((self.google_api_url + method))\n    req.add_header('Content-type', 'application/json')\n    if need_service_account:\n        if self.credentials:\n            access_token = self.credentials.get_access_token().access_token\n        elif (self.service_account_email and self.service_account_key):\n            access_token = self._GetAccessToken()\n        else:\n            raise errors.GitkitClientError('Missing service account credentials')\n        req.add_header('Authorization', ('Bearer ' + access_token))\n    try:\n        binary_body = (body.encode('utf-8') if body else None)\n        raw_response = urllib_request.urlopen(req, binary_body).read()\n    except urllib_request.HTTPError as err:\n        if (err.code == 400):\n            raw_response = err.read()\n        else:\n            raise\n    return self._CheckGitkitError(raw_response)\n", "label": 1}
{"function": "\n\ndef build_channels(api_user, nick, channels, path, taxonomy, country=False):\n    res = []\n    tags = build_tags(channels, taxonomy, path)\n    for channel in channels:\n        tags = [path, ((path + '/') + channel)]\n        if country:\n            rel_ref = Relation(owner='/tag_geo/Global', relation='tags_hierarchical', target=((path + '/') + channel))\n            rel_ref = rel_ref.put()\n            tags.append('/tag_geo/Global')\n        description = ('Group P.O.Box for %s' % channel)\n        try:\n            channel_ref = api.channel_create(api_user, nick=nick, channel=channel.title().replace(' ', ''), tags=tags, description=description)\n        except exception.ApiException:\n            logging.info('Channel Exists')\n            nick = api.clean.channel(channel.title().replace(' ', ''))\n            query = Actor.gql('WHERE nick=:1', nick)\n            channel_ref = query.get()\n            if channel_ref:\n                if (not (path in channel_ref.tags)):\n                    channel_ref.tags.append(path)\n                    channel_key = channel_ref.put()\n        res.append(channel_ref)\n    logging.info('End Creating Channels')\n    return res\n", "label": 0}
{"function": "\n\ndef process_response(self, request, response):\n    csrf_token = None\n    try:\n        cookie = response.cookies[settings.SESSION_COOKIE_NAME]\n        csrf_token = _make_token(cookie.value)\n    except KeyError:\n        try:\n            session_id = request.COOKIES[settings.SESSION_COOKIE_NAME]\n            csrf_token = _make_token(session_id)\n        except KeyError:\n            pass\n    if ((csrf_token is not None) and (response['Content-Type'].split(';')[0] in _HTML_TYPES)):\n        idattributes = itertools.chain((\"id='csrfmiddlewaretoken'\",), itertools.repeat(''))\n\n        def add_csrf_field(match):\n            'Returns the matched <form> tag plus the added <input> element'\n            return ((((((match.group() + \"<div style='display:none;'>\") + \"<input type='hidden' \") + idattributes.next()) + \" name='csrfmiddlewaretoken' value='\") + csrf_token) + \"' /></div>\")\n        response.content = _POST_FORM_RE.sub(add_csrf_field, response.content)\n    return response\n", "label": 0}
{"function": "\n\ndef kl(Y, Y_hat, batch_axis):\n    \"\\n    Warning: This function expects a sigmoid nonlinearity in the\\n    output layer. Returns a batch (vector) of mean across units of\\n    KL divergence for each example,\\n    KL(P || Q) where P is defined by Y and Q is defined by Y_hat:\\n\\n    p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)\\n    For binary p, some terms drop out:\\n    - p log q - (1-p) log (1-q)\\n    - p log sigmoid(z) - (1-p) log sigmoid(-z)\\n    p softplus(-z) + (1-p) softplus(z)\\n\\n    Parameters\\n    ----------\\n    Y : Variable\\n        targets for the sigmoid outputs. Currently Y must be purely binary.\\n        If it's not, you'll still get the right gradient, but the\\n        value in the monitoring channel will be wrong.\\n    Y_hat : Variable\\n        predictions made by the sigmoid layer. Y_hat must be generated by\\n        fprop, i.e., it must be a symbolic sigmoid.\\n    batch_axis : list\\n        list of axes to compute average kl divergence across.\\n\\n    Returns\\n    -------\\n    ave : Variable\\n        average kl divergence between Y and Y_hat.\\n    \"\n    assert hasattr(Y_hat, 'owner')\n    assert (batch_axis is not None)\n    owner = Y_hat.owner\n    assert (owner is not None)\n    op = owner.op\n    if (not hasattr(op, 'scalar_op')):\n        raise ValueError(((('Expected Y_hat to be generated by an Elemwise op, got ' + str(op)) + ' of type ') + str(type(op))))\n    assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid)\n    for Yv in get_debug_values(Y):\n        if (not ((Yv.min() >= 0.0) and (Yv.max() <= 1.0))):\n            raise ValueError(('Expected Y to be between 0 and 1. Either Y' + '< 0 or Y > 1 was found in the input.'))\n    (z,) = owner.inputs\n    term_1 = (Y * T.nnet.softplus((- z)))\n    term_2 = ((1 - Y) * T.nnet.softplus(z))\n    total = (term_1 + term_2)\n    naxes = total.ndim\n    axes_to_reduce = list(range(naxes))\n    del axes_to_reduce[batch_axis]\n    ave = total.mean(axis=axes_to_reduce)\n    return ave\n", "label": 1}
{"function": "\n\ndef _get_history_lines(self):\n    'Returns list of history entries.'\n    history_file_name = self._get_history_file_name()\n    if os.path.isfile(history_file_name):\n        with io.open(history_file_name, 'r', encoding='utf-8', errors='ignore') as history_file:\n            lines = history_file.readlines()\n            if settings.history_limit:\n                lines = lines[(- settings.history_limit):]\n            for line in lines:\n                prepared = self._script_from_history(line).strip()\n                if prepared:\n                    (yield prepared)\n", "label": 0}
{"function": "\n\n@bacpypes_debugging\ndef octet_string_endec(x):\n    'Pass the value to OctetString, construct a tag from the hex string,\\n    and compare results of encode and decoding each other.'\n    if _debug:\n        octet_string_endec._debug('octet_string_endec %r', x)\n    tag = octet_string_tag(x)\n    if _debug:\n        octet_string_endec._debug('    - tag: %r, %r', tag, tag.tagData)\n    obj = OctetString(xtob(x))\n    if _debug:\n        octet_string_endec._debug('    - obj: %r, %r', obj, obj.value)\n    assert (octet_string_encode(obj) == tag)\n    assert (octet_string_decode(tag) == obj)\n", "label": 0}
{"function": "\n\ndef test_extract_simple():\n    weak = 1.0\n    strong = 2.0\n    nc = 4\n    ns = 20\n    data = np.random.uniform(size=(ns, nc), low=0.0, high=1.0)\n    data[(10, 0)] = 0.5\n    data[(11, 0)] = 1.5\n    data[(12, 0)] = 1.0\n    data[(10, 1)] = 1.5\n    data[(11, 1)] = 2.5\n    data[(12, 1)] = 2.0\n    component = np.array([[10, 0], [10, 1], [11, 0], [11, 1], [12, 0], [12, 1]])\n    we = WaveformExtractor(extract_before=3, extract_after=5)\n    we.set_thresholds(weak=weak, strong=strong)\n    comp = we._component(component, n_samples=ns)\n    ae(comp.comp_s, [10, 10, 11, 11, 12, 12])\n    ae(comp.comp_ch, [0, 1, 0, 1, 0, 1])\n    assert ((comp.s_min, comp.s_max) == ((10 - 3), (12 + 4)))\n    assert (we._normalize(weak) == 0)\n    assert (we._normalize(strong) == 1)\n    ae(we._normalize([((weak + strong) / 2.0)]), [0.5])\n    wave = we._comp_wave(data, comp)\n    assert (wave.shape == (((3 + 5) + 1), nc))\n    ae(wave[3:6, :], [[0.5, 1.5, 0.0, 0.0], [1.5, 2.5, 0.0, 0.0], [1.0, 2.0, 0.0, 0.0]])\n    masks = we.masks(data, wave, comp)\n    ae(masks, [0.5, 1.0, 0, 0])\n    s = we.spike_sample_aligned(wave, comp)\n    assert (11 <= s < 12)\n    wave_e = we.extract(data, s)\n    assert (wave_e.shape[1] == wave.shape[1])\n    ae(wave[3:6, :2], wave_e[3:6, :2])\n    wave_a = we.align(wave_e, s)\n    assert (wave_a.shape == ((3 + 5), nc))\n    (s_f, masks_f, wave_f) = we(component, data=data, data_t=data)\n    assert (s_f == s)\n    ae(masks_f, masks)\n    ae(wave_f, wave_a)\n    we = WaveformExtractor(extract_before=3, extract_after=5, thresholds={\n        'weak': weak,\n        'strong': strong,\n    })\n    (s_f_o, masks_f_o, wave_f_o) = we(component, data=data, data_t=data)\n    assert (s_f == s_f_o)\n    assert np.allclose(wave_f, wave_f_o)\n    ae(masks_f_o, [0.5, 1.0, 0.0, 0.0])\n", "label": 1}
{"function": "\n\ndef test_log_pdf(self):\n    X_1 = numpy.array([[1], [0]])\n    parameters = dict(mu=0.0, rho=1.0)\n    log_pdf = self.component_class.log_pdf(X_1, parameters)\n    assert (len(log_pdf) == 2)\n    assert (math.fabs(((- 1.4189385332046727) - log_pdf[(0, 0)])) < 1e-08)\n    assert (math.fabs(((- 0.9189385332046727) - log_pdf[(1, 0)])) < 1e-08)\n    parameters = dict(mu=2.2, rho=12.1)\n    log_pdf = self.component_class.log_pdf(X_1, parameters)\n    assert (len(log_pdf) == 2)\n    assert (math.fabs(((- 8.38433580690333) - log_pdf[(0, 0)])) < 1e-08)\n    assert (math.fabs(((- 28.954335806903334) - log_pdf[(1, 0)])) < 1e-08)\n    parameters = dict(mu=0.0, rho=1.0)\n    lspc = numpy.linspace(0, 10, num=20)\n    X_2 = numpy.array([[x] for x in lspc])\n    log_pdf = self.component_class.log_pdf(X_2, parameters)\n    assert (len(log_pdf) == 20)\n    for n in range(1, 20):\n        assert (log_pdf[((n - 1), 0)] > log_pdf[(n, 0)])\n", "label": 1}
{"function": "\n\ndef checkout(path, branch='master', use_sudo=False, user=None, force=False):\n    '\\n    Checkout a branch to the working directory.\\n\\n    :param path: Path of the working copy directory.  This directory must exist\\n                 and be a Git working copy.\\n    :type path: str\\n\\n    :param branch: Name of the branch to checkout.\\n    :type branch: str\\n\\n    :param use_sudo: If ``True`` execute ``git`` with\\n                     :func:`fabric.operations.sudo`, else with\\n                     :func:`fabric.operations.run`.\\n    :type use_sudo: bool\\n\\n    :param user: If ``use_sudo is True``, run :func:`fabric.operations.sudo`\\n                 with the given user.  If ``use_sudo is False`` this parameter\\n                 has no effect.\\n    :type user: str\\n    :param force: If ``True``, append the ``--force`` option to the command.\\n    :type force: bool\\n    '\n    if (path is None):\n        raise ValueError('Path to the working copy is needed to checkout a branch')\n    options = []\n    if force:\n        options.append('--force')\n    options = ' '.join(options)\n    cmd = ('git checkout %s %s' % (branch, options))\n    with cd(path):\n        if (use_sudo and (user is None)):\n            run_as_root(cmd)\n        elif use_sudo:\n            sudo(cmd, user=user)\n        else:\n            run(cmd)\n", "label": 0}
{"function": "\n\ndef _send_request(self):\n    if (self.editor is None):\n        return\n    cursor = self.editor.textCursor()\n    self._sub = TextHelper(self.editor).word_under_cursor(select_whole_word=True).selectedText()\n    if ((not cursor.hasSelection()) or (cursor.selectedText() == self._sub)):\n        request_data = {\n            'string': self.editor.toPlainText(),\n            'sub': self._sub,\n            'regex': False,\n            'whole_word': True,\n            'case_sensitive': True,\n        }\n        try:\n            self.editor.backend.send_request(findall, request_data, self._on_results_available)\n        except NotRunning:\n            self._request_highlight()\n", "label": 0}
{"function": "\n\ndef _populate_vars(self):\n    if ('variables' in self.config.keys()):\n        for (var, question) in self.config['variables'].items():\n            if (self.app.pargs.defaults is True):\n                try:\n                    res = self.app.config.get('answers', var)\n                except ConfigParser.NoOptionError as e:\n                    res = 'MISSING VARIABLE'\n            elif (var.lower() in self.app.config.keys('answers')):\n                default = self.app.config.get('answers', var.lower())\n                res = input(('%s: [%s] ' % (question, default)))\n                if (len(res) == 0):\n                    res = default\n            else:\n                res = input(('%s: ' % question))\n            self._vars[var] = res.strip()\n", "label": 0}
{"function": "\n\ndef apply(self, fgraph):\n    '\\n        WRITEME\\n\\n        Applies each L{Optimizer} in self in turn.\\n\\n        '\n    l = []\n    if fgraph.profile:\n        validate_before = fgraph.profile.validate_time\n        sub_validate_time = [validate_before]\n    else:\n        sub_validate_time = []\n    callback_before = fgraph.execute_callbacks_time\n    nb_node_before = len(fgraph.apply_nodes)\n    sub_profs = []\n    for optimizer in self:\n        try:\n            t0 = time.time()\n            sub_prof = optimizer.optimize(fgraph)\n            l.append(float((time.time() - t0)))\n            sub_profs.append(sub_prof)\n            if fgraph.profile:\n                sub_validate_time.append(fgraph.profile.validate_time)\n        except AssertionError:\n            raise\n        except Exception as e:\n            if self.failure_callback:\n                self.failure_callback(e, self, optimizer)\n                continue\n            else:\n                raise\n    if fgraph.profile:\n        validate_time = (fgraph.profile.validate_time - validate_before)\n    else:\n        validate_time = None\n    callback_time = (fgraph.execute_callbacks_time - callback_before)\n    return (self, l, validate_time, callback_time, nb_node_before, len(fgraph.apply_nodes), sub_profs, sub_validate_time)\n", "label": 1}
{"function": "\n\ndef spynner_open(self, url, data=None, headers=None, method='GET', wait_for_text=None, wait_for_selector=None, tries=None):\n    try:\n        from PyQt4.QtNetwork import QNetworkAccessManager\n    except ImportError:\n        raise DependencyNotInstalledError('PyQt4')\n    if (wait_for_text is not None):\n\n        def wait_callback(br):\n            return (wait_for_text in br.html)\n    elif (wait_for_selector is not None):\n\n        def wait_callback(br):\n            return (not br.webframe.findFirstElement(wait_for_selector).isNull())\n    else:\n        wait_callback = None\n    operation = QNetworkAccessManager.GetOperation\n    if (method == 'POST'):\n        operation = QNetworkAccessManager.PostOperation\n    self.br.load(url, wait_callback=wait_callback, tries=tries, operation=operation, body=data, headers=headers)\n    return self.br\n", "label": 0}
{"function": "\n\ndef _indexes(gumt, gdmt, gwmt, gdnt):\n    'Count Understemming Index (UI), Overstemming Index (OI) and Stemming Weight (SW).\\n\\n    :param gumt, gdmt, gwmt, gdnt: Global unachieved merge total (gumt),\\n    global desired merge total (gdmt),\\n    global wrongly merged total (gwmt) and\\n    global desired non-merge total (gdnt).\\n    :type gumt, gdmt, gwmt, gdnt: float\\n    :return: Understemming Index (UI),\\n    Overstemming Index (OI) and\\n    Stemming Weight (SW).\\n    :rtype: tuple(float, float, float)\\n    '\n    try:\n        ui = (gumt / gdmt)\n    except ZeroDivisionError:\n        ui = 0.0\n    try:\n        oi = (gwmt / gdnt)\n    except ZeroDivisionError:\n        oi = 0.0\n    try:\n        sw = (oi / ui)\n    except ZeroDivisionError:\n        if (oi == 0.0):\n            sw = float('nan')\n        else:\n            sw = float('inf')\n    return (ui, oi, sw)\n", "label": 0}
{"function": "\n\ndef extend_list(self, data, parsed_args):\n    'Add subnet information to a network list.'\n    neutron_client = self.get_client()\n    search_opts = {\n        'fields': ['id', 'cidr'],\n    }\n    if self.pagination_support:\n        page_size = parsed_args.page_size\n        if page_size:\n            search_opts.update({\n                'limit': page_size,\n            })\n    subnet_ids = []\n    for n in data:\n        if ('subnets' in n):\n            subnet_ids.extend(n['subnets'])\n\n    def _get_subnet_list(sub_ids):\n        search_opts['id'] = sub_ids\n        return neutron_client.list_subnets(**search_opts).get('subnets', [])\n    try:\n        subnets = _get_subnet_list(subnet_ids)\n    except exceptions.RequestURITooLong as uri_len_exc:\n        subnet_count = len(subnet_ids)\n        max_size = ((self.subnet_id_filter_len * subnet_count) - uri_len_exc.excess)\n        chunk_size = (max_size // self.subnet_id_filter_len)\n        subnets = []\n        for i in range(0, subnet_count, chunk_size):\n            subnets.extend(_get_subnet_list(subnet_ids[i:(i + chunk_size)]))\n    subnet_dict = dict([(s['id'], s) for s in subnets])\n    for n in data:\n        if ('subnets' in n):\n            n['subnets'] = [(subnet_dict.get(s) or {\n                'id': s,\n            }) for s in n['subnets']]\n", "label": 1}
{"function": "\n\ndef _load_config(self):\n    try:\n        scf = self.conf.glance_store.swift_store_config_file\n        conf_file = self.conf.find_file(scf)\n        CONFIG.read(conf_file)\n    except Exception as e:\n        msg = (i18n._('swift config file %(conf)s:%(exc)s not found') % {\n            'conf': self.conf.glance_store.swift_store_config_file,\n            'exc': e,\n        })\n        LOG.error(msg)\n        raise exceptions.BadStoreConfiguration(store_name='swift', reason=msg)\n    account_params = {\n        \n    }\n    account_references = CONFIG.sections()\n    for ref in account_references:\n        reference = {\n            \n        }\n        try:\n            for param in ('auth_address', 'user', 'key', 'project_domain_id', 'project_domain_name', 'user_domain_id', 'user_domain_name'):\n                reference[param] = CONFIG.get(ref, param)\n            try:\n                reference['auth_version'] = CONFIG.get(ref, 'auth_version')\n            except configparser.NoOptionError:\n                av = self.conf.glance_store.swift_store_auth_version\n                reference['auth_version'] = av\n            account_params[ref] = reference\n        except (ValueError, SyntaxError, configparser.NoOptionError) as e:\n            LOG.exception(i18n._('Invalid format of swift store configcfg'))\n    return account_params\n", "label": 0}
{"function": "\n\ndef iterbusinessdays(self, d1, d2):\n    '\\n        Date iterator returning dates in d1 <= x < d2, excluding weekends and holidays\\n        '\n    assert (d2 >= d1)\n    if ((d1.date() == d2.date()) and (d2.time() < self.business_hours[0])):\n        return\n    first = True\n    for dt in self.iterdays(d1, d2):\n        if (first and (d1.time() > self.business_hours[1])):\n            first = False\n            continue\n        first = False\n        if ((not self.isweekend(dt)) and (not self.isholiday(dt))):\n            (yield dt)\n", "label": 1}
{"function": "\n\ndef test_vecwrapper(self):\n    unknowns_dict = OrderedDict()\n    unknowns_dict['y1'] = {\n        'shape': (3, 2),\n        'size': 6,\n        'val': np.ones((3, 2)),\n    }\n    unknowns_dict['y2'] = {\n        'shape': 1,\n        'size': 1,\n        'val': 2.0,\n    }\n    unknowns_dict['y3'] = {\n        'size': 0,\n        'val': 'foo',\n        'pass_by_obj': True,\n    }\n    unknowns_dict['y4'] = {\n        'shape': (2, 1),\n        'size': 2,\n        'val': np.zeros((2, 1)),\n    }\n    unknowns_dict['s1'] = {\n        'shape': 1,\n        'size': 1,\n        'val': (- 1.0),\n        'state': True,\n    }\n    sd = _SysData('')\n    for (u, meta) in unknowns_dict.items():\n        meta['pathname'] = u\n        meta['top_promoted_name'] = u\n        sd.to_prom_name[u] = u\n    u = SrcVecWrapper(sd, pbd)\n    u.setup(unknowns_dict, store_byobjs=True)\n    self.assertEqual(u.vec.size, 10)\n    self.assertEqual(len(u), 5)\n    self.assertEqual(list(u.keys()), ['y1', 'y2', 'y3', 'y4', 's1'])\n    self.assertTrue(np.all((u['y1'] == np.ones((3, 2)))))\n    self.assertEqual(u['y2'], 2.0)\n    self.assertEqual(u['y3'], 'foo')\n    self.assertTrue(np.all((u['y4'] == np.zeros((2, 1)))))\n    self.assertEqual(u['s1'], (- 1.0))\n    self.assertEqual([t[0] for t in u.vec_val_iter()], ['y1', 'y2', 'y4', 's1'])\n    u['y1'] = (np.ones((3, 2)) * 3.0)\n    u['y2'] = 2.5\n    u['y3'] = 'bar'\n    u['y4'] = (np.ones((2, 1)) * 7.0)\n    u['s1'] = 5.0\n    self.assertTrue(np.all((u['y1'] == (np.ones((3, 2)) * 3.0))))\n    self.assertTrue(np.all((u['y4'] == (np.ones((2, 1)) * 7.0))))\n    self.assertEqual(u['y2'], 2.5)\n    self.assertEqual(u['y3'], 'bar')\n    self.assertEqual(u['s1'], 5.0)\n    try:\n        u['y1'] = np.ones((3, 3))\n    except Exception as err:\n        self.assertEqual(str(err), 'could not broadcast input array from shape (9) into shape (6)')\n    else:\n        self.fail('Exception expected')\n    params = OrderedDict()\n    params['y1'] = {\n        'shape': (3, 2),\n        'size': 6,\n        'val': np.ones((3, 2)),\n    }\n    params['y2'] = {\n        'shape': 1,\n        'size': 1,\n        'val': 2.0,\n    }\n    params['y3'] = {\n        'size': 0,\n        'val': 'foo',\n    }\n    params['y4'] = {\n        'shape': (2, 1),\n        'size': 6,\n        'val': np.zeros((2, 1)),\n    }\n    for (p, meta) in params.items():\n        meta['pathname'] = p\n        meta['top_promoted_name'] = p\n        sd.to_prom_name[u] = u\n    connections = {\n        \n    }\n    for p in params:\n        connections[p] = (p, None)\n    s = _SysData('')\n    s._unknowns_dict = u._dat\n    p = TgtVecWrapper(s, pbd)\n    p.setup(None, params, u, params.keys(), connections, store_byobjs=True)\n    self.assertEqual(p.vec.size, 9)\n    self.assertEqual(len(p), 4)\n    self.assertEqual(list(p.keys()), ['y1', 'y2', 'y3', 'y4'])\n    self.assertTrue(np.all((p['y1'] == np.zeros((3, 2)))))\n    self.assertEqual(p['y2'], 0.0)\n    self.assertEqual(p['y3'], 'bar')\n    self.assertTrue(np.all((p['y4'] == np.zeros((2, 1)))))\n    p['y1'] = (np.ones((3, 2)) * 9.0)\n    self.assertTrue(np.all((p['y1'] == (np.ones((3, 2)) * 9.0))))\n", "label": 0}
{"function": "\n\ndef transaction(self, func, *watches, **kwargs):\n    \"\\n        Convenience method for executing the callable `func` as a transaction\\n        while watching all keys specified in `watches`. The 'func' callable\\n        should expect a single argument which is a Pipeline object.\\n        \"\n    shard_hint = kwargs.pop('shard_hint', None)\n    value_from_callable = kwargs.pop('value_from_callable', False)\n    watch_delay = kwargs.pop('watch_delay', None)\n    with self.pipeline(True, shard_hint) as pipe:\n        while 1:\n            try:\n                if watches:\n                    pipe.watch(*watches)\n                func_value = func(pipe)\n                exec_value = pipe.execute()\n                return (func_value if value_from_callable else exec_value)\n            except WatchError:\n                if ((watch_delay is not None) and (watch_delay > 0)):\n                    time.sleep(watch_delay)\n                continue\n", "label": 0}
{"function": "\n\ndef transform_Index(self, expr):\n    arr = self.transform_expr(expr.value)\n    idx = self.transform_expr(expr.index)\n    idx = self.assign_name(idx, 'idx')\n    arr_t = arr.type\n    if (arr_t.__class__ is PtrT):\n        assert isinstance(idx.type, IntT)\n        return expr\n    assert (arr_t.__class__ is ArrayT), ('Unexpected array %s : %s' % (arr, arr.type))\n    if self.is_tuple(idx):\n        indices = self.tuple_elts(idx)\n    else:\n        indices = [idx]\n    n_given = len(indices)\n    n_required = arr_t.rank\n    if (n_given < n_required):\n        extra_indices = ([syntax.helpers.slice_none] * (n_required - n_given))\n        indices.extend(extra_indices)\n    if syntax.helpers.all_scalars(indices):\n        data_ptr = self.attr(arr, 'data')\n        strides = self.attr(arr, 'strides')\n        offset_elts = self.attr(arr, 'offset')\n        for (i, idx_i) in enumerate(indices):\n            stride_i = self.tuple_proj(strides, i)\n            elts_i = self.mul(stride_i, idx_i, ('offset_elts_%d' % i))\n            offset_elts = self.add(offset_elts, elts_i, 'total_offset')\n        return self.index(data_ptr, offset_elts, temp=False)\n    else:\n        return self.array_slice(arr, indices)\n", "label": 1}
{"function": "\n\ndef __init__(self, fields, objective_id=None, data_locale=None, missing_tokens=None):\n    if isinstance(fields, dict):\n        try:\n            self.objective_id = objective_id\n            self.uniquify_varnames(fields)\n            self.inverted_fields = invert_dictionary(fields)\n            self.fields = {\n                \n            }\n            self.fields.update(fields)\n            self.data_locale = data_locale\n            self.missing_tokens = missing_tokens\n            if (self.data_locale is None):\n                self.data_locale = DEFAULT_LOCALE\n            if (self.missing_tokens is None):\n                self.missing_tokens = DEFAULT_MISSING_TOKENS\n        except KeyError:\n            raise Exception('Wrong field structure.')\n", "label": 0}
{"function": "\n\ndef test_scale():\n    (m, ctl, config) = init()\n    m.load(config)\n    pids = m.pids()\n    cmd = TestCommand('scale', ['dummy', 1])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids1 = m.pids()\n    cmd = TestCommand('scale', ['dummy', (- 1)])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids2 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '+4'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids3 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '-1'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids4 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '=1'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids5 = m.pids()\n    m.stop()\n    m.run()\n    assert (len(pids) == 1)\n    assert (len(pids1) == 2)\n    assert (len(pids2) == 1)\n    assert (len(pids3) == 5)\n    assert (len(pids4) == 4)\n    assert (len(pids5) == 1)\n", "label": 0}
{"function": "\n\ndef test_load_from_entry_point():\n    from giblets.core import ComponentManager, Component, ExtensionPoint\n    from giblets.search import find_plugins_by_entry_point\n\n    class PluginFinder(Component):\n        found_plugins = ExtensionPoint(TestEggInterface)\n    mgr = ComponentManager()\n    pf = PluginFinder(mgr)\n    assert (len(pf.found_plugins) == 0)\n    find_plugins_by_entry_point('giblets_load_from_entry_point_test')\n    expected_plugins = ['TestEggPlugin1', 'TestEggPlugin2', 'TestEggPlugin3']\n    got_plugins = set()\n    assert (len(pf.found_plugins) == len(expected_plugins))\n    for plugin in pf.found_plugins:\n        plugin_name = plugin.__class__.__name__\n        assert (plugin_name in expected_plugins)\n        got_plugins.add(plugin_name)\n    for plugin_name in expected_plugins:\n        assert (plugin_name in got_plugins)\n", "label": 0}
{"function": "\n\ndef test_floats(self):\n    t = self.table\n    for f in self.functions:\n        expr = f(t.v5)\n        assert isinstance(expr, ir.DoubleArray)\n        expr = f(5.5, t.v5)\n        assert isinstance(expr, ir.DoubleArray)\n        expr = f(5.5, 5)\n        assert isinstance(expr, ir.DoubleScalar)\n", "label": 0}
{"function": "\n\ndef install(self, install_options, global_options=(), *args, **kwargs):\n    '\\n        Install everything in this set (after having downloaded and unpacked\\n        the packages)\\n        '\n    to_install = self._to_install()\n    if to_install:\n        logger.info('Installing collected packages: %s', ', '.join([req.name for req in to_install]))\n    with indent_log():\n        for requirement in to_install:\n            if requirement.conflicts_with:\n                logger.info('Found existing installation: %s', requirement.conflicts_with)\n                with indent_log():\n                    requirement.uninstall(auto_confirm=True)\n            try:\n                requirement.install(install_options, global_options, *args, **kwargs)\n            except:\n                if (requirement.conflicts_with and (not requirement.install_succeeded)):\n                    requirement.rollback_uninstall()\n                raise\n            else:\n                if (requirement.conflicts_with and requirement.install_succeeded):\n                    requirement.commit_uninstall()\n            requirement.remove_temporary_source()\n    self.successfully_installed = to_install\n", "label": 1}
{"function": "\n\ndef BuildTable(self, start_row, end_row, request):\n    'Builds a table of rdfvalues.'\n    try:\n        aff4_path = (self.state.get('aff4_path') or request.REQ.get('aff4_path'))\n        collection = aff4.FACTORY.Open(aff4_path, aff4_type='RDFValueCollection', token=request.token)\n    except IOError:\n        return\n    try:\n        self.size = len(collection)\n    except AttributeError:\n        self.show_total_count = False\n    row_index = start_row\n    for value in itertools.islice(collection, start_row, end_row):\n        self.AddCell(row_index, 'Value', value)\n        row_index += 1\n", "label": 0}
{"function": "\n\n@ignore_warnings\ndef test_warm_start():\n    (X, y) = (iris.data, iris.target)\n    solvers = ['newton-cg', 'sag']\n    if (sp_version >= (0, 12)):\n        solvers.append('lbfgs')\n    for warm_start in [True, False]:\n        for fit_intercept in [True, False]:\n            for solver in solvers:\n                for multi_class in ['ovr', 'multinomial']:\n                    clf = LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept)\n                    clf.fit(X, y)\n                    coef_1 = clf.coef_\n                    clf.max_iter = 1\n                    with ignore_warnings():\n                        clf.fit(X, y)\n                    cum_diff = np.sum(np.abs((coef_1 - clf.coef_)))\n                    msg = ('Warm starting issue with %s solver in %s mode with fit_intercept=%s and warm_start=%s' % (solver, multi_class, str(fit_intercept), str(warm_start)))\n                    if warm_start:\n                        assert_greater(2.0, cum_diff, msg)\n                    else:\n                        assert_greater(cum_diff, 2.0, msg)\n", "label": 0}
{"function": "\n\ndef _addedDataToIncoming(self, inc, skipFinish=False):\n    if (not inc.receivedAllData()):\n        return inc\n    (rdata, extra) = ('', '')\n    try:\n        (rdata, extra) = inc.data\n        if isControlMessage(rdata):\n            raise ValueError(('Error: received control message \"%s\"; expecting incoming data.' % str(rdata)))\n        rEnv = ReceiveEnvelope(*rdata)\n    except Exception:\n        import traceback\n        thesplog('OUCH!  Error deserializing received data: %s  (rdata=\"%s\", extra=\"%s\")', traceback.format_exc(), rdata, extra)\n        try:\n            inc.socket.sendall(ackDataErrMsg)\n        except Exception:\n            pass\n        inc.close()\n        return None\n    inc.socket.sendall(ackMsg)\n    inc.fromAddress = rdata[0]\n    self._processReceivedEnvelope(rEnv)\n    if (extra and isinstance(inc, TCPIncomingPersistent)):\n        newinc = TCPIncomingPersistent(inc.fromAddress, inc.socket)\n        newinc.addData(rdata)\n        return self._addedDataToIncoming(newinc)\n    if (not skipFinish):\n        self._finishIncoming(inc, rEnv.sender)\n    return None\n", "label": 1}
{"function": "\n\ndef assertDictMatch(self, d1, d2, approx_equal=False, tolerance=0.001):\n    \"Assert two dicts are equivalent.\\n\\n        This is a 'deep' match in the sense that it handles nested\\n        dictionaries appropriately.\\n\\n        NOTE:\\n\\n            If you don't care (or don't know) a given value, you can specify\\n            the string DONTCARE as the value. This will cause that dict-item\\n            to be skipped.\\n\\n        \"\n\n    def raise_assertion(msg):\n        d1str = str(d1)\n        d2str = str(d2)\n        base_msg = ('Dictionaries do not match. %(msg)s d1: %(d1str)s d2: %(d2str)s' % locals())\n        raise AssertionError(base_msg)\n    d1keys = set(d1.keys())\n    d2keys = set(d2.keys())\n    if (d1keys != d2keys):\n        d1only = (d1keys - d2keys)\n        d2only = (d2keys - d1keys)\n        raise_assertion(('Keys in d1 and not d2: %(d1only)s. Keys in d2 and not d1: %(d2only)s' % locals()))\n    for key in d1keys:\n        d1value = d1[key]\n        d2value = d2[key]\n        try:\n            error = abs((float(d1value) - float(d2value)))\n            within_tolerance = (error <= tolerance)\n        except (ValueError, TypeError):\n            within_tolerance = False\n        if (hasattr(d1value, 'keys') and hasattr(d2value, 'keys')):\n            self.assertDictMatch(d1value, d2value)\n        elif ('DONTCARE' in (d1value, d2value)):\n            continue\n        elif (approx_equal and within_tolerance):\n            continue\n        elif (d1value != d2value):\n            raise_assertion((\"d1['%(key)s']=%(d1value)s != d2['%(key)s']=%(d2value)s\" % locals()))\n", "label": 1}
{"function": "\n\n@periodic_task.periodic_task(spacing=CONF.instance_delete_interval)\ndef _cleanup_incomplete_migrations(self, context):\n    'Delete instance files on failed resize/revert-resize operation\\n\\n        During resize/revert-resize operation, if that instance gets deleted\\n        in-between then instance files might remain either on source or\\n        destination compute node because of race condition.\\n        '\n    LOG.debug('Cleaning up deleted instances with incomplete migration ')\n    migration_filters = {\n        'host': CONF.host,\n        'status': 'error',\n    }\n    migrations = objects.MigrationList.get_by_filters(context, migration_filters)\n    if (not migrations):\n        return\n    inst_uuid_from_migrations = set([migration.instance_uuid for migration in migrations])\n    inst_filters = {\n        'deleted': True,\n        'soft_deleted': False,\n        'uuid': inst_uuid_from_migrations,\n        'host': CONF.host,\n    }\n    attrs = ['info_cache', 'security_groups', 'system_metadata']\n    with utils.temporary_mutation(context, read_deleted='yes'):\n        instances = objects.InstanceList.get_by_filters(context, inst_filters, expected_attrs=attrs, use_slave=True)\n    for instance in instances:\n        for migration in migrations:\n            if (instance.uuid == migration.instance_uuid):\n                self.driver.delete_instance_files(instance)\n                try:\n                    migration.status = 'failed'\n                    with migration.obj_as_admin():\n                        migration.save()\n                except exception.MigrationNotFound:\n                    LOG.warning(_LW('Migration %s is not found.'), migration.id, context=context, instance=instance)\n                break\n", "label": 0}
{"function": "\n\ndef _test_renewal_common(self, due_for_renewal, extra_args, log_out=None, args=None, should_renew=True, error_expected=False):\n    cert_path = 'certbot/tests/testdata/cert.pem'\n    chain_path = '/etc/letsencrypt/live/foo.bar/fullchain.pem'\n    mock_lineage = mock.MagicMock(cert=cert_path, fullchain=chain_path)\n    mock_lineage.should_autorenew.return_value = due_for_renewal\n    mock_certr = mock.MagicMock()\n    mock_key = mock.MagicMock(pem='pem_key')\n    mock_client = mock.MagicMock()\n    stdout = None\n    mock_client.obtain_certificate.return_value = (mock_certr, 'chain', mock_key, 'csr')\n    try:\n        with mock.patch('certbot.main._find_duplicative_certs') as mock_fdc:\n            mock_fdc.return_value = (mock_lineage, None)\n            with mock.patch('certbot.main._init_le_client') as mock_init:\n                mock_init.return_value = mock_client\n                get_utility_path = 'certbot.main.zope.component.getUtility'\n                with mock.patch(get_utility_path) as mock_get_utility:\n                    with mock.patch('certbot.main.renewal.OpenSSL') as mock_ssl:\n                        mock_latest = mock.MagicMock()\n                        mock_latest.get_issuer.return_value = 'Fake fake'\n                        mock_ssl.crypto.load_certificate.return_value = mock_latest\n                        with mock.patch('certbot.main.renewal.crypto_util'):\n                            if (not args):\n                                args = ['-d', 'isnot.org', '-a', 'standalone', 'certonly']\n                            if extra_args:\n                                args += extra_args\n                            try:\n                                (ret, stdout, _, _) = self._call(args)\n                                if ret:\n                                    print('Returned', ret)\n                                    raise AssertionError(ret)\n                                assert (not error_expected), 'renewal should have errored'\n                            except:\n                                if (not error_expected):\n                                    raise AssertionError(('Unexpected renewal error:\\n' + traceback.format_exc()))\n        if should_renew:\n            mock_client.obtain_certificate.assert_called_once_with(['isnot.org'])\n        else:\n            self.assertEqual(mock_client.obtain_certificate.call_count, 0)\n    except:\n        self._dump_log()\n        raise\n    finally:\n        if log_out:\n            with open(os.path.join(self.logs_dir, 'letsencrypt.log')) as lf:\n                self.assertTrue((log_out in lf.read()))\n    return (mock_lineage, mock_get_utility, stdout)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse(config):\n    \" Create a validator that does an extract from body and applies a comparator,\\n            Then does comparison vs expected value\\n            Syntax sample:\\n              { jsonpath_mini: 'node.child',\\n                operator: 'eq',\\n                expected: 'myValue'\\n              }\\n        \"\n    output = ComparatorValidator()\n    config = parsing.lowercase_keys(parsing.flatten_dictionaries(config))\n    output.config = config\n    output.extractor = _get_extractor(config)\n    if (output.extractor is None):\n        raise ValueError('Extract function for comparison is not valid or not found!')\n    if ('comparator' not in config):\n        output.comparator_name = 'eq'\n    else:\n        output.comparator_name = config['comparator'].lower()\n    output.comparator = COMPARATORS[output.comparator_name]\n    if (not output.comparator):\n        raise ValueError('Invalid comparator given!')\n    try:\n        expected = config['expected']\n    except KeyError:\n        raise ValueError('No expected value found in comparator validator config, one must be!')\n    if (isinstance(expected, basestring) or isinstance(expected, (int, long, float, complex))):\n        output.expected = expected\n    elif isinstance(expected, dict):\n        expected = parsing.lowercase_keys(expected)\n        template = expected.get('template')\n        if template:\n            if (not isinstance(template, basestring)):\n                raise ValueError(\"Can't template a comparator-validator unless template value is a string\")\n            output.isTemplateExpected = True\n            output.expected = template\n        else:\n            output.expected = _get_extractor(expected)\n            if (not output.expected):\n                raise ValueError(\"Can't supply a non-template, non-extract dictionary to comparator-validator\")\n    return output\n", "label": 1}
{"function": "\n\n@contextmanager\ndef build_with_altered_context(self, name, conf, context, stream, dockerfile, volumes_from=None, command=None, tag=False):\n    conf_image_name = conf.prefixed_image_name\n    new_conf = conf.clone()\n    if (name is not None):\n        if tag:\n            new_name = '{0}-{1}'.format(conf.prefixed_image_name, name)\n        else:\n            new_name = None\n        new_conf.name = name\n        new_conf.image_name = new_name\n        new_conf.container_id = None\n        new_conf.container_name = '{0}-{1}'.format(new_name, str(uuid.uuid1())).replace('/', '__')\n    else:\n        new_name = conf.image_name\n    new_conf.bash = NotSpecified\n    new_conf.command = NotSpecified\n    if (command is not None):\n        new_conf.bash = command\n    if volumes_from:\n        new_conf.volumes = new_conf.volumes.clone()\n        new_conf.volumes.share_with = (list(conf.volumes.share_with) + volumes_from)\n    if (context is not None):\n        maker = context.clone_with_new_dockerfile(conf, dockerfile)\n    else:\n        new_conf.context = Context(enabled=False, parent_dir=new_conf.context.parent_dir)\n        maker = new_conf.make_context(docker_file=dockerfile)\n\n    @contextmanager\n    def remover(conf):\n        (yield)\n    if (new_name is not None):\n        remover = self.remove_replaced_images\n    with remover(new_conf):\n        cached = False\n        with maker as new_context:\n            cached = NormalBuilder(new_name).build(new_conf, new_context, stream)\n            new_conf.image_name = stream.current_container\n    (yield (new_conf, cached))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef from_dict(values):\n    '\\n        Instantiate a BlockadeConfig instance based on\\n        a given dictionary of configuration values\\n        '\n    try:\n        containers = values['containers']\n        parsed_containers = {\n            \n        }\n        for (name, container_dict) in containers.items():\n            try:\n                for cnt in BlockadeContainerConfig.from_dict(name, container_dict):\n                    if cnt.container_name:\n                        cname = cnt.container_name\n                        existing = [c for c in parsed_containers.values() if (c.container_name == cname)]\n                        if existing:\n                            raise BlockadeConfigError((\"Duplicate 'container_name' definition: %s\" % cname))\n                    parsed_containers[cnt.name] = cnt\n            except Exception as err:\n                raise BlockadeConfigError((\"Container '%s' config problem: %s\" % (name, err)))\n        network = values.get('network')\n        if network:\n            defaults = _DEFAULT_NETWORK_CONFIG.copy()\n            defaults.update(network)\n            network = defaults\n        else:\n            network = _DEFAULT_NETWORK_CONFIG.copy()\n        return BlockadeConfig(parsed_containers, network=network)\n    except KeyError as err:\n        raise BlockadeConfigError(('Config missing value: ' + str(err)))\n    except Exception as err:\n        raise BlockadeConfigError(('Failed to load config: ' + str(err)))\n", "label": 1}
{"function": "\n\ndef _stata_elapsed_date_to_datetime(date, fmt):\n    '\\n    Convert from SIF to datetime. http://www.stata.com/help.cgi?datetime\\n\\n    Parameters\\n    ----------\\n    date : int\\n        The Stata Internal Format date to convert to datetime according to fmt\\n    fmt : str\\n        The format to convert to. Can be, tc, td, tw, tm, tq, th, ty\\n\\n    Examples\\n    --------\\n    >>> _stata_elapsed_date_to_datetime(52, \"%tw\")                                datetime.datetime(1961, 1, 1, 0, 0)\\n\\n    Notes\\n    -----\\n    datetime/c - tc\\n        milliseconds since 01jan1960 00:00:00.000, assuming 86,400 s/day\\n    datetime/C - tC - NOT IMPLEMENTED\\n        milliseconds since 01jan1960 00:00:00.000, adjusted for leap seconds\\n    date - td\\n        days since 01jan1960 (01jan1960 = 0)\\n    weekly date - tw\\n        weeks since 1960w1\\n        This assumes 52 weeks in a year, then adds 7 * remainder of the weeks.\\n        The datetime value is the start of the week in terms of days in the\\n        year, not ISO calendar weeks.\\n    monthly date - tm\\n        months since 1960m1\\n    quarterly date - tq\\n        quarters since 1960q1\\n    half-yearly date - th\\n        half-years since 1960h1 yearly\\n    date - ty\\n        years since 0000\\n\\n    If you don\\'t have pandas with datetime support, then you can\\'t do\\n    milliseconds accurately.\\n    '\n    date = int(date)\n    stata_epoch = datetime.datetime(1960, 1, 1)\n    if (fmt in ['%tc', 'tc']):\n        from dateutil.relativedelta import relativedelta\n        return (stata_epoch + relativedelta(microseconds=(date * 1000)))\n    elif (fmt in ['%tC', 'tC']):\n        from warnings import warn\n        warn('Encountered %tC format. Leaving in Stata Internal Format.', UserWarning)\n        return date\n    elif (fmt in ['%td', 'td']):\n        return (stata_epoch + datetime.timedelta(int(date)))\n    elif (fmt in ['%tw', 'tw']):\n        year = datetime.datetime((stata_epoch.year + (date // 52)), 1, 1)\n        day_delta = ((date % 52) * 7)\n        return (year + datetime.timedelta(int(day_delta)))\n    elif (fmt in ['%tm', 'tm']):\n        year = (stata_epoch.year + (date // 12))\n        month_delta = ((date % 12) + 1)\n        return datetime.datetime(year, month_delta, 1)\n    elif (fmt in ['%tq', 'tq']):\n        year = (stata_epoch.year + (date // 4))\n        month_delta = (((date % 4) * 3) + 1)\n        return datetime.datetime(year, month_delta, 1)\n    elif (fmt in ['%th', 'th']):\n        year = (stata_epoch.year + (date // 2))\n        month_delta = (((date % 2) * 6) + 1)\n        return datetime.datetime(year, month_delta, 1)\n    elif (fmt in ['%ty', 'ty']):\n        if (date > 0):\n            return datetime.datetime(date, 1, 1)\n        else:\n            raise ValueError('Year 0 and before not implemented')\n    else:\n        raise ValueError(('Date fmt %s not understood' % fmt))\n", "label": 1}
{"function": "\n\ndef test_success(self):\n    count = CompatReport.objects.count()\n    r = self.client.post(self.url, self.json, content_type='application/json')\n    assert (r.status_code == 204)\n    assert (CompatReport.objects.count() == (count + 1))\n    cr = CompatReport.objects.order_by('-id')[0]\n    assert (cr.app_build == incoming_data['appBuild'])\n    assert (cr.app_guid == incoming_data['appGUID'])\n    assert (cr.works_properly == incoming_data['worksProperly'])\n    assert (cr.comments == incoming_data['comments'])\n    assert (cr.client_ip == '127.0.0.1')\n    vals = CompatReport.objects.filter(id=cr.id).values('other_addons')\n    assert (vals[0]['other_addons'] == json.dumps(incoming_data['otherAddons'], separators=(',', ':')))\n", "label": 1}
{"function": "\n\ndef _sync_recv_msg(self):\n    \"Internal use only; use 'recv_msg' instead.\\n\\n        Synchronous version of async_recv_msg.\\n        \"\n    n = struct.calcsize('>L')\n    try:\n        data = self._sync_recvall(n)\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            return ''\n        else:\n            raise\n    if (len(data) != n):\n        return ''\n    n = struct.unpack('>L', data)[0]\n    assert (n >= 0)\n    try:\n        data = self._sync_recvall(n)\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            return ''\n        else:\n            raise\n    if (len(data) != n):\n        return ''\n    return data\n", "label": 1}
{"function": "\n\ndef get_header(self, create=True):\n    try:\n        hdr = self.metadata['header']\n        if (self.inherit_primary_header and (self._primary_hdr is not None)):\n            displayhdr = AstroHeader()\n            for key in hdr.keyorder:\n                card = hdr.get_card(key)\n                bnch = displayhdr.__setitem__(card.key, card.value)\n                bnch.comment = card.comment\n            for key in self._primary_hdr.keyorder:\n                if (key not in hdr):\n                    card = self._primary_hdr.get_card(key)\n                    bnch = displayhdr.__setitem__(card.key, card.value)\n                    bnch.comment = card.comment\n        else:\n            displayhdr = hdr\n    except KeyError as e:\n        if (not create):\n            raise e\n        hdr = AstroHeader()\n        self.metadata['header'] = hdr\n        displayhdr = hdr\n    return displayhdr\n", "label": 1}
{"function": "\n\ndef new(self, key=None, data=None, content_type='application/json', encoded_data=None):\n    'A shortcut for manually instantiating a new\\n        :class:`~riak.riak_object.RiakObject` or a new\\n        :class:`~riak.datatypes.Datatype`, based on the presence and value\\n        of the :attr:`datatype <BucketType.datatype>` bucket property. When\\n        the bucket contains a :class:`~riak.datatypes.Datatype`, all\\n        arguments are ignored except ``key``, otherwise they are used to\\n        initialize the :class:`~riak.riak_object.RiakObject`.\\n\\n        :param key: Name of the key. Leaving this to be None (default)\\n                    will make Riak generate the key on store.\\n        :type key: str\\n        :param data: The data to store in a\\n           :class:`~riak.riak_object.RiakObject`, see\\n           :attr:`RiakObject.data <riak.riak_object.RiakObject.data>`.\\n        :type data: object\\n        :param content_type: The media type of the data stored in the\\n           :class:`~riak.riak_object.RiakObject`, see\\n           :attr:`RiakObject.content_type\\n           <riak.riak_object.RiakObject.content_type>`.\\n        :type content_type: str\\n        :param encoded_data: The encoded data to store in a\\n           :class:`~riak.riak_object.RiakObject`, see\\n           :attr:`RiakObject.encoded_data\\n           <riak.riak_object.RiakObject.encoded_data>`.\\n        :type encoded_data: str\\n        :rtype: :class:`~riak.riak_object.RiakObject` or\\n                :class:`~riak.datatypes.Datatype`\\n\\n        '\n    from riak import RiakObject\n    if self.bucket_type.datatype:\n        return TYPES[self.bucket_type.datatype](bucket=self, key=key)\n    if PY2:\n        try:\n            if isinstance(data, string_types):\n                data = data.encode('ascii')\n        except UnicodeError:\n            raise TypeError('Unicode data values are not supported.')\n    obj = RiakObject(self._client, self, key)\n    obj.content_type = content_type\n    if (data is not None):\n        obj.data = data\n    if (encoded_data is not None):\n        obj.encoded_data = encoded_data\n    return obj\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.args = args\n    self.options = options\n    self.mailer = utils.mail.Mailer(options)\n    self.stdout.write(('args: %r\\n' % (args,)))\n    self.stdout.write(('options: %r\\n' % options))\n    self.collector_config = CollectorConfig(self.args, self.options)\n    self.last_status = {\n        \n    }\n    try:\n        self.last_status = json.load(open(STATUS_FILE_PATH, 'r'))\n    except Exception as e:\n        logger.warning('Failed to load status file: %r', e)\n    status_checker = StatusChecker(self.collector_config, self.last_status, self.options, self.mailer)\n    while True:\n        try:\n            status_checker.check_status()\n        except Exception as e:\n            logger.warning('OWL cluster checker error: %r', e)\n            admin_email = ''\n            try:\n                admin_email = settings.ADMINS[0][1]\n            except:\n                pass\n            self.mailer.send_email(subject='OWL cluster check error', content=repr(e), to_email=admin_email)\n        time.sleep(int(self.options['period']))\n", "label": 0}
{"function": "\n\ndef _unpack_response(response, cursor_id=None, as_class=dict, tz_aware=False, uuid_subtype=OLD_UUID_SUBTYPE, compile_re=True):\n    'Unpack a response from the database.\\n\\n    Check the response for errors and unpack, returning a dictionary\\n    containing the response data.\\n\\n    :Parameters:\\n      - `response`: byte string as returned from the database\\n      - `cursor_id` (optional): cursor_id we sent to get this response -\\n        used for raising an informative exception when we get cursor id not\\n        valid at server response\\n      - `as_class` (optional): class to use for resulting documents\\n    '\n    response_flag = struct.unpack('<i', response[:4])[0]\n    if (response_flag & 1):\n        assert (cursor_id is not None)\n        raise CursorNotFound((\"cursor id '%s' not valid at server\" % cursor_id))\n    elif (response_flag & 2):\n        error_object = bson.BSON(response[20:]).decode()\n        if error_object['$err'].startswith('not master'):\n            raise AutoReconnect(error_object['$err'])\n        elif (error_object.get('code') == 50):\n            raise ExecutionTimeout(error_object.get('$err'), error_object.get('code'), error_object)\n        raise OperationFailure(('database error: %s' % error_object.get('$err')), error_object.get('code'), error_object)\n    result = {\n        \n    }\n    result['cursor_id'] = struct.unpack('<q', response[4:12])[0]\n    result['starting_from'] = struct.unpack('<i', response[12:16])[0]\n    result['number_returned'] = struct.unpack('<i', response[16:20])[0]\n    result['data'] = bson.decode_all(response[20:], as_class, tz_aware, uuid_subtype, compile_re)\n    assert (len(result['data']) == result['number_returned'])\n    return result\n", "label": 0}
{"function": "\n\ndef get_cat_model(model):\n    '\\n    Return a class from a string or class\\n    '\n    try:\n        if isinstance(model, string_types):\n            model_class = apps.get_model(*model.split('.'))\n        elif issubclass(model, CategoryBase):\n            model_class = model\n        if (model_class is None):\n            raise TypeError\n    except TypeError:\n        raise TemplateSyntaxError(('Unknown model submitted: %s' % model))\n    return model_class\n", "label": 0}
{"function": "\n\ndef optwrap(text):\n    'Wrap all paragraphs in the provided text.'\n    if (not BODY_WIDTH):\n        return text\n    assert wrap, 'Requires Python 2.3.'\n    result = ''\n    newlines = 0\n    for para in text.split('\\n'):\n        if (len(para) > 0):\n            if ((para[0] != ' ') and (para[0] != '-') and (para[0] != '*')):\n                for line in wrap(para, BODY_WIDTH):\n                    result += (line + '\\n')\n                result += '\\n'\n                newlines = 2\n            elif (not onlywhite(para)):\n                result += (para + '\\n')\n                newlines = 1\n        elif (newlines < 2):\n            result += '\\n'\n            newlines += 1\n    return result\n", "label": 1}
{"function": "\n\ndef _parse_settings_bond_0(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond0.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '0',\n    }\n    valid = ['list of ips (up to 16)']\n    if ('arp_ip_target' in opts):\n        if isinstance(opts['arp_ip_target'], list):\n            if (1 <= len(opts['arp_ip_target']) <= 16):\n                bond.update({\n                    'arp_ip_target': '',\n                })\n                for ip in opts['arp_ip_target']:\n                    if (len(bond['arp_ip_target']) > 0):\n                        bond['arp_ip_target'] = ((bond['arp_ip_target'] + ',') + ip)\n                    else:\n                        bond['arp_ip_target'] = ip\n            else:\n                _raise_error_iface(iface, 'arp_ip_target', valid)\n        else:\n            _raise_error_iface(iface, 'arp_ip_target', valid)\n    else:\n        _raise_error_iface(iface, 'arp_ip_target', valid)\n    if ('arp_interval' in opts):\n        try:\n            int(opts['arp_interval'])\n            bond.update({\n                'arp_interval': opts['arp_interval'],\n            })\n        except ValueError:\n            _raise_error_iface(iface, 'arp_interval', ['integer'])\n    else:\n        _log_default_iface(iface, 'arp_interval', bond_def['arp_interval'])\n        bond.update({\n            'arp_interval': bond_def['arp_interval'],\n        })\n    return bond\n", "label": 1}
{"function": "\n\ndef get_cache(backend, **kwargs):\n    \"\\n    Function to load a cache backend dynamically. This is flexible by design\\n    to allow different use cases:\\n\\n    To load a backend with the old URI-based notation::\\n\\n        cache = get_cache('locmem://')\\n\\n    To load a backend that is pre-defined in the settings::\\n\\n        cache = get_cache('default')\\n\\n    To load a backend with its dotted import path,\\n    including arbitrary options::\\n\\n        cache = get_cache('django.core.cache.backends.memcached.MemcachedCache', **{\\n            'LOCATION': '127.0.0.1:11211', 'TIMEOUT': 30,\\n        })\\n\\n    \"\n    try:\n        if ('://' in backend):\n            (backend, location, params) = parse_backend_uri(backend)\n            if (backend in BACKENDS):\n                backend = ('django.core.cache.backends.%s' % BACKENDS[backend])\n            params.update(kwargs)\n            mod = importlib.import_module(backend)\n            backend_cls = mod.CacheClass\n        else:\n            (backend, location, params) = parse_backend_conf(backend, **kwargs)\n            (mod_path, cls_name) = backend.rsplit('.', 1)\n            mod = importlib.import_module(mod_path)\n            backend_cls = getattr(mod, cls_name)\n    except (AttributeError, ImportError) as e:\n        raise InvalidCacheBackendError((\"Could not find backend '%s': %s\" % (backend, e)))\n    cache = backend_cls(location, params)\n    if hasattr(cache, 'close'):\n        signals.request_finished.connect(cache.close)\n    return cache\n", "label": 0}
{"function": "\n\ndef inNodeList(self, nodeList, nodeToFind):\n    assert (type(nodeToFind) is PomTreeNode)\n    for node in nodeList:\n        if ((node.getGroupId() == nodeToFind.getGroupId()) and (node.getArtifactId() == nodeToFind.getArtifactId())):\n            self.nodeList.append(node)\n            return node\n    self.nodeList.append(nodeToFind)\n    return nodeToFind\n", "label": 0}
{"function": "\n\ndef generate_gcd_app(app_id):\n    'Generates an app in tmp for a cloud datastore implementation.'\n    if (sys.platform == 'win32'):\n        user_format = ''\n    else:\n        try:\n            user_name = getpass.getuser()\n        except Exception:\n            user_format = ''\n        else:\n            user_format = ('.%s' % user_name)\n    tempdir = tempfile.gettempdir()\n    version = sdk_update_checker.GetVersionObject()\n    sdk_version = (version['release'] if version else 'unknown')\n    gcd_path = os.path.join(tempdir, ('appengine-gcd-war.%s%s%s' % (sdk_version, app_id, user_format)))\n    if (not os.path.exists(gcd_path)):\n        os.mkdir(gcd_path)\n        webinf_path = os.path.join(gcd_path, 'WEB-INF')\n        os.mkdir(webinf_path)\n        filter_path = os.path.join(webinf_path, 'lib')\n        os.mkdir(filter_path)\n        if runtime_factories.java_runtime:\n            filter_jar = _get_filter_jar()\n            shutil.copy(filter_jar, filter_path)\n    with open(os.path.join(gcd_path, 'WEB-INF', 'web.xml'), 'w') as f:\n        f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<web-app version=\"2.5\" xmlns=\"http://java.sun.com/xml/ns/javaee\"\\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n         xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee\\n         http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\">\\n\\n  <security-constraint>\\n    <web-resource-collection>\\n      <web-resource-name>datastore_constraint</web-resource-name>\\n      <url-pattern>/datastore/*</url-pattern>\\n    </web-resource-collection>\\n    <user-data-constraint>\\n      <transport-guarantee>CONFIDENTIAL</transport-guarantee>\\n    </user-data-constraint>\\n  </security-constraint>\\n\\n  <filter>\\n    <filter-name>ProtoJsonFilter</filter-name>\\n    <filter-class>\\n      com.google.apphosting.client.datastoreservice.app.filter.ProtoJsonFilter\\n    </filter-class>\\n  </filter>\\n\\n  <filter-mapping>\\n    <filter-name>ProtoJsonFilter</filter-name>\\n    <url-pattern>/datastore/*</url-pattern>\\n  </filter-mapping>\\n\\n  <servlet>\\n    <servlet-name>DatastoreApiServlet</servlet-name>\\n    <servlet-class>\\n      com.google.apphosting.client.datastoreservice.app.DatastoreApiServlet\\n    </servlet-class>\\n    <load-on-startup>1</load-on-startup>\\n  </servlet>\\n\\n  <servlet-mapping>\\n    <servlet-name>DatastoreApiServlet</servlet-name>\\n    <url-pattern>/datastore/*</url-pattern>\\n  </servlet-mapping>\\n\\n</web-app>')\n    gcd_app_xml = os.path.join(gcd_path, 'WEB-INF', 'appengine-web.xml')\n    with open(gcd_app_xml, 'w') as f:\n        f.write(('<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<appengine-web-app xmlns=\"http://appengine.google.com/ns/1.0\">\\n  <application>%s</application>\\n  <version>1</version>\\n  <module>google-cloud-datastore</module>\\n\\n  <precompilation-enabled>true</precompilation-enabled>\\n  <threadsafe>true</threadsafe>\\n\\n</appengine-web-app>' % app_id))\n    return gcd_app_xml\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if self.request.user.is_anonymous():\n        self.instance.author_ip = self.request.META['REMOTE_ADDR']\n    else:\n        self.instance.author = self.request.user\n    if (not self.page.pk):\n        self.page.save()\n    self.instance.page = self.page\n    (text, created) = Text.objects.get_or_create(content=self.cleaned_data['text'])\n    self.instance.text = text\n    self.instance.comment = self.cleaned_data['comment']\n    if (not created):\n        try:\n            revision = text.revision_set.latest('pub_date')\n            self.instance.comment = (_('Reverted to revision %(revision)s by %(author)s') % {\n                'revision': revision.pk,\n                'author': (revision.author or revision.author_ip),\n            })\n        except ObjectDoesNotExist:\n            pass\n    self.instance.save()\n", "label": 0}
{"function": "\n\ndef mapKeyword2Script(path):\n    'collect keywords from scripts.'\n    map_keyword2script = collections.defaultdict(list)\n    for script in glob.glob(os.path.join(path, '*.py')):\n        s = os.path.basename(script)[:(- 3)]\n        with open(script, 'r') as inf:\n            data = [x for x in inf.readlines(10000) if x.startswith(':Tags:')]\n            if data:\n                keywords = [x.strip() for x in data[0][6:].split(' ')]\n                for x in keywords:\n                    if x:\n                        map_keyword2script[x].append(s)\n    return map_keyword2script\n", "label": 1}
{"function": "\n\ndef wait(self, timeout=None):\n    '\\n        Return the result, or throw the exception if result is a failure.\\n\\n        It may take an unknown amount of time to return the result, so a\\n        timeout option is provided. If the given number of seconds pass with\\n        no result, a TimeoutError will be thrown.\\n\\n        If a previous call timed out, additional calls to this function will\\n        still wait for a result and return it if available. If a result was\\n        returned or raised on one call, additional calls will return/raise the\\n        same result.\\n        '\n    if threadable.isInIOThread():\n        raise RuntimeError('EventualResult.wait() must not be run in the reactor thread.')\n    if imp.lock_held():\n        try:\n            imp.release_lock()\n        except RuntimeError:\n            pass\n        else:\n            raise RuntimeError('EventualResult.wait() must not be run at module import time.')\n    result = self._result(timeout)\n    if isinstance(result, Failure):\n        result.raiseException()\n    return result\n", "label": 0}
{"function": "\n\ndef do_jumpbox_connections(via, dest):\n    k = jb.cluster.locate(via)\n    if (not k):\n        print('Jumpbox', via, 'not found')\n        return\n    jump = jb.cluster.connections[k]\n    if (not jump.is_authenticated()):\n        print('Jumpbox', via, 'does not have authenticated connection')\n        print('Skipping connections through jumpbox to:', dest)\n        return\n    for y in dest:\n        try:\n            s = jump.open_channel('direct-tcpip', (y, 22), ('', 0))\n            (yield ('--'.join([via, y]), y, s))\n        except Exception as e:\n            print(('Unable to connect to %s via jumpbox %s' % (y, via)))\n            print(repr(e))\n", "label": 0}
{"function": "\n\ndef test_define(self):\n    assert (self.l2_net_dev.forward.mode == 'nat')\n    self.l2_net_dev.define()\n    assert isinstance(self.l2_net_dev.uuid, str)\n    assert (len(self.l2_net_dev.uuid) == 36)\n    assert (self.l2_net_dev.network_name() == 'test_env_test_l2_net_dev')\n    assert (self.l2_net_dev.exists() is True)\n    assert (self.l2_net_dev.is_active() == 0)\n    assert (self.l2_net_dev.bridge_name() == 'virbr1')\n    assert (self.l2_net_dev._libvirt_network.autostart() == 1)\n    xml = self.l2_net_dev._libvirt_network.XMLDesc(0)\n    assert (xml == \"<network>\\n  <name>test_env_test_l2_net_dev</name>\\n  <uuid>{0}</uuid>\\n  <forward mode='nat'/>\\n  <bridge name='virbr1' stp='on' delay='0'/>\\n  <ip address='172.0.0.1' prefix='24'>\\n  </ip>\\n</network>\\n\".format(self.l2_net_dev.uuid))\n", "label": 1}
{"function": "\n\ndef _get_conn(self, timeout=None):\n    '\\n        Get a connection. Will return a pooled connection if one is available.\\n\\n        If no connections are available and :prop:`.block` is ``False``, then a\\n        fresh connection is returned.\\n\\n        :param timeout:\\n            Seconds to wait before giving up and raising\\n            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and\\n            :prop:`.block` is ``True``.\\n        '\n    conn = None\n    try:\n        conn = self.pool.get(block=self.block, timeout=timeout)\n    except AttributeError:\n        raise ClosedPoolError(self, 'Pool is closed.')\n    except Empty:\n        if self.block:\n            raise EmptyPoolError(self, 'Pool reached maximum size and no more connections are allowed.')\n        pass\n    if (conn and is_connection_dropped(conn)):\n        log.info(('Resetting dropped connection: %s' % self.host))\n        conn.close()\n    return (conn or self._new_conn())\n", "label": 0}
{"function": "\n\ndef get_placeholders(template_name):\n    'Return a list of PlaceholderNode found in the given template.\\n\\n    :param template_name: the name of the template file\\n    '\n    dummy_context.template = template.Template('')\n    try:\n        temp_wrapper = template.loader.get_template(template_name)\n    except template.TemplateDoesNotExist:\n        return []\n    (plist, blist) = ([], [])\n    temp = temp_wrapper.template\n    _placeholders_recursif(temp.nodelist, plist, blist)\n    previous = {\n        \n    }\n    block_to_remove = []\n    for block in blist:\n        if (block.name in previous):\n            if (not hasattr(block, 'has_super_var')):\n                block_to_remove.append(previous[block.name])\n        previous[block.name] = block\n\n    def keep(p):\n        return (not (p.found_in_block in block_to_remove))\n    placeholders = [p for p in plist if keep(p)]\n    names = []\n    pfiltered = []\n    for p in placeholders:\n        if (p.ctype not in names):\n            pfiltered.append(p)\n            names.append(p.ctype)\n    return pfiltered\n", "label": 1}
{"function": "\n\ndef interface_name(self):\n    self.update_attrs()\n    try:\n        (itype, primary, alias) = ('', '', '')\n        itype = self.attrs.interface_type\n        primary = self.attrs.primary\n        alias = self.attrs.alias\n    except AttributeError:\n        pass\n    if ((itype == '') or (primary == '')):\n        return 'None'\n    elif (alias == ''):\n        return '{0}{1}'.format(itype, primary)\n    else:\n        return '{0}{1}.{2}'.format(itype, primary, alias)\n", "label": 0}
{"function": "\n\n@app.route('/', methods=['POST'])\ndef hook_listen():\n    if (request.method == 'POST'):\n        token = request.args.get('token')\n        if (token == config['token']):\n            hook = request.args.get('hook')\n            if hook:\n                hook_value = config['hooks'].get(hook)\n                if hook_value:\n                    try:\n                        subprocess.call(hook_value)\n                        return (jsonify(success=True), 200)\n                    except OSError as e:\n                        return (jsonify(success=False, error=str(e)), 400)\n                else:\n                    return (jsonify(success=False, error='Hook not found'), 404)\n            else:\n                return (jsonify(success=False, error='Invalid request: missing hook'), 400)\n        else:\n            return (jsonify(success=False, error='Invalid token'), 400)\n", "label": 0}
{"function": "\n\ndef test_index(self):\n    index_name = 'test_idxV'\n    james = self.vertices.create({\n        'name': 'James',\n    })\n    self.vertices.index.put(james._id, 'name', 'James')\n    self.vertices.index.put(james._id, 'location', 'Dallas')\n    results = self.vertices.index.lookup('name', 'James')\n    results = list(results)\n    assert (len(results) == 1)\n    assert (results[0].name == 'James')\n    total_size = self.vertices.index.count('name', 'James')\n    assert (total_size == 1)\n    i2 = self.indicesV.get(index_name)\n    assert (self.vertices.index.index_name == i2.index_name)\n    self.indicesV.delete(index_name)\n", "label": 0}
{"function": "\n\ndef get_initial(self):\n    initial = super(CreateProjectView, self).get_initial()\n    domain = api.keystone.get_default_domain(self.request)\n    initial['domain_id'] = domain.id\n    initial['domain_name'] = domain.name\n    try:\n        quota_defaults = quotas.get_default_quota_data(self.request)\n        try:\n            if (api.base.is_service_enabled(self.request, 'network') and api.neutron.is_quotas_extension_supported(self.request)):\n                project_id = self.request.user.project_id\n                quota_defaults += api.neutron.tenant_quota_get(self.request, tenant_id=project_id)\n        except Exception:\n            error_msg = _('Unable to retrieve default Neutron quota values.')\n            self.add_error_to_step(error_msg, 'create_quotas')\n        for field in quotas.QUOTA_FIELDS:\n            initial[field] = quota_defaults.get(field).limit\n    except Exception:\n        error_msg = _('Unable to retrieve default quota values.')\n        self.add_error_to_step(error_msg, 'create_quotas')\n    return initial\n", "label": 0}
{"function": "\n\ndef do(self, workflow_dict):\n    try:\n        workflow_dict['disks'] = []\n        for instance in workflow_dict['instances']:\n            host = instance.hostname\n            if instance.is_arbiter:\n                LOG.info('Do not creat nfsaas disk for Arbiter...')\n                continue\n            LOG.info('Creating nfsaas disk...')\n            disk = NfsaasProvider().create_disk(environment=workflow_dict['environment'], plan=workflow_dict['plan'], host=host)\n            if (not disk):\n                return False\n            workflow_dict['disks'].append(disk)\n        return True\n    except Exception:\n        traceback = full_stack()\n        workflow_dict['exceptions']['error_codes'].append(DBAAS_0009)\n        workflow_dict['exceptions']['traceback'].append(traceback)\n        return False\n", "label": 0}
{"function": "\n\ndef test_generate_with_params(self):\n    api = APISpecification(version='v1', base_url='http://api.globo.com')\n    api.add_resource(Resource('dogs', paths=[Path('/dogs/{key}', params=[Param('key')], methods=[Method('POST'), Method('GET')])]))\n    result = self.gen(api)\n    doc = ElementTree.fromstring(result)\n    resources = doc.getchildren()[0]\n    resource = resources.getchildren()[0]\n    param = resource.getchildren()[0]\n    assert param.tag.endswith('param')\n    assert (param.get('name') == 'key')\n    assert (param.get('required') == 'true')\n    assert (param.get('type') == 'xsd:string')\n    assert (param.get('style') == 'template')\n    method_1 = resource.getchildren()[1]\n    assert method_1.tag.endswith('method')\n    assert (method_1.get('name') == 'POST')\n    method_2 = resource.getchildren()[2]\n    assert method_2.tag.endswith('method')\n    assert (method_2.get('name') == 'GET')\n", "label": 1}
{"function": "\n\ndef visit_Const(self, expr):\n    t = expr.type\n    c = t.__class__\n    if (c == BoolT):\n        return ('1' if expr.value else '0')\n    elif (c == NoneT):\n        return '0'\n    assert isinstance(t, ScalarT), (\"Don't know how to translate Const %s : %s\" % (expr, t))\n    v = expr.value\n    if np.isinf(v):\n        return 'INFINITY'\n    elif np.isnan(v):\n        return 'NAN'\n    return ('%s' % expr.value)\n", "label": 0}
{"function": "\n\ndef main(world_folder, chunkx, chunkz, height):\n    world = nbt.world.WorldFolder(world_folder)\n    if (not isinstance(world, nbt.world.AnvilWorldFolder)):\n        print(('%s is not an Anvil world' % world_folder))\n        return 65\n    (chunky, yoffset) = divmod(height, 16)\n    try:\n        section = get_section(world, chunkx, chunky, chunkz)\n        try:\n            blocks = section['Blocks'].value\n            data = section['Data'].value\n        except (KeyError, AttributeError):\n            blocks = bytearray(4096)\n            data = bytearray(2048)\n        try:\n            add = section['Add'].value\n        except (KeyError, AttributeError):\n            add = None\n    except nbt.region.InconceivedChunk:\n        print('Section undefined')\n        blocks = bytearray(4096)\n        data = bytearray(2048)\n        add = None\n    print_chunklayer(blocks, data, add, yoffset)\n    return 0\n", "label": 0}
{"function": "\n\ndef test_testing_disabled_flag(self):\n    Flag.objects.create(name='foo')\n    request = get(dwft_foo='1')\n    assert (not waffle.flag_is_active(request, 'foo'))\n    assert (not hasattr(request, 'waffle_tests'))\n    request = get(dwft_foo='0')\n    assert (not waffle.flag_is_active(request, 'foo'))\n    assert (not hasattr(request, 'waffle_tests'))\n", "label": 0}
{"function": "\n\ndef _update_tickets(self, tickets, changeset, comment, date):\n    'Update the tickets with the given comment.'\n    authname = self._authname(changeset)\n    perm = PermissionCache(self.env, authname)\n    for (tkt_id, cmds) in tickets.iteritems():\n        try:\n            self.log.debug('Updating ticket #%d', tkt_id)\n            save = False\n            with self.env.db_transaction:\n                ticket = Ticket(self.env, tkt_id)\n                ticket_perm = perm(ticket.resource)\n                for cmd in cmds:\n                    if (cmd(ticket, changeset, ticket_perm) is not False):\n                        save = True\n                if save:\n                    ticket.save_changes(authname, comment, date)\n            if save:\n                self._notify(ticket, date, changeset.author, comment)\n        except Exception as e:\n            self.log.error('Unexpected error while processing ticket #%s: %s', tkt_id, exception_to_unicode(e))\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    try:\n        record.message = record.getMessage()\n    except Exception as e:\n        record.message = ('Bad message (%r): %r' % (e, record.__dict__))\n    record.asctime = time.strftime('%y%m%d %H:%M:%S', self.converter(record.created))\n    prefix = ('[%(levelname)1.1s %(asctime)s %(module)s:%(lineno)d]' % record.__dict__)\n    if self._color:\n        prefix = ((self._colors.get(record.levelno, self._normal) + prefix) + self._normal)\n\n    def safe_unicode(s):\n        try:\n            return unicode(s)\n        except UnicodeDecodeError:\n            return repr(s)\n    formatted = ((prefix + ' ') + safe_unicode(record.message))\n    if record.exc_info:\n        if (not record.exc_text):\n            record.exc_text = self.formatException(record.exc_info)\n    if record.exc_text:\n        lines = [formatted.rstrip()]\n        lines.extend((safe_unicode(ln) for ln in record.exc_text.split('\\n')))\n        formatted = '\\n'.join(lines)\n    return formatted.replace('\\n', '\\n    ')\n", "label": 0}
{"function": "\n\ndef sendBanner(self):\n    '\\n        Display contents of <honeyfs>/etc/issue.net\\n        '\n    if self.bannerSent:\n        return\n    self.bannerSent = True\n    try:\n        honeyfs = self.portal.realm.cfg.get('honeypot', 'contents_path')\n        issuefile = (honeyfs + '/etc/issue.net')\n        data = open(issuefile).read()\n    except IOError:\n        return\n    if ((not data) or (not len(data.strip()))):\n        return\n    self.transport.sendPacket(userauth.MSG_USERAUTH_BANNER, (NS(data) + NS('en')))\n", "label": 0}
{"function": "\n\ndef decimal_to_ratio(d):\n    'Convert Decimal d to exact integer ratio (numerator, denominator).\\n    '\n    (sign, digits, exp) = d.as_tuple()\n    if (exp in ('F', 'n', 'N')):\n        assert (not d.is_finite())\n        raise ValueError\n    num = 0\n    for digit in digits:\n        num = ((num * 10) + digit)\n    if sign:\n        num = (- num)\n    den = (10 ** (- exp))\n    return (num, den)\n", "label": 0}
{"function": "\n\ndef _CreateNewHandle(self, extension):\n    cdescriptor = extension._cdescriptor\n    if ((cdescriptor.label != _LABEL_REPEATED) and (cdescriptor.cpp_type == _CPPTYPE_MESSAGE)):\n        cmessage = self._cmsg.NewSubMessage(cdescriptor)\n        return extension.message_type._concrete_class(__cmessage=cmessage)\n    if (cdescriptor.label == _LABEL_REPEATED):\n        if (cdescriptor.cpp_type == _CPPTYPE_MESSAGE):\n            return RepeatedCompositeContainer(self._message, cdescriptor, extension.message_type._concrete_class)\n        else:\n            return RepeatedScalarContainer(self._message, cdescriptor)\n    assert False\n    return None\n", "label": 0}
{"function": "\n\ndef __call__(self, form, field):\n    try:\n        obj = self.get_session().query(self.model).filter((self.column == field.data)).one()\n        if ((not hasattr(form, '_obj')) or (not (form._obj == obj))):\n            if (self.message is None):\n                self.message = field.gettext('Already exists.')\n            raise ValidationError(self.message)\n    except NoResultFound:\n        pass\n", "label": 0}
{"function": "\n\n@register.inclusion_tag('auxiliary/_bar.html')\ndef bar(quantity, start, end, bar_class=None, show_label=True):\n    '\\n    Draws a bar.\\n\\n    :param quantity: The quantity to represent\\n    :param start: Start of scale\\n    :param end: End of scale\\n    :param bar_class: Bootstrap bar class (One of: info, success, warning,\\n                      danger). If not passed, will be calculated from percentage\\n    :param show_label: Show the descriptive label ?\\n    '\n    assert (quantity <= end), 'bar: quantity > end'\n    try:\n        value = (((quantity - start) * 100) / (end - start))\n    except (TypeError, ZeroDivisionError):\n        return {\n            'applicable': False,\n        }\n    VALUES = ((20, 'danger', _('Extremely below average')), (40, 'warning', _('Below average')), (60, 'info', _('Average')), (80, 'info', _('Above average')), (101, 'success', _('Extremely above average')))\n    for (boundary, css_class, label) in VALUES:\n        if (value < boundary):\n            break\n    if (not bar_class):\n        bar_class = css_class\n    if (not show_label):\n        label = ''\n    return {\n        'width': value,\n        'bar_class': bar_class,\n        'label': label,\n        'quantity': quantity,\n        'applicable': True,\n    }\n", "label": 0}
{"function": "\n\ndef RunTest():\n    try:\n        arcpy.AddMessage('Starting Test: TestRemoveDuplicateData')\n        inputTrackPointsFC = os.path.join(TestUtilities.inputGDB, 'GPSData')\n        outputPointsFC = os.path.join(TestUtilities.outputGDB, 'GPSData_Duplicates')\n        try:\n            desc = arcpy.Describe(outputPointsFC)\n            if (desc != None):\n                print(('Deleting: ' + str(outputPointsFC)))\n                arcpy.Delete_management(outputPointsFC)\n        except:\n            print(('Delete failed for: ' + str(outputPointsFC)))\n        try:\n            print(((('Copying ' + str(inputTrackPointsFC)) + ' --> ') + str(outputPointsFC)))\n            arcpy.Copy_management(inputTrackPointsFC, outputPointsFC)\n        except:\n            print(((('Copy failed for: ' + str(inputTrackPointsFC)) + ':') + str(outputPointsFC)))\n        toolbox = TestUtilities.toolbox\n        print(('Running from: ' + str(TestUtilities.currentPath)))\n        print(('Geodatabase path: ' + str(TestUtilities.geodatabasePath)))\n        arcpy.env.overwriteOutput = True\n        arcpy.ImportToolbox(toolbox, 'pdc')\n        arcpy.RemoveDuplicateGPSData_pdc(outputPointsFC)\n        inputFeatureCount = int(arcpy.GetCount_management(inputTrackPointsFC).getOutput(0))\n        print(('Input FeatureClass: ' + str(inputTrackPointsFC)))\n        print(('Input Feature Count: ' + str(inputFeatureCount)))\n        if (inputFeatureCount < 1):\n            print(('Invalid Output Feature Count: ' + str(inputFeatureCount)))\n            raise Exception('Test Failed')\n        outputFeatureCount = int(arcpy.GetCount_management(outputPointsFC).getOutput(0))\n        print(('Output FeatureClass: ' + str(outputPointsFC)))\n        print(('Output Feature Count: ' + str(outputFeatureCount)))\n        if (outputFeatureCount >= inputFeatureCount):\n            print(('Output Feature Count >= Input Feature Count: ' + str(outputFeatureCount)))\n            raise Exception('Test Failed')\n        print('Test Successful')\n    except arcpy.ExecuteError:\n        msgs = arcpy.GetMessages()\n        arcpy.AddError(msgs)\n        sys.exit((- 1))\n    except Exception as e:\n        tb = sys.exc_info()[2]\n        tbinfo = traceback.format_tb(tb)[0]\n        pymsg = ((('PYTHON ERRORS:\\nTraceback info:\\n' + tbinfo) + '\\nError Info:\\n') + str(sys.exc_info()[1]))\n        msgs = (('ArcPy ERRORS:\\n' + arcpy.GetMessages()) + '\\n')\n        arcpy.AddError(pymsg)\n        arcpy.AddError(msgs)\n        sys.exit((- 1))\n", "label": 1}
{"function": "\n\ndef parse(self, response):\n    checker = response.request.meta['checker']\n    rpt = response.request.meta['rpt']\n    if (checker.checker_type == '4'):\n        return\n    try:\n        test_select = response.xpath(checker.checker_x_path).extract()\n    except ValueError:\n        self.log('Invalid x_path ({c})!'.format(c=str(checker)), logging.ERROR)\n        return\n    if (len(test_select) == 0):\n        self.log('Checker configuration not working (no elements found for xpath on reference url page) ({c})!'.format(c=str(checker)), logging.ERROR)\n    elif (checker.checker_x_path_result == ''):\n        self.log('Checker configuration working (elements for x_path found on reference url page (no x_path result defined)) ({c}).'.format(c=str(checker)), logging.INFO)\n    elif (test_select[0] != checker.checker_x_path_result):\n        self.log('Checker configuration not working (expected x_path result not found on reference url page) ({c})!'.format(c=str(checker)), logging.ERROR)\n    else:\n        self.log('Checker configuration working (expected x_path result found on reference url page) ({c}).'.format(c=str(checker)), logging.INFO)\n", "label": 0}
{"function": "\n\ndef main():\n    try:\n        _setup()\n        return _run_server()\n    except SystemExit as exit_code:\n        sys.exit(exit_code)\n    except KeyboardInterrupt:\n        listener = get_listener_if_set()\n        if listener:\n            listener.shutdown()\n    except Exception:\n        LOG.exception('(PID=%s) ST2 Stream API quit due to exception.', os.getpid())\n        return 1\n    finally:\n        _teardown()\n", "label": 0}
{"function": "\n\ndef download_default_pages(names, prefix):\n    from httplib import HTTPSConnection\n    host = 'trac.edgewall.org'\n    if (prefix and (not prefix.endswith('/'))):\n        prefix += '/'\n    conn = HTTPSConnection(host)\n    for name in names:\n        if (name in ('WikiStart', 'SandBox')):\n            continue\n        sys.stdout.write(('Downloading %s%s' % (prefix, name)))\n        conn.request('GET', ('/wiki/%s%s?format=txt' % (prefix, name)))\n        response = conn.getresponse()\n        content = response.read()\n        if (prefix and ((response.status != 200) or (not content))):\n            sys.stdout.write((' %s' % name))\n            conn.request('GET', ('/wiki/%s?format=txt' % name))\n            response = conn.getresponse()\n            content = response.read()\n        if ((response.status == 200) and content):\n            with open(('trac/wiki/default-pages/' + name), 'w') as f:\n                lines = content.replace('\\r\\n', '\\n').splitlines(True)\n                f.write(''.join((line for line in lines if (line.strip() != '[[TranslatedPages]]'))))\n            sys.stdout.write('\\tdone.\\n')\n        else:\n            sys.stdout.write('\\tmissing or empty.\\n')\n    conn.close()\n", "label": 1}
{"function": "\n\ndef test_reset_repeating(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, repeat=True, _time_function=time)\n    time.time = 12\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 8)\n    timer.reset()\n    assert (timer.sleep_time() == 10)\n    time.time = 34\n    assert (timer.sleep_time() == 0)\n    timer.reset()\n    assert (timer.sleep_time() == 10)\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 10)\n", "label": 1}
{"function": "\n\ndef test_contsant_sleep(self):\n    time = TestingTimeFunction()\n    manager = TimerManager(_time_function=time)\n    c5 = MockCallback()\n    manager.add_timer(5, c5, repeat=True)\n    c7 = MockCallback()\n    manager.add_timer(7, c7, repeat=True)\n    c13 = MockCallback()\n    manager.add_timer(13, c13, repeat=True)\n    for time.time in xrange(100):\n        manager.run()\n        if (time.time == 42):\n\n            def check_time():\n                assert (time.time == 48)\n            one_shot = MockCallback()\n            manager.add_timer(6, check_time)\n            manager.add_timer(6, one_shot)\n    assert (c5.nb_calls == 19)\n    assert (c7.nb_calls == 14)\n    assert (c13.nb_calls == 7)\n    assert (one_shot.nb_calls == 1)\n", "label": 0}
{"function": "\n\n@idiokit.stream\ndef poll(self):\n    self.log.info('Downloading {0}'.format(self.feed_url))\n    try:\n        (info, fileobj) = (yield utils.fetch_url(self.feed_url))\n    except utils.FetchUrlFailed as fuf:\n        raise bot.PollSkipped('failed to download {0} ({1})'.format(self.feed_url, fuf))\n    self.log.info('Downloaded')\n    for line in fileobj:\n        (url, netloc) = parseURL(line)\n        if (url is None):\n            continue\n        event = events.Event()\n        event.add('url', url)\n        if i_am_a_name(netloc):\n            event.add('domain name', netloc)\n        else:\n            event.add('ip', netloc)\n        event.add('feed', 'vxvault')\n        event.add('feed url', self.feed_url)\n        event.add('type', 'malware')\n        event.add('description', 'This host is most likely hosting a malware URL.')\n        (yield idiokit.send(event))\n", "label": 0}
{"function": "\n\ndef _decode(self, data):\n    '\\n        Safely decodes a encoded text stream back into a list of messages.\\n\\n        If the encoded text stream contained an invalid hash or was in an\\n        invalid format, ``None`` is returned.\\n        '\n    if (not data):\n        return None\n    bits = data.split('$', 1)\n    if (len(bits) == 2):\n        (hash, value) = bits\n        if constant_time_compare(hash, self._hash(value)):\n            try:\n                return json.loads(value, cls=MessageDecoder)\n            except ValueError:\n                pass\n    self.used = True\n    return None\n", "label": 0}
{"function": "\n\ndef _copy_func_details(func, funcopy):\n    funcopy.__name__ = func.__name__\n    funcopy.__doc__ = func.__doc__\n    try:\n        funcopy.__text_signature__ = func.__text_signature__\n    except AttributeError:\n        pass\n    try:\n        funcopy.__module__ = func.__module__\n    except AttributeError:\n        pass\n    try:\n        funcopy.__defaults__ = func.__defaults__\n    except AttributeError:\n        pass\n    try:\n        funcopy.__kwdefaults__ = func.__kwdefaults__\n    except AttributeError:\n        pass\n    if six.PY2:\n        funcopy.func_defaults = func.func_defaults\n        return\n", "label": 0}
{"function": "\n\ndef test_disk_saver_write(tmpdir):\n    '\\n    Ensure that when you close a cassette after changing it it does\\n    rewrite the file\\n    '\n    fname = str(tmpdir.join('synopsis.yaml'))\n    with vcr.use_cassette(fname) as cass:\n        urlopen('http://www.iana.org/domains/reserved').read()\n        assert (cass.play_count == 0)\n    last_mod = os.path.getmtime(fname)\n    time.sleep(1)\n    with vcr.use_cassette(fname, record_mode='any') as cass:\n        urlopen('http://www.iana.org/domains/reserved').read()\n        urlopen('http://httpbin.org/').read()\n        assert (cass.play_count == 1)\n        assert cass.dirty\n    last_mod2 = os.path.getmtime(fname)\n    assert (last_mod != last_mod2)\n", "label": 0}
{"function": "\n\n@classmethod\ndef wrap(cls, app):\n    '\\n        Adds test live server capability to a Flask app module.\\n        \\n        :param app:\\n            A :class:`flask.Flask` app instance.\\n        '\n    (host, port) = cls.parse_args()\n    ssl = cls._argument_parser.parse_args().ssl\n    ssl_context = None\n    if host:\n        if ssl:\n            try:\n                import OpenSSL\n            except ImportError:\n                sys.path.append('/System/Library/Frameworks/Python.framework/Versions/{0}.{1}/Extras/lib/python/'.format(sys.version_info.major, sys.version_info.minor))\n            try:\n                import OpenSSL\n            except ImportError:\n                sys.path.append('/usr/lib/python{0}.{1}/dist-packages/'.format(sys.version_info.major, sys.version_info.minor))\n            try:\n                import OpenSSL\n            except ImportError:\n                raise LiveAndLetDieError('Flask app could not be launched because the pyopenssl library is not installed on your system!')\n            ssl_context = 'adhoc'\n        app.run(host=host, port=port, ssl_context=ssl_context)\n        sys.exit()\n", "label": 0}
{"function": "\n\ndef test_tokenizer():\n    texts = ['The cat sat on the mat.', 'The dog sat on the log.', 'Dogs and cats living together.']\n    tokenizer = Tokenizer(nb_words=10)\n    tokenizer.fit_on_texts(texts)\n    sequences = []\n    for seq in tokenizer.texts_to_sequences_generator(texts):\n        sequences.append(seq)\n    assert (np.max(np.max(sequences)) < 10)\n    assert (np.min(np.min(sequences)) == 1)\n    tokenizer.fit_on_sequences(sequences)\n    for mode in ['binary', 'count', 'tfidf', 'freq']:\n        matrix = tokenizer.texts_to_matrix(texts, mode)\n", "label": 0}
{"function": "\n\ndef _packagePaths(self):\n    '\\n        Yield a sequence of FilePath-like objects which represent path segments.\\n        '\n    if (not self.isPackage()):\n        return\n    if self.isLoaded():\n        load = self.load()\n        if hasattr(load, '__path__'):\n            for fn in load.__path__:\n                if (fn == self.parentPath.path):\n                    assert self.parentPath.exists()\n                    (yield self.parentPath)\n                else:\n                    smp = self.pathEntry.pythonPath._smartPath(fn)\n                    if smp.exists():\n                        (yield smp)\n    else:\n        (yield self.parentPath)\n", "label": 1}
{"function": "\n\ndef render_option(self, name, selected_choices, option_value, option_label):\n    option_value = force_unicode(option_value)\n    if (option_label == BLANK_CHOICE_DASH[0][1]):\n        option_label = _('All')\n    data = self.data.copy()\n    data[name] = option_value\n    selected = ((data == self.data) or (option_value in selected_choices))\n    try:\n        url = data.urlencode()\n    except AttributeError:\n        url = urlencode(data)\n    return (self.option_string() % {\n        'attrs': ((selected and ' class=\"selected\"') or ''),\n        'query_string': url,\n        'label': force_unicode(option_label),\n    })\n", "label": 0}
{"function": "\n\ndef parse(self, path):\n    try:\n        if (path.startswith('http://') or path.startswith('https://')):\n            if ('requests' not in IMPORTS):\n                e = 'HTTP library not found: requests'\n                raise ImportError(e)\n            headers = {\n                'User-Agent': 'Mozilla/5.0 Gecko Firefox',\n            }\n            r = requests.get(path, headers=headers)\n            r.raise_for_status()\n            f = StringIO(r.content)\n            self.parser_func(f, path)\n            return\n        elif os.path.isfile(path):\n            with open(path, 'rb') as f:\n                self.parser_func(f, path)\n            return\n        elif os.path.isdir(path):\n            for (walk_root, walk_dirs, walk_files) in os.walk(path):\n                for walk_file in fnmatch.filter(walk_files, self.ext_filter):\n                    fpath = os.path.join(walk_root, walk_file)\n                    with open(fpath, 'rb') as f:\n                        self.parser_func(f, fpath)\n            return\n        e = ('File path is not a file, directory or URL: %s' % path)\n        raise IOError(e)\n    except (KeyboardInterrupt, SystemExit):\n        raise\n    except Exception as e:\n        self.handler.print_error(path, e)\n", "label": 1}
{"function": "\n\n@messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)\n@wrap_exception()\n@reverts_task_state\n@wrap_instance_event\n@wrap_instance_fault\ndef rebuild_instance(self, context, instance, orig_image_ref, image_ref, injected_files, new_pass, orig_sys_metadata, bdms, recreate, on_shared_storage=None, preserve_ephemeral=False, migration=None, scheduled_node=None, limits=None):\n    \"Destroy and re-make this instance.\\n\\n        A 'rebuild' effectively purges all existing data from the system and\\n        remakes the VM with given 'metadata' and 'personalities'.\\n\\n        :param context: `nova.RequestContext` object\\n        :param instance: Instance object\\n        :param orig_image_ref: Original image_ref before rebuild\\n        :param image_ref: New image_ref for rebuild\\n        :param injected_files: Files to inject\\n        :param new_pass: password to set on rebuilt instance\\n        :param orig_sys_metadata: instance system metadata from pre-rebuild\\n        :param bdms: block-device-mappings to use for rebuild\\n        :param recreate: True if the instance is being recreated (e.g. the\\n            hypervisor it was on failed) - cleanup of old state will be\\n            skipped.\\n        :param on_shared_storage: True if instance files on shared storage.\\n                                  If not provided then information from the\\n                                  driver will be used to decide if the instance\\n                                  files are available or not on the target host\\n        :param preserve_ephemeral: True if the default ephemeral storage\\n                                   partition must be preserved on rebuild\\n        :param migration: a Migration object if one was created for this\\n                          rebuild operation (if it's a part of evacaute)\\n        :param scheduled_node: A node of the host chosen by the scheduler. If a\\n                               host was specified by the user, this will be\\n                               None\\n        :param limits: Overcommit limits set by the scheduler. If a host was\\n                       specified by the user, this will be None\\n        \"\n    context = context.elevated()\n    LOG.info(_LI('Rebuilding instance'), context=context, instance=instance)\n    if (scheduled_node is not None):\n        rt = self._get_resource_tracker(scheduled_node)\n        rebuild_claim = rt.rebuild_claim\n    else:\n        rebuild_claim = claims.NopClaim\n    image_meta = {\n        \n    }\n    if image_ref:\n        image_meta = self.image_api.get(context, image_ref)\n    if (not scheduled_node):\n        try:\n            compute_node = self._get_compute_info(context, self.host)\n            scheduled_node = compute_node.hypervisor_hostname\n        except exception.ComputeHostNotFound:\n            LOG.exception(_LE('Failed to get compute_info for %s'), self.host)\n    with self._error_out_instance_on_exception(context, instance):\n        try:\n            claim_ctxt = rebuild_claim(context, instance, limits=limits, image_meta=image_meta, migration=migration)\n            self._do_rebuild_instance_with_claim(claim_ctxt, context, instance, orig_image_ref, image_ref, injected_files, new_pass, orig_sys_metadata, bdms, recreate, on_shared_storage, preserve_ephemeral)\n        except exception.ComputeResourcesUnavailable as e:\n            LOG.debug('Could not rebuild instance on this host, not enough resources available.', instance=instance)\n            self._set_migration_status(migration, 'failed')\n            self._notify_about_instance_usage(context, instance, 'rebuild.error', fault=e)\n            raise exception.BuildAbortException(instance_uuid=instance.uuid, reason=e.format_message())\n        except (exception.InstanceNotFound, exception.UnexpectedDeletingTaskStateError) as e:\n            LOG.debug('Instance was deleted while rebuilding', instance=instance)\n            self._set_migration_status(migration, 'failed')\n            self._notify_about_instance_usage(context, instance, 'rebuild.error', fault=e)\n        except Exception as e:\n            self._set_migration_status(migration, 'failed')\n            self._notify_about_instance_usage(context, instance, 'rebuild.error', fault=e)\n            raise\n        else:\n            instance.apply_migration_context()\n            instance.host = self.host\n            instance.node = scheduled_node\n            instance.save()\n            instance.drop_migration_context()\n            self._set_migration_status(migration, 'done')\n", "label": 1}
{"function": "\n\ndef setecho(self, state):\n    \"This sets the terminal echo mode on or off. Note that anything the\\n        child sent before the echo will be lost, so you should be sure that\\n        your input buffer is empty before you call setecho(). For example, the\\n        following will work as expected::\\n\\n            p = pexpect.spawn('cat') # Echo is on by default.\\n            p.sendline('1234') # We expect see this twice from the child...\\n            p.expect(['1234']) # ... once from the tty echo...\\n            p.expect(['1234']) # ... and again from cat itself.\\n            p.setecho(False) # Turn off tty echo\\n            p.sendline('abcd') # We will set this only once (echoed by cat).\\n            p.sendline('wxyz') # We will set this only once (echoed by cat)\\n            p.expect(['abcd'])\\n            p.expect(['wxyz'])\\n\\n        The following WILL NOT WORK because the lines sent before the setecho\\n        will be lost::\\n\\n            p = pexpect.spawn('cat')\\n            p.sendline('1234')\\n            p.setecho(False) # Turn off tty echo\\n            p.sendline('abcd') # We will set this only once (echoed by cat).\\n            p.sendline('wxyz') # We will set this only once (echoed by cat)\\n            p.expect(['1234'])\\n            p.expect(['1234'])\\n            p.expect(['abcd'])\\n            p.expect(['wxyz'])\\n\\n\\n        Not supported on platforms where ``isatty()`` returns False.\\n        \"\n    if (not self.isatty()):\n        self.echo = state\n        return\n    errmsg = 'setecho() may not be called on this platform'\n    try:\n        attr = termios.tcgetattr(self.child_fd)\n    except termios.error as err:\n        if (err.args[0] == errno.EINVAL):\n            raise IOError(err.args[0], ('%s: %s.' % (err.args[1], errmsg)))\n        raise\n    if state:\n        attr[3] = (attr[3] | termios.ECHO)\n    else:\n        attr[3] = (attr[3] & (~ termios.ECHO))\n    try:\n        termios.tcsetattr(self.child_fd, termios.TCSANOW, attr)\n    except IOError as err:\n        if (err.args[0] == errno.EINVAL):\n            raise IOError(err.args[0], ('%s: %s.' % (err.args[1], errmsg)))\n        raise\n    self.echo = state\n", "label": 0}
{"function": "\n\ndef get_field_types(field_list, iterations=3):\n    assert (iterations > 0), 'iterations should be a positive integer (not a negative integer or 0)'\n\n    def test_boolean(value):\n        if (value.lower() not in ('false', 'true')):\n            raise ValueError((_('%s is not a boolean value') % value))\n\n    def test_timestamp(value):\n        if (not value):\n            raise ValueError()\n        if ((len(value) > 50) or (len(value) < 3)):\n            raise ValueError()\n        if (value.startswith('[') and value.endswith(']')):\n            value = value[1:(- 1)]\n        try:\n            parse(value)\n        except OverflowError:\n            raise ValueError()\n\n    def test_int(value):\n        if ((len(bin(int(value))) - 2) > 32):\n            raise ValueError()\n\n    def test_string(value):\n        if (len(smart_str(value).split(' ')) > 4):\n            raise ValueError()\n    test_fns = [('boolean', test_boolean), ('tint', test_int), ('tlong', int), ('tdouble', float), ('tdate', test_timestamp), ('string', test_string), ('text_general', any)]\n    all_field_types = []\n    for row in field_list:\n        if (iterations == 0):\n            break\n        iterations -= 1\n        row_field_types = []\n        for field in row:\n            field_type_index = None\n            for index in range(0, len(test_fns)):\n                try:\n                    test_fns[index][1](field)\n                    field_type_index = index\n                    break\n                except ValueError:\n                    pass\n            row_field_types.append(field_type_index)\n        all_field_types.append(row_field_types)\n    if (not all_field_types):\n        return []\n    final_field_types = all_field_types[0]\n    for row_field_types in all_field_types:\n        for index in range(0, len(row_field_types)):\n            if (row_field_types[index] > final_field_types[index]):\n                final_field_types[index] = row_field_types[index]\n    return [test_fns[index][0] for index in final_field_types]\n", "label": 1}
{"function": "\n\ndef __init__(self, record_num=0, records=None):\n    self.record_num = record_num\n    records = (records or [])\n    assert isinstance(records, list)\n    for record in records:\n        assert isinstance(record, mldv2_report_group)\n    self.records = records\n", "label": 0}
{"function": "\n\ndef add_methods(cfc_file, hint_text):\n    with open(cfc_file, 'r') as f:\n        read_data = f.read()\n    methods = []\n    method_lines = re.findall('function\\\\s[^{]+', read_data)\n    for l in method_lines:\n        l = re.sub('[\\\\n|\\\\s]+', ' ', l)\n        s = re.search('(\\\\w+)\\\\s?\\\\(.*\\\\)', l)\n        if s:\n            methods.append(s.group().strip())\n    for c in methods:\n        snippet = c\n        params = re.sub('\\\\w+\\\\(', '', snippet, 1)[:(- 1)].split(',')\n        num = 1\n        if len(params[0]):\n            for p in params:\n                snippet = snippet.replace(p, (((('${' + str(num)) + ':') + p) + '}'))\n                num = (num + 1)\n        c = re.sub('\\\\(.*\\\\)', '', c)\n        completions.append((((c + '\\tfn. ') + hint_text), snippet))\n", "label": 0}
{"function": "\n\n@frappe.whitelist()\ndef get_script(report_name):\n    report = get_report_doc(report_name)\n    module = (report.module or frappe.db.get_value('DocType', report.ref_doctype, 'module'))\n    module_path = get_module_path(module)\n    report_folder = os.path.join(module_path, 'report', scrub(report.name))\n    script_path = os.path.join(report_folder, (scrub(report.name) + '.js'))\n    print_path = os.path.join(report_folder, (scrub(report.name) + '.html'))\n    script = None\n    if os.path.exists(script_path):\n        with open(script_path, 'r') as f:\n            script = f.read()\n    html_format = get_html_format(print_path)\n    if ((not script) and report.javascript):\n        script = report.javascript\n    if (not script):\n        script = (\"frappe.query_reports['%s']={}\" % report_name)\n    if (frappe.lang != 'en'):\n        send_translations(frappe.get_lang_dict('report', report_name))\n    return {\n        'script': script,\n        'html_format': html_format,\n    }\n", "label": 0}
{"function": "\n\ndef get_item(self, key):\n    try:\n        if (self.game_store_items.get_resource(key).type == 'own'):\n            user_item = self.user_items.get(key)\n            if (user_item and (user_item['amount'] > 1)):\n                return {\n                    'amount': 1,\n                }\n    except StoreError:\n        pass\n    return self.user_items[key]\n", "label": 0}
{"function": "\n\ndef test_list(self, app, model):\n    for i in range(1, 12):\n        model.create({\n            'title': 'Book {0}'.format(i),\n        })\n    with app.test_client() as c:\n        rv = c.get('/books/')\n    assert (rv.status == '200 OK')\n    body = rv.data.decode('utf-8')\n    assert ('Book 1' in body), 'Should show books'\n    assert ('Book 9' not in body), 'Should not show more than 10 books'\n    assert ('More' in body), 'Should have more than one page'\n", "label": 0}
{"function": "\n\n@app.route('/v1/credentials/<id>', methods=['PUT'])\n@authnz.require_auth\n@authnz.require_csrf_token\ndef update_credential(id):\n    try:\n        _cred = Credential.get(id)\n    except Credential.DoesNotExist:\n        return (jsonify({\n            'error': 'Credential not found.',\n        }), 404)\n    if (_cred.data_type != 'credential'):\n        msg = 'id provided is not a credential.'\n        return (jsonify({\n            'error': msg,\n        }), 400)\n    data = request.get_json()\n    update = {\n        \n    }\n    revision = (_cred.revision + 1)\n    update['name'] = data.get('name', _cred.name)\n    if ('enabled' in data):\n        if (not isinstance(data['enabled'], bool)):\n            return (jsonify({\n                'error': 'Enabled must be a boolean.',\n            }), 400)\n        update['enabled'] = data['enabled']\n    else:\n        update['enabled'] = _cred.enabled\n    services = _get_services_for_credential(id)\n    if ('credential_pairs' in data):\n        credential_pairs = _lowercase_credential_pairs(data['credential_pairs'])\n        if (not _check_credential_pair_uniqueness(credential_pairs)):\n            ret = {\n                'error': 'credential pairs must be key: value',\n            }\n            return (jsonify(ret), 400)\n        conflicts = _pair_key_conflicts_for_services(id, credential_pairs, services)\n        if conflicts:\n            ret = {\n                'error': 'Conflicting key pairs in mapped service.',\n                'conflicts': conflicts,\n            }\n            return (jsonify(ret), 400)\n        update['credential_pairs'] = json.dumps(credential_pairs)\n    else:\n        data_key = keymanager.decrypt_key(_cred.data_key, encryption_context={\n            'id': id,\n        })\n        cipher_version = _cred.cipher_version\n        cipher = CipherManager(data_key, cipher_version)\n        update['credential_pairs'] = cipher.decrypt(_cred.credential_pairs)\n    data_key = keymanager.create_datakey(encryption_context={\n        'id': id,\n    })\n    cipher = CipherManager(data_key['plaintext'], version=2)\n    credential_pairs = cipher.encrypt(update['credential_pairs'])\n    try:\n        Credential(id='{0}-{1}'.format(id, revision), name=update['name'], data_type='archive-credential', credential_pairs=credential_pairs, enabled=update['enabled'], revision=revision, data_key=data_key['ciphertext'], cipher_version=2, modified_by=authnz.get_logged_in_user_email()).save(id__null=True)\n    except PutError as e:\n        logging.error(e)\n        return (jsonify({\n            'error': 'Failed to add credential to archive.',\n        }), 500)\n    try:\n        cred = Credential(id=id, name=update['name'], data_type='credential', credential_pairs=credential_pairs, enabled=update['enabled'], revision=revision, data_key=data_key['ciphertext'], cipher_version=2, modified_by=authnz.get_logged_in_user_email())\n        cred.save()\n    except PutError as e:\n        logging.error(e)\n        return (jsonify({\n            'error': 'Failed to update active credential.',\n        }), 500)\n    if services:\n        service_names = [x.id for x in services]\n        msg = 'Updated credential \"{0}\" ({1}); Revision {2}'\n        msg = msg.format(cred.name, cred.id, cred.revision)\n        graphite.send_event(service_names, msg)\n    return jsonify({\n        'id': cred.id,\n        'name': cred.name,\n        'credential_pairs': json.loads(cipher.decrypt(cred.credential_pairs)),\n        'revision': cred.revision,\n        'enabled': cred.enabled,\n        'modified_date': cred.modified_date,\n        'modified_by': cred.modified_by,\n    })\n", "label": 1}
{"function": "\n\ndef test_hassignals_init():\n\n    class Str(InputSignal):\n\n        def __init__(self, func=None, upstream=[], *args):\n            InputSignal.__init__(self, str, [], *args)\n\n    class Test(HasSignals):\n        name = Str\n\n        @input\n        def title(self, v='a'):\n            return str(v)\n    raises(ValueError, Test, foo=3)\n    t = Test()\n    assert (t.title() == 'a')\n    assert (t.name() == '')\n    t = Test(title='b', name='c')\n    assert (t.title() == 'b')\n    assert (t.name() == 'c')\n", "label": 0}
{"function": "\n\ndef email(request, form_class=AddEmailForm, template_name='account/email.html'):\n    if ((request.method == 'POST') and request.user.is_authenticated()):\n        if (request.POST['action'] == 'add'):\n            add_email_form = form_class(request.user, request.POST)\n            if add_email_form.is_valid():\n                add_email_form.save()\n                add_email_form = form_class()\n        else:\n            add_email_form = form_class()\n            if (request.POST['action'] == 'send'):\n                email = request.POST['email']\n                try:\n                    email_address = EmailAddress.objects.get(user=request.user, email=email)\n                    request.user.message_set.create(message=('Confirmation email sent to %s' % email))\n                    EmailConfirmation.objects.send_confirmation(email_address)\n                except EmailAddress.DoesNotExist:\n                    pass\n            elif (request.POST['action'] == 'remove'):\n                email = request.POST['email']\n                try:\n                    email_address = EmailAddress.objects.get(user=request.user, email=email)\n                    email_address.delete()\n                    request.user.message_set.create(message=('Removed email address %s' % email))\n                except EmailAddress.DoesNotExist:\n                    pass\n            elif (request.POST['action'] == 'primary'):\n                email = request.POST['email']\n                email_address = EmailAddress.objects.get(user=request.user, email=email)\n                email_address.set_as_primary()\n    else:\n        add_email_form = form_class()\n    return render_to_response(template_name, {\n        'add_email_form': add_email_form,\n    }, context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef main(self, *args):\n    server = self.options.server\n    if (not server):\n        server = input('Enter the Review Board server URL: ')\n    (repository_info, tool) = self.initialize_scm_tool()\n    (api_client, api_root) = self.get_api(server)\n    self.setup_tool(tool, api_root=api_root)\n    repository_info = repository_info.find_server_repository_info(api_root)\n    selected_repo = self.prompt_rb_repository(tool.name, repository_info, api_root)\n    if (not selected_repo):\n        print(('No %s repository found or selected for %s. %s not created.' % (tool.name, server, CONFIG_FILE)))\n        return\n    config = [('REVIEWBOARD_URL', server), ('REPOSITORY', selected_repo['name'])]\n    try:\n        branch = tool.get_current_branch()\n        config.append(('BRANCH', branch))\n        config.append(('LAND_DEST_BRANCH', branch))\n    except NotImplementedError:\n        pass\n    outfile_path = os.path.join(os.getcwd(), CONFIG_FILE)\n    output = self._get_output(config)\n    if (not os.path.exists(outfile_path)):\n        question = ('Create \"%s\" with the following?\\n\\n%s\\n' % (outfile_path, output))\n    else:\n        question = ('\"%s\" exists. Overwrite with the following?\\n\\n%s\\n' % (outfile_path, output))\n    if (not confirm(question)):\n        return\n    self.generate_config_file(outfile_path, config)\n", "label": 0}
{"function": "\n\ndef disable(**kwargs):\n    \"\\n    Disable all beaconsd jobs on the minion\\n\\n    :return:                Boolean and status message on success or failure of disable.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' beacons.disable\\n    \"\n    ret = {\n        'comment': [],\n        'result': True,\n    }\n    if (('test' in kwargs) and kwargs['test']):\n        ret['comment'] = 'Beacons would be disabled.'\n    else:\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({\n                'func': 'disable',\n            }, 'manage_beacons')\n            if res:\n                event_ret = eventer.get_event(tag='/salt/minion/minion_beacons_disabled_complete', wait=30)\n                log.debug('event_ret {0}'.format(event_ret))\n                if (event_ret and event_ret['complete']):\n                    beacons = event_ret['beacons']\n                    if (('enabled' in beacons) and (not beacons['enabled'])):\n                        ret['result'] = True\n                        ret['comment'] = 'Disabled beacons on minion.'\n                    else:\n                        ret['result'] = False\n                        ret['comment'] = 'Failed to disable beacons on minion.'\n                    return ret\n        except KeyError:\n            ret['comment'] = 'Event module not available. Beacons enable job failed.'\n    return ret\n", "label": 1}
{"function": "\n\ndef test_assertRaises(self):\n\n    def _raise(e):\n        raise e\n    self.assertRaises(KeyError, _raise, KeyError)\n    self.assertRaises(KeyError, _raise, KeyError('key'))\n    try:\n        self.assertRaises(KeyError, (lambda : None))\n    except self.failureException as e:\n        self.assertIn('KeyError not raised', str(e))\n    else:\n        self.fail(\"assertRaises() didn't fail\")\n    try:\n        self.assertRaises(KeyError, _raise, ValueError)\n    except ValueError:\n        pass\n    else:\n        self.fail(\"assertRaises() didn't let exception pass through\")\n    with self.assertRaises(KeyError) as cm:\n        try:\n            raise KeyError\n        except Exception as e:\n            exc = e\n            raise\n    self.assertIs(cm.exception, exc)\n    with self.assertRaises(KeyError):\n        raise KeyError('key')\n    try:\n        with self.assertRaises(KeyError):\n            pass\n    except self.failureException as e:\n        self.assertIn('KeyError not raised', str(e))\n    else:\n        self.fail(\"assertRaises() didn't fail\")\n    try:\n        with self.assertRaises(KeyError):\n            raise ValueError\n    except ValueError:\n        pass\n    else:\n        self.fail(\"assertRaises() didn't let exception pass through\")\n", "label": 1}
{"function": "\n\ndef import_string(import_name, silent=False):\n    'Imports an object based on a string.  This is useful if you want to\\n    use import paths as endpoints or something similar.  An import path can\\n    be specified either in dotted notation (``xml.sax.saxutils.escape``)\\n    or with a colon as object delimiter (``xml.sax.saxutils:escape``).\\n\\n    If `silent` is True the return value will be `None` if the import fails.\\n\\n    :param import_name: the dotted name for the object to import.\\n    :param silent: if set to `True` import errors are ignored and\\n                   `None` is returned instead.\\n    :return: imported object\\n    '\n    assert isinstance(import_name, string_types)\n    import_name = str(import_name)\n    try:\n        if (':' in import_name):\n            (module, obj) = import_name.split(':', 1)\n        elif ('.' in import_name):\n            (module, obj) = import_name.rsplit('.', 1)\n        else:\n            return __import__(import_name)\n        if (PY2 and isinstance(obj, unicode)):\n            obj = obj.encode('utf-8')\n        try:\n            return getattr(__import__(module, None, None, [obj]), obj)\n        except (ImportError, AttributeError):\n            modname = ((module + '.') + obj)\n            __import__(modname)\n            return sys.modules[modname]\n    except ImportError as e:\n        if (not silent):\n            raise\n", "label": 1}
{"function": "\n\ndef run(self):\n    while self.processor.shared.paused():\n        time.sleep(1)\n    self.ircname = ((self.host + ' ') + self.getname())\n    irc.client.ServerConnection.buffer_class = irc.buffer.LenientDecodingLineBuffer\n    logger.info('joining IRC')\n    t = threading.Thread(target=self.who_thread)\n    t.start()\n    while (not self.processor.shared.stopped()):\n        client = irc.client.Reactor()\n        try:\n            c = client.server().connect('irc.freenode.net', 6667, self.nick, self.password, ircname=self.ircname)\n        except irc.client.ServerConnectionError:\n            logger.error('irc', exc_info=True)\n            time.sleep(10)\n            continue\n        c.add_global_handler('welcome', self.on_connect)\n        c.add_global_handler('join', self.on_join)\n        c.add_global_handler('quit', self.on_quit)\n        c.add_global_handler('kick', self.on_kick)\n        c.add_global_handler('whoreply', self.on_who)\n        c.add_global_handler('namreply', self.on_name)\n        c.add_global_handler('disconnect', self.on_disconnect)\n        c.set_keepalive(60)\n        self.connection = c\n        try:\n            client.process_forever()\n        except BaseException as e:\n            logger.error('irc', exc_info=True)\n            time.sleep(10)\n            continue\n    logger.info('quitting IRC')\n", "label": 0}
{"function": "\n\ndef indication(self, pdu):\n    if _debug:\n        MiddleMan._debug('indication %r', pdu)\n    if (not pdu.pduData):\n        stop()\n        return\n    line = pdu.pduData.decode('utf_8')[:(- 1)]\n    if _debug:\n        MiddleMan._debug('    - line: %r', line)\n    line_parts = line.split(' ', 1)\n    if _debug:\n        MiddleMan._debug('    - line_parts: %r', line_parts)\n    if (len(line_parts) != 2):\n        sys.stderr.write(('err: invalid line: %r\\n' % (line,)))\n        return\n    (addr, msg) = line_parts\n    if (addr == '*'):\n        dest = local_broadcast_tuple\n    elif (':' in addr):\n        (addr, port) = addr.split(':')\n        if (addr == '*'):\n            dest = (local_broadcast_tuple[0], int(port))\n        else:\n            dest = (addr, int(port))\n    else:\n        dest = (addr, local_unicast_tuple[1])\n    if _debug:\n        MiddleMan._debug('    - dest: %r', dest)\n    try:\n        self.request(PDU(msg.encode('utf_8'), destination=dest))\n    except Exception as err:\n        sys.stderr.write(('err: %r\\n' % (err,)))\n        return\n", "label": 1}
{"function": "\n\ndef ensure_new_type(obj):\n    from future.types.newbytes import newbytes\n    from future.types.newstr import newstr\n    from future.types.newint import newint\n    from future.types.newdict import newdict\n    native_type = type(native(obj))\n    if issubclass(native_type, type(obj)):\n        if (native_type == str):\n            return newbytes(obj)\n        elif (native_type == unicode):\n            return newstr(obj)\n        elif (native_type == int):\n            return newint(obj)\n        elif (native_type == long):\n            return newint(obj)\n        elif (native_type == dict):\n            return newdict(obj)\n        else:\n            return NotImplementedError(('type %s not supported' % type(obj)))\n    else:\n        assert (type(obj) in [newbytes, newstr])\n        return obj\n", "label": 1}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(UpdatePolicyRuleForm, self).__init__(request, *args, **kwargs)\n    try:\n        policyrule_id = self.initial['policyrule_id']\n        rule = client.policyrule_get(request, policyrule_id)\n        for item in ['name', 'description', 'policy_classifier_id', 'policy_actions', 'shared']:\n            self.fields[item].initial = getattr(rule, item)\n        actions = client.policyaction_list(request, tenant_id=request.user.tenant_id)\n        action_list = [a.id for a in actions]\n        for action in actions:\n            action.set_id_as_name_if_empty()\n        actions = sorted(actions, key=(lambda action: action.name))\n        action_list = [(a.id, a.name) for a in actions]\n        self.fields['policy_actions'].choices = action_list\n        classifiers = client.policyclassifier_list(request, tenant_id=request.user.tenant_id)\n        classifier_list = [(c.id, c.name) for c in classifiers]\n        self.fields['policy_classifier_id'].choices = classifier_list\n    except Exception:\n        exceptions.handle(request, _('Unable to retrive policy rule details.'))\n", "label": 0}
{"function": "\n\ndef test_neibs_manual(self):\n    shape = (2, 3, 4, 4)\n    for dtype in self.dtypes:\n        images = shared(numpy.arange(numpy.prod(shape), dtype=dtype).reshape(shape))\n        neib_shape = T.as_tensor_variable((2, 2))\n        for border in ['valid', 'ignore_borders']:\n            f = function([], images2neibs(images, neib_shape, mode=border), mode=self.mode)\n            assert any([isinstance(node.op, self.op) for node in f.maker.fgraph.toposort()])\n            neibs = f()\n            assert numpy.allclose(neibs, [[0, 1, 4, 5], [2, 3, 6, 7], [8, 9, 12, 13], [10, 11, 14, 15], [16, 17, 20, 21], [18, 19, 22, 23], [24, 25, 28, 29], [26, 27, 30, 31], [32, 33, 36, 37], [34, 35, 38, 39], [40, 41, 44, 45], [42, 43, 46, 47], [48, 49, 52, 53], [50, 51, 54, 55], [56, 57, 60, 61], [58, 59, 62, 63], [64, 65, 68, 69], [66, 67, 70, 71], [72, 73, 76, 77], [74, 75, 78, 79], [80, 81, 84, 85], [82, 83, 86, 87], [88, 89, 92, 93], [90, 91, 94, 95]])\n            g = function([], neibs2images(neibs, neib_shape, images.shape), mode=self.mode)\n            assert numpy.allclose(images.get_value(borrow=True), g())\n", "label": 0}
{"function": "\n\ndef plot_survfunc(survfuncs, ax=None):\n    \"\\n    Plot one or more survivor functions.\\n\\n    Arguments\\n    ---------\\n    survfuncs : object or array-like\\n        A single SurvfuncRight object, or a list or SurvfuncRight\\n        objects that are plotted together.\\n\\n    Returns\\n    -------\\n    A figure instance on which the plot was drawn.\\n\\n    Examples\\n    --------\\n    Add a legend:\\n\\n    >>> fig = plot_survfunc([sf0, sf1])\\n    >>> ax = fig.get_axes()[0]\\n    >>> ax.set_position([0.1, 0.1, 0.64, 0.8])\\n    >>> ha, lb = ax.get_legend_handles_labels()\\n    >>> leg = fig.legend((ha[0], ha[2]), (lb[0], lb[2]), 'center right')\\n\\n    Change the line colors:\\n\\n    >>> fig = plot_survfunc([sf0, sf1])\\n    >>> ax = fig.get_axes()[0]\\n    >>> ax.set_position([0.1, 0.1, 0.64, 0.8])\\n    >>> ha, lb = ax.get_legend_handles_labels()\\n    >>> ha[0].set_color('purple')\\n    >>> ha[1].set_color('purple')\\n    >>> ha[2].set_color('orange')\\n    >>> ha[3].set_color('orange')\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    try:\n        assert (type(survfuncs[0]) is SurvfuncRight)\n    except:\n        survfuncs = [survfuncs]\n    for (gx, sf) in enumerate(survfuncs):\n        surv_times = np.concatenate(([0], sf.surv_times))\n        surv_prob = np.concatenate(([1], sf.surv_prob))\n        mxt = max(sf.time)\n        if (mxt > surv_times[(- 1)]):\n            surv_times = np.concatenate((surv_times, [mxt]))\n            surv_prob = np.concatenate((surv_prob, [surv_prob[(- 1)]]))\n        label = getattr(sf, 'title', ('Group %d' % (gx + 1)))\n        (li,) = ax.step(surv_times, surv_prob, '-', label=label, lw=2, where='post')\n        ii = np.flatnonzero(np.logical_not(sf.status))\n        ti = sf.time[ii]\n        jj = (np.searchsorted(surv_times, ti) - 1)\n        sp = surv_prob[jj]\n        ax.plot(ti, sp, '+', ms=12, color=li.get_color(), label=(label + ' points'))\n    ax.set_ylim(0, 1.01)\n    return fig\n", "label": 0}
{"function": "\n\ndef test_deprecated_worker(self):\n    account_sid = 'AC123'\n    auth_token = 'foobar'\n    workspace_sid = 'WS456'\n    worker_sid = 'WK789'\n    capability = TaskRouterCapability(account_sid, auth_token, workspace_sid, worker_sid)\n    capability.generate_token()\n    token = capability.generate_token()\n    self.assertNotEqual(None, token)\n    decoded = jwt.decode(token, auth_token)\n    self.assertNotEqual(None, decoded)\n    self.check_decoded(decoded, account_sid, workspace_sid, worker_sid, worker_sid)\n    policies = decoded['policies']\n    self.assertEqual(len(policies), 6)\n    for (method, url, policy) in [('GET', 'https://event-bridge.twilio.com/v1/wschannels/AC123/WK789', policies[0]), ('POST', 'https://event-bridge.twilio.com/v1/wschannels/AC123/WK789', policies[1]), ('GET', 'https://taskrouter.twilio.com/v1/Workspaces/WS456/Activities', policies[2]), ('GET', 'https://taskrouter.twilio.com/v1/Workspaces/WS456/Tasks/**', policies[3]), ('GET', 'https://taskrouter.twilio.com/v1/Workspaces/WS456/Workers/WK789/Reservations/**', policies[4]), ('GET', 'https://taskrouter.twilio.com/v1/Workspaces/WS456/Workers/WK789', policies[5])]:\n        (yield (self.check_policy, method, url, policy))\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        capability.allow_worker_fetch_attributes()\n        assert (len(w) == 1)\n        assert issubclass(w[(- 1)].category, DeprecationWarning)\n        assert ('deprecated' in str(w[(- 1)].message))\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        capability.allow_worker_activity_updates()\n        assert (len(w) == 1)\n        assert issubclass(w[(- 1)].category, DeprecationWarning)\n        assert ('deprecated' in str(w[(- 1)].message))\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        capability.allow_task_reservation_updates()\n        assert (len(w) == 1)\n        assert issubclass(w[(- 1)].category, DeprecationWarning)\n        assert ('deprecated' in str(w[(- 1)].message))\n", "label": 1}
{"function": "\n\ndef process_group(hg, crawl_id, requesters, processed_groups, dbpool):\n    'Gevent worker that should process single hitgroup.\\n\\n    This should write some data into database and do not return any important\\n    data.\\n    '\n    hg['keywords'] = ', '.join(hg['keywords'])\n    hg['qualifications'] = ', '.join(hg['qualifications'])\n    conn = dbpool.getconn(thread.get_ident())\n    db = DB(conn)\n    try:\n        hit_group_content_id = db.hit_group_content_id(hg['group_id'])\n        if (hit_group_content_id is None):\n            profile = requesters.get(hg['requester_id'], None)\n            if (profile and (profile.is_public is False)):\n                hg['is_public'] = False\n            else:\n                hg['is_public'] = True\n            hg['occurrence_date'] = datetime.datetime.now()\n            hg['first_crawl_id'] = crawl_id\n            if (not hg['group_id_hashed']):\n                hg.update(hits_group_info(hg['group_id']))\n            else:\n                hg['html'] = ''\n            hit_group_content_id = db.insert_hit_group_content(hg)\n            log.debug('new hit group content: %s;;%s', hit_group_content_id, hg['group_id'])\n        hg['hit_group_content_id'] = hit_group_content_id\n        hg['crawl_id'] = crawl_id\n        hg['now'] = datetime.datetime.now()\n        db.insert_hit_group_status(hg)\n        conn.commit()\n    except Exception:\n        processed_groups.remove(hg['group_id'])\n        log.exception('process_group fail - rollback')\n        conn.rollback()\n    finally:\n        db.curr.close()\n        dbpool.putconn(conn, thread.get_ident())\n        msg = 'This really should not happen, Hitgroupstatus was processed but is not on the list, race condition?'\n        assert (hg['group_id'] in processed_groups), msg\n    return True\n", "label": 0}
{"function": "\n\ndef test_defer_connect(self):\n    import socket\n    for db in self.databases:\n        d = db.copy()\n        try:\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n            sock.connect(d['unix_socket'])\n        except KeyError:\n            sock = socket.create_connection((d.get('host', 'localhost'), d.get('port', 3306)))\n        for k in ['unix_socket', 'host', 'port']:\n            try:\n                del d[k]\n            except KeyError:\n                pass\n        c = pymysql.connect(defer_connect=True, **d)\n        self.assertFalse(c.open)\n        c.connect(sock)\n        c.close()\n", "label": 0}
{"function": "\n\ndef _onPresence(self, element):\n    '\\n        Called when a presence stanza has been received.\\n        '\n    stanza = Stanza.fromElement(element)\n    presenceType = (stanza.stanzaType or 'available')\n    try:\n        parser = self.presenceTypeParserMap[presenceType]\n    except KeyError:\n        return\n    presence = parser.fromElement(element)\n    try:\n        handler = getattr(self, ('%sReceived' % presenceType))\n    except AttributeError:\n        return\n    else:\n        handler(presence)\n", "label": 0}
{"function": "\n\ndef _runner(init, shape, target_mean=None, target_std=None, target_max=None, target_min=None):\n    variable = init(shape)\n    output = K.get_value(variable)\n    lim = 0.01\n    if (target_std is not None):\n        assert (abs((output.std() - target_std)) < lim)\n    if (target_mean is not None):\n        assert (abs((output.mean() - target_mean)) < lim)\n    if (target_max is not None):\n        assert (abs((output.max() - target_max)) < lim)\n    if (target_min is not None):\n        assert (abs((output.min() - target_min)) < lim)\n", "label": 1}
{"function": "\n\ndef blame(self, extensions=None, ignore_dir=None, committer=True, by='repository'):\n    '\\n        Returns the blame from the current HEAD of the repositories as a DataFrame.  The DataFrame is grouped by committer\\n        name, so it will be the sum of all contributions to all repositories by each committer. As with the commit history\\n        method, extensions and ignore_dirs parameters can be passed to exclude certain directories, or focus on certain\\n        file extensions. The DataFrame will have the columns:\\n\\n         * committer\\n         * loc\\n\\n        :param extensions: (optional, default=None) a list of file extensions to return commits for\\n        :param ignore_dir: (optional, default=None) a list of directory names to ignore\\n        :param committer: (optional, default=True) true if committer should be reported, false if author\\n        :param by: (optional, default=repository) whether to group by repository or by file\\n        :return: DataFrame\\n        '\n    df = None\n    for repo in self.repos:\n        try:\n            if (df is None):\n                df = repo.blame(extensions=extensions, ignore_dir=ignore_dir, committer=committer, by=by)\n            else:\n                df = df.append(repo.blame(extensions=extensions, ignore_dir=ignore_dir, committer=committer, by=by))\n        except GitCommandError as err:\n            print(('Warning! Repo: %s couldnt be blamed' % (repo,)))\n            pass\n    df = df.reset_index(level=1)\n    df = df.reset_index(level=1)\n    if committer:\n        if (by == 'repository'):\n            df = df.groupby('committer').agg({\n                'loc': np.sum,\n            })\n        elif (by == 'file'):\n            df = df.groupby(['committer', 'file']).agg({\n                'loc': np.sum,\n            })\n    elif (by == 'repository'):\n        df = df.groupby('author').agg({\n            'loc': np.sum,\n        })\n    elif (by == 'file'):\n        df = df.groupby(['author', 'file']).agg({\n            'loc': np.sum,\n        })\n    df = df.sort_values(by=['loc'], ascending=False)\n    return df\n", "label": 1}
{"function": "\n\ndef drag_release_frame(self, frame, pos):\n    ' Handle the dock frame being released by the user.\\n\\n        This method is called by the framework at the appropriate times\\n        and should not be called directly by user code. It will redock\\n        a floating dock item if it is released over a dock guide.\\n\\n        Parameters\\n        ----------\\n        frame : QDockFrame\\n            The dock frame being dragged by the user.\\n\\n        pos : QPoint\\n            The global coordinates of the mouse position.\\n\\n        '\n    overlay = self._overlay\n    overlay.hide()\n    guide = overlay.guide_at(pos)\n    if (guide == QGuideRose.Guide.NoGuide):\n        return\n    if self._proximity_handler.hasLinkedFrames(frame):\n        return\n    builder = LayoutBuilder(self)\n    target = self._dock_target(frame, pos)\n    if isinstance(target, QDockArea):\n        if (target.maximizedWidget() is not None):\n            return\n        with builder.drop_frame(frame):\n            local = target.mapFromGlobal(pos)\n            widget = layout_hit_test(target, local)\n            plug_frame(target, widget, frame, guide)\n    elif isinstance(target, QDockContainer):\n        with builder.dock_context(target):\n            with builder.drop_frame(frame):\n                area = target.parentDockArea()\n                if (area is not None):\n                    plug_frame(area, target, frame, guide)\n", "label": 0}
{"function": "\n\ndef wait_until_evaluation_status_code_is(step, code1, code2, secs):\n    start = datetime.utcnow()\n    i_get_the_evaluation(step, world.evaluation['resource'])\n    status = get_status(world.evaluation)\n    while ((status['code'] != int(code1)) and (status['code'] != int(code2))):\n        time.sleep(3)\n        assert ((datetime.utcnow() - start) < timedelta(seconds=int(secs)))\n        i_get_the_evaluation(step, world.evaluation['resource'])\n        status = get_status(world.evaluation)\n    assert (status['code'] == int(code1))\n", "label": 0}
{"function": "\n\ndef __init__(self, domain, sensor, backscatter_model=None):\n    self.domain = domain\n    self.sensor = sensor\n    self.backscatter_model = []\n    for i in range(len(sensor.band_names)):\n        try:\n            if (backscatter_model != None):\n                self.backscatter_model.append(backscatter_model)\n            elif (sensor.water_distributions[sensor.band_names[i]]['model'] == 'peak'):\n                self.backscatter_model.append(RadarHistogram.BACKSCATTER_MODEL_PEAK)\n            elif (sensor.water_distributions[sensor.band_names[i]]['model'] == 'dip'):\n                self.backscatter_model.append(RadarHistogram.BACKSCATTER_MODEL_DIP)\n            elif (sensor.water_distributions[sensor.band_names[i]]['model'] == 'lambda'):\n                self.backscatter_model.append(RadarHistogram.BACKSCATTER_MODEL_GAMMA)\n            else:\n                self.backscatter_model.append(RadarHistogram.BACKSCATTER_MODEL_GAUSSIAN)\n        except KeyError:\n            pass\n    self.hist_image = self.__preprocess_image(sensor)\n    self.histograms = self.__compute_histogram(self.hist_image)\n    self.__find_thresholds()\n", "label": 0}
{"function": "\n\ndef move_page(self, direction, n_windows):\n    '\\n        Move the page down (positive direction) or up (negative direction).\\n\\n        Paging down:\\n            The post on the bottom of the page becomes the post at the top of\\n            the page and the cursor is moved to the top.\\n        Paging up:\\n            The post at the top of the page becomes the post at the bottom of\\n            the page and the cursor is moved to the bottom.\\n        '\n    assert (direction in ((- 1), 1))\n    assert (n_windows >= 0)\n    if ((self.absolute_index < 0) | (n_windows == 0)):\n        (valid, redraw) = self.move(direction, n_windows)\n    else:\n        if ((self.absolute_index < n_windows) and (direction < 0)):\n            self.page_index = (- 1)\n            self.cursor_index = 0\n            self.inverted = False\n            if (not self._is_valid(self.absolute_index)):\n                self.page_index = 0\n            valid = True\n        else:\n            if (((direction > 0) & (self.inverted is True)) | ((direction < 0) & (self.inverted is False))):\n                self.page_index += (self.step * (n_windows - 1))\n                self.inverted = (not self.inverted)\n                self.cursor_index = ((n_windows - (direction < 0)) - self.cursor_index)\n            valid = False\n            adj = 0\n            while (not valid):\n                n_move = (n_windows - adj)\n                if (n_move == 0):\n                    break\n                self.page_index += (n_move * direction)\n                valid = self._is_valid(self.absolute_index)\n                if (not valid):\n                    self.page_index -= (n_move * direction)\n                    adj += 1\n        redraw = True\n    return (valid, redraw)\n", "label": 1}
{"function": "\n\ndef test_profile(self):\n\n    def g():\n        pass\n\n    def f():\n        g()\n\n    def main():\n        f()\n        f()\n        f()\n    with tracebin.record() as recorder:\n        main()\n    assert (recorder.calls is None)\n    start = time.time()\n    with tracebin.record(profile=True) as recorder:\n        main()\n    assert (len(recorder.calls) == 3)\n    [call1, call2, call3] = recorder.calls\n    assert (call1.func_name == 'high_res_time')\n    assert (len(call1.subcalls) == 1)\n    assert (call2.func_name == 'main')\n    assert ((call2.start_time - start) < 1)\n    assert ((call2.end_time - call2.start_time) < 1)\n    assert (len(call2.subcalls) == 3)\n    assert ({c.func_name for c in call2.subcalls} == {'f'})\n    assert (call2.subcalls[1].subcalls[0].func_name == 'g')\n", "label": 1}
{"function": "\n\ndef test_compression():\n    assert (set(compress) == set(decompress))\n    a = b'Hello, world!'\n    for k in compress:\n        comp = compress[k]\n        decomp = decompress[k]\n        b = comp(a)\n        c = decomp(b)\n        assert (a == c)\n        if (k is not None):\n            assert (a != b)\n", "label": 0}
{"function": "\n\n@cached_property\ndef templates(self):\n    if (self._templates is None):\n        self._templates = settings.TEMPLATES\n    templates = OrderedDict()\n    backend_names = []\n    for tpl in self._templates:\n        tpl = tpl.copy()\n        try:\n            default_name = tpl['BACKEND'].rsplit('.', 2)[(- 2)]\n        except Exception:\n            invalid_backend = tpl.get('BACKEND', '<not defined>')\n            raise ImproperlyConfigured('Invalid BACKEND for a template engine: {}. Check your TEMPLATES setting.'.format(invalid_backend))\n        tpl.setdefault('NAME', default_name)\n        tpl.setdefault('DIRS', [])\n        tpl.setdefault('APP_DIRS', False)\n        tpl.setdefault('OPTIONS', {\n            \n        })\n        templates[tpl['NAME']] = tpl\n        backend_names.append(tpl['NAME'])\n    counts = Counter(backend_names)\n    duplicates = [alias for (alias, count) in counts.most_common() if (count > 1)]\n    if duplicates:\n        raise ImproperlyConfigured(\"Template engine aliases aren't unique, duplicates: {}. Set a unique NAME for each engine in settings.TEMPLATES.\".format(', '.join(duplicates)))\n    return templates\n", "label": 0}
{"function": "\n\n@task\ndef table():\n    '\\n    Make an html table of the downloads.\\n\\n    This is for pasting into the GitHub releases page. See GitHub_release().\\n    '\n    tarball_formatter_dict = tarball_formatter()\n    shortversion = get_sympy_short_version()\n    tarball_formatter_dict['version'] = shortversion\n    md5s = [i.split('\\t') for i in md5(print_=False).split('\\n')]\n    md5s_dict = {name: md5 for (md5, name) in md5s}\n    sizes = [i.split('\\t') for i in size(print_=False).split('\\n')]\n    sizes_dict = {name: size for (size, name) in sizes}\n    table = []\n\n    @contextmanager\n    def tag(name):\n        table.append(('<%s>' % name))\n        (yield)\n        table.append(('</%s>' % name))\n    with tag('table'):\n        with tag('tr'):\n            for headname in ['Filename', 'Description', 'size', 'md5']:\n                with tag('th'):\n                    table.append(headname)\n        for key in descriptions:\n            name = get_tarball_name(key)\n            with tag('tr'):\n                with tag('td'):\n                    with tag('b'):\n                        table.append(name)\n                with tag('td'):\n                    table.append(descriptions[key].format(**tarball_formatter_dict))\n                with tag('td'):\n                    table.append(sizes_dict[name])\n                with tag('td'):\n                    table.append(md5s_dict[name])\n    out = ' '.join(table)\n    return out\n", "label": 0}
{"function": "\n\n@Xephyr(True, ShConfig())\ndef test_call(self):\n    self.sh = libqtile.sh.QSh(self.c)\n    assert (self.sh._call('status', []) == 'OK')\n    v = self.sh._call('nonexistent', '')\n    assert ('No such command' in v)\n    v = self.sh._call('status', '(((')\n    assert ('Syntax error' in v)\n    v = self.sh._call('status', '(1)')\n    assert ('Command exception' in v)\n", "label": 0}
{"function": "\n\ndef attach(self, timeout, wait=True):\n    if (self.attachment_status in (apiAttachPendingAuthorization, apiAttachSuccess)):\n        return\n    self.acquire()\n    try:\n        try:\n            self.start()\n        except AssertionError:\n            pass\n        t = threading.Timer(timeout2float(timeout), (lambda : setattr(self, 'wait', False)))\n        try:\n            self.init_observer()\n            self.client_id = (- 1)\n            self.set_attachment_status(apiAttachPendingAuthorization)\n            self.post('SKSkypeAPIAttachRequest')\n            self.wait = True\n            if wait:\n                t.start()\n            while (self.wait and (self.attachment_status == apiAttachPendingAuthorization)):\n                if self.run_main_loop:\n                    time.sleep(1.0)\n                else:\n                    EventLoop.run(1.0)\n        finally:\n            t.cancel()\n        if (not self.wait):\n            self.set_attachment_status(apiAttachUnknown)\n            raise SkypeAPIError('Skype attach timeout')\n    finally:\n        self.release()\n    command = Command(('PROTOCOL %s' % self.protocol), Blocking=True)\n    self.send_command(command)\n    self.protocol = int(command.Reply.rsplit(None, 1)[(- 1)])\n", "label": 1}
{"function": "\n\ndef test_deselect(self):\n    menu = Termenu(OPTIONS, height=4)\n    assert (strmenu(menu) == '(01) 02 03 04')\n    menu._on_space()\n    assert (' '.join(menu.get_result()) == '01')\n    menu._on_up()\n    menu._on_space()\n    assert (strmenu(menu) == '01 (02) 03 04')\n    assert (' '.join(menu.get_result()) == '02')\n", "label": 0}
{"function": "\n\ndef _load_config():\n    '\\n    Loads and parses /usbkey/config\\n    '\n    config = {\n        \n    }\n    if os.path.isfile('/usbkey/config'):\n        with salt.utils.fopen('/usbkey/config', 'r') as config_file:\n            for optval in config_file:\n                if (optval[0] == '#'):\n                    continue\n                if ('=' not in optval):\n                    continue\n                optval = optval.split('=')\n                config[optval[0].lower()] = optval[1].strip().strip('\"')\n    log.debug('smartos.config - read /usbkey/config: {0}'.format(config))\n    return config\n", "label": 0}
{"function": "\n\ndef test_related_articles_plugin(self):\n    main_article = self.create_article(app_config=self.app_config)\n    static_placeholder = StaticPlaceholder.objects.get_or_create(code='newsblog_social', site__isnull=True)[0]\n    placeholder = static_placeholder.draft\n    api.add_plugin(placeholder, 'NewsBlogRelatedPlugin', self.language)\n    static_placeholder.publish(None, language=self.language, force=True)\n    plugin = placeholder.get_plugins()[0].get_plugin_instance()[0]\n    plugin.save()\n    self.plugin_page.publish(self.language)\n    main_article.save()\n    for _ in range(3):\n        a = self.create_article()\n        a.save()\n        main_article.related.add(a)\n    another_language_articles = []\n    with override('de'):\n        for _ in range(4):\n            a = self.create_article()\n            main_article.related.add(a)\n            another_language_articles.append(a)\n    self.assertEquals(main_article.related.count(), 7)\n    unrelated = []\n    for _ in range(5):\n        unrelated.append(self.create_article())\n    response = self.client.get(main_article.get_absolute_url())\n    for article in main_article.related.all():\n        self.assertContains(response, article.title)\n    for article in unrelated:\n        self.assertNotContains(response, article.title)\n    self.page.unpublish('de')\n    cache.clear()\n    response = self.client.get(main_article.get_absolute_url())\n    for article in another_language_articles:\n        self.assertNotContains(response, article.title)\n", "label": 0}
{"function": "\n\ndef _dump(obj):\n    nodefmt = unicode((((((('\\n' + _i()) + '\"{0}\"\\n') + _i()) + '{{\\n{1}') + _i()) + '}}\\n\\n'))\n    podfmt = unicode((_i() + '\"{0}\" \"{1}\"\\n'))\n    lstfmt = unicode(((_i() + (' ' * mult)) + '\"{0}\" \"1\"'))\n    global indent\n    indent += 1\n    nodes = []\n    for (k, v) in obj.iteritems():\n        if isinstance(v, dict):\n            nodes.append(nodefmt.format(k, _dump(v)))\n        else:\n            try:\n                try:\n                    v.isdigit\n                    nodes.append(podfmt.format(k, v))\n                except AttributeError:\n                    lst = map(lstfmt.format, v)\n                    nodes.append(nodefmt.format(k, ('\\n'.join(lst) + '\\n')))\n            except TypeError:\n                nodes.append(podfmt.format(k, v))\n    indent -= 1\n    return unicode(''.join(nodes))\n", "label": 0}
{"function": "\n\ndef test_boolean_value():\n    success_list = range(200, 207)\n    for code in RESPONSE_CODES:\n        response = CalvinResponse(code)\n        if (code in success_list):\n            assert response\n        else:\n            assert (not response)\n", "label": 0}
{"function": "\n\ndef test_inc_adv_subtensor1_with_broadcasting(self):\n    if (inplace_increment is None):\n        raise inplace_increment_missing\n    inc = dscalar()\n    a = inc_subtensor(self.m[self.ix1], inc)\n    g_inc = tensor.grad(a.sum(), inc)\n    assert (a.type == self.m.type), (a.type, self.m.type)\n    f = theano.function([self.m, self.ix1, inc], [a, g_inc], allow_input_downcast=True)\n    (aval, gval) = f([[0.4, 0.9, 0.1], [5, 6, 7], [0.5, 0.3, 0.15]], [0, 1, 0], 2.1)\n    assert numpy.allclose(aval, [[(0.4 + (2.1 * 2)), (0.9 + (2.1 * 2)), (0.1 + (2.1 * 2))], [(5 + 2.1), (6 + 2.1), (7 + 2.1)], [0.5, 0.3, 0.15]]), aval\n    assert numpy.allclose(gval, 9.0), gval\n", "label": 0}
{"function": "\n\ndef test_urls(client):\n    r = client.get('/')\n    assert (r.status_code == 200)\n    r = client.get('/hello')\n    assert (r.status_code == 200)\n    r = client.get('/world')\n    assert (r.status_code == 200)\n    r = client.get('/pages/hello')\n    assert (r.status_code == 200)\n    r = client.get('/pages/world')\n    assert (r.status_code == 200)\n", "label": 0}
{"function": "\n\ndef test_with_no_existing_documents(self):\n    'When there are no matching Documents it creates and returns one.'\n    document_ = document.Document()\n    docuri = document.DocumentURI(claimant='https://en.wikipedia.org/wiki/Main_Page', uri='https://en.wikipedia.org/wiki/Main_Page', document=document_)\n    db.Session.add(docuri)\n    db.Session.flush()\n    documents = document.Document.find_or_create_by_uris(db.Session, 'https://en.wikipedia.org/wiki/Pluto', ['https://m.en.wikipedia.org/wiki/Pluto'])\n    assert (documents.count() == 1)\n    actual = documents.first()\n    assert isinstance(actual, document.Document)\n    assert (len(actual.document_uris) == 1)\n    docuri = actual.document_uris[0]\n    assert (docuri.claimant == 'https://en.wikipedia.org/wiki/Pluto')\n    assert (docuri.uri == 'https://en.wikipedia.org/wiki/Pluto')\n    assert (docuri.type == 'self-claim')\n", "label": 0}
{"function": "\n\ndef query_library(self, query, tie_breaker=no_tiebreak, modifiers=None, auto=False):\n    'Queries the library for songs.\\n        returns a list of matches, or None.\\n        '\n    if (not modifiers):\n        modifiers = []\n    try:\n        if (not auto):\n            return self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, modifiers, auto))\n        else:\n            current_mods = modifiers[:]\n            future_mods = (m for m in self.auto_modifiers if (m not in modifiers))\n            while True:\n                results = self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, current_mods, auto))\n                if (not results):\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        return results\n                elif (len(results) == 1):\n                    return results\n                else:\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        raise self.TieBroken(tie_breaker(query, results))\n                    next_results = self.query_library(query, tie_breaker, current_mods, auto)\n                    if (not next_results):\n                        raise self.TieBroken(tie_breaker(query, results))\n                    else:\n                        return next_results\n    except self.TieBroken as tie:\n        return tie.results\n", "label": 1}
{"function": "\n\ndef pygmentize(code_string, lexer_name=LEXER_DEFAULT):\n    if (lexer_name == PLAIN_CODE):\n        return '\\n'.join(['<span class=\"nn\">{}</span>'.format(escape(l)) for l in code_string.splitlines()])\n    try:\n        lexer = ((lexer_name and get_lexer_by_name(lexer_name)) or PythonLexer())\n    except Exception:\n        lexer = PythonLexer()\n    return highlight(code_string, lexer, NakedHtmlFormatter())\n", "label": 0}
{"function": "\n\ndef check(self):\n    assert isinstance(self._map, tuple)\n    for (key, value) in self._map:\n        assert isinstance(key, self.KEYTYPE)\n        assert isinstance(value, self.VALUETYPE)\n        keycheck = key.in_scope(*self.scopes()).check()\n        valuecheck = value.in_scope(*self.scopes()).check()\n        if (not keycheck.ok()):\n            return TypeCheck.failure(('%s key %s failed check: %s' % (self.__class__.__name__, key, keycheck.message())))\n        if (not valuecheck.ok()):\n            return TypeCheck.failure(('%s[%s] value %s failed check: %s' % (self.__class__.__name__, key, value, valuecheck.message())))\n    return TypeCheck.success()\n", "label": 0}
{"function": "\n\ndef test_with_context(self):\n    app = Flask('alchemist')\n    app.config['A_SETTING'] = 1\n    with app.app_context():\n        assert ('A_SETTING' in settings)\n        assert (settings['A_SETTING'] == 1)\n        assert (len(settings) > 0)\n        assert (len(list(iter(settings))) > 0)\n", "label": 0}
{"function": "\n\ndef check_field_spec(self, cls, model, flds, label):\n    '\\n        Validate the fields specification in `flds` from a ModelAdmin subclass\\n        `cls` for the `model` model. Use `label` for reporting problems to the user.\\n\\n        The fields specification can be a ``fields`` option or a ``fields``\\n        sub-option from a ``fieldsets`` option component.\\n        '\n    for fields in flds:\n        if (type(fields) != tuple):\n            fields = (fields,)\n        for field in fields:\n            if (field in cls.readonly_fields):\n                continue\n            try:\n                model._meta.get_field(field)\n            except models.FieldDoesNotExist:\n                continue\n", "label": 0}
{"function": "\n\ndef walk_branches(self, sort, *branches):\n    \"\\n        Simple iterator which take a sorting strategy and some branch and\\n        iterates through those branches one commit at a time, yielding a list\\n        of commits\\n\\n        :param sort: a sorting option `GIT_SORT_NONE, GIT_SORT_TOPOLOGICAL,\\n        GIT_SORT_TIME, GIT_SORT_REVERSE`. Default is 'GIT_SORT_TOPOLOGICAL'\\n        :param branches: branch to iterate through\\n        :type branches: list\\n        :returns: yields a list of commits corresponding to given branches\\n        :rtype: list\\n\\n        \"\n    iterators = [self._repo.walk(branch.target, sort) for branch in branches]\n    stop_iteration = [False for branch in branches]\n    commits = []\n    for iterator in iterators:\n        try:\n            commit = next(iterator)\n        except StopIteration:\n            commit = None\n        commits.append(commit)\n    (yield (commit for commit in commits))\n    while (not all(stop_iteration)):\n        for (index, iterator) in enumerate(iterators):\n            try:\n                commit = next(iterator)\n                commits[index] = commit\n            except StopIteration:\n                stop_iteration[index] = True\n        if (not all(stop_iteration)):\n            (yield (commit for commit in commits))\n", "label": 1}
{"function": "\n\ndef login(self, server, username, password='', original_prompt='[#$]', login_timeout=10, auto_prompt_reset=True, sync_multiplier=1, port=23):\n    cmd = 'telnet -l {} {} {}'.format(username, server, port)\n    spawn._spawn(self, cmd)\n    try:\n        i = self.expect('(?i)(?:password)', timeout=login_timeout)\n        if (i == 0):\n            self.sendline(password)\n            i = self.expect([original_prompt, 'Login incorrect'], timeout=login_timeout)\n        if i:\n            raise pxssh.ExceptionPxssh('could not log in: password was incorrect')\n    except TIMEOUT:\n        if (not password):\n            pass\n        else:\n            raise pxssh.ExceptionPxssh('could not log in: did not see a password prompt')\n    if (not self.sync_original_prompt(sync_multiplier)):\n        self.close()\n        raise pxssh.ExceptionPxssh('could not synchronize with original prompt')\n    if auto_prompt_reset:\n        if (not self.set_unique_prompt()):\n            self.close()\n            message = 'could not set shell prompt (recieved: {}, expected: {}).'\n            raise pxssh.ExceptionPxssh(message.format(self.before, self.PROMPT))\n    return True\n", "label": 1}
{"function": "\n\ndef do_load(self, arg=None):\n    'Runs script of command(s) from a file or URL.'\n    if (arg is None):\n        targetname = self.default_file_name\n    else:\n        arg = arg.split(None, 1)\n        (targetname, args) = (arg[0], (arg[1:] or [''])[0].strip())\n    try:\n        target = self.read_file_or_url(targetname)\n    except IOError as e:\n        self.perror(('Problem accessing script from %s: \\n%s' % (targetname, e)))\n        return\n    keepstate = Statekeeper(self, ('stdin', 'use_rawinput', 'prompt', 'continuation_prompt', 'current_script_dir'))\n    self.stdin = target\n    self.use_rawinput = False\n    self.prompt = self.continuation_prompt = ''\n    self.current_script_dir = os.path.split(targetname)[0]\n    stop = self._cmdloop()\n    self.stdin.close()\n    keepstate.restore()\n    self.lastcmd = ''\n    return (stop and (stop != self._STOP_SCRIPT_NO_EXIT))\n", "label": 0}
{"function": "\n\ndef _addNote(self, nick, whence, text, at=None, maximum=None):\n    if (at is None):\n        at = time.time()\n    if (maximum is None):\n        maximum = self.registryValue('maximum')\n    try:\n        notes = self._notes[nick]\n        if (maximum and (len(notes) >= maximum)):\n            raise QueueIsFull()\n        else:\n            notes.append((at, whence, text))\n    except KeyError:\n        self._notes[nick] = [(at, whence, text)]\n    if (('?' in nick) or (('*' in nick) and (nick not in self.wildcards))):\n        self.wildcards.append(nick)\n    self._flushNotes()\n", "label": 1}
{"function": "\n\ndef azure(account):\n    myCredAccount = CredAccountAzure()\n    if (not ('rsaPrivateKey' in account)):\n        printer.out('rsaPrivateKey in azure account not found', printer.ERROR)\n        return\n    if (not ('certKey' in account)):\n        printer.out('certKey in azure account not found', printer.ERROR)\n        return\n    if (not ('subscriptionId' in account)):\n        printer.out('subscriptionId for azure account not found', printer.ERROR)\n        return\n    if (not ('name' in account)):\n        printer.out('name for azure account not found', printer.ERROR)\n        return\n    myCredAccount.accountNumber = account['subscriptionId']\n    myCredAccount.name = account['name']\n    myCertificates = certificates()\n    myCredAccount.certificates = myCertificates\n    try:\n        myCertificate = certificate()\n        with open(account['rsaPrivateKey'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'azureRSAKey'\n        myCertificate.name = ntpath.basename(account['rsaPrivateKey'])\n        myCertificates.add_certificate(myCertificate)\n        myCertificate = certificate()\n        with open(account['certKey'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'azureCertKey'\n        myCertificate.name = ntpath.basename(account['certKey'])\n        myCertificates.add_certificate(myCertificate)\n    except IOError as e:\n        printer.out(('File error: ' + str(e)), printer.ERROR)\n        return\n    return myCredAccount\n", "label": 0}
{"function": "\n\n@login_required\n@permission_required('workshops.add_person', raise_exception=True)\ndef person_bulk_add(request):\n    if (request.method == 'POST'):\n        form = PersonBulkAddForm(request.POST, request.FILES)\n        if form.is_valid():\n            charset = (request.FILES['file'].charset or settings.DEFAULT_CHARSET)\n            stream = io.TextIOWrapper(request.FILES['file'].file, charset)\n            try:\n                (persons_tasks, empty_fields) = upload_person_task_csv(stream)\n            except csv.Error as e:\n                messages.add_message(request, messages.ERROR, 'Error processing uploaded .CSV file: {}'.format(e))\n            except UnicodeDecodeError as e:\n                messages.add_message(request, messages.ERROR, 'Please provide a file in {} encoding.'.format(charset))\n            else:\n                if empty_fields:\n                    msg_template = 'The following required fields were not found in the uploaded file: {}'\n                    msg = msg_template.format(', '.join(empty_fields))\n                    messages.add_message(request, messages.ERROR, msg)\n                else:\n                    request.session['bulk-add-people'] = persons_tasks\n                    return redirect('person_bulk_add_confirmation')\n    else:\n        form = PersonBulkAddForm()\n    context = {\n        'title': 'Bulk Add People',\n        'form': form,\n        'charset': settings.DEFAULT_CHARSET,\n    }\n    return render(request, 'workshops/person_bulk_add_form.html', context)\n", "label": 1}
{"function": "\n\ndef test_updating_without_id(self):\n    assert (len(self.pet_tools.lister()) == 0)\n    self.pet_tools.setter(pet_name='gina', pet_desc='good catcher')\n    assert (len(self.pet_tools.lister()) == 1)\n    self.pet_tools.setter(pet_name='gina', pet_desc='very compulsive')\n    assert (len(self.pet_tools.lister()) == 1)\n    for pet in self.pet_tools.lister():\n        assert (pet.pet_desc == 'very compulsive')\n", "label": 0}
{"function": "\n\n@staticmethod\ndef get_tags(problem_link):\n    response = get_request(problem_link)\n    if ((response == (- 1)) or (response == {\n        \n    })):\n        return ['-']\n    tags = BeautifulSoup(response.text).find_all('div', id='problem-tags')\n    try:\n        tags = tags[0].findAll('span')\n    except IndexError:\n        return ['-']\n    all_tags = []\n    for tag in tags:\n        tmp = tag.contents\n        if (tmp != []):\n            all_tags.append(tmp[0][1:])\n    return all_tags\n", "label": 0}
{"function": "\n\ndef migrate(name, target=''):\n    '\\n    Migrate a VM from one host to another. This routine will just start\\n    the migration and display information on how to look up the progress.\\n    '\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    data = query(quiet=True)\n    origin_data = _find_vm(name, data, quiet=True)\n    try:\n        origin_host = list(origin_data.keys())[0]\n    except IndexError:\n        __jid_event__.fire_event({\n            'error': 'Named VM {0} was not found to migrate'.format(name),\n        }, 'progress')\n        return ''\n    disks = origin_data[origin_host][name]['disks']\n    if (not origin_data):\n        __jid_event__.fire_event({\n            'error': 'Named VM {0} was not found to migrate'.format(name),\n        }, 'progress')\n        return ''\n    if (not target):\n        target = _determine_host(data, origin_host)\n    if (target not in data):\n        __jid_event__.fire_event({\n            'error': 'Target host {0} not found'.format(origin_data),\n        }, 'progress')\n        return ''\n    try:\n        client.cmd(target, 'virt.seed_non_shared_migrate', [disks, True])\n        jid = client.cmd_async(origin_host, 'virt.migrate_non_shared', [name, target])\n    except SaltClientError as client_error:\n        return 'Virtual machine {0} could not be migrated: {1}'.format(name, client_error)\n    msg = 'The migration of virtual machine {0} to host {1} has begun, and can be tracked via jid {2}. The ``salt-run virt.query`` runner can also be used, the target VM will be shown as paused until the migration is complete.'.format(name, target, jid)\n    __jid_event__.fire_event({\n        'message': msg,\n    }, 'progress')\n", "label": 0}
{"function": "\n\ndef main(model, out_fpath):\n    store = pd.HDFStore(model)\n    from_ = store['from_'][0][0]\n    to = store['to'][0][0]\n    assert (from_ == 0)\n    trace_fpath = store['trace_fpath'][0][0]\n    kernel_class = store['kernel_class'][0][0]\n    kernel_class = eval(kernel_class)\n    Theta_zh = store['Theta_zh'].values\n    Psi_sz = store['Psi_sz'].values\n    count_z = store['count_z'].values[:, 0]\n    P = store['P'].values\n    residency_priors = store['residency_priors'].values[:, 0]\n    previous_stamps = StampLists(count_z.shape[0])\n    mem_size = store['Dts'].values.shape[1]\n    tstamps = store['Dts'].values[:, 0]\n    assign = store['assign'].values[:, 0]\n    for z in xrange(count_z.shape[0]):\n        idx = (assign == z)\n        previous_stamps._extend(z, tstamps[idx])\n    hyper2id = dict(store['hyper2id'].values)\n    obj2id = dict(store['source2id'].values)\n    HSDs = []\n    Dts = []\n    with open(trace_fpath) as trace_file:\n        for (i, l) in enumerate(trace_file):\n            if (i < to):\n                continue\n            spl = l.strip().split('\\t')\n            dts_line = [float(x) for x in spl[:mem_size]]\n            h = spl[mem_size]\n            d = spl[(- 1)]\n            sources = spl[(mem_size + 1):(- 1)]\n            all_in = ((h in hyper2id) and (d in obj2id))\n            for s in sources:\n                all_in = (all_in and (s in obj2id))\n            if all_in:\n                trace_line = (([hyper2id[h]] + [obj2id[s] for s in sources]) + [obj2id[d]])\n                HSDs.append(trace_line)\n                Dts.append(dts_line)\n    trace_size = sum(count_z)\n    kernel = kernel_class()\n    kernel.build(trace_size, count_z.shape[0], residency_priors)\n    kernel.update_state(P)\n    num_queries = min(10000, len(HSDs))\n    queries = np.random.choice(len(HSDs), size=num_queries)\n    HSDs = np.array(HSDs, dtype='i4')[queries].copy()\n    Dts = np.array(Dts, dtype='d')[queries].copy()\n    rrs = _learn.reciprocal_rank(Dts, HSDs, previous_stamps, Theta_zh, Psi_sz, count_z, kernel)\n    np.savetxt(out_fpath, rrs)\n    print(rrs.mean(axis=0))\n    store.close()\n", "label": 1}
{"function": "\n\n@skipIf(True, 'Skipping until we can find why Jenkins is bailing out')\ndef test_max_open_files(self):\n    with TestsLoggingHandler() as handler:\n        logmsg_dbg = 'DEBUG:This salt-master instance has accepted {0} minion keys.'\n        logmsg_chk = \"{0}:The number of accepted minion keys({1}) should be lower than 1/4 of the max open files soft setting({2}). According to the system's hard limit, there's still a margin of {3} to raise the salt's max_open_files setting. Please consider raising this value.\"\n        logmsg_crash = \"{0}:The number of accepted minion keys({1}) should be lower than 1/4 of the max open files soft setting({2}). salt-master will crash pretty soon! According to the system's hard limit, there's still a margin of {3} to raise the salt's max_open_files setting. Please consider raising this value.\"\n        (mof_s, mof_h) = resource.getrlimit(resource.RLIMIT_NOFILE)\n        tempdir = tempfile.mkdtemp(prefix='fake-keys')\n        keys_dir = os.path.join(tempdir, 'minions')\n        os.makedirs(keys_dir)\n        mof_test = 256\n        resource.setrlimit(resource.RLIMIT_NOFILE, (mof_test, mof_h))\n        try:\n            prev = 0\n            for (newmax, level) in ((24, None), (66, 'INFO'), (127, 'WARNING'), (196, 'CRITICAL')):\n                for n in range(prev, newmax):\n                    kpath = os.path.join(keys_dir, str(n))\n                    with salt.utils.fopen(kpath, 'w') as fp_:\n                        fp_.write(str(n))\n                opts = {\n                    'max_open_files': newmax,\n                    'pki_dir': tempdir,\n                }\n                check_max_open_files(opts)\n                if (level is None):\n                    self.assertEqual([logmsg_dbg.format(newmax)], handler.messages)\n                else:\n                    self.assertIn(logmsg_dbg.format(newmax), handler.messages)\n                    self.assertIn(logmsg_chk.format(level, newmax, mof_test, (mof_h - newmax)), handler.messages)\n                handler.clear()\n                prev = newmax\n            newmax = mof_test\n            for n in range(prev, newmax):\n                kpath = os.path.join(keys_dir, str(n))\n                with salt.utils.fopen(kpath, 'w') as fp_:\n                    fp_.write(str(n))\n            opts = {\n                'max_open_files': newmax,\n                'pki_dir': tempdir,\n            }\n            check_max_open_files(opts)\n            self.assertIn(logmsg_dbg.format(newmax), handler.messages)\n            self.assertIn(logmsg_crash.format('CRITICAL', newmax, mof_test, (mof_h - newmax)), handler.messages)\n            handler.clear()\n        except IOError as err:\n            if (err.errno == 24):\n                self.skipTest(\"We've hit the max open files setting\")\n            raise\n        finally:\n            shutil.rmtree(tempdir)\n            resource.setrlimit(resource.RLIMIT_NOFILE, (mof_s, mof_h))\n", "label": 0}
{"function": "\n\ndef is_server_new_enough(client, min_version, default=False, prior_version=None):\n    'Checks if a client is attached to a new enough redis server.'\n    if (not prior_version):\n        try:\n            server_info = client.info()\n        except redis_exceptions.ResponseError:\n            server_info = {\n                \n            }\n        version_text = server_info.get('redis_version', '')\n    else:\n        version_text = prior_version\n    version_pieces = []\n    for p in version_text.split('.'):\n        try:\n            version_pieces.append(int(p))\n        except ValueError:\n            break\n    if (not version_pieces):\n        return (default, version_text)\n    else:\n        version_pieces = tuple(version_pieces)\n        return ((version_pieces >= min_version), version_text)\n", "label": 0}
{"function": "\n\ndef init_config():\n    '\\n    Init configuration\\n    '\n    config = (os.path.dirname(__file__) + '/colorset/config')\n    try:\n        data = load_config(config)\n        for d in data:\n            c[d] = data[d]\n    except:\n        pass\n    rainbow_config = ((os.path.expanduser('~') + os.sep) + '.rainbow_config.json')\n    try:\n        data = load_config(rainbow_config)\n        for d in data:\n            c[d] = data[d]\n    except (IOError, ValueError) as e:\n        c['USER_JSON_ERROR'] = str(e)\n    theme_file = (((os.path.dirname(__file__) + '/colorset/') + c['THEME']) + '.json')\n    try:\n        data = load_config(theme_file)\n        for d in data:\n            c[d] = data[d]\n    except:\n        pass\n", "label": 0}
{"function": "\n\ndef __new__(cls, value):\n    if (type(value) is cls):\n        value = value.value\n    try:\n        if (value in cls._value2member_map_):\n            return cls._value2member_map_[value]\n    except TypeError:\n        for member in cls._member_map_.values():\n            if (member.value == value):\n                return member\n    raise ValueError(('%s is not a valid %s' % (value, cls.__name__)))\n", "label": 0}
{"function": "\n\ndef check(self, ip, port=None):\n    '\\n        '\n    try:\n        rport = (self.port if (port is None) else port)\n        url = 'http://{0}:{1}{2}'.format(ip, rport, self.uri)\n        response = utility.requests_get(url)\n        found = findall('Apache Tomcat/(.*?)\\n', response.content)\n        if ((len(found) > 0) and (self.version in found[0])):\n            return True\n    except exceptions.Timeout:\n        utility.Msg('{0} timeout to {1}:{2}'.format(self.platform, ip, rport), LOG.DEBUG)\n    except exceptions.ConnectionError:\n        utility.Msg('{0} connection error to {1}:{2}'.format(self.platform, ip, rport), LOG.DEBUG)\n    return False\n", "label": 0}
{"function": "\n\n@click.group(context_settings={\n    'max_content_width': 100,\n}, epilog='Suggestions and bug reports are greatly appreciated: https://github.com/Diaoul/subliminal/')\n@click.option('--addic7ed', type=click.STRING, nargs=2, metavar='USERNAME PASSWORD', help='Addic7ed configuration.')\n@click.option('--opensubtitles', type=click.STRING, nargs=2, metavar='USERNAME PASSWORD', help='OpenSubtitles configuration.')\n@click.option('--subscenter', type=click.STRING, nargs=2, metavar='USERNAME PASSWORD', help='SubsCenter configuration.')\n@click.option('--cache-dir', type=click.Path(writable=True, resolve_path=True, file_okay=False), default=app_dir, show_default=True, expose_value=True, help='Path to the cache directory.')\n@click.option('--debug', is_flag=True, help='Print useful information for debugging subliminal and for reporting bugs.')\n@click.version_option(__version__)\n@click.pass_context\ndef subliminal(ctx, addic7ed, opensubtitles, subscenter, cache_dir, debug):\n    'Subtitles, faster than your thoughts.'\n    try:\n        os.makedirs(cache_dir)\n    except OSError:\n        if (not os.path.isdir(cache_dir)):\n            raise\n    region.configure('dogpile.cache.dbm', expiration_time=timedelta(days=30), arguments={\n        'filename': os.path.join(cache_dir, cache_file),\n        'lock_factory': MutexLock,\n    })\n    if debug:\n        handler = logging.StreamHandler()\n        handler.setFormatter(logging.Formatter(logging.BASIC_FORMAT))\n        logging.getLogger('subliminal').addHandler(handler)\n        logging.getLogger('subliminal').setLevel(logging.DEBUG)\n    ctx.obj = {\n        'provider_configs': {\n            \n        },\n    }\n    if addic7ed:\n        ctx.obj['provider_configs']['addic7ed'] = {\n            'username': addic7ed[0],\n            'password': addic7ed[1],\n        }\n    if opensubtitles:\n        ctx.obj['provider_configs']['opensubtitles'] = {\n            'username': opensubtitles[0],\n            'password': opensubtitles[1],\n        }\n    if subscenter:\n        ctx.obj['provider_configs']['subscenter'] = {\n            'username': subscenter[0],\n            'password': subscenter[1],\n        }\n", "label": 0}
{"function": "\n\ndef test_manual_clustering_move_2(manual_clustering):\n    mc = manual_clustering\n    mc.select([20])\n    mc.similarity_view.select([10])\n    assert (mc.selected == [20, 10])\n    mc.move([10], 'noise')\n    assert (mc.selected == [20, 2])\n    mc.undo()\n    assert (mc.selected == [20, 10])\n    mc.redo()\n    assert (mc.selected == [20, 2])\n", "label": 0}
{"function": "\n\ndef _next_method(self):\n    'Read the next method from the source and process it\\n\\n        Once one complete method has been assembled, it is placed in the internal queue. This\\n        method will block until a complete `Method` has been constructed, which may consist of one\\n        or more frames.\\n        '\n    while (not self.method_queue):\n        try:\n            frame = self.transport.read_frame()\n        except Exception as exc:\n            if six.PY2:\n                (_, _, tb) = sys.exc_info()\n                exc.tb = tb\n            self.method_queue.append(exc)\n            break\n        self.frames_recv += 1\n        if (frame.frame_type not in (self.expected_types[frame.channel], 8)):\n            msg = 'Received frame type {} while expecting type: {}'.format(frame.frame_type, self.expected_types[frame.channel])\n            self.method_queue.append(UnexpectedFrame(msg, channel_id=frame.channel))\n        elif (frame.frame_type == FrameType.METHOD):\n            self._process_method_frame(frame)\n        elif (frame.frame_type == FrameType.HEADER):\n            self._process_content_header(frame)\n        elif (frame.frame_type == FrameType.BODY):\n            self._process_content_body(frame)\n", "label": 1}
{"function": "\n\ndef __aggregate_operation_result(self, total_result, key, value):\n    agg_val = total_result.get(key)\n    assert (agg_val is not None), ('Unknow operation result %s=%s (unrecognized key)' % (key, value))\n    if isinstance(agg_val, int):\n        total_result[key] += value\n    elif isinstance(agg_val, list):\n        if (key == 'upserted'):\n            new_element = {\n                'index': len(agg_val),\n                '_id': value,\n            }\n            agg_val.append(new_element)\n        else:\n            agg_val.append(value)\n    else:\n        assert False, ('Fixme: missed aggreation rule for type: %s for key {%s=%s}' % (type(agg_val), key, agg_val))\n", "label": 0}
{"function": "\n\ndef check_hash(password, hash_):\n    'Check a password against an existing hash.'\n    if isinstance(password, unicode):\n        password = password.encode('utf-8')\n    (algorithm, hash_function, cost_factor, salt, hash_a) = hash_.split('$')\n    assert (algorithm == 'PBKDF2')\n    hash_a = b64decode(hash_a)\n    hash_b = pbkdf2_bin(password, salt, int(cost_factor), len(hash_a), getattr(hashlib, hash_function))\n    assert (len(hash_a) == len(hash_b))\n    diff = 0\n    for (char_a, char_b) in izip(hash_a, hash_b):\n        diff |= (ord(char_a) ^ ord(char_b))\n    return (diff == 0)\n", "label": 0}
{"function": "\n\ndef read_conf(conf_file, out_format='simple'):\n    \"\\n    Read in an LXC configuration file. By default returns a simple, unsorted\\n    dict, but can also return a more detailed structure including blank lines\\n    and comments.\\n\\n    out_format:\\n        set to 'simple' if you need the old and unsupported behavior.\\n        This won't support the multiple lxc values (eg: multiple network nics)\\n\\n    CLI Examples:\\n\\n    .. code-block:: bash\\n\\n        salt 'minion' lxc.read_conf /etc/lxc/mycontainer.conf\\n        salt 'minion' lxc.read_conf /etc/lxc/mycontainer.conf out_format=commented\\n    \"\n    ret_commented = []\n    ret_simple = {\n        \n    }\n    with salt.utils.fopen(conf_file, 'r') as fp_:\n        for line in fp_.readlines():\n            if ('=' not in line):\n                ret_commented.append(line)\n                continue\n            comps = line.split('=')\n            value = '='.join(comps[1:]).strip()\n            comment = None\n            if value.strip().startswith('#'):\n                vcomps = value.strip().split('#')\n                value = vcomps[1].strip()\n                comment = '#'.join(vcomps[1:]).strip()\n                ret_commented.append({\n                    comps[0].strip(): {\n                        'value': value,\n                        'comment': comment,\n                    },\n                })\n            else:\n                ret_commented.append({\n                    comps[0].strip(): value,\n                })\n                ret_simple[comps[0].strip()] = value\n    if (out_format == 'simple'):\n        return ret_simple\n    return ret_commented\n", "label": 0}
{"function": "\n\ndef db_change_module(self, module):\n    self.is_dirty = True\n    try:\n        i = self.db_modules_inverted_id_index[module._db_id]\n    except KeyError:\n        found = False\n        for i in xrange(len(self._db_modules)):\n            if (self._db_modules[i]._db_id == module._db_id):\n                found = True\n                break\n        if (not found):\n            self.db_add_module(module)\n            return\n    self._db_modules[i] = module\n    self.db_modules_id_index[module._db_id] = module\n", "label": 0}
{"function": "\n\ndef __init__(self, optimizers, failure_callback=None, ignore_newtrees=True, max_use_ratio=None, final_optimizers=None, cleanup_optimizers=None):\n    super(EquilibriumOptimizer, self).__init__(None, ignore_newtrees=ignore_newtrees, failure_callback=failure_callback)\n    self.local_optimizers_map = OrderedDict()\n    self.local_optimizers_all = []\n    self.global_optimizers = []\n    self.final_optimizers = []\n    self.cleanup_optimizers = []\n    for opt in optimizers:\n        if isinstance(opt, LocalOptimizer):\n            if (opt.tracks() is None):\n                self.local_optimizers_all.append(opt)\n            else:\n                for c in opt.tracks():\n                    self.local_optimizers_map.setdefault(c, []).append(opt)\n        else:\n            self.global_optimizers.append(opt)\n    if final_optimizers:\n        self.final_optimizers = final_optimizers\n    if cleanup_optimizers:\n        self.cleanup_optimizers = cleanup_optimizers\n    self.max_use_ratio = max_use_ratio\n    assert (self.max_use_ratio is not None), 'max_use_ratio has to be a number'\n", "label": 1}
{"function": "\n\ndef is_enabled(revision_config=None):\n    is_compatible = True\n    is_enabled = common_module.config.get('flamegraph', False)\n    if revision_config:\n        if (revision_config.get('product', 'cassandra') == 'dse'):\n            logger.info('Flamegraph is not compatible with DSE yet')\n            is_compatible = False\n        jvm = revision_config.get('java_home', '')\n        try:\n            jvm = os.path.basename(jvm)\n            jvm = jvm[jvm.index('1'):]\n            if (v.LooseVersion(jvm) < v.LooseVersion('1.8.0_60')):\n                logger.info('Flamegraph is not compatible with java <1.8.0_60')\n                is_compatible = False\n        except ValueError:\n            pass\n    return (is_enabled and is_compatible)\n", "label": 0}
{"function": "\n\ndef process_regular_2d_scalars(*args, **kwargs):\n    ' Converts different signatures to (x, y, s). '\n    args = convert_to_arrays(args)\n    for (index, arg) in enumerate(args):\n        if (not callable(arg)):\n            args[index] = np.atleast_2d(arg)\n    if (len(args) == 1):\n        s = args[0]\n        assert (len(s.shape) == 2), '2D array required'\n        (x, y) = np.indices(s.shape)\n    elif (len(args) == 3):\n        (x, y, s) = args\n        if callable(s):\n            s = s(x, y)\n    else:\n        raise ValueError('wrong number of arguments')\n    assert (len(s.shape) == 2), '2D array required'\n    if ('mask' in kwargs):\n        mask = kwargs['mask']\n        s[mask.astype('bool')] = np.nan\n        s = s.astype('float')\n    return (x, y, s)\n", "label": 1}
{"function": "\n\ndef draw_networkx_labels(G, pos, labels=None, font_size=12, font_color='k', font_family='sans-serif', font_weight='normal', alpha=1.0, ax=None, **kwds):\n    \"Draw node labels on the graph G.\\n\\n    Parameters\\n    ----------\\n    G : graph\\n       A networkx graph\\n\\n    pos : dictionary, optional\\n       A dictionary with nodes as keys and positions as values.\\n       If not specified a spring layout positioning will be computed.\\n       See networkx.layout for functions that compute node positions.\\n\\n    labels : dictionary, optional (default=None)\\n       Node labels in a dictionary keyed by node of text labels\\n\\n    font_size : int\\n       Font size for text labels (default=12)\\n\\n    font_color : string\\n       Font color string (default='k' black)\\n\\n    font_family : string\\n       Font family (default='sans-serif')\\n\\n    font_weight : string\\n       Font weight (default='normal')\\n\\n    alpha : float\\n       The text transparency (default=1.0)\\n\\n    ax : Matplotlib Axes object, optional\\n       Draw the graph in the specified Matplotlib axes.\\n\\n\\n    Examples\\n    --------\\n    >>> G=nx.dodecahedral_graph()\\n    >>> labels=nx.draw_networkx_labels(G,pos=nx.spring_layout(G))\\n\\n    Also see the NetworkX drawing examples at\\n    http://networkx.lanl.gov/gallery.html\\n\\n\\n    See Also\\n    --------\\n    draw()\\n    draw_networkx()\\n    draw_networkx_nodes()\\n    draw_networkx_edges()\\n    draw_networkx_edge_labels()\\n    \"\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.cbook as cb\n    except ImportError:\n        raise ImportError('Matplotlib required for draw()')\n    except RuntimeError:\n        print('Matplotlib unable to open display')\n        raise\n    if (ax is None):\n        ax = plt.gca()\n    if (labels is None):\n        labels = dict(((n, n) for n in G.nodes()))\n    horizontalalignment = kwds.get('horizontalalignment', 'center')\n    verticalalignment = kwds.get('verticalalignment', 'center')\n    text_items = {\n        \n    }\n    for (n, label) in labels.items():\n        (x, y) = pos[n]\n        if (not cb.is_string_like(label)):\n            label = str(label)\n        t = ax.text(x, y, label, size=font_size, color=font_color, family=font_family, weight=font_weight, horizontalalignment=horizontalalignment, verticalalignment=verticalalignment, transform=ax.transData, clip_on=True)\n        text_items[n] = t\n    return text_items\n", "label": 1}
{"function": "\n\ndef norm_cspace_id(cspace):\n    try:\n        cspace = ALIASES[cspace]\n    except (KeyError, TypeError):\n        pass\n    if isinstance(cspace, str):\n        if _CIECAM02_axes.issuperset(cspace):\n            return {\n                'name': 'CIECAM02-subset',\n                'ciecam02_space': CIECAM02Space.sRGB,\n                'axes': cspace,\n            }\n        else:\n            return {\n                'name': cspace,\n            }\n    elif isinstance(cspace, CIECAM02Space):\n        return {\n            'name': 'CIECAM02',\n            'ciecam02_space': cspace,\n        }\n    elif isinstance(cspace, LuoEtAl2006UniformSpace):\n        return {\n            'name': \"J'a'b'\",\n            'ciecam02_space': CIECAM02Space.sRGB,\n            'luoetal2006_space': cspace,\n        }\n    elif isinstance(cspace, dict):\n        if (cspace['name'] in ALIASES):\n            base = ALIASES[cspace['name']]\n            if (isinstance(base, dict) and (base['name'] == cspace['name'])):\n                return cspace\n            else:\n                base = norm_cspace_id(base)\n                cspace = dict(cspace)\n                del cspace['name']\n                base = dict(base)\n                base.update(cspace)\n                return base\n        return cspace\n    else:\n        raise ValueError(('unrecognized color space %r' % (cspace,)))\n", "label": 1}
{"function": "\n\ndef rpm_install(rpm_name):\n    _LOGGER.info('Installing the rpm')\n    nodeps = ''\n    if env.nodeps:\n        nodeps = '--nodeps '\n    if (('java8_home' not in env) or (env.java8_home is None)):\n        ret = sudo(('rpm -i %s%s' % (nodeps, os.path.join(constants.REMOTE_PACKAGES_PATH, rpm_name))))\n    else:\n        with shell_env(JAVA8_HOME=('%s' % env.java8_home)):\n            ret = sudo(('rpm -i %s%s' % (nodeps, os.path.join(constants.REMOTE_PACKAGES_PATH, rpm_name))))\n    if ret.succeeded:\n        print(('Package installed successfully on: ' + env.host))\n", "label": 0}
{"function": "\n\ndef rapidReconnectionTest(self, harness, totalThreadCount, passCount, blockUntilConnected):\n    for passIndex in range(passCount):\n        successes = []\n        failures = []\n        logging.info('Starting connect-pass %s', passIndex)\n        threads = []\n        connectedThreads = []\n        viewList = []\n        allConnectedEvent = threading.Event()\n        for threadIndex in range(totalThreadCount):\n            threads.append(threading.Thread(target=self.clientThread, args=(harness, passIndex, threadIndex, totalThreadCount, successes, failures, connectedThreads, allConnectedEvent, viewList, blockUntilConnected)))\n            threads[(- 1)].start()\n        for t in threads:\n            t.join()\n        logging.debug('All reconnection test threads joined.')\n        viewList = []\n        logging.debug('All views destroyed.')\n        for f in failures:\n            logging.warn('Failure: %s', f)\n        assert (len(successes) == totalThreadCount), ('Failed on pass %s. %s instead of %s. %s marked failures' % (passIndex, len(successes), totalThreadCount, len(failures)))\n        successes = []\n        failures = []\n        time.sleep(0.25)\n        logging.info('DONE SLEEPING\\n\\n\\n\\n\\n')\n", "label": 0}
{"function": "\n\ndef eucalyptus(account):\n    myCredAccount = CredAccountEws()\n    if (not ('secretKey' in account)):\n        printer.out('secretKey in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('queryId' in account)):\n        printer.out('queryId in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('endpoint' in account)):\n        printer.out('endpoint in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('cloudCert' in account)):\n        printer.out('cloudCert in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('x509Cert' in account)):\n        printer.out('x509Cert in eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('x509PrivateKey' in account)):\n        printer.out('x509PrivateKey in azure eucalyptus not found', printer.ERROR)\n        return\n    if (not ('accountNumber' in account)):\n        printer.out('accountNumber for eucalyptus account not found', printer.ERROR)\n        return\n    if (not ('name' in account)):\n        printer.out('name for eucalyptus account not found', printer.ERROR)\n        return\n    myCredAccount.accountNumber = account['accountNumber']\n    myCredAccount.name = account['name']\n    myCredAccount.hostname = account['endpoint']\n    myCredAccount.secretAccessKeyID = account['secretKey']\n    myCredAccount.accessKeyID = account['queryId']\n    myCertificates = certificates()\n    myCredAccount.certificates = myCertificates\n    try:\n        myCertificate = certificate()\n        with open(account['x509Cert'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'x509'\n        myCertificate.name = ntpath.basename(account['x509Cert'])\n        myCertificates.add_certificate(myCertificate)\n        myCertificate = certificate()\n        with open(account['x509PrivateKey'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'ec2PrivateKey'\n        myCertificate.name = ntpath.basename(account['x509PrivateKey'])\n        myCertificates.add_certificate(myCertificate)\n        myCertificate = certificate()\n        with open(account['cloudCert'], 'r') as myfile:\n            myCertificate.certStr = myfile.read()\n        myCertificate.type_ = 'eucCert'\n        myCertificate.name = ntpath.basename(account['cloudCert'])\n        myCertificates.add_certificate(myCertificate)\n    except IOError as e:\n        printer.out(('File error: ' + str(e)), printer.ERROR)\n        return\n    return myCredAccount\n", "label": 1}
{"function": "\n\ndef _do_rebuild_instance(self, context, instance, orig_image_ref, image_ref, injected_files, new_pass, orig_sys_metadata, bdms, recreate, on_shared_storage, preserve_ephemeral):\n    orig_vm_state = instance.vm_state\n    if recreate:\n        if (not self.driver.capabilities['supports_recreate']):\n            raise exception.InstanceRecreateNotSupported\n        self._check_instance_exists(context, instance)\n        if (on_shared_storage is None):\n            LOG.debug('on_shared_storage is not provided, using driverinformation to decide if the instance needs tobe recreated')\n            on_shared_storage = self.driver.instance_on_disk(instance)\n        elif (on_shared_storage != self.driver.instance_on_disk(instance)):\n            raise exception.InvalidSharedStorage(_('Invalid state of instance files on shared storage'))\n        if on_shared_storage:\n            LOG.info(_LI('disk on shared storage, recreating using existing disk'))\n        else:\n            image_ref = orig_image_ref = instance.image_ref\n            LOG.info(_LI(\"disk not on shared storage, rebuilding from: '%s'\"), str(image_ref))\n    if image_ref:\n        image_meta = objects.ImageMeta.from_image_ref(context, self.image_api, image_ref)\n    else:\n        image_meta = objects.ImageMeta.from_dict({\n            \n        })\n    orig_image_ref_url = glance.generate_image_url(orig_image_ref)\n    extra_usage_info = {\n        'image_ref_url': orig_image_ref_url,\n    }\n    compute_utils.notify_usage_exists(self.notifier, context, instance, current_period=True, system_metadata=orig_sys_metadata, extra_usage_info=extra_usage_info)\n    extra_usage_info = {\n        'image_name': self._get_image_name(image_meta),\n    }\n    self._notify_about_instance_usage(context, instance, 'rebuild.start', extra_usage_info=extra_usage_info)\n    instance.power_state = self._get_power_state(context, instance)\n    instance.task_state = task_states.REBUILDING\n    instance.save(expected_task_state=[task_states.REBUILDING])\n    if recreate:\n        self.network_api.setup_networks_on_host(context, instance, self.host)\n        self.network_api.setup_instance_network_on_host(context, instance, self.host)\n    network_info = compute_utils.get_nw_info_for_instance(instance)\n    if (bdms is None):\n        bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(context, instance.uuid)\n    block_device_info = self._get_instance_block_device_info(context, instance, bdms=bdms)\n\n    def detach_block_devices(context, bdms):\n        for bdm in bdms:\n            if bdm.is_volume:\n                self._detach_volume(context, bdm.volume_id, instance, destroy_bdm=False)\n    files = self._decode_files(injected_files)\n    kwargs = dict(context=context, instance=instance, image_meta=image_meta, injected_files=files, admin_password=new_pass, bdms=bdms, detach_block_devices=detach_block_devices, attach_block_devices=self._prep_block_device, block_device_info=block_device_info, network_info=network_info, preserve_ephemeral=preserve_ephemeral, recreate=recreate)\n    try:\n        with instance.mutated_migration_context():\n            self.driver.rebuild(**kwargs)\n    except NotImplementedError:\n        self._rebuild_default_impl(**kwargs)\n    self._update_instance_after_spawn(context, instance)\n    instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])\n    if (orig_vm_state == vm_states.STOPPED):\n        LOG.info(_LI(\"bringing vm to original state: '%s'\"), orig_vm_state, instance=instance)\n        instance.vm_state = vm_states.ACTIVE\n        instance.task_state = task_states.POWERING_OFF\n        instance.progress = 0\n        instance.save()\n        self.stop_instance(context, instance, False)\n    self._update_scheduler_instance_info(context, instance)\n    self._notify_about_instance_usage(context, instance, 'rebuild.end', network_info=network_info, extra_usage_info=extra_usage_info)\n", "label": 1}
{"function": "\n\n@classmethod\ndef create_user(cls, auth_id, unique_properties=None, **user_values):\n    'Creates a new user.\\n\\n        :param auth_id:\\n            A string that is unique to the user. Users may have multiple\\n            auth ids. Example auth ids:\\n\\n            - own:username\\n            - own:email@example.com\\n            - google:username\\n            - yahoo:username\\n\\n            The value of `auth_id` must be unique.\\n        :param unique_properties:\\n            Sequence of extra property names that must be unique.\\n        :param user_values:\\n            Keyword arguments to create a new user entity. Since the model is\\n            an ``Expando``, any provided custom properties will be saved.\\n            To hash a plain password, pass a keyword ``password_raw``.\\n        :returns:\\n            A tuple (boolean, info). The boolean indicates if the user\\n            was created. If creation succeeds, ``info`` is the user entity;\\n            otherwise it is a list of duplicated unique properties that\\n            caused creation to fail.\\n        '\n    assert (user_values.get('password') is None), 'Use password_raw instead of password to create new users.'\n    assert (not isinstance(auth_id, list)), 'Creating a user with multiple auth_ids is not allowed, please provide a single auth_id.'\n    if ('password_raw' in user_values):\n        user_values['password'] = security.generate_password_hash(user_values.pop('password_raw'), length=12)\n    user_values['auth_ids'] = [auth_id]\n    user = cls(**user_values)\n    uniques = [(('%s.auth_id:%s' % (cls.__name__, auth_id)), 'auth_id')]\n    if unique_properties:\n        for name in unique_properties:\n            key = ('%s.%s:%s' % (cls.__name__, name, user_values[name]))\n            uniques.append((key, name))\n    (ok, existing) = cls.unique_model.create_multi((k for (k, v) in uniques))\n    if ok:\n        user.put()\n        return (True, user)\n    else:\n        properties = [v for (k, v) in uniques if (k in existing)]\n        return (False, properties)\n", "label": 1}
{"function": "\n\ndef __setitem__(self, key, value):\n    if (isinstance(value, bytearray) or isinstance(value, str)):\n        start_addr = 0\n        stop_addr = 0\n        if isinstance(key, slice):\n            start_addr = int(key.start)\n            stop_addr = int(key.stop)\n        else:\n            start_addr = int(key)\n            stop_addr = (int(key) + 1)\n        self.mem[start_addr:stop_addr] = value\n    else:\n        start_addr = 0\n        num_bytes = 0\n        if isinstance(key, slice):\n            start_addr = int(key.start)\n            num_bytes = (int(key.stop) - int(key.start))\n        else:\n            start_addr = int(key)\n            num_bytes = 1\n        if isinstance(value, Bits):\n            bits = value\n            assert ((value.nbits % 8) == 0)\n        else:\n            bits = Bits((num_bytes * 8), value)\n        for i in range(num_bytes):\n            self.mem[(start_addr + i)] = bits[(i * 8):((i * 8) + 8)]\n", "label": 1}
{"function": "\n\ndef __call__(self, name, *args):\n    name = name.decode('ascii')\n    if name.startswith('_'):\n        return (b'_error', b'bad_name')\n    meth = getattr(self, name, None)\n    if ((meth is None) or (not callable(meth))):\n        return (b'_error', b'no_method')\n    try:\n        args = tuple(map(self.loads, args))\n    except Exception as e:\n        return (b'_error', b'unpacking_error')\n    try:\n        result = meth(*args)\n    except Exception as e:\n        return (b'_exception', repr(e))\n    try:\n        result = self.dumps(result)\n    except Exception as e:\n        return (b'_error', b'packing_error')\n    return (b'_result', result)\n", "label": 0}
{"function": "\n\ndef set_size(self, width, height):\n    '\\n        Set terminal size.\\n        '\n    assert isinstance(width, int)\n    assert isinstance(height, int)\n    if (self.master is not None):\n        if ((self.sx, self.sy) != (width, height)):\n            set_terminal_size(self.master, height, width)\n    self.screen.resize(lines=height, columns=width)\n    self.screen.lines = height\n    self.screen.columns = width\n    self.sx = width\n    self.sy = height\n", "label": 0}
{"function": "\n\ndef find_credentials(host):\n    '\\n    Cycle through all the possible credentials and return the first one that\\n    works.\\n    '\n    user_names = [__pillar__['proxy'].get('username', 'root')]\n    passwords = __pillar__['proxy']['passwords']\n    for user in user_names:\n        for password in passwords:\n            try:\n                ret = __salt__['vsphere.system_info'](host=host, username=user, password=password)\n            except SaltSystemExit:\n                continue\n            if ret:\n                DETAILS['username'] = user\n                DETAILS['password'] = password\n                return (user, password)\n    raise SaltSystemExit('Cannot complete login due to an incorrect user name or password.')\n", "label": 0}
{"function": "\n\ndef prepare(args):\n    '\\n    %prog prepare genomesize *.fastq\\n\\n    Prepare MERACULOUS configuation file. Genome size should be entered in Mb.\\n    '\n    p = OptionParser((prepare.__doc__ + FastqNamings))\n    p.add_option('-K', default=51, type='int', help='K-mer size')\n    p.set_cpus(cpus=32)\n    (opts, args) = p.parse_args(args)\n    if (len(args) < 2):\n        sys.exit((not p.print_help()))\n    genomesize = (float(args[0]) / 1000)\n    fnames = args[1:]\n    for x in fnames:\n        assert op.exists(x), 'File `{0}` not found.'.format(x)\n    s = (comment_banner('Meraculous params file') + '\\n')\n    s += (comment_banner('Basic parameters') + '\\n')\n    s += '# Describe the libraries ( one line per library )\\n'\n    s += (('# ' + ' '.join(header.split())) + '\\n')\n    libs = get_libs(fnames)\n    lib_seqs = []\n    rank = 0\n    for (lib, fs) in libs:\n        size = lib.size\n        if (size == 0):\n            continue\n        rank += 1\n        library_name = lib.library_name\n        name = library_name.replace('-', '')\n        wildcard = '{0}*.1.*,{0}*.2.*'.format(library_name)\n        rl = max((readlen([x]) for x in fs))\n        lib_seq = lib.get_lib_seq(wildcard, name, rl, rank)\n        lib_seqs.append(lib_seq)\n    s += (('\\n' + '\\n'.join(load_csv(None, lib_seqs, sep=' '))) + '\\n')\n    params = [('genome_size', genomesize), ('is_diploid', 0), ('mer_size', opts.K), ('num_prefix_blocks', 1), ('no_read_validation', 0), ('local_num_procs', opts.cpus)]\n    s += (('\\n' + '\\n'.join(load_csv(None, params, sep=' '))) + '\\n')\n    cfgfile = 'meraculous.config'\n    write_file(cfgfile, s, tee=True)\n    s = '~/export/meraculous/bin/run_meraculous.sh -c {0}'.format(cfgfile)\n    runsh = 'run.sh'\n    write_file(runsh, s)\n", "label": 0}
{"function": "\n\ndef success(self):\n    if ((self.status == httplib.OK) or (self.status == httplib.CREATED)):\n        try:\n            j_body = json.loads(self.body)\n        except ValueError:\n            self.error = 'JSON response cannot be decoded.'\n            return False\n        if (j_body['errno'] == 0):\n            return True\n        else:\n            self.error = ('ECP error: %s' % j_body['message'])\n            return False\n    elif (self.status == httplib.UNAUTHORIZED):\n        raise InvalidCredsException()\n    else:\n        self.error = ('HTTP Error Code: %s' % self.status)\n    return False\n", "label": 0}
{"function": "\n\ndef push_monitor(model, name, transfer_experience=False, save_records=False):\n    '\\n    When you load a model in a yaml file and you want to store its\\n    old monitor under a different name and start a new monitor, wrap\\n    the model in this function call.\\n\\n\\n    Parameters\\n    ----------\\n    model : pylearn2.models.model.Model\\n        The model you loaded\\n    name : str\\n        Will save the old monitor to model.name\\n    transfer_experience : bool\\n        If True, the new monitor will start with its epochs seen,\\n        batches seen, and examples seen set to where the old monitor\\n        left off. This is nice for stitching together learning curves\\n        across multiple stages of learning.\\n    save_records : bool\\n        If True, val_record, batch_record, example_record, epoch_record,\\n        and time_record of the new monitor will be initialzed with the\\n        records of old monitor.\\n\\n    Returns\\n    -------\\n    model : WRITEME\\n        Returns the model itself so you can use an !obj:push_monitor\\n        call as the definition of a model in a YAML file.\\n    '\n    assert hasattr(model, 'monitor')\n    old_monitor = model.monitor\n    setattr(model, name, old_monitor)\n    del model.monitor\n    if transfer_experience:\n        monitor = Monitor.get_monitor(model)\n        assert (monitor is not old_monitor)\n        monitor._num_batches_seen = old_monitor._num_batches_seen\n        monitor._examples_seen = old_monitor._examples_seen\n        monitor._epochs_seen = old_monitor._epochs_seen\n        if save_records:\n            monitor.on_channel_conflict = 'copy_history'\n            monitor.channels = copy.copy(old_monitor.channels)\n            for (key, value) in list(monitor.channels.items()):\n                value.prereqs = None\n    return model\n", "label": 0}
{"function": "\n\ndef on_message(self, message):\n    if (not message):\n        return\n    logger.debug('Received WebSocket message from %s: %r', self.request.remote_ip, message)\n    try:\n        response = self.jsonrpc.handle_json(tornado.escape.native_str(message))\n        if (response and self.write_message(response)):\n            logger.debug('Sent WebSocket message to %s: %r', self.request.remote_ip, response)\n    except Exception as e:\n        error_msg = encoding.locale_decode(e)\n        logger.error('WebSocket request error: %s', error_msg)\n        if self.ws_connection:\n            self.close()\n", "label": 0}
{"function": "\n\ndef test_get_open_files(self):\n    p = psutil.Process(os.getpid())\n    files = p.get_open_files()\n    self.assertFalse((TESTFN in files))\n    f = open(TESTFN, 'w')\n    call_until(p.get_open_files, ('len(ret) != %i' % len(files)))\n    filenames = [x.path for x in p.get_open_files()]\n    self.assertIn(TESTFN, filenames)\n    f.close()\n    for file in filenames:\n        assert os.path.isfile(file), file\n    cmdline = (\"import time; f = open(r'%s', 'r'); time.sleep(100);\" % TESTFN)\n    sproc = get_test_subprocess([PYTHON, '-c', cmdline], wait=True)\n    p = psutil.Process(sproc.pid)\n    for x in range(100):\n        filenames = [x.path for x in p.get_open_files()]\n        if (TESTFN in filenames):\n            break\n        time.sleep(0.01)\n    else:\n        self.assertIn(TESTFN, filenames)\n    for file in filenames:\n        assert os.path.isfile(file), file\n", "label": 1}
{"function": "\n\ndef test_trailing_slash(self):\n    factory = APIRequestFactory()\n    urlpatterns = format_suffix_patterns([url('^test/$', dummy_view)])\n    resolver = urlresolvers.RegexURLResolver('^/', urlpatterns)\n    test_paths = [(URLTestPath('/test.api', (), {\n        'format': 'api',\n    }), True), (URLTestPath('/test/.api', (), {\n        'format': 'api',\n    }), False), (URLTestPath('/test.api/', (), {\n        'format': 'api',\n    }), True)]\n    for (test_path, expected_resolved) in test_paths:\n        request = factory.get(test_path.path)\n        try:\n            (callback, callback_args, callback_kwargs) = resolver.resolve(request.path_info)\n        except urlresolvers.Resolver404:\n            (callback, callback_args, callback_kwargs) = (None, None, None)\n        if (not expected_resolved):\n            assert (callback is None)\n            continue\n        assert (callback_args == test_path.args)\n        assert (callback_kwargs == test_path.kwargs)\n", "label": 0}
{"function": "\n\ndef checkStatements(value):\n    ' Check that statements list value property.\\n\\n    Must not be None, must not contain None, and of course only statements,\\n    may be empty.\\n    '\n    assert (value is not None)\n    assert (None not in value)\n    for statement in value:\n        assert (statement.isStatement() or statement.isStatementsFrame()), statement.asXmlText()\n    return tuple(value)\n", "label": 0}
{"function": "\n\ndef matches(self, new, old):\n    try:\n        if ((new is None) or (old is None)):\n            return (new is old)\n        else:\n            return (new == old)\n    except (KeyboardInterrupt, SystemExit):\n        raise\n    except Exception:\n        pass\n    return False\n", "label": 0}
{"function": "\n\ndef lmul_T(self, x):\n    '\\n        .. todo::\\n\\n            WRITEME properly\\n\\n        Override the original Conv2D lmul_T to make it work\\n        with pylearn format of topological data using dimshuffles\\n        '\n    assert (x.dtype == self._filters.dtype)\n    op_axes = ('b', 'c', 0, 1)\n    axes = self.output_axes\n    if (tuple(axes) != op_axes):\n        x = x.dimshuffle(axes.index('b'), axes.index('c'), axes.index(0), axes.index(1))\n    dummy_v = T.tensor4()\n    dummy_v.name = 'dummy_v'\n    if (theano.config.compute_test_value == 'raise'):\n        dummy_v.tag.test_value = np.zeros((x.tag.test_value.shape[0], self.input_space.num_channels, self.input_space.shape[0], self.input_space.shape[1]), dtype=dummy_v.dtype)\n    z_hs = conv2d(dummy_v, self._filters, image_shape=self._img_shape, filter_shape=self._filters_shape, subsample=self._subsample, border_mode=self._border_mode)\n    (rval, xdummy) = z_hs.owner.op.grad((dummy_v, self._filters), (x,))\n    axes = self.input_space.axes\n    assert (len(axes) == 4)\n    if (tuple(axes) != op_axes):\n        rval = rval.dimshuffle(op_axes.index(axes[0]), op_axes.index(axes[1]), op_axes.index(axes[2]), op_axes.index(axes[3]))\n    return rval\n", "label": 0}
{"function": "\n\ndef expand_macro(self, formatter, name, content, args=None):\n    args = (args or {\n        \n    })\n    reponame = (args.get('repository') or '')\n    rev = args.get('revision')\n    repos = RepositoryManager(self.env).get_repository(reponame)\n    try:\n        changeset = repos.get_changeset(rev)\n        message = changeset.message\n        rev = changeset.rev\n        resource = repos.resource\n    except Exception:\n        message = content\n        resource = Resource('repository', reponame)\n    if (formatter.context.resource.realm == 'ticket'):\n        ticket_re = CommitTicketUpdater.ticket_re\n        if (not any(((int(tkt_id) == int(formatter.context.resource.id)) for tkt_id in ticket_re.findall(message)))):\n            return tag.p(_(\"(The changeset message doesn't reference this ticket)\"), class_='hint')\n    if ChangesetModule(self.env).wiki_format_messages:\n        return tag.div(format_to_html(self.env, formatter.context.child('changeset', rev, parent=resource), message, escape_newlines=True), class_='message')\n    else:\n        return tag.pre(message, class_='message')\n", "label": 1}
{"function": "\n\ndef compute_fixed_point(T, v, error_tol=0.001, max_iter=50, verbose=1, print_skip=5, *args, **kwargs):\n    '\\n    Computes and returns :math:`T^k v`, an approximate fixed point.\\n\\n    Here T is an operator, v is an initial condition and k is the number\\n    of iterates. Provided that T is a contraction mapping or similar,\\n    :math:`T^k v` will be an approximation to the fixed point.\\n\\n    Parameters\\n    ----------\\n    T : callable\\n        A callable object (e.g., function) that acts on v\\n    v : object\\n        An object such that T(v) is defined\\n    error_tol : scalar(float), optional(default=1e-3)\\n        Error tolerance\\n    max_iter : scalar(int), optional(default=50)\\n        Maximum number of iterations\\n    verbose : bool, optional(default=True)\\n        If True then print current error at each iterate.\\n    args, kwargs :\\n        Other arguments and keyword arguments that are passed directly\\n        to  the function T each time it is called\\n\\n    Returns\\n    -------\\n    v : object\\n        The approximate fixed point\\n\\n    '\n    iterate = 0\n    error = (error_tol + 1)\n    if verbose:\n        start_time = time.time()\n        _print_after_skip(print_skip, it=None)\n    while ((iterate < max_iter) and (error > error_tol)):\n        new_v = T(v, *args, **kwargs)\n        iterate += 1\n        error = np.max(np.abs((new_v - v)))\n        if verbose:\n            etime = (time.time() - start_time)\n            _print_after_skip(print_skip, iterate, error, etime)\n        try:\n            v[:] = new_v\n        except TypeError:\n            v = new_v\n    return v\n", "label": 0}
{"function": "\n\ndef run(self, command_line=''):\n    assert command_line, 'expected non-empty command_line'\n    parsed = parse_command_line(command_line)\n    global_range = None\n    if parsed.line_range.is_empty:\n        global_range = R(0, self._view.size())\n    else:\n        global_range = parsed.line_range.resolve(self._view)\n    pattern = parsed.command.pattern\n    if pattern:\n        ExGlobal.most_recent_pat = pattern\n    else:\n        pattern = ExGlobal.most_recent_pat\n    subcmd = parsed.command.subcommand\n    try:\n        matches = find_all_in_range(self._view, pattern, global_range.begin(), global_range.end())\n    except Exception as e:\n        msg = (\"Vintageous (global): %s ... in pattern '%s'\" % (str(e), pattern))\n        sublime.status_message(msg)\n        print(msg)\n        return\n    if ((not matches) or (not parsed.command.subcommand.cooperates_with_global)):\n        return\n    matches = [self._view.full_line(r.begin()) for r in matches]\n    matches = [[r.a, r.b] for r in matches]\n    self.window.run_command(subcmd.target_command, {\n        'command_line': str(subcmd),\n        'global_lines': matches,\n    })\n", "label": 1}
{"function": "\n\n@pytest.mark.skipif(os.path.islink(tempfile.gettempdir()), reason='Test fails when /tmp is a symlink. See #231')\ndef test_two_levels_deep(self):\n    '\\n        Test nested namespace packages\\n        Create namespace packages in the following tree :\\n            site-packages-1/pkg1/pkg2\\n            site-packages-2/pkg1/pkg2\\n        Check both are in the _namespace_packages dict and that their __path__\\n        is correct\\n        '\n    sys.path.append(os.path.join(self._tmpdir, 'site-pkgs2'))\n    os.makedirs(os.path.join(self._tmpdir, 'site-pkgs', 'pkg1', 'pkg2'))\n    os.makedirs(os.path.join(self._tmpdir, 'site-pkgs2', 'pkg1', 'pkg2'))\n    ns_str = \"__import__('pkg_resources').declare_namespace(__name__)\\n\"\n    for site in ['site-pkgs', 'site-pkgs2']:\n        pkg1_init = open(os.path.join(self._tmpdir, site, 'pkg1', '__init__.py'), 'w')\n        pkg1_init.write(ns_str)\n        pkg1_init.close()\n        pkg2_init = open(os.path.join(self._tmpdir, site, 'pkg1', 'pkg2', '__init__.py'), 'w')\n        pkg2_init.write(ns_str)\n        pkg2_init.close()\n    import pkg1\n    assert ('pkg1' in pkg_resources._namespace_packages)\n    import pkg1.pkg2\n    assert ('pkg1.pkg2' in pkg_resources._namespace_packages)\n    assert (pkg_resources._namespace_packages['pkg1'] == ['pkg1.pkg2'])\n    expected = [os.path.join(self._tmpdir, 'site-pkgs', 'pkg1', 'pkg2'), os.path.join(self._tmpdir, 'site-pkgs2', 'pkg1', 'pkg2')]\n    assert (pkg1.pkg2.__path__ == expected)\n", "label": 0}
{"function": "\n\ndef _request(self, url, method='GET', params=None):\n    'Internal stream request handling'\n    self.connected = True\n    retry_counter = 0\n    method = method.lower()\n    func = getattr(self.client, method)\n\n    def _send(retry_counter):\n        while self.connected:\n            try:\n                if (method == 'get'):\n                    response = func(url, params=params, timeout=self.timeout)\n                else:\n                    response = func(url, data=params, timeout=self.timeout)\n            except requests.exceptions.Timeout:\n                self.on_timeout()\n            else:\n                if (response.status_code != 200):\n                    self.on_error(response.status_code, response.content)\n                if (self.retry_count and ((self.retry_count - retry_counter) > 0)):\n                    time.sleep(self.retry_in)\n                    retry_counter += 1\n                    _send(retry_counter)\n                return response\n    while self.connected:\n        response = _send(retry_counter)\n        for line in response.iter_lines():\n            if (not self.connected):\n                break\n            if line:\n                try:\n                    if (not is_py3):\n                        self.on_success(json.loads(line))\n                    else:\n                        line = line.decode('utf-8')\n                        self.on_success(json.loads(line))\n                except ValueError:\n                    self.on_error(response.status_code, 'Unable to decode response, not vaild JSON.')\n    response.close()\n", "label": 0}
{"function": "\n\ndef connectionLost(self, reason):\n    \"\\n        FIXME: this method is called 4 times on logout....\\n        it's called once from Avatar.closed() if disconnected\\n        \"\n    for i in self.interactors:\n        i.sessionClosed()\n    if self.stdinlog_open:\n        try:\n            with open(self.stdinlogFile, 'rb') as f:\n                shasum = hashlib.sha256(f.read()).hexdigest()\n                shasumfile = ((self.downloadPath + '/') + shasum)\n                if os.path.exists(shasumfile):\n                    os.remove(self.stdinlogFile)\n                else:\n                    os.rename(self.stdinlogFile, shasumfile)\n                os.symlink(shasum, self.stdinlogFile)\n            log.msg(eventid='cowrie.session.file_download', format='Saved stdin contents to %(outfile)s', url='stdin', outfile=shasumfile, shasum=shasum)\n        except IOError as e:\n            pass\n        finally:\n            self.stdinlog_open = False\n    if self.ttylog_open:\n        log.msg(eventid='cowrie.log.closed', format='Closing TTY Log: %(ttylog)s after %(duration)d seconds', ttylog=self.ttylogFile, size=self.ttylogSize, duration=(time.time() - self.startTime))\n        ttylog.ttylog_close(self.ttylogFile, time.time())\n        self.ttylog_open = False\n    insults.ServerProtocol.connectionLost(self, reason)\n", "label": 0}
{"function": "\n\ndef leastsq(func, x0, args=(), Dfun=None, ftol=1e-07, xtol=1e-07, gtol=1e-07, maxfev=0, epsfcn=None, factor=100, diag=None):\n    '\\n    Minimize the sum of squares of a set of equations.\\n    Adopted from scipy.optimize.leastsq\\n\\n    ::\\n\\n        x = arg min(sum(func(y)**2,axis=0))\\n                 y\\n\\n    Parameters\\n    ----------\\n    func : callable\\n        should take at least one (possibly length N vector) argument and\\n        returns M floating point numbers.\\n    x0 : ndarray\\n        The starting estimate for the minimization.\\n    args : tuple\\n        Any extra arguments to func are placed in this tuple.\\n    Dfun : callable\\n        A function or method to compute the Jacobian of func with derivatives\\n        across the rows. If this is None, the Jacobian will be estimated.\\n    ftol : float\\n        Relative error desired in the sum of squares.\\n    xtol : float\\n        Relative error desired in the approximate solution.\\n    gtol : float\\n        Orthogonality desired between the function vector and the columns of\\n        the Jacobian.\\n    maxfev : int\\n        The maximum number of calls to the function. If zero, then 100*(N+1) is\\n        the maximum where N is the number of elements in x0.\\n    epsfcn : float\\n        A suitable step length for the forward-difference approximation of the\\n        Jacobian (for Dfun=None). If epsfcn is less than the machine precision,\\n        it is assumed that the relative errors in the functions are of the\\n        order of the machine precision.\\n    factor : float\\n        A parameter determining the initial step bound\\n        (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\\n    diag : sequence\\n        N positive entries that serve as a scale factors for the variables.\\n\\n    Returns\\n    -------\\n    x : ndarray\\n        The solution (or the result of the last iteration for an unsuccessful\\n        call).\\n    cov_x : ndarray\\n        Uses the fjac and ipvt optional outputs to construct an\\n        estimate of the jacobian around the solution.  ``None`` if a\\n        singular matrix encountered (indicates very flat curvature in\\n        some direction).  This matrix must be multiplied by the\\n        residual variance to get the covariance of the\\n        parameter estimates -- see curve_fit.\\n    infodict : dict\\n        a dictionary of optional outputs with the key s::\\n\\n            - \\'nfev\\' : the number of function calls\\n            - \\'fvec\\' : the function evaluated at the output\\n            - \\'fjac\\' : A permutation of the R matrix of a QR\\n                     factorization of the final approximate\\n                     Jacobian matrix, stored column wise.\\n                     Together with ipvt, the covariance of the\\n                     estimate can be approximated.\\n            - \\'ipvt\\' : an integer array of length N which defines\\n                     a permutation matrix, p, such that\\n                     fjac*p = q*r, where r is upper triangular\\n                     with diagonal elements of nonincreasing\\n                     magnitude. Column j of p is column ipvt(j)\\n                     of the identity matrix.\\n            - \\'qtf\\'  : the vector (transpose(q) * fvec).\\n\\n    mesg : str\\n        A string message giving information about the cause of failure.\\n    ier : int\\n        An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\\n        found.  Otherwise, the solution was not found. In either case, the\\n        optional output variable \\'mesg\\' gives more information.\\n\\n    Notes\\n    -----\\n    \"leastsq\" is a wrapper around MINPACK\\'s lmdif and lmder algorithms.\\n\\n    cov_x is a Jacobian approximation to the Hessian of the least squares\\n    objective function.\\n    This approximation assumes that the objective function is based on the\\n    difference between some observed target data (ydata) and a (non-linear)\\n    function of the parameters `f(xdata, params)` ::\\n\\n           func(params) = ydata - f(xdata, params)\\n\\n    so that the objective function is ::\\n\\n           min   sum((ydata - f(xdata, params))**2, axis=0)\\n         params\\n\\n    '\n    x0 = asarray(x0).flatten()\n    n = len(x0)\n    if (not isinstance(args, tuple)):\n        args = (args,)\n    shape = _check_func('leastsq', 'func', func, x0, args, n)\n    if (isinstance(shape, tuple) and (len(shape) > 1)):\n        shape = shape[0]\n    m = shape[0]\n    if (n > m):\n        raise TypeError(('Improper input: N=%s must not exceed M=%s' % (n, m)))\n    if (maxfev == 0):\n        maxfev = (200 * (n + 1))\n    if (epsfcn is None):\n        epsfcn = 2e-05\n    if (Dfun is None):\n        retval = _minpack._lmdif(func, x0, args, 1, ftol, xtol, gtol, maxfev, epsfcn, factor, diag)\n    else:\n        _check_func('leastsq', 'Dfun', Dfun, x0, args, n, (m, n))\n        retval = _minpack._lmder(func, Dfun, x0, args, 1, 0, ftol, xtol, gtol, maxfev, factor, diag)\n    errors = {\n        0: ['Improper input parameters.', TypeError],\n        1: [('Both actual and predicted relative reductions in the sum of squares\\n  are at most %f' % ftol), None],\n        2: [('The relative error between two consecutive iterates is at most %f' % xtol), None],\n        3: [('Both actual and predicted relative reductions in the sum of squares\\n  are at most %f and the relative error between two consecutive iterates is at \\n  most %f' % (ftol, xtol)), None],\n        4: [('The cosine of the angle between func(x) and any column of the\\n  Jacobian is at most %f in absolute value' % gtol), None],\n        5: [('Number of calls to function has reached maxfev = %d.' % maxfev), ValueError],\n        6: [('ftol=%f is too small, no further reduction in the sum of squares\\n  is possible.' % ftol), ValueError],\n        7: [('xtol=%f is too small, no further improvement in the approximate\\n  solution is possible.' % xtol), ValueError],\n        8: [('gtol=%f is too small, func(x) is orthogonal to the columns of\\n  the Jacobian to machine precision.' % gtol), ValueError],\n        'unknown': ['Unknown error.', TypeError],\n    }\n    info = retval[(- 1)]\n    mesg = errors[info][0]\n    cov_x = None\n    if (info in [1, 2, 3, 4]):\n        perm = take(eye(n), (retval[1]['ipvt'] - 1), 0)\n        r = triu(transpose(retval[1]['fjac'])[:n, :])\n        R = dot(r, perm)\n        try:\n            cov_x = inv(dot(transpose(R), R))\n        except (LinAlgError, ValueError):\n            pass\n    return (((retval[0], cov_x) + retval[1:(- 1)]) + (mesg, info))\n", "label": 1}
{"function": "\n\ndef makeStepMessages(self, stepFrames, leadingFoot, snapToTerrain=False):\n    assert (leadingFoot in ('left', 'right'))\n    isRightFootOffset = (0 if (leadingFoot == 'left') else 1)\n    (leftPoints, rightPoints) = FootstepsDriver.getContactPts()\n    footOriginToSole = (- np.mean(leftPoints, axis=0))\n    stepMessages = []\n    for (i, stepFrame) in enumerate(stepFrames):\n        t = transformUtils.copyFrame(stepFrame)\n        t.PreMultiply()\n        t.Translate(footOriginToSole)\n        step = lcmdrc.footstep_t()\n        step.pos = positionMessageFromFrame(t)\n        step.is_right_foot = ((i + isRightFootOffset) % 2)\n        step.params = self.footstepsDriver.getDefaultStepParams()\n        step.fixed_x = True\n        step.fixed_y = True\n        step.fixed_z = True\n        step.fixed_roll = True\n        step.fixed_pitch = True\n        step.fixed_yaw = True\n        if snapToTerrain:\n            step.fixed_z = False\n            step.fixed_roll = False\n            step.fixed_pitch = False\n        stepMessages.append(step)\n    return stepMessages\n", "label": 0}
{"function": "\n\ndef run_forever(self, sockopt=()):\n    '\\n        run event loop for WebSocket framework.\\n        This loop is infinite loop and is alive during websocket is available.\\n        sockopt: values for socket.setsockopt.\\n            sockopt must be tuple and each element is argument of sock.setscokopt.\\n        '\n    if self.sock:\n        raise WebSocketException('socket is already opened')\n    try:\n        self.sock = WebSocket(self.get_mask_key, sockopt=sockopt)\n        self.sock.connect(self.url, header=self.header)\n        self._run_with_no_err(self.on_open)\n        while self.keep_running:\n            data = self.sock.recv()\n            if (data is None):\n                break\n            self._run_with_no_err(self.on_message, data)\n    except Exception as e:\n        self._run_with_no_err(self.on_error, e)\n    finally:\n        self.sock.close()\n        self._run_with_no_err(self.on_close)\n        self.sock = None\n", "label": 0}
{"function": "\n\ndef parse_shapename(shapename):\n    try:\n        shape = shapename.split('_kdl_')[1].split('[')[0].split('x')\n    except AttributeError:\n        raise AttributeError(('Unable to parse shapename. Has the expression been tagged with a shape by tag_expression?  input shapename was %s' % shapename))\n    if ('[' in shapename.split('_kdl_')[1]):\n        shape = shape[1:]\n    name = shapename.split('_kdl_')[0]\n    shape = tuple([int(s) for s in shape if (s != '')])\n    return (name, shape)\n", "label": 0}
{"function": "\n\ndef get(self, block=True, timeout=None):\n    if (not block):\n        return self.get_nowait()\n    start_time = time.time()\n    while True:\n        try:\n            return self.get_nowait()\n        except self.Empty:\n            if timeout:\n                lasted = (time.time() - start_time)\n                if (timeout > lasted):\n                    time.sleep(min(self.max_timeout, (timeout - lasted)))\n                else:\n                    raise\n            else:\n                time.sleep(self.max_timeout)\n", "label": 0}
{"function": "\n\ndef remove_sensor(self, name):\n    '\\n        Remove a sensor (if it exists), associated metrics and its children.\\n\\n        Arguments:\\n            name (str): The name of the sensor to be removed\\n        '\n    sensor = self._sensors.get(name)\n    if sensor:\n        child_sensors = None\n        with sensor._lock:\n            with self._lock:\n                val = self._sensors.pop(name, None)\n                if (val and (val == sensor)):\n                    for metric in sensor.metrics:\n                        self.remove_metric(metric.metric_name)\n                    logger.debug('Removed sensor with name %s', name)\n                    child_sensors = self._children_sensors.pop(sensor, None)\n        if child_sensors:\n            for child_sensor in child_sensors:\n                self.remove_sensor(child_sensor.name)\n", "label": 0}
{"function": "\n\ndef testFromNetworkXGraph(self):\n    try:\n        import networkx\n    except ImportError as error:\n        logging.debug(error)\n        return\n    nxGraph = networkx.Graph()\n    nxGraph.graph['VListType'] = GeneralVertexList\n    nxGraph.add_edge(0, 1)\n    nxGraph.add_edge(1, 2)\n    nxGraph.add_edge(1, 3)\n    graph = self.GraphType.fromNetworkXGraph(nxGraph)\n    self.assertTrue((graph.getNumVertices() == 4))\n    self.assertTrue((graph.isUndirected() == True))\n    self.assertTrue((graph.getNumEdges() == 3))\n    self.assertTrue((graph.getEdge(0, 1) == 1))\n    self.assertTrue((graph.getEdge(1, 2) == 1))\n    self.assertTrue((graph.getEdge(1, 3) == 1))\n    nxGraph = networkx.DiGraph()\n    nxGraph.graph['VListType'] = GeneralVertexList\n    nxGraph.add_edge(0, 1)\n    nxGraph.add_edge(1, 2)\n    nxGraph.add_edge(1, 3)\n    graph = self.GraphType.fromNetworkXGraph(nxGraph)\n    self.assertTrue((graph.getNumVertices() == 4))\n    self.assertTrue((graph.isUndirected() == False))\n    self.assertTrue((graph.getNumEdges() == 3))\n    self.assertTrue((graph.getEdge(0, 1) == 1))\n    self.assertTrue((graph.getEdge(1, 2) == 1))\n    self.assertTrue((graph.getEdge(1, 3) == 1))\n    nxGraph = networkx.MultiGraph()\n    self.assertRaises(ValueError, self.GraphType.fromNetworkXGraph, nxGraph)\n    nxGraph = networkx.DiGraph()\n    nxGraph.graph['VListType'] = GeneralVertexList\n    nxGraph.add_node('a', label='abc')\n    nxGraph.add_node('b', label='i')\n    nxGraph.add_node('c', label='am')\n    nxGraph.add_node('d', label='here')\n    nxGraph.add_edge('a', 'b')\n    nxGraph.add_edge('b', 'c')\n    nxGraph.add_edge('b', 'd')\n    graph = self.GraphType.fromNetworkXGraph(nxGraph)\n    nodeDict = {\n        \n    }\n    for i in range(len(nxGraph.nodes())):\n        nodeDict[nxGraph.nodes()[i]] = i\n    self.assertTrue((graph.getNumVertices() == 4))\n    self.assertTrue((graph.isUndirected() == False))\n    self.assertTrue((graph.getNumEdges() == 3))\n    self.assertTrue((graph.getEdge(nodeDict['a'], nodeDict['b']) == 1))\n    self.assertTrue((graph.getEdge(nodeDict['b'], nodeDict['c']) == 1))\n    self.assertTrue((graph.getEdge(nodeDict['b'], nodeDict['d']) == 1))\n    self.assertTrue((graph.getVertex(0) == 'abc'))\n    self.assertTrue((graph.getVertex(1) == 'am'))\n    self.assertTrue((graph.getVertex(2) == 'i'))\n    self.assertTrue((graph.getVertex(3) == 'here'))\n    numVertices = 10\n    numFeatures = 2\n    vList = VertexList(numVertices, numFeatures)\n    vList.setVertices(numpy.random.rand(numVertices, numFeatures))\n    graph = self.GraphType(vList)\n    graph.addEdge(0, 1)\n    graph.addEdge(0, 5)\n    graph.addEdge(2, 5)\n    graph.addEdge(3, 4)\n    nxGraph = graph.toNetworkXGraph()\n    graph2 = self.GraphType.fromNetworkXGraph(nxGraph)\n    tol = (10 ** (- 6))\n    self.assertTrue((numpy.linalg.norm((graph.getVertexList().getVertices(list(range(numVertices))) - graph2.getVertexList().getVertices(list(range(numVertices))))) < tol))\n    self.assertEquals(graph.getNumEdges(), graph2.getNumEdges())\n    for i in range(numVertices):\n        for j in range(numVertices):\n            self.assertEquals(graph.getEdge(i, j), graph2.getEdge(i, j))\n    numVertices = 10\n    vList = GeneralVertexList(numVertices)\n    for i in range(numVertices):\n        vList.setVertex(i, ('s' + str(i)))\n    graph = self.GraphType(vList)\n    graph.addEdge(0, 1)\n    graph.addEdge(0, 5)\n    graph.addEdge(2, 5)\n    graph.addEdge(3, 4)\n    nxGraph = graph.toNetworkXGraph()\n    graph2 = self.GraphType.fromNetworkXGraph(nxGraph)\n    for i in range(numVertices):\n        self.assertEquals(graph.getVertex(i), graph2.getVertex(i))\n    self.assertEquals(graph.getNumEdges(), graph2.getNumEdges())\n    for i in range(numVertices):\n        for j in range(numVertices):\n            self.assertEquals(graph.getEdge(i, j), graph2.getEdge(i, j))\n", "label": 1}
{"function": "\n\n@webapi_check_local_site\n@webapi_check_login_required\n@webapi_response_errors(DOES_NOT_EXIST, REPO_NOT_IMPLEMENTED, REPO_INFO_ERROR)\ndef get(self, request, *args, **kwargs):\n    'Returns repository-specific information from a server.'\n    try:\n        repository = resources.repository.get_object(request, *args, **kwargs)\n    except ObjectDoesNotExist:\n        return DOES_NOT_EXIST\n    try:\n        tool = repository.get_scmtool()\n        return (200, {\n            self.item_result_key: tool.get_repository_info(),\n        })\n    except NotImplementedError:\n        return REPO_NOT_IMPLEMENTED\n    except AuthenticationError as e:\n        return REPO_INFO_ERROR.with_message(six.text_type(e))\n    except:\n        return REPO_INFO_ERROR\n", "label": 0}
{"function": "\n\n@Xephyr(True, ShConfig())\ndef test_complete(self):\n    self.sh = libqtile.sh.QSh(self.c)\n    assert (self.sh._complete('c', 'c') == ['cd', 'commands', 'critical'])\n    assert (self.sh._complete('cd l', 'l') == ['layout'])\n    print(self.sh._complete('cd layout/', 'layout/'))\n    assert (self.sh._complete('cd layout/', 'layout/') == [('layout/' + x) for x in ['group', 'window', 'screen', '0']])\n    assert (self.sh._complete('cd layout/', 'layout/g') == ['layout/group'])\n", "label": 0}
{"function": "\n\ndef try_import_module(module_name):\n    \"\\n    Imports a module, but catches import errors.  Only catches errors\\n    when that module doesn't exist; if that module itself has an\\n    import error it will still get raised.  Returns None if the module\\n    doesn't exist.\\n    \"\n    try:\n        return import_module(module_name)\n    except ImportError as e:\n        if (not getattr(e, 'args', None)):\n            raise\n        desc = e.args[0]\n        if (not desc.startswith('No module named ')):\n            raise\n        desc = desc[len('No module named '):]\n        parts = module_name.split('.')\n        for i in range(len(parts)):\n            if (desc == '.'.join(parts[i:])):\n                return None\n        raise\n", "label": 0}
{"function": "\n\ndef _collect(self):\n    LOG.debug(('collecting arguments/commands for %s' % self))\n    arguments = []\n    commands = []\n    arguments = list(self._meta.arguments)\n    for member in dir(self.__class__):\n        if member.startswith('_'):\n            continue\n        try:\n            func = getattr(self.__class__, member).__cement_meta__\n        except AttributeError:\n            continue\n        else:\n            func['controller'] = self\n            commands.append(func)\n    for contr in handler.list('controller'):\n        if (contr == self.__class__):\n            continue\n        contr = contr()\n        contr._setup(self.app)\n        if (contr._meta.stacked_on == self._meta.label):\n            if (contr._meta.stacked_type == 'embedded'):\n                (contr_arguments, contr_commands) = contr._collect()\n                for arg in contr_arguments:\n                    arguments.append(arg)\n                for func in contr_commands:\n                    commands.append(func)\n            elif (contr._meta.stacked_type == 'nested'):\n                metadict = {\n                    \n                }\n                metadict['label'] = re.sub('_', '-', contr._meta.label)\n                metadict['func_name'] = '_dispatch'\n                metadict['exposed'] = True\n                metadict['hide'] = contr._meta.hide\n                metadict['help'] = contr._meta.description\n                metadict['aliases'] = contr._meta.aliases\n                metadict['aliases_only'] = contr._meta.aliases_only\n                metadict['controller'] = contr\n                commands.append(metadict)\n    return (arguments, commands)\n", "label": 1}
{"function": "\n\ndef test_call_some_more():\n    from commonast import Name, Num, Starred, Keyword\n    code = 'foo(1, a, *b, c=3, **d)'\n    node = commonast.parse(code).body_nodes[0].value_node\n    assert isinstance(node, commonast.Call)\n    assert (len(node.arg_nodes) == 3)\n    assert (len(node.kwarg_nodes) == 2)\n    for (arg, cls) in zip((node.arg_nodes + node.kwarg_nodes), [Num, Name, Starred, Num, None.__class__]):\n        isinstance(arg, cls)\n    assert (node.arg_nodes[2].value_node.name == 'b')\n    assert (node.kwarg_nodes[1].name is None)\n    assert (node.kwarg_nodes[1].value_node.name == 'd')\n", "label": 1}
{"function": "\n\ndef test_timeout_value(self):\n    'Test that timeout value is ``None`` or an integer.'\n    ok_values = ('0', '1', '9999', '\"0\"', '\"1\"', '\"9999\"', 'None')\n    ko_values = ('-1', '-9999', '\"-1\"', '\"-9999\"', '\"foo\"', '\"\"', '12.3', '\"12.3\"')\n    t = '\\n            {%% load adv_cache %%}\\n            {%% cache %s test_cached_template obj.pk obj.updated_at %%}\\n                {{ obj.get_name }}\\n            {%% endcache %%}\\n        '\n    for value in ok_values:\n\n        def test_value(value):\n            self.render((t % value))\n        if hasattr(self, 'subTest'):\n            with self.subTest(value=value):\n                test_value(value)\n        else:\n            test_value(value)\n    for value in ko_values:\n\n        def test_value(value):\n            with self.assertRaises(template.TemplateSyntaxError) as raise_context:\n                self.render((t % value))\n            self.assertIn('tag got a non-integer (or None) timeout value', str(raise_context.exception))\n        if hasattr(self, 'subTest'):\n            with self.subTest(value=value):\n                test_value(value)\n        else:\n            test_value(value)\n", "label": 0}
{"function": "\n\ndef test_multi_process(self):\n    skip_if_twisted()\n    with ExpectLog(gen_log, '(Starting .* processes|child .* exited|uncaught exception)'):\n        self.assertFalse(IOLoop.initialized())\n        (sock, port) = bind_unused_port()\n\n        def get_url(path):\n            return ('http://127.0.0.1:%d%s' % (port, path))\n        signal.alarm(5)\n        try:\n            id = fork_processes(3, max_restarts=3)\n            self.assertTrue((id is not None))\n            signal.alarm(5)\n        except SystemExit as e:\n            self.assertEqual(e.code, 0)\n            self.assertTrue((task_id() is None))\n            sock.close()\n            return\n        try:\n            if (id in (0, 1)):\n                self.assertEqual(id, task_id())\n                server = HTTPServer(self.get_app())\n                server.add_sockets([sock])\n                IOLoop.instance().start()\n            elif (id == 2):\n                self.assertEqual(id, task_id())\n                sock.close()\n                client = HTTPClient(SimpleAsyncHTTPClient)\n\n                def fetch(url, fail_ok=False):\n                    try:\n                        return client.fetch(get_url(url))\n                    except HTTPError as e:\n                        if (not (fail_ok and (e.code == 599))):\n                            raise\n                fetch('/?exit=2', fail_ok=True)\n                fetch('/?exit=3', fail_ok=True)\n                int(fetch('/').body)\n                fetch('/?exit=0', fail_ok=True)\n                pid = int(fetch('/').body)\n                fetch('/?exit=4', fail_ok=True)\n                pid2 = int(fetch('/').body)\n                self.assertNotEqual(pid, pid2)\n                fetch('/?exit=0', fail_ok=True)\n                os._exit(0)\n        except Exception:\n            logging.error('exception in child process %d', id, exc_info=True)\n            raise\n", "label": 0}
{"function": "\n\ndef _write(self, sock, data):\n    if (sock not in self._clients):\n        return\n    try:\n        nbytes = sock.send(data)\n        if (nbytes < len(data)):\n            self._buffers[sock].appendleft(data[nbytes:])\n    except SocketError as e:\n        if (e.args[0] not in (EINTR, EWOULDBLOCK, ENOBUFS)):\n            self.fire(error(sock, e))\n            self._close(sock)\n        else:\n            self._buffers[sock].appendleft(data)\n", "label": 0}
{"function": "\n\ndef _handle(self, data):\n    self._buf_in += data\n    if self._handling:\n        return\n    self._handling = True\n    while True:\n        (before, sep, after) = self._buf_in.partition(b'\\n')\n        if (not sep):\n            break\n        try:\n            before = before.decode('utf-8', 'ignore')\n            data = json.loads(before)\n        except Exception as e:\n            msg.error('Unable to parse json: ', str_e(e))\n            msg.error('Data: ', before)\n            self._buf_in = after\n            continue\n        name = data.get('name')\n        self._buf_in = after\n        try:\n            msg.debug(('got data ' + (name or 'no name')))\n            self.emit('data', name, data)\n        except Exception as e:\n            api.send_error(('Error handling %s event.' % name), str_e(e))\n            if (name == 'room_info'):\n                editor.error_message(('Error joining workspace: %s' % str_e(e)))\n                self.stop()\n    self._handling = False\n", "label": 1}
{"function": "\n\ndef test_registering_commands(app):\n    command = 'sing'\n    token = 'mytoken'\n    team_id = 'MYTEAMID'\n    methods = ['POST']\n    text = 'little apple'\n    get_url = '/?token={0}&team_id={1}&command={2}&text={3}'.format(token, team_id, command, text)\n\n    @app.slack.command(command, token, team_id, methods)\n    def _sing_a_song(**kwargs):\n        lyrics = 'You are my littttle apple...'\n        return app.slack.response(lyrics)\n    expected_commands = {\n        (team_id, command): (_sing_a_song, token, methods, {\n            \n        }),\n    }\n    assert (app.slack._commands == expected_commands)\n    get_res = app.client.get(get_url)\n    assert (get_res.status_code == 200)\n    assert (_jsonify_response(get_res)['text'] == 'GET request is not allowed')\n    post_data = {\n        'command': command,\n        'token': token,\n        'team_id': team_id,\n        'text': text,\n    }\n    post_res = app.client.post('/', data=post_data)\n    assert (post_res.status_code == 200)\n    assert (_jsonify_response(post_res)['text'] == 'You are my littttle apple...')\n", "label": 0}
{"function": "\n\ndef test_docopt_parser_with_tabs():\n    help_string = 'Some Tool\\n\\nUsage: tools [-t] [-i <input>...] <cmd>\\n\\nInputs:\\n\\t-i, --input <input>...\\tThe input\\n\\nOptions:\\n\\t-t\\tSome boolean\\n\\t<cmd>\\tThe command\\n    '\n    opts = Options.from_docopt(help_string)\n    assert (len(opts) == 3)\n    assert (opts['input'] is not None)\n    assert (opts['input'].nargs == '*')\n    assert (not opts['input'].required)\n    assert (opts['t'] is not None)\n    assert (opts['t'].nargs == 0)\n    assert (not opts['t'].required)\n    assert (opts['cmd'] is not None)\n    assert (opts['cmd'].nargs == 1)\n    assert opts['cmd'].required\n", "label": 1}
{"function": "\n\ndef supported(cls, stream=sys.stdout):\n    '\\n        A class method that returns True if the current platform supports\\n        coloring terminal output using this method. Returns False otherwise.\\n        '\n    if (not stream.isatty()):\n        return False\n    try:\n        import curses\n    except ImportError:\n        return False\n    else:\n        try:\n            try:\n                return (curses.tigetnum('colors') > 2)\n            except curses.error:\n                curses.setupterm()\n                return (curses.tigetnum('colors') > 2)\n        except:\n            raise\n            return False\n", "label": 0}
{"function": "\n\ndef __init__(self, RGB1, RGB2, numColors=33.0, divide=255.0, method='moreland', filename=''):\n    self.numColors = numColors\n    assert (np.mod(numColors, 2) == 1), 'For diverging colormaps odd numbers of colors are desireable!'\n    knownMethods = ['moreland', 'lab']\n    assert (method in knownMethods), 'Unknown method was specified!'\n    if (method == knownMethods[0]):\n        self.colorMap = self.generateColorMap(RGB1, RGB2, divide)\n    elif (method == knownMethods[1]):\n        self.colorMap = self.generateColorMapLab(RGB1, RGB2, divide)\n    if (filename == ''):\n        for c in self.colorMap:\n            pass\n    else:\n        with open(filename, 'w') as f:\n            for c in self.colorMap:\n                f.write('{0}, {1}, {2}\\n'.format(c[0], c[1], c[2]))\n", "label": 1}
{"function": "\n\n@app.route('/notes/<hostname>', methods=['GET', 'POST'])\n@login_required\ndef list_notes(hostname):\n    \"Retrieve a list of notes associated with a host. Or given\\n      {'user': 'username', 'note': 'some message'} post a note.\"\n    if (request.method == 'GET'):\n        try:\n            limit = request.args.get('limit', 50, type=int)\n        except ValueError:\n            abort(400)\n        notes = list(r.table('notes').filter({\n            'hostname': hostname,\n        }).order_by(r.desc('ts')).limit(limit).run(rdb.conn))\n        if notes:\n            return jsonify({\n                'notes': sorted(notes, key=(lambda k: k['ts'])),\n            })\n        else:\n            abort(404)\n    elif (request.method == 'POST'):\n        if (not request.json):\n            abort(400)\n        if ((not request.json.get('user')) or (not request.json.get('note'))):\n            abort(400)\n        if (not r.table('hosts').get_all(hostname, index='hostname').run(rdb.conn)):\n            abort(404)\n        alerting = [x['check'] for x in r.table('checks').filter({\n            'h stname': hostname,\n            'status': False,\n        }).run(rdb.conn)]\n        q = r.table('notes').insert({\n            'hostname': hostname,\n            'user': request.json.get('user'),\n            'note': request.json.get('note'),\n            'ts': time(),\n            'alerting': alerting,\n        }).run(rdb.conn)\n        if (q['inserted'] == 1):\n            return jsonify({\n                'success': True,\n            })\n        else:\n            logger.error(q)\n            abort(500)\n    else:\n        abort(400)\n", "label": 1}
{"function": "\n\ndef test_values_list_form_has_metadata(self):\n    'Test default results form has metadata.'\n    searcher = list(self.get_s().query(foo='bar').values_list('id'))\n    assert hasattr(searcher[0], '_id')\n    assert hasattr(searcher[0].es_meta, 'id')\n    assert hasattr(searcher[0].es_meta, 'score')\n    assert hasattr(searcher[0].es_meta, 'source')\n    assert hasattr(searcher[0].es_meta, 'type')\n    assert hasattr(searcher[0].es_meta, 'explanation')\n    assert hasattr(searcher[0].es_meta, 'highlight')\n", "label": 1}
{"function": "\n\ndef build_grad(self):\n    '\\n        actually back-propagate the gradient\\n        '\n    assert (self.grad_op_tree is not None)\n    if (type(self.op_tree) == OpTreeNode):\n        (left_increment, right_increment) = GradUtil.get_grad_back(self)\n        if (self.left.grad_op_tree is None):\n            self.left.grad_op_tree = left_increment\n        else:\n            self.left.grad_op_tree = (self.left.grad_op_tree + left_increment)\n        self.left.build_grad()\n        if (right_increment is None):\n            return\n        if (self.right.grad_op_tree is None):\n            self.right.grad_op_tree = right_increment\n        else:\n            self.right.grad_op_tree = (self.right.grad_op_tree + right_increment)\n        self.right.build_grad()\n    elif isinstance(self.op_tree, Tensor):\n        self.ad.map_tensor_grad_op_tree[self.op_tree._original_base] = self.grad_op_tree\n", "label": 0}
{"function": "\n\n@sensitive_variables()\ndef clean(self):\n    default_domain = getattr(settings, 'OPENSTACK_KEYSTONE_DEFAULT_DOMAIN', 'Default')\n    username = self.cleaned_data.get('username')\n    password = self.cleaned_data.get('password')\n    region = self.cleaned_data.get('region')\n    domain = self.cleaned_data.get('domain', default_domain)\n    if (not (username and password)):\n        return self.cleaned_data\n    try:\n        self.user_cache = authenticate(request=self.request, username=username, password=password, user_domain_name=domain, auth_url=region)\n        msg = ('Login successful for user \"%(username)s\".' % {\n            'username': username,\n        })\n        LOG.info(msg)\n    except exceptions.KeystoneAuthException as exc:\n        msg = ('Login failed for user \"%(username)s\".' % {\n            'username': username,\n        })\n        LOG.warning(msg)\n        raise forms.ValidationError(exc)\n    if hasattr(self, 'check_for_test_cookie'):\n        self.check_for_test_cookie()\n    return self.cleaned_data\n", "label": 0}
{"function": "\n\ndef test_complex(self):\n    rng = numpy.random.RandomState(2333)\n    m = fmatrix()\n    c = complex(m[0], m[1])\n    assert (c.type == cvector)\n    (r, i) = [real(c), imag(c)]\n    assert (r.type == fvector)\n    assert (i.type == fvector)\n    f = theano.function([m], [r, i])\n    mval = numpy.asarray(rng.randn(2, 5), dtype='float32')\n    (rval, ival) = f(mval)\n    assert numpy.all((rval == mval[0])), (rval, mval[0])\n    assert numpy.all((ival == mval[1])), (ival, mval[1])\n", "label": 0}
{"function": "\n\ndef subclass_view(self, request, path):\n    '\\n        Forward any request to a custom view of the real admin.\\n        '\n    ct_id = int(request.GET.get('ct_id', 0))\n    if (not ct_id):\n        try:\n            pos = path.find('/')\n            if (pos == (- 1)):\n                object_id = long(path)\n            else:\n                object_id = long(path[0:pos])\n        except ValueError:\n            raise Http404(\"No ct_id parameter, unable to find admin subclass for path '{0}'.\".format(path))\n        ct_id = self.model.objects.values_list('polymorphic_ctype_id', flat=True).get(pk=object_id)\n    real_admin = self._get_real_admin_by_ct(ct_id)\n    resolver = RegexURLResolver('^', real_admin.urls)\n    resolvermatch = resolver.resolve(path)\n    if (not resolvermatch):\n        raise Http404(\"No match for path '{0}' in admin subclass.\".format(path))\n    return resolvermatch.func(request, *resolvermatch.args, **resolvermatch.kwargs)\n", "label": 0}
{"function": "\n\ndef _start_child(self, wrap):\n    if (len(wrap.forktimes) > wrap.workers):\n        if ((time.time() - wrap.forktimes[0]) < wrap.workers):\n            LOG.info(_('Forking too fast, sleeping'))\n            time.sleep(1)\n        wrap.forktimes.pop(0)\n    wrap.forktimes.append(time.time())\n    pid = os.fork()\n    if (pid == 0):\n        status = 0\n        try:\n            self._child_process(wrap.service)\n        except SignalExit as exc:\n            signame = {\n                signal.SIGTERM: 'SIGTERM',\n                signal.SIGINT: 'SIGINT',\n            }[exc.signo]\n            LOG.info(_('Caught %s, exiting'), signame)\n            status = exc.code\n        except SystemExit as exc:\n            status = exc.code\n        except BaseException:\n            LOG.exception(_('Unhandled exception'))\n            status = 2\n        finally:\n            wrap.service.stop()\n        os._exit(status)\n    LOG.info(_('Started child %d'), pid)\n    wrap.children.add(pid)\n    self.children[pid] = wrap\n    return pid\n", "label": 0}
{"function": "\n\ndef process_uplink_event(self, msg, phy_uplink):\n    LOG.info(_LI('Received New uplink Msg %(msg)s for uplink %(uplink)s'), {\n        'msg': msg.get_status(),\n        'uplink': phy_uplink,\n    })\n    if (msg.get_status() == 'up'):\n        ovs_exc_raised = False\n        try:\n            self.ovs_vdp_obj_dict[phy_uplink] = ovs_vdp.OVSNeutronVdp(phy_uplink, msg.get_integ_br(), msg.get_ext_br(), msg.get_root_helper())\n        except Exception as exc:\n            LOG.error(_LE('OVS VDP Object creation failed %s'), str(exc))\n            ovs_exc_raised = True\n        if (ovs_exc_raised or (not self.ovs_vdp_obj_dict[phy_uplink].is_lldpad_setup_done())):\n            LOG.error(_LE('UP Event Processing NOT Complete'))\n            self.err_que.enqueue(constants.Q_UPL_PRIO, msg)\n        else:\n            self.uplink_det_compl = True\n            veth_intf = self.ovs_vdp_obj_dict[self.phy_uplink].get_lldp_bridge_port()\n            LOG.info(_LI('UP Event Processing Complete Saving uplink %(ul)s and veth %(veth)s'), {\n                'ul': self.phy_uplink,\n                'veth': veth_intf,\n            })\n            self.save_uplink(uplink=self.phy_uplink, veth_intf=veth_intf)\n    elif (msg.get_status() == 'down'):\n        if (phy_uplink in self.ovs_vdp_obj_dict):\n            self.ovs_vdp_obj_dict[phy_uplink].clear_obj_params()\n        else:\n            ovs_vdp.delete_uplink_and_flows(self.root_helper, self.br_ex, phy_uplink)\n        self.save_uplink()\n", "label": 0}
{"function": "\n\ndef reverse_apicontroller_action(url, status, response):\n    '\\n    Make an API call look like a direct action call by reversing the\\n    exception -> HTTP response translation that ApiController.action does\\n    '\n    try:\n        parsed = json.loads(response)\n        if parsed.get('success'):\n            return parsed['result']\n        if hasattr(parsed, 'get'):\n            err = parsed.get('error', {\n                \n            })\n        else:\n            err = {\n                \n            }\n    except (AttributeError, ValueError):\n        err = {\n            \n        }\n    if (not isinstance(err, dict)):\n        raise ServerIncompatibleError(repr([url, status, response]))\n    etype = err.get('__type')\n    emessage = err.get('message', '').split(': ', 1)[(- 1)]\n    if (etype == 'Search Query Error'):\n        raise SearchQueryError(emessage)\n    elif (etype == 'Search Error'):\n        raise SearchError(emessage)\n    elif (etype == 'Search Index Error'):\n        raise SearchIndexError(emessage)\n    elif (etype == 'Validation Error'):\n        raise ValidationError(err)\n    elif (etype == 'Not Found Error'):\n        raise NotFound(emessage)\n    elif (etype == 'Authorization Error'):\n        raise NotAuthorized(err)\n    raise CKANAPIError(repr([url, status, response]))\n", "label": 1}
{"function": "\n\ndef install_pyfile(self, node):\n    path = self.bld.get_install_path(((self.install_path + os.sep) + node.name), self.env)\n    self.bld.install_files(self.install_path, [node], self.env, self.chmod, postpone=False)\n    if (self.bld.is_install < 0):\n        info('* removing byte compiled python files')\n        for x in 'co':\n            try:\n                os.remove((path + x))\n            except OSError:\n                pass\n    if (self.bld.is_install > 0):\n        if (self.env['PYC'] or self.env['PYO']):\n            info(('* byte compiling %r' % path))\n        if self.env['PYC']:\n            program = \"\\nimport sys, py_compile\\nfor pyfile in sys.argv[1:]:\\n\\tpy_compile.compile(pyfile, pyfile + 'c')\\n\"\n            argv = [self.env['PYTHON'], '-c', program, path]\n            ret = Utils.pproc.Popen(argv).wait()\n            if ret:\n                raise Utils.WafError(('bytecode compilation failed %r' % path))\n        if self.env['PYO']:\n            program = \"\\nimport sys, py_compile\\nfor pyfile in sys.argv[1:]:\\n\\tpy_compile.compile(pyfile, pyfile + 'o')\\n\"\n            argv = [self.env['PYTHON'], self.env['PYFLAGS_OPT'], '-c', program, path]\n            ret = Utils.pproc.Popen(argv).wait()\n            if ret:\n                raise Utils.WafError(('bytecode compilation failed %r' % path))\n", "label": 1}
{"function": "\n\ndef test_reify():\n    (x, y, z) = (var(), var(), var())\n    s = {\n        x: 1,\n        y: 2,\n        z: (x, y),\n    }\n    assert (reify(x, s) == 1)\n    assert (reify(10, s) == 10)\n    assert (reify((1, y), s) == (1, 2))\n    assert (reify((1, (x, (y, 2))), s) == (1, (1, (2, 2))))\n    assert (reify(z, s) == (1, 2))\n", "label": 0}
{"function": "\n\ndef raise_cannot_open(path):\n    \"\\n    Raise an exception saying we can't open `path`.\\n\\n    Parameters\\n    ----------\\n    path : str\\n        The path we cannot open\\n    \"\n    pieces = path.split('/')\n    for i in xrange(1, (len(pieces) + 1)):\n        so_far = '/'.join(pieces[0:i])\n        if (not os.path.exists(so_far)):\n            if (i == 1):\n                if (so_far == ''):\n                    continue\n                reraise_as(IOError((((('Cannot open ' + path) + ' (') + so_far) + ' does not exist)')))\n            parent = '/'.join(pieces[0:(i - 1)])\n            bad = pieces[(i - 1)]\n            if (not os.path.isdir(parent)):\n                reraise_as(IOError((((('Cannot open ' + path) + ' because ') + parent) + ' is not a directory.')))\n            candidates = os.listdir(parent)\n            if (len(candidates) == 0):\n                reraise_as(IOError((((('Cannot open ' + path) + ' because ') + parent) + ' is empty.')))\n            if (len(candidates) > 100):\n                reraise_as(IOError((((('Cannot open ' + path) + ' but can open ') + parent) + '.')))\n            if os.path.islink(path):\n                reraise_as(IOError((path + ' appears to be a symlink to a non-existent file')))\n            reraise_as(IOError((((((((('Cannot open ' + path) + ' but can open ') + parent) + '. Did you mean ') + match(bad, candidates)) + ' instead of ') + bad) + '?')))\n    assert False\n", "label": 1}
{"function": "\n\n@public\ndef lcm_list(seq, *gens, **args):\n    '\\n    Compute LCM of a list of polynomials.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy import lcm_list\\n    >>> from sympy.abc import x\\n\\n    >>> lcm_list([x**3 - 1, x**2 - 1, x**2 - 3*x + 2])\\n    x**5 - x**4 - 2*x**3 - x**2 + x + 2\\n\\n    '\n    seq = sympify(seq)\n\n    def try_non_polynomial_lcm(seq):\n        if ((not gens) and (not args)):\n            (domain, numbers) = construct_domain(seq)\n            if (not numbers):\n                return domain.one\n            elif domain.is_Numerical:\n                (result, numbers) = (numbers[0], numbers[1:])\n                for number in numbers:\n                    result = domain.lcm(result, number)\n                return domain.to_sympy(result)\n        return None\n    result = try_non_polynomial_lcm(seq)\n    if (result is not None):\n        return result\n    options.allowed_flags(args, ['polys'])\n    try:\n        (polys, opt) = parallel_poly_from_expr(seq, *gens, **args)\n    except PolificationFailed as exc:\n        result = try_non_polynomial_lcm(exc.exprs)\n        if (result is not None):\n            return result\n        else:\n            raise ComputationFailed('lcm_list', len(seq), exc)\n    if (not polys):\n        if (not opt.polys):\n            return S.One\n        else:\n            return Poly(1, opt=opt)\n    (result, polys) = (polys[0], polys[1:])\n    for poly in polys:\n        result = result.lcm(poly)\n    if (not opt.polys):\n        return result.as_expr()\n    else:\n        return result\n", "label": 1}
{"function": "\n\ndef create(self, ddlFile):\n    startedAt = time.time()\n    self.showStatus('Dropping prior tables')\n    for table in self.tablesInDB():\n        result = self.execute(('DROP TABLE %s' % self.dbTableName(table)), close=False, commit=False, fetch=False, action='dropping table')\n    self.showStatus('Dropping prior sequences')\n    for sequence in self.sequencesInDB():\n        result = self.execute(('DROP SEQUENCE %s' % sequence), close=False, commit=False, fetch=False, action='dropping sequence')\n    self.modelXbrl.profileStat(_('XbrlPublicDB: drop prior tables'), (time.time() - startedAt))\n    startedAt = time.time()\n    with io.open(((os.path.dirname(__file__) + os.sep) + ddlFile), 'rt', encoding='utf-8') as fh:\n        sql = fh.read().replace('%', '%%')\n    sqlstatements = []\n\n    def findstatements(start, end, laststatement):\n        for line in sql[start:end].split('\\n'):\n            (stmt, comment1, comment2) = line.partition('--')\n            laststatement += (stmt + '\\n')\n            if (';' in stmt):\n                sqlstatements.append(laststatement)\n                laststatement = ''\n        return laststatement\n    stmt = ''\n    i = 0\n    patternDollarEsc = re.compile('([$]\\\\w*[$])', (re.DOTALL + re.MULTILINE))\n    while (i < len(sql)):\n        match = patternDollarEsc.search(sql, i)\n        if (not match):\n            stmt = findstatements(i, len(sql), stmt)\n            sqlstatements.append(stmt)\n            break\n        dollarescape = match.group()\n        j = match.end()\n        stmt = findstatements(i, j, stmt)\n        i = sql.find(dollarescape, j)\n        if (i > j):\n            if (self.product == 'mysql'):\n                stmt = sql[j:i]\n                i += len(dollarescape)\n            else:\n                i += len(dollarescape)\n                stmt += sql[j:i]\n            sqlstatements.append(stmt)\n            stmt = ''\n    action = 'executing ddl in {}'.format(os.path.basename(ddlFile))\n    for (i, sql) in enumerate(sqlstatements):\n        if any(((cmd in sql) for cmd in ('CREATE TABLE', 'CREATE SEQUENCE', 'INSERT INTO', 'CREATE TYPE', 'CREATE FUNCTION', 'SET', 'CREATE INDEX', 'CREATE UNIQUE INDEX'))):\n            (statusMsg, sep, rest) = sql.strip().partition('\\n')\n            self.showStatus(statusMsg[0:50])\n            result = self.execute(sql, close=False, commit=False, fetch=False, action=action)\n            if TRACESQLFILE:\n                with io.open(TRACESQLFILE, 'a', encoding='utf-8') as fh:\n                    fh.write('\\n\\n>>> ddl {0}: \\n{1} \\n\\n>>> result: \\n{2}\\n'.format(i, sql, result))\n                    fh.write(sql)\n    self.showStatus('')\n    self.conn.commit()\n    self.modelXbrl.profileStat(_('XbrlPublicDB: create tables'), (time.time() - startedAt))\n    self.closeCursor()\n", "label": 1}
{"function": "\n\ndef recv_into(self, buffer, nbytes=None, flags=None):\n    '\\n        Receive data on the connection and store the data into a buffer rather\\n        than creating a new string.\\n\\n        :param buffer: The buffer to copy into.\\n        :param nbytes: (optional) The maximum number of bytes to read into the\\n            buffer. If not present, defaults to the size of the buffer. If\\n            larger than the size of the buffer, is reduced to the size of the\\n            buffer.\\n        :param flags: (optional) The only supported flag is ``MSG_PEEK``,\\n            all other flags are ignored.\\n        :return: The number of bytes read into the buffer.\\n        '\n    if (nbytes is None):\n        nbytes = len(buffer)\n    else:\n        nbytes = min(nbytes, len(buffer))\n    buf = _ffi.new('char[]', nbytes)\n    if ((flags is not None) and (flags & socket.MSG_PEEK)):\n        result = _lib.SSL_peek(self._ssl, buf, nbytes)\n    else:\n        result = _lib.SSL_read(self._ssl, buf, nbytes)\n    self._raise_ssl_error(self._ssl, result)\n    try:\n        buffer[:result] = memoryview(_ffi.buffer(buf, result))\n    except NameError:\n        buffer[:result] = _ffi.buffer(buf, result)\n    return result\n", "label": 0}
{"function": "\n\n@convert_local_errors\ndef write_file(self, file_path, contents, mode=None, owner=None, group=None):\n    full_path = os.path.join(self.tar_dir, self.node['host'], 'image.tar')\n    if (not os.path.isdir(os.path.dirname(full_path))):\n        os.makedirs(os.path.dirname(full_path))\n    file_obj = BytesIO(contents)\n    with tarfile.open(full_path, 'a') as output:\n        info = tarfile.TarInfo(file_path)\n        info.size = len(contents)\n        if (mode is not None):\n            info.mode = mode\n        if (owner is not None):\n            info.uid = owner\n        if (group is not None):\n            info.gid = group\n        info.mtime = time.time()\n        output.addfile(info, file_obj)\n", "label": 0}
{"function": "\n\ndef test_bounded_read_expect_close_with_content_length(self):\n    headers = {\n        b'connection': [b'close'],\n        b'content-length': [b'15'],\n    }\n    d = DummySocket()\n    r = HTTP11Response(200, 'OK', headers, d, None)\n    d._buffer = BytesIO(b'hello there sir')\n    assert (r.read(5) == b'hello')\n    assert (r.read(6) == b' there')\n    assert (r.read(8) == b' sir')\n    assert (r.read(9) == b'')\n    assert (r._sock is None)\n", "label": 0}
{"function": "\n\n@bacpypes_debugging\ndef integer_endec(v, x):\n    'Pass the value to Integer, construct a tag from the hex string,\\n    and compare results of encode and decoding each other.'\n    if _debug:\n        integer_endec._debug('integer_endec %r %r', v, x)\n    tag = integer_tag(x)\n    if _debug:\n        integer_endec._debug('    - tag: %r, %r', tag, tag.tagData)\n    obj = Integer(v)\n    if _debug:\n        integer_endec._debug('    - obj: %r, %r', obj, obj.value)\n    assert (integer_encode(obj) == tag)\n    assert (integer_decode(tag) == obj)\n", "label": 0}
{"function": "\n\ndef update_fingerprint_index(db, index, limit=1000):\n    with closing(index.connect()) as index:\n        max_id = int((index.get_attribute('max_document_id') or '0'))\n        query = sql.select([schema.fingerprint.c.id, sql.func.acoustid_extract_query(schema.fingerprint.c.fingerprint)]).where((schema.fingerprint.c.id > max_id)).order_by(schema.fingerprint.c.id).limit(limit)\n        in_transaction = False\n        for (id, fingerprint) in db.execute(query):\n            if (not in_transaction):\n                index.begin()\n                in_transaction = True\n            logger.debug('Adding fingerprint %s to index %s', id, index)\n            index.insert(id, fingerprint)\n        if in_transaction:\n            index.commit()\n", "label": 0}
{"function": "\n\ndef load_pkcs12(buffer, passphrase=None):\n    '\\n    Load a PKCS12 object from a buffer\\n\\n    :param buffer: The buffer the certificate is stored in\\n    :param passphrase: (Optional) The password to decrypt the PKCS12 lump\\n    :returns: The PKCS12 object\\n    '\n    passphrase = _text_to_bytes_and_warn('passphrase', passphrase)\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if (not passphrase):\n        passphrase = _ffi.NULL\n    p12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\n    if (p12 == _ffi.NULL):\n        _raise_current_error()\n    p12 = _ffi.gc(p12, _lib.PKCS12_free)\n    pkey = _ffi.new('EVP_PKEY**')\n    cert = _ffi.new('X509**')\n    cacerts = _ffi.new('Cryptography_STACK_OF_X509**')\n    parse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\n    if (not parse_result):\n        _raise_current_error()\n    cacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\n    try:\n        _raise_current_error()\n    except Error:\n        pass\n    if (pkey[0] == _ffi.NULL):\n        pykey = None\n    else:\n        pykey = PKey.__new__(PKey)\n        pykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\n    if (cert[0] == _ffi.NULL):\n        pycert = None\n        friendlyname = None\n    else:\n        pycert = X509.__new__(X509)\n        pycert._x509 = _ffi.gc(cert[0], _lib.X509_free)\n        friendlyname_length = _ffi.new('int*')\n        friendlyname_buffer = _lib.X509_alias_get0(cert[0], friendlyname_length)\n        friendlyname = _ffi.buffer(friendlyname_buffer, friendlyname_length[0])[:]\n        if (friendlyname_buffer == _ffi.NULL):\n            friendlyname = None\n    pycacerts = []\n    for i in range(_lib.sk_X509_num(cacerts)):\n        pycacert = X509.__new__(X509)\n        pycacert._x509 = _lib.sk_X509_value(cacerts, i)\n        pycacerts.append(pycacert)\n    if (not pycacerts):\n        pycacerts = None\n    pkcs12 = PKCS12.__new__(PKCS12)\n    pkcs12._pkey = pykey\n    pkcs12._cert = pycert\n    pkcs12._cacerts = pycacerts\n    pkcs12._friendlyname = friendlyname\n    return pkcs12\n", "label": 1}
{"function": "\n\ndef _run(self):\n    args = self.args\n\n    def notes(noteArgs, **kwargs):\n        gitArgs = ('notes --ref=%s %s %s' % (noteNamespace, noteArgs, args.commit))\n        return git(gitArgs, **kwargs)\n\n    def add(note):\n        notes(('add -fm \"%s\"' % note))\n    note = notes('show', errorValue='')\n    if args.delete:\n        if (not note):\n            return\n        workitem = args.workitem\n        if workitem:\n            items = note.split(',')\n            try:\n                items.remove(str(workitem))\n            except ValueError:\n                fail(('Workitem %s is not associated with %s' % (workitem, args.commit)))\n            if items:\n                add(','.join(items))\n                return\n        notes('remove')\n    elif args.workitem:\n        if note:\n            note += ','\n        add((note + str(args.workitem)))\n    elif note:\n        print(note)\n", "label": 1}
{"function": "\n\ndef get_common_replies(locale=settings.WIKI_DEFAULT_LANGUAGE):\n    'Returns the common replies.\\n\\n    Parses the KB article with the replies puts them in a list of dicts.\\n\\n    The KB article should have the following wiki syntax structure::\\n\\n        =Category 1=\\n\\n        ==Reply 1==\\n        Reply goes here http://example.com/kb-article\\n\\n        ==Reply 2==\\n        Another reply here\\n\\n        =Category 2=\\n        ==Reply 3==\\n        And another reply\\n\\n\\n    Which results in the following HTML::\\n\\n        <h1 id=\"w_category-1\">Category 1</h1>\\n        <h2 id=\"w_snippet-1\">Reply 1</h2>\\n        <p>Reply goes here <a href=\"http://example.com/kb-article\">\\n        http://example.com/kb-article</a>\\n        </p>\\n        <h2 id=\"w_snippet-2\">Reply 2</h2>\\n        <p>Another reply here\\n        </p>\\n        <h1 id=\"w_category-2\">Category 2</h1>\\n        <h2 id=\"w_snippet-3\">Reply 3</h2>\\n        <p>And another reply\\n        </p>\\n\\n\\n    The resulting list returned would be::\\n\\n        [{\\'title\\': \\'Category 1\\',\\n          \\'responses\\':\\n            [{\\'title\\': \\'Reply 1\\',\\n              \\'response\\': \\'Reply goes here http://example.com/kb-article\\'},\\n             {\\'title\\': \\'Reply 2\\',\\n              \\'response\\': \\'Another reply here\\'}]\\n         },\\n         {\\'title\\': \\'Category 2\\',\\n          \\'responses\\':\\n            [{\\'title\\': \\'Reply 3\\',\\n              \\'response\\': \\'And another reply\\'}]\\n         }]\\n\\n    '\n    replies = []\n    try:\n        default_doc = Document.objects.get(slug=REPLIES_DOCUMENT_SLUG, locale=settings.WIKI_DEFAULT_LANGUAGE)\n    except Document.DoesNotExist:\n        return replies\n    if (locale != default_doc.locale):\n        translated_doc = default_doc.translated_to(locale)\n        if (translated_doc and translated_doc.current_revision):\n            doc = translated_doc\n        else:\n            doc = default_doc\n    else:\n        doc = default_doc\n    pq = PyQuery(doc.html)\n    try:\n        current_node = pq('h1')[0]\n    except IndexError:\n        return replies\n    current_category = None\n    current_response = None\n    while (current_node is not None):\n        if (current_node.tag == 'h1'):\n            current_category = {\n                'title': current_node.text,\n                'responses': [],\n            }\n            replies.append(current_category)\n        elif (current_node.tag == 'h2'):\n            current_response = {\n                'title': current_node.text,\n                'response': '',\n            }\n            current_category['responses'].append(current_response)\n        elif (current_node.tag == 'p'):\n            text = current_node.text_content().strip()\n            if (text and current_response):\n                current_response['response'] = text\n        current_node = current_node.getnext()\n    return replies\n", "label": 1}
{"function": "\n\ndef _read(self, data):\n    if (not self.stream):\n        return\n    try:\n        self.parser.feed(data)\n    except i.StreamError as exc:\n        self.stream_error(exc.condition, exc.text, exc)\n    except xml.XMLSyntaxError as exc:\n        self.stream_error('bad-format', str(exc), exc)\n    except Exception as exc:\n        self.stream_error('internal-server-error', str(exc), exc)\n", "label": 0}
{"function": "\n\ndef test_create(self):\n    self.service.update_role.return_value = self.my_request\n    mock_role = self.mock_role()\n    self.service.get_role = mock.Mock(return_value=mock_role)\n    endpoint_len = len(mock_role.configuration_sets[0].input_endpoints)\n    result = self.endpoint.create(self.udp_endpoint_name, self.udp_port, self.instance_port, self.udp_protocol, self.idle_timeout)\n    result_endpoint_length = len(mock_role.configuration_sets[0].input_endpoints)\n    new_endpoint = mock_role.configuration_sets[0].input_endpoints[(- 1)]\n    assert (result_endpoint_length == (endpoint_len + 1))\n    assert (new_endpoint.name == self.udp_endpoint_name)\n    assert (new_endpoint.port == self.udp_port)\n    assert (new_endpoint.local_port == self.instance_port)\n    assert (new_endpoint.protocol == self.udp_protocol)\n    self.service.update_role.assert_called_once_with(self.cloud_service_name, self.instance_name, self.cloud_service_name, os_virtual_hard_disk=mock_role.os_virtual_hard_disk, network_config=mock_role.configuration_sets[0], availability_set_name=mock_role.availability_set_name, data_virtual_hard_disks=mock_role.data_virtual_hard_disks)\n    assert (result == self.my_request.request_id)\n", "label": 0}
{"function": "\n\ndef test_update_changes_with_autoflush(self):\n    User = self.classes.User\n    sess = Session()\n    (john, jack, jill, jane) = sess.query(User).order_by(User.id).all()\n    john.age = 50\n    jack.age = 37\n    sess.query(User).filter((User.age > 29)).update({\n        'age': (User.age - 10),\n    }, synchronize_session='evaluate')\n    for x in (john, jack, jill, jane):\n        assert (not sess.is_modified(x))\n    eq_([john.age, jack.age, jill.age, jane.age], [40, 27, 29, 27])\n    john.age = 25\n    assert (john in sess.dirty)\n    assert (jack not in sess.dirty)\n    assert (jill not in sess.dirty)\n    assert sess.is_modified(john)\n    assert (not sess.is_modified(jack))\n", "label": 1}
{"function": "\n\ndef _return_handler(self, ret_value, func, arguments):\n    'Check return values for errors and warnings.\\n        '\n    logger.debug('%s%s -> %r', func.__name__, _args_to_str(arguments), ret_value, extra=self._logging_extra)\n    try:\n        ret_value = constants.StatusCode(ret_value)\n    except ValueError:\n        pass\n    self._last_status = ret_value\n    session = None\n    if (func.__name__ not in ('viFindNext',)):\n        try:\n            session = arguments[0]\n        except KeyError:\n            raise Exception(('Function %r does not seem to be a valid visa function (len args %d)' % (func, len(arguments))))\n        if (func.__name__ in ('viOpenDefaultRM',)):\n            session = session._obj.value\n        if isinstance(session, integer_types):\n            self._last_status_in_session[session] = ret_value\n        elif (func.__name__ not in ('viClose', 'viGetAttribute', 'viSetAttribute', 'viStatusDesc')):\n            raise Exception(('Function %r does not seem to be a valid visa function (type args[0] %r)' % (func, type(session))))\n    if (ret_value < 0):\n        raise errors.VisaIOError(ret_value)\n    if (ret_value in self.issue_warning_on):\n        if (session and (ret_value not in self._ignore_warning_in_session[session])):\n            warnings.warn(errors.VisaIOWarning(ret_value), stacklevel=2)\n    return ret_value\n", "label": 1}
{"function": "\n\ndef _parse_waf(context, repos, record, identifier):\n    recobjs = []\n    content = util.http_request('GET', record)\n    LOGGER.debug(content)\n    try:\n        parser = etree.HTMLParser()\n        tree = etree.fromstring(content, parser)\n    except Exception as err:\n        raise Exception(('Could not parse WAF: %s' % str(err)))\n    up = urlparse(record)\n    links = []\n    LOGGER.debug('collecting links')\n    for link in tree.xpath('//a/@href'):\n        link = link.strip()\n        if (not link):\n            continue\n        if (link.find('?') != (- 1)):\n            continue\n        if (not link.endswith('.xml')):\n            LOGGER.debug('Skipping, not .xml')\n            continue\n        if ('/' in link):\n            if (link[(- 1)] == '/'):\n                continue\n            if (link[0] == '/'):\n                link = ('%s://%s%s' % (up.scheme, up.netloc, link))\n        else:\n            link = ('%s/%s' % (record, link))\n        LOGGER.debug('URL is: %s', link)\n        links.append(link)\n    LOGGER.debug('%d links found', len(links))\n    for link in links:\n        LOGGER.debug('Processing link %s', link)\n        linkcontent = util.http_request('GET', link)\n        recobj = _parse_metadata(context, repos, linkcontent)[0]\n        recobj.source = link\n        recobj.mdsource = link\n        recobjs.append(recobj)\n    return recobjs\n", "label": 1}
{"function": "\n\n@xmlrpc_func(returns='string', args=['string', 'string'])\ndef pingback_ping(source, target):\n    \"\\n    pingback.ping(sourceURI, targetURI) => 'Pingback message'\\n\\n    Notifies the server that a link has been added to sourceURI,\\n    pointing to targetURI.\\n\\n    See: http://hixie.ch/specs/pingback/pingback-1.0\\n    \"\n    try:\n        if (source == target):\n            return UNDEFINED_ERROR\n        site = Site.objects.get_current()\n        try:\n            document = ''.join(map((lambda byte_line: byte_line.decode('utf-8')), urlopen(source).readlines()))\n        except (HTTPError, URLError):\n            return SOURCE_DOES_NOT_EXIST\n        if (target not in document):\n            return SOURCE_DOES_NOT_LINK\n        (scheme, netloc, path, query, fragment) = urlsplit(target)\n        if (netloc != site.domain):\n            return TARGET_DOES_NOT_EXIST\n        try:\n            (view, args, kwargs) = resolve(path)\n        except Resolver404:\n            return TARGET_DOES_NOT_EXIST\n        try:\n            entry = Entry.published.get(slug=kwargs['slug'], publication_date__year=kwargs['year'], publication_date__month=kwargs['month'], publication_date__day=kwargs['day'])\n            if (not entry.pingbacks_are_open):\n                return TARGET_IS_NOT_PINGABLE\n        except (KeyError, Entry.DoesNotExist):\n            return TARGET_IS_NOT_PINGABLE\n        soup = BeautifulSoup(document, 'html.parser')\n        title = six.text_type(soup.find('title'))\n        title = ((title and strip_tags(title)) or _('No title'))\n        description = generate_pingback_content(soup, target, PINGBACK_CONTENT_LENGTH)\n        (pingback, created) = comments.get_model().objects.get_or_create(content_type=ContentType.objects.get_for_model(Entry), object_pk=entry.pk, user_url=source, site=site, defaults={\n            'comment': description,\n            'user_name': title,\n            'submit_date': timezone.now(),\n        })\n        if created:\n            pingback.flags.create(user=get_user_flagger(), flag=PINGBACK)\n            pingback_was_posted.send(pingback.__class__, pingback=pingback, entry=entry)\n            return ('Pingback from %s to %s registered.' % (source, target))\n        return PINGBACK_ALREADY_REGISTERED\n    except:\n        return UNDEFINED_ERROR\n", "label": 1}
{"function": "\n\ndef test_bind_arguments(self):\n    (users, Address, addresses, User) = (self.tables.users, self.classes.Address, self.tables.addresses, self.classes.User)\n    mapper(User, users)\n    mapper(Address, addresses)\n    e1 = engines.testing_engine()\n    e2 = engines.testing_engine()\n    e3 = engines.testing_engine()\n    sess = Session(e3)\n    sess.bind_mapper(User, e1)\n    sess.bind_mapper(Address, e2)\n    assert (sess.connection().engine is e3)\n    assert (sess.connection(bind=e1).engine is e1)\n    assert (sess.connection(mapper=Address, bind=e1).engine is e1)\n    assert (sess.connection(mapper=Address).engine is e2)\n    assert (sess.connection(clause=addresses.select()).engine is e2)\n    assert (sess.connection(mapper=User, clause=addresses.select()).engine is e1)\n    assert (sess.connection(mapper=User, clause=addresses.select(), bind=e2).engine is e2)\n    sess.close()\n", "label": 1}
{"function": "\n\ndef _parse_settings_bond_5(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond5.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '5',\n    }\n    for binding in ['miimon', 'downdelay', 'updelay']:\n        if (binding in opts):\n            try:\n                int(opts[binding])\n                bond.update({\n                    binding: opts[binding],\n                })\n            except ValueError:\n                _raise_error_iface(iface, binding, ['integer'])\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({\n                binding: bond_def[binding],\n            })\n    if ('use_carrier' in opts):\n        if (opts['use_carrier'] in _CONFIG_TRUE):\n            bond.update({\n                'use_carrier': '1',\n            })\n        elif (opts['use_carrier'] in _CONFIG_FALSE):\n            bond.update({\n                'use_carrier': '0',\n            })\n        else:\n            valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({\n            'use_carrier': bond_def['use_carrier'],\n        })\n    return bond\n", "label": 0}
{"function": "\n\n@mock.patch('mficlient.client.MFiClient')\n@mock.patch('homeassistant.components.switch.mfi.MfiSwitch')\ndef test_setup_adds_proper_devices(self, mock_switch, mock_client):\n    'Test if setup adds devices.'\n    ports = {i: mock.MagicMock(model=model) for (i, model) in enumerate(mfi.SWITCH_MODELS)}\n    ports['bad'] = mock.MagicMock(model='notaswitch')\n    print(ports['bad'].model)\n    mock_client.return_value.get_devices.return_value = [mock.MagicMock(ports=ports)]\n    assert self.COMPONENT.setup(self.hass, self.GOOD_CONFIG)\n    for (ident, port) in ports.items():\n        if (ident != 'bad'):\n            mock_switch.assert_any_call(port)\n    assert (mock.call(ports['bad'], self.hass) not in mock_switch.mock_calls)\n", "label": 0}
{"function": "\n\ndef test_solve_biquadratic():\n    (x0, y0, x1, y1, r) = symbols('x0 y0 x1 y1 r')\n    f_1 = ((((x - 1) ** 2) + ((y - 1) ** 2)) - (r ** 2))\n    f_2 = ((((x - 2) ** 2) + ((y - 2) ** 2)) - (r ** 2))\n    assert (solve_poly_system([f_1, f_2], x, y) == [(((S(3) / 2) - (sqrt(((- 1) + (2 * (r ** 2)))) / 2)), ((S(3) / 2) + (sqrt(((- 1) + (2 * (r ** 2)))) / 2))), (((S(3) / 2) + (sqrt(((- 1) + (2 * (r ** 2)))) / 2)), ((S(3) / 2) - (sqrt(((- 1) + (2 * (r ** 2)))) / 2)))])\n    f_1 = ((((x - 1) ** 2) + ((y - 2) ** 2)) - (r ** 2))\n    f_2 = ((((x - 1) ** 2) + ((y - 1) ** 2)) - (r ** 2))\n    assert (solve_poly_system([f_1, f_2], x, y) == [((1 - (sqrt((((2 * r) - 1) * ((2 * r) + 1))) / 2)), (S(3) / 2)), ((1 + (sqrt((((2 * r) - 1) * ((2 * r) + 1))) / 2)), (S(3) / 2))])\n    query = (lambda expr: (expr.is_Pow and (expr.exp is S.Half)))\n    f_1 = ((((x - 1) ** 2) + ((y - 2) ** 2)) - (r ** 2))\n    f_2 = ((((x - x1) ** 2) + ((y - 1) ** 2)) - (r ** 2))\n    result = solve_poly_system([f_1, f_2], x, y)\n    assert ((len(result) == 2) and all(((len(r) == 2) for r in result)))\n    assert all(((r.count(query) == 1) for r in flatten(result)))\n    f_1 = ((((x - x0) ** 2) + ((y - y0) ** 2)) - (r ** 2))\n    f_2 = ((((x - x1) ** 2) + ((y - y1) ** 2)) - (r ** 2))\n    result = solve_poly_system([f_1, f_2], x, y)\n    assert ((len(result) == 2) and all(((len(r) == 2) for r in result)))\n    assert all(((len(r.find(query)) == 1) for r in flatten(result)))\n", "label": 1}
{"function": "\n\ndef load_profile(self, profile_directory):\n    list_files = os.listdir(profile_directory)\n    if (not list_files):\n        raise LangDetectException(ErrorCode.NeedLoadProfileError, ('Not found profile: ' + profile_directory))\n    (langsize, index) = (len(list_files), 0)\n    for filename in list_files:\n        if filename.startswith('.'):\n            continue\n        filename = path.join(profile_directory, filename)\n        if (not path.isfile(filename)):\n            continue\n        f = None\n        try:\n            if (sys.version_info[0] < 3):\n                f = open(filename, 'r')\n            else:\n                f = open(filename, 'r', encoding='utf-8')\n            json_data = json.load(f)\n            profile = LangProfile(**json_data)\n            self.add_profile(profile, index, langsize)\n            index += 1\n        except IOError:\n            raise LangDetectException(ErrorCode.FileLoadError, ('Cannot open \"%s\"' % filename))\n        except:\n            raise LangDetectException(ErrorCode.FormatError, ('Profile format error in \"%s\"' % filename))\n        finally:\n            if f:\n                f.close()\n", "label": 1}
{"function": "\n\ndef main():\n    '\\n    '\n    setup_logger(environ.get('AWS_SNS_ARN'))\n    config = load_config()\n    checkin_time = time()\n    try:\n        cw = connect_cloudwatch()\n    except:\n        cw = False\n    while True:\n        try:\n            with db_connect(config['DATABASE_URL']) as conn:\n                task_Q = db_queue(conn, TASK_QUEUE)\n                done_Q = db_queue(conn, DONE_QUEUE)\n                due_Q = db_queue(conn, DUE_QUEUE)\n                beat_Q = db_queue(conn, HEARTBEAT_QUEUE)\n                pop_task_from_donequeue(done_Q, config['GITHUB_AUTH'])\n                pop_task_from_duequeue(due_Q, config['GITHUB_AUTH'])\n                flush_heartbeat_queue(beat_Q)\n                if (time() < checkin_time):\n                    continue\n                with beat_Q as db:\n                    recent_workers = get_recent_workers(db)\n                    workers_n = sum(map(len, recent_workers.values()))\n                (task_n, done_n, due_n) = map(len, (task_Q, done_Q, due_Q))\n                _L.info('{workers_n} active workers; queue lengths: {task_n} tasks, {done_n} done, {due_n} due'.format(**locals()))\n                if cw:\n                    cw.put_metric_data('openaddr.ci', 'tasks queue', task_n, unit='Count')\n                    cw.put_metric_data('openaddr.ci', 'done queue', done_n, unit='Count')\n                    cw.put_metric_data('openaddr.ci', 'due queue', due_n, unit='Count')\n                    cw.put_metric_data('openaddr.ci', 'expected results', (task_n + workers_n), unit='Count')\n                    cw.put_metric_data('openaddr.ci', 'active workers', workers_n, unit='Count')\n                checkin_time = (time() + 30)\n        except KeyboardInterrupt:\n            raise\n        except:\n            _L.error('Error in dequeue main()', exc_info=True)\n            sleep(5)\n", "label": 0}
{"function": "\n\ndef test_basic_request(graph, groups, permissions, session, standard_graph, users):\n    group_sre = groups['team-sre']\n    group_not_sre = [g for (name, g) in groups.items() if (name != 'team-sre')]\n    assert (not any([group.my_requests(status='pending').all() for group in groups.values()])), 'no group should start with pending requests'\n    group_sre.add_member(users['testuser@a.co'], users['testuser@a.co'], reason='for the lulz')\n    session.commit()\n    request_not_sre = [group.my_requests(status='pending').all() for group in group_not_sre]\n    assert (not any(request_not_sre)), 'only affected group should show pending requests'\n    request_sre = group_sre.my_requests(status='pending').all()\n    assert (len(request_sre) == 1), 'affected group should have request'\n    request = session.query(Request).filter_by(id=request_sre[0].id).scalar()\n    request.update_status(users['gary@a.co'], 'actioned', 'for being a good person')\n    session.commit()\n    assert (not any([group.my_requests(status='pending').all() for group in groups.values()])), 'no group should have requests after being actioned'\n", "label": 1}
{"function": "\n\ndef login_by_token(request):\n    ret = None\n    auth_exception = None\n    if (request.method == 'POST'):\n        form = FacebookConnectForm(request.POST)\n        if form.is_valid():\n            try:\n                provider = providers.registry.by_id(FacebookProvider.id)\n                login_options = provider.get_fb_login_options(request)\n                app = providers.registry.by_id(FacebookProvider.id).get_app(request)\n                access_token = form.cleaned_data['access_token']\n                if (login_options.get('auth_type') == 'reauthenticate'):\n                    info = requests.get((GRAPH_API_URL + '/oauth/access_token_info'), params={\n                        'client_id': app.client_id,\n                        'access_token': access_token,\n                    }).json()\n                    nonce = provider.get_nonce(request, pop=True)\n                    ok = (nonce and (nonce == info.get('auth_nonce')))\n                else:\n                    ok = True\n                if (ok and provider.get_settings().get('EXCHANGE_TOKEN')):\n                    resp = requests.get((GRAPH_API_URL + '/oauth/access_token'), params={\n                        'grant_type': 'fb_exchange_token',\n                        'client_id': app.client_id,\n                        'client_secret': app.secret,\n                        'fb_exchange_token': access_token,\n                    }).json()\n                    access_token = resp['access_token']\n                if ok:\n                    token = SocialToken(app=app, token=access_token)\n                    login = fb_complete_login(request, app, token)\n                    login.token = token\n                    login.state = SocialLogin.state_from_request(request)\n                    ret = complete_social_login(request, login)\n            except requests.RequestException as e:\n                logger.exception('Error accessing FB user profile')\n                auth_exception = e\n    if (not ret):\n        ret = render_authentication_error(request, FacebookProvider.id, exception=auth_exception)\n    return ret\n", "label": 1}
{"function": "\n\ndef test_asbool(self):\n    dset1 = dictset({\n        'foo': 'true',\n        'fruit': 'false',\n        'baz': True,\n        'zoo': False,\n    })\n    assert dset1.asbool('foo')\n    assert dset1.asbool('baz')\n    assert isinstance(dset1.asbool('foo'), bool)\n    assert isinstance(dset1.asbool('baz'), bool)\n    assert (not dset1.asbool('fruit'))\n    assert (not dset1.asbool('zoo'))\n", "label": 0}
{"function": "\n\ndef client_send_osc_message(self, category, name, data):\n    \"Sends an OSC message to the client to update it\\n        Parameters:\\n        category - type of update, sw, coil, lamp, led, etc.\\n        name - the name of the object we're updating\\n        data - the data we're sending\\n        \"\n    if self.OSC_clients:\n        self.OSC_message = OSCmodule.OSCMessage(((('/' + str(category)) + '/') + name))\n        self.OSC_message.append(data)\n        for k in self.OSC_clients.items():\n            try:\n                if self.config['debug_messages']:\n                    self.log.info('Sending OSC Message to client:%s: %s', k, self.OSC_message)\n                k[1].send(self.OSC_message)\n            except OSCmodule.OSCClientError:\n                self.log.info('OSC client at address %s disconnected', k[0])\n                self.clients_to_delete.append(k)\n                break\n    for client in self.clients_to_delete:\n        if (client in self.OSC_clients):\n            del self.OSC_clients[client]\n    self.clients_to_delete = []\n    for client in self.clients_to_add:\n        self.setup_osc_client(client)\n", "label": 1}
{"function": "\n\ndef post_process_extensions(self, extensions, resp_obj, request, action_args):\n    for ext in extensions:\n        response = None\n        if inspect.isgenerator(ext):\n            try:\n                with ResourceExceptionHandler():\n                    response = ext.send(resp_obj)\n            except StopIteration:\n                continue\n            except Fault as ex:\n                response = ex\n        else:\n            try:\n                with ResourceExceptionHandler():\n                    response = ext(req=request, resp_obj=resp_obj, **action_args)\n            except exception.VersionNotFoundForAPIMethod:\n                continue\n            except Fault as ex:\n                response = ex\n        if response:\n            return response\n    return None\n", "label": 1}
{"function": "\n\ndef validate_params(method, D):\n    if (type(D['params']) == Object):\n        keys = list(method.json_arg_types.keys())\n        if (len(keys) != len(D['params'])):\n            raise InvalidParamsError(('Not enough params provided for %s' % method.json_sig))\n        for k in keys:\n            if (not (k in D['params'])):\n                print('\\n\\n\\n\\nSHITTER SHITTER', k, D, '\\n\\n\\n\\n')\n                raise InvalidParamsError(('%s is not a valid parameter for %s' % (k, method.json_sig)))\n            if (not (Any.kind(D['params'][k]) == method.json_arg_types[k])):\n                raise InvalidParamsError(('%s is not the correct type %s for %s' % (type(D['params'][k]), method.json_arg_types[k], method.json_sig)))\n    elif (type(D['params']) == Array):\n        arg_types = list(method.json_arg_types.values())\n        try:\n            for (i, arg) in enumerate(D['params']):\n                if (not (Any.kind(arg) == arg_types[i])):\n                    raise InvalidParamsError(('%s is not the correct type %s for %s' % (type(arg), arg_types[i], method.json_sig)))\n        except IndexError:\n            raise InvalidParamsError(('Too many params provided for %s' % method.json_sig))\n        else:\n            if (len(D['params']) != len(arg_types)):\n                raise InvalidParamsError(('Not enough params provided for %s' % method.json_sig))\n", "label": 1}
{"function": "\n\ndef test_setAllFieldsToZero_K1_D1(self, K=1, D=1):\n    A = ParamBag(K=K, D=D)\n    s = 123\n    N = np.ones(K)\n    x = np.ones((K, D))\n    xxT = np.ones((K, D, D))\n    W = np.ones((K, K))\n    A.setField('s', s)\n    A.setField('N', N, dims='K')\n    A.setField('x', x, dims=('K', 'D'))\n    A.setField('xxT', xxT, dims=('K', 'D', 'D'))\n    A.setField('W', W, dims=('K', 'K'))\n    A.setAllFieldsToZero()\n    assert np.allclose(A.s, 0.0)\n    assert np.allclose(A.N, np.zeros(K))\n    assert np.allclose(A.x, np.zeros(K))\n    assert np.allclose(A.xxT, np.zeros(K))\n    assert np.allclose(A.xxT, np.zeros((K, K)))\n", "label": 0}
{"function": "\n\ndef handleJsonMessage(self, incomingJsonMessage):\n    if (not isinstance(incomingJsonMessage, dict)):\n        raise MalformedMessageException(('Incoming message was not a dictionary: %s' % incomingJsonMessage))\n    if (not ('messageId' in incomingJsonMessage)):\n        raise MalformedMessageException(('Invalid incoming message id: %s' % incomingJsonMessage))\n    if (incomingJsonMessage['messageId'] != self.expectedMessageId):\n        raise MalformedMessageException(('Invalid incoming message id: expected %s, but got %s. %s' % (self.expectedMessageId, incomingJsonMessage['messageId'], incomingJsonMessage)))\n    try:\n        self.expectedMessageId += 1\n        if (incomingJsonMessage['messageType'] not in self.messageTypeHandlers):\n            raise MalformedMessageException('Invalid incoming messageType')\n        if (not ('objectDefinition' in incomingJsonMessage)):\n            raise MalformedMessageException('No object definition given')\n        if (incomingJsonMessage['messageType'] != 'ServerFlushObjectIdsBelow'):\n            obj = self.extractObjectDefinition(incomingJsonMessage['objectDefinition'])\n        else:\n            obj = None\n        return self.messageTypeHandlers[incomingJsonMessage['messageType']](incomingJsonMessage, obj)\n    except MalformedMessageException:\n        raise\n    except Exception as e:\n        return [unexpectedExceptionJson(incomingJsonMessage, Exceptions.wrapException(e).message)]\n", "label": 1}
{"function": "\n\ndef get_token_status(token, serializer, max_age=None, return_data=False):\n    'Get the status of a token.\\n\\n    :param token: The token to check\\n    :param serializer: The name of the seriailzer. Can be one of the\\n                       following: ``confirm``, ``login``, ``reset``\\n    :param max_age: The name of the max age config option. Can be on of\\n                    the following: ``CONFIRM_EMAIL``, ``LOGIN``, ``RESET_PASSWORD``\\n    '\n    serializer = getattr(_security, (serializer + '_serializer'))\n    max_age = get_max_age(max_age)\n    (user, data) = (None, None)\n    (expired, invalid) = (False, False)\n    try:\n        data = serializer.loads(token, max_age=max_age)\n    except SignatureExpired:\n        (d, data) = serializer.loads_unsafe(token)\n        expired = True\n    except (BadSignature, TypeError, ValueError):\n        invalid = True\n    if data:\n        user = _datastore.find_user(id=data[0])\n    expired = (expired and (user is not None))\n    if return_data:\n        return (expired, invalid, user, data)\n    else:\n        return (expired, invalid, user)\n", "label": 0}
{"function": "\n\ndef get_default_role(request):\n    'Gets the default role object from Keystone and saves it as a global.\\n\\n    Since this is configured in settings and should not change from request\\n    to request. Supports lookup by name or id.\\n    '\n    global DEFAULT_ROLE\n    default = getattr(settings, 'OPENSTACK_KEYSTONE_DEFAULT_ROLE', None)\n    if (default and (DEFAULT_ROLE is None)):\n        try:\n            roles = keystoneclient(request, admin=True).roles.list()\n        except Exception:\n            roles = []\n            exceptions.handle(request)\n        for role in roles:\n            if ((role.id == default) or (role.name == default)):\n                DEFAULT_ROLE = role\n                break\n    return DEFAULT_ROLE\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize(['gevent_count', 'subpool_size', 'iterations', 'expected_clients'], [(None, None, 1, 1), (None, None, 2, 1), (None, 10, 1, 10), (None, 10, 2, 10), (None, 200, 1, 100), (None, 200, 2, 100), (4, None, 1, 4), (4, None, 2, 4), (2, 2, 1, 4), (2, 2, 2, 4)])\ndef test_redis_disconnections(gevent_count, subpool_size, iterations, expected_clients, worker):\n    \" mrq.context.connections is not the actual connections pool that the worker uses.\\n        this worker's pool is not accessible from here, since it runs in a different thread.\\n    \"\n    from mrq.context import connections\n    worker.start_deps()\n    gevent_count = (gevent_count if (gevent_count is not None) else 1)\n    get_clients = (lambda : [c for c in connections.redis.client_list() if (c.get('cmd') != 'client')])\n    assert (len(get_clients()) == 0)\n    kwargs = {\n        'flags': '--redis_max_connections 100',\n        'deps': False,\n    }\n    if gevent_count:\n        kwargs['flags'] += (' --gevent %s' % gevent_count)\n    worker.start(**kwargs)\n    for i in range(0, iterations):\n        worker.send_tasks('tests.tasks.redis.Disconnections', ([{\n            'subpool_size': subpool_size,\n        }] * gevent_count))\n    assert (len(get_clients()) == expected_clients)\n    worker.stop(deps=False)\n    assert (len(get_clients()) == 0)\n    worker.stop_deps()\n", "label": 1}
{"function": "\n\ndef get_model(self):\n    '\\n        Returns the fitted bayesian model\\n\\n        Example\\n        ----------\\n        >>> from pgmpy.readwrite import BIFReader\\n        >>> reader = BIFReader(\"bif_test.bif\")\\n        >>> reader.get_model()\\n        <pgmpy.models.BayesianModel.BayesianModel object at 0x7f20af154320>\\n        '\n    try:\n        model = BayesianModel(self.variable_edges)\n        model.name = self.network_name\n        model.add_nodes_from(self.variable_names)\n        tabular_cpds = []\n        for var in sorted(self.variable_cpds.keys()):\n            values = self.variable_cpds[var]\n            cpd = TabularCPD(var, len(self.variable_states[var]), values, evidence=self.variable_parents[var], evidence_card=[len(self.variable_states[evidence_var]) for evidence_var in self.variable_parents[var]])\n            tabular_cpds.append(cpd)\n        model.add_cpds(*tabular_cpds)\n        for (node, properties) in self.variable_properties.items():\n            for prop in properties:\n                (prop_name, prop_value) = map((lambda t: t.strip()), prop.split('='))\n                model.node[node][prop_name] = prop_value\n        return model\n    except AttributeError:\n        raise AttributeError('First get states of variables, edges, parents and network name')\n", "label": 0}
{"function": "\n\ndef _calculate_approval(self):\n    'Calculates the approval information for the review request.'\n    from reviewboard.extensions.hooks import ReviewRequestApprovalHook\n    approved = True\n    failure = None\n    if (self.shipit_count == 0):\n        approved = False\n        failure = 'The review request has not been marked \"Ship It!\"'\n    elif (self.issue_open_count > 0):\n        approved = False\n        failure = 'The review request has open issues.'\n    for hook in ReviewRequestApprovalHook.hooks:\n        try:\n            result = hook.is_approved(self, approved, failure)\n            if isinstance(result, tuple):\n                (approved, failure) = result\n            elif isinstance(result, bool):\n                approved = result\n            else:\n                raise ValueError(('%r returned an invalid value %r from is_approved' % (hook, result)))\n            if approved:\n                failure = None\n        except Exception as e:\n            extension = hook.extension\n            logging.error('Error when running ReviewRequestApprovalHook.is_approved function in extension: \"%s\": %s', extension.id, e, exc_info=1)\n    self._approval_failure = failure\n    self._approved = approved\n", "label": 1}
{"function": "\n\ndef test_ambiguous_to_bool(self):\n    amb_values = ('', 'randomstring', '2', '-32', 'not true')\n    for value in amb_values:\n        for ba in (True, False):\n            jd = ('\"%s\"' if ba else '{\"a\": \"%s\"}')\n            try:\n                parse((jd % value), {\n                    'a': bool,\n                }, ba)\n                assert False, (\"Value '%s' should not parse correctly for %s.\" % (value, bool))\n            except ClientSideError as e:\n                self.assertIsInstance(e, InvalidInput)\n                self.assertEqual(e.fieldname, 'a')\n                self.assertEqual(e.value, value)\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('page', [None, 1, 5])\ndef test_with_classifiers(self, monkeypatch, db_request, page):\n    params = MultiDict([('q', 'foo bar'), ('c', 'foo :: bar'), ('c', 'fiz :: buz')])\n    if (page is not None):\n        params['page'] = page\n    db_request.params = params\n    es_query = pretend.stub(suggest=pretend.call_recorder((lambda *a, **kw: es_query)), filter=pretend.call_recorder((lambda *a, **kw: es_query)), sort=pretend.call_recorder((lambda *a, **kw: es_query)))\n    db_request.es = pretend.stub(query=pretend.call_recorder((lambda *a, **kw: es_query)))\n    classifier1 = ClassifierFactory.create(classifier='foo :: bar')\n    classifier2 = ClassifierFactory.create(classifier='foo :: baz')\n    classifier3 = ClassifierFactory.create(classifier='fiz :: buz')\n    page_obj = pretend.stub(page_count=((page or 1) + 10))\n    page_cls = pretend.call_recorder((lambda *a, **kw: page_obj))\n    monkeypatch.setattr(views, 'ElasticsearchPage', page_cls)\n    url_maker = pretend.stub()\n    url_maker_factory = pretend.call_recorder((lambda request: url_maker))\n    monkeypatch.setattr(views, 'paginate_url_factory', url_maker_factory)\n    assert (search(db_request) == {\n        'page': page_obj,\n        'term': params.get('q', ''),\n        'order': params.get('o', ''),\n        'applied_filters': params.getall('c'),\n        'available_filters': [('fiz', [classifier3.classifier]), ('foo', [classifier1.classifier, classifier2.classifier])],\n    })\n    assert (page_cls.calls == [pretend.call(es_query, url_maker=url_maker, page=(page or 1))])\n    assert (url_maker_factory.calls == [pretend.call(db_request)])\n    assert (db_request.es.query.calls == [pretend.call('multi_match', query='foo bar', fields=['name^2', 'version', 'author', 'author_email', 'maintainer', 'maintainer_email', 'home_page', 'license', 'summary', 'description', 'keywords', 'platform', 'download_url'])])\n    assert (es_query.suggest.calls == [pretend.call(name='name_suggestion', term={\n        'field': 'name',\n    }, text='foo bar')])\n    assert (es_query.filter.calls == [pretend.call('terms', classifiers=['foo :: bar', 'fiz :: buz'])])\n", "label": 1}
{"function": "\n\n@property\ndef cyclic_form(self):\n    '\\n        This is used to convert to the cyclic notation\\n        from the canonical notation. Singletons are omitted.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.combinatorics.permutations import Permutation\\n        >>> Permutation.print_cyclic = False\\n        >>> p = Permutation([0, 3, 1, 2])\\n        >>> p.cyclic_form\\n        [[1, 3, 2]]\\n        >>> Permutation([1, 0, 2, 4, 3, 5]).cyclic_form\\n        [[0, 1], [3, 4]]\\n\\n        See Also\\n        ========\\n\\n        array_form, full_cyclic_form\\n        '\n    if (self._cyclic_form is not None):\n        return list(self._cyclic_form)\n    array_form = self.array_form\n    unchecked = ([True] * len(array_form))\n    cyclic_form = []\n    for i in range(len(array_form)):\n        if unchecked[i]:\n            cycle = []\n            cycle.append(i)\n            unchecked[i] = False\n            j = i\n            while unchecked[array_form[j]]:\n                j = array_form[j]\n                cycle.append(j)\n                unchecked[j] = False\n            if (len(cycle) > 1):\n                cyclic_form.append(cycle)\n                assert (cycle == list(minlex(cycle, is_set=True)))\n    cyclic_form.sort()\n    self._cyclic_form = cyclic_form[:]\n    return cyclic_form\n", "label": 0}
{"function": "\n\ndef test_result_fragments_with_base(self):\n    rdfXml = serialize(self.sourceGraph, self.serializer, extra_args={\n        'base': 'http://example.org/',\n        'xml_base': 'http://example.org/',\n    })\n    assert (b('xml:base=\"http://example.org/\"') in rdfXml)\n    assert (b('<rdf:Description rdf:about=\"data/a\">') in rdfXml)\n    assert (b('<rdf:type rdf:resource=\"model/test#Test\"/>') in rdfXml)\n    assert (b('<rdf:Description rdf:about=\"data/b\">') in rdfXml)\n    assert (b('<value rdf:datatype=\"http://www.w3.org/2001/XMLSchema#integer\">3</value>') in rdfXml)\n    assert (b('<rdf:Description rdf:nodeID=\"') in rdfXml), 'expected one identified bnode in serialized graph'\n", "label": 0}
{"function": "\n\ndef load_chart(self, *args):\n    'Load a chart from a pickle file'\n    filename = askopenfilename(filetypes=self.CHART_FILE_TYPES, defaultextension='.pickle')\n    if (not filename):\n        return\n    try:\n        with open(filename, 'rb') as infile:\n            chart = pickle.load(infile)\n        self._chart = chart\n        self._cv.update(chart)\n        if self._matrix:\n            self._matrix.set_chart(chart)\n        if self._matrix:\n            self._matrix.deselect_cell()\n        if self._results:\n            self._results.set_chart(chart)\n        self._cp.set_chart(chart)\n    except Exception as e:\n        raise\n        tkinter.messagebox.showerror('Error Loading Chart', ('Unable to open file: %r' % filename))\n", "label": 0}
{"function": "\n\ndef lock(remote=None):\n    '\\n    Place an update.lk\\n\\n    ``remote`` can either be a dictionary containing repo configuration\\n    information, or a pattern. If the latter, then remotes for which the URL\\n    matches the pattern will be locked.\\n    '\n\n    def _do_lock(repo):\n        success = []\n        failed = []\n        if (not os.path.exists(repo['lockfile'])):\n            try:\n                with salt.utils.fopen(repo['lockfile'], 'w+') as fp_:\n                    fp_.write('')\n            except (IOError, OSError) as exc:\n                msg = 'Unable to set update lock for {0} ({1}): {2} '.format(repo['url'], repo['lockfile'], exc)\n                log.debug(msg)\n                failed.append(msg)\n            else:\n                msg = 'Set lock for {0}'.format(repo['url'])\n                log.debug(msg)\n                success.append(msg)\n        return (success, failed)\n    if isinstance(remote, dict):\n        return _do_lock(remote)\n    locked = []\n    errors = []\n    for repo in init():\n        if remote:\n            try:\n                if (not fnmatch.fnmatch(repo['url'], remote)):\n                    continue\n            except TypeError:\n                if (not fnmatch.fnmatch(repo['url'], six.text_type(remote))):\n                    continue\n        (success, failed) = _do_lock(repo)\n        locked.extend(success)\n        errors.extend(failed)\n    return (locked, errors)\n", "label": 0}
{"function": "\n\ndef sleep_while_creating(self, timeoutSec=120):\n    Util.validate_type(timeoutSec, 'int')\n    step = 3\n    isOk = False\n    while (0 < timeoutSec):\n        try:\n            if self.exists():\n                self.reload()\n                isOk = True\n        except saklient.errors.httpexception.HttpException:\n            pass\n        timeoutSec -= step\n        if isOk:\n            timeoutSec = 0\n        if (0 < timeoutSec):\n            Util.sleep(step)\n    return isOk\n", "label": 0}
{"function": "\n\n@never_cache\n@login_required\ndef change(request, app_label, model_name, instance_id, mode_name='change', form_fields=None, form_exclude=None):\n    instance_return = _get_instance(request, mode_name, app_label, model_name, instance_id, form_fields=form_fields, form_exclude=form_exclude)\n    if isinstance(instance_return, HttpResponseForbidden):\n        return instance_return\n    (model, instance_form, instance) = instance_return\n    cancel = _handle_cancel(request)\n    if cancel:\n        return cancel\n    if (request.method == 'POST'):\n        form = instance_form(request.POST, request.FILES, instance=instance)\n        if form.is_valid():\n            instance = form.save()\n            msg = (_('Your %(model_name)s was changed successfully') % {\n                'model_name': model._meta.verbose_name,\n            })\n            try:\n                request.user.message_set.create(message=msg)\n            except AttributeError:\n                messages.success(request, msg)\n            if request.is_ajax():\n                return success(request)\n            return _handle_response(request, instance)\n    else:\n        form = instance_form(instance=instance)\n    template_context = {\n        'action': 'change',\n        'action_url': request.get_full_path(),\n        'model_title': model._meta.verbose_name,\n        'form': form,\n    }\n    return render_to_response(_get_template(request, app_label, model_name), template_context, RequestContext(request))\n", "label": 0}
{"function": "\n\n@testhelp.context('sysmodel')\ndef testImageGroup(self):\n    repos = self.openRepository()\n    self.addComponent('foo:runtime=1.0')\n    self.addCollection('foo=1.0', [':runtime'])\n    grp = self._build('grp = world[\"foo\"].createGroup(\"group-sub\")', 'r.Group(grp)')\n    subGrp = repos.getTrove('group-sub', grp.getVersion(), grp.getFlavor())\n    assert (not grp.troveInfo.imageGroup())\n    assert (not subGrp.troveInfo.imageGroup())\n    grp = self._build('grp = world[\"foo\"].createGroup(\"group-sub\")', 'r.Group(grp, imageGroup = True)')\n    subGrp = repos.getTrove('group-sub', grp.getVersion(), grp.getFlavor())\n    assert grp.troveInfo.imageGroup()\n    assert (not subGrp.troveInfo.imageGroup())\n    grp = self._build('grp = world[\"foo\"].createGroup(\"group-sub\", imageGroup = True)', 'r.Group(grp, imageGroup = True)')\n    subGrp = repos.getTrove('group-sub', grp.getVersion(), grp.getFlavor())\n    assert grp.troveInfo.imageGroup()\n    assert subGrp.troveInfo.imageGroup()\n", "label": 0}
{"function": "\n\ndef mklink(self, source, target, force=None):\n    linked = False\n    if (not os.path.exists(source)):\n        raise SymlinkError('symlink source \"{0}\" does not exist'.format(collapseuser(source)))\n    try:\n        os.symlink(source, target)\n        linked = True\n        tty.puts('symlinked {0} -> {1}'.format(tty.color(collapseuser(target), tty.MAGENTA), collapseuser(source)))\n    except OSError as e:\n        if (e.errno != errno.EEXIST):\n            raise\n        if os.path.islink(target):\n            if os.path.samefile(os.path.realpath(target), os.path.realpath(source)):\n                linked = True\n                tty.putdebug('Already linked: {0} -> {1}'.format(tty.color(collapseuser(target), tty.MAGENTA), collapseuser(source)), self.debug)\n            else:\n                fmt = 'Linked to wrong target: {0} -> {1} (instead of {2})'\n                tty.puterr(fmt.format(tty.color(target, tty.MAGENTA), os.path.realpath(collapseuser(target)), os.path.realpath(collapseuser(source))), warning=force)\n        else:\n            tty.puterr('{0} symlink target already exists at: {1}'.format(collapseuser(source), collapseuser(target)), warning=force)\n    if ((not linked) and force):\n        try:\n            osx.move_to_trash(target)\n            print(tty.progress('Moved {0} to trash').format(target))\n        except OSError as e:\n            tty.puterr('Error moving {0} to trash: {1}'.format(target, str(e)))\n            return False\n        return self.mklink(source, target, force)\n    return linked\n", "label": 1}
{"function": "\n\ndef __init__(self, **kwargs):\n    '\\n        Use keyword arguments to overwrite\\n        any of the class attributes for this instance.\\n\\n        '\n    assert all((hasattr(type(self), attr) for attr in kwargs.keys()))\n    self.__dict__.update(**kwargs)\n    if (self.stdin_encoding is None):\n        self.stdin_encoding = (getattr(self.stdin, 'encoding', None) or 'utf8')\n    if (self.stdout_encoding is None):\n        actual_stdout = self.stdout\n        if is_windows:\n            from colorama import AnsiToWin32\n            if isinstance(self.stdout, AnsiToWin32):\n                actual_stdout = self.stdout.wrapped\n        self.stdout_encoding = (getattr(actual_stdout, 'encoding', None) or 'utf8')\n", "label": 1}
{"function": "\n\ndef _make_plugin_call(self, plugin, method, instance=None, vm_ref=None, **addl_args):\n    'Abstracts out the process of calling a method of a xenapi plugin.\\n        Any errors raised by the plugin will in turn raise a RuntimeError here.\\n        '\n    args = {\n        \n    }\n    if (instance or vm_ref):\n        args['dom_id'] = self._get_dom_id(instance, vm_ref)\n    args.update(addl_args)\n    try:\n        return self._session.call_plugin(plugin, method, args)\n    except self._session.XenAPI.Failure as e:\n        err_msg = e.details[(- 1)].splitlines()[(- 1)]\n        if ('TIMEOUT:' in err_msg):\n            LOG.error(_LE('TIMEOUT: The call to %(method)s timed out. args=%(args)r'), {\n                'method': method,\n                'args': args,\n            }, instance=instance)\n            return {\n                'returncode': 'timeout',\n                'message': err_msg,\n            }\n        elif ('NOT IMPLEMENTED:' in err_msg):\n            LOG.error(_LE('NOT IMPLEMENTED: The call to %(method)s is not supported by the agent. args=%(args)r'), {\n                'method': method,\n                'args': args,\n            }, instance=instance)\n            return {\n                'returncode': 'notimplemented',\n                'message': err_msg,\n            }\n        else:\n            LOG.error(_LE('The call to %(method)s returned an error: %(e)s. args=%(args)r'), {\n                'method': method,\n                'args': args,\n                'e': e,\n            }, instance=instance)\n            return {\n                'returncode': 'error',\n                'message': err_msg,\n            }\n", "label": 0}
{"function": "\n\n@app.before_first_request\ndef check_db_schema():\n    if (db.engine.name != 'sqlite'):\n        version = AlembicVersion.query.scalar()\n        assert version, (\"no schema revision found. please run '%s db upgrade'\" % __file__)\n        migrations = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'migrations', 'versions')\n        revisions = [_parse_revision(os.path.join(migrations, filename)) for filename in os.listdir(migrations) if filename.endswith('.py')]\n        max_revision = max(revisions)\n        db_version = int(version.version_num)\n        assert (db_version == max_revision), (\"old database schema found: revision %d. please run '%s db upgrade' to upgrade to revision %d\" % (db_version, __file__, max_revision))\n", "label": 0}
{"function": "\n\ndef generate_lprofile(self, fun):\n    '\\n        Generate div containing profiled source code with timings of each line,\\n        taken from iline_profiler.\\n        '\n    try:\n        filename = fun.co_filename\n        firstlineno = fun.co_firstlineno\n        name = fun.co_name\n    except AttributeError:\n        return\n    ltimings_key = (filename, firstlineno, name)\n    try:\n        ltimings = self.lprofile.timings[ltimings_key]\n    except KeyError:\n        return\n    filename = ltimings[(- 1)]\n    if filename.endswith(('.pyc', '.pyo')):\n        filename = openpy.source_from_cache(filename)\n    if ('.egg/' in filename):\n        add_zipped_file_to_linecache(filename)\n    raw_code = ''\n    linenos = range(firstlineno, (ltimings[(- 2)][0] + 1))\n    for lineno in linenos:\n        raw_code += ulinecache.getline(filename, lineno)\n    formatter = LProfileFormatter(firstlineno, ltimings, noclasses=True)\n    self.value_cache += highlight(raw_code, PythonLexer(), formatter)\n", "label": 0}
{"function": "\n\ndef test_column_types(self):\n    df = self.alltypes.execute()\n    assert (df.tinyint_col.dtype.name == 'int8')\n    assert (df.smallint_col.dtype.name == 'int16')\n    assert (df.int_col.dtype.name == 'int32')\n    assert (df.bigint_col.dtype.name == 'int64')\n    assert (df.float_col.dtype.name == 'float32')\n    assert (df.double_col.dtype.name == 'float64')\n    assert pd.core.common.is_datetime64_dtype(df.timestamp_col.dtype)\n", "label": 1}
{"function": "\n\ndef resolve_rosdep_key(key, os_name, os_version, ros_distro=None, ignored=None, retry=True):\n    ignored = (ignored or [])\n    ctx = create_default_installer_context()\n    try:\n        installer_key = ctx.get_default_os_installer_key(os_name)\n    except KeyError:\n        BloomGenerator.exit(\"Could not determine the installer for '{0}'\".format(os_name))\n    installer = ctx.get_installer(installer_key)\n    ros_distro = (ros_distro or DEFAULT_ROS_DISTRO)\n    view = get_view(os_name, os_version, ros_distro)\n    try:\n        return resolve_more_for_os(key, view, installer, os_name, os_version)\n    except (KeyError, ResolutionError) as exc:\n        debug(traceback.format_exc())\n        if (key in ignored):\n            return (None, None, None)\n        if isinstance(exc, KeyError):\n            error(\"Could not resolve rosdep key '{0}'\".format(key))\n            returncode = code.GENERATOR_NO_SUCH_ROSDEP_KEY\n        else:\n            error(\"Could not resolve rosdep key '{0}' for distro '{1}':\".format(key, os_version))\n            info(str(exc), use_prefix=False)\n            returncode = code.GENERATOR_NO_ROSDEP_KEY_FOR_DISTRO\n        if retry:\n            error('Try to resolve the problem with rosdep and then continue.')\n            if maybe_continue():\n                update_rosdep()\n                invalidate_view_cache()\n                return resolve_rosdep_key(key, os_name, os_version, ros_distro, ignored, retry=True)\n        BloomGenerator.exit(\"Failed to resolve rosdep key '{0}', aborting.\".format(key), returncode=returncode)\n", "label": 1}
{"function": "\n\ndef test_that_all_str_methods_are_tested(self):\n    tested = set([x.split('_')[1] for x in dir(self) if x.startswith('test_')])\n    needed = set([x for x in dir(str) if (not x.startswith('_'))])\n    ignore = 'encode decode format format_map isdecimal isdigit isprintable maketrans'\n    needed = needed.difference(ignore.split(' '))\n    not_tested = needed.difference(tested)\n    assert (not not_tested)\n", "label": 0}
{"function": "\n\ndef assert_params_for_cmd(self, cmd, params=None, expected_rc=0, stderr_contains=None, ignore_params=None):\n    (stdout, stderr, rc) = self.run_cmd(cmd, expected_rc)\n    if (stderr_contains is not None):\n        self.assertIn(stderr_contains, stderr)\n    if (params is not None):\n        last_kwargs = copy.copy(self.last_kwargs)\n        if (ignore_params is not None):\n            for key in ignore_params:\n                try:\n                    del last_kwargs[key]\n                except KeyError:\n                    pass\n        if (params != last_kwargs):\n            self.fail(('Actual params did not match expected params.\\nExpected:\\n\\n%s\\nActual:\\n\\n%s\\n' % (pformat(params), pformat(last_kwargs))))\n    return (stdout, stderr, rc)\n", "label": 0}
{"function": "\n\ndef spawn_worker(self):\n    self.worker_age += 1\n    worker = self.worker_class(self.worker_age, self.pid, self.LISTENERS, self.app, (self.timeout / 2.0), self.cfg, self.log)\n    self.cfg.pre_fork(self, worker)\n    pid = os.fork()\n    if (pid != 0):\n        self.WORKERS[pid] = worker\n        return pid\n    worker_pid = os.getpid()\n    try:\n        util._setproctitle(('worker [%s]' % self.proc_name))\n        self.log.info('Booting worker with pid: %s', worker_pid)\n        self.cfg.post_fork(self, worker)\n        worker.init_process()\n        sys.exit(0)\n    except SystemExit:\n        raise\n    except AppImportError as e:\n        self.log.debug('Exception while loading the application: \\n%s', traceback.format_exc())\n        print(('%s' % e), file=sys.stderr)\n        sys.stderr.flush()\n        sys.exit(self.APP_LOAD_ERROR)\n    except:\n        self.log.exception('Exception in worker process:\\n%s', traceback.format_exc())\n        if (not worker.booted):\n            sys.exit(self.WORKER_BOOT_ERROR)\n        sys.exit((- 1))\n    finally:\n        self.log.info('Worker exiting (pid: %s)', worker_pid)\n        try:\n            worker.tmp.close()\n            self.cfg.worker_exit(self, worker)\n        except:\n            pass\n", "label": 0}
{"function": "\n\n@login_required\ndef rule_new(request, testplan_id):\n    try:\n        testplan = TestPlan(auth_token=request.user.password).get(testplan_id)\n    except UnauthorizedException:\n        logger.warning('User unauthorized. Signing out...')\n        return signout(request)\n    except NotFoundException:\n        return render(request, '404.html')\n    except Exception as inst:\n        logger.error('Unexpected exception', exc_info=True)\n        messages.error(request, (inst.message if inst.message else 'Unexpected error'))\n        return HttpResponseRedirect(reverse('testplan_list'))\n    form = RuleForm((request.POST or None))\n    if form.is_valid():\n        try:\n            rule_id = Rule(testplan_id, auth_token=request.user.password).create(form.cleaned_data)\n            return HttpResponseRedirect(reverse('rule_details', args=(str(testplan_id), str(rule_id))))\n        except UnauthorizedException:\n            logger.warning('User unauthorized. Signing out...')\n            return signout(request)\n        except Exception as inst:\n            messages.error(request, (inst.message if inst.message else 'Unexpected error'))\n            return HttpResponseRedirect(reverse('rule_new', args=(str(testplan_id),)))\n    return render(request, 'rules/rule_new.html', {\n        'form': form,\n        'testplan': testplan,\n    })\n", "label": 1}
{"function": "\n\ndef test_update(self):\n    kc = KeyBundle([{\n        'kty': 'oct',\n        'key': 'supersecret',\n        'use': 'sig',\n    }])\n    assert (len(kc.get('oct')) == 1)\n    assert (len(kc.get('rsa')) == 0)\n    assert (kc.remote is False)\n    assert (kc.source is None)\n    kc.update()\n    assert (len(kc.get('oct')) == 1)\n    assert (len(kc.get('rsa')) == 0)\n    assert (kc.remote is False)\n    assert (kc.source is None)\n", "label": 1}
{"function": "\n\ndef test_passes_on_kwargs():\n    'Verify that we pass on kwargs to the Pool constructor.'\n    mocked_pool = mock.Mock(spec=['join_all', 'responses', 'exceptions'])\n    with mock.patch('requests_toolbelt.threaded.pool.Pool') as Pool:\n        Pool.return_value = mocked_pool\n        threaded.map([{\n            \n        }, {\n            \n        }], num_processes=1000, initializer=test_passes_on_kwargs)\n    (_, kwargs) = Pool.call_args\n    assert ('job_queue' in kwargs)\n    assert ('num_processes' in kwargs)\n    assert ('initializer' in kwargs)\n    assert (kwargs['num_processes'] == 1000)\n    assert (kwargs['initializer'] == test_passes_on_kwargs)\n", "label": 0}
{"function": "\n\ndef test_givens(self):\n    x = shared(0)\n    assign = pfunc([], x, givens={\n        x: 3,\n    })\n    assert (assign() == 3)\n    assert (x.get_value(borrow=True) == 0)\n    y = tensor.ivector()\n    f = pfunc([y], (y * x), givens={\n        x: 6,\n    })\n    assert numpy.all((f([1, 1, 1]) == [6, 6, 6]))\n    assert (x.get_value() == 0)\n    z = tensor.ivector()\n    c = (z * y)\n    f = pfunc([y], (c + 7), givens={\n        z: theano._asarray([4, 4, 4], dtype='int32'),\n    })\n    assert numpy.all((f([1, 1, 1]) == [11, 11, 11]))\n    assert (x.get_value() == 0)\n", "label": 0}
{"function": "\n\ndef assert_raise_message(exceptions, message, function, *args, **kwargs):\n    'Helper function to test error messages in exceptions.\\n\\n    Parameters\\n    ----------\\n    exceptions : exception or tuple of exception\\n        Name of the estimator\\n\\n    func : callable\\n        Calable object to raise error\\n\\n    *args : the positional arguments to `func`.\\n\\n    **kw : the keyword arguments to `func`\\n    '\n    try:\n        function(*args, **kwargs)\n    except exceptions as e:\n        error_message = str(e)\n        if (message not in error_message):\n            raise AssertionError(('Error message does not include the expected string: %r. Observed error message: %r' % (message, error_message)))\n    else:\n        if isinstance(exceptions, tuple):\n            names = ' or '.join((e.__name__ for e in exceptions))\n        else:\n            names = exceptions.__name__\n        raise AssertionError(('%s not raised by %s' % (names, function.__name__)))\n", "label": 0}
{"function": "\n\ndef put(self, obj, block=True, timeout=None):\n    if (not block):\n        return self.put_nowait()\n    start_time = time.time()\n    while True:\n        try:\n            return self.put_nowait(obj)\n        except self.Full:\n            if timeout:\n                lasted = (time.time() - start_time)\n                if (timeout > lasted):\n                    time.sleep(min(self.max_timeout, (timeout - lasted)))\n                else:\n                    raise\n            else:\n                time.sleep(self.max_timeout)\n", "label": 0}
{"function": "\n\ndef run_backup(self):\n    \"The actual backup is performed. The data for all added classes is\\n        extracted and written to a file per class where each line (terminated\\n        by a line feed character) is the JSON representing a single object.\\n        Those files are all archived in a single gzip'ed tarball which is\\n        stored in the AWS S3 bucket specified when the current instance of\\n        Backup was created\"\n    self.log('Starting backup at %s', now_field())\n    self.log('Backup config object created at %s', self.timestamp)\n    for fld in ['aws_access_key', 'aws_secret_key', 'bucketname']:\n        val = getattr(self, fld, None)\n        if (not val):\n            self.log('Backup cannot start: %s is a required field', fld)\n            raise ValueError(self.backup_log[(- 1)])\n    backup_file = NamedTemporaryFile(suffix='.tar.gz')\n    backup_tarfile = tarfile.open(fileobj=backup_file, mode='w:gz')\n    for (cls_name, cls) in self.classes.items():\n        self.log('Backing up %s', cls_name)\n        rec_count = 0\n        with NamedTemporaryFile() as record_file:\n            for rec in cls.find_all():\n                write_line(record_file, rec.to_data())\n                rec_count += 1\n            record_file.flush()\n            backup_tarfile.add(record_file.name, arcname=(cls_name + '.json'))\n        self.log('%s => %d records backed up', cls_name, rec_count)\n    backup_tarfile.close()\n    backup_file.flush()\n    backup_size = os.stat(backup_file.name)[6]\n    key_name = (('Backup_' + now_field()) + '.tar.gz').replace(':', '_')\n    if (os.environ.get('DEBUG', False) or os.environ.get('travis', False)):\n        conn = S3Connection('', '', is_secure=False, port=8888, host='localhost', calling_format=OrdinaryCallingFormat())\n    else:\n        conn = S3Connection(self.aws_access_key, self.aws_secret_key)\n    bucket = conn.get_bucket(self.bucketname)\n    key = Key(bucket)\n    key.key = key_name\n    self.log('Sending %s [size=%d bytes] with key name %s', backup_file.name, backup_size, key_name)\n    key.set_contents_from_filename(backup_file.name)\n    self.log('Sent %s', backup_file.name)\n    backup_file.close()\n    self.log('Backup completed')\n    return (self.bucketname, key_name)\n", "label": 0}
{"function": "\n\ndef test_external_proxy(request, io_loop):\n    'Test a proxy started before the Hub'\n    auth_token = 'secret!'\n    proxy_ip = '127.0.0.1'\n    proxy_port = 54321\n    app = MockHub.instance(proxy_api_ip=proxy_ip, proxy_api_port=proxy_port, proxy_auth_token=auth_token)\n\n    def fin():\n        MockHub.clear_instance()\n        app.stop()\n    request.addfinalizer(fin)\n    env = os.environ.copy()\n    env['CONFIGPROXY_AUTH_TOKEN'] = auth_token\n    cmd = (app.proxy_cmd + ['--ip', app.ip, '--port', str(app.port), '--api-ip', proxy_ip, '--api-port', str(proxy_port), '--default-target', ('http://%s:%i' % (app.hub_ip, app.hub_port))])\n    if app.subdomain_host:\n        cmd.append('--host-routing')\n    proxy = Popen(cmd, env=env)\n\n    def _cleanup_proxy():\n        if (proxy.poll() is None):\n            proxy.terminate()\n    request.addfinalizer(_cleanup_proxy)\n\n    def wait_for_proxy():\n        io_loop.run_sync((lambda : wait_for_http_server(('http://%s:%i' % (proxy_ip, proxy_port)))))\n    wait_for_proxy()\n    app.start([])\n    assert (app.proxy_process is None)\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    assert (list(routes.keys()) == ['/'])\n    name = 'river'\n    r = api_request(app, 'users', name, method='post')\n    r.raise_for_status()\n    r = api_request(app, 'users', name, 'server', method='post')\n    r.raise_for_status()\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    user_path = '/user/river'\n    if app.subdomain_host:\n        domain = urlparse(app.subdomain_host).hostname\n        user_path = (('/%s.%s' % (name, domain)) + user_path)\n    assert (sorted(routes.keys()) == ['/', user_path])\n    proxy.terminate()\n    proxy = Popen(cmd, env=env)\n    wait_for_proxy()\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    assert (list(routes.keys()) == ['/'])\n    r = api_request(app, 'proxy', method='post')\n    r.raise_for_status()\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    assert (sorted(routes.keys()) == ['/', user_path])\n    proxy.terminate()\n    new_auth_token = 'different!'\n    env['CONFIGPROXY_AUTH_TOKEN'] = new_auth_token\n    proxy_port = 55432\n    cmd = (app.proxy_cmd + ['--ip', app.ip, '--port', str(app.port), '--api-ip', app.proxy_api_ip, '--api-port', str(proxy_port), '--default-target', ('http://%s:%i' % (app.hub_ip, app.hub_port))])\n    if app.subdomain_host:\n        cmd.append('--host-routing')\n    proxy = Popen(cmd, env=env)\n    wait_for_proxy()\n    r = api_request(app, 'proxy', method='patch', data=json.dumps({\n        'port': proxy_port,\n        'protocol': 'http',\n        'ip': app.ip,\n        'auth_token': new_auth_token,\n    }))\n    r.raise_for_status()\n    assert (app.proxy.api_server.port == proxy_port)\n\n    def get_app_proxy_token():\n        q = Queue()\n        app.io_loop.add_callback((lambda : q.put(app.proxy.auth_token)))\n        return q.get(timeout=2)\n    assert (get_app_proxy_token() == new_auth_token)\n    app.proxy.auth_token = new_auth_token\n    routes = io_loop.run_sync(app.proxy.get_routes)\n    assert (sorted(routes.keys()) == ['/', user_path])\n", "label": 1}
{"function": "\n\ndef downloader(args):\n    url = args.download\n    from urllib2 import urlopen, HTTPError\n    try:\n        res = urlopen(url)\n    except HTTPError:\n        from initpy.prompt import color_print\n        color_print('Wrong downloadable url!', 'red')\n        return\n    from initpy.compact import StringIO\n    from zipfile import ZipFile, BadZipfile\n    try:\n        template_zip = ZipFile(StringIO(res.read()))\n    except BadZipfile:\n        from initpy.prompt import color_print\n        color_print('initpy only support zip file!', 'red')\n        return\n    from os import path, getcwd, mkdir\n    proj_path = path.join(getcwd(), args.name)\n    try:\n        mkdir(proj_path)\n    except OSError:\n        pass\n    zip_root = template_zip.namelist()[0]\n    for fn in template_zip.namelist()[1:]:\n        file_name = fn.replace(zip_root, '')\n        file_path = path.join(proj_path, file_name)\n        if file_path.endswith('/'):\n            try:\n                mkdir(file_path)\n            except OSError:\n                pass\n        else:\n            _file = open(file_path, 'w')\n            _file.write(template_zip.read(fn))\n            _file.close()\n", "label": 0}
{"function": "\n\ndef db_delete_connection(self, connection):\n    self.is_dirty = True\n    try:\n        i = self.db_connections_inverted_id_index[connection._db_id]\n    except KeyError:\n        for i in xrange(len(self._db_connections)):\n            if (self._db_connections[i]._db_id == connection._db_id):\n                break\n    if (not self._db_connections[i].is_new):\n        self.db_deleted_connections.append(self._db_connections[i])\n    for conn in self._db_connections[(i + 1):]:\n        self.db_connections_inverted_id_index[conn._db_id] -= 1\n    del self._db_connections[i]\n    del self.db_connections_id_index[connection._db_id]\n    del self.db_connections_inverted_id_index[connection._db_id]\n", "label": 0}
{"function": "\n\ndef get_module(app, modname, verbose, failfast):\n    '\\n    Internal function to load a module from a single app.\\n    '\n    module_name = ('%s.%s' % (app, modname))\n    app_mod = import_module(app)\n    try:\n        imp.find_module(modname, (app_mod.__path__ if hasattr(app_mod, '__path__') else None))\n    except ImportError:\n        if failfast:\n            raise\n        elif verbose:\n            print(('Could not find %r from %r' % (modname, app)))\n            traceback.print_exc()\n        return None\n    module = import_module(module_name)\n    if verbose:\n        print(('Loaded %r from %r' % (modname, app)))\n    return module\n", "label": 0}
{"function": "\n\ndef handle(self, **options):\n    database = options['database']\n    connection = connections[database]\n    verbosity = options['verbosity']\n    interactive = options['interactive']\n    reset_sequences = options.get('reset_sequences', True)\n    allow_cascade = options.get('allow_cascade', False)\n    inhibit_post_migrate = options.get('inhibit_post_migrate', False)\n    self.style = no_style()\n    for app_config in apps.get_app_configs():\n        try:\n            import_module('.management', app_config.name)\n        except ImportError:\n            pass\n    sql_list = sql_flush(self.style, connection, only_django=True, reset_sequences=reset_sequences, allow_cascade=allow_cascade)\n    if interactive:\n        confirm = input((\"You have requested a flush of the database.\\nThis will IRREVERSIBLY DESTROY all data currently in the %r database,\\nand return each table to an empty state.\\nAre you sure you want to do this?\\n\\n    Type 'yes' to continue, or 'no' to cancel: \" % connection.settings_dict['NAME']))\n    else:\n        confirm = 'yes'\n    if (confirm == 'yes'):\n        try:\n            with transaction.atomic(using=database, savepoint=connection.features.can_rollback_ddl):\n                with connection.cursor() as cursor:\n                    for sql in sql_list:\n                        cursor.execute(sql)\n        except Exception as e:\n            new_msg = (\"Database %s couldn't be flushed. Possible reasons:\\n  * The database isn't running or isn't configured correctly.\\n  * At least one of the expected database tables doesn't exist.\\n  * The SQL was invalid.\\nHint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.\\nThe full error: %s\" % (connection.settings_dict['NAME'], e))\n            six.reraise(CommandError, CommandError(new_msg), sys.exc_info()[2])\n        if (sql_list and (not inhibit_post_migrate)):\n            emit_post_migrate_signal(verbosity, interactive, database)\n    else:\n        self.stdout.write('Flush cancelled.\\n')\n", "label": 1}
{"function": "\n\n@handle_search_errors\ndef search_share_atom(**kwargs):\n    json_query = request.args.get('jsonQuery')\n    start = util.compute_start(request.args.get('page', 1), RESULTS_PER_PAGE)\n    if (not json_query):\n        q = request.args.get('q', '*')\n        sort = request.args.get('sort')\n        query = build_query(q, size=RESULTS_PER_PAGE, start=start, sort=sort)\n    else:\n        query = json.loads(unquote(json_query))\n        query['from'] = start\n        query['size'] = RESULTS_PER_PAGE\n        for field in ['aggs', 'aggregations']:\n            if query.get(field):\n                del query[field]\n        q = query\n    try:\n        search_results = search.search_share(query)\n    except MalformedQueryError:\n        raise HTTPError(http.BAD_REQUEST)\n    except IndexNotFoundError:\n        search_results = {\n            'count': 0,\n            'results': [],\n        }\n    atom_url = api_url_for('search_share_atom', _xml=True, _absolute=True)\n    return util.create_atom_feed(name='SHARE', data=search_results['results'], query=q, size=RESULTS_PER_PAGE, start=start, url=atom_url, to_atom=share_search.to_atom)\n", "label": 0}
{"function": "\n\ndef testMemory(self):\n    global logger\n    res = Memory(logger).check({\n        \n    })\n    if Platform.is_linux():\n        MEM_METRICS = ['swapTotal', 'swapFree', 'swapPctFree', 'swapUsed', 'physTotal', 'physFree', 'physUsed', 'physBuffers', 'physCached', 'physUsable', 'physPctUsable', 'physShared']\n        for k in MEM_METRICS:\n            if ((k == 'swapPctFree') and (res['swapTotal'] == 0)):\n                continue\n            assert (k in res), res\n        assert (res['swapTotal'] == (res['swapFree'] + res['swapUsed']))\n        assert (res['physTotal'] == (res['physFree'] + res['physUsed']))\n    elif (sys.platform == 'darwin'):\n        for k in ('swapFree', 'swapUsed', 'physFree', 'physUsed'):\n            assert (k in res), res\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    self.login_default_admin()\n    project = self.create_project()\n    plan = self.create_plan(project, label='Foo')\n    step = self.create_step(plan=plan)\n    self.login_default_admin()\n    path = '/api/0/steps/{0}/'.format(step.id.hex)\n    resp = self.client.post(path, data={\n        'order': 1,\n        'implementation': 'changes.buildsteps.dummy.DummyBuildStep',\n        'data': '{}',\n        'build.timeout': '1',\n    })\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (data['data'] == '{}')\n    assert (data['order'] == 1)\n    assert (data['implementation'] == 'changes.buildsteps.dummy.DummyBuildStep')\n    assert (data['options'] == {\n        'build.timeout': '1',\n    })\n    db.session.expire(step)\n    step = Step.query.get(step.id)\n    assert (step.data == {\n        \n    })\n    assert (step.order == 1)\n    assert (step.implementation == 'changes.buildsteps.dummy.DummyBuildStep')\n    options = list(ItemOption.query.filter((ItemOption.item_id == step.id)))\n    assert (len(options) == 1)\n    assert (options[0].name == 'build.timeout')\n    assert (options[0].value == '1')\n", "label": 1}
{"function": "\n\ndef rs_tan(p, x, prec):\n    \"\\n    Tangent of a series.\\n\\n    Return the series expansion of the tan of ``p``, about 0.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_tan\\n    >>> R, x, y = ring('x, y', QQ)\\n    >>> rs_tan(x + x*y, x, 4)\\n    1/3*x**3*y**3 + x**3*y**2 + x**3*y + 1/3*x**3 + x*y + x\\n\\n   See Also\\n   ========\\n\\n   _tan1, tan\\n   \"\n    if rs_is_puiseux(p, x):\n        r = rs_puiseux(rs_tan, p, x, prec)\n        return r\n    R = p.ring\n    const = 0\n    c = _get_constant_term(p, x)\n    if c:\n        if (R.domain is EX):\n            c_expr = c.as_expr()\n            const = tan(c_expr)\n        elif isinstance(c, PolyElement):\n            try:\n                c_expr = c.as_expr()\n                const = R(tan(c_expr))\n            except ValueError:\n                R = R.add_gens([tan(c_expr)])\n                p = p.set_ring(R)\n                x = x.set_ring(R)\n                c = c.set_ring(R)\n                const = R(tan(c_expr))\n        else:\n            try:\n                const = R(tan(c))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        p1 = (p - c)\n        t2 = rs_tan(p1, x, prec)\n        t = rs_series_inversion((1 - (const * t2)), x, prec)\n        return rs_mul((const + t2), t, x, prec)\n    if (R.ngens == 1):\n        return _tan1(p, x, prec)\n    else:\n        return rs_fun(p, rs_tan, x, prec)\n", "label": 1}
{"function": "\n\ndef test_get_customer_list(self):\n    CustomerFactory.create_batch(40)\n    url = reverse('customer-list')\n    response = self.client.get(url)\n    full_url = None\n    for field in response.data:\n        full_url = field.get('url', None)\n        if full_url:\n            break\n    if full_url:\n        domain = full_url.split('/')[2]\n        full_url = ((full_url.split(domain)[0] + domain) + url)\n    assert (response.status_code == status.HTTP_200_OK)\n    assert (response._headers['link'] == ('Link', (((((((('<' + full_url) + '?page=2; rel=\"next\">, ') + '<') + full_url) + '?page=1; rel=\"first\">, ') + '<') + full_url) + '?page=2; rel=\"last\">')))\n    response = self.client.get((url + '?page=2'))\n    assert (response.status_code == status.HTTP_200_OK)\n    assert (response._headers['link'] == ('Link', (((((((('<' + full_url) + '; rel=\"prev\">, ') + '<') + full_url) + '?page=1; rel=\"first\">, ') + '<') + full_url) + '?page=2; rel=\"last\">')))\n", "label": 1}
{"function": "\n\ndef test_cardinality_qk():\n    rules = {\n        'max_cardinality': 2,\n        'timeframe': datetime.timedelta(minutes=10),\n        'cardinality_field': 'foo',\n        'timestamp_field': '@timestamp',\n        'query_key': 'user',\n    }\n    rule = CardinalityRule(rules)\n    users = ['foo', 'bar', 'baz']\n    for user in users:\n        event = {\n            '@timestamp': datetime.datetime.now(),\n            'user': user,\n            'foo': ('foo' + user),\n        }\n        rule.add_data([event])\n        assert (len(rule.matches) == 0)\n    rule.garbage_collect(datetime.datetime.now())\n    values = ['faz', 'fuz', 'fiz']\n    for value in values:\n        event = {\n            '@timestamp': (datetime.datetime.now() + datetime.timedelta(minutes=5)),\n            'user': 'baz',\n            'foo': value,\n        }\n        rule.add_data([event])\n    rule.garbage_collect((datetime.datetime.now() + datetime.timedelta(minutes=5)))\n    assert (len(rule.matches) == 2)\n    assert (rule.matches[0]['user'] == 'baz')\n    assert (rule.matches[1]['user'] == 'baz')\n    assert (rule.matches[0]['foo'] == 'fuz')\n    assert (rule.matches[1]['foo'] == 'fiz')\n", "label": 1}
{"function": "\n\ndef pre_process_json(obj):\n    '\\n    Preprocess items in a dictionary or list and prepare them to be json serialized.\\n    '\n    if (type(obj) is dict):\n        new_dict = {\n            \n        }\n        for (key, value) in obj.items():\n            new_dict[key] = pre_process_json(value)\n        return new_dict\n    elif (type(obj) is list):\n        new_list = []\n        for item in obj:\n            new_list.append(pre_process_json(item))\n        return new_list\n    elif hasattr(obj, 'todict'):\n        return dict(obj.todict())\n    else:\n        try:\n            json.dumps(obj)\n        except TypeError:\n            try:\n                json.dumps(obj.__dict__)\n            except TypeError:\n                return str(obj)\n            else:\n                return obj.__dict__\n        else:\n            return obj\n", "label": 1}
{"function": "\n\ndef handle_save_embeds(sender, instance, **kwargs):\n    embedded_media_fields = FieldRegistry.get_fields(sender)\n    if (not embedded_media_fields):\n        return\n    urls = []\n    for field in instance._meta.fields:\n        if isinstance(field, TextField):\n            urls.extend(re.findall(URL_RE, getattr(instance, field.name)))\n    urls = set(urls)\n    for embedded_field in embedded_media_fields:\n        m2m = getattr(instance, embedded_field.name)\n        m2m.clear()\n        for url in urls:\n            try:\n                provider = oembed.site.provider_for_url(url)\n            except OEmbedMissingEndpoint:\n                pass\n            else:\n                if ((not embedded_field.media_type) or (provider.resource_type in embedded_field.media_type)):\n                    (media_obj, created) = AggregateMedia.objects.get_or_create(url=url)\n                    m2m.add(media_obj)\n", "label": 1}
{"function": "\n\ndef test_multi_explicit_fail():\n    'Model a failure of the explicit segment under concurrency.'\n    group = worker.WalTransferGroup(FakeWalUploader())\n    segments = list(prepare_multi_upload_segments())\n    exp = Explosion('fail')\n    segments[0]._upload_explosive = exp\n    for seg in segments:\n        group.start(seg)\n    with pytest.raises(Explosion) as e:\n        group.join()\n    assert (e.value is exp)\n    assert failed(segments[0])\n    for seg in segments[1:]:\n        assert success(seg)\n", "label": 0}
{"function": "\n\ndef get_ordering(self, request, queryset, view):\n    '\\n        Return a tuple of strings, that may be used in an `order_by` method.\\n        '\n    ordering_filters = [filter_cls for filter_cls in getattr(view, 'filter_backends', []) if hasattr(filter_cls, 'get_ordering')]\n    if ordering_filters:\n        filter_cls = ordering_filters[0]\n        filter_instance = filter_cls()\n        ordering = filter_instance.get_ordering(request, queryset, view)\n        assert (ordering is not None), 'Using cursor pagination, but filter class {filter_cls} returned a `None` ordering.'.format(filter_cls=filter_cls.__name__)\n    else:\n        ordering = self.ordering\n        assert (ordering is not None), 'Using cursor pagination, but no ordering attribute was declared on the pagination class.'\n        assert ('__' not in ordering), 'Cursor pagination does not support double underscore lookups for orderings. Orderings should be an unchanging, unique or nearly-unique field on the model, such as \"-created\" or \"pk\".'\n    assert isinstance(ordering, (six.string_types, list, tuple)), 'Invalid ordering. Expected string or tuple, but got {type}'.format(type=type(ordering).__name__)\n    if isinstance(ordering, six.string_types):\n        return (ordering,)\n    return tuple(ordering)\n", "label": 1}
{"function": "\n\ndef test_zip_folder_content():\n    extension_file = 'src/olympia/files/fixtures/files/extension.xpi'\n    (temp_filename, temp_folder) = (None, None)\n    try:\n        temp_folder = utils.extract_zip(extension_file)\n        assert (sorted(os.listdir(temp_folder)) == ['chrome', 'chrome.manifest', 'install.rdf'])\n        temp_filename = amo.tests.get_temp_filename()\n        utils.zip_folder_content(temp_folder, temp_filename)\n        with zipfile.ZipFile(temp_filename, mode='r') as new:\n            with zipfile.ZipFile(extension_file, mode='r') as orig:\n                assert (sorted(new.namelist()) == sorted(orig.namelist()))\n    finally:\n        if ((temp_folder is not None) and os.path.exists(temp_folder)):\n            amo.utils.rm_local_tmp_dir(temp_folder)\n        if ((temp_filename is not None) and os.path.exists(temp_filename)):\n            os.unlink(temp_filename)\n", "label": 0}
{"function": "\n\ndef _visit_goal(self, goal, context, goal_info_by_goal, target_roots_replacement):\n    if (goal in goal_info_by_goal):\n        return\n    tasktypes_by_name = OrderedDict()\n    goal_dependencies = set()\n    visited_task_types = set()\n    for task_name in reversed(goal.ordered_task_names()):\n        task_type = goal.task_type_by_name(task_name)\n        tasktypes_by_name[task_name] = task_type\n        visited_task_types.add(task_type)\n        alternate_target_roots = task_type._alternate_target_roots(context.options, context.address_mapper, context.build_graph)\n        target_roots_replacement.propose_alternates(task_type, alternate_target_roots)\n        round_manager = RoundManager(context)\n        task_type._prepare(context.options, round_manager)\n        try:\n            dependencies = round_manager.get_dependencies()\n            for producer_info in dependencies:\n                producer_goal = producer_info.goal\n                if (producer_goal == goal):\n                    if (producer_info.task_type == task_type):\n                        pass\n                    elif (producer_info.task_type in visited_task_types):\n                        ordering = '\\n\\t'.join((\"[{0}] '{1}' {2}\".format(i, tn, goal.task_type_by_name(tn).__name__) for (i, tn) in enumerate(goal.ordered_task_names())))\n                        raise self.TaskOrderError(\"TaskRegistrar '{name}' with action {consumer_task} depends on {data} from task {producer_task} which is ordered after it in the '{goal}' goal:\\n\\t{ordering}\".format(name=task_name, consumer_task=task_type.__name__, data=producer_info.product_type, producer_task=producer_info.task_type.__name__, goal=goal.name, ordering=ordering))\n                    else:\n                        pass\n                else:\n                    goal_dependencies.add(producer_goal)\n        except round_manager.MissingProductError as e:\n            raise self.MissingProductError(\"Could not satisfy data dependencies for goal '{name}' with action {action}: {error}\".format(name=task_name, action=task_type.__name__, error=e))\n    goal_info = self.GoalInfo(goal, tasktypes_by_name, goal_dependencies)\n    goal_info_by_goal[goal] = goal_info\n    for goal_dependency in goal_dependencies:\n        self._visit_goal(goal_dependency, context, goal_info_by_goal, target_roots_replacement)\n", "label": 1}
{"function": "\n\ndef put(self, key, value, cas=None, flags=None, acquire=None, release=None, token=None, dc=None):\n    \"\\n            Sets *key* to the given *value*.\\n\\n            *value* can either be None (useful for marking a key as a\\n            directory) or any string type, including binary data (e.g. a\\n            msgpack'd data structure)\\n\\n            The optional *cas* parameter is used to turn the PUT into a\\n            Check-And-Set operation. This is very useful as it allows clients\\n            to build more complex syncronization primitives on top. If the\\n            index is 0, then Consul will only put the key if it does not\\n            already exist. If the index is non-zero, then the key is only set\\n            if the index matches the ModifyIndex of that key.\\n\\n            An optional *flags* can be set. This can be used to specify an\\n            unsigned value between 0 and 2^64-1.\\n\\n            *acquire* is an optional session_id. if supplied a lock acquisition\\n            will be attempted.\\n\\n            *release* is an optional session_id. if supplied a lock release\\n            will be attempted.\\n\\n            *token* is an optional `ACL token`_ to apply to this request. If\\n            the token's policy is not allowed to write to this key an\\n            *ACLPermissionDenied* exception will be raised.\\n\\n            *dc* is the optional datacenter that you wish to communicate with.\\n            If None is provided, defaults to the agent's datacenter.\\n\\n            The return value is simply either True or False. If False is\\n            returned, then the update has not taken place.\\n            \"\n    assert (not key.startswith('/')), 'keys should not start with a forward slash'\n    assert ((value is None) or isinstance(value, (six.string_types, six.binary_type))), 'value should be None or a string / binary data'\n    params = {\n        \n    }\n    if (cas is not None):\n        params['cas'] = cas\n    if (flags is not None):\n        params['flags'] = flags\n    if acquire:\n        params['acquire'] = acquire\n    if release:\n        params['release'] = release\n    token = (token or self.agent.token)\n    if token:\n        params['token'] = token\n    dc = (dc or self.agent.dc)\n    if dc:\n        params['dc'] = dc\n    return self.agent.http.put(callback(is_json=True), ('/v1/kv/%s' % key), params=params, data=value)\n", "label": 1}
{"function": "\n\ndef encode(string, encodings=None):\n    if ((not PY2) and isinstance(string, bytes)):\n        return string\n    if (PY2 and isinstance(string, str)):\n        return string\n    if (encodings is None):\n        encodings = ['utf-8', 'latin1', 'ascii']\n    for encoding in encodings:\n        try:\n            return string.encode(encoding)\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            pass\n    return string.encode(encodings[0], errors='ignore')\n", "label": 1}
{"function": "\n\ndef template_source(request):\n    \"\\n    Return the source of a template, syntax-highlighted by Pygments if\\n    it's available.\\n    \"\n    template_name = request.GET.get('template', None)\n    if (template_name is None):\n        return HttpResponseBadRequest('\"template\" key is required')\n    final_loaders = []\n    loaders = get_template_loaders()\n    for loader in loaders:\n        if (loader is not None):\n            if hasattr(loader, 'loaders'):\n                final_loaders += loader.loaders\n            else:\n                final_loaders.append(loader)\n    for loader in final_loaders:\n        try:\n            (source, display_name) = loader.load_template_source(template_name)\n            break\n        except TemplateDoesNotExist:\n            source = ('Template Does Not Exist: %s' % (template_name,))\n    try:\n        from pygments import highlight\n        from pygments.lexers import HtmlDjangoLexer\n        from pygments.formatters import HtmlFormatter\n        source = highlight(source, HtmlDjangoLexer(), HtmlFormatter())\n        source = mark_safe(source)\n        source.pygmentized = True\n    except ImportError:\n        pass\n    return render_to_response('debug_toolbar/panels/template_source.html', {\n        'source': source,\n        'template_name': template_name,\n    })\n", "label": 1}
{"function": "\n\ndef render(self, context):\n    'Output the content of the `PlaceholdeNode` as a template.'\n    content = self.get_render_content(context)\n    if (not content):\n        return ''\n    if self.parsed:\n        try:\n            t = template.Template(content, name=self.name)\n            content = mark_safe(t.render(context))\n        except TemplateSyntaxError as error:\n            if global_settings.DEBUG:\n                content = (PLACEHOLDER_ERROR % {\n                    'name': self.name,\n                    'error': error,\n                })\n            else:\n                content = ''\n    if (self.as_varname is None):\n        return content\n    context[self.as_varname] = content\n    return ''\n", "label": 0}
{"function": "\n\ndef _forward(self, src, dst, direction):\n    assert (direction in ('w', 'r'))\n    while True:\n        try:\n            data = src.recv(self.chunk_size)\n        except socket.error as e:\n            if (e.args and (e.args[0] == 'timed out')):\n                self.logger.debug(('%s TIMEOUT' % direction))\n                return\n        if (not data):\n            self.logger.debug(('%s EOF' % direction))\n            return\n        self.logger.debug(('%s %r bytes' % (direction, len(data))))\n        dst.sendall(data)\n", "label": 0}
{"function": "\n\ndef execute_sql(self, sql, params=None, require_commit=True, named_cursor=False):\n    logger.debug((sql, params))\n    use_named_cursor = (named_cursor or (self.server_side_cursors and sql.lower().startswith('select')))\n    with self.exception_wrapper():\n        if use_named_cursor:\n            cursor = self.get_cursor(name=str(uuid.uuid1()))\n            require_commit = False\n        else:\n            cursor = self.get_cursor()\n        try:\n            cursor.execute(sql, (params or ()))\n        except Exception as exc:\n            if (self.get_autocommit() and self.autorollback):\n                self.rollback()\n            raise\n        else:\n            if (require_commit and self.get_autocommit()):\n                self.commit()\n    return cursor\n", "label": 1}
{"function": "\n\ndef fsub(ctx, x, y, **kwargs):\n    \"\\n        Subtracts the numbers *x* and *y*, giving a floating-point result,\\n        optionally using a custom precision and rounding mode.\\n\\n        See the documentation of :func:`~mpmath.fadd` for a detailed description\\n        of how to specify precision and rounding.\\n\\n        **Examples**\\n\\n        Using :func:`~mpmath.fsub` with precision and rounding control::\\n\\n            >>> from mpmath import *\\n            >>> mp.dps = 15; mp.pretty = False\\n            >>> fsub(2, 1e-20)\\n            mpf('2.0')\\n            >>> fsub(2, 1e-20, rounding='d')\\n            mpf('1.9999999999999998')\\n            >>> nprint(fsub(2, 1e-20, prec=100), 25)\\n            1.99999999999999999999\\n            >>> nprint(fsub(2, 1e-20, dps=15), 25)\\n            2.0\\n            >>> nprint(fsub(2, 1e-20, dps=25), 25)\\n            1.99999999999999999999\\n            >>> nprint(fsub(2, 1e-20, exact=True), 25)\\n            1.99999999999999999999\\n\\n        Exact subtraction avoids cancellation errors, enforcing familiar laws\\n        of numbers such as `x-y+y = x`, which don't hold in floating-point\\n        arithmetic with finite precision::\\n\\n            >>> x, y = mpf(2), mpf('1e1000')\\n            >>> print(x - y + y)\\n            0.0\\n            >>> print(fsub(x, y, prec=inf) + y)\\n            2.0\\n            >>> print(fsub(x, y, exact=True) + y)\\n            2.0\\n\\n        Exact addition can be inefficient and may be impossible to perform\\n        with large magnitude differences::\\n\\n            >>> fsub(1, '1e-100000000000000000000', prec=inf)\\n            Traceback (most recent call last):\\n              ...\\n            OverflowError: the exact result does not fit in memory\\n\\n        \"\n    (prec, rounding) = ctx._parse_prec(kwargs)\n    x = ctx.convert(x)\n    y = ctx.convert(y)\n    try:\n        if hasattr(x, '_mpf_'):\n            if hasattr(y, '_mpf_'):\n                return ctx.make_mpf(mpf_sub(x._mpf_, y._mpf_, prec, rounding))\n            if hasattr(y, '_mpc_'):\n                return ctx.make_mpc(mpc_sub((x._mpf_, fzero), y._mpc_, prec, rounding))\n        if hasattr(x, '_mpc_'):\n            if hasattr(y, '_mpf_'):\n                return ctx.make_mpc(mpc_sub_mpf(x._mpc_, y._mpf_, prec, rounding))\n            if hasattr(y, '_mpc_'):\n                return ctx.make_mpc(mpc_sub(x._mpc_, y._mpc_, prec, rounding))\n    except (ValueError, OverflowError):\n        raise OverflowError(ctx._exact_overflow_msg)\n    raise ValueError('Arguments need to be mpf or mpc compatible numbers')\n", "label": 1}
{"function": "\n\ndef generate_global_orchestrate_file(self):\n    accounts = set([host.cloud_image.account for host in self.hosts.all()])\n    orchestrate = {\n        \n    }\n    for account in accounts:\n        target = 'G@stack_id:{0} and G@cloud_account:{1}'.format(self.id, account.slug)\n        groups = {\n            \n        }\n        for component in account.formula_components.all():\n            groups.setdefault(component.order, set()).add(component.sls_path)\n        for order in sorted(groups.keys()):\n            for role in groups[order]:\n                state_title = '{0}_{1}'.format(account.slug, role)\n                orchestrate[state_title] = {\n                    'salt.state': [{\n                        'tgt': target,\n                    }, {\n                        'tgt_type': 'compound',\n                    }, {\n                        'sls': role,\n                    }],\n                }\n                depend = (order - 1)\n                while (depend >= 0):\n                    if (depend in groups.keys()):\n                        orchestrate[role]['salt.state'].append({\n                            'require': [{\n                                'salt': req,\n                            } for req in groups[depend]],\n                        })\n                        break\n                    depend -= 1\n    yaml_data = yaml.safe_dump(orchestrate, default_flow_style=False)\n    if (not self.global_orchestrate_file):\n        self.global_orchestrate_file.save('global_orchestrate.sls', ContentFile(yaml_data))\n    else:\n        with open(self.global_orchestrate_file.path, 'w') as f:\n            f.write(yaml_data)\n", "label": 1}
{"function": "\n\ndef get_results(self, request):\n    paginator = self.model_admin.get_paginator(request, self.queryset, self.list_per_page)\n    result_count = paginator.count\n    if self.model_admin.show_full_result_count:\n        full_result_count = self.root_queryset.count()\n    else:\n        full_result_count = None\n    can_show_all = (result_count <= self.list_max_show_all)\n    multi_page = (result_count > self.list_per_page)\n    if ((self.show_all and can_show_all) or (not multi_page)):\n        result_list = self.queryset._clone()\n    else:\n        try:\n            result_list = paginator.page((self.page_num + 1)).object_list\n        except InvalidPage:\n            raise IncorrectLookupParameters\n    self.result_count = result_count\n    self.show_full_result_count = self.model_admin.show_full_result_count\n    self.show_admin_actions = ((not self.show_full_result_count) or bool(full_result_count))\n    self.full_result_count = full_result_count\n    self.result_list = result_list\n    self.can_show_all = can_show_all\n    self.multi_page = multi_page\n    self.paginator = paginator\n", "label": 0}
{"function": "\n\ndef max_pool_c01b(c01b, pool_shape, pool_stride, image_shape=None, start=0):\n    '\\n    .. todo::\\n\\n        WRITEME\\n    '\n    assert (pool_shape[0] == pool_shape[1])\n    assert (pool_shape[0] > 0)\n    assert (pool_stride[0] > 0)\n    assert (pool_stride[0] <= pool_shape[0])\n    if (pool_stride[0] != pool_stride[1]):\n        raise ValueError(('pool strides must match, but got ' + str(pool_stride)))\n    if (image_shape is not None):\n        warnings.warn(\"image_shape argument isn't needed anymore, quit passing it.\")\n    op = MaxPool(pool_shape[0], pool_stride[0], start)\n    c01b = gpu_contiguous(c01b)\n    return op(c01b)\n", "label": 0}
{"function": "\n\ndef _patch_dict(self):\n    'Unpatch the dict.'\n    values = self.values\n    in_dict = self.in_dict\n    clear = self.clear\n    try:\n        original = in_dict.copy()\n    except AttributeError:\n        original = {\n            \n        }\n        for key in in_dict:\n            original[key] = in_dict[key]\n    self._original = original\n    if clear:\n        _clear_dict(in_dict)\n    try:\n        in_dict.update(values)\n    except AttributeError:\n        for key in values:\n            in_dict[key] = values[key]\n", "label": 0}
{"function": "\n\ndef assert_attr_equal(attr, left, right, obj='Attributes'):\n    \"checks attributes are equal. Both objects must have attribute.\\n\\n    Parameters\\n    ----------\\n    attr : str\\n        Attribute name being compared.\\n    left : object\\n    right : object\\n    obj : str, default 'Attributes'\\n        Specify object name being compared, internally used to show appropriate\\n        assertion message\\n    \"\n    left_attr = getattr(left, attr)\n    right_attr = getattr(right, attr)\n    if (left_attr is right_attr):\n        return True\n    elif (is_number(left_attr) and np.isnan(left_attr) and is_number(right_attr) and np.isnan(right_attr)):\n        return True\n    try:\n        result = (left_attr == right_attr)\n    except TypeError:\n        result = False\n    if (not isinstance(result, bool)):\n        result = result.all()\n    if result:\n        return True\n    else:\n        raise_assert_detail(obj, 'Attribute \"{0}\" are different'.format(attr), left_attr, right_attr)\n", "label": 1}
{"function": "\n\ndef test_contains(session):\n    set_ = session.set(key('test_sortedset_contains'), S('abc'), SortedSet)\n    assert ('a' in set_)\n    assert ('d' not in set_)\n    setx = session.set(key('test_sortedsetx_contains'), S([1, 2, 3]), IntSet)\n    assert (1 in setx)\n    assert (4 not in setx)\n    assert ('1' not in setx)\n    assert ('4' not in setx)\n", "label": 0}
{"function": "\n\ndef _post_handle_entry_debug(self, entry, successors, _locals):\n    '\\n\\n        :return:\\n        '\n    simrun = _locals['simrun']\n    call_stack_suffix = _locals['call_stack_suffix']\n    extra_info = _locals['extra_info']\n    successor_status = _locals['successor_status']\n    function_name = self.project.loader.find_symbol_name(simrun.addr)\n    module_name = self.project.loader.find_module_name(simrun.addr)\n    l.debug('Basic block %s %s', simrun, '->'.join([hex(i) for i in call_stack_suffix if (i is not None)]))\n    l.debug('(Function %s of binary %s)', function_name, module_name)\n    l.debug('|    Call jump: %s', extra_info['is_call_jump'])\n    for suc in successors:\n        jumpkind = suc.scratch.jumpkind\n        if (jumpkind == 'Ijk_FakeRet'):\n            exit_type_str = 'Simulated Ret'\n        else:\n            exit_type_str = '-'\n        try:\n            l.debug('|    target: %#x %s [%s] %s', suc.se.exactly_int(suc.ip), successor_status[suc], exit_type_str, jumpkind)\n        except (simuvex.SimValueError, simuvex.SimSolverModeError):\n            l.debug('|    target cannot be concretized. %s [%s] %s', successor_status[suc], exit_type_str, jumpkind)\n    l.debug('%d exits remaining, %d exits pending.', len(self._entries), len(self._pending_entries))\n    l.debug('%d unique basic blocks are analyzed so far.', len(self._analyzed_addrs))\n", "label": 0}
{"function": "\n\ndef config_file_opt(self, path2json):\n    if os.path.exists(path2json):\n        with codecs.open(opt.config, 'r', encoding='ascii') as f:\n            json_con = json_loads(f.read())\n        self.server = (self.server or json_con.get('server', None))\n        self.password = (self.password or json_con.get('password', None))\n        self.server_port = (self.server_port or json_con.get('server_port', None))\n        self.local_port = (self.local_port or json_con.get('local_port', None))\n        self.method = (self.method or json_con.get('method', 'table'))\n        self.timeout = (self.timeout or json_con.get('timeout', None))\n        self.debug = (self.debug or json_con.get('debug', False))\n    else:\n        logger.warning(('the json file path `%s` is not exists' % path2json))\n", "label": 1}
{"function": "\n\ndef send_file(self, reqfile, content_type, cached=False):\n    reqfile = join(self.dir, reqfile)\n    if (not self.is_subpath(reqfile, self.dir)):\n        return self.render_no_access()\n    if ((not exists(reqfile)) or (not access(reqfile, os.R_OK))):\n        return self.render_not_found()\n    self.status = '200'\n    self.headers['Content-Type'] = content_type\n    self.headers['Last-Modified'] = format_date_time(getmtime(reqfile))\n    if cached:\n        self.hdr_cache_forenver()\n    else:\n        self.hdr_nocache()\n    size = getsize(reqfile)\n    if size:\n        self.headers['Content-Length'] = size\n\n        def read_file():\n            with open(reqfile, 'rb') as f:\n                while True:\n                    part = f.read(8192)\n                    if (not part):\n                        break\n                    (yield part)\n        return read_file()\n    else:\n        with open(reqfile, 'rb') as f:\n            part = f.read()\n            self.headers['Content-Length'] = str(len(part))\n        return [part]\n", "label": 0}
{"function": "\n\ndef get_content(self, context=None):\n    ' Does all context binding and pathing to get content, templated out '\n    if self.is_file:\n        path = self.content\n        if (self.is_template_path and context):\n            path = string.Template(path).safe_substitute(context.get_values())\n        data = None\n        with open(path, 'r') as f:\n            data = f.read()\n        if (self.is_template_content and context):\n            return string.Template(data).safe_substitute(context.get_values())\n        else:\n            return data\n    elif (self.is_template_content and context):\n        return safe_substitute_unicode_template(self.content, context.get_values())\n    else:\n        return self.content\n", "label": 1}
{"function": "\n\ndef __call__(self):\n    request = current.request\n    response = current.response\n    view = path.join(request.folder, 'modules', 'templates', THEME, 'views', 'index.html')\n    try:\n        response.view = open(view, 'rb')\n    except IOError:\n        from gluon.http import HTTP\n        raise HTTP(404, ('Unable to open Custom View: %s' % view))\n    if current.auth.is_logged_in():\n        grid = 'grid_12'\n    else:\n        grid = 'grid_8'\n    latest_projects = DIV(_id='front-latest-body', _class=('%s alpha' % grid))\n    lappend = latest_projects.append\n    db = current.db\n    s3db = current.s3db\n    table = s3db.project_project\n    table_drrpp = s3db.project_drrpp\n    query = ((table.deleted != True) & (table.approved_by != None))\n    rows = db(query).select(table.id, table.name, table_drrpp.activities, table.organisation_id, table.start_date, left=table_drrpp.on((table.id == table_drrpp.project_id)), limitby=(0, 3))\n    project_ids = [r.project_project.id for r in rows]\n    ltable = s3db.project_location\n    gtable = s3db.gis_location\n    query = ((((ltable.deleted != True) & (ltable.project_id == table.id)) & (gtable.id == ltable.location_id)) & (gtable.level == 'L0'))\n    locations = db(query).select(ltable.project_id, gtable.L0)\n    odd = True\n    for row in rows:\n        countries = [l.gis_location.L0 for l in locations if (l.project_location.project_id == row.project_project.id)]\n        location = ', '.join(countries)\n        if odd:\n            _class = ('front-latest-item odd %s alpha' % grid)\n        else:\n            _class = ('front-latest-item even %s alpha' % grid)\n        card = DIV(DIV(A(row.project_project.name, _href=URL(c='project', f='project', args=[row.project_project.id])), _class=('front-latest-title %s' % grid)), DIV(('Lead Organization: %s' % s3db.org_organisation_represent(row.project_project.organisation_id)), _class=('front-latest-desc %s' % grid)), DIV(SPAN(('Start Date: %s' % row.project_project.start_date), _class='front-latest-info-date'), SPAN(('Countries: %s' % location), _class='front-latest-info-location'), _class=('front-latest-info %s' % grid)), DIV((row.project_drrpp.activities or ''), _class=('front-latest-desc %s' % grid)), _class=_class)\n        lappend(card)\n        odd = (False if odd else True)\n    login = current.auth.login(inline=True)\n    appname = request.application\n    s3 = response.s3\n    if current.session.s3.debug:\n        s3.scripts.append(('/%s/static/themes/DRRPP/js/slides.jquery.js' % appname))\n    else:\n        s3.scripts.append(('/%s/static/themes/DRRPP/js/slides.min.jquery.js' % appname))\n    s3.jquery_ready.append(\"\\n$('#slides').slides({\\n play:8000,\\n animationStart:function(current){\\n  $('.caption').animate({\\n   bottom:-35\\n  },100);\\n },\\n animationComplete:function(current){\\n  $('.caption').animate({\\n   bottom:0\\n  },200);\\n },\\n slidesLoaded:function() {\\n  $('.caption').animate({\\n   bottom:0\\n  },200);\\n }\\n})\")\n    return dict(title='Home', form=login, latest_projects=latest_projects)\n", "label": 1}
{"function": "\n\ndef test_sparsity_cache_hit(self, backend, base_set, base_map):\n    dsets = (base_set, base_set)\n    maps = (base_map, base_map)\n    sp = op2.Sparsity(dsets, maps)\n    sp2 = op2.Sparsity(dsets, maps)\n    assert (sp is sp2)\n    assert (not (sp != sp2))\n    assert (sp == sp2)\n    dsets = op2.MixedSet([base_set, base_set])\n    maps = op2.MixedMap([base_map, base_map])\n    sp = op2.Sparsity(dsets, maps)\n    dsets2 = op2.MixedSet([base_set, base_set])\n    maps2 = op2.MixedMap([base_map, base_map])\n    sp2 = op2.Sparsity(dsets2, maps2)\n    assert (sp is sp2)\n    assert (not (sp != sp2))\n    assert (sp == sp2)\n", "label": 0}
{"function": "\n\ndef test_simple(self):\n    fake_author_id = uuid4()\n    project = self.create_project()\n    self.create_build(project)\n    path = '/api/0/authors/{0}/builds/'.format(fake_author_id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 404)\n    data = self.unserialize(resp)\n    assert (len(data) == 0)\n    author = Author(email=self.default_user.email, name='Foo Bar')\n    db.session.add(author)\n    build = self.create_build(project, author=author)\n    path = '/api/0/authors/{0}/builds/'.format(author.id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 401)\n    self.login(self.default_user)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    (username, domain) = self.default_user.email.split('@', 1)\n    author = self.create_author('{}+foo@{}'.format(username, domain))\n    self.create_build(project, author=author)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 2)\n", "label": 1}
{"function": "\n\ndef Cleanup(benchmark_spec):\n    'Cleans up SPEC CPU2006 from the target vm.\\n\\n  Args:\\n    benchmark_spec: The benchmark specification. Contains all data that is\\n        required to run the benchmark.\\n  '\n    vm = benchmark_spec.vms[0]\n    speccpu_vm_state = getattr(vm, _BENCHMARK_SPECIFIC_VM_STATE_ATTR, None)\n    if speccpu_vm_state:\n        if speccpu_vm_state.mount_dir:\n            try:\n                vm.RemoteCommand('sudo umount {0}'.format(speccpu_vm_state.mount_dir))\n            except errors.VirtualMachine.RemoteCommandError:\n                logging.exception('umount failed.')\n        targets = ' '.join((p for p in speccpu_vm_state.__dict__.values() if p))\n        vm.RemoteCommand('rm -rf {0}'.format(targets))\n", "label": 0}
{"function": "\n\n@require_POST\n@csrf_exempt\n@cross_domain_post_response\n@host_check\ndef dislike_comment(request, comment_id):\n    try:\n        comment = Comment.objects.get(id=comment_id)\n    except Comment.DoesNotExist as e:\n        data = {\n            'placement': 'comments_container',\n            'content': str(e),\n        }\n        return HttpResponseNotFound(json.dumps(data))\n    if request.user.is_anonymous():\n        site_admin = False\n    else:\n        site_admin = bool(request.user.get_comments(comment_id))\n    if site_admin:\n        if request.user.hidden.filter(id=comment.thread.site.id):\n            return HttpResponseBadRequest((_('user is disabled on site with id %s') % comment.thread.site.id))\n    liked_comments = request.session.get('liked_comments', [])\n    disliked_comments = request.session.get('disliked_comments', [])\n    if (comment.id in liked_comments):\n        comment.undo_like(request.user)\n        remove_from_session_list(request, 'liked_comments', comment.id)\n        comment.dislike(request.user)\n        add_to_session_list(request, 'disliked_comments', comment.id)\n    elif (comment.id not in disliked_comments):\n        comment.dislike(request.user)\n        add_to_session_list(request, 'disliked_comments', comment.id)\n    comments = Comment.objects.filter(thread=comment.thread)\n    posted_comments = request.session.get('posted_comments', [])\n    site = comment.thread.site\n    resp = render(request, 'comments.html', {\n        'comments': comments,\n        'posted_comments': posted_comments,\n        'last_posted_comment_id': (posted_comments[(- 1)] if posted_comments else None),\n        'rs_customer_id': site.rs_customer_id,\n        'all_comments': request.session.get('all_comments', False),\n        'site_admin': site_admin,\n    })\n    data = {\n        'placement': 'comments_container',\n        'content': resp.content,\n    }\n    return HttpResponse(json.dumps(data))\n", "label": 1}
{"function": "\n\ndef _setQuotaDefault(self, model, value, testVal='__NOCHECK__', error=None):\n    \"\\n        Set the default quota for a particular model.\\n\\n        :param model: either 'user' or 'collection'.\\n        :param value: the value to set.  Either None or a positive integer.\\n        :param testVal: if not __NOCHECK__, test the current value to see if it\\n                        matches this.\\n        :param error: if set, this is a substring expected in an error message.\\n        \"\n    from girder.plugins.user_quota import constants\n    if (model == 'user'):\n        key = constants.PluginSettings.QUOTA_DEFAULT_USER_QUOTA\n    elif (model == 'collection'):\n        key = constants.PluginSettings.QUOTA_DEFAULT_COLLECTION_QUOTA\n    try:\n        self.model('setting').set(key, value)\n    except ValidationException as err:\n        if (not error):\n            raise\n        if (error not in err.args[0]):\n            raise\n        return\n    if (testVal is not '__NOCHECK__'):\n        newVal = self.model('setting').get(key)\n        self.assertEqual(newVal, testVal)\n", "label": 0}
{"function": "\n\ndef _read_object_from_repo(self, rev=None, relpath=None, sha=None):\n    'Read an object from the git repo.\\n    This is implemented via a pipe to git cat-file --batch\\n    '\n    if sha:\n        spec = (sha + '\\n')\n    else:\n        assert (rev is not None)\n        assert (relpath is not None)\n        relpath = self._fixup_dot_relative(relpath)\n        spec = '{}:{}\\n'.format(rev, relpath)\n    self._maybe_start_cat_file_process()\n    self._cat_file_process.stdin.write(spec)\n    self._cat_file_process.stdin.flush()\n    header = None\n    while (not header):\n        header = self._cat_file_process.stdout.readline()\n        if (self._cat_file_process.poll() is not None):\n            raise self.GitDiedException(\"Git cat-file died while trying to read '{}'.\".format(spec))\n    header = header.rstrip()\n    parts = header.rsplit(SPACE, 2)\n    if (len(parts) == 2):\n        assert (parts[1] == 'missing')\n        raise self.MissingFileException(rev, relpath)\n    (_, object_type, object_len) = parts\n    blob = self._cat_file_process.stdout.read(int(object_len))\n    assert (self._cat_file_process.stdout.read(1) == '\\n')\n    assert (len(blob) == int(object_len))\n    return (object_type, blob)\n", "label": 1}
{"function": "\n\n@messaging.expected_exceptions(exception.ConsoleTypeInvalid, exception.InstanceNotReady, exception.InstanceNotFound, exception.ConsoleTypeUnavailable, NotImplementedError)\n@wrap_exception()\n@wrap_instance_fault\ndef get_vnc_console(self, context, console_type, instance):\n    'Return connection information for a vnc console.'\n    context = context.elevated()\n    LOG.debug('Getting vnc console', instance=instance)\n    token = str(uuid.uuid4())\n    if (not CONF.vnc.enabled):\n        raise exception.ConsoleTypeUnavailable(console_type=console_type)\n    if (console_type == 'novnc'):\n        access_url = ('%s?token=%s' % (CONF.vnc.novncproxy_base_url, token))\n    elif (console_type == 'xvpvnc'):\n        access_url = ('%s?token=%s' % (CONF.vnc.xvpvncproxy_base_url, token))\n    else:\n        raise exception.ConsoleTypeInvalid(console_type=console_type)\n    try:\n        console = self.driver.get_vnc_console(context, instance)\n        connect_info = console.get_connection_info(token, access_url)\n    except exception.InstanceNotFound:\n        if (instance.vm_state != vm_states.BUILDING):\n            raise\n        raise exception.InstanceNotReady(instance_id=instance.uuid)\n    return connect_info\n", "label": 0}
{"function": "\n\ndef test_imread():\n    with random_images(4, (5, 6, 3)) as globstring:\n        im = da_imread(globstring)\n        assert (im.shape == (4, 5, 6, 3))\n        assert (im.chunks == ((1, 1, 1, 1), (5,), (6,), (3,)))\n        assert (im.dtype == 'uint8')\n        assert (im.compute().shape == (4, 5, 6, 3))\n        assert (im.compute().dtype == 'uint8')\n", "label": 0}
{"function": "\n\ndef _get_python_variables(python_exe, variables, imports=['import sys']):\n    'Run a python interpreter and print some variables'\n    program = list(imports)\n    program.append('')\n    for v in variables:\n        program.append(('print(repr(%s))' % v))\n    os_env = dict(os.environ)\n    try:\n        del os_env['MACOSX_DEPLOYMENT_TARGET']\n    except KeyError:\n        pass\n    proc = Utils.pproc.Popen([python_exe, '-c', '\\n'.join(program)], stdout=Utils.pproc.PIPE, env=os_env)\n    output = proc.communicate()[0].split('\\n')\n    if proc.returncode:\n        if Options.options.verbose:\n            warn(('Python program to extract python configuration variables failed:\\n%s' % '\\n'.join([('line %03i: %s' % ((lineno + 1), line)) for (lineno, line) in enumerate(program)])))\n        raise RuntimeError\n    return_values = []\n    for s in output:\n        s = s.strip()\n        if (not s):\n            continue\n        if (s == 'None'):\n            return_values.append(None)\n        elif ((s[0] == \"'\") and (s[(- 1)] == \"'\")):\n            return_values.append(s[1:(- 1)])\n        elif s[0].isdigit():\n            return_values.append(int(s))\n        else:\n            break\n    return return_values\n", "label": 1}
{"function": "\n\ndef read_results(results_fpath, lamb):\n    last = {\n        \n    }\n    classes = {\n        \n    }\n    objs = set()\n    users = set()\n    joint = defaultdict((lambda : defaultdict(int)))\n    glob = defaultdict((lambda : defaultdict(int)))\n    with open(results_fpath) as results_file:\n        results_file.readline()\n        for line in results_file:\n            spl = line.strip().split()\n            user = spl[0]\n            class_ = spl[1]\n            last_stage = spl[(- 1)].split('-')[(- 1)]\n            last[user] = last_stage\n            classes[user] = class_\n            for item in spl[2:]:\n                (obj, stage) = item.split('-')\n                joint[class_][(stage, obj)] += 1\n                glob[class_][stage] += 1\n                objs.add(obj)\n            users.add(user)\n    probs = {\n        \n    }\n    for u in last:\n        probs[u] = {\n            \n        }\n        class_ = classes[u]\n        for o in objs:\n            probs[u][o] = ((lamb + joint[class_][(last[u], o)]) / ((len(objs) * lamb) + glob[class_][last[u]]))\n    for u in probs:\n        sum_ = sum(probs[u].values())\n        for o in objs:\n            probs[u][o] = ((probs[u][o] / sum_) if (sum_ > 0) else 0)\n    sorted_probs = {\n        \n    }\n    for u in probs:\n        sorted_probs[u] = sorted([(v, o) for (o, v) in probs[u].items()], reverse=True)\n    return (sorted_probs, users, objs)\n", "label": 1}
{"function": "\n\ndef test_flush(self):\n    m1 = Markov(prefix='one', db=5)\n    m2 = Markov(prefix='two', db=5)\n    line = ['i', 'ate', 'a', 'peach']\n    line1 = ['i', 'ate', 'one', 'peach']\n    line2 = ['i', 'ate', 'a', 'sandwich']\n    m1.add_line_to_index(line)\n    m1.add_line_to_index(line1)\n    m1.add_line_to_index(line2)\n    important_line = ['we', 'all', 'have', 'phones']\n    m2.add_line_to_index(important_line)\n    r = redis.Redis(db=5)\n    assert (len(r.keys('one:*')) == 6)\n    assert (len(r.keys('two:*')) == 3)\n    m1.flush(prefix='one')\n    assert (len(r.keys('one:*')) == 0)\n    assert (len(r.keys('two:*')) == 3)\n    m2.flush(prefix='two')\n    assert (len(r.keys('one:*')) == 0)\n    assert (len(r.keys('two:*')) == 0)\n", "label": 0}
{"function": "\n\ndef execute(self):\n    if self.get_options().skip:\n        self.context.log.info('Skipping scalastyle.')\n        return\n    targets = self.get_non_synthetic_scala_targets(self.context.targets())\n    if (not targets):\n        return\n    with self.invalidated(targets) as invalidation_check:\n        invalid_targets = [vt.target for vt in invalidation_check.invalid_vts]\n        scalastyle_config = self.validate_scalastyle_config()\n        scalastyle_verbose = self.get_options().verbose\n        scalastyle_quiet = (self.get_options().quiet or False)\n        scalastyle_excluder = self.create_file_excluder()\n        self.context.log.debug('Non synthetic scala targets to be checked:')\n        for target in invalid_targets:\n            self.context.log.debug('  {address_spec}'.format(address_spec=target.address.spec))\n        scala_sources = self.get_non_excluded_scala_sources(scalastyle_excluder, invalid_targets)\n        self.context.log.debug('Non excluded scala sources to be checked:')\n        for source in scala_sources:\n            self.context.log.debug('  {source}'.format(source=source))\n        if scala_sources:\n\n            def call(srcs):\n\n                def to_java_boolean(x):\n                    return str(x).lower()\n                cp = ScalaPlatform.global_instance().style_classpath(self.context.products)\n                scalastyle_args = ['-c', scalastyle_config, '-v', to_java_boolean(scalastyle_verbose), '-q', to_java_boolean(scalastyle_quiet)]\n                return self.runjava(classpath=cp, main=self._MAIN, jvm_options=self.get_options().jvm_options, args=(scalastyle_args + srcs))\n            result = Xargs(call).execute(scala_sources)\n            if (result != 0):\n                raise TaskError('java {entry} ... exited non-zero ({exit_code})'.format(entry=Scalastyle._MAIN, exit_code=result))\n", "label": 1}
{"function": "\n\ndef run_app(**kwargs):\n    sys_args = sys.argv\n    runner_name = os.path.basename(sys_args[0])\n    (args, command, command_args) = parse_args(sys_args[1:])\n    if (not command):\n        print(('usage: %s [--config=/path/to/settings.py] [command] [options]' % runner_name))\n        sys.exit(1)\n    default_config_path = kwargs.get('default_config_path')\n    parser = OptionParser()\n    if (command == 'init'):\n        (options, opt_args) = parser.parse_args()\n        settings_initializer = kwargs.get('settings_initializer')\n        config_path = os.path.expanduser((' '.join(opt_args[1:]) or default_config_path))\n        if os.path.exists(config_path):\n            resp = None\n            while (resp not in ('Y', 'n')):\n                resp = raw_input(('File already exists at %r, overwrite? [nY] ' % config_path))\n                if (resp == 'n'):\n                    print('Aborted!')\n                    return\n        try:\n            create_default_settings(config_path, settings_initializer)\n        except OSError as e:\n            raise e.__class__(('Unable to write default settings file to %r' % config_path))\n        print(('Configuration file created at %r' % config_path))\n        return\n    parser.add_option('--config', metavar='CONFIG')\n    (options, logan_args) = parser.parse_args(args)\n    config_path = options.config\n    configure_app(config_path=config_path, **kwargs)\n    management.execute_from_command_line(([runner_name, command] + command_args))\n    sys.exit(0)\n", "label": 1}
{"function": "\n\ndef is_concyclic(*points):\n    'Is a sequence of points concyclic?\\n\\n        Test whether or not a sequence of points are concyclic (i.e., they lie\\n        on a circle).\\n\\n        Parameters\\n        ==========\\n\\n        points : sequence of Points\\n\\n        Returns\\n        =======\\n\\n        is_concyclic : boolean\\n            True if points are concyclic, False otherwise.\\n\\n        See Also\\n        ========\\n\\n        sympy.geometry.ellipse.Circle\\n\\n        Notes\\n        =====\\n\\n        No points are not considered to be concyclic. One or two points\\n        are definitely concyclic and three points are conyclic iff they\\n        are not collinear.\\n\\n        For more than three points, create a circle from the first three\\n        points. If the circle cannot be created (i.e., they are collinear)\\n        then all of the points cannot be concyclic. If the circle is created\\n        successfully then simply check the remaining points for containment\\n        in the circle.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.geometry import Point\\n        >>> p1, p2 = Point(-1, 0), Point(1, 0)\\n        >>> p3, p4 = Point(0, 1), Point(-1, 2)\\n        >>> Point.is_concyclic(p1, p2, p3)\\n        True\\n        >>> Point.is_concyclic(p1, p2, p3, p4)\\n        False\\n\\n        '\n    if (len(points) == 0):\n        return False\n    if (len(points) <= 2):\n        return True\n    points = [Point(p) for p in points]\n    if (len(points) == 3):\n        return (not Point.is_collinear(*points))\n    try:\n        from .ellipse import Circle\n        c = Circle(points[0], points[1], points[2])\n        for point in points[3:]:\n            if (point not in c):\n                return False\n        return True\n    except GeometryError:\n        return False\n", "label": 1}
{"function": "\n\ndef _get_boot_device(node, drac_boot_devices=None):\n    client = drac_common.get_drac_client(node)\n    try:\n        boot_modes = client.list_boot_modes()\n        next_boot_modes = [mode.id for mode in boot_modes if mode.is_next]\n        if (NON_PERSISTENT_BOOT_MODE in next_boot_modes):\n            next_boot_mode = NON_PERSISTENT_BOOT_MODE\n        else:\n            next_boot_mode = next_boot_modes[0]\n        if (drac_boot_devices is None):\n            drac_boot_devices = client.list_boot_devices()\n        drac_boot_device = drac_boot_devices[next_boot_mode][0]\n        boot_device = next((key for (key, value) in _BOOT_DEVICES_MAP.items() if (value in drac_boot_device.id)))\n        return {\n            'boot_device': boot_device,\n            'persistent': (next_boot_mode == PERSISTENT_BOOT_MODE),\n        }\n    except (drac_exceptions.BaseClientException, IndexError) as exc:\n        LOG.error(_LE('DRAC driver failed to get next boot mode for node %(node_uuid)s. Reason: %(error)s.'), {\n            'node_uuid': node.uuid,\n            'error': exc,\n        })\n        raise exception.DracOperationError(error=exc)\n", "label": 1}
{"function": "\n\ndef smart_str(s, encoding='utf-8', errors='strict'):\n    \"\\n    Returns a bytestring version of 's', encoded as specified in 'encoding'.\\n    Borrowed and simplified for this purpose from `django.utils.encoding`.\\n    \"\n    if (not isinstance(s, basestring)):\n        try:\n            return str(s)\n        except UnicodeEncodeError:\n            return unicode(s).encode(encoding, errors)\n    elif isinstance(s, unicode):\n        return s.encode(encoding, errors)\n    elif (s and (encoding != 'utf-8')):\n        return s.decode('utf-8', errors).encode(encoding, errors)\n    else:\n        return s\n", "label": 0}
{"function": "\n\ndef test__init__(self):\n    fixture = Dispatcher()\n    assert (fixture.actionManager == None)\n    assert (fixture.currentMode == None)\n    assert (fixture.currentAction == None)\n    assert (fixture.currentCommand == None)\n    assert (fixture.forceTokenToString == False)\n", "label": 0}
{"function": "\n\ndef ismount_raw(path):\n    '\\n    Test whether a path is a mount point. Whereas ismount will catch\\n    any exceptions and just return False, this raw version will not\\n    catch exceptions.\\n\\n    This is code hijacked from C Python 2.6.8, adapted to remove the extra\\n    lstat() system call.\\n    '\n    try:\n        s1 = os.lstat(path)\n    except os.error as err:\n        if (err.errno == errno.ENOENT):\n            return False\n        raise\n    if stat.S_ISLNK(s1.st_mode):\n        return False\n    s2 = os.lstat(os.path.join(path, '..'))\n    dev1 = s1.st_dev\n    dev2 = s2.st_dev\n    if (dev1 != dev2):\n        return True\n    ino1 = s1.st_ino\n    ino2 = s2.st_ino\n    if (ino1 == ino2):\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef changeActivity(old, new):\n    Base = declarative_base()\n    Session = sessionmaker(bind=op.get_bind())\n\n    class Move(Base):\n        __tablename__ = 'move'\n        id = sa.Column(sa.Integer, name='id', primary_key=True)\n        activity = sa.Column(sa.String, name='activity')\n        strokeCount = sa.Column(sa.Integer, name='stroke_count')\n\n    class Sample(Base):\n        __tablename__ = 'sample'\n        id = sa.Column(sa.Integer, name='id', primary_key=True)\n        moveId = sa.Column(sa.Integer, sa.ForeignKey(Move.id), name='move_id', nullable=False)\n        move = sa.orm.relationship(Move, backref=sa.orm.backref('samples', lazy='dynamic'))\n        events = sa.Column(sa.String, name='events')\n    session = Session()\n    for move in session.query(Move):\n        strokeCount = 0\n        for (eventData,) in session.query(Sample.events).filter((Sample.move == move), (Sample.events != None)):\n            events = json.loads(eventData)\n            if (('swimming' in events) and (events['swimming']['type'] == 'Stroke')):\n                strokeCount += 1\n        if ('swimming' in move.activity):\n            assert (strokeCount > 0)\n        if (strokeCount > 0):\n            move.strokeCount = strokeCount\n    session.commit()\n", "label": 1}
{"function": "\n\ndef request(self, **kwargs):\n    '\\n        Perform network request.\\n\\n        You can specify grab settings in ``**kwargs``.\\n        Any keyword argument will be passed to ``self.config``.\\n\\n        Returns: ``Document`` objects.\\n        '\n    self.prepare_request(**kwargs)\n    refresh_count = 0\n    while True:\n        self.log_request()\n        try:\n            self.transport.request()\n        except error.GrabError:\n            self.reset_temporary_options()\n            self.save_failed_dump()\n            raise\n        else:\n            doc = self.process_request_result()\n            if self.config['follow_location']:\n                if (doc.code in (301, 302, 303, 307, 308)):\n                    if doc.headers.get('Location'):\n                        refresh_count += 1\n                        if (refresh_count > self.config['redirect_limit']):\n                            raise error.GrabTooManyRedirectsError()\n                        else:\n                            url = doc.headers.get('Location')\n                            self.prepare_request(url=self.make_url_absolute(url), referer=None)\n                            continue\n            if self.config['follow_refresh']:\n                refresh_url = self.doc.get_meta_refresh_url()\n                if (refresh_url is not None):\n                    refresh_count += 1\n                    if (refresh_count > self.config['redirect_limit']):\n                        raise error.GrabTooManyRedirectsError()\n                    else:\n                        self.prepare_request(url=self.make_url_absolute(refresh_url), referer=None)\n                        continue\n            return doc\n", "label": 1}
{"function": "\n\ndef test_remove_key(self):\n    ks = KeyJar()\n    ks[''] = KeyBundle([{\n        'kty': 'oct',\n        'key': 'a1b2c3d4',\n        'use': 'sig',\n    }, {\n        'kty': 'oct',\n        'key': 'a1b2c3d4',\n        'use': 'ver',\n    }])\n    ks['http://www.example.org'] = [KeyBundle([{\n        'kty': 'oct',\n        'key': 'e5f6g7h8',\n        'use': 'sig',\n    }, {\n        'kty': 'oct',\n        'key': 'e5f6g7h8',\n        'use': 'ver',\n    }]), keybundle_from_local_file(RSAKEY, 'rsa', ['enc', 'dec'])]\n    ks['http://www.example.com'] = keybundle_from_local_file(RSA0, 'rsa', ['enc', 'dec'])\n    coll = ks['http://www.example.org']\n    assert (len(coll) == 2)\n    keys = ks.get_encrypt_key(key_type='RSA', owner='http://www.example.org')\n    assert (len(keys) == 1)\n    _key = keys[0]\n    ks.remove_key('http://www.example.org', 'RSA', _key)\n    coll = ks['http://www.example.org']\n    assert (len(coll) == 1)\n    keys = ks.get_encrypt_key(key_type='rsa', owner='http://www.example.org')\n    assert (len(keys) == 0)\n    keys = ks.verify_keys('http://www.example.com')\n    assert (len(keys) == 1)\n    assert (len([k for k in keys if (k.kty == 'oct')]) == 1)\n    keys = ks.decrypt_keys('http://www.example.org')\n    assert (keys == [])\n", "label": 1}
{"function": "\n\ndef new_clustered_sortind(x, k=10, row_key=None, cluster_key=None):\n    '\\n    Uses MiniBatch k-means clustering to cluster matrix into groups.\\n\\n    Each cluster of rows is then sorted by `scorefunc` -- by default, the max\\n    peak height when all rows in a cluster are averaged, or\\n    cluster.mean(axis=0).max().\\n\\n    Returns the index that will sort the rows of `x` and a list of \"breaks\".\\n    `breaks` is essentially a cumulative row count for each cluster boundary.\\n    In other words, after plotting the array you can use axhline on each\\n    \"break\" to plot the cluster boundary.\\n\\n    If `k` is a list or tuple, iteratively try each one and select the best\\n    with the lowest mean distance from cluster centers.\\n\\n    :param x: Matrix whose rows are to be clustered\\n    :param k: Number of clusters to create or a list of potential clusters; the\\n        optimum will be chosen from the list\\n    :param row_key:\\n        Optional function to act as a sort key for sorting rows within\\n        clusters.  Signature should be `scorefunc(a)` where `a` is a 1-D NumPy\\n        array.\\n    :param cluster_key:\\n        Optional function for sorting clusters.  Signature is `clusterfunc(a)`\\n        where `a` is a NumPy array containing all rows of `x` for cluster `i`.\\n        It must return a single value.\\n    '\n    try:\n        from sklearn.cluster import MiniBatchKMeans\n    except ImportError:\n        raise ImportError('please install scikits.learn for clustering.')\n    if isinstance(k, int):\n        best_k = k\n    else:\n        mean_dists = {\n            \n        }\n        for _k in k:\n            mbk = MiniBatchKMeans(init='k-means++', n_clusters=_k)\n            mbk.fit(x)\n            mean_dists[_k] = mbk.transform(x).mean()\n        best_k = sorted(mean_dists.items(), key=(lambda x: x[1]))[(- 1)][0]\n    mbk = MiniBatchKMeans(init='k-means++', n_clusters=best_k)\n    mbk.fit(x)\n    k = best_k\n    labels = mbk.labels_\n    scores = np.zeros(labels.shape, dtype=float)\n    if cluster_key:\n\n        def _cluster_key(i):\n            return cluster_key(x[(labels == i), :])\n        sorted_labels = sorted(range(k), key=_cluster_key)\n    else:\n        sorted_labels = range(k)\n    if row_key:\n\n        def _row_key(i):\n            return row_key(x[i, :])\n    final_ind = []\n    breaks = []\n    pos = 0\n    for label in sorted_labels:\n        label_inds = np.nonzero((labels == label))[0]\n        if row_key:\n            label_sort_ind = sorted(label_inds, key=_row_key)\n        else:\n            label_sort_ind = label_inds\n        for li in label_sort_ind:\n            final_ind.append(li)\n        pos += len(label_inds)\n        breaks.append(pos)\n    return (np.array(final_ind), np.array(breaks))\n", "label": 1}
{"function": "\n\ndef test_hassignals_without_self():\n    title_lengths = []\n\n    class Test(HasSignals):\n\n        @input\n        def title(v=''):\n            return str(v)\n\n        @connect('title')\n        def title_len(v):\n            return len(v)\n\n        @connect('title_len')\n        def show_title(v):\n            title_lengths.append(v)\n    t = Test()\n    assert (t.title._self is t)\n    assert (set(t.__signals__) == set(['title', 'title_len', 'show_title']))\n    assert (len(title_lengths) == 1)\n    assert (title_lengths[(- 1)] == 0)\n    t.title('foo')\n    assert (len(title_lengths) == 2)\n    assert (title_lengths[(- 1)] == 3)\n", "label": 0}
{"function": "\n\ndef __init__(self, patterns_ini=None, input_format='pdf', dedup=False, library='pdfminer', output_format='csv', output_handler=None):\n    basedir = os.path.dirname(os.path.abspath(__file__))\n    if (patterns_ini is None):\n        patterns_ini = os.path.join(basedir, 'patterns.ini')\n    self.load_patterns(patterns_ini)\n    self.whitelist = WhiteList(basedir)\n    self.dedup = dedup\n    if output_handler:\n        self.handler = output_handler\n    else:\n        self.handler = output.getHandler(output_format)\n    self.ext_filter = ('*.' + input_format)\n    parser_format = ('parse_' + input_format)\n    try:\n        self.parser_func = getattr(self, parser_format)\n    except AttributeError:\n        e = ('Selected parser format is not supported: %s' % input_format)\n        raise NotImplementedError(e)\n    self.library = library\n    if (input_format == 'pdf'):\n        if (library not in IMPORTS):\n            e = ('Selected PDF parser library not found: %s' % library)\n            raise ImportError(e)\n    elif (input_format == 'html'):\n        if ('beautifulsoup' not in IMPORTS):\n            e = 'HTML parser library not found: BeautifulSoup'\n            raise ImportError(e)\n", "label": 1}
{"function": "\n\ndef _get_and_update_relay(self, available_offers):\n    \"\\n        Get num tasks I should create and evaluate whether to use Relay's\\n        warmer or cooler command.  Update the MV with number of commands about\\n        to be created.\\n\\n        Competes for the MV with these other threads, and will wait\\n        indefinitely for it:\\n\\n          - other Mesos resourceOffers(...) calls to the Framework scheduler\\n          - Relay warmer and cooler functions attempting to ask the Framework\\n            to execute more tasks.\\n        \"\n    command = None\n    with self.MV.get_lock():\n        (MV, t) = self.MV\n        if (MV == 0):\n            log.debug('mesos scheduler has received no requests from relay', extra=dict(mesos_framework_name=self.ns.mesos_framework_name))\n        else:\n            if ((MV > 0) and self.ns.warmer):\n                command = self.ns.warmer\n            elif ((MV < 0) and self.ns.cooler):\n                command = self.ns.cooler\n            if (abs(MV) < len(available_offers)):\n                self.MV[:] = [0, time.time()]\n            else:\n                new_MV = (MV - (((MV > 0) or (- 1)) * max(abs(MV), len(available_offers))))\n                self.MV[:] = [new_MV, time.time()]\n    return (MV, command)\n", "label": 1}
{"function": "\n\ndef register(self, object_type, template_attribute, secret, credential=None):\n    self.logger.debug('register() called')\n    self.logger.debug(('object type = %s' % object_type))\n    attributes = template_attribute.attributes\n    ret_attributes = []\n    if (object_type is None):\n        self.logger.debug('invalid object type')\n        return self._get_missing_field_result('object type')\n    if (object_type.value != OT.SYMMETRIC_KEY):\n        self.logger.debug('invalid object type')\n        return self._get_invalid_field_result('invalid object type')\n    if ((secret is None) or (not isinstance(secret, SymmetricKey))):\n        msg = 'object type does not match that of secret'\n        self.logger.debug(msg)\n        return self._get_invalid_field_result(msg)\n    self.logger.debug('Collecting all attributes')\n    if (attributes is None):\n        attributes = []\n    attributes.extend(self._get_key_block_attributes(secret.key_block))\n    self.logger.debug('Verifying all attributes are valid and set')\n    try:\n        self._validate_req_field(attributes, AT.CRYPTOGRAPHIC_ALGORITHM.value, (CA.AES,), 'unsupported algorithm')\n        self._validate_req_field(attributes, AT.CRYPTOGRAPHIC_LENGTH.value, (128, 256, 512), 'unsupported key length')\n        self._validate_req_field(attributes, AT.CRYPTOGRAPHIC_USAGE_MASK.value, (), '')\n    except InvalidFieldException as e:\n        self.logger.debug('InvalidFieldException raised')\n        return RegisterResult(e.result.result_status, e.result.result_reason, e.result.result_message)\n    (s_uuid, uuid_attribute) = self._save(secret, attributes)\n    ret_attributes.append(uuid_attribute)\n    template_attribute = TemplateAttribute(attributes=ret_attributes)\n    return RegisterResult(ResultStatus(RS.SUCCESS), uuid=UniqueIdentifier(s_uuid), template_attribute=template_attribute)\n", "label": 0}
{"function": "\n\ndef find_matching_paren(source, start=0):\n    'Given a string and the index of an opening parenthesis, determines \\n    the index of the matching closing paren.'\n    assert (source[start] == '(')\n    pos = start\n    open_brackets = 1\n    while (open_brackets > 0):\n        pos += 1\n        if (len(source) == pos):\n            raise LispError(('Incomplete expression: %s' % source[start:]))\n        if (source[pos] == '('):\n            open_brackets += 1\n        if (source[pos] == ')'):\n            open_brackets -= 1\n    return pos\n", "label": 0}
{"function": "\n\ndef _centroid(self, cluster, mean):\n    if self._avoid_empty_clusters:\n        centroid = copy.copy(mean)\n        for vector in cluster:\n            centroid += vector\n        return (centroid / (1 + len(cluster)))\n    else:\n        if (not len(cluster)):\n            sys.stderr.write('Error: no centroid defined for empty cluster.\\n')\n            sys.stderr.write(\"Try setting argument 'avoid_empty_clusters' to True\\n\")\n            assert False\n        centroid = copy.copy(cluster[0])\n        for vector in cluster[1:]:\n            centroid += vector\n        return (centroid / len(cluster))\n", "label": 0}
{"function": "\n\ndef recurring(self, credit_card_details, options=None):\n    '\\n            Recurring Payment Implementation using eWay recurring API (http://www.eway.com.au/developers/api/recurring)\\n\\n            Input Parameters:\\n                ( Please find the details here in this Gist: https://gist.github.com/df67e02f7ffb39f415e6 )\\n                credit_card   :    Customer Credit Card details\\n                options       :    Customer and Recurring Payment details\\n\\n            Output Dict:\\n                status: \\'SUCCESS\\' or \\'FAILURE\\'\\n                response : Response list of rebill event request in order of provided input\\n                           in options[\"customer_rebill_details\"] list.\\n        '\n    error_response = {\n        \n    }\n    try:\n        if (not options):\n            error_response = {\n                'reason': 'Not enough information Available!',\n            }\n            raise\n        '\\n                # Validate Entered credit card details.\\n            '\n        credit_card = CreditCard(**credit_card_details)\n        if (not self.validate_card(credit_card)):\n            raise InvalidCard('Invalid Card')\n        rebillClient = RebillEwayClient(customer_id=self.customer_id, username=self.eway_username, password=self.eway_password, url=self.rebill_url)\n        customer_detail = rebillClient.client.factory.create('CustomerDetails')\n        self.add_customer_details(credit_card, customer_detail, options)\n        '\\n                # Create Rebill customer and retrieve customer rebill ID.\\n            '\n        rebill_customer_response = rebillClient.create_rebill_customer(customer_detail)\n        if rebill_customer_response.ErrorSeverity:\n            transaction_was_unsuccessful.send(sender=self, type='recurringCreateRebill', response=rebill_customer_response)\n            error_response = rebill_customer_response\n            raise\n        rebile_customer_id = rebill_customer_response.RebillCustomerID\n        '\\n                For Each rebill profile\\n                # Create Rebill events using rebill customer ID and customer rebill details.\\n            '\n        rebill_event_response_list = []\n        for each_rebill_profile in options.get('customer_rebill_details', []):\n            rebill_detail = rebillClient.client.factory.create('RebillEventDetails')\n            self.add_rebill_details(rebill_detail, rebile_customer_id, credit_card, each_rebill_profile)\n            rebill_event_response = rebillClient.create_rebill_event(rebill_detail)\n            if rebill_event_response.ErrorSeverity:\n                transaction_was_unsuccessful.send(sender=self, type='recurringRebillEvent', response=rebill_event_response)\n                error_response = rebill_event_response\n                raise\n            rebill_event_response_list.append(rebill_event_response)\n        transaction_was_successful.send(sender=self, type='recurring', response=rebill_event_response_list)\n    except Exception as e:\n        error_response['exception'] = e\n        return {\n            'status': 'Failure',\n            'response': error_response,\n        }\n    return {\n        'status': 'SUCCESS',\n        'response': rebill_event_response_list,\n    }\n", "label": 0}
{"function": "\n\ndef cluster_list(tokeniser):\n    clusterids = []\n    value = tokeniser()\n    try:\n        if (value == '['):\n            while True:\n                value = tokeniser()\n                if (value == ']'):\n                    break\n                clusterids.append(ClusterID(value))\n        else:\n            clusterids.append(ClusterID(value))\n        if (not clusterids):\n            raise ValueError('no cluster-id in the cluster list')\n        return ClusterList(clusterids)\n    except ValueError:\n        raise ValueError('invalud cluster list')\n", "label": 0}
{"function": "\n\ndef test_panzoom_pan_keyboard(qtbot, canvas_pz, panzoom):\n    c = canvas_pz\n    pz = panzoom\n    c.events.key_press(key=keys.UP)\n    assert (pz.pan[0] == 0)\n    assert (pz.pan[1] < 0)\n    c.events.key_press(key=keys.LEFT)\n    c.events.key_press(key=keys.DOWN)\n    c.events.key_press(key=keys.RIGHT)\n    assert (pz.pan == [0, 0])\n    c.events.key_press(key=keys.RIGHT)\n    c.events.key_press(key=keys.Key('r'))\n    assert (pz.pan == [0, 0])\n    c.events.key_press(key=keys.UP, modifiers=(keys.CONTROL,))\n    assert (pz.pan == [0, 0])\n    pz.enable_keyboard_pan = False\n    c.events.key_press(key=keys.UP, modifiers=(keys.CONTROL,))\n    assert (pz.pan == [0, 0])\n", "label": 0}
{"function": "\n\ndef _match_headers(self, request):\n    for (k, vals) in six.iteritems(self._request_headers):\n        try:\n            header = request.headers[k]\n        except KeyError:\n            if (not isinstance(k, six.text_type)):\n                return False\n            try:\n                header = request.headers[k.encode('utf-8')]\n            except KeyError:\n                return False\n        if (header != vals):\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef _parse_settings_bond_1(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond1.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '1',\n    }\n    for binding in ['miimon', 'downdelay', 'updelay']:\n        if (binding in opts):\n            try:\n                int(opts[binding])\n                bond.update({\n                    binding: opts[binding],\n                })\n            except Exception:\n                _raise_error_iface(iface, binding, ['integer'])\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({\n                binding: bond_def[binding],\n            })\n    if ('use_carrier' in opts):\n        if (opts['use_carrier'] in _CONFIG_TRUE):\n            bond.update({\n                'use_carrier': '1',\n            })\n        elif (opts['use_carrier'] in _CONFIG_FALSE):\n            bond.update({\n                'use_carrier': '0',\n            })\n        else:\n            valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({\n            'use_carrier': bond_def['use_carrier'],\n        })\n    return bond\n", "label": 0}
{"function": "\n\ndef fruchterman_reingold_layout(G, dim=2, k=None, pos=None, fixed=None, iterations=50, weight='weight', scale=1.0):\n    \"Position nodes using Fruchterman-Reingold force-directed algorithm. \\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n\\n    dim : int\\n       Dimension of layout\\n\\n    k : float (default=None)\\n       Optimal distance between nodes.  If None the distance is set to\\n       1/sqrt(n) where n is the number of nodes.  Increase this value\\n       to move nodes farther apart.\\n\\n\\n    pos : dict or None  optional (default=None)\\n       Initial positions for nodes as a dictionary with node as keys\\n       and values as a list or tuple.  If None, then nuse random initial\\n       positions.\\n\\n    fixed : list or None  optional (default=None)\\n      Nodes to keep fixed at initial position.\\n\\n    iterations : int  optional (default=50)\\n       Number of iterations of spring-force relaxation\\n\\n    weight : string or None   optional (default='weight')\\n        The edge attribute that holds the numerical value used for\\n        the edge weight.  If None, then all edge weights are 1.\\n\\n    scale : float (default=1.0)\\n        Scale factor for positions. The nodes are positioned \\n        in a box of size [0,scale] x [0,scale].  \\n\\n\\n    Returns\\n    -------\\n    dict :\\n       A dictionary of positions keyed by node\\n\\n    Examples\\n    --------\\n    >>> G=nx.path_graph(4)\\n    >>> pos=nx.spring_layout(G)\\n\\n    # The same using longer function name\\n    >>> pos=nx.fruchterman_reingold_layout(G)\\n    \"\n    try:\n        import numpy as np\n    except ImportError:\n        raise ImportError('fruchterman_reingold_layout() requires numpy: http://scipy.org/ ')\n    if (fixed is not None):\n        nfixed = dict(zip(G, range(len(G))))\n        fixed = np.asarray([nfixed[v] for v in fixed])\n    if (pos is not None):\n        pos_arr = np.asarray(np.random.random((len(G), dim)))\n        for (i, n) in enumerate(G):\n            if (n in pos):\n                pos_arr[i] = np.asarray(pos[n])\n    else:\n        pos_arr = None\n    if (len(G) == 0):\n        return {\n            \n        }\n    if (len(G) == 1):\n        return {\n            G.nodes()[0]: ((1,) * dim),\n        }\n    try:\n        if (len(G) < 500):\n            raise ValueError\n        A = nx.to_scipy_sparse_matrix(G, weight=weight, dtype='f')\n        pos = _sparse_fruchterman_reingold(A, dim, k, pos_arr, fixed, iterations)\n    except:\n        A = nx.to_numpy_matrix(G, weight=weight)\n        pos = _fruchterman_reingold(A, dim, k, pos_arr, fixed, iterations)\n    if (fixed is None):\n        pos = _rescale_layout(pos, scale=scale)\n    return dict(zip(G, pos))\n", "label": 1}
{"function": "\n\ndef check(self):\n    script = self.script\n    s = script.engine.current_scene\n    src = s.children[0]\n    g = src.children[0].children[1]\n    assert (g.glyph.glyph_source.glyph_position == 'center')\n    assert (g.glyph.glyph.vector_mode == 'use_normal')\n    assert (g.glyph.glyph.scale_factor == 0.5)\n    assert (g.actor.property.line_width == 1.0)\n    v = src.children[0].children[2]\n    glyph = v.glyph\n    gs = glyph.glyph_source\n    assert (gs.glyph_position == 'tail')\n    assert (gs.glyph_source == gs.glyph_list[1])\n    assert numpy.allclose(v.implicit_plane.normal, (0.0, 1.0, 0.0))\n    v = src.children[0].children[3]\n    glyph = v.glyph\n    gs = glyph.glyph_source\n    assert (gs.glyph_source == gs.glyph_list[2])\n    assert (gs.glyph_position == 'head')\n    assert numpy.allclose(v.implicit_plane.normal, (0.0, 1.0, 0.0))\n", "label": 1}
{"function": "\n\ndef _get_new_field_html(self, field_name):\n    try:\n        (f, attr, value) = lookup_field(field_name, self.org_obj, self)\n    except (AttributeError, ObjectDoesNotExist):\n        return EMPTY_CHANGELIST_VALUE\n    else:\n        allow_tags = False\n        if (f is None):\n            allow_tags = getattr(attr, 'allow_tags', False)\n            boolean = getattr(attr, 'boolean', False)\n            if boolean:\n                allow_tags = True\n                text = boolean_icon(value)\n            else:\n                text = smart_unicode(value)\n        elif isinstance(f.rel, models.ManyToOneRel):\n            field_val = getattr(self.org_obj, f.name)\n            if (field_val is None):\n                text = EMPTY_CHANGELIST_VALUE\n            else:\n                text = field_val\n        else:\n            text = display_for_field(value, f)\n        return (mark_safe(text) if allow_tags else conditional_escape(text))\n", "label": 1}
{"function": "\n\ndef inputhook_wx3():\n    'Run the wx event loop by processing pending events only.\\n    \\n    This is like inputhook_wx1, but it keeps processing pending events\\n    until stdin is ready.  After processing all pending events, a call to \\n    time.sleep is inserted.  This is needed, otherwise, CPU usage is at 100%.\\n    This sleep time should be tuned though for best performance.\\n    '\n    try:\n        app = wx.GetApp()\n        if (app is not None):\n            assert wx.Thread_IsMain()\n            if (not isinstance(signal.getsignal(signal.SIGINT), collections.Callable)):\n                signal.signal(signal.SIGINT, signal.default_int_handler)\n            evtloop = wx.EventLoop()\n            ea = wx.EventLoopActivator(evtloop)\n            t = clock()\n            while (not stdin_ready()):\n                while evtloop.Pending():\n                    t = clock()\n                    evtloop.Dispatch()\n                app.ProcessIdle()\n                used_time = (clock() - t)\n                if (used_time > (5 * 60.0)):\n                    time.sleep(5.0)\n                elif (used_time > 10.0):\n                    time.sleep(1.0)\n                elif (used_time > 0.1):\n                    time.sleep(0.05)\n                else:\n                    time.sleep(0.001)\n            del ea\n    except KeyboardInterrupt:\n        pass\n    return 0\n", "label": 1}
{"function": "\n\ndef get_file_content(url, comes_from=None):\n    'Gets the content of a file; it may be a filename, file: URL, or\\n    http: URL.  Returns (location, content)'\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if ((scheme == 'file') and comes_from and comes_from.startswith('http')):\n            raise InstallationError(('Requirements file %s references URL %s, which is local' % (comes_from, url)))\n        if (scheme == 'file'):\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = ((match.group(1) + ':') + path.split('|', 1)[1])\n            path = urllib.unquote(path)\n            if path.startswith('/'):\n                path = ('/' + path.lstrip('/'))\n            url = path\n        else:\n            resp = urlopen(url)\n            return (geturl(resp), resp.read())\n    try:\n        f = open(url)\n        content = f.read()\n    except IOError:\n        e = sys.exc_info()[1]\n        raise InstallationError(('Could not open requirements file: %s' % str(e)))\n    else:\n        f.close()\n    return (url, content)\n", "label": 1}
{"function": "\n\ndef test_single_inheritance(self):\n    (Employee, JuniorEngineer, Manager, Engineer) = (self.classes.Employee, self.classes.JuniorEngineer, self.classes.Manager, self.classes.Engineer)\n    session = create_session()\n    m1 = Manager(name='Tom', manager_data='knows how to manage things')\n    e1 = Engineer(name='Kurt', engineer_info='knows how to hack')\n    e2 = JuniorEngineer(name='Ed', engineer_info='oh that ed')\n    session.add_all([m1, e1, e2])\n    session.flush()\n    assert (session.query(Employee).all() == [m1, e1, e2])\n    assert (session.query(Engineer).all() == [e1, e2])\n    assert (session.query(Manager).all() == [m1])\n    assert (session.query(JuniorEngineer).all() == [e2])\n    m1 = session.query(Manager).one()\n    session.expire(m1, ['manager_data'])\n    eq_(m1.manager_data, 'knows how to manage things')\n    row = session.query(Engineer.name, Engineer.employee_id).filter((Engineer.name == 'Kurt')).first()\n    assert (row.name == 'Kurt')\n    assert (row.employee_id == e1.employee_id)\n", "label": 0}
{"function": "\n\ndef vim2vim(filename):\n    'Transform output from `:highlight` to a flat, plain vim colorscheme.\\n\\n    '\n    with open(filename, 'r') as f:\n        for line in f:\n            log.debug(line.rstrip())\n            l = line.strip()\n            if ((not l) or ('xxx' not in l)):\n                continue\n            (name, attrs) = l.split('xxx')\n            name = name.strip()\n            attrs = attrs.strip()\n            attrdict = dict.fromkeys(ATTRS, None)\n            attrdict['name'] = name\n            if attrs.startswith('links to'):\n                attrdict['link'] = attrs.split('links to ')[(- 1)]\n            elif attrs.startswith('cleared'):\n                attrdict['cleared'] = True\n            else:\n                _attrs = [x.split('=', 1) for x in attrs.split(' ')]\n                try:\n                    attrdict.update(_attrs)\n                except ValueError:\n                    log.error(_attrs)\n                    raise\n            (yield ColorGroup(**attrdict))\n", "label": 1}
{"function": "\n\n@click.command()\n@click.option('-f', '--separate-folders', is_flag=True, default=False)\n@click.option('-st', '--style-type', type=click.Choice(STYLE_TYPES), default=None)\n@click.option('-sf', '--style-filename', default='style.json')\n@click.argument('kml_path')\n@click.argument('output_dir')\ndef main(kml_path, output_dir, separate_folders=False, style_type=None, style_filename='style.json'):\n    \"\\n    Given a path to a KML file, convert it to one or several GeoJSON\\n    FeatureCollection files and save the result(s) to the given\\n    output directory.\\n\\n    If ``separate_folders == False`` (the default), then create one\\n    GeoJSON file.\\n    If ``separate_folders == True``, then create several GeoJSON\\n    files, one for each folder in the KML file\\n    that contains geodata or that has a descendant node that contains geodata.\\n    Warning: this can produce GeoJSON files with the same geodata in case\\n    the KML file has nested folders with geodata.\\n\\n    If a ``style_type`` is given (default is ``None``),\\n    then also build a JSON style\\n    file of the given style type and save it to the output directory\\n    under the name given by ``style_filename``\\n    (which defaults to 'style.json')\\n    \"\n    kml_path = pathlib.Path(kml_path).resolve()\n    if (output_dir is None):\n        output_dir = kml_path.parent\n    else:\n        output_dir = pathlib.Path(output_dir)\n    if (not output_dir.exists()):\n        output_dir.mkdir()\n    output_dir = output_dir.resolve()\n    with kml_path.open('r') as src:\n        kml_str = src.read()\n    root = md.parseString(kml_str)\n    if separate_folders:\n        layers = build_layers(root)\n    else:\n        name = kml_path.name.replace('.kml', '')\n        layers = [build_feature_collection(root, name=name)]\n    filenames = disambiguate([to_filename(layer['properties']['name']) for layer in layers])\n    filenames = [(name + '.geojson') for name in filenames]\n    for i in range(len(layers)):\n        path = pathlib.Path(output_dir, filenames[i])\n        with path.open('w') as tgt:\n            json.dump(layers[i], tgt)\n    if (style_type in STYLE_TYPES):\n        builder_name = 'build_{!s}_style'.format(style_type)\n        style_dict = globals()[builder_name](root)\n        path = pathlib.Path(output_dir, style_filename)\n        with path.open('w') as tgt:\n            json.dump(style_dict, tgt)\n", "label": 1}
{"function": "\n\ndef from_qvariant(qobj=None, convfunc=None):\n    'Convert QVariant object to Python object\\n        This is a transitional function from PyQt API #1 (QVariant exist) \\n        to PyQt API #2 and Pyside (QVariant does not exist)'\n    if PYQT_API_1:\n        assert isinstance(convfunc, collections.Callable)\n        if ((convfunc in TEXT_TYPES) or (convfunc is to_text_string)):\n            return convfunc(qobj.toString())\n        elif (convfunc is bool):\n            return qobj.toBool()\n        elif (convfunc is int):\n            return qobj.toInt()[0]\n        elif (convfunc is float):\n            return qobj.toDouble()[0]\n        else:\n            return convfunc(qobj)\n    else:\n        return qobj\n", "label": 1}
{"function": "\n\ndef test_headerData(self):\n    model = ColumnDtypeModel()\n    ret = model.headerData(0, Qt.Horizontal)\n    assert (ret == 'column')\n    ret = model.headerData(1, Qt.Horizontal)\n    assert (ret == 'data type')\n    ret = model.headerData(2, Qt.Horizontal)\n    assert (ret == None)\n    ret = model.headerData(0, Qt.Horizontal, Qt.EditRole)\n    assert (ret == None)\n    ret = model.headerData(0, Qt.Vertical)\n    assert (ret == None)\n", "label": 0}
{"function": "\n\ndef exchange_declare(self, exchange=None, type='direct', durable=False, auto_delete=False, arguments=None, nowait=False, passive=False):\n    'Declare exchange.'\n    type = (type or 'direct')\n    exchange = (exchange or ('amq.%s' % type))\n    if passive:\n        if (exchange not in self.state.exchanges):\n            raise ChannelError('NOT_FOUND - no exchange {0!r} in vhost {1!r}'.format(exchange, (self.connection.client.virtual_host or '/')), (50, 10), 'Channel.exchange_declare', '404')\n        return\n    try:\n        prev = self.state.exchanges[exchange]\n        if (not self.typeof(exchange).equivalent(prev, exchange, type, durable, auto_delete, arguments)):\n            raise NotEquivalentError(NOT_EQUIVALENT_FMT.format(exchange, (self.connection.client.virtual_host or '/')))\n    except KeyError:\n        self.state.exchanges[exchange] = {\n            'type': type,\n            'durable': durable,\n            'auto_delete': auto_delete,\n            'arguments': (arguments or {\n                \n            }),\n            'table': [],\n        }\n", "label": 1}
{"function": "\n\ndef test_setup(tmpdir):\n    tmpdir = str(tmpdir)\n    etcdir = os.path.join(tmpdir, 'first', 'etc', 'squadron')\n    vardir = os.path.join(tmpdir, 'first', 'var', 'squadron')\n    with mock.patch('__builtin__.raw_input', side_effect=[etcdir, vardir]):\n        setup.setup(None, None)\n        assert os.path.exists(os.path.join(etcdir, 'config'))\n        assert os.path.exists(os.path.join(vardir, 'info.json'))\n    etcdir = os.path.join(tmpdir, 'second', 'etc', 'squadron')\n    vardir = os.path.join(tmpdir, 'second', 'var', 'squadron')\n    with mock.patch('__builtin__.raw_input', return_value=etcdir):\n        setup.setup(None, vardir)\n        assert os.path.exists(os.path.join(etcdir, 'config'))\n        assert os.path.exists(os.path.join(vardir, 'info.json'))\n    etcdir = os.path.join(tmpdir, 'third', 'etc', 'squadron')\n    vardir = os.path.join(tmpdir, 'third', 'var', 'squadron')\n    with mock.patch('__builtin__.raw_input', return_value=vardir):\n        setup.setup(etcdir, None)\n        assert os.path.exists(os.path.join(etcdir, 'config'))\n        assert os.path.exists(os.path.join(vardir, 'info.json'))\n    etcdir = os.path.join(tmpdir, 'fourth', 'etc', 'squadron')\n    vardir = os.path.join(tmpdir, 'fourth', 'var', 'squadron')\n    with mock.patch('__builtin__.raw_input', side_effect=Exception(\"Don't call this\")):\n        setup.setup(etcdir, vardir)\n        assert os.path.exists(os.path.join(etcdir, 'config'))\n        assert os.path.exists(os.path.join(vardir, 'info.json'))\n", "label": 1}
{"function": "\n\n@expose_action\ndef status(self):\n    'Get the status of the daemon.'\n    if (self.pidfile is None):\n        raise DaemonError('Cannot get status of daemon without PID file')\n    pid = self._read_pidfile()\n    if (pid is None):\n        self._emit_message('{prog} -- not running\\n'.format(prog=self.prog))\n        sys.exit(1)\n    proc = psutil.Process(pid)\n    data = {\n        'prog': self.prog,\n        'pid': pid,\n        'status': proc.status(),\n        'uptime': '0m',\n        'cpu': 0.0,\n        'memory': 0.0,\n    }\n    pgid = os.getpgid(pid)\n    for gproc in psutil.process_iter():\n        try:\n            if ((os.getpgid(gproc.pid) == pgid) and (gproc.pid != 0)):\n                data['cpu'] += gproc.cpu_percent(interval=0.1)\n                data['memory'] += gproc.memory_percent()\n        except (psutil.Error, OSError):\n            continue\n    try:\n        uptime_mins = int(round(((time.time() - proc.create_time()) / 60)))\n        (uptime_hours, uptime_mins) = divmod(uptime_mins, 60)\n        data['uptime'] = (str(uptime_mins) + 'm')\n        if uptime_hours:\n            (uptime_days, uptime_hours) = divmod(uptime_hours, 24)\n            data['uptime'] = ((str(uptime_hours) + 'h ') + data['uptime'])\n            if uptime_days:\n                data['uptime'] = ((str(uptime_days) + 'd ') + data['uptime'])\n    except psutil.Error:\n        pass\n    template = '{prog} -- pid: {pid}, status: {status}, uptime: {uptime}, %cpu: {cpu:.1f}, %mem: {memory:.1f}\\n'\n    self._emit_message(template.format(**data))\n", "label": 1}
{"function": "\n\ndef _match_search_query(left, right):\n    left_filter = set([value for (param_name, value) in left if ('filter' in _maybe_decode(param_name))])\n    right_filter = set([value for (param_name, value) in right if ('filter' in _maybe_decode(param_name))])\n    left_rest = set([(param_name, value) for (param_name, value) in left if ('filter' not in _maybe_decode(param_name))])\n    right_rest = set([(param_name, value) for (param_name, value) in right if ('filter' not in _maybe_decode(param_name))])\n    try:\n        log.info(simplejson.dumps({\n            'filter_differences': list(left_filter.symmetric_difference(right_filter)),\n            'rest_differences': list(left_rest.symmetric_difference(right_rest)),\n        }, encoding='utf-8'))\n    except Exception as e:\n        log.warning(e)\n    return ((left_filter == right_filter) and (left_rest == right_rest))\n", "label": 1}
{"function": "\n\n@memoized\ndef _get_service_url(request):\n    'Get freezer api url'\n    hardcoded_url = getattr(settings, 'FREEZER_API_URL', None)\n    if (hardcoded_url is not None):\n        LOG.warn('Using hardcoded FREEZER_API_URL: {0}'.format(hardcoded_url))\n        return hardcoded_url\n    e_type = getattr(settings, 'OPENSTACK_ENDPOINT_TYPE', '')\n    endpoint_type_priority = [e_type, ['internal', 'internalURL'], ['public', 'publicURL']]\n    try:\n        catalog = getattr(request.user, 'service_catalog', [])\n        for c in catalog:\n            if (c['name'] == 'freezer'):\n                for endpoint_type in endpoint_type_priority:\n                    for e in c['endpoints']:\n                        if (e['interface'] in endpoint_type):\n                            return e['url']\n        raise ValueError('Could no get FREEZER_API_URL from config or Keystone')\n    except Exception:\n        LOG.warn('Could no get FREEZER_API_URL from config or Keystone')\n        raise\n", "label": 1}
{"function": "\n\ndef page(self):\n    changemsg = []\n    if (cherrypy.session.id != cherrypy.session.originalid):\n        if (cherrypy.session.originalid is None):\n            changemsg.append('Created new session because no session id was given.')\n        if cherrypy.session.missing:\n            changemsg.append('Created new session due to missing (expired or malicious) session.')\n        if cherrypy.session.regenerated:\n            changemsg.append('Application generated a new session.')\n    try:\n        expires = cherrypy.response.cookie['session_id']['expires']\n    except KeyError:\n        expires = ''\n    return (page % {\n        'sessionid': cherrypy.session.id,\n        'changemsg': '<br>'.join(changemsg),\n        'respcookie': cherrypy.response.cookie.output(),\n        'reqcookie': cherrypy.request.cookie.output(),\n        'sessiondata': copyitems(cherrypy.session),\n        'servertime': (datetime.utcnow().strftime('%Y/%m/%d %H:%M') + ' UTC'),\n        'serverunixtime': calendar.timegm(datetime.utcnow().timetuple()),\n        'cpversion': cherrypy.__version__,\n        'pyversion': sys.version,\n        'expires': expires,\n    })\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize(('css_source', 'expected_result'), [('', None), (' /* hey */\\n', None), ('4', None), ('top', None), ('/**/transparent', (0, 0, 0, 0)), ('transparent', (0, 0, 0, 0)), (' transparent\\n', (0, 0, 0, 0)), ('TransParent', (0, 0, 0, 0)), ('currentColor', 'currentColor'), ('CURRENTcolor', 'currentColor'), ('current_Color', None), ('black', (0, 0, 0, 1)), ('white', (1, 1, 1, 1)), ('fuchsia', (1, 0, 1, 1)), ('cyan', (0, 1, 1, 1)), ('CyAn', (0, 1, 1, 1)), ('darkkhaki', ((189 / 255.0), (183 / 255.0), (107 / 255.0), 1)), ('#', None), ('#f', None), ('#ff', None), ('#fff', (1, 1, 1, 1)), ('#ffg', None), ('#ffff', None), ('#fffff', None), ('#ffffff', (1, 1, 1, 1)), ('#fffffg', None), ('#fffffff', None), ('#ffffffff', None), ('#fffffffff', None), ('#cba987', ((203 / 255.0), (169 / 255.0), (135 / 255.0), 1)), ('#CbA987', ((203 / 255.0), (169 / 255.0), (135 / 255.0), 1)), ('#1122aA', ((17 / 255.0), (34 / 255.0), (170 / 255.0), 1)), ('#12a', ((17 / 255.0), (34 / 255.0), (170 / 255.0), 1)), ('rgb(203, 169, 135)', ((203 / 255.0), (169 / 255.0), (135 / 255.0), 1)), ('RGB(255, 255, 255)', (1, 1, 1, 1)), ('rgB(0, 0, 0)', (0, 0, 0, 1)), ('rgB(0, 51, 255)', (0, 0.2, 1, 1)), ('rgb(0,51,255)', (0, 0.2, 1, 1)), ('rgb(0\\t,  51 ,255)', (0, 0.2, 1, 1)), ('rgb(/* R */0, /* G */51, /* B */255)', (0, 0.2, 1, 1)), ('rgb(-51, 306, 0)', ((- 0.2), 1.2, 0, 1)), ('rgb(42%, 3%, 50%)', (0.42, 0.03, 0.5, 1)), ('RGB(100%, 100%, 100%)', (1, 1, 1, 1)), ('rgB(0%, 0%, 0%)', (0, 0, 0, 1)), ('rgB(10%, 20%, 30%)', (0.1, 0.2, 0.3, 1)), ('rgb(10%,20%,30%)', (0.1, 0.2, 0.3, 1)), ('rgb(10%\\t,  20% ,30%)', (0.1, 0.2, 0.3, 1)), ('rgb(/* R */10%, /* G */20%, /* B */30%)', (0.1, 0.2, 0.3, 1)), ('rgb(-12%, 110%, 1400%)', ((- 0.12), 1.1, 14, 1)), ('rgb(10%, 50%, 0)', None), ('rgb(255, 50%, 0%)', None), ('rgb(0, 0 0)', None), ('rgb(0, 0, 0deg)', None), ('rgb(0, 0, light)', None), ('rgb()', None), ('rgb(0)', None), ('rgb(0, 0)', None), ('rgb(0, 0, 0, 0)', None), ('rgb(0%)', None), ('rgb(0%, 0%)', None), ('rgb(0%, 0%, 0%, 0%)', None), ('rgb(0%, 0%, 0%, 0)', None), ('rgba(0, 0, 0, 0)', (0, 0, 0, 0)), ('rgba(203, 169, 135, 0.3)', ((203 / 255.0), (169 / 255.0), (135 / 255.0), 0.3)), ('RGBA(255, 255, 255, 0)', (1, 1, 1, 0)), ('rgBA(0, 51, 255, 1)', (0, 0.2, 1, 1)), ('rgba(0, 51, 255, 1.1)', (0, 0.2, 1, 1)), ('rgba(0, 51, 255, 37)', (0, 0.2, 1, 1)), ('rgba(0, 51, 255, 0.42)', (0, 0.2, 1, 0.42)), ('rgba(0, 51, 255, 0)', (0, 0.2, 1, 0)), ('rgba(0, 51, 255, -0.1)', (0, 0.2, 1, 0)), ('rgba(0, 51, 255, -139)', (0, 0.2, 1, 0)), ('rgba(42%, 3%, 50%, 0.3)', (0.42, 0.03, 0.5, 0.3)), ('RGBA(100%, 100%, 100%, 0)', (1, 1, 1, 0)), ('rgBA(0%, 20%, 100%, 1)', (0, 0.2, 1, 1)), ('rgba(0%, 20%, 100%, 1.1)', (0, 0.2, 1, 1)), ('rgba(0%, 20%, 100%, 37)', (0, 0.2, 1, 1)), ('rgba(0%, 20%, 100%, 0.42)', (0, 0.2, 1, 0.42)), ('rgba(0%, 20%, 100%, 0)', (0, 0.2, 1, 0)), ('rgba(0%, 20%, 100%, -0.1)', (0, 0.2, 1, 0)), ('rgba(0%, 20%, 100%, -139)', (0, 0.2, 1, 0)), ('rgba(255, 255, 255, 0%)', None), ('rgba(10%, 50%, 0, 1)', None), ('rgba(255, 50%, 0%, 1)', None), ('rgba(0, 0, 0 0)', None), ('rgba(0, 0, 0, 0deg)', None), ('rgba(0, 0, 0, light)', None), ('rgba()', None), ('rgba(0)', None), ('rgba(0, 0, 0)', None), ('rgba(0, 0, 0, 0, 0)', None), ('rgba(0%)', None), ('rgba(0%, 0%)', None), ('rgba(0%, 0%, 0%)', None), ('rgba(0%, 0%, 0%, 0%)', None), ('rgba(0%, 0%, 0%, 0%, 0%)', None), ('HSL(0, 0%, 0%)', (0, 0, 0, 1)), ('hsL(0, 100%, 50%)', (1, 0, 0, 1)), ('hsl(60, 100%, 37.5%)', (0.75, 0.75, 0, 1)), ('hsl(780, 100%, 37.5%)', (0.75, 0.75, 0, 1)), ('hsl(-300, 100%, 37.5%)', (0.75, 0.75, 0, 1)), ('hsl(300, 50%, 50%)', (0.75, 0.25, 0.75, 1)), ('hsl(10, 50%, 0)', None), ('hsl(50%, 50%, 0%)', None), ('hsl(0, 0% 0%)', None), ('hsl(30deg, 100%, 100%)', None), ('hsl(0, 0%, light)', None), ('hsl()', None), ('hsl(0)', None), ('hsl(0, 0%)', None), ('hsl(0, 0%, 0%, 0%)', None), ('HSLA(-300, 100%, 37.5%, 1)', (0.75, 0.75, 0, 1)), ('hsLA(-300, 100%, 37.5%, 12)', (0.75, 0.75, 0, 1)), ('hsla(-300, 100%, 37.5%, 0.2)', (0.75, 0.75, 0, 0.2)), ('hsla(-300, 100%, 37.5%, 0)', (0.75, 0.75, 0, 0)), ('hsla(-300, 100%, 37.5%, -3)', (0.75, 0.75, 0, 0)), ('hsla(10, 50%, 0, 1)', None), ('hsla(50%, 50%, 0%, 1)', None), ('hsla(0, 0% 0%, 1)', None), ('hsla(30deg, 100%, 100%, 1)', None), ('hsla(0, 0%, light, 1)', None), ('hsla()', None), ('hsla(0)', None), ('hsla(0, 0%)', None), ('hsla(0, 0%, 0%, 50%)', None), ('hsla(0, 0%, 0%, 1, 0%)', None), ('cmyk(0, 0, 0, 0)', None)])\ndef test_color(css_source, expected_result):\n    result = parse_color_string(css_source)\n    if isinstance(result, tuple):\n        for (got, expected) in zip(result, expected_result):\n            assert (abs((got - expected)) < 1e-10)\n        for (i, attr) in enumerate(['red', 'green', 'blue', 'alpha']):\n            assert (getattr(result, attr) == result[i])\n    else:\n        assert (result == expected_result)\n", "label": 0}
{"function": "\n\ndef download(self, url):\n    ret = b''\n    response = self.requester.get(url, stream=True, verify=self.verify)\n    if (not response.ok):\n        raise ConanException(('Error %d downloading file %s' % (response.status_code, url)))\n    try:\n        total_length = response.headers.get('content-length')\n        if (total_length is None):\n            ret += response.content\n        else:\n            dl = 0\n            total_length = int(total_length)\n            last_progress = None\n            for data in response.iter_content(chunk_size=1024):\n                dl += len(data)\n                ret += data\n                units = progress_units(dl, total_length)\n                if (last_progress != units):\n                    if self.output:\n                        print_progress(self.output, units)\n                    last_progress = units\n        return ret\n    except Exception as e:\n        raise ConanConnectionError(('Download failed, check server, possibly try again\\n%s' % str(e)))\n", "label": 0}
{"function": "\n\ndef get_profile_fields(self, user):\n    field_data = []\n    for field_name in User._meta.additional_fields:\n        field_data.append(self.get_model_field_data(user, field_name))\n    profile_class = get_profile_class()\n    if profile_class:\n        try:\n            profile = profile_class.objects.get(user=user)\n        except ObjectDoesNotExist:\n            profile = profile_class(user=user)\n        field_names = [f.name for f in profile._meta.local_fields]\n        for field_name in field_names:\n            if (field_name in ('user', 'id')):\n                continue\n            field_data.append(self.get_model_field_data(profile, field_name))\n    return field_data\n", "label": 0}
{"function": "\n\ndef normal(self, size, avg=0.0, std=1.0, ndim=None, dtype=None, nstreams=None):\n    '\\n        Parameters\\n        ----------\\n        size\\n            Can be a list of integers or Theano variables (ex: the shape\\n            of another Theano Variable).\\n        dtype\\n            The output data type. If dtype is not specified, it will be\\n            inferred from the dtype of low and high, but will be at\\n            least as precise as floatX.\\n        nstreams\\n            Number of streams.\\n\\n        '\n    avg = as_tensor_variable(avg)\n    std = as_tensor_variable(std)\n    if (dtype is None):\n        dtype = scal.upcast(config.floatX, avg.dtype, std.dtype)\n    avg = cast(avg, dtype)\n    std = cast(std, dtype)\n    evened = False\n    constant = False\n    if (isinstance(size, tuple) and all([isinstance(i, (numpy.integer, int)) for i in size])):\n        constant = True\n        n_samples = numpy.prod(size, dtype='int64')\n        if ((n_samples % 2) == 1):\n            n_samples += 1\n            evened = True\n    else:\n        n_samples = (prod(size) + (prod(size) % 2))\n    flattened = self.uniform(size=(n_samples,), dtype=dtype, nstreams=nstreams)\n    if constant:\n        U1 = flattened[:(n_samples // 2)]\n        U2 = flattened[(n_samples // 2):]\n    else:\n        U1 = flattened[:(prod(flattened.shape) // 2)]\n        U2 = flattened[(prod(flattened.shape) // 2):]\n    sqrt_ln_U1 = sqrt(((- 2.0) * log(U1)))\n    first_half = (sqrt_ln_U1 * cos((numpy.array((2.0 * numpy.pi), dtype=dtype) * U2)))\n    second_half = (sqrt_ln_U1 * sin((numpy.array((2.0 * numpy.pi), dtype=dtype) * U2)))\n    normal_samples = join(0, first_half, second_half)\n    final_samples = None\n    if evened:\n        final_samples = normal_samples[:(- 1)]\n    elif constant:\n        final_samples = normal_samples\n    else:\n        final_samples = normal_samples[:prod(size)]\n    if (not size):\n        size = tensor.constant(size, dtype='int64')\n    final_samples = final_samples.reshape(size)\n    final_samples = (avg + (std * final_samples))\n    assert (final_samples.dtype == dtype)\n    return final_samples\n", "label": 1}
{"function": "\n\ndef test_ed25519py():\n    kp0 = ed25519py.crypto_sign_keypair(binary((' ' * 32)))\n    kp = ed25519py.crypto_sign_keypair()\n    signed = ed25519py.crypto_sign(binary('test'), kp.sk)\n    ed25519py.crypto_sign_open(signed, kp.vk)\n    try:\n        ed25519py.crypto_sign_open(signed, kp0.vk)\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign_keypair(binary((' ' * 33)))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign(binary(''), (binary(' ') * 31))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign_open(binary(''), (binary(' ') * 31))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n", "label": 1}
{"function": "\n\ndef whataremyips(bind_ip=None):\n    '\\n    Get \"our\" IP addresses (\"us\" being the set of services configured by\\n    one `*.conf` file). If our REST listens on a specific address, return it.\\n    Otherwise, if listen on \\'0.0.0.0\\' or \\'::\\' return all addresses, including\\n    the loopback.\\n\\n    :param str bind_ip: Optional bind_ip from a config file; may be IP address\\n                        or hostname.\\n    :returns: list of Strings of ip addresses\\n    '\n    if bind_ip:\n        try:\n            (_, _, _, _, sockaddr) = socket.getaddrinfo(bind_ip, None, 0, socket.SOCK_STREAM, 0, socket.AI_NUMERICHOST)[0]\n            if (sockaddr[0] not in ('0.0.0.0', '::')):\n                return [bind_ip]\n        except socket.gaierror:\n            pass\n    addresses = []\n    for interface in netifaces.interfaces():\n        try:\n            iface_data = netifaces.ifaddresses(interface)\n            for family in iface_data:\n                if (family not in (netifaces.AF_INET, netifaces.AF_INET6)):\n                    continue\n                for address in iface_data[family]:\n                    addr = address['addr']\n                    if (family == netifaces.AF_INET6):\n                        addr = expand_ipv6(addr.split('%')[0])\n                    addresses.append(addr)\n        except ValueError:\n            pass\n    return addresses\n", "label": 1}
{"function": "\n\ndef test_stream_write(self):\n    self._storage.buffer_size = ((7 * 1024) * 1024)\n    filename = self.gen_random_string()\n    content = self.gen_random_string(((8 * 1024) * 1024))\n    io = StringIOWithError(content)\n    assert (not self._storage.exists(filename))\n    try:\n        self._storage.stream_write(filename, io)\n    except IOError:\n        pass\n    assert self._storage.exists(filename)\n    try:\n        self._storage.stream_write(filename, io)\n    except IOError:\n        pass\n    io.close()\n    self._storage.remove(filename)\n    self._storage.buffer_size = ((5 * 1024) * 1024)\n    assert (not self._storage.exists(filename))\n", "label": 0}
{"function": "\n\ndef handle(self, fn_name, action, *args, **kwds):\n    self.parent.calls.append((self, fn_name, args, kwds))\n    if (action is None):\n        return None\n    elif (action == 'return self'):\n        return self\n    elif (action == 'return response'):\n        res = MockResponse(200, 'OK', {\n            \n        }, '')\n        return res\n    elif (action == 'return request'):\n        return Request('http://blah/')\n    elif action.startswith('error'):\n        code = action[(action.rfind(' ') + 1):]\n        try:\n            code = int(code)\n        except ValueError:\n            pass\n        res = MockResponse(200, 'OK', {\n            \n        }, '')\n        return self.parent.error('http', args[0], res, code, '', {\n            \n        })\n    elif (action == 'raise'):\n        raise urllib.error.URLError('blah')\n    assert False\n", "label": 1}
{"function": "\n\ndef ParseDepends(self, filename, must_exist=None, only_one=0):\n    '\\n        Parse a mkdep-style file for explicit dependencies.  This is\\n        completely abusable, and should be unnecessary in the \"normal\"\\n        case of proper SCons configuration, but it may help make\\n        the transition from a Make hierarchy easier for some people\\n        to swallow.  It can also be genuinely useful when using a tool\\n        that can write a .d file, but for which writing a scanner would\\n        be too complicated.\\n        '\n    filename = self.subst(filename)\n    try:\n        fp = open(filename, 'r')\n    except IOError:\n        if must_exist:\n            raise\n        return\n    lines = SCons.Util.LogicalLines(fp).readlines()\n    lines = [l for l in lines if (l[0] != '#')]\n    tdlist = []\n    for line in lines:\n        try:\n            (target, depends) = line.split(':', 1)\n        except (AttributeError, ValueError):\n            pass\n        else:\n            tdlist.append((target.split(), depends.split()))\n    if only_one:\n        targets = []\n        for td in tdlist:\n            targets.extend(td[0])\n        if (len(targets) > 1):\n            raise SCons.Errors.UserError((\"More than one dependency target found in `%s':  %s\" % (filename, targets)))\n    for (target, depends) in tdlist:\n        self.Depends(target, depends)\n", "label": 1}
{"function": "\n\ndef run_tests(test_labels, verbosity=1, interactive=True, extra_tests=[]):\n    \"\\n    worsk exactly as per normal test\\n    but only creates the test_db if it doesn't yet exist\\n    and does not destroy it when done\\n    tables are flushed and fixtures loaded between tests as per usual\\n    but if your schema has not changed then this saves significant amounts of time\\n    and speeds up the test cycle\\n\\n    Run the unit tests for all the test labels in the provided list.\\n    Labels must be of the form:\\n     - app.TestClass.test_method\\n        Run a single specific test method\\n     - app.TestClass\\n        Run all the test methods in a given class\\n     - app\\n        Search for doctests and unittests in the named application.\\n\\n    When looking for tests, the test runner will look in the models and\\n    tests modules for the application.\\n\\n    A list of 'extra' tests may also be provided; these tests\\n    will be added to the test suite.\\n\\n    Returns the number of tests that failed.\\n    \"\n    setup_test_environment()\n    settings.DEBUG = False\n    suite = unittest.TestSuite()\n    if test_labels:\n        for label in test_labels:\n            if ('.' in label):\n                suite.addTest(build_test(label))\n            else:\n                app = get_app(label)\n                suite.addTest(build_suite(app))\n    else:\n        for app in get_apps():\n            suite.addTest(build_suite(app))\n    for test in extra_tests:\n        suite.addTest(test)\n    suite = reorder_suite(suite, (TestCase,))\n    old_name = settings.DATABASES['default']['NAME']\n    from django.db.backends import creation\n    from django.db import connection, DatabaseError\n    if settings.DATABASES['default']['TEST_NAME']:\n        settings.DATABASES['default']['NAME'] = settings.DATABASES['default']['TEST_NAME']\n    else:\n        settings.DATABASES['default']['NAME'] = (creation.TEST_DATABASE_PREFIX + settings.DATABASES['default']['NAME'])\n    connection.settings_dict['DATABASE_NAME'] = settings.DATABASES['default']['NAME']\n    try:\n        if (settings.DATABASES['default']['ENGINE'] == 'sqlite3'):\n            if (not os.path.exists(settings.DATABASES['default']['NAME'])):\n                raise DatabaseError\n        cursor = connection.cursor()\n    except Exception:\n        settings.DATABASES['default']['NAME'] = old_name\n        connection.settings_dict['DATABASE_NAME'] = old_name\n        connection.creation.create_test_db(verbosity, autoclobber=True)\n    else:\n        connection.close()\n    settings.DATABASES['default']['SUPPORTS_TRANSACTIONS'] = connections_support_transactions()\n    result = unittest.TextTestRunner(verbosity=verbosity).run(suite)\n    settings.DATABASES['default']['NAME'] = old_name\n    connection.settings_dict['DATABASE_NAME'] = old_name\n    teardown_test_environment()\n    return (len(result.failures) + len(result.errors))\n", "label": 1}
{"function": "\n\ndef _select_range(self, multiselect, keep_anchor, node, idx):\n    'Selects a range between self._anchor and node or idx.\\n        If multiselect is True, it will be added to the selection, otherwise\\n        it will unselect everything before selecting the range. This is only\\n        called if self.multiselect is True.\\n        If keep anchor is False, the anchor is moved to node. This should\\n        always be True for keyboard selection.\\n        '\n    select = self.select_node\n    sister_nodes = self.get_selectable_nodes()\n    end = (len(sister_nodes) - 1)\n    last_node = self._anchor\n    last_idx = self._anchor_idx\n    if (last_node is None):\n        last_idx = end\n        last_node = sister_nodes[end]\n    elif ((last_idx > end) or (sister_nodes[last_idx] != last_node)):\n        try:\n            last_idx = self.get_index_of_node(last_node, sister_nodes)\n        except ValueError:\n            return\n    if ((idx > end) or (sister_nodes[idx] != node)):\n        try:\n            idx = self.get_index_of_node(node, sister_nodes)\n        except ValueError:\n            return\n    if (last_idx > idx):\n        (last_idx, idx) = (idx, last_idx)\n    if (not multiselect):\n        self.clear_selection()\n    for item in sister_nodes[last_idx:(idx + 1)]:\n        select(item)\n    if keep_anchor:\n        self._anchor = last_node\n        self._anchor_idx = last_idx\n    else:\n        self._anchor = node\n        self._anchor_idx = idx\n    self._last_selected_node = node\n    self._last_node_idx = idx\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    if (not hasattr(request, 'user')):\n        raise ImproperlyConfigured(\"The Django remote user auth middleware requires the authentication middleware to be installed.  Edit your MIDDLEWARE_CLASSES setting to insert 'django.contrib.auth.middleware.AuthenticationMiddleware' before the RemoteUserMiddleware class.\")\n    try:\n        username = request.META[self.header]\n    except KeyError:\n        if request.user.is_authenticated():\n            self._remove_invalid_user(request)\n        return\n    if request.user.is_authenticated():\n        if (request.user.get_username() == self.clean_username(username, request)):\n            return\n        else:\n            self._remove_invalid_user(request)\n    user = auth.authenticate(remote_user=username)\n    if user:\n        request.user = user\n        auth.login(request, user)\n", "label": 0}
{"function": "\n\ndef run_until_complete(self, *coroutines):\n    \"\\n        Executes a set of coroutines that communicate between each other. Each\\n        one is, in order, passed the output of the previous coroutine until\\n        one is exhausted. If a coroutine does not initially yield data (that\\n        is, its first action is to receive data), the calling code should prime\\n        it by using the 'server' decorator on this class.\\n\\n        Once a coroutine is exhausted, the method performs a final check to\\n        ensure that all other coroutines are exhausted. This ensures that all\\n        assertions in those coroutines got executed.\\n        \"\n    looping_coroutines = itertools.cycle(coroutines)\n    data = None\n    for coro in looping_coroutines:\n        try:\n            data = coro.send(data)\n        except StopIteration:\n            break\n    for coro in coroutines:\n        try:\n            next(coro)\n        except StopIteration:\n            continue\n        else:\n            pytest.fail(('Coroutine %s not exhausted' % coro))\n", "label": 0}
{"function": "\n\ndef __init__(self, values, critical=False):\n    assert (len(values) > 0), 'Need at least one value.'\n    new = []\n    for value in values:\n        if isinstance(value, Certificate):\n            super(IssuerAltName, self).__init__('issuerAltName', critical, 'issuer:copy', issuer=value)\n            break\n        elif isinstance(value, crypto.X509):\n            super(IssuerAltName, self).__init__('issuerAltName', critical, 'issuer:copy', issuer=Certificate(_cert=value))\n            break\n        pre = guess_altname_prefix(value)\n        new.append('{}:{}'.format(pre, value))\n    else:\n        super(IssuerAltName, self).__init__('issuerAltName', critical, ','.join(new))\n", "label": 0}
{"function": "\n\ndef _build_one(self, url):\n    'Get the given ``url`` from the app and write the matching file.\\n        '\n    client = self.app.test_client()\n    base_url = self.app.config['FREEZER_BASE_URL']\n    with conditional_context(self.url_for_logger, self.log_url_for):\n        with conditional_context(patch_url_for(self.app), self.app.config['FREEZER_RELATIVE_URLS']):\n            response = client.get(url, follow_redirects=True, base_url=base_url)\n    ignore_404 = self.app.config['FREEZER_IGNORE_404_NOT_FOUND']\n    if (response.status_code != 200):\n        if ((response.status_code == 404) and ignore_404):\n            warnings.warn(('Ignored %r on URL %s' % (response.status, url)), NotFoundWarning, stacklevel=3)\n        else:\n            raise ValueError(('Unexpected status %r on URL %s' % (response.status, url)))\n    destination_path = self.urlpath_to_filepath(url)\n    filename = os.path.join(self.root, *destination_path.split('/'))\n    if (not self.app.config['FREEZER_IGNORE_MIMETYPE_WARNINGS']):\n        basename = os.path.basename(filename)\n        (guessed_type, guessed_encoding) = mimetypes.guess_type(basename)\n        if (not guessed_type):\n            guessed_type = self.app.config['FREEZER_DEFAULT_MIMETYPE']\n        if (not (guessed_type == response.mimetype)):\n            warnings.warn(('Filename extension of %r (type %s) does not match Content-Type: %s' % (basename, guessed_type, response.content_type)), MimetypeMismatchWarning, stacklevel=3)\n    dirname = os.path.dirname(filename)\n    if (not os.path.isdir(dirname)):\n        os.makedirs(dirname)\n    content = response.data\n    if os.path.isfile(filename):\n        with open(filename, 'rb') as fd:\n            previous_content = fd.read()\n    else:\n        previous_content = None\n    if (content != previous_content):\n        with open(filename, 'wb') as fd:\n            fd.write(content)\n    response.close()\n    return filename\n", "label": 1}
{"function": "\n\ndef test_mapkeys_with_correct_parameters_set_length_extra(self):\n    set_name = 'a'\n    for _ in range(100):\n        set_name = (set_name + 'a')\n    policy = {\n        \n    }\n    try:\n        TestMapKeysIndex.client.index_map_keys_create('test', set_name, 'string_map', aerospike.INDEX_STRING, 'test_string_map_index', policy)\n        assert False\n    except e.InvalidRequest as exception:\n        assert (exception.code == 4)\n    except Exception as exception:\n        assert (type(exception) == e.InvalidRequest)\n", "label": 0}
{"function": "\n\n@periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)\ndef _poll_shelved_instances(self, context):\n    if (CONF.shelved_offload_time <= 0):\n        return\n    filters = {\n        'vm_state': vm_states.SHELVED,\n        'task_state': None,\n        'host': self.host,\n    }\n    shelved_instances = objects.InstanceList.get_by_filters(context, filters=filters, expected_attrs=['system_metadata'], use_slave=True)\n    to_gc = []\n    for instance in shelved_instances:\n        sys_meta = instance.system_metadata\n        shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])\n        if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):\n            to_gc.append(instance)\n    for instance in to_gc:\n        try:\n            instance.task_state = task_states.SHELVING_OFFLOADING\n            instance.save(expected_task_state=(None,))\n            self.shelve_offload_instance(context, instance, clean_shutdown=False)\n        except Exception:\n            LOG.exception(_LE('Periodic task failed to offload instance.'), instance=instance)\n", "label": 0}
{"function": "\n\ndef calibrate(args):\n    '\\n    %prog calibrate calibrate.JPG boxsize\\n\\n    Calibrate pixel-inch ratio and color adjustment.\\n    - `calibrate.JPG` is the photo containig a colorchecker\\n    - `boxsize` is the measured size for the boxes on printed colorchecker, in\\n      squared centimeter (cm2) units\\n    '\n    xargs = args[2:]\n    p = OptionParser(calibrate.__doc__)\n    (opts, args, iopts) = add_seeds_options(p, args)\n    if (len(args) != 2):\n        sys.exit((not p.print_help()))\n    (imagefile, boxsize) = args\n    boxsize = float(boxsize)\n    colorcheckerfile = op.join(datadir, 'colorchecker.txt')\n    colorchecker = []\n    expected = 0\n    for row in open(colorcheckerfile):\n        boxes = row.split()\n        colorchecker.append(boxes)\n        expected += len(boxes)\n    folder = op.split(imagefile)[0]\n    objects = seeds(([imagefile, '--outdir={0}'.format(folder)] + xargs))\n    nseeds = len(objects)\n    logging.debug('Found {0} boxes (expected={1})'.format(nseeds, expected))\n    assert ((expected - 4) <= nseeds <= (expected + 4)), 'Number of boxes drastically different from {0}'.format(expected)\n    boxes = [t.area for t in objects]\n    reject = reject_outliers(boxes)\n    retained_boxes = [b for (r, b) in zip(reject, boxes) if (not r)]\n    mbox = np.median(retained_boxes)\n    pixel_cm_ratio = ((mbox / boxsize) ** 0.5)\n    logging.debug('Median box size: {0} pixels. Measured box size: {1} cm2'.format(mbox, boxsize))\n    logging.debug('Pixel-cm ratio: {0}'.format(pixel_cm_ratio))\n    xs = [t.x for t in objects]\n    ys = [t.y for t in objects]\n    idx_xs = get_kmeans(xs, 6)\n    idx_ys = get_kmeans(ys, 4)\n    for (xi, yi, s) in zip(idx_xs, idx_ys, objects):\n        s.rank = (yi, xi)\n    objects.sort(key=(lambda x: x.rank))\n    colormap = []\n    for s in objects:\n        (x, y) = s.rank\n        (observed, expected) = (s.rgb, rgb_to_triplet(colorchecker[x][y]))\n        colormap.append((np.array(observed), np.array(expected)))\n    tr0 = np.eye(3).flatten()\n    ((print >> sys.stderr), 'Initial distance:', total_error(tr0, colormap))\n    tr = fmin(total_error, tr0, args=(colormap,))\n    tr.resize((3, 3))\n    ((print >> sys.stderr), 'RGB linear transform:\\n', tr)\n    calib = {\n        'PixelCMratio': pixel_cm_ratio,\n        'RGBtransform': tr.tolist(),\n    }\n    jsonfile = op.join(folder, 'calibrate.json')\n    fw = must_open(jsonfile, 'w')\n    ((print >> fw), json.dumps(calib, indent=4))\n    fw.close()\n    logging.debug('Calibration specs written to `{0}`.'.format(jsonfile))\n    return jsonfile\n", "label": 1}
{"function": "\n\ndef _to_list(self, bounds):\n    if isinstance(bounds, Expr):\n        if isinstance(bounds.type, ScalarT):\n            return [bounds]\n        else:\n            assert isinstance(bounds.type, TupleT), ('Expected tuple but got %s : %s' % (bounds, bounds.type))\n            return self.tuple_elts(bounds)\n    elif isinstance(bounds, (int, long)):\n        return [bounds]\n    else:\n        assert isinstance(bounds, (tuple, list))\n        return bounds\n", "label": 0}
{"function": "\n\ndef iptables_get_source_chains(blockade_id):\n    'Get a map of blockade chains IDs -> list of IPs targeted at them\\n\\n    For figuring out which container is in which partition\\n    '\n    result = {\n        \n    }\n    if (not blockade_id):\n        raise ValueError('invalid blockade_id')\n    lines = iptables_get_chain_rules('FORWARD')\n    for line in lines:\n        parts = line.split()\n        if (len(parts) < 4):\n            continue\n        try:\n            partition_index = parse_partition_index(blockade_id, parts[0])\n        except ValueError:\n            continue\n        source = parts[3]\n        if source:\n            result[source] = partition_index\n    return result\n", "label": 0}
{"function": "\n\ndef test_sync_get(self):\n    request = '&'.join([('assignmentId=debug%s' % self.assignment_id), ('workerId=debug%s' % self.worker_id), ('hitId=debug%s' % self.hit_id), 'mode=debug'])\n    rv = self.app.get(('/exp?%s' % request))\n    uniqueid = ('debug%s:debug%s' % (self.worker_id, self.assignment_id))\n    rv = self.app.put(('/sync/%s' % uniqueid))\n    uniqueid = ('debug%s:debug%s' % (self.worker_id, self.assignment_id))\n    rv = self.app.get(('/sync/%s' % uniqueid))\n    response = json.loads(rv.data)\n    assert (response.get('assignmentId', '') == ('debug%s' % self.assignment_id))\n    assert (response.get('workerId', '') == ('debug%s' % self.worker_id))\n    assert (response.get('hitId', '') == ('debug%s' % self.hit_id))\n    assert (response.get('condition', None) == 0)\n    assert (response.get('counterbalance', None) == 0)\n    assert (response.get('bonus', None) == 0.0)\n", "label": 0}
{"function": "\n\ndef test_invalid_list_fromjson(self):\n    jlist = 'invalid'\n    try:\n        parse(('{\"a\": \"%s\"}' % jlist), {\n            'a': ArrayType(str),\n        }, False)\n        assert False\n    except Exception as e:\n        assert isinstance(e, InvalidInput)\n        assert (e.fieldname == 'a')\n        assert (e.value == jlist)\n        assert (e.msg == ('Value not a valid list: %s' % jlist))\n", "label": 0}
{"function": "\n\ndef save_wf(wf, output_file, mode='soma-workflow'):\n    'Save the workflow in a file.\\n\\n    Support simple JSON commands list (cmd-list) or soma-workflow.\\n\\n    Parameters:\\n    ----------\\n    wf : tuple (cmd-dict, dependancies),\\n        Workflow to save.\\n    output_file : str,\\n        filename for the workflow.\\n    mode : str in [\"soma-workflow\", \"cmd_list\"],\\n           optional (default=\"soma-workflow\")\\n        format to save the workflow.\\n    '\n    cmd = wf[0]\n    dep_orig = wf[1]\n    if (mode == 'soma-workflow'):\n        from soma_workflow.client import Job, Workflow, Helper\n        for (k, v) in cmd.iteritems():\n            cmd[k] = Job(command=v, name=k)\n        dep = [(cmd[a], cmd[b]) for (a, b) in dep_orig]\n        jobs = np.asarray(cmd.values())[np.argsort(cmd.keys())]\n        workflow = Workflow(jobs=jobs.tolist(), dependencies=dep)\n        Helper.serialize(output_file, workflow)\n        return workflow\n    elif (mode == 'cmd-list'):\n        import json\n        for (k, v) in cmd.iteritems():\n            cmd[k] = ' '.join(v)\n        with open(output_file, 'w') as fd:\n            json.dump(dict(cmd=cmd, dep=dep_orig), fd, indent=True)\n        return cmd\n    else:\n        raise TypeError(\"Invalid workflow mode '{}'\".format(mode))\n", "label": 0}
{"function": "\n\ndef right_context(token_list, token, context_size, idx):\n    right_window = []\n    if (idx >= len(token_list)):\n        return ['</s>' for i in range(context_size)]\n    assert (token_list[idx] == token), 'Token in token list: {}, index: {}, token provided in parameters: {}'.format(token_list[idx], idx, token)\n    for i in range((idx + 1), ((idx + context_size) + 1)):\n        if (i > (len(token_list) - 1)):\n            right_window.append('</s>')\n        else:\n            right_window.append(token_list[i])\n    return right_window\n", "label": 0}
{"function": "\n\n@register_canonicalize\n@register_specialize\n@gof.local_optimizer([Subtensor])\ndef local_subtensor_merge(node):\n    '\\n    Refactored optimization to deal with all cases of tensor merging.\\n    Given a subgraph of the form Subtensor(Subtensor(u)), the optimization\\n    expresses all slices in a canonical form, and then merges them together.\\n\\n    '\n    if isinstance(node.op, Subtensor):\n        u = node.inputs[0]\n        if (u.owner and isinstance(u.owner.op, Subtensor)):\n            x = u.owner.inputs[0]\n            slices1 = get_idx_list(u.owner.inputs, u.owner.op.idx_list)\n            slices2 = get_idx_list(node.inputs, node.op.idx_list)\n            try:\n                xshape = node.fgraph.shape_feature.shape_of[x]\n                ushape = node.fgraph.shape_feature.shape_of[u]\n            except AttributeError:\n                xshape = x.shape\n                ushape = u.shape\n            merged_slices = []\n            pos_2 = 0\n            pos_1 = 0\n            while ((pos_1 < len(slices1)) and (pos_2 < len(slices2))):\n                slice1 = slices1[pos_1]\n                if (type(slice1) is slice):\n                    merged_slices.append(merge_two_slices(slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]))\n                    pos_2 += 1\n                else:\n                    merged_slices.append(slice1)\n                pos_1 += 1\n            if (pos_2 < len(slices2)):\n                merged_slices += slices2[pos_2:]\n            else:\n                merged_slices += slices1[pos_1:]\n            merged_slices = make_constant(merged_slices)\n            subtens = Subtensor(merged_slices)\n            sl_ins = Subtensor.collapse(merged_slices, (lambda x: isinstance(x, T.Variable)))\n            out = subtens(x, *sl_ins)\n            copy_stack_trace([node.outputs[0], node.inputs[0]], out)\n            return [out]\n", "label": 1}
{"function": "\n\ndef connect(self):\n    'Connect to the MongoDB server and register the documents from\\n        :attr:`registered_documents`. If you set ``MONGODB_USERNAME`` and\\n        ``MONGODB_PASSWORD`` then you will be authenticated at the\\n        ``MONGODB_DATABASE``. You can also enable timezone awareness if\\n        you set to True ``MONGODB_TZ_AWARE`.\\n        '\n    if (self.app is None):\n        raise RuntimeError('The flask-mongokit extension was not init to the current application.  Please make sure to call init_app() first.')\n    ctx = ctx_stack.top\n    mongokit_connection = getattr(ctx, 'mongokit_connection', None)\n    if (mongokit_connection is None):\n        ctx.mongokit_connection = Connection(host=ctx.app.config.get('MONGODB_HOST'), port=ctx.app.config.get('MONGODB_PORT'), slave_okay=ctx.app.config.get('MONGODB_SLAVE_OKAY'), tz_aware=ctx.app.config.get('MONGODB_TZ_AWARE', False))\n        ctx.mongokit_connection.register(self.registered_documents)\n    mongokit_database = getattr(ctx, 'mongokit_database', None)\n    if (mongokit_database is None):\n        ctx.mongokit_database = Database(ctx.mongokit_connection, ctx.app.config.get('MONGODB_DATABASE'))\n    if (ctx.app.config.get('MONGODB_USERNAME') is not None):\n        try:\n            auth_success = ctx.mongokit_database.authenticate(ctx.app.config.get('MONGODB_USERNAME'), ctx.app.config.get('MONGODB_PASSWORD'))\n        except OperationFailure:\n            auth_success = False\n        if (not auth_success):\n            raise AuthenticationIncorrect('Server authentication failed')\n", "label": 0}
{"function": "\n\ndef on_done_password(self, value):\n    'Callback for the password show_input_panel'\n    self.github_password = value\n    try:\n        api = GitHubApi(self.base_uri, debug=self.debug)\n        self.github_token = api.get_token(self.github_user, self.github_password, self.github_one_time_password)\n        self.github_password = self.github_one_time_password = None\n        self.accounts[self.active_account]['github_token'] = self.github_token\n        self.settings.set('accounts', self.accounts)\n        sublime.save_settings('GitHub.sublime-settings')\n        self.gistapi = GitHubApi(self.base_uri, self.github_token, debug=self.debug)\n        try:\n            if self.callback:\n                sublime.error_message(self.MSG_TOKEN_SUCCESS)\n                callback = self.callback\n                self.callback = None\n                sublime.set_timeout(callback, 50)\n        except AttributeError:\n            pass\n    except GitHubApi.OTPNeededException:\n        sublime.set_timeout(self.get_one_time_password, 50)\n    except GitHubApi.UnauthorizedException:\n        sublime.error_message(self.ERR_UNAUTHORIZED)\n        sublime.set_timeout(self.get_username, 50)\n    except GitHubApi.UnknownException as e:\n        sublime.error_message(e.message)\n", "label": 0}
{"function": "\n\ndef get_case_groups_in_domain(domain, limit=None, skip=None):\n\n    def _get_case_groups_generator(domain_name):\n        for case_group in get_docs_in_domain_by_class(domain_name, CommCareCaseGroup):\n            (yield case_group)\n    case_groups_generator = _get_case_groups_generator(domain)\n    if (skip is not None):\n        try:\n            for _ in range(skip):\n                next(case_groups_generator)\n        except StopIteration:\n            pass\n    if (limit is not None):\n        return list((next(case_groups_generator) for _ in range(limit)))\n    return list(case_groups_generator)\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('grid_type', ALL_GRID_TYPES)\ndef test_append_two_quantities(self, grid_type):\n    g = self.grid[grid_type]\n    g['density'] = []\n    g['energy'] = []\n    g['density'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    assert g['density'][0]\n    assert g['density'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['density'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert g['energy'][0]\n    assert g['energy'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['energy'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert (g.n_dust == 1)\n", "label": 1}
{"function": "\n\ndef Time2Internaldate(date_time):\n    '\\'\"DD-Mmm-YYYY HH:MM:SS +HHMM\"\\' = Time2Internaldate(date_time)\\n\\n    Convert \\'date_time\\' to IMAP4 INTERNALDATE representation.\\n\\n    The date_time argument can be a number (int or float) representing\\n    seconds since epoch (as returned by time.time()), a 9-tuple\\n    representing local time, an instance of time.struct_time (as\\n    returned by time.localtime()), an aware datetime instance or a\\n    double-quoted string.  In the last case, it is assumed to already\\n    be in the correct format.'\n    from datetime import datetime, timezone, timedelta\n    if isinstance(date_time, (int, float)):\n        tt = time.localtime(date_time)\n    elif isinstance(date_time, tuple):\n        try:\n            gmtoff = date_time.tm_gmtoff\n        except AttributeError:\n            if time.daylight:\n                dst = date_time[8]\n                if (dst == (- 1)):\n                    dst = time.localtime(time.mktime(date_time))[8]\n                gmtoff = (- (time.timezone, time.altzone)[dst])\n            else:\n                gmtoff = (- time.timezone)\n        delta = timedelta(seconds=gmtoff)\n        dt = datetime(*date_time[:6], tzinfo=timezone(delta))\n    elif isinstance(date_time, datetime):\n        if (date_time.tzinfo is None):\n            raise ValueError('date_time must be aware')\n        dt = date_time\n    elif (isinstance(date_time, str) and ((date_time[0], date_time[(- 1)]) == ('\"', '\"'))):\n        return date_time\n    else:\n        raise ValueError('date_time not of a known type')\n    fmt = '\"%d-{}-%Y %H:%M:%S %z\"'.format(MonthNames[dt.month])\n    return dt.strftime(fmt)\n", "label": 1}
{"function": "\n\ndef legacy_encrypt(claims, jwk, adata='', add_header=None, alg='RSA-OAEP', enc='A128CBC-HS256', rng=get_random_bytes, compression=None, version=None):\n    header = dict(((add_header or {\n        \n    }).items() + [('enc', enc), ('alg', alg)]))\n    if (version == 1):\n        claims = deepcopy(claims)\n        assert (jose._TEMP_VER_KEY not in claims)\n        claims[jose._TEMP_VER_KEY] = version\n        assert (jose._TEMP_VER_KEY not in header)\n        header[jose._TEMP_VER_KEY] = version\n    plaintext = jose.json_encode(claims)\n    if (compression is not None):\n        header['zip'] = compression\n        try:\n            (compress, _) = jose.COMPRESSION[compression]\n        except KeyError:\n            raise jose.Error('Unsupported compression algorithm: {}'.format(compression))\n        plaintext = compress(plaintext)\n    (((cipher, _), key_size), ((hash_fn, _), hash_mod)) = jose.JWA[enc]\n    iv = rng(AES.block_size)\n    if (version == 1):\n        encryption_key = rng(hash_mod.digest_size)\n        cipher_key = encryption_key[((- hash_mod.digest_size) / 2):]\n        mac_key = encryption_key[:((- hash_mod.digest_size) / 2)]\n    else:\n        encryption_key = rng(((key_size // 8) + hash_mod.digest_size))\n        cipher_key = encryption_key[:(- hash_mod.digest_size)]\n        mac_key = encryption_key[(- hash_mod.digest_size):]\n    ciphertext = cipher(plaintext, cipher_key, iv)\n    hash = hash_fn(jose._jwe_hash_str(ciphertext, iv, adata, version), mac_key, hash_mod)\n    ((cipher, _), _) = jose.JWA[alg]\n    encryption_key_ciphertext = cipher(encryption_key, jwk)\n    return jose.JWE(*map(jose.b64encode_url, (jose.json_encode(header), encryption_key_ciphertext, iv, ciphertext, jose.auth_tag(hash))))\n", "label": 1}
{"function": "\n\ndef process_m2m(self, instance, field):\n    auto_created_through_model = False\n    through = get_remote_field(field).through\n    auto_created_through_model = through._meta.auto_created\n    if auto_created_through_model:\n        return self.process_field(instance, field)\n    kwargs = {\n        \n    }\n    if (field.name in self.generate_m2m):\n        related_fks = [fk for fk in through._meta.fields if (isinstance(fk, related.ForeignKey) and (get_remote_field_to(fk) is get_remote_field_to(field)))]\n        self_fks = [fk for fk in through._meta.fields if (isinstance(fk, related.ForeignKey) and (get_remote_field_to(fk) is self.model))]\n        assert (len(related_fks) == 1)\n        assert (len(self_fks) == 1)\n        related_fk = related_fks[0]\n        self_fk = self_fks[0]\n        (min_count, max_count) = self.generate_m2m[field.name]\n        intermediary_model = generators.MultipleInstanceGenerator(AutoFixture(through, field_values={\n            self_fk.name: instance,\n            related_fk.name: generators.InstanceGenerator(autofixture.get(get_remote_field_to(field))),\n        }), min_count=min_count, max_count=max_count, **kwargs).generate()\n", "label": 1}
{"function": "\n\ndef test_hide_after():\n    for f in formats:\n        try:\n            pass_through('AABB', hide, 'after', pass_through, 'CCDD', hide, 'reset', raise_error)\n        except:\n            result = format(f)\n            assert ('AABB' in result)\n            assert ('CCDD' not in result)\n            assert ('raise_error' in result)\n        else:\n            assert 0\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('stages', [1, 3, 12])\ndef test_Pipeline(dump_vcd, stages):\n    pipeline = Pipeline(stages)\n    pipeline.vcd_file = dump_vcd\n    i = (- 1)\n    for i in range((stages - 1)):\n        pipeline.advance()\n        pipeline.insert(i)\n        assert (not pipeline.ready())\n    pipeline.advance()\n    pipeline.insert((i + 1))\n    assert pipeline.ready()\n    for i in range(stages):\n        assert pipeline.ready()\n        assert (pipeline.remove() == i)\n        pipeline.advance()\n    assert (not pipeline.ready())\n", "label": 1}
{"function": "\n\ndef _submit_batch(self, batch, stream_name):\n    if (len(batch) < 1):\n        return\n    sorted_batch = sorted(batch, key=itemgetter('timestamp'), reverse=False)\n    kwargs = dict(logGroupName=self.log_group, logStreamName=stream_name, logEvents=sorted_batch)\n    if (self.sequence_tokens[stream_name] is not None):\n        kwargs['sequenceToken'] = self.sequence_tokens[stream_name]\n    try:\n        response = self.cwl_client.put_log_events(**kwargs)\n    except ClientError as e:\n        if (e.response.get('Error', {\n            \n        }).get('Code') in ('DataAlreadyAcceptedException', 'InvalidSequenceTokenException')):\n            kwargs['sequenceToken'] = e.response['Error']['Message'].rsplit(' ', 1)[(- 1)]\n            response = self.cwl_client.put_log_events(**kwargs)\n        else:\n            raise\n    if ('rejectedLogEventsInfo' in response):\n        raise Exception('Failed to deliver logs: {}'.format(response))\n", "label": 0}
{"function": "\n\ndef test_free(self):\n    '\\n        Make test on free() function\\n        '\n    x = T.vector('x')\n    func = function([x], (x + 1))\n    func.fn.allow_gc = False\n    func([1])\n    check_list = []\n    for (key, val) in iteritems(func.fn.storage_map):\n        if (not isinstance(key, theano.gof.Constant)):\n            check_list.append(val)\n    assert any([val[0] for val in check_list])\n    func.free()\n    for (key, val) in iteritems(func.fn.storage_map):\n        if (not isinstance(key, theano.gof.Constant)):\n            assert (val[0] is None)\n", "label": 0}
{"function": "\n\ndef reboot(name, path=None):\n    \"\\n    Reboot a container.\\n\\n\\n    path\\n        path to the container parent\\n        default: /var/lib/lxc (system default)\\n\\n        .. versionadded:: 2015.8.0\\n\\n    CLI Examples:\\n\\n    .. code-block:: bash\\n\\n        salt 'minion' lxc.reboot myvm\\n\\n    \"\n    ret = {\n        'result': True,\n        'changes': {\n            \n        },\n        'comment': '{0} rebooted'.format(name),\n    }\n    does_exist = exists(name, path=path)\n    if (does_exist and (state(name, path=path) == 'running')):\n        try:\n            stop(name, path=path)\n        except (SaltInvocationError, CommandExecutionError) as exc:\n            ret['comment'] = 'Unable to stop container: {0}'.format(exc)\n            ret['result'] = False\n            return ret\n    if (does_exist and (state(name, path=path) != 'running')):\n        try:\n            start(name, path=path)\n        except (SaltInvocationError, CommandExecutionError) as exc:\n            ret['comment'] = 'Unable to stop container: {0}'.format(exc)\n            ret['result'] = False\n            return ret\n    ret['changes'][name] = 'rebooted'\n    return ret\n", "label": 0}
{"function": "\n\ndef to_internal_value(self, data):\n    '\\n        Range instances <- Dicts of primitive datatypes.\\n        '\n    if html.is_html_input(data):\n        data = html.parse_html_dict(data)\n    if (not isinstance(data, dict)):\n        self.fail('not_a_dict', input_type=type(data).__name__)\n    validated_dict = {\n        \n    }\n    for key in ('lower', 'upper'):\n        try:\n            value = data.pop(key)\n        except KeyError:\n            continue\n        validated_dict[six.text_type(key)] = self.child.run_validation(value)\n    for key in ('bounds', 'empty'):\n        try:\n            value = data.pop(key)\n        except KeyError:\n            continue\n        validated_dict[six.text_type(key)] = value\n    if data:\n        self.fail('too_much_content', extra=', '.join(map(str, data.keys())))\n    return self.range_type(**validated_dict)\n", "label": 1}
{"function": "\n\n@glacier_connect\n@log_class_call('Listing vaults.', 'Listing vaults complete.')\ndef lsvault(self, limit=None):\n    \"\\n        Lists available vaults.\\n\\n        :returns: List of vault descriptions.\\n\\n            .. code-block:: python\\n\\n                [{u'CreationDate': u'2012-09-20T14:29:14.710Z',\\n                  u'LastInventoryDate': u'2012-10-01T02:10:12.497Z',\\n                  u'NumberOfArchives': 15,\\n                  u'SizeInBytes': 33932739443L,\\n                  u'VaultARN': u'arn:aws:glacier:us-east-1:012345678901:vaults/your_vault_name',\\n                  u'VaultName': u'your_vault_name'},\\n                  ...\\n                ]\\n\\n        :rtype: list\\n        :raises: :py:exc:`glacier.glacierexception.CommunicationException`,\\n                 :py:exc:`glacier.glacierexception.ResponseException`\\n        \"\n    marker = None\n    vault_list = []\n    while True:\n        try:\n            response = self.glacierconn.list_vaults(marker=marker)\n        except boto.glacier.exceptions.UnexpectedHTTPResponseError as e:\n            raise ResponseException('Failed to recieve vault list.', cause=self._decode_error_message(e.body), code=e.code)\n        vault_list += response.copy()['VaultList']\n        marker = response.copy()['Marker']\n        if (limit and (len(vault_list) >= limit)):\n            vault_list = vault_list[:limit]\n            break\n        if (not marker):\n            break\n    return vault_list\n", "label": 0}
{"function": "\n\ndef test_msg2obj():\n    am = dict(x=1)\n    ao = Message(am)\n    assert (ao.x == am['x'])\n    am['y'] = dict(z=1)\n    ao = Message(am)\n    assert (ao.y.z == am['y']['z'])\n    (k1, k2) = ('y', 'z')\n    assert (ao[k1][k2] == am[k1][k2])\n    am2 = dict(ao)\n    assert (am['x'] == am2['x'])\n    assert (am['y']['z'] == am2['y']['z'])\n", "label": 0}
{"function": "\n\ndef check_link(result, left, right):\n    assert isinstance(result, core.component_link.ComponentLink)\n    if isinstance(left, ComponentID):\n        assert (left in result.get_from_ids())\n    if isinstance(right, ComponentID):\n        assert (right in result.get_from_ids())\n", "label": 0}
{"function": "\n\ndef client_config(path, env_var='SALT_CLIENT_CONFIG', defaults=None):\n    \"\\n    Load Master configuration data\\n\\n    Usage:\\n\\n    .. code-block:: python\\n\\n        import salt.config\\n        master_opts = salt.config.client_config('/etc/salt/master')\\n\\n    Returns a dictionary of the Salt Master configuration file with necessary\\n    options needed to communicate with a locally-running Salt Master daemon.\\n    This function searches for client specific configurations and adds them to\\n    the data from the master configuration.\\n\\n    This is useful for master-side operations like\\n    :py:class:`~salt.client.LocalClient`.\\n    \"\n    if (defaults is None):\n        defaults = DEFAULT_MASTER_OPTS\n    xdg_dir = salt.utils.xdg.xdg_config_dir()\n    if os.path.isdir(xdg_dir):\n        client_config_dir = xdg_dir\n        saltrc_config_file = 'saltrc'\n    else:\n        client_config_dir = os.path.expanduser('~')\n        saltrc_config_file = '.saltrc'\n    opts = {\n        'token_file': defaults.get('token_file', os.path.join(client_config_dir, 'salt_token')),\n    }\n    opts.update(master_config(path, defaults=defaults))\n    saltrc_config = os.path.join(client_config_dir, saltrc_config_file)\n    opts.update(load_config(saltrc_config, env_var, saltrc_config))\n    if ('token_file' in opts):\n        opts['token_file'] = os.path.abspath(os.path.expanduser(opts['token_file']))\n    if os.path.isfile(opts['token_file']):\n        expire = opts.get('token_expire', 43200)\n        if ((os.stat(opts['token_file']).st_mtime + expire) > time.mktime(time.localtime())):\n            with salt.utils.fopen(opts['token_file']) as fp_:\n                opts['token'] = fp_.read().strip()\n    if (opts['interface'] == '0.0.0.0'):\n        opts['interface'] = '127.0.0.1'\n    if ('master_uri' not in opts):\n        opts['master_uri'] = 'tcp://{ip}:{port}'.format(ip=salt.utils.ip_bracket(opts['interface']), port=opts['ret_port'])\n    _validate_opts(opts)\n    return opts\n", "label": 1}
{"function": "\n\n@access.user\n@loadmodel(model='item', level=AccessType.WRITE)\n@describeRoute(Description('Set geospatial fields on an item.').notes('Set geospatial fields to null to delete them.').param('id', 'The ID of the item.', paramType='path').param('body', 'A JSON object containing the geospatial fields to add.', paramType='body').errorResponse('ID was invalid.').errorResponse('Invalid JSON was passed in request body.').errorResponse('Geospatial key name was invalid.').errorResponse('Geospatial field did not contain valid GeoJSON.').errorResponse('Write access was denied for the item.', 403))\ndef setGeospatial(self, item, params):\n    \"\\n        Set geospatial data on an item.\\n\\n        :param item: item on which to set geospatial data.\\n        :type item: dict[str, unknown]\\n        :param params: parameters to the API call, unused.\\n        :type params: dict[str, unknown]\\n        :returns: filtered fields of the item with geospatial data appended to\\n                 its 'geo' field.\\n        :rtype : dict[str, unknown]\\n        :raise RestException: on malformed, forbidden, or unauthorized API call.\\n        \"\n    geospatial = self.getBodyJson()\n    for (k, v) in six.viewitems(geospatial):\n        if (('.' in k) or (k[0] == '$')):\n            raise RestException(('Geospatial key name %s must not contain a period or begin with a dollar sign.' % k))\n        if v:\n            try:\n                GeoJSON.to_instance(v, strict=True)\n            except ValueError:\n                raise RestException(('Geospatial field with key %s does not contain valid GeoJSON: %s' % (k, v)))\n    if (GEOSPATIAL_FIELD not in item):\n        item[GEOSPATIAL_FIELD] = dict()\n    item[GEOSPATIAL_FIELD].update(six.viewitems(geospatial))\n    keys = [k for (k, v) in six.viewitems(item[GEOSPATIAL_FIELD]) if (v is None)]\n    for key in keys:\n        del item[GEOSPATIAL_FIELD][key]\n    item = self.model('item').updateItem(item)\n    return self._filter(item)\n", "label": 1}
{"function": "\n\ndef register(request, invitation_key, wrong_key_template='invitation/wrong_invitation_key.html', redirect_to_if_authenticated='/', success_url=None, form_class=RegistrationFormInvitation, template_name='registration/registration_form.html', extra_context=None):\n    '\\n    Allow a new user to register via invitation.\\n\\n    Send invitation email and then redirect to success URL if the\\n    invitation form is valid. Redirect named URL ``invitation_unavailable``\\n    on InvitationError. Render invitation form template otherwise. Sends\\n    registration.signals.user_registered after creating the user.\\n\\n    **Required arguments:**\\n\\n    :invitation_key:\\n        An invitation key in the form of ``[\\\\da-e]{40}``\\n\\n    **Optional arguments:**\\n\\n    :wrong_key_template:\\n        Template to be used when an invalid invitation key is supplied.\\n        Default value is ``invitation/wrong_invitation_key.html``.\\n\\n    :redirect_to_if_authenticated:\\n        URL to be redirected when an authenticated user calls this view.\\n        Defaults value is ``/``\\n\\n    :success_url:\\n        The URL to redirect to on successful registration. Default value is\\n        ``None``, ``invitation_registered`` will be resolved in this case.\\n\\n    :form_class:\\n        A form class to use for registration. Takes the invited email as first\\n        argument to its constructor.\\n\\n    :template_name:\\n        A custom template to use. Default value is\\n        ``registration/registration_form.html``.\\n\\n    :extra_context:\\n        A dictionary of variables to add to the template context. Any\\n        callable object in this dictionary will be called to produce\\n        the end result which appears in the context.\\n\\n    **Templates:**\\n\\n    ``invitation/invitation_form.html`` or ``template_name`` keyword\\n    argument as the *main template*.\\n\\n    ``invitation/wrong_invitation_key.html`` or ``wrong_key_template`` keyword\\n    argument as the *wrong key template*.\\n\\n    **Context:**\\n\\n    ``RequestContext`` instances are used rendering both templates. Context,\\n    in addition to ``extra_context``, contains:\\n\\n    For wrong key template\\n        :invitation_key: supplied invitation key\\n\\n    For main template\\n        :form:\\n            The registration form.\\n    '\n    if request.user.is_authenticated():\n        return HttpResponseRedirect(redirect_to_if_authenticated)\n    try:\n        invitation = Invitation.objects.find(invitation_key)\n    except Invitation.DoesNotExist:\n        context = apply_extra_context(RequestContext(request), extra_context)\n        return render_to_response(wrong_key_template, {\n            'invitation_key': invitation_key,\n        }, context_instance=context)\n    if (request.method == 'POST'):\n        form = form_class(invitation.email, request.POST, request.FILES)\n        if form.is_valid():\n            new_user = form.save()\n            invitation.mark_accepted(new_user)\n            user_registered.send(sender='invitation', user=new_user, request=request)\n            return HttpResponseRedirect((success_url or reverse('invitation_registered')))\n    else:\n        form = form_class(invitation.email)\n    context = apply_extra_context(RequestContext(request), extra_context)\n    return render_to_response(template_name, {\n        'form': form,\n    }, context_instance=context)\n", "label": 0}
{"function": "\n\ndef __call__(self, environ, start_response):\n    match = self._mapper.match(environ['PATH_INFO'])\n    if (not match):\n        return self._not_found(environ, start_response)\n    controller_name = match['controller']\n    action_name = match['action']\n    controller_action = (controller_name, action_name)\n    del match['controller']\n    del match['action']\n    environ['concurrence.application'] = self\n    request = Request(environ)\n    response = Response(content_type=self.default_content_type, charset=self.default_charset)\n    with self._scoped_request.set(request):\n        with self._scoped_response.set(response):\n            if (not (controller_action in self._filter_chain)):\n\n                def last(next, *args, **kwargs):\n                    return self.call_controller(controller_name, action_name, *args, **kwargs)\n                filter_chain = []\n                for (i, filter) in enumerate((self._filters.get(controller_action, []) + [last, None])):\n\n                    def create_next(_i, _filter):\n\n                        def next(*args, **kwargs):\n                            return _filter(filter_chain[(_i + 1)], *args, **kwargs)\n                        return next\n                    filter_chain.append(create_next(i, filter))\n                self._filter_chain[controller_action] = filter_chain\n            result = self._filter_chain[controller_action][0](**match)\n    if (type(result) == str):\n        response.body = result\n    elif (type(result) == unicode):\n        response.unicode_body = result\n    elif (result is None):\n        response.body = ''\n    elif (type(result) == type(response)):\n        response = result\n    else:\n        assert False, ('result must be None, str, unicode or response object, found: %s' % type(result))\n    return response(environ, start_response)\n", "label": 1}
{"function": "\n\ndef Buffer(self, geometries, distances, unit=None, unionResults=False, inSR=None, outSR=None, bufferSR=None):\n    'The buffer operation is performed on a geometry service resource.\\n           The result of this operation is buffer polygons at the specified\\n           distances for the input geometry array. An option is available to\\n           union buffers at each distance.'\n    if isinstance(geometries, geometry.Geometry):\n        geometries = [geometries]\n    if isinstance(distances, (list, tuple)):\n        distances = ','.join((str(distance) for distance in distances))\n    geometry_types = set([x.__geometry_type__ for x in geometries])\n    assert (len(geometry_types) == 1), 'Too many geometry types'\n    geo_json = json.dumps({\n        'geometryType': list(geometry_types)[0],\n        'geometries': [geo._json_struct_without_sr for geo in geometries],\n    })\n    if (inSR is None):\n        inSR = geometries[0].spatialReference.wkid\n    if (outSR is None):\n        outSR = geometries[0].spatialReference.wkid\n    if (bufferSR is None):\n        bufferSR = geometries[0].spatialReference.wkid\n    return self._get_subfolder('buffer', GeometryResult, {\n        'geometries': geo_json,\n        'distances': distances,\n        'unit': unit,\n        'unionResults': unionResults,\n        'inSR': inSR,\n        'outSR': outSR,\n        'bufferSR': bufferSR,\n    })\n", "label": 1}
{"function": "\n\ndef status(self):\n    columns = '{0:<14}{1:<19}{2:<34}{3:<11}\\n'\n    result = columns.format('ID', 'NODE', 'COMMAND', 'STATUS')\n    for container in self.containers:\n        container_id = self.containers[container].desc['container_id']\n        node_name = ((container[:15] + '..') if (len(container) > 17) else container)\n        command = ''\n        status = 'OFF'\n        try:\n            state = docker.Client().inspect_container(container_id)\n            command = ''.join((([state['Path']] + state['Args']) + [' ']))\n            command = ((command[:30] + '..') if (len(command) > 32) else command)\n            if state['State']['Running']:\n                status = 'Running'\n        except HTTPError:\n            status = 'Destroyed'\n        result += columns.format(container_id, node_name, command, status)\n    return result.rstrip('\\n')\n", "label": 0}
{"function": "\n\ndef render(self, name, value, attrs):\n    encoded = value\n    final_attrs = self.build_attrs(attrs)\n    if ((not encoded) or encoded.startswith(UNUSABLE_PASSWORD_PREFIX)):\n        summary = mark_safe(('<strong>%s</strong>' % ugettext('No password set.')))\n    else:\n        try:\n            hasher = identify_hasher(encoded)\n        except ValueError:\n            summary = mark_safe(('<strong>%s</strong>' % ugettext('Invalid password format or unknown hashing algorithm.')))\n        else:\n            summary = format_html_join('', '<strong>{0}</strong>: {1} ', ((ugettext(key), value) for (key, value) in hasher.safe_summary(encoded).items()))\n    return format_html('<div{0}>{1}</div>', flatatt(final_attrs), summary)\n", "label": 0}
{"function": "\n\ndef _validatePolicy(self, policy):\n    '\\n        Validate a policy JSON object.  Only a limited set of keys is\\n        supported, and each of them has a restricted data type.\\n\\n        :param policy: JSON object to validate.  This may also be a Python\\n                           dictionary as if the JSON was already decoded.\\n        :returns: a validate policy dictionary.\\n        '\n    if (not isinstance(policy, dict)):\n        try:\n            policy = json.loads(policy)\n        except ValueError:\n            raise RestException('The policy parameter must be JSON.')\n    if (not isinstance(policy, dict)):\n        raise RestException('The policy parameter must be a dictionary.')\n    validKeys = []\n    for key in dir(self):\n        if key.startswith('_validate_'):\n            validKeys.append(key.split('_validate_', 1)[1])\n    for key in list(policy):\n        if key.startswith('_'):\n            del policy[key]\n    for key in policy:\n        if (key not in validKeys):\n            raise RestException(('%s is not a valid quota policy key.  Valid keys are %s.' % (key, ', '.join(sorted(validKeys)))))\n        funcName = ('_validate_' + key)\n        policy[key] = getattr(self, funcName)(policy[key])\n    return policy\n", "label": 1}
{"function": "\n\ndef test_grad_1d(self):\n    subi = 0\n    data = numpy.asarray(rand(2, 3), dtype=self.dtype)\n    n = self.shared(data)\n    z = scal.constant(subi)\n    t = n[z:, z]\n    gn = theano.tensor.grad(theano.tensor.sum(theano.tensor.exp(t)), n)\n    f = inplace_func([], gn, mode=self.mode)\n    topo = f.maker.fgraph.toposort()\n    topo_ = [node for node in topo if (not isinstance(node.op, self.ignore_topo))]\n    if (not self.fast_compile):\n        assert (len(topo_) == 6)\n    assert (numpy.sum([isinstance(node.op, self.inc_sub) for node in topo_]) == 1)\n    assert (numpy.sum([isinstance(node.op, self.sub) for node in topo_]) == 1)\n    gval = f()\n    good = numpy.zeros_like(data)\n    good[subi:, subi] = numpy.exp(data[subi:, subi])\n    self.assertTrue(numpy.allclose(gval, good), (gval, good))\n", "label": 0}
{"function": "\n\ndef mprint(m, mname, sufix='', header=None):\n    if sufix:\n        filename = 'print_{mname}_{sufix}.txt'.format(mname=mname, sufix=sufix)\n    else:\n        filename = 'print_{mname}.txt'.format(mname=mname)\n    with open(filename, 'w') as f:\n        if header:\n            f.write(header)\n\n        def myprint(sth):\n            f.write((str(sth).strip() + '\\n'))\n        if sufix:\n            myprint('{mname}_{sufix}'.format(mname=mname, sufix=sufix))\n        else:\n            myprint('{mname}'.format(mname=mname))\n        for ((i, j), v) in np.ndenumerate(m):\n            if v:\n                myprint('{mname}[row+{i},col+{j}] += {v}'.format(mname=mname, v=v, i=i, j=j))\n", "label": 0}
{"function": "\n\ndef test_masking_existing_user_attribute_when_verbose_causes_warning(self):\n    warns = []\n\n    def catch_warning(*args, **kwargs):\n        warns.append(args[0])\n    old_showwarning = warnings.showwarning\n    warnings.showwarning = catch_warning\n    self.config.verbose = True\n    with self.context.use_with_user_mode():\n        self.context.thing = 'stuff'\n        self.context._push()\n        self.context.thing = 'other stuff'\n    warnings.showwarning = old_showwarning\n    print(repr(warns))\n    assert warns, 'warns is empty!'\n    warning = warns[0]\n    assert isinstance(warning, runner.ContextMaskWarning), 'warning is not a ContextMaskWarning'\n    info = warning.args[0]\n    assert info.startswith('user code'), (\"%r doesn't start with 'user code'\" % info)\n    assert (\"'thing'\" in info), ('%r not in %r' % (\"'thing'\", info))\n    assert ('tutorial' in info), ('\"tutorial\" not in %r' % (info,))\n", "label": 0}
{"function": "\n\n@wsgi.response(204)\n@extensions.expected_errors(404)\ndef delete(self, req, id):\n    'Delete an server group.'\n    context = _authorize_context(req)\n    try:\n        sg = objects.InstanceGroup.get_by_uuid(context, id)\n    except nova.exception.InstanceGroupNotFound as e:\n        raise webob.exc.HTTPNotFound(explanation=e.format_message())\n    quotas = objects.Quotas(context=context)\n    (project_id, user_id) = objects.quotas.ids_from_server_group(context, sg)\n    try:\n        quotas.reserve(project_id=project_id, user_id=user_id, server_groups=(- 1))\n    except Exception:\n        quotas = None\n        LOG.exception(_LE('Failed to update usages deallocating server group'))\n    try:\n        sg.destroy()\n    except nova.exception.InstanceGroupNotFound as e:\n        if quotas:\n            quotas.rollback()\n        raise webob.exc.HTTPNotFound(explanation=e.format_message())\n    if quotas:\n        quotas.commit()\n", "label": 0}
{"function": "\n\ndef add_vote(self, v, force_replace=False):\n    if (v in self.lockset):\n        return\n    self.log('rm.adding', vote=v, received_proposal=self.proposal)\n    try:\n        success = self.lockset.add(v, force_replace)\n    except InvalidVoteError:\n        self.cm.tracked_protocol_failures.append(InvalidVoteEvidence(None, v))\n        return\n    if self.lockset.is_valid:\n        self.log('lockset is valid', ls=self.lockset)\n        if ((not self.proposal) and self.lockset.has_noquorum):\n            self.cm.tracked_protocol_failures.append(FailedToProposeEvidence(None, self.lockset))\n    return success\n", "label": 0}
{"function": "\n\ndef python(self, options):\n    import code\n    imported_objects = {\n        \n    }\n    try:\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter\n        readline.set_completer(rlcompleter.Completer(imported_objects).complete)\n        readline_doc = getattr(readline, '__doc__', '')\n        if ((readline_doc is not None) and ('libedit' in readline_doc)):\n            readline.parse_and_bind('bind ^I rl_complete')\n        else:\n            readline.parse_and_bind('tab:complete')\n    if (not options['no_startup']):\n        for pythonrc in (os.environ.get('PYTHONSTARTUP'), '~/.pythonrc.py'):\n            if (not pythonrc):\n                continue\n            pythonrc = os.path.expanduser(pythonrc)\n            if (not os.path.isfile(pythonrc)):\n                continue\n            try:\n                with open(pythonrc) as handle:\n                    exec(compile(handle.read(), pythonrc, 'exec'), imported_objects)\n            except NameError:\n                pass\n    code.interact(local=imported_objects)\n", "label": 1}
{"function": "\n\ndef paginate_queryset(self, queryset, request, view=None):\n    '\\n        Paginate a queryset if required, either returning a\\n        page object, or `None` if pagination is not configured for this view.\\n        '\n    page_size = self.get_page_size(request)\n    if (not page_size):\n        return None\n    paginator = self.django_paginator_class(queryset, page_size)\n    page_number = request.query_params.get(self.page_query_param, 1)\n    if (page_number in self.last_page_strings):\n        page_number = paginator.num_pages\n    try:\n        self.page = paginator.page(page_number)\n    except InvalidPage as exc:\n        msg = self.invalid_page_message.format(page_number=page_number, message=six.text_type(exc))\n        raise NotFound(msg)\n    if ((paginator.num_pages > 1) and (self.template is not None)):\n        self.display_page_controls = True\n    self.request = request\n    return list(self.page)\n", "label": 0}
{"function": "\n\ndef _do_test_lock_externally(self):\n    'We can lock across multiple processes.'\n\n    def lock_files(handles_dir):\n        with lockutils.lock('external', 'test-', external=True):\n            handles = []\n            for n in range(50):\n                path = os.path.join(handles_dir, ('file-%s' % n))\n                handles.append(open(path, 'w'))\n            count = 0\n            for handle in handles:\n                try:\n                    fcntl.flock(handle, (fcntl.LOCK_EX | fcntl.LOCK_NB))\n                    count += 1\n                    fcntl.flock(handle, fcntl.LOCK_UN)\n                except IOError:\n                    os._exit(2)\n                finally:\n                    handle.close()\n            self.assertEqual(50, count)\n    handles_dir = tempfile.mkdtemp()\n    try:\n        children = []\n        for n in range(50):\n            pid = os.fork()\n            if pid:\n                children.append(pid)\n            else:\n                try:\n                    lock_files(handles_dir)\n                finally:\n                    os._exit(0)\n        for child in children:\n            (pid, status) = os.waitpid(child, 0)\n            if pid:\n                self.assertEqual(0, status)\n    finally:\n        if os.path.exists(handles_dir):\n            shutil.rmtree(handles_dir, ignore_errors=True)\n", "label": 0}
{"function": "\n\ndef put_file(self, key, file, *args, **kwargs):\n    hm = self.__new_hmac(key)\n    bufsize = (1024 * 1024)\n    if isinstance(file, str):\n        with open(file, 'rb+') as source:\n            while True:\n                buf = source.read(bufsize)\n                hm.update(buf)\n                if (len(buf) < bufsize):\n                    break\n            source.write(hm.digest())\n        return self._dstore.put_file(key, file, *args, **kwargs)\n    else:\n        tmpfile = tempfile.NamedTemporaryFile(delete=False)\n        try:\n            while True:\n                buf = file.read(bufsize)\n                hm.update(buf)\n                tmpfile.write(buf)\n                if (len(buf) < bufsize):\n                    break\n            tmpfile.write(hm.digest())\n            tmpfile.close()\n            return self._dstore.put_file(key, tmpfile.name, *args, **kwargs)\n        finally:\n            os.unlink(tmpfile.name)\n", "label": 0}
{"function": "\n\ndef getCovMatrix(self, x=None, z=None, mode=None):\n    self.checkInputGetCovMatrix(x, z, mode)\n    Q = self.para[0]\n    if (mode == 'self_test'):\n        (nn, D) = z.shape\n    else:\n        (nn, D) = x.shape\n    assert (Q == old_div(len(self.hyp), (1 + (2 * D))))\n    w = np.exp(self.hyp[:Q])\n    m = np.exp(np.reshape(self.hyp[Q:(Q + (Q * D))], (D, Q)))\n    v = np.exp((2 * np.reshape(self.hyp[(Q + (Q * D)):], (D, Q))))\n    if (mode == 'self_test'):\n        d2 = np.zeros((nn, 1, D))\n    elif (mode == 'train'):\n        d2 = np.zeros((nn, nn, D))\n        for j in range(D):\n            xslice = np.atleast_2d(x[:, j]).T\n            d2[:, :, j] = spdist.cdist(xslice, xslice, 'sqeuclidean')\n    elif (mode == 'cross'):\n        d2 = np.zeros((nn, z.shape[0], D))\n        for j in range(D):\n            xslice = np.atleast_2d(x[:, j]).T\n            zslice = np.atleast_2d(z[:, j]).T\n            d2[:, :, j] = spdist.cdist(xslice, zslice, 'sqeuclidean')\n    d = np.sqrt(d2)\n    k = (lambda d2v_dm: (np.exp((((- 2) * (np.pi ** 2)) * d2v_dm[0])) * np.cos(((2 * np.pi) * d2v_dm[1]))))\n    km = (lambda dm: ((((- 2) * np.pi) * np.tan(((2 * np.pi) * dm))) * dm))\n    kv = (lambda d2v: ((- d2v) * ((2 * np.pi) ** 2)))\n    A = 0.0\n    c = 1.0\n    qq = list(range(Q))\n    for q in qq:\n        C = (w[q] * c)\n        for j in range(D):\n            C = (C * k(((d2[:, :, j] * v[(j, q)]), (d[:, :, j] * m[(j, q)]))))\n            A = (A + C)\n    return A\n", "label": 1}
{"function": "\n\ndef handle(self, request, context):\n    url = reverse('horizon:project:application_policy:index')\n    try:\n        policyclassifier_id = self.initial['policyclassifier_id']\n        protocol = context.get('protocol').lower()\n        if (protocol in PROTOCOL_MAP):\n            context['protocol'] = PROTOCOL_MAP[protocol]\n        elif (protocol == 'any'):\n            context['protocol'] = None\n        if (('port_range' in context) and (context['port_range'] == '')):\n            context['port_range'] = None\n        if context.get('name'):\n            context['name'] = html.escape(context['name'])\n        if context.get('description'):\n            context['description'] = html.escape(context['description'])\n        client.policyclassifier_update(self.request, policyclassifier_id, **context)\n        messages.success(request, _('Policy classifier successfully updated.'))\n        return http.HttpResponseRedirect(url)\n    except Exception:\n        exceptions.handle(request, _('Unable to update policy classifier.'), redirect=url)\n", "label": 1}
{"function": "\n\ndef force_text(s, encoding='utf-8', strings_only=False, errors='strict'):\n    \"\\n    Similar to smart_text, except that lazy instances are resolved to\\n    strings, rather than kept as lazy objects.\\n\\n    If strings_only is True, don't convert (some) non-string-like objects.\\n    \"\n    if isinstance(s, text_type):\n        return s\n    if (strings_only and is_protected_type(s)):\n        return s\n    try:\n        if (not isinstance(s, string_types)):\n            if hasattr(s, '__unicode__'):\n                s = s.__unicode__()\n            elif (not PY2):\n                if isinstance(s, bytes):\n                    s = text_type(s, encoding, errors)\n                else:\n                    s = text_type(s)\n            else:\n                s = text_type(bytes(s), encoding, errors)\n        else:\n            s = s.decode(encoding, errors)\n    except UnicodeDecodeError as e:\n        if (not isinstance(s, Exception)):\n            raise UnicodeDecodeError(*e.args)\n        else:\n            s = ' '.join([force_text(arg, encoding, strings_only, errors) for arg in s])\n    return s\n", "label": 1}
{"function": "\n\n@click.command(context_settings=CONTEXT_SETTINGS, cls=ConfigurableLoader)\n@click.version_option(version=__version__, message='%(prog)s %(version)s')\n@click.option('-d', '--debug/--no-debug', default=False, help='Drop into the debugger if the command execution raises an exception.')\n@click.option('--help-all', is_flag=True, is_eager=True, expose_value=False, callback=ConfigurableLoader.show_help_all, help='Show this message including hidden subcommands.')\n@click.pass_context\ndef main(ctx, debug):\n    if debug:\n\n        def exception_handler(type, value, traceback):\n            click.secho('\\nAn exception occurred while executing the requested command:', fg='red')\n            hr(fg='red')\n            sys.__excepthook__(type, value, traceback)\n            click.secho('\\nAs requested I will now drop you inside an interactive debugging session:', fg='red')\n            hr(fg='red')\n            pdb.post_mortem(traceback)\n        sys.excepthook = exception_handler\n    try:\n        integration_helper = ShellIntegrationHelper(os.environ['LANCET_SHELL_HELPER'])\n    except KeyError:\n        integration_helper = WarnIntegrationHelper()\n    if os.path.exists(PROJECT_CONFIG):\n        config = load_config(PROJECT_CONFIG)\n    else:\n        config = load_config()\n    ctx.obj = Lancet(config, integration_helper)\n    ctx.obj.call_on_close = ctx.call_on_close\n    ctx.call_on_close(integration_helper.close)\n    sentry_dsn = config.get('lancet', 'sentry_dsn')\n    if (sentry_dsn and (not debug)):\n        if (not raven):\n            click.secho('You provided a Sentry DSN but the raven module is not installed. Sentry logging will not be enabled.', fg='yellow')\n        else:\n            sentry_client = get_sentry_client(sentry_dsn)\n\n            def exception_handler(type, value, traceback):\n                settings_diff = diff_config(load_config(DEFAULT_CONFIG, defaults=False), config, exclude=set([('lancet', 'sentry_dsn')]))\n                sys.__excepthook__(type, value, traceback)\n                if (type in IGNORED_EXCEPTIONS):\n                    return\n                click.echo()\n                hr(fg='yellow')\n                click.secho('\\nAs requested, I am sending details about this error to Sentry, please report the following ID when seeking support:')\n                error_id = sentry_client.captureException((type, value, traceback), extra={\n                    'settings': as_dict(settings_diff),\n                    'working_dir': os.getcwd(),\n                })[0]\n                click.secho('\\n    {}\\n'.format(error_id), fg='yellow')\n            sys.excepthook = exception_handler\n", "label": 0}
{"function": "\n\ndef process_limit(start, page, limit):\n    try:\n        limit = int(limit)\n        if ((start is not None) and (page is not None)):\n            raise ValueError('Can not specify _start and _page at the same time')\n        if (start is not None):\n            start = int(start)\n        elif (page is not None):\n            start = (int(page) * limit)\n        else:\n            start = 0\n        if ((limit < 0) or (start < 0)):\n            raise ValueError('_limit/_page or _limit/_start can not be < 0')\n    except (ValueError, TypeError) as e:\n        raise ValueError(e)\n    return (start, limit)\n", "label": 1}
{"function": "\n\ndef calculate_hash(self, callback=None):\n    cookie_string = ''\n    try:\n        if self.blacklist:\n            string_with_spaces = re.sub(self.cookie_regex, '', self.request.headers['Cookie']).strip()\n            cookie_string = ''.join(string_with_spaces.split(' '))\n        else:\n            cookies_matrix = re.findall(self.cookie_regex, self.request.headers['Cookie'])\n            for cookie_tuple in cookies_matrix:\n                for item in cookie_tuple:\n                    if item:\n                        cookie_string += item.strip()\n    except KeyError:\n        pass\n    request_mod = ((self.request.method + self.request.url) + self.request.version)\n    request_mod = ((request_mod + self.request.body) + cookie_string)\n    try:\n        request_mod = (request_mod + self.request.headers['User-Agent'])\n    except KeyError:\n        pass\n    try:\n        request_mod = (request_mod + self.request.headers['Sec-Websocket-Key'])\n    except KeyError:\n        pass\n    md5_hash = hashlib.md5()\n    md5_hash.update(request_mod)\n    self.request_hash = md5_hash.hexdigest()\n    self.file_path = os.path.join(self.cache_dir, self.request_hash)\n    if callback:\n        callback(self.request_hash)\n", "label": 1}
{"function": "\n\ndef __get__(self, instance, cls=None):\n    \"\\n        Get the related instance through the reverse relation.\\n\\n        With the example above, when getting ``place.restaurant``:\\n\\n        - ``self`` is the descriptor managing the ``restaurant`` attribute\\n        - ``instance`` is the ``place`` instance\\n        - ``instance_type`` in the ``Place`` class (we don't need it)\\n\\n        Keep in mind that ``Restaurant`` holds the foreign key to ``Place``.\\n        \"\n    if (instance is None):\n        return self\n    try:\n        rel_obj = getattr(instance, self.cache_name)\n    except AttributeError:\n        related_pk = instance._get_pk_val()\n        if (related_pk is None):\n            rel_obj = None\n        else:\n            filter_args = self.related.field.get_forward_related_filter(instance)\n            try:\n                rel_obj = self.get_queryset(instance=instance).get(**filter_args)\n            except self.related.related_model.DoesNotExist:\n                rel_obj = None\n            else:\n                setattr(rel_obj, self.related.field.get_cache_name(), instance)\n        setattr(instance, self.cache_name, rel_obj)\n    if (rel_obj is None):\n        raise self.RelatedObjectDoesNotExist(('%s has no %s.' % (instance.__class__.__name__, self.related.get_accessor_name())))\n    else:\n        return rel_obj\n", "label": 0}
{"function": "\n\ndef update_index_images(namespace, repository, data_arg):\n    path = store.index_images_path(namespace, repository)\n    sender = flask.current_app._get_current_object()\n    try:\n        images = {\n            \n        }\n        data = (json.loads(data_arg.decode('utf8')) + store.get_json(path))\n        for i in data:\n            iid = i['id']\n            if ((iid in images) and ('checksum' in images[iid])):\n                continue\n            i_data = {\n                'id': iid,\n            }\n            for key in ['checksum']:\n                if (key in i):\n                    i_data[key] = i[key]\n            images[iid] = i_data\n        data = images.values()\n        store.put_json(path, data)\n        signals.repository_updated.send(sender, namespace=namespace, repository=repository, value=data)\n    except exceptions.FileNotFoundError:\n        signals.repository_created.send(sender, namespace=namespace, repository=repository, value=json.loads(data_arg.decode('utf8')))\n        store.put_content(path, data_arg)\n", "label": 0}
{"function": "\n\ndef test_sqlite_backend():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with SqliteKeyBackend(loop, conf) as h:\n        h.set_key('test', {\n            'permission': {\n                \n            },\n        })\n        with pytest.raises(KeyConflict):\n            h.set_key('test', {\n                'permission': {\n                    \n                },\n            })\n        assert (h.has_key('test') == True)\n        key = h.get_key('test')\n        assert (key == {\n            'key': 'test',\n            'permission': {\n                \n            },\n        })\n        key = h.delete_key('test')\n        with pytest.raises(KeyNotFound):\n            key = h.get_key('test')\n        h.set_key('test', {\n            'permission': {\n                \n            },\n        })\n        h.set_key('test1', {\n            'permission': {\n                \n            },\n        }, 'test')\n        assert (h.has_key('test1') == True)\n        assert (h.all_subkeys('test') == [{\n            'key': 'test1',\n            'permission': {\n                \n            },\n        }])\n        assert (len(h.all_keys()) == 2)\n        assert (h.all_keys() == ['test', 'test1'])\n        assert (h.all_keys(include_key=True) == [{\n            'key': 'test',\n            'permission': {\n                \n            },\n        }, {\n            'key': 'test1',\n            'permission': {\n                \n            },\n        }])\n", "label": 1}
{"function": "\n\ndef call_driver(self, action, network, **action_kwargs):\n    'Invoke an action on a DHCP driver instance.'\n    LOG.debug('Calling driver for network: %(net)s action: %(action)s', {\n        'net': network.id,\n        'action': action,\n    })\n    try:\n        driver = self.dhcp_driver_cls(self.conf, network, self._process_monitor, self.dhcp_version, self.plugin_rpc)\n        getattr(driver, action)(**action_kwargs)\n        return True\n    except exceptions.Conflict:\n        LOG.warning(_LW('Unable to %(action)s dhcp for %(net_id)s: there is a conflict with its current state; please check that the network and/or its subnet(s) still exist.'), {\n            'net_id': network.id,\n            'action': action,\n        })\n    except Exception as e:\n        if (getattr(e, 'exc_type', '') != 'IpAddressGenerationFailure'):\n            self.schedule_resync(e, network.id)\n        if ((isinstance(e, oslo_messaging.RemoteError) and (e.exc_type == 'NetworkNotFound')) or isinstance(e, exceptions.NetworkNotFound)):\n            LOG.debug('Network %s has been deleted.', network.id)\n        else:\n            LOG.exception(_LE('Unable to %(action)s dhcp for %(net_id)s.'), {\n                'net_id': network.id,\n                'action': action,\n            })\n", "label": 0}
{"function": "\n\ndef initial_validation(request, prefix):\n    \"\\n    Returns the related model instance and post data to use in the\\n    comment/rating views below.\\n\\n    Both comments and ratings have a ``prefix_ACCOUNT_REQUIRED``\\n    setting. If this is ``True`` and the user is unauthenticated, we\\n    store their post data in their session, and redirect to login with\\n    the view's url (also defined by the prefix arg) as the ``next``\\n    param. We can then check the session data once they log in,\\n    and complete the action authenticated.\\n\\n    On successful post, we pass the related object and post data back,\\n    which may have come from the session, for each of the comments and\\n    ratings view functions to deal with as needed.\\n    \"\n    post_data = request.POST\n    login_required_setting_name = (prefix.upper() + 'S_ACCOUNT_REQUIRED')\n    posted_session_key = ('unauthenticated_' + prefix)\n    redirect_url = ''\n    if getattr(settings, login_required_setting_name, False):\n        if (not request.user.is_authenticated()):\n            request.session[posted_session_key] = request.POST\n            error(request, _('You must be logged in. Please log in or sign up to complete this action.'))\n            redirect_url = ('%s?next=%s' % (settings.LOGIN_URL, reverse(prefix)))\n        elif (posted_session_key in request.session):\n            post_data = request.session.pop(posted_session_key)\n    if (not redirect_url):\n        model_data = post_data.get('content_type', '').split('.', 1)\n        if (len(model_data) != 2):\n            return HttpResponseBadRequest()\n        try:\n            model = apps.get_model(*model_data)\n            obj = model.objects.get(id=post_data.get('object_pk', None))\n        except (TypeError, ObjectDoesNotExist, LookupError):\n            redirect_url = '/'\n    if redirect_url:\n        if request.is_ajax():\n            return HttpResponse(dumps({\n                'location': redirect_url,\n            }))\n        else:\n            return redirect(redirect_url)\n    return (obj, post_data)\n", "label": 1}
{"function": "\n\ndef ssl_options_to_context(ssl_options):\n    'Try to convert an ``ssl_options`` dictionary to an\\n    `~ssl.SSLContext` object.\\n\\n    The ``ssl_options`` dictionary contains keywords to be passed to\\n    `ssl.wrap_socket`.  In Python 3.2+, `ssl.SSLContext` objects can\\n    be used instead.  This function converts the dict form to its\\n    `~ssl.SSLContext` equivalent, and may be used when a component which\\n    accepts both forms needs to upgrade to the `~ssl.SSLContext` version\\n    to use features like SNI or NPN.\\n    '\n    if isinstance(ssl_options, dict):\n        assert all(((k in _SSL_CONTEXT_KEYWORDS) for k in ssl_options)), ssl_options\n    if ((not hasattr(ssl, 'SSLContext')) or isinstance(ssl_options, ssl.SSLContext)):\n        return ssl_options\n    context = ssl.SSLContext(ssl_options.get('ssl_version', ssl.PROTOCOL_SSLv23))\n    if ('certfile' in ssl_options):\n        context.load_cert_chain(ssl_options['certfile'], ssl_options.get('keyfile', None))\n    if ('cert_reqs' in ssl_options):\n        context.verify_mode = ssl_options['cert_reqs']\n    if ('ca_certs' in ssl_options):\n        context.load_verify_locations(ssl_options['ca_certs'])\n    if ('ciphers' in ssl_options):\n        context.set_ciphers(ssl_options['ciphers'])\n    if hasattr(ssl, 'OP_NO_COMPRESSION'):\n        context.options |= ssl.OP_NO_COMPRESSION\n    return context\n", "label": 1}
{"function": "\n\ndef __init__(self, global_conf, script, path=None, include_os_environ=True, query_string=None):\n    if global_conf:\n        raise NotImplemented('global_conf is no longer supported for CGIApplication (use make_cgi_application); please pass None instead')\n    self.script_filename = script\n    if (path is None):\n        path = os.environ.get('PATH', '').split(':')\n    self.path = path\n    if ('?' in script):\n        assert (query_string is None), (\"You cannot have '?' in your script name (%r) and also give a query_string (%r)\" % (script, query_string))\n        (script, query_string) = script.split('?', 1)\n    if (os.path.abspath(script) != script):\n        for path_dir in self.path:\n            if os.path.exists(os.path.join(path_dir, script)):\n                self.script = os.path.join(path_dir, script)\n                break\n        else:\n            raise CGIError(('Script %r not found in path %r' % (script, self.path)))\n    else:\n        self.script = script\n    self.include_os_environ = include_os_environ\n    self.query_string = query_string\n", "label": 1}
{"function": "\n\ndef generate_example_rst(app):\n    ' Generate the list of examples, as well as the contents of\\n        examples.\\n    '\n    root_dir = os.path.join(app.builder.srcdir, 'auto_examples')\n    example_dir = os.path.abspath(os.path.join(app.builder.srcdir, '..', 'examples'))\n    generated_dir = os.path.abspath(os.path.join(app.builder.srcdir, 'modules', 'generated'))\n    try:\n        plot_gallery = eval(app.builder.config.plot_gallery)\n    except TypeError:\n        plot_gallery = bool(app.builder.config.plot_gallery)\n    if (not os.path.exists(example_dir)):\n        os.makedirs(example_dir)\n    if (not os.path.exists(root_dir)):\n        os.makedirs(root_dir)\n    if (not os.path.exists(generated_dir)):\n        os.makedirs(generated_dir)\n    fhindex = open(os.path.join(root_dir, 'index.rst'), 'w')\n    fhindex.write('\\n\\n\\n.. raw:: html\\n\\n\\n    <style type=\"text/css\">\\n    div#sidebarbutton {\\n        /* hide the sidebar collapser, while ensuring vertical arrangement */\\n        display: none;\\n    }\\n    </style>\\n\\n.. _examples-index:\\n\\nExamples\\n========\\n\\n')\n    seen_backrefs = set()\n    generate_dir_rst('.', fhindex, example_dir, root_dir, plot_gallery, seen_backrefs)\n    for directory in sorted(os.listdir(example_dir)):\n        if os.path.isdir(os.path.join(example_dir, directory)):\n            generate_dir_rst(directory, fhindex, example_dir, root_dir, plot_gallery, seen_backrefs)\n    fhindex.flush()\n", "label": 0}
{"function": "\n\n@classmethod\ndef build_network_xml(cls, network_name, bridge_name, addresses=None, forward=None, ip_network_address=None, ip_network_prefixlen=None, stp=True, has_pxe_server=False, has_dhcp_server=False, dhcp_range_start=None, dhcp_range_end=None, tftp_root_dir=None):\n    'Generate network XML\\n\\n        :type network: Network\\n            :rtype : String\\n        '\n    if (addresses is None):\n        addresses = []\n    network_xml = XMLGenerator('network')\n    network_xml.name(cls._crop_name(network_name))\n    network_xml.bridge(name=bridge_name, stp=('on' if stp else 'off'), delay='0')\n    if forward:\n        network_xml.forward(mode=forward)\n    if (ip_network_address is None):\n        return str(network_xml)\n    with network_xml.ip(address=ip_network_address, prefix=ip_network_prefixlen):\n        if (has_pxe_server and tftp_root_dir):\n            network_xml.tftp(root=tftp_root_dir)\n        if has_dhcp_server:\n            with network_xml.dhcp:\n                network_xml.range(start=dhcp_range_start, end=dhcp_range_end)\n                for address in addresses:\n                    network_xml.host(mac=address['mac'], ip=address['ip'], name=address['name'])\n                if has_pxe_server:\n                    network_xml.bootp(file='pxelinux.0')\n    return str(network_xml)\n", "label": 1}
{"function": "\n\ndef cascade_visibility_down(element, visibility_mode):\n    'Sets visibility for all descendents of an element. (cascades down).'\n    links = [rel.get_accessor_name() for rel in element._meta.get_all_related_objects()]\n    for link in links:\n        objects = getattr(element, link).all()\n        for object in objects:\n            try:\n                if (visibility_mode == 'private'):\n                    if object.public:\n                        object.public = False\n                        object.save()\n                elif (visibility_mode == 'public'):\n                    if (not object.public):\n                        object.public = True\n                        object.save()\n            except Exception as e:\n                pass\n            if object._meta.get_all_related_objects():\n                cascade_visibility_down(object, visibility_mode)\n", "label": 1}
{"function": "\n\ndef checkcache(filename=None):\n    'Discard cache entries that are out of date.\\n    (This is not checked upon each call!)'\n    if (filename is None):\n        filenames = cache.keys()\n    elif (filename in cache):\n        filenames = [filename]\n    else:\n        return\n    for filename in filenames:\n        (size, mtime, lines, fullname) = cache[filename]\n        if (mtime is None):\n            continue\n        try:\n            stat = os.stat(fullname)\n        except os.error:\n            del cache[filename]\n            continue\n        if ((size != stat.st_size) or (mtime != stat.st_mtime)):\n            del cache[filename]\n", "label": 1}
{"function": "\n\ndef gather_candidates(self, context):\n    line = self.vim.current.window.cursor[0]\n    column = context['complete_position']\n    buf = self.vim.current.buffer\n    offset = ((self.vim.call('line2byte', line) + charpos2bytepos(self.vim, context['input'][:column], column)) - 1)\n    source = '\\n'.join(buf).encode()\n    process = subprocess.Popen([self.GoCodeBinary(), '-f=json', 'autocomplete', buf.name, str(offset)], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True)\n    process.stdin.write(source)\n    (stdout_data, stderr_data) = process.communicate()\n    result = loads(stdout_data.decode())\n    try:\n        if (result[1][0]['class'] == 'PANIC'):\n            error(self.vim, 'gocode panicked')\n            return []\n        if self.sort_class:\n            class_dict = {\n                'package': [],\n                'func': [],\n                'type': [],\n                'var': [],\n                'const': [],\n            }\n        out = []\n        sep = ' '\n        for complete in result[1]:\n            word = complete['name']\n            info = complete['type']\n            _class = complete['class']\n            abbr = str(((word + sep) + info)).replace(' func', '', 1)\n            kind = _class\n            if ((_class == 'package') and self.package_dot):\n                word += '.'\n            candidates = dict(word=word, abbr=abbr, kind=kind, info=info, menu=self.mark, dup=1)\n            if ((not self.sort_class) or (_class == 'import')):\n                out.append(candidates)\n            else:\n                class_dict[_class].append(candidates)\n        if self.sort_class:\n            for c in self.sort_class:\n                for x in class_dict[c]:\n                    out.append(x)\n        return out\n    except Exception:\n        return []\n", "label": 1}
{"function": "\n\n@staticmethod\ndef validateInfo(doc):\n    '\\n        Makes sure the root field is a valid absolute path and is writeable.\\n        '\n    if ('prefix' not in doc):\n        doc['prefix'] = ''\n    doc['prefix'] = doc['prefix'].strip('/')\n    if (not doc.get('bucket')):\n        raise ValidationException('Bucket must not be empty.', 'bucket')\n    if (not doc.get('readOnly')):\n        if (not doc.get('secret')):\n            raise ValidationException('Secret key must not be empty.', 'secret')\n        if (not doc.get('accessKeyId')):\n            raise ValidationException('Access key ID must not be empty.', 'accessKeyId')\n    if ('service' not in doc):\n        doc['service'] = ''\n    if (doc['service'] != ''):\n        service = re.match('^((https?)://)?([^:/]+)(:([0-9]+))?$', doc['service'])\n        if (not service):\n            raise ValidationException('The service must of the form [http[s]://](host domain)[:(port)].', 'service')\n    doc['botoConnect'] = makeBotoConnectParams(doc['accessKeyId'], doc['secret'], doc['service'])\n    conn = botoConnectS3(doc['botoConnect'])\n    if doc.get('readOnly'):\n        try:\n            conn.get_bucket(bucket_name=doc['bucket'], validate=True)\n        except Exception:\n            logger.exception('S3 assetstore validation exception')\n            raise ValidationException(('Unable to connect to bucket \"%s\".' % doc['bucket']), 'bucket')\n    else:\n        try:\n            bucket = conn.get_bucket(bucket_name=doc['bucket'], validate=True)\n            testKey = boto.s3.key.Key(bucket=bucket, name='/'.join(filter(None, (doc['prefix'], 'test'))))\n            testKey.set_contents_from_string('')\n        except Exception:\n            logger.exception('S3 assetstore validation exception')\n            raise ValidationException(('Unable to write into bucket \"%s\".' % doc['bucket']), 'bucket')\n    return doc\n", "label": 1}
{"function": "\n\ndef safe_text_dupfile(f, mode, default_encoding='UTF8'):\n    \" return a open text file object that's a duplicate of f on the\\n        FD-level if possible.\\n    \"\n    encoding = getattr(f, 'encoding', None)\n    try:\n        fd = f.fileno()\n    except Exception:\n        if (('b' not in getattr(f, 'mode', '')) and hasattr(f, 'encoding')):\n            return f\n    else:\n        newfd = os.dup(fd)\n        if ('b' not in mode):\n            mode += 'b'\n        f = os.fdopen(newfd, mode, 0)\n    return EncodedFile(f, (encoding or default_encoding))\n", "label": 0}
{"function": "\n\ndef test_tokens(db):\n    user = orm.User(name='inara')\n    db.add(user)\n    db.commit()\n    token = user.new_api_token()\n    assert any((t.match(token) for t in user.api_tokens))\n    user.new_api_token()\n    assert (len(user.api_tokens) == 2)\n    found = orm.APIToken.find(db, token=token)\n    assert found.match(token)\n    found = orm.APIToken.find(db, 'something else')\n    assert (found is None)\n    secret = 'super-secret-preload-token'\n    token = user.new_api_token(secret)\n    assert (token == secret)\n    assert (len(user.api_tokens) == 3)\n    with pytest.raises(ValueError):\n        user.new_api_token(token)\n    assert (len(user.api_tokens) == 3)\n", "label": 1}
{"function": "\n\n@resource.register('hive://.+::.+', priority=16)\ndef resource_hive_table(uri, stored_as='TEXTFILE', external=True, dshape=None, **kwargs):\n    if dshape:\n        dshape = datashape.dshape(dshape)\n    (uri, table) = uri.split('::')\n    engine = resource(uri)\n    metadata = metadata_of_engine(engine)\n    with ignoring(sa.exc.NoSuchTableError):\n        return sa.Table(table, metadata, autoload=True, autoload_with=engine)\n    if (dshape and ((not external) or (external and kwargs.get('path')))):\n        table_type = ('EXTERNAL' if external else '')\n        statement = create_hive_statement(table, dshape, db_name=engine.url.database, stored_as=stored_as, table_type=table_type, **kwargs)\n        with engine.connect() as conn:\n            conn.execute(statement)\n        return sa.Table(table, metadata, autoload=True, autoload_with=engine)\n    else:\n        return TableProxy(engine, table, stored_as)\n", "label": 0}
{"function": "\n\n@domain_admin_required\n@requires_privilege_with_fallback(privileges.OUTBOUND_SMS)\n@get_file('bulk_upload_file')\ndef upload_sms_translations(request, domain):\n    try:\n        workbook = WorkbookJSONReader(request.file)\n        translations = workbook.get_worksheet(title='translations')\n        with StandaloneTranslationDoc.get_locked_obj(domain, 'sms') as tdoc:\n            msg_ids = sorted(_MESSAGES.keys())\n            result = {\n                \n            }\n            for lang in tdoc.langs:\n                result[lang] = {\n                    \n                }\n            for row in translations:\n                for lang in tdoc.langs:\n                    if row.get(lang):\n                        msg_id = row['property']\n                        if (msg_id in msg_ids):\n                            val = row[lang]\n                            if (not isinstance(val, basestring)):\n                                val = str(val)\n                            val = val.strip()\n                            result[lang][msg_id] = val\n            tdoc.translations = result\n            tdoc.save()\n        messages.success(request, _('SMS Translations Updated.'))\n    except Exception:\n        notify_exception(request, 'SMS Upload Translations Error')\n        messages.error(request, _(\"Update failed. We're looking into it.\"))\n    return HttpResponseRedirect(reverse('sms_languages', args=[domain]))\n", "label": 1}
{"function": "\n\ndef _real_extract(self, url):\n    mobj = re.match(self._VALID_URL, url)\n    if (mobj.group('course') and mobj.group('video')):\n        course = mobj.group('course')\n        video = mobj.group('video')\n        info = {\n            'id': ((course + '_') + video),\n            'uploader': None,\n            'upload_date': None,\n        }\n        baseUrl = (('http://openclassroom.stanford.edu/MainFolder/courses/' + course) + '/videos/')\n        xmlUrl = ((baseUrl + video) + '.xml')\n        mdoc = self._download_xml(xmlUrl, info['id'])\n        try:\n            info['title'] = mdoc.findall('./title')[0].text\n            info['url'] = (baseUrl + mdoc.findall('./videoFile')[0].text)\n        except IndexError:\n            raise ExtractorError('Invalid metadata XML file')\n        return info\n    elif mobj.group('course'):\n        course = mobj.group('course')\n        info = {\n            'id': course,\n            '_type': 'playlist',\n            'uploader': None,\n            'upload_date': None,\n        }\n        coursepage = self._download_webpage(url, info['id'], note='Downloading course info page', errnote='Unable to download course info page')\n        info['title'] = self._html_search_regex('<h1>([^<]+)</h1>', coursepage, 'title', default=info['id'])\n        info['description'] = self._html_search_regex('(?s)<description>([^<]+)</description>', coursepage, 'description', fatal=False)\n        links = orderedSet(re.findall('<a href=\"(VideoPage.php\\\\?[^\"]+)\">', coursepage))\n        info['entries'] = [self.url_result(('http://openclassroom.stanford.edu/MainFolder/%s' % unescapeHTML(l))) for l in links]\n        return info\n    else:\n        info = {\n            'id': 'Stanford OpenClassroom',\n            '_type': 'playlist',\n            'uploader': None,\n            'upload_date': None,\n        }\n        info['title'] = info['id']\n        rootURL = 'http://openclassroom.stanford.edu/MainFolder/HomePage.php'\n        rootpage = self._download_webpage(rootURL, info['id'], errnote='Unable to download course info page')\n        links = orderedSet(re.findall('<a href=\"(CoursePage.php\\\\?[^\"]+)\">', rootpage))\n        info['entries'] = [self.url_result(('http://openclassroom.stanford.edu/MainFolder/%s' % unescapeHTML(l))) for l in links]\n        return info\n", "label": 0}
{"function": "\n\n@require_POST\n@csrf_exempt\n@cross_domain_post_response\n@host_check\ndef like_comment(request, comment_id):\n    try:\n        comment = Comment.objects.get(id=comment_id)\n    except Comment.DoesNotExist as e:\n        data = {\n            'placement': 'comments_container',\n            'content': str(e),\n        }\n        return HttpResponseNotFound(json.dumps(data))\n    if request.user.is_anonymous():\n        site_admin = False\n    else:\n        site_admin = bool(request.user.get_comments(comment_id))\n    if site_admin:\n        if request.user.hidden.filter(id=comment.thread.site.id):\n            return HttpResponseBadRequest((_('user is disabled on site with id %s') % comment.thread.site.id))\n    liked_comments = request.session.get('liked_comments', [])\n    disliked_comments = request.session.get('disliked_comments', [])\n    if (comment.id in disliked_comments):\n        comment.undo_dislike(request.user)\n        remove_from_session_list(request, 'disliked_comments', comment.id)\n        comment.like(request.user)\n        add_to_session_list(request, 'liked_comments', comment.id)\n    elif (comment.id not in liked_comments):\n        comment.like(request.user)\n        add_to_session_list(request, 'liked_comments', comment.id)\n    comments = Comment.objects.filter(thread=comment.thread)\n    posted_comments = request.session.get('posted_comments', [])\n    site = comment.thread.site\n    resp = render(request, 'comments.html', {\n        'comments': comments,\n        'posted_comments': posted_comments,\n        'last_posted_comment_id': (posted_comments[(- 1)] if posted_comments else None),\n        'rs_customer_id': site.rs_customer_id,\n        'all_comments': request.session.get('all_comments', False),\n        'site_admin': site_admin,\n    })\n    data = {\n        'placement': 'comments_container',\n        'content': resp.content,\n    }\n    return HttpResponse(json.dumps(data))\n", "label": 1}
{"function": "\n\ndef parse_for(tokens, name, context):\n    (first, pos) = tokens[0]\n    tokens = tokens[1:]\n    context = (('for',) + context)\n    content = []\n    assert first.startswith('for ')\n    if first.endswith(':'):\n        first = first[:(- 1)]\n    first = first[3:].strip()\n    match = in_re.search(first)\n    if (not match):\n        raise TemplateError(('Bad for (no \"in\") in %r' % first), position=pos, name=name)\n    vars = first[:match.start()]\n    if ('(' in vars):\n        raise TemplateError(('You cannot have () in the variable section of a for loop (%r)' % vars), position=pos, name=name)\n    vars = tuple([v.strip() for v in first[:match.start()].split(',') if v.strip()])\n    expr = first[match.end():]\n    while 1:\n        if (not tokens):\n            raise TemplateError('No {{endfor}}', position=pos, name=name)\n        if (isinstance(tokens[0], tuple) and (tokens[0][0] == 'endfor')):\n            return (('for', pos, vars, expr, content), tokens[1:])\n        (next_chunk, tokens) = parse_expr(tokens, name, context)\n        content.append(next_chunk)\n", "label": 1}
{"function": "\n\ndef phaseN(self, environ, query, server_env, session):\n    'Step 2: Once the consumer has redirected the user back to the\\n        callback URL you can request the access token the user has\\n        approved.'\n    client = session['client']\n    logger.debug('info: %s', query)\n    logger.debug('keyjar: %s', client.keyjar)\n    authresp = client.parse_response(AuthorizationResponse, query, sformat='dict', keyjar=client.keyjar)\n    if isinstance(authresp, ErrorResponse):\n        return (False, 'Access denied')\n    try:\n        client.id_token = authresp['id_token']\n    except:\n        pass\n    logger.debug('callback environ: %s', environ)\n    if (self.flow_type == 'code'):\n        try:\n            tokenresp = self.get_accesstoken(client, authresp)\n        except Exception as err:\n            logger.error('%s', err)\n            raise\n        if isinstance(tokenresp, ErrorResponse):\n            return (False, ('Invalid response %s.' % tokenresp['error']))\n        access_token = tokenresp['access_token']\n    else:\n        access_token = authresp['access_token']\n    userinfo = self.verify_token(client, access_token)\n    inforesp = self.get_userinfo(client, authresp, access_token)\n    if isinstance(inforesp, ErrorResponse):\n        return (False, ('Invalid response %s.' % inforesp['error']), session)\n    tot_info = userinfo.update(inforesp.to_dict())\n    logger.debug('UserInfo: %s', inforesp)\n    return (True, userinfo, access_token, client)\n", "label": 0}
{"function": "\n\ndef get_category(category_string, model=Category):\n    '\\n    Convert a string, including a path, and return the Category object\\n    '\n    model_class = get_cat_model(model)\n    category = str(category_string).strip('\\'\"')\n    category = category.strip('/')\n    cat_list = category.split('/')\n    if (len(cat_list) == 0):\n        return None\n    try:\n        categories = model_class.objects.filter(name=cat_list[(- 1)], level=(len(cat_list) - 1))\n        if ((len(cat_list) == 1) and (len(categories) > 1)):\n            return None\n        if (len(categories) == 1):\n            return categories[0]\n        else:\n            for item in categories:\n                if (item.parent.name == cat_list[(- 2)]):\n                    return item\n    except model_class.DoesNotExist:\n        return None\n", "label": 1}
{"function": "\n\ndef run(self, sync):\n    app = sync.app\n    to_sync = set()\n    with app.db.getSession() as session:\n        project = session.getProject(self.project_key)\n        repo = None\n        try:\n            repo = gitrepo.get_repo(project.name, app.config)\n        except gitrepo.GitCloneError:\n            pass\n        for change in project.open_changes:\n            if repo:\n                for revision in change.revisions:\n                    if (not (repo.hasCommit(revision.parent) and repo.hasCommit(revision.commit))):\n                        to_sync.add(change.id)\n            else:\n                to_sync.add(change.id)\n    for change_id in to_sync:\n        sync.submitTask(SyncChangeTask(change_id, priority=self.priority))\n", "label": 1}
{"function": "\n\ndef __init__(self, job_name=None, jenkins_url=None, jenkins_diff_url=None, auth_keyname=None, verify=True, cluster=None, debug_config=None, cpus=4, memory=(8 * 1024)):\n    '\\n        The JenkinsBuildStep constructor here, which is used as a base\\n        for all Jenkins builds, only accepts parameters which are used\\n        to determine where and how to schedule the builds and does not\\n        deal with any of the logic regarding what to actually schedule\\n        on those jenkins masters.\\n\\n        jenkins_url and jenkins_diff_url are either a single url or a list\\n        of possible jenkins masters for use of queueing. We don\\'t worry\\n        about which slaves to schedule them on at this level.\\n\\n        Args:\\n            auth_keyname: A key in the Changes config file that specifies the\\n                auth parameter to pass to the requests library. This could be\\n                a (username, password) tuple, in which case the requests library\\n                uses HTTPBasicAuth. For details, see http://docs.python-requests.org/en/latest/user/authentication/#basic-authentication\\n            verify (str or bool): The verify parameter to pass to the requests\\n                library for verifying SSL certificates. For details,\\n                see http://docs.python-requests.org/en/latest/user/advanced/#ssl-cert-verification\\n            cpus (int): Number of CPUs this buildstep requires to run.\\n            memory (int): Memory required for this buildstep in megabytes.\\n            cluster (Optional[str]): The Jenkins label to apply to be used to restrict the build to a subset of\\n                slaves where the master supports it.\\n            debug_config: A dictionary of debug config options. These are passed through\\n                to changes-client. There is also an infra_failures option, which takes a\\n                dictionary used to force infrastructure failures in builds. The keys of\\n                this dictionary refer to the phase (either \\'primary\\' or \\'expanded\\' if\\n                applicable), and the values are the probabilities with which\\n                a JobStep in that phase will fail.\\n                An example: \"debug_config\": {\"infra_failures\": {\"primary\": 0.5}}\\n                This will then cause an infra failure in the primary JobStep with\\n                probability 0.5.\\n        '\n    if (job_name is None):\n        raise ValueError('Missing required config: need job_name.')\n    if any(((int_field and (type(int_field) != int)) for int_field in (cpus, memory))):\n        raise ValueError('cpus and memory fields must be JSON ints')\n    if (not isinstance(jenkins_url, (list, tuple))):\n        if jenkins_url:\n            jenkins_url = [jenkins_url]\n        else:\n            jenkins_url = []\n    if (not isinstance(jenkins_diff_url, (list, tuple))):\n        if jenkins_diff_url:\n            jenkins_diff_url = [jenkins_diff_url]\n        else:\n            jenkins_diff_url = []\n    self.job_name = job_name\n    self.jenkins_urls = jenkins_url\n    self.jenkins_diff_urls = jenkins_diff_url\n    self.auth_keyname = auth_keyname\n    self.verify = verify\n    self.cluster = cluster\n    self.debug_config = (debug_config or {\n        \n    })\n    self._resources = {\n        \n    }\n    if cpus:\n        self._resources['cpus'] = cpus\n    if memory:\n        self._resources['memory'] = memory\n", "label": 1}
{"function": "\n\ndef test_equivalence():\n    preferences = DummyPrefs()\n    query = Query('')\n    query['sum_by'] = {\n        'core': [''],\n    }\n    targets = {\n        'servers.host.cpu.cpu0.irq': {\n            'id': 'servers.host.cpu.cpu0.irq',\n            'tags': {\n                'core': 'cpu0',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'irq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.cpu0.softirq': {\n            'id': 'servers.host.cpu.cpu0.softirq',\n            'tags': {\n                'core': 'cpu0',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'softirq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.cpu2.irq': {\n            'id': 'servers.host.cpu.cpu2.irq',\n            'tags': {\n                'core': 'cpu2',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'irq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.cpu2.softirq': {\n            'id': 'servers.host.cpu.cpu2.softirq',\n            'tags': {\n                'core': 'cpu2',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'softirq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.total.irq': {\n            'id': 'servers.host.cpu.total.irq',\n            'tags': {\n                'core': '_sum_',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'irq',\n                'unit': 'cpu_state',\n            },\n        },\n        'servers.host.cpu.total.softirq': {\n            'id': 'servers.host.cpu.total.softirq',\n            'tags': {\n                'core': '_sum_',\n                'plugin': 'cpu',\n                'server': 'host',\n                'target_type': 'gauge_pct',\n                'type': 'softirq',\n                'unit': 'cpu_state',\n            },\n        },\n    }\n    (graphs, _query) = g.build_from_targets(targets, query, preferences)\n    assert (len(graphs) == 1)\n    (_, graph) = graphs.popitem()\n    assert (len(graph['targets']) == 2)\n    ids = [t['id'] for t in graph['targets']]\n    assert (ids == ['servers.host.cpu.total.irq', 'servers.host.cpu.total.softirq'])\n    query = Query('core:(_sum_|cpu0|cpu2) sum by core')\n    (graphs, _query) = g.build_from_targets(targets, query, preferences)\n    assert (len(graphs) == 1)\n    (_, graph) = graphs.popitem()\n    assert (len(graph['targets']) == 2)\n    ids = [t['id'] for t in graph['targets']]\n    assert (ids == [['servers.host.cpu.cpu0.softirq', 'servers.host.cpu.cpu2.softirq'], ['servers.host.cpu.cpu0.irq', 'servers.host.cpu.cpu2.irq']])\n", "label": 1}
{"function": "\n\ndef _updateCheckResult(self, check_value, status, detail_msg, notification_msg=''):\n    '\\n        update check result in cfg table and log table\\n        '\n    if (self.check_cfg.enabled_flag != 'Y'):\n        self.logger.info(('The check item %s is disable in database. So the coming result will be not stored anymore.' % self.itm_code))\n        return\n    self.logger.info(('check_value %s' % check_value))\n    self.logger.info(('check_status %s' % status))\n    check_timestamp = datetime.now().strftime('%Y%m%d %H:%M:%S.%f')\n    try:\n        assert (status in [const.CHECK_STATUS_NORMAL, const.CHECK_STATUS_WARN, const.CHECK_STATUS_CRITICAL])\n    except Exception as ex:\n        self.logger.exception(ex)\n        raise ex\n    self.check_cfg.updateStatus(check_value, status, check_timestamp, detail_msg, notification_msg)\n    is_critical_event = self.check_cfg.is_critical_event\n    if (is_critical_event == 'Y'):\n        self.logger.warn('The alarm is a critical level alarm.')\n    is_warning_event = self.check_cfg.is_warning_event\n    if (is_warning_event == 'Y'):\n        self.logger.warn('The alarm is a warning level alarm.')\n    is_new_critical_event = self.check_cfg.is_new_critical_event\n    if (is_new_critical_event == 'Y'):\n        self.logger.warn('The alarm is a new critical level alarm.')\n    is_new_warning_event = self.check_cfg.is_new_warning_event\n    if (is_new_warning_event == 'Y'):\n        self.logger.warn('The alarm is a new warning level alarm.')\n    check_log = dashboard_check_log()\n    check_log.initCheckStatus(self.itm_code, check_timestamp, status, check_value, self.check_cfg.warning_limit, self.check_cfg.critical_limit, self.check_cfg.shadow_data, detail_msg, notification_msg, is_critical_event, is_warning_event, is_new_critical_event, is_new_warning_event)\n    check_log.insertLog()\n", "label": 1}
{"function": "\n\ndef addUser(self, user):\n    'Adds a given user to the ChannelState.  Power prefixes are handled.'\n    nick = user.lstrip('@%+&~!')\n    if (not nick):\n        return\n    while (user and (user[0] in '@%+&~!')):\n        (marker, user) = (user[0], user[1:])\n        assert user, 'Looks like my caller is passing chars, not nicks.'\n        if (marker in '@&~!'):\n            self.ops.add(nick)\n        elif (marker == '%'):\n            self.halfops.add(nick)\n        elif (marker == '+'):\n            self.voices.add(nick)\n    self.users.add(nick)\n", "label": 1}
{"function": "\n\ndef test_hide():\n    for f in formats:\n        try:\n            hide(True, raise_error)\n        except:\n            result = format(f)\n            print(result)\n            assert ('in hide_inner' not in result)\n            assert ('inner(*args, **kw)' not in result)\n        else:\n            assert 0\n", "label": 0}
{"function": "\n\ndef make_temp_dir(filename, signal=False):\n    'Creates a temporary folder and returns the joined filename'\n    try:\n        if (((testParams['user_tempdir'] is not None) and (testParams['user_tempdir'] != '')) and (testParams['actual_tempdir'] == '')):\n            testParams['actual_tempdir'] = testParams['user_tempdir']\n        if (not os.path.isdir(testParams['actual_tempdir'])):\n            os.makedirs(testParams['actual_tempdir'])\n        return os.path.join(testParams['actual_tempdir'], filename)\n    except OSError as exc:\n        actual_tempdir = os.path.join(tempfile.gettempdir(), testParams['tempdir'])\n        if signal:\n            errwrite(('I used `tempfile.gettempdir()` to create the temporary folder `%s`.' % actual_tempdir))\n        testParams['actual_tempdir'] = actual_tempdir\n        if (not os.path.isdir(testParams['actual_tempdir'])):\n            try:\n                os.makedirs(testParams['actual_tempdir'])\n            except Exception:\n                pass\n        return os.path.join(actual_tempdir, filename)\n    except:\n        get_root_logger().error('Could not create a directory.')\n        raise\n", "label": 1}
{"function": "\n\ndef parse_innotop_mode_c(self):\n    with open(self.infile, 'r') as infh:\n        headerline = infh.readline()\n        columns = headerline.split()[2:]\n        outfilehandlers = {\n            \n        }\n        for line in infh:\n            l = line.strip().split(' ', 1)\n            if (len(l) <= 1):\n                continue\n            ts = l[0].strip().replace('T', ' ')\n            try:\n                nameval = l[1].strip().split('\\t', 1)\n            except IndexError:\n                logger.warn('Badly formatted line: %s', line)\n                logger.warn('Expected tab separated values')\n                continue\n            command = nameval[0]\n            if (command not in outfilehandlers):\n                if (len(outfilehandlers) > self.C_MAX_COMMANDS):\n                    continue\n                outfilehandlers[command] = {\n                    \n                }\n            words = nameval[1].split('\\t')\n            for i in range(len(words)):\n                if (self.options and (columns[i] not in self.options)):\n                    continue\n                if (columns[i] not in outfilehandlers[command]):\n                    outfilehandlers[command][columns[i]] = open(self.get_csv_C(command, columns[i]), 'w')\n                    self.csv_files.append(self.get_csv_C(command, columns[i]))\n                ts = naarad.utils.reconcile_timezones(ts, self.timezone, self.graph_timezone)\n                outfilehandlers[command][columns[i]].write((ts + ','))\n                outfilehandlers[command][columns[i]].write(words[i])\n                outfilehandlers[command][columns[i]].write('\\n')\n        for command in outfilehandlers:\n            for column in outfilehandlers[command]:\n                outfilehandlers[command][column].close()\n    return True\n", "label": 1}
{"function": "\n\ndef get_title_extension_admin(self, language=None):\n    '\\n        Get the admin urls for the title extensions menu items, depending on whether a TitleExtension instance exists\\n        for each Title in the current page.\\n        A single language can be passed to only work on a single title.\\n\\n        Return a list of tuples of the title extension and the url; the extension is None if no instance exists,\\n        the url is None is no admin is registered for the extension.\\n        '\n    page = self._get_page()\n    urls = []\n    if language:\n        titles = (page.get_title_obj(language),)\n    else:\n        titles = page.title_set.filter(language__in=get_language_list(page.site_id))\n    for title in titles:\n        try:\n            title_extension = self.model.objects.get(extended_object_id=title.pk)\n        except self.model.DoesNotExist:\n            title_extension = None\n        try:\n            model_name = self.model.__name__.lower()\n            if title_extension:\n                admin_url = admin_reverse(('%s_%s_change' % (self.model._meta.app_label, model_name)), args=(title_extension.pk,))\n            else:\n                admin_url = ('%s?extended_object=%s' % (admin_reverse(('%s_%s_add' % (self.model._meta.app_label, model_name))), title.pk))\n        except NoReverseMatch:\n            admin_url = None\n        if admin_url:\n            urls.append((title_extension, admin_url))\n    return urls\n", "label": 0}
{"function": "\n\ndef test_route_decorator_custom_endpoint_with_dots():\n    bp = flask.Blueprint('bp', __name__)\n\n    @bp.route('/foo')\n    def foo():\n        return flask.request.endpoint\n    try:\n\n        @bp.route('/bar', endpoint='bar.bar')\n        def foo_bar():\n            return flask.request.endpoint\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError('expected AssertionError not raised')\n    try:\n\n        @bp.route('/bar/123', endpoint='bar.123')\n        def foo_bar_foo():\n            return flask.request.endpoint\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError('expected AssertionError not raised')\n\n    def foo_foo_foo():\n        pass\n    pytest.raises(AssertionError, (lambda : bp.add_url_rule('/bar/123', endpoint='bar.123', view_func=foo_foo_foo)))\n    pytest.raises(AssertionError, bp.route('/bar/123', endpoint='bar.123'), (lambda : None))\n    app = flask.Flask(__name__)\n    app.register_blueprint(bp, url_prefix='/py')\n    c = app.test_client()\n    assert (c.get('/py/foo').data == b'bp.foo')\n    rv = c.get('/py/bar')\n    assert (rv.status_code == 404)\n    rv = c.get('/py/bar/123')\n    assert (rv.status_code == 404)\n", "label": 1}
{"function": "\n\ndef Project(self, geometries, inSR=None, outSR=None):\n    'The project operation is performed on a geometry service resource.\\n           The result of this operation is an array of projected geometries.\\n           This resource projects an array of input geometries from an input\\n           spatial reference to an output spatial reference.'\n    if isinstance(geometries, geometry.Geometry):\n        geometries = [geometries]\n    if (inSR is None):\n        inSR = geometries[0].spatialReference.wkid\n    assert outSR, 'Cannot project to an empty output projection.'\n    geometry_types = set([x.__geometry_type__ for x in geometries])\n    assert (len(geometry_types) == 1), 'Too many geometry types'\n    geo_json = json.dumps({\n        'geometryType': list(geometry_types)[0],\n        'geometries': [geo._json_struct_without_sr for geo in geometries],\n    })\n    return self._get_subfolder('project', GeometryResult, {\n        'geometries': geo_json,\n        'inSR': inSR,\n        'outSR': outSR,\n    })\n", "label": 0}
{"function": "\n\ndef broadcast_like(value, template, fgraph, dtype=None):\n    '\\n    Return a Variable with the same shape and dtype as the template,\\n    filled by broadcasting value through it. `value` will be cast as\\n    necessary.\\n\\n    '\n    value = T.as_tensor_variable(value)\n    if (value.type == template.type):\n        return value\n    if (template not in fgraph.variables):\n        raise NotImplementedError('broadcast_like currently requires the template Variable to be in the fgraph already')\n    if hasattr(fgraph, 'shape_feature'):\n        new_shape = fgraph.shape_feature.shape_of[template]\n    else:\n        new_shape = template.shape\n    if (dtype is None):\n        dtype = template.dtype\n    rval = T.alloc(T.cast(value, dtype), *new_shape)\n    if (rval.broadcastable != template.broadcastable):\n        rval = T.unbroadcast(rval, *[i for i in xrange(rval.ndim) if (rval.broadcastable[i] and (not template.broadcastable[i]))])\n    assert (rval.type.dtype == dtype)\n    if (rval.type.broadcastable != template.broadcastable):\n        raise AssertionError(((('rval.type.broadcastable is ' + str(rval.type.broadcastable)) + ' but template.broadcastable is') + str(template.broadcastable)))\n    return rval\n", "label": 1}
{"function": "\n\n@extensions.expected_errors((400, 403, 404))\ndef create(self, req, body):\n    context = _authorize_context(req)\n    sg_rule = self._from_body(body, 'security_group_rule')\n    try:\n        parent_group_id = self.security_group_api.validate_id(sg_rule.get('parent_group_id'))\n        security_group = self.security_group_api.get(context, None, parent_group_id, map_exception=True)\n        new_rule = self._rule_args_to_dict(context, to_port=sg_rule.get('to_port'), from_port=sg_rule.get('from_port'), ip_protocol=sg_rule.get('ip_protocol'), cidr=sg_rule.get('cidr'), group_id=sg_rule.get('group_id'))\n    except (exception.Invalid, exception.InvalidCidr) as exp:\n        raise exc.HTTPBadRequest(explanation=exp.format_message())\n    except exception.SecurityGroupNotFound as exp:\n        raise exc.HTTPNotFound(explanation=exp.format_message())\n    if (new_rule is None):\n        msg = _('Not enough parameters to build a valid rule.')\n        raise exc.HTTPBadRequest(explanation=msg)\n    new_rule['parent_group_id'] = security_group['id']\n    if ('cidr' in new_rule):\n        (net, prefixlen) = netutils.get_net_and_prefixlen(new_rule['cidr'])\n        if ((net not in ('0.0.0.0', '::')) and (prefixlen == '0')):\n            msg = (_('Bad prefix for network in cidr %s') % new_rule['cidr'])\n            raise exc.HTTPBadRequest(explanation=msg)\n    group_rule_data = None\n    try:\n        if sg_rule.get('group_id'):\n            source_group = self.security_group_api.get(context, id=sg_rule['group_id'])\n            group_rule_data = {\n                'name': source_group.get('name'),\n                'tenant_id': source_group.get('project_id'),\n            }\n        security_group_rule = self.security_group_api.create_security_group_rule(context, security_group, new_rule)\n    except exception.Invalid as exp:\n        raise exc.HTTPBadRequest(explanation=exp.format_message())\n    except exception.SecurityGroupNotFound as exp:\n        raise exc.HTTPNotFound(explanation=exp.format_message())\n    except exception.SecurityGroupLimitExceeded as exp:\n        raise exc.HTTPForbidden(explanation=exp.format_message())\n    formatted_rule = self._format_security_group_rule(context, security_group_rule, group_rule_data)\n    return {\n        'security_group_rule': formatted_rule,\n    }\n", "label": 1}
{"function": "\n\n@pytest.mark.django_db\ndef test_jurisdiction_update():\n    tj = FakeJurisdiction()\n    ji = JurisdictionImporter('jurisdiction-id')\n    (_, what) = ji.import_item(tj.as_dict())\n    assert (what == 'insert')\n    (_, what) = ji.import_item(tj.as_dict())\n    assert (what == 'noop')\n    assert (Jurisdiction.objects.count() == 1)\n    tj.name = 'different name'\n    (obj, what) = ji.import_item(tj.as_dict())\n    assert (what == 'update')\n    assert (Jurisdiction.objects.count() == 1)\n    assert (Jurisdiction.objects.get().name == 'different name')\n", "label": 0}
{"function": "\n\ndef test_segment_field(self):\n    \"Upload a segment and test it's fields\"\n    makeAnno(p, 4)\n    parentseed = random.randint(0, 100)\n    f = setField(p, 'parentseed', parentseed)\n    f = getField(p, 'parentseed')\n    assert (parentseed == int(f.read()))\n    segmentclass = random.randint(0, 100)\n    f = setField(p, 'segmentclass', segmentclass)\n    f = getField(p, 'segmentclass')\n    assert (segmentclass == int(f.read()))\n    neuron = random.randint(0, 100)\n    f = setField(p, 'neuron', neuron)\n    f = getField(p, 'neuron')\n    assert (neuron == int(f.read()))\n    status = random.randint(0, 100)\n    f = setField(p, 'status', status)\n    f = getField(p, 'status')\n    assert (status == int(f.read()))\n    synapses = [random.randint(0, 100), random.randint(0, 100), random.randint(0, 100)]\n    f = setField(p, 'synapses', ','.join([str(i) for i in synapses]))\n    f = getField(p, 'synapses')\n    assert (','.join([str(i) for i in synapses]) == f.read())\n", "label": 0}
{"function": "\n\ndef test_agent_commit(self):\n    (dbapi, p) = self._fixture(reset_on_return='commit')\n\n    class Agent(object):\n\n        def __init__(self, conn):\n            self.conn = conn\n\n        def rollback(self):\n            self.conn.special_rollback()\n\n        def commit(self):\n            self.conn.special_commit()\n    c1 = p.connect()\n    c1._reset_agent = Agent(c1)\n    c1.close()\n    assert (not dbapi.connect().special_rollback.called)\n    assert dbapi.connect().special_commit.called\n    assert (not dbapi.connect().rollback.called)\n    assert (not dbapi.connect().commit.called)\n    c1 = p.connect()\n    c1.close()\n    eq_(dbapi.connect().special_rollback.call_count, 0)\n    eq_(dbapi.connect().special_commit.call_count, 1)\n    assert (not dbapi.connect().rollback.called)\n    assert dbapi.connect().commit.called\n", "label": 0}
{"function": "\n\ndef match(self, match):\n    args = []\n    for argument in match.arguments:\n        argument_value = argument.value\n        if (not isinstance(argument_value, self.json_scalar_types)):\n            argument_value = argument.original\n        assert isinstance(argument_value, self.json_scalar_types)\n        arg = {\n            'value': argument_value,\n        }\n        if argument.name:\n            arg['name'] = argument.name\n        if (argument.original != argument_value):\n            arg['original'] = argument.original\n        args.append(arg)\n    match_data = {\n        'location': (six.text_type(match.location) or ''),\n        'arguments': args,\n    }\n    if match.location:\n        steps = self.current_feature_element['steps']\n        steps[self._step_index]['match'] = match_data\n", "label": 1}
{"function": "\n\n@gen.engine\ndef Transform(self, client, viewpoint, callback):\n    from viewpoint import Viewpoint\n    from viewfinder.backend.www import system_users\n    if (viewpoint.type == Viewpoint.SYSTEM):\n        (episodes, _) = (yield gen.Task(Viewpoint.QueryEpisodes, client, viewpoint.viewpoint_id))\n        for episode in episodes:\n            if (episode.parent_ep_id == 'egAZn7AjQ-F7'):\n                assert ((viewpoint.cover_photo['episode_id'] == episode.parent_ep_id) or episode.episode_id), episode\n                assert (viewpoint.cover_photo['photo_id'] == 'pgAZn7AjQ-FB'), viewpoint.cover_photo\n                viewpoint.cover_photo = {\n                    'episode_id': episode.episode_id,\n                    'photo_id': 'pgAZn7AjQ-FB',\n                }\n                self._LogUpdate(viewpoint)\n                if Version._mutate_items:\n                    (yield gen.Task(viewpoint.Update, client))\n    callback(viewpoint)\n", "label": 0}
{"function": "\n\ndef commit(self):\n    self.log('in commit')\n    for p in [c for c in self.block_candidates.values() if (c.block.prevhash == self.head.hash)]:\n        assert isinstance(p, BlockProposal)\n        ls = self.heights[p.height].last_quorum_lockset\n        if (ls and (ls.has_quorum == p.blockhash)):\n            self.store_proposal(p)\n            self.store_last_committing_lockset(ls)\n            success = self.chainservice.commit_block(p.block)\n            assert success\n            if success:\n                self.log('commited', p=p, hash=phx(p.blockhash))\n                assert (self.head == p.block)\n                self.commit()\n                return True\n            else:\n                self.log('could not commit', p=p)\n        else:\n            self.log('no quorum for', p=p)\n            if ls:\n                self.log('votes', votes=ls.votes)\n", "label": 1}
{"function": "\n\ndef makedirs_count(path, count=0):\n    \"\\n    Same as os.makedirs() except that this method returns the number of\\n    new directories that had to be created.\\n\\n    Also, this does not raise an error if target directory already exists.\\n    This behaviour is similar to Python 3.x's os.makedirs() called with\\n    exist_ok=True. Also similar to swift.common.utils.mkdirs()\\n\\n    https://hg.python.org/cpython/file/v3.4.2/Lib/os.py#l212\\n    \"\n    (head, tail) = os.path.split(path)\n    if (not tail):\n        (head, tail) = os.path.split(head)\n    if (head and tail and (not os.path.exists(head))):\n        count = makedirs_count(head, count)\n        if (tail == os.path.curdir):\n            return\n    try:\n        os.mkdir(path)\n    except OSError as e:\n        if ((e.errno != errno.EEXIST) or (not os.path.isdir(path))):\n            raise\n    else:\n        count += 1\n    return count\n", "label": 1}
{"function": "\n\ndef put_logging(Bucket, TargetBucket=None, TargetPrefix=None, TargetGrants=None, region=None, key=None, keyid=None, profile=None):\n    \"\\n    Given a valid config, update the logging parameters for a bucket.\\n\\n    Returns {updated: true} if parameters were updated and returns\\n    {updated: False} if parameters were not updated.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt myminion boto_s3_bucket.put_logging my_bucket log_bucket '[{...}]' prefix\\n\\n    \"\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        logstate = {\n            \n        }\n        for (key, val) in {\n            'TargetBucket': TargetBucket,\n            'TargetGrants': TargetGrants,\n            'TargetPrefix': TargetPrefix,\n        }.iteritems():\n            if (val is not None):\n                logstate[key] = val\n        if logstate:\n            logstatus = {\n                'LoggingEnabled': logstate,\n            }\n        else:\n            logstatus = {\n                \n            }\n        if ((TargetGrants is not None) and isinstance(TargetGrants, string_types)):\n            TargetGrants = json.loads(TargetGrants)\n        conn.put_bucket_logging(Bucket=Bucket, BucketLoggingStatus=logstatus)\n        return {\n            'updated': True,\n            'name': Bucket,\n        }\n    except ClientError as e:\n        return {\n            'updated': False,\n            'error': __utils__['boto3.get_error'](e),\n        }\n", "label": 0}
{"function": "\n\ndef _configure(self, qtile, bar):\n    base._Widget._configure(self, qtile, bar)\n    if (not self.filename):\n        raise ValueError('Filename not set!')\n    self.filename = os.path.expanduser(self.filename)\n    try:\n        self.image = cairocffi.ImageSurface.create_from_png(self.filename)\n    except MemoryError:\n        raise ValueError((\"The image '%s' doesn't seem to be a valid PNG\" % self.filename))\n    self.pattern = cairocffi.SurfacePattern(self.image)\n    self.image_width = self.image.get_width()\n    self.image_height = self.image.get_height()\n    if self.scale:\n        if self.bar.horizontal:\n            new_height = (self.bar.height - (self.margin_y * 2))\n            if (new_height and (self.image_height != new_height)):\n                scaler = cairocffi.Matrix()\n                sp = (self.image_height / new_height)\n                self.image_height = new_height\n                self.image_width = int((self.image_width / sp))\n                scaler.scale(sp, sp)\n                self.pattern.set_matrix(scaler)\n        else:\n            new_width = (self.bar.width - (self.margin_x * 2))\n            if (new_width and (self.image_width != new_width)):\n                scaler = cairocffi.Matrix()\n                sp = (self.image_width / new_width)\n                self.image_width = new_width\n                self.image_height = int((self.image_height / sp))\n                scaler.scale(sp, sp)\n                self.pattern.set_matrix(scaler)\n", "label": 1}
{"function": "\n\ndef _check_neutron_floating_ip():\n    parser = argparse.ArgumentParser(description=((('Check an Floating ip creation. Note that it is able ' + 'to delete *all* floating ips from a account, so ') + 'ensure that nothing important is running on the ') + 'specified account.'))\n    parser.add_argument('--auth_url', metavar='URL', type=str, default=os.getenv('OS_AUTH_URL'), help='Keystone URL')\n    parser.add_argument('--username', metavar='username', type=str, default=os.getenv('OS_USERNAME'), help='username to use for authentication')\n    parser.add_argument('--password', metavar='password', type=str, default=os.getenv('OS_PASSWORD'), help='password to use for authentication')\n    parser.add_argument('--tenant', metavar='tenant', type=str, default=os.getenv('OS_TENANT_NAME'), help='tenant name to use for authentication')\n    parser.add_argument('--endpoint_url', metavar='endpoint_url', type=str, help='Override the catalog endpoint.')\n    parser.add_argument('--endpoint_type', metavar='endpoint_type', type=str, default='publicURL', help=('Endpoint type in the catalog request. ' + 'Public by default.'))\n    parser.add_argument('--force_delete', action='store_true', help=(('If matching floating ip are found, delete them ' + 'and add a notification in the message instead of ') + 'getting out in critical state.'))\n    parser.add_argument('--timeout', metavar='timeout', type=int, default=120, help=('Max number of second to create/delete a ' + 'floating ip (120 by default).'))\n    parser.add_argument('--floating_ip', metavar='floating_ip', type=fip_type, default=None, help=(((('Regex of IP(s) to check for existance. ' + 'This value can be \"all\" for conveniance (match ') + 'all ip). This permit to avoid certain floating ') + 'ip to be kept. Its default value prevents the ') + 'removal of any existing floating ip'))\n    parser.add_argument('--ext_router_name', metavar='ext_router_name', type=str, default='public', help='Name of the \"public\" router (public by default)')\n    parser.add_argument('--verbose', action='count', help='Print requests on stderr.')\n    args = parser.parse_args()\n    try:\n        nova_client = client.Client(username=args.username, tenant_name=args.tenant, password=args.password, auth_url=args.auth_url)\n        nova_client.authenticate()\n    except Exception as e:\n        utils.critical(('Authentication error: %s\\n' % e))\n    try:\n        endpoint = nova_client.service_catalog.get_endpoints('network')['network'][0][args.endpoint_type]\n        if args.endpoint_url:\n            endpoint = mangle_url(endpoint, args.endpoint_url)\n        token = nova_client.service_catalog.get_token()['id']\n        if args.verbose:\n            logging.basicConfig(level=logging.DEBUG)\n        neutron_client = neutron.Client('2.0', endpoint_url=endpoint, token=token)\n    except Exception as e:\n        utils.critical(('Error creating neutron object: %s\\n' % e))\n    util = Novautils(neutron_client, nova_client.tenant_id)\n    util.check_connection()\n    if args.floating_ip:\n        util.check_existing_floatingip(args.floating_ip, args.force_delete)\n    util.get_network_id(args.ext_router_name)\n    util.create_floating_ip()\n    util.delete_floating_ip()\n    if util.msgs:\n        utils.critical(', '.join(util.msgs))\n    duration = util.get_duration()\n    notification = ''\n    if util.notifications:\n        notification = (('(' + ', '.join(util.notifications)) + ')')\n    utils.ok(('Floating ip created and deleted %s| time=%d' % (notification, duration)))\n", "label": 1}
{"function": "\n\ndef main(prettypath, verify=False):\n    for pins in range(2, 9):\n        for generator in (top_pth_fp, side_pth_fp, top_smd_fp, side_smd_fp):\n            (name, fp) = generator(pins)\n            path = os.path.join(prettypath, (name + '.kicad_mod'))\n            if verify:\n                print('Verifying', path)\n            if os.path.isfile(path):\n                with open(path) as f:\n                    old = f.read()\n                old = [n for n in sexp_parse(old) if (n[0] != 'tedit')]\n                new = [n for n in sexp_parse(fp) if (n[0] != 'tedit')]\n                if (new == old):\n                    continue\n            if verify:\n                return False\n            else:\n                with open(path, 'w') as f:\n                    f.write(fp)\n    if verify:\n        return True\n", "label": 1}
{"function": "\n\ndef test_dynamic_sections_without_default(self):\n    result = config.load_config()\n    result['dynamic-section'] = {\n        'OS_REGION_NAME': 'DFW;ORD',\n    }\n    config.create_dynamic_configs(result)\n    assert ('dynamic-section-ORD' in result.sections)\n    assert ('dynamic-section-DFW' in result.sections)\n    assert ('dynamic-section' not in result.sections)\n    assert ('dynamic-section' in result['dynamic-section-ORD'].get('SUPERNOVA_GROUP'))\n    assert ('dynamic-section' == result['dynamic-section-DFW'].get('SUPERNOVA_GROUP'))\n    assert ('ORD' == result['dynamic-section-ORD'].get('OS_REGION_NAME'))\n    assert ('DFW' == result['dynamic-section-DFW'].get('OS_REGION_NAME'))\n", "label": 1}
{"function": "\n\ndef test_global_shell_configuration_zshell(self):\n    ' The global shell should dictate what files are injected (zsh, no bash, no gui)'\n    global_config = create_default_config()\n    global_config.set('shell', 'bash', 'false')\n    global_config.set('shell', 'zsh', 'true')\n    global_config.set('shell', 'gui', 'false')\n    with MockEnvironment(target_config=test_target, global_config=global_config) as environment:\n        environment.install()\n        assert [x for x in environment.injections.inject_dict.keys() if x.endswith('.zshrc')]\n        env_injected = False\n        for profile in ['.zprofile', '.zlogin']:\n            env_injected = (env_injected or filter((lambda x: x.endswith(profile)), environment.injections.inject_dict.keys()))\n        assert env_injected\n        assert (not [x for x in environment.injections.inject_dict.keys() if x.endswith('.bashrc')])\n        for profile in ['.bash_profile', '.bash_login']:\n            assert (not [x for x in environment.injections.inject_dict.keys() if x.endswith(profile)])\n", "label": 1}
{"function": "\n\ndef test_verbose(self, space, tmpdir, capfd):\n    self.run(space, tmpdir, 'puts 5', ruby_args=['-v'])\n    (out, err) = capfd.readouterr()\n    [version, out] = out.splitlines()\n    assert version.startswith('topaz')\n    assert ('1.9.3' in version)\n    assert (os.uname()[4] in version)\n    assert (platform.system().lower() in version)\n    assert (subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).rstrip() in version)\n    assert (out == '5')\n    self.run(space, tmpdir, ruby_args=['-v'])\n    (out, err) = capfd.readouterr()\n    [version] = out.splitlines()\n    assert version.startswith('topaz')\n", "label": 1}
{"function": "\n\ndef _destroy_evacuated_instances(self, context):\n    'Destroys evacuated instances.\\n\\n        While nova-compute was down, the instances running on it could be\\n        evacuated to another host. Check that the instances reported\\n        by the driver are still associated with this host.  If they are\\n        not, destroy them, with the exception of instances which are in\\n        the MIGRATING, RESIZE_MIGRATING, RESIZE_MIGRATED, RESIZE_FINISH\\n        task state or RESIZED vm state.\\n        '\n    filters = {\n        'source_compute': self.host,\n        'status': ['accepted', 'done'],\n        'migration_type': 'evacuation',\n    }\n    evacuations = objects.MigrationList.get_by_filters(context, filters)\n    if (not evacuations):\n        return\n    evacuations = {mig.instance_uuid: mig for mig in evacuations}\n    filters = {\n        'deleted': False,\n    }\n    local_instances = self._get_instances_on_driver(context, filters)\n    evacuated = [inst for inst in local_instances if (inst.uuid in evacuations)]\n    for instance in evacuated:\n        migration = evacuations[instance.uuid]\n        LOG.info(_LI('Deleting instance as it has been evacuated from this host'), instance=instance)\n        try:\n            network_info = self.network_api.get_instance_nw_info(context, instance)\n            bdi = self._get_instance_block_device_info(context, instance)\n            destroy_disks = (not self._is_instance_storage_shared(context, instance))\n        except exception.InstanceNotFound:\n            network_info = network_model.NetworkInfo()\n            bdi = {\n                \n            }\n            LOG.info(_LI('Instance has been marked deleted already, removing it from the hypervisor.'), instance=instance)\n            destroy_disks = True\n        self.driver.destroy(context, instance, network_info, bdi, destroy_disks)\n        migration.status = 'completed'\n        migration.save()\n", "label": 0}
{"function": "\n\ndef __init__(self, v, aspectValues=None):\n    global Aspect\n    if (Aspect is None):\n        from arelle.ModelFormulaObject import Aspect\n    self.modelXbrl = v.modelXbrl\n    if (aspectValues is None):\n        aspectValues = {\n            \n        }\n    self.aspectEntryObjectId = aspectValues.get('aspectEntryObjectId', None)\n    if (Aspect.CONCEPT in aspectValues):\n        qname = aspectValues[Aspect.CONCEPT]\n        self.qname = qname\n        self.concept = v.modelXbrl.qnameConcepts.get(qname)\n        self.isItem = ((self.concept is not None) and self.concept.isItem)\n        self.isTuple = ((self.concept is not None) and self.concept.isTuple)\n    else:\n        self.qname = None\n        self.concept = None\n        self.isItem = False\n        self.isTuple = False\n    if (Aspect.LOCATION in aspectValues):\n        self.parent = aspectValues[Aspect.LOCATION]\n        try:\n            self.isTuple = self.parent.isTuple\n        except AttributeError:\n            self.isTuple = False\n    else:\n        self.parent = v.modelXbrl.modelDocument.xmlRootElement\n    self.isNumeric = ((self.concept is not None) and self.concept.isNumeric)\n    self.context = ContextPrototype(v, aspectValues)\n    if (Aspect.UNIT in aspectValues):\n        self.unit = UnitPrototype(v, aspectValues)\n    else:\n        self.unit = None\n    self.factObjectId = None\n", "label": 1}
{"function": "\n\ndef load_from_master_cf(self):\n    'Load services from the master.cf file.\\n\\n        Parse the configuration file to update the service table. New\\n        entries are saved and outdated ones (ie. present in the\\n        database but not in the file) are removed.\\n        '\n    with open(parameters.get_admin('MASTER_CF_PATH')) as fp:\n        content = fp.read()\n    services = []\n    for line in content.split('\\n'):\n        if ((not line) or line.startswith('#')):\n            continue\n        parts = line.strip().split()\n        if (len(parts) != 8):\n            continue\n        if (parts[7] != 'smtp'):\n            continue\n        (srv, created) = self.get_or_create(name=parts[0])\n        services.append(parts[0])\n    to_delete = []\n    for service in self.all():\n        if (service.name not in services):\n            to_delete.append(service.name)\n    Service.objects.filter(name__in=to_delete).delete()\n", "label": 1}
{"function": "\n\ndef get_new_comment(self, clear=False, staged=False, ready=True):\n    try:\n        with io.open(self.get_local_path(constants.TICKET_NEW_COMMENT), 'r+', encoding='utf-8') as c:\n            local_contents = c.read().strip()\n        if ready:\n            contents = self.get_local_file_at_revision(constants.TICKET_NEW_COMMENT, 'HEAD')\n            if contents:\n                contents = contents.strip()\n            else:\n                contents = ''\n        else:\n            contents = local_contents\n        if ((not ready) and (contents == self.get_new_comment(ready=True))):\n            contents = ''\n        if ((contents == local_contents) and clear):\n            with io.open(self.get_local_path(constants.TICKET_NEW_COMMENT), 'r+', encoding='utf-8') as c:\n                c.truncate()\n    except IOError:\n        contents = ''\n    contents = self.process_macros(contents)\n    return self.execute_plugin_method_series(name='alter_new_comment', args=(contents,), single_response=True)\n", "label": 1}
{"function": "\n\ndef staggered_retries(run, *a, **kw):\n    '\\n    A version of spawn that will block will it is done\\n    running the function, and which will call the function\\n    repeatedly as time progresses through the timeouts list.\\n\\n    Best used for idempotent network calls (e.g. HTTP GETs).\\n\\n    e.g.::\\n\\n        user_data = async.staggered_retries(get_data, max_results,\\n                                            latent_data_ok, public_credential_load,\\n                                            timeouts_secs=[0.1, 0.5, 1, 2])\\n\\n    returns None on timeout.\\n    '\n    ctx = context.get_context()\n    ready = gevent.event.Event()\n    ready.clear()\n\n    def callback(source):\n        if source.successful():\n            ready.set()\n    if ('timeouts_secs' in kw):\n        timeouts_secs = kw.pop('timeouts_secs')\n    else:\n        timeouts_secs = [0.05, 0.1, 0.15, 0.2]\n    if (timeouts_secs[0] > 0):\n        timeouts_secs.insert(0, 0)\n    gs = gevent.spawn(run, *a, **kw)\n    gs.link_value(callback)\n    running = [gs]\n    for i in range(1, len(timeouts_secs)):\n        this_timeout = (timeouts_secs[i] - timeouts_secs[(i - 1)])\n        if ctx.dev:\n            this_timeout = (this_timeout * 5.0)\n        ml.ld2('Using timeout {0}', this_timeout)\n        try:\n            with gevent.Timeout(this_timeout):\n                ready.wait()\n                break\n        except gevent.Timeout:\n            ml.ld2('Timed out!')\n            log_rec = ctx.log.critical('ASYNC.STAGGER', run.__name__)\n            log_rec.failure('timed out after {timeout}', timeout=this_timeout)\n            gs = gevent.spawn(run, *a, **kw)\n            gs.link_value(callback)\n            running.append(gs)\n    vals = [l.value for l in running if l.successful()]\n    for g in running:\n        g.kill()\n    if vals:\n        return vals[0]\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef tearDown(self):\n    'Runs after each test method to tear down test environment.'\n    try:\n        self.mox.UnsetStubs()\n        self.stubs.UnsetAll()\n        self.stubs.SmartUnsetAll()\n        self.mox.VerifyAll()\n        super(TestCase, self).tearDown()\n    finally:\n        if FLAGS.fake_rabbit:\n            fakerabbit.reset_all()\n        if (FLAGS.connection_type == 'fake'):\n            if hasattr(fake.FakeConnection, '_instance'):\n                del fake.FakeConnection._instance\n        if (FLAGS.image_service == 'nova.image.fake.FakeImageService'):\n            nova.image.fake.FakeImageService_reset()\n        self.reset_flags()\n        for x in self.injected:\n            try:\n                x.stop()\n            except AssertionError:\n                pass\n        for x in self._services:\n            try:\n                x.kill()\n            except Exception:\n                pass\n", "label": 1}
{"function": "\n\n@property\ndef previous_time(self):\n    now = timezone.now()\n    recurring_end = occurring_end = None\n    try:\n        occurring_rule = self.occurring_rule\n    except OccurringRule.DoesNotExist:\n        pass\n    else:\n        if (occurring_rule and (occurring_rule.dt_end < now)):\n            occurring_end = (occurring_rule.dt_end, occurring_rule)\n    rrules = self.recurring_rules.filter(begin__lt=now)\n    recurring_ends = [(rule.dt_end, rule) for rule in rrules if (rule.dt_end is not None)]\n    recurring_ends.sort(key=itemgetter(0), reverse=True)\n    try:\n        recurring_end = recurring_ends[0]\n    except IndexError:\n        pass\n    ends = [i for i in (recurring_end, occurring_end) if (i is not None)]\n    ends.sort(key=itemgetter(0), reverse=True)\n    try:\n        return ends[0][1]\n    except IndexError:\n        return None\n", "label": 1}
{"function": "\n\ndef test_invalid_date_fromjson(self):\n    jdate = '2015-01-invalid'\n    try:\n        parse(('{\"a\": \"%s\"}' % jdate), {\n            'a': datetime.date,\n        }, False)\n        assert False\n    except Exception as e:\n        assert isinstance(e, InvalidInput)\n        assert (e.fieldname == 'a')\n        assert (e.value == jdate)\n        assert (e.msg == (\"'%s' is not a legal date value\" % jdate))\n", "label": 0}
{"function": "\n\ndef test_oracle_tns_parsing(self):\n    url = 'oracle://scott:tiger@/tnsname'\n    url = dj_database_url.parse(url)\n    assert (url['ENGINE'] == 'django.db.backends.oracle')\n    assert (url['USER'] == 'scott')\n    assert (url['PASSWORD'] == 'tiger')\n    assert (url['NAME'] == 'tnsname')\n    assert (url['HOST'] == '')\n    assert (url['PORT'] == '')\n", "label": 0}
{"function": "\n\ndef get_substream_rstates(self, n_streams, dtype, inc_rstate=True):\n    '\\n        Initialize a matrix in which each row is a MRG stream state,\\n        and they are spaced by 2**72 samples.\\n\\n        '\n    assert isinstance(dtype, str)\n    assert (n_streams < (2 ** 72))\n    assert (n_streams > 0)\n    rval = numpy.zeros((n_streams, 6), dtype='int32')\n    rval[0] = self.rstate\n    if (multMatVect.dot_modulo is None):\n        multMatVect(rval[0], A1p72, M1, A2p72, M2)\n    f = multMatVect.dot_modulo\n    f.input_storage[0].storage[0] = A1p72\n    f.input_storage[2].storage[0] = M1\n    f.input_storage[3].storage[0] = A2p72\n    f.input_storage[5].storage[0] = M2\n    for i in xrange(1, n_streams):\n        v = rval[(i - 1)]\n        f.input_storage[1].storage[0] = v[:3]\n        f.input_storage[4].storage[0] = v[3:]\n        f.fn()\n        rval[i] = f.output_storage[0].storage[0]\n    if inc_rstate:\n        self.inc_rstate()\n    if (self.use_cuda and (dtype == 'float32')):\n        rval = rval.flatten()\n        tmp_float_buf = numpy.frombuffer(rval.data, dtype='float32')\n        assert (tmp_float_buf.shape == rval.shape)\n        assert (tmp_float_buf.view('int32') == rval).all()\n        rval = tmp_float_buf\n    return rval\n", "label": 1}
{"function": "\n\ndef apply(self, tree):\n    '\\n        Applies the configured optimizations to the given node tree. Modifies the tree in-place\\n        to be sure to have a deep copy if you need the original one. It raises an error instance\\n        whenever any optimization could not be applied to the given tree.\\n        '\n    enabled = self.__optimizations\n    if ('wrap' in enabled):\n        try:\n            ClosureWrapper.optimize(tree)\n        except CryptPrivates.Error as err:\n            raise Error(err)\n    if ('declarations' in enabled):\n        try:\n            CombineDeclarations.optimize(tree)\n        except CombineDeclarations.Error as err:\n            raise Error(err)\n    if ('blocks' in enabled):\n        try:\n            BlockReducer.optimize(tree)\n        except BlockReducer.Error as err:\n            raise Error(err)\n    if ('variables' in enabled):\n        try:\n            LocalVariables.optimize(tree)\n        except LocalVariables.Error as err:\n            raise Error(err)\n    if ('privates' in enabled):\n        try:\n            CryptPrivates.optimize(tree, tree.fileId)\n        except CryptPrivates.Error as err:\n            raise Error(err)\n", "label": 1}
{"function": "\n\ndef starttls(self):\n    assert ('ssl' in globals())\n    network_config = getattr(conf.supybot.networks, self.irc.network)\n    certfile = network_config.certfile()\n    if (not certfile):\n        certfile = conf.supybot.protocols.irc.certfile()\n    if (not certfile):\n        certfile = None\n    elif (not os.path.isfile(certfile)):\n        drivers.log.warning(('Could not find cert file %s.' % certfile))\n        certfile = None\n    verifyCertificates = conf.supybot.protocols.ssl.verifyCertificates()\n    if (not verifyCertificates):\n        drivers.log.warning('Not checking SSL certificates, connections are vulnerable to man-in-the-middle attacks. Set supybot.protocols.ssl.verifyCertificates to \"true\" to enable validity checks.')\n    try:\n        self.conn = utils.net.ssl_wrap_socket(self.conn, logger=drivers.log, hostname=self.server[0], certfile=certfile, verify=verifyCertificates, trusted_fingerprints=network_config.ssl.serverFingerprints(), ca_file=network_config.ssl.authorityCertificate())\n    except getattr(ssl, 'CertificateError', None) as e:\n        drivers.log.error((\"Certificate validation failed when connecting to %s: %s\\nThis means either someone is doing a man-in-the-middle attack on your connection, or the server's certificate is not in your trusted fingerprints list.\" % (self.irc.network, e.args[0])))\n        raise ssl.SSLError('Aborting because of failed certificate verification.')\n    except ssl.SSLError as e:\n        drivers.log.error((\"Certificate validation failed when connecting to %s: %s\\nThis means either someone is doing a man-in-the-middle attack on your connection, or the server's certificate is not trusted.\" % (self.irc.network, e.args[1])))\n        raise ssl.SSLError('Aborting because of failed certificate verification.')\n", "label": 1}
{"function": "\n\ndef _get_updates(self, bn_ps, bn_share):\n    cg = ComputationGraph(bn_ps)\n    updates = OrderedDict([(up, cg.updates[up]) for up in cg.updates if ((up.name == 'counter') or (up in bn_share))])\n    assert (self._counter == self._param_from_updates(cg.updates, 'counter'))\n    assert (self._counter_max == self._param_from_updates(cg.updates, 'counter_max'))\n    assert (len(updates) == (len(bn_ps) + 1)), 'Counter or var missing from update'\n    return updates\n", "label": 0}
{"function": "\n\ndef fit(self, X, y, sample_weight=None):\n    'Fit the model according to the given training data.\\n\\n        Parameters\\n        ----------\\n        X : array-like, shape (n_samples, n_features)\\n            Training vector, where n_samples in the number of samples and\\n            n_features is the number of features.\\n\\n        y : array-like, shape (n_samples,)\\n            Target vector relative to X.\\n\\n        sample_weight : array-like, shape (n_samples,)\\n            Weight given to each sample.\\n\\n        Returns\\n        -------\\n        self : object\\n            Returns self.\\n        '\n    (X, y) = check_X_y(X, y, copy=False, accept_sparse=['csr'], y_numeric=True)\n    if (sample_weight is not None):\n        sample_weight = np.array(sample_weight)\n        check_consistent_length(y, sample_weight)\n    else:\n        sample_weight = np.ones_like(y)\n    if (self.epsilon < 1.0):\n        raise ValueError(('epsilon should be greater than or equal to 1.0, got %f' % self.epsilon))\n    if (self.warm_start and hasattr(self, 'coef_')):\n        parameters = np.concatenate((self.coef_, [self.intercept_, self.scale_]))\n    elif self.fit_intercept:\n        parameters = np.zeros((X.shape[1] + 2))\n    else:\n        parameters = np.zeros((X.shape[1] + 1))\n    bounds = np.tile([(- np.inf), np.inf], (parameters.shape[0], 1))\n    bounds[(- 1)][0] = 1e-12\n    try:\n        (parameters, f, dict_) = optimize.fmin_l_bfgs_b(_huber_loss_and_gradient, parameters, args=(X, y, self.epsilon, self.alpha, sample_weight), maxiter=self.max_iter, tol=self.tol, bounds=bounds, iprint=0)\n    except TypeError:\n        (parameters, f, dict_) = optimize.fmin_l_bfgs_b(_huber_loss_and_gradient, parameters, args=(X, y, self.epsilon, self.alpha, sample_weight), bounds=bounds)\n    self.n_iter_ = dict_.get('nit', None)\n    self.scale_ = parameters[(- 1)]\n    if self.fit_intercept:\n        self.intercept_ = parameters[(- 2)]\n    else:\n        self.intercept_ = 0.0\n    self.coef_ = parameters[:X.shape[1]]\n    residual = np.abs(((y - safe_sparse_dot(X, self.coef_)) - self.intercept_))\n    self.outliers_ = (residual > (self.scale_ * self.epsilon))\n    return self\n", "label": 1}
{"function": "\n\ndef download_yang(request, req):\n    '\\n    This API download yang schema from device\\n    '\n    logging.debug('Download Yang Schema')\n    req = req.replace('<metadata>', '')\n    req = req.replace('</metadata>', '')\n    (protocol, device, fmt, payload) = Adapter.parse_request(req)\n    if (device.get('host', None) is None):\n        return HttpResponse(Response.error('download', 'no host info'))\n    session_dir = ServerSettings.schema_path(request.session.session_key)\n    if (not os.path.exists(session_dir)):\n        os.makedirs(session_dir)\n    if (not os.path.exists(session_dir)):\n        return HttpResponse(Response.error('download', 'No session directory'))\n    for fname in os.listdir(session_dir):\n        if fname.endswith('.yang'):\n            fn = os.path.join(session_dir, fname)\n            os.remove(fn)\n    modules = ET.Element('modules')\n    reqxml = ET.fromstring(req)\n    schemas = reqxml.find('schemas')\n    for sc in schemas:\n        id = sc.text\n        module = ET.Element('module')\n        get_sc = ET.Element('get-schema')\n        get_sc.set('xmlns', 'urn:ietf:params:xml:ns:yang:ietf-netconf-monitoring')\n        identifier = ET.Element('identifier')\n        sfile = id.split('@')[0]\n        identifier.text = sfile\n        module.text = (id + '.yang')\n        get_sc.append(identifier)\n        rpc.append(get_sc)\n        modules.append(module)\n        schema = Adapter.run_netconf(request.user.username, device, rpc)\n        fname = os.path.join(session_dir, (id + '.yang'))\n        with open(fname, 'w') as f:\n            f.write(schema[0][0].text)\n        rpc.remove(get_sc)\n    return modules\n", "label": 0}
{"function": "\n\ndef test_cpu_percent(self):\n    p = psutil.Process(os.getpid())\n    p.get_cpu_percent(interval=0.001)\n    p.get_cpu_percent(interval=0.001)\n    for x in range(100):\n        percent = p.get_cpu_percent(interval=None)\n        assert isinstance(percent, float)\n        assert (percent >= 0.0), percent\n        if (os.name != 'posix'):\n            assert (percent <= 100.0), percent\n        else:\n            assert (percent >= 0.0), percent\n", "label": 0}
{"function": "\n\ndef lookup_domain(self, domain, lookup_record=None, **kw):\n    'Looks up the DNS record for *domain* and returns:\\n        \\n        * None if it does not exist,\\n        * The IP address if looking up the \"A\" record, or\\n        * The list of hosts in the \"MX\" record.\\n        \\n        The return value, if treated as a boolean, says whether a domain exists.\\n        \\n        You can pass \"a\" or \"mx\" as the *lookup_record* parameter. Otherwise,\\n        the *lookup_dns* parameter from the constructor is used.\\n        \"a\" means verify that the domain exists.\\n        \"mx\" means verify that the domain exists and specifies mail servers.\\n        '\n    import DNS\n    lookup_record = (lookup_record.lower() if lookup_record else self._lookup_dns)\n    if (lookup_record not in ('a', 'mx')):\n        raise RuntimeError(('Not a valid lookup_record value: ' + lookup_record))\n    if (lookup_record == 'a'):\n        request = DNS.Request(domain, **kw)\n        try:\n            answers = request.req().answers\n        except (DNS.Lib.PackError, UnicodeError):\n            return False\n        if (not answers):\n            return False\n        result = answers[0]['data']\n        if (result in self.false_positive_ips):\n            return False\n        return result\n    try:\n        return DNS.mxlookup(domain)\n    except UnicodeError:\n        pass\n    return False\n", "label": 1}
{"function": "\n\ndef assertFormErrors(self, response, count=0, message=None, context_name='form'):\n    \"\\n        Asserts that the response does contain a form in it's\\n        context, and that form has errors, if count were given,\\n        it must match the exact numbers of errors\\n        \"\n    context = getattr(response, 'context', {\n        \n    })\n    assert (context and (context_name in context)), 'The response did not contain a form.'\n    errors = response.context[context_name]._errors\n    if count:\n        assert (len(errors) == count), ('%d errors were found on the form, %d expected' % (len(errors), count))\n        if (message and (message not in unicode(errors))):\n            self.fail(('Expected message not found, instead found: %s' % [('%s: %s' % (key, [e for e in field_errors])) for (key, field_errors) in errors.items()]))\n    else:\n        assert (len(errors) > 0), 'No errors were found on the form'\n", "label": 1}
{"function": "\n\ndef discover_modules(self):\n    \" Return module sequence discovered from ``self.package_name``\\n\\n\\n        Parameters\\n        ----------\\n        None\\n\\n        Returns\\n        -------\\n        mods : sequence\\n            Sequence of module names within ``self.package_name``\\n\\n        Examples\\n        --------\\n        >>> dw = ApiDocWriter('sphinx')\\n        >>> mods = dw.discover_modules()\\n        >>> 'sphinx.util' in mods\\n        True\\n        >>> dw.skip_patterns.append('\\\\.util$')\\n        >>> 'sphinx.util' in dw.discover_modules()\\n        False\\n        >>>\\n        \"\n    modules = [self.package_name]\n    for (dirpath, dirnames, filenames) in os.walk(self.root_path, topdown=False):\n        root_uri = self._path2uri(os.path.join(self.root_path, dirpath))\n        filenames = [f for f in filenames if f.endswith('.py')]\n        uris = [u.strip('.py') for u in (dirnames + filenames)]\n        for uri in uris:\n            package_uri = '.'.join((root_uri, uri))\n            if (self._uri2path(package_uri) and self._survives_exclude(package_uri)):\n                try:\n                    mod = __import__(package_uri, fromlist=['cesium'])\n                    mod.__all__\n                    modules.append(package_uri)\n                except (ImportError, AttributeError):\n                    pass\n    return sorted(modules)\n", "label": 1}
{"function": "\n\ndef draw_inline_level(context, page, box, enable_hinting):\n    if isinstance(box, StackingContext):\n        stacking_context = box\n        assert isinstance(stacking_context.box, boxes.InlineBlockBox)\n        draw_stacking_context(context, stacking_context, enable_hinting)\n    else:\n        draw_background(context, box.background, enable_hinting)\n        draw_border(context, box, enable_hinting)\n        if isinstance(box, (boxes.InlineBox, boxes.LineBox)):\n            for child in box.children:\n                if isinstance(child, boxes.TextBox):\n                    draw_text(context, child, enable_hinting)\n                else:\n                    draw_inline_level(context, page, child, enable_hinting)\n        elif isinstance(box, boxes.InlineReplacedBox):\n            draw_replacedbox(context, box)\n        else:\n            assert isinstance(box, boxes.TextBox)\n            draw_text(context, box, enable_hinting)\n", "label": 1}
{"function": "\n\ndef test_database_url_with_options(self):\n    os.environ['DATABASE_URL'] = 'postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn?sslrootcert=rds-combined-ca-bundle.pem&sslmode=verify-full'\n    url = dj_database_url.config()\n    assert (url['ENGINE'] == 'django.db.backends.postgresql_psycopg2')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 5431)\n    assert (url['OPTIONS'] == {\n        'sslrootcert': 'rds-combined-ca-bundle.pem',\n        'sslmode': 'verify-full',\n    })\n    os.environ['DATABASE_URL'] = 'postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn?'\n    url = dj_database_url.config()\n    assert ('OPTIONS' not in url)\n", "label": 1}
{"function": "\n\n@classmethod\n@gen.engine\ndef MakeFriendAndUpdate(cls, client, user_id, friend_dict, callback):\n    'Ensures that the given user has at least a one-way friend relationship with the given\\n    friend. Updates the friend relationship attributes with those given in \"friend_dict\".\\n    '\n    from viewfinder.backend.db.user import User\n    friend = (yield gen.Task(Friend.Query, client, user_id, friend_dict['user_id'], None, must_exist=False))\n    if (friend is None):\n        friend_user = (yield gen.Task(User.Query, client, friend_dict['user_id'], None, must_exist=False))\n        if (friend_user is None):\n            raise NotFoundError(('User %d does not exist.' % friend_dict['user_id']))\n        friend = Friend.CreateFromKeywords(user_id=user_id, friend_id=friend_dict['user_id'], status=Friend.FRIEND)\n    assert (friend_dict['user_id'] == friend.friend_id), (friend_dict, friend)\n    for (key, value) in friend_dict.iteritems():\n        if (key != 'user_id'):\n            assert (key in Friend.FRIEND_ATTRIBUTES), friend_dict\n            setattr(friend, key, value)\n    (yield gen.Task(friend.Update, client))\n    callback()\n", "label": 0}
{"function": "\n\ndef get_instance_stream(self, instream):\n    for source in instream:\n        try:\n            is_doc = issubclass(source, schema.Document)\n            is_instance = False\n        except TypeError:\n            is_doc = False\n            is_instance = isinstance(source, schema.Document)\n        if is_doc:\n            queryset = source.objects.all().iterator()\n        elif is_instance:\n            queryset = [source]\n        elif hasattr(source, 'iterator'):\n            queryset = source.iterator()\n        else:\n            queryset = source\n        for item in queryset:\n            (yield item)\n", "label": 0}
{"function": "\n\ndef getEncodableAttributes(self, obj, codec=None):\n    attrs = pyamf.ClassAlias.getEncodableAttributes(self, obj, codec=codec)\n    gae_objects = (getGAEObjects(codec.context) if codec else None)\n    if (self.reference_properties and gae_objects):\n        for (name, prop) in self.reference_properties.iteritems():\n            klass = prop.reference_class\n            key = prop.get_value_for_datastore(obj)\n            if (not key):\n                continue\n            try:\n                attrs[name] = gae_objects.getClassKey(klass, key)\n            except KeyError:\n                ref_obj = getattr(obj, name)\n                gae_objects.addClassKey(klass, key, ref_obj)\n                attrs[name] = ref_obj\n    for k in attrs.keys()[:]:\n        if k.startswith('_'):\n            del attrs[k]\n    for attr in obj.dynamic_properties():\n        attrs[attr] = getattr(obj, attr)\n    if (not self.no_key_attr):\n        attrs[self.KEY_ATTR] = (str(obj.key()) if obj.is_saved() else None)\n    return attrs\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_hdf(node):\n    data = json.loads(node[0])\n    desc = node[1]\n    try:\n        cmap = json.loads(node[2])\n    except:\n        cmap = node[2]\n    vmin = json.loads(node[3])\n    vmax = json.loads(node[4])\n    state = json.loads(node[5])\n    attrs = json.loads(node[6])\n    try:\n        xfmname = json.loads(node[7])\n    except ValueError:\n        xfmname = None\n    if (not isinstance(vmin, list)):\n        vmin = [vmin]\n    if (not isinstance(vmax, list)):\n        vmax = [vmax]\n    if (not isinstance(cmap, list)):\n        cmap = [cmap]\n    if (len(data) == 1):\n        xfm = (None if (xfmname is None) else xfmname[0])\n        return _from_hdf_view(node.file, data[0], xfmname=xfm, cmap=cmap[0], description=desc, vmin=vmin[0], vmax=vmax[0], state=state, **attrs)\n    else:\n        views = [_from_hdf_view(node.file, d, xfmname=x) for (d, x) in zip(data, xfname)]\n        raise NotImplementedError\n", "label": 1}
{"function": "\n\ndef __init__(self, publication, publisher=None, publisher_authid=None, publisher_authrole=None, topic=None, enc_algo=None):\n    '\\n\\n        :param publication: The publication ID of the event (always present).\\n        :type publication: int\\n        :param publisher: The WAMP session ID of the original publisher of this event.\\n            Only filled when publisher is disclosed.\\n        :type publisher: None or int\\n        :param publisher_authid: The WAMP authid of the original publisher of this event.\\n            Only filled when publisher is disclosed.\\n        :type publisher_authid: None or unicode\\n        :param publisher_authrole: The WAMP authrole of the original publisher of this event.\\n            Only filled when publisher is disclosed.\\n        :type publisher_authrole: None or unicode\\n        :param topic: For pattern-based subscriptions, the actual topic URI being published to.\\n            Only filled for pattern-based subscriptions.\\n        :type topic: None or unicode\\n        :param enc_algo: Payload encryption algorithm that\\n            was in use (currently, either `None` or `\"cryptobox\"`).\\n        :type enc_algo: None or unicode\\n        '\n    assert (type(publication) in six.integer_types)\n    assert ((publisher is None) or (type(publisher) in six.integer_types))\n    assert ((publisher_authid is None) or (type(publisher_authid) == six.text_type))\n    assert ((publisher_authrole is None) or (type(publisher_authrole) == six.text_type))\n    assert ((topic is None) or (type(topic) == six.text_type))\n    assert ((enc_algo is None) or ((type(enc_algo) == six.text_type) and (enc_algo in ['cryptobox'])))\n    self.publication = publication\n    self.publisher = publisher\n    self.publisher_authid = publisher_authid\n    self.publisher_authrole = publisher_authrole\n    self.topic = topic\n    self.enc_algo = enc_algo\n", "label": 0}
{"function": "\n\ndef wait_for_schema_agreement(self, connection=None, preloaded_results=None, wait_time=None):\n    total_timeout = (wait_time if (wait_time is not None) else self._cluster.max_schema_agreement_wait)\n    if (total_timeout <= 0):\n        return True\n    with self._schema_agreement_lock:\n        if self._is_shutdown:\n            return\n        if (not connection):\n            connection = self._connection\n        if preloaded_results:\n            log.debug('[control connection] Attempting to use preloaded results for schema agreement')\n            peers_result = preloaded_results[0]\n            local_result = preloaded_results[1]\n            schema_mismatches = self._get_schema_mismatches(peers_result, local_result, connection.host)\n            if (schema_mismatches is None):\n                return True\n        log.debug('[control connection] Waiting for schema agreement')\n        start = self._time.time()\n        elapsed = 0\n        cl = ConsistencyLevel.ONE\n        schema_mismatches = None\n        while (elapsed < total_timeout):\n            peers_query = QueryMessage(query=self._SELECT_SCHEMA_PEERS, consistency_level=cl)\n            local_query = QueryMessage(query=self._SELECT_SCHEMA_LOCAL, consistency_level=cl)\n            try:\n                timeout = min(self._timeout, (total_timeout - elapsed))\n                (peers_result, local_result) = connection.wait_for_responses(peers_query, local_query, timeout=timeout)\n            except OperationTimedOut as timeout:\n                log.debug('[control connection] Timed out waiting for response during schema agreement check: %s', timeout)\n                elapsed = (self._time.time() - start)\n                continue\n            except ConnectionShutdown:\n                if self._is_shutdown:\n                    log.debug('[control connection] Aborting wait for schema match due to shutdown')\n                    return None\n                else:\n                    raise\n            schema_mismatches = self._get_schema_mismatches(peers_result, local_result, connection.host)\n            if (schema_mismatches is None):\n                return True\n            log.debug('[control connection] Schemas mismatched, trying again')\n            self._time.sleep(0.2)\n            elapsed = (self._time.time() - start)\n        log.warning('Node %s is reporting a schema disagreement: %s', connection.host, schema_mismatches)\n        return False\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_2d_freq():\n    ' reading/writing of 2D RNMRTK frequency domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_2d', 'freq_2d.sec'))\n    assert (data.shape == (2048, 4096))\n    assert (np.abs((data[(0, 1)] - (- 0.19))) <= 0.01)\n    assert (np.abs((data[(10, 18)] - 0.88)) <= 0.01)\n    assert (dic['sw'][1] == 50000.0)\n    assert (dic['sf'][1] == 125.69)\n    assert (dic['ppm'][1] == 55.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 50.65)\n    assert (dic['ppm'][0] == 120.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef redis_client(name):\n    'Get a Redis client.\\n\\n    Uses the name argument to lookup the connection string in the\\n    settings.REDIS_BACKEND dict.\\n    '\n    if (name not in settings.REDIS_BACKENDS):\n        raise RedisError('{k} is not defined in settings.REDIS_BACKENDS'.format(k=name))\n    uri = settings.REDIS_BACKENDS[name]\n    (_, server, params) = parse_backend_uri(uri)\n    db = params.pop('db', 1)\n    try:\n        db = int(db)\n    except (ValueError, TypeError):\n        db = 1\n    try:\n        socket_timeout = float(params.pop('socket_timeout'))\n    except (KeyError, ValueError):\n        socket_timeout = None\n    password = params.pop('password', None)\n    if (':' in server):\n        (host, port) = server.split(':')\n        try:\n            port = int(port)\n        except (ValueError, TypeError):\n            port = 6379\n    else:\n        host = server\n        port = 6379\n    redis = Redis(host=host, port=port, db=db, password=password, socket_timeout=socket_timeout)\n    try:\n        redis.exists('dummy-key')\n    except ConnectionError:\n        raise RedisError('Unable to connect to redis backend: {k}'.format(k=name))\n    return redis\n", "label": 0}
{"function": "\n\n@pytest.mark.skipif((sys.platform == 'darwin'), reason='Microsecond granularity does not work on Mac OS X')\ndef test_local_with_microsecond_auto_version(tmpdir):\n    after_dt = (datetime.now() - timedelta(seconds=1))\n    component_dir = tmpdir.mkdir('component')\n    bower_json_file = component_dir.join('bower.json')\n    bower_json_file.write(json.dumps({\n        'name': 'component',\n        'version': '2.1',\n        'main': 'main.js',\n    }))\n    main_js_file = component_dir.join('main.js')\n    main_js_file.write('/* this is main.js */')\n    bower = bowerstatic.Bower(autoversion=filesystem_microsecond_autoversion)\n    components = bower.components('components', os.path.join(os.path.dirname(__file__), 'bower_components'))\n    local = bower.local_components('local', components)\n    local.component(component_dir.strpath, version=None)\n\n    def wsgi(environ, start_response):\n        start_response('200 OK', [('Content-Type', 'text/html;charset=UTF-8')])\n        include = local.includer(environ)\n        include('component/main.js')\n        return [b'<html><head></head><body>Hello!</body></html>']\n    wrapped = bower.wrap(wsgi)\n    c = Client(wrapped)\n    response = c.get('/')\n    before_dt = datetime.now()\n\n    def get_url_dt(response):\n        s = compat.text_type(response.body, 'UTF-8')\n        start = (s.find('src=\"') + len('src=\"'))\n        end = s.find('\"', start)\n        url = s[start:end]\n        parts = url.split('/')\n        url_dt_str = parts[4]\n        url_dt = datetime.strptime(url_dt_str, '%Y-%m-%dT%H:%M:%S.%f')\n        return (url_dt_str, url_dt)\n    (url_dt_str, url_dt) = get_url_dt(response)\n    assert (url_dt >= after_dt)\n    assert (url_dt <= before_dt)\n    response = c.get(('/bowerstatic/local/component/%s/main.js' % url_dt_str))\n    assert (response.body == b'/* this is main.js */')\n    after_dt = (datetime.now() - timedelta(seconds=1))\n    main_js_file.write('/* this is main.js, modified */')\n    response = c.get('/')\n    before_dt = datetime.now()\n    (original_url_dt_str, original_url_dt) = (url_dt_str, url_dt)\n    (url_dt_str, url_dt) = get_url_dt(response)\n    assert (original_url_dt_str != url_dt_str)\n    assert (url_dt >= after_dt)\n    assert (url_dt <= before_dt)\n    assert (url_dt > original_url_dt)\n    c.get(('/bowerstatic/local/component/%s/main.js' % original_url_dt_str), status=404)\n    response = c.get(('/bowerstatic/local/component/%s/main.js' % url_dt_str))\n    assert (response.body == b'/* this is main.js, modified */')\n", "label": 1}
{"function": "\n\ndef store(self):\n    \"Save the object's metadata into the library database.\\n        \"\n    self._check_db()\n    assignments = []\n    subvars = []\n    for key in self._fields:\n        if ((key != 'id') and (key in self._dirty)):\n            self._dirty.remove(key)\n            assignments.append((key + '=?'))\n            value = self._type(key).to_sql(self[key])\n            subvars.append(value)\n    assignments = ','.join(assignments)\n    with self._db.transaction() as tx:\n        if assignments:\n            query = 'UPDATE {0} SET {1} WHERE id=?'.format(self._table, assignments)\n            subvars.append(self.id)\n            tx.mutate(query, subvars)\n        for (key, value) in self._values_flex.items():\n            if (key in self._dirty):\n                self._dirty.remove(key)\n                tx.mutate('INSERT INTO {0} (entity_id, key, value) VALUES (?, ?, ?);'.format(self._flex_table), (self.id, key, value))\n        for key in self._dirty:\n            tx.mutate('DELETE FROM {0} WHERE entity_id=? AND key=?'.format(self._flex_table), (self.id, key))\n    self.clear_dirty()\n", "label": 1}
{"function": "\n\ndef find_key(self, items, write=False):\n    overwrite = self.config['overwrite'].get(bool)\n    bin = util.bytestring_path(self.config['bin'].get(unicode))\n    for item in items:\n        if (item['initial_key'] and (not overwrite)):\n            continue\n        try:\n            output = util.command_output([bin, b'-f', util.syspath(item.path)])\n        except (subprocess.CalledProcessError, OSError) as exc:\n            self._log.error('execution failed: {0}', exc)\n            continue\n        except UnicodeEncodeError:\n            self._log.error('execution failed for Unicode path: {0!r}', item.path)\n            continue\n        key_raw = output.rsplit(None, 1)[(- 1)]\n        try:\n            key = key_raw.decode('utf8')\n        except UnicodeDecodeError:\n            self._log.error('output is invalid UTF-8')\n            continue\n        item['initial_key'] = key\n        self._log.info('added computed initial key {0} for {1}', key, util.displayable_path(item.path))\n        if write:\n            item.try_write()\n        item.store()\n", "label": 1}
{"function": "\n\ndef testHashingMultipleFiles(self):\n    'Test hashing files.'\n    input_data = [(str(i), str(i)) for i in range(100)]\n    input_data.sort()\n    bucket_name = 'testbucket'\n    test_filename = 'testfile'\n    full_filename = ('/%s/%s' % (bucket_name, test_filename))\n    with cloudstorage.open(full_filename, mode='w') as f:\n        with records.RecordsWriter(f) as w:\n            for (k, v) in input_data:\n                proto = kv_pb.KeyValue()\n                proto.set_key(k)\n                proto.set_value(v)\n                w.write(proto.Encode())\n    p = shuffler._HashPipeline('testjob', bucket_name, [full_filename, full_filename, full_filename])\n    p.start()\n    test_support.execute_until_empty(self.taskqueue)\n    p = shuffler._HashPipeline.from_id(p.pipeline_id)\n    list_of_output_files = p.outputs.default.value\n    output_data = []\n    for output_files in list_of_output_files:\n        for output_file in output_files:\n            with cloudstorage.open(output_file) as f:\n                for binary_record in records.RecordsReader(f):\n                    proto = kv_pb.KeyValue()\n                    proto.ParseFromString(binary_record)\n                    output_data.append((proto.key(), proto.value()))\n    output_data.sort()\n    self.assertEquals(300, len(output_data))\n    for i in range(len(input_data)):\n        self.assertEquals(input_data[i], output_data[(3 * i)])\n        self.assertEquals(input_data[i], output_data[((3 * i) + 1)])\n        self.assertEquals(input_data[i], output_data[((3 * i) + 2)])\n    self.assertEquals(1, len(self.emails))\n", "label": 0}
{"function": "\n\ndef transform(self, node, get_nodes=True):\n    '\\n        Checks if the graph from node corresponds to in_pattern. If it does,\\n        constructs out_pattern and performs the replacement.\\n\\n        '\n    if (get_nodes and (self.get_nodes is not None)):\n        for real_node in self.get_nodes(node):\n            if (real_node == 'output'):\n                continue\n            ret = self.transform(real_node, get_nodes=False)\n            if ((ret is not False) and (ret is not None)):\n                assert (len(real_node.outputs) == len(ret))\n                if self.values_eq_approx:\n                    ret.tag.values_eq_approx = self.values_eq_approx\n                return dict(izip(real_node.outputs, ret))\n    if (node.op != self.op):\n        return False\n\n    def match(pattern, expr, u, allow_multiple_clients=False, pdb=False):\n\n        def retry_with_equiv():\n            if (not self.skip_identities_fn):\n                return False\n            expr_equiv = self.skip_identities_fn(expr)\n            if (expr_equiv is None):\n                return False\n            return match(pattern, expr_equiv, u, allow_multiple_clients=allow_multiple_clients)\n        if isinstance(pattern, (list, tuple)):\n            if (expr.owner is None):\n                return False\n            if ((not (expr.owner.op == pattern[0])) or ((not allow_multiple_clients) and (len(expr.clients) > 1))):\n                return retry_with_equiv()\n            if ((len(pattern) - 1) != len(expr.owner.inputs)):\n                return retry_with_equiv()\n            for (p, v) in zip(pattern[1:], expr.owner.inputs):\n                u = match(p, v, u, self.allow_multiple_clients)\n                if (not u):\n                    return False\n        elif isinstance(pattern, dict):\n            try:\n                real_pattern = pattern['pattern']\n            except KeyError:\n                raise KeyError((\"Malformed pattern: %s (expected key 'pattern')\" % pattern))\n            constraint = pattern.get('constraint', (lambda expr: True))\n            if constraint(expr):\n                return match(real_pattern, expr, u, pattern.get('allow_multiple_clients', allow_multiple_clients))\n            else:\n                return retry_with_equiv()\n        elif isinstance(pattern, string_types):\n            v = unify.Var(pattern)\n            if ((u[v] is not v) and (u[v] is not expr)):\n                return retry_with_equiv()\n            else:\n                u = u.merge(expr, v)\n        elif (isinstance(pattern, (int, float)) and isinstance(expr, graph.Constant)):\n            if numpy.all((theano.tensor.constant(pattern).value == expr.value)):\n                return u\n            else:\n                return retry_with_equiv()\n        elif (isinstance(pattern, graph.Constant) and isinstance(expr, graph.Constant) and pattern.equals(expr)):\n            return u\n        else:\n            return retry_with_equiv()\n        if pdb:\n            import pdb\n            pdb.set_trace()\n        return u\n    u = match(self.in_pattern, node.out, unify.Unification(), True, self.pdb)\n    if u:\n\n        def build(pattern, u):\n            if isinstance(pattern, (list, tuple)):\n                args = [build(p, u) for p in pattern[1:]]\n                return pattern[0](*args)\n            elif isinstance(pattern, string_types):\n                return u[unify.Var(pattern)]\n            elif isinstance(pattern, (int, float)):\n                return pattern\n            else:\n                return pattern.clone()\n        p = self.out_pattern\n        ret = build(p, u)\n        if self.values_eq_approx:\n            ret.tag.values_eq_approx = self.values_eq_approx\n        return [ret]\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef test_jobs():\n    (m, ctl, config) = init()\n    m.load(config)\n    cmd = TestCommand('jobs')\n    ctl.process_command(cmd)\n    cmd1 = TestCommand('jobs', ['default'])\n    ctl.process_command(cmd1)\n    m.stop()\n    m.run()\n    assert isinstance(cmd.result, dict)\n    assert ('jobs' in cmd.result)\n    assert (cmd.result['jobs'] == ['default.dummy'])\n    assert isinstance(cmd1.result, dict)\n    assert ('jobs' in cmd1.result)\n    assert (cmd1.result['jobs'] == ['default.dummy'])\n", "label": 0}
{"function": "\n\ndef pop_chunk(self, chunk_max_size):\n    'Pops a chunk of the given max size.\\n\\n        Optimized to avoid too much string copies.\\n\\n        Args:\\n            chunk_max_size (int): max size of the returned chunk.\\n\\n        Returns:\\n            string (bytes) with a size <= chunk_max_size.\\n        '\n    if (self._total_length < chunk_max_size):\n        res = self._tobytes()\n        self.clear()\n        return res\n    first_iteration = True\n    while True:\n        try:\n            data = self._deque.popleft()\n            data_length = len(data)\n            self._total_length -= data_length\n            if first_iteration:\n                if (data_length == chunk_max_size):\n                    return data\n                elif (data_length > chunk_max_size):\n                    view = self._get_pointer_or_memoryview(data, data_length)\n                    self.appendleft(view[chunk_max_size:])\n                    return view[:chunk_max_size]\n                else:\n                    chunk_write_buffer = WriteBuffer()\n            elif ((chunk_write_buffer._total_length + data_length) > chunk_max_size):\n                view = self._get_pointer_or_memoryview(data, data_length)\n                limit = ((chunk_max_size - chunk_write_buffer._total_length) - data_length)\n                self.appendleft(view[limit:])\n                data = view[:limit]\n            chunk_write_buffer.append(data)\n            if (chunk_write_buffer._total_length >= chunk_max_size):\n                break\n        except IndexError:\n            self._has_view = False\n            break\n        first_iteration = False\n    return chunk_write_buffer._tobytes()\n", "label": 1}
{"function": "\n\ndef _read_tree(self, path):\n    'Given a revision and path, parse the tree data out of git cat-file output.\\n\\n    :returns: a dict from filename -> [list of Symlink, Dir, and File objects]\\n    '\n    path = self._fixup_dot_relative(path)\n    tree = self._trees.get(path)\n    if tree:\n        return tree\n    tree = {\n        \n    }\n    (object_type, tree_data) = self._read_object_from_repo(rev=self.rev, relpath=path)\n    assert (object_type == 'tree')\n    i = 0\n    while (i < len(tree_data)):\n        start = i\n        while (tree_data[i] != ' '):\n            i += 1\n        mode = tree_data[start:i]\n        i += 1\n        start = i\n        while (tree_data[i] != NUL):\n            i += 1\n        name = tree_data[start:i]\n        sha = tree_data[(i + 1):((i + 1) + GIT_HASH_LENGTH)].encode('hex')\n        i += (1 + GIT_HASH_LENGTH)\n        if (mode == '120000'):\n            tree[name] = self.Symlink(name, sha)\n        elif (mode == '40000'):\n            tree[name] = self.Dir(name, sha)\n        else:\n            tree[name] = self.File(name, sha)\n    self._trees[path] = tree\n    return tree\n", "label": 1}
{"function": "\n\ndef generate(self, outdir):\n    if os.path.exists(outdir):\n        if os.path.isdir(outdir):\n            for entry in os.listdir(outdir):\n                path = os.path.join(outdir, entry)\n                if os.path.isdir(path):\n                    shutil.rmtree(path)\n                else:\n                    os.remove(path)\n        else:\n            raise ValueError(('%r is not a directory' % outdir))\n    try:\n        os.mkdir(outdir)\n    except OSError as err:\n        if (err.errno != errno.EEXIST):\n            raise\n    for filespec in self.files:\n        path = os.path.join(outdir, filespec.realpath(self.index_file))\n        if (not os.path.exists(os.path.dirname(path))):\n            os.makedirs(os.path.dirname(path))\n        with open(path, 'wb') as fobj:\n            data = self.view(filespec, mode='generating')\n            fobj.write(data)\n", "label": 1}
{"function": "\n\ndef handle_openid_login_response():\n    conn = db.session.connection()\n    consumer = openid.Consumer(session, None)\n    info = consumer.complete(request.args, request.url)\n    if (info.status == openid.SUCCESS):\n        openid_url = info.identity_url\n        values = {\n            \n        }\n        ax_resp = ax.FetchResponse.fromSuccessResponse(info)\n        if ax_resp:\n            attrs = {\n                'email': 'http://schema.openid.net/contact/email',\n                'name': 'http://schema.openid.net/namePerson/friendly',\n            }\n            for (name, uri) in attrs.iteritems():\n                try:\n                    value = ax_resp.getSingle(uri)\n                    if value:\n                        values[name] = value\n                except KeyError:\n                    pass\n        account_id = lookup_account_id_by_openid(conn, openid_url)\n        if (not account_id):\n            (account_id, account_api_key) = insert_account(conn, {\n                'name': 'OpenID User',\n                'openid': openid_url,\n            })\n        logger.info(\"Successfuly identified OpenID user %s (%d) with email '%s' and nickname '%s'\", openid_url, account_id, values.get('email', ''), values.get('name', ''))\n        return login_user_and_redirect(account_id)\n    elif (info.status == openid.CANCEL):\n        raise Exception('OpenID login has been canceled')\n    else:\n        raise Exception('OpenID login failed')\n", "label": 1}
{"function": "\n\ndef quo(f, g, auto=True):\n    \"\\n        Computes polynomial quotient of ``f`` by ``g``.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy import Poly\\n        >>> from sympy.abc import x\\n\\n        >>> Poly(x**2 + 1, x).quo(Poly(2*x - 4, x))\\n        Poly(1/2*x + 1, x, domain='QQ')\\n\\n        >>> Poly(x**2 - 1, x).quo(Poly(x - 1, x))\\n        Poly(x + 1, x, domain='ZZ')\\n\\n        \"\n    (dom, per, F, G) = f._unify(g)\n    retract = False\n    if (auto and dom.has_Ring and (not dom.has_Field)):\n        (F, G) = (F.to_field(), G.to_field())\n        retract = True\n    if hasattr(f.rep, 'quo'):\n        q = F.quo(G)\n    else:\n        raise OperationNotSupported(f, 'quo')\n    if retract:\n        try:\n            q = q.to_ring()\n        except CoercionFailed:\n            pass\n    return per(q)\n", "label": 0}
{"function": "\n\ndef test_defaults():\n    c1 = Chart()\n    defaults.plot_height = 1000\n    defaults.plot_width = 1000\n    defaults.tools = False\n    c2 = Chart()\n    c3 = Chart()\n    assert (c1.plot_height == 600)\n    assert (c2.plot_height == c3.plot_height == 1000)\n    assert (c1.plot_width == 600)\n    assert (c2.plot_width == c3.plot_width == 1000)\n    assert c1.tools\n    assert (c2.tools == c3.tools == [])\n", "label": 0}
{"function": "\n\ndef _downloadVariable(varname, dbname, dts, bbox):\n    'Downloads the PRISM data products for a specific variable and a set of\\n    dates *dt*. *varname* can be ppt, tmax or tmin.'\n    url = 'prism.oregonstate.edu'\n    ftp = FTP(url)\n    ftp.login()\n    ftp.cwd('daily/{0}'.format(varname))\n    outpath = tempfile.mkdtemp()\n    years = list(set([t.year for t in dts]))\n    for yr in years:\n        ftp.cwd('{0}'.format(yr))\n        filenames = [f for f in ftp.nlst() if ((datetime.strptime(f.split('_')[(- 2)], '%Y%m%d') >= dts[0]) and (datetime.strptime(f.split('_')[(- 2)], '%Y%m%d') <= dts[1]))]\n        for fname in filenames:\n            dt = datetime.strptime(fname.split('_')[(- 2)], '%Y%m%d')\n            with open('{0}/{1}'.format(outpath, fname), 'wb') as f:\n                ftp.retrbinary('RETR {0}'.format(fname), f.write)\n            if fname.endswith('zip'):\n                fz = zipfile.ZipFile('{0}/{1}'.format(outpath, fname))\n                lfilename = filter((lambda s: s.endswith('bil')), fz.namelist())[0]\n                fz.extractall(outpath)\n            else:\n                lfilename = fname\n            tfilename = lfilename.replace('.bil', '.tif')\n            if (bbox is not None):\n                subprocess.call(['gdal_translate', '-projwin', '{0}'.format(bbox[0]), '{0}'.format(bbox[3]), '{0}'.format(bbox[2]), '{0}'.format(bbox[1]), '{0}/{1}'.format(outpath, lfilename), '{0}/{1}'.format(outpath, tfilename)])\n                dbio.ingest(dbname, '{0}/{1}'.format(outpath, tfilename), dt, table[varname], False)\n            else:\n                dbio.ingest(dbname, '{0}/{1}'.format(outpath, lfilename), dt, table[varname], False)\n        ftp.cwd('..')\n", "label": 1}
{"function": "\n\ndef main():\n    a = A()\n    b = unknown(a)\n    assert (b.x == 1)\n    assert (b.y == 2)\n    assert (b.z == 'z')\n    assert (b.w == 'w')\n    c = B()\n    d = unknown(c)\n    assert (d.x == 1)\n    assert (d.y == 2)\n    assert (d.z == 'z')\n    assert (d.w == 'XXX')\n", "label": 1}
{"function": "\n\ndef introduce_vdi(session, sr_ref, vdi_uuid=None, target_lun=None):\n    'Introduce VDI in the host.'\n    try:\n        vdi_ref = _get_vdi_ref(session, sr_ref, vdi_uuid, target_lun)\n        if (vdi_ref is None):\n            greenthread.sleep(CONF.xenserver.introduce_vdi_retry_wait)\n            session.call_xenapi('SR.scan', sr_ref)\n            vdi_ref = _get_vdi_ref(session, sr_ref, vdi_uuid, target_lun)\n    except session.XenAPI.Failure:\n        LOG.exception(_LE('Unable to introduce VDI on SR'))\n        raise exception.StorageError(reason=(_('Unable to introduce VDI on SR %s') % sr_ref))\n    if (not vdi_ref):\n        raise exception.StorageError(reason=(_('VDI not found on SR %(sr)s (vdi_uuid %(vdi_uuid)s, target_lun %(target_lun)s)') % {\n            'sr': sr_ref,\n            'vdi_uuid': vdi_uuid,\n            'target_lun': target_lun,\n        }))\n    try:\n        vdi_rec = session.call_xenapi('VDI.get_record', vdi_ref)\n        LOG.debug(vdi_rec)\n    except session.XenAPI.Failure:\n        LOG.exception(_LE('Unable to get record of VDI'))\n        raise exception.StorageError(reason=(_('Unable to get record of VDI %s on') % vdi_ref))\n    if vdi_rec['managed']:\n        return vdi_ref\n    try:\n        return session.call_xenapi('VDI.introduce', vdi_rec['uuid'], vdi_rec['name_label'], vdi_rec['name_description'], vdi_rec['SR'], vdi_rec['type'], vdi_rec['sharable'], vdi_rec['read_only'], vdi_rec['other_config'], vdi_rec['location'], vdi_rec['xenstore_data'], vdi_rec['sm_config'])\n    except session.XenAPI.Failure:\n        LOG.exception(_LE('Unable to introduce VDI for SR'))\n        raise exception.StorageError(reason=(_('Unable to introduce VDI for SR %s') % sr_ref))\n", "label": 0}
{"function": "\n\ndef process_options(self, args, opts):\n    try:\n        self.settings.setdict(arglist_to_dict(opts.set), priority='cmdline')\n    except ValueError:\n        raise UsageError('Invalid -s value, use -s NAME=VALUE', print_help=False)\n    if opts.logfile:\n        self.settings.set('LOG_ENABLED', True, priority='cmdline')\n        self.settings.set('LOG_FILE', opts.logfile, priority='cmdline')\n    if opts.loglevel:\n        self.settings.set('LOG_ENABLED', True, priority='cmdline')\n        self.settings.set('LOG_LEVEL', opts.loglevel, priority='cmdline')\n    if opts.nolog:\n        self.settings.set('LOG_ENABLED', False, priority='cmdline')\n    if opts.pidfile:\n        with open(opts.pidfile, 'w') as f:\n            f.write((str(os.getpid()) + os.linesep))\n    if opts.pdb:\n        failure.startDebugMode()\n", "label": 0}
{"function": "\n\ndef _process_file(path, formatting=None, verbose=False):\n    \"Check syntax/formatting and fix formatting of a JSON file.\\n\\n    :param formatting: one of 'check' or 'fix' (default: None)\\n\\n    Raises ValueError if JSON syntax is invalid or reformatting needed.\\n    \"\n    with open(path, 'r') as infile:\n        raw = infile.read()\n        try:\n            parsed = _parse_json(raw)\n        except ParserException as err:\n            raise ValueError(err)\n        else:\n            if (formatting in ('check', 'fix')):\n                formatted = _format_parsed_json(parsed)\n                if (formatted != raw):\n                    if (formatting == 'fix'):\n                        with open(path, 'w') as outfile:\n                            outfile.write(formatted)\n                        if verbose:\n                            print(('%s\\n%s' % (path, _indent_note('Reformatted'))))\n                    else:\n                        raise ValueError('Reformatting needed')\n            elif (formatting is not None):\n                raise ValueError('Called with invalid formatting value.')\n", "label": 1}
{"function": "\n\ndef test_AccumBounds():\n    assert (limit((sin(k) - sin((k + 1))), k, oo) == AccumBounds((- 2), 2))\n    assert (limit(((cos(k) - cos((k + 1))) + 1), k, oo) == AccumBounds((- 1), 3))\n    assert (limit((sin(k) - (sin(k) * cos(k))), k, oo) == AccumBounds((- 2), 2))\n    t1 = Mul((S(1) / 2), (1 / ((- 1) + cos(1))), Add(AccumBounds((- 3), 1), cos(1)))\n    assert (limit(simplify(Sum(cos(n).rewrite(exp), (n, 0, k)).doit().rewrite(sin)), k, oo) == t1)\n    t2 = Mul(Add(AccumBounds((- 2), 2), sin(1)), (1 / (((- 2) * cos(1)) + 2)))\n    assert (limit(simplify(Sum(sin(n).rewrite(exp), (n, 0, k)).doit().rewrite(sin)), k, oo) == t2)\n    assert (limit((frac(x) ** x), x, oo) == AccumBounds(0, oo))\n    assert (limit((((sin(x) + 1) / 2) ** x), x, oo) == AccumBounds(0, oo))\n", "label": 1}
{"function": "\n\ndef run(self, command_line=''):\n    assert command_line, 'expected non-empty command line'\n    if (not all((v.file_name() for v in self.window.views()))):\n        show_error(VimError(ERR_NO_FILE_NAME))\n        utils.blink()\n        return\n    if any((v.is_read_only() for v in self.window.views())):\n        show_error(VimError(ERR_READONLY_FILE))\n        utils.blink()\n        return\n    self.window.run_command('save_all')\n    assert (not any((v.is_dirty() for v in self.window.views())))\n    self.window.run_command('close_all')\n    self.window.run_command('exit')\n", "label": 0}
{"function": "\n\ndef __init__(self, elem, nmSpc):\n    try:\n        self.name = elem.find(((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('IndividualName'))).text\n    except AttributeError:\n        self.name = None\n    try:\n        self.organization = elem.find(nmSpc.OWS('ProviderName')).text\n    except AttributeError:\n        self.organization = None\n    try:\n        self.address = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('DeliveryPoint'))).text\n    except AttributeError:\n        self.address = None\n    try:\n        self.city = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('City'))).text\n    except AttributeError:\n        self.city = None\n    try:\n        self.region = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('AdministrativeArea'))).text\n    except AttributeError:\n        self.region = None\n    try:\n        self.postcode = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('PostalCode'))).text\n    except AttributeError:\n        self.postcode = None\n    try:\n        self.country = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('Country'))).text\n    except AttributeError:\n        self.country = None\n    try:\n        self.email = elem.find(((((((nmSpc.OWS('ServiceContact') + '/') + nmSpc.OWS('ContactInfo')) + '/') + nmSpc.OWS('Address')) + '/') + nmSpc.OWS('ElectronicMailAddress'))).text\n    except AttributeError:\n        self.email = None\n", "label": 1}
{"function": "\n\ndef process_filing_body(filingnum, fp=None, logger=None):\n    if (not fp):\n        fp = form_parser()\n    if (not logger):\n        logger = fec_logger()\n    msg = ('process_filing_body: Starting # %s' % filingnum)\n    logger.info(msg)\n    connection = get_connection()\n    cursor = connection.cursor()\n    cmd = ('select fec_id, is_superceded, data_is_processed from fec_alerts_new_filing where filing_number=%s' % filingnum)\n    cursor.execute(cmd)\n    cd = CSV_dumper(connection)\n    result = cursor.fetchone()\n    if (not result):\n        msg = (\"process_filing_body: Couldn't find a new_filing for filing %s\" % filingnum)\n        logger.error(msg)\n        raise FilingHeaderDoesNotExist(msg)\n    header_id = 1\n    is_amended = result[1]\n    is_already_processed = result[2]\n    if is_already_processed:\n        msg = 'process_filing_body: This filing has already been entered.'\n        logger.error(msg)\n        raise FilingHeaderAlreadyProcessed(msg)\n    f1 = filing(filingnum)\n    form = f1.get_form_type()\n    version = f1.get_version()\n    filer_id = f1.get_filer_id()\n    if (not fp.is_allowed_form(form)):\n        if verbose:\n            msg = ('process_filing_body: Not a parseable form: %s - %s' % (form, filingnum))\n            logger.error(msg)\n        return None\n    linenum = 0\n    while True:\n        linenum += 1\n        row = f1.get_body_row()\n        if (not row):\n            break\n        try:\n            linedict = fp.parse_form_line(row, version)\n            process_body_row(linedict, filingnum, header_id, is_amended, cd, filer_id)\n        except ParserMissingError:\n            msg = ('process_filing_body: Unknown line type in filing %s line %s: type=%s Skipping.' % (filingnum, linenum, row[0]))\n            logger.warn(msg)\n            continue\n    cd.commit_all()\n    cd.close()\n    counter = cd.get_counter()\n    total_rows = 0\n    for i in counter:\n        total_rows += counter[i]\n    msg = ('process_filing_body: Filing # %s Total rows: %s Tally is: %s' % (filingnum, total_rows, counter))\n    logger.info(msg)\n    header_data = dict_to_hstore(counter)\n    cmd = (\"update fec_alerts_new_filing set lines_present='%s'::hstore where filing_number=%s\" % (header_data, filingnum))\n    cursor.execute(cmd)\n    cmd = ('update fec_alerts_new_filing set data_is_processed = True where filing_number=%s' % filingnum)\n    cursor.execute(cmd)\n    cmd = (\"update summary_data_committee_overlay set is_dirty=True where fec_id='%s'\" % filer_id)\n    cursor.execute(cmd)\n", "label": 1}
{"function": "\n\ndef test_exists_many_with_initkey_as_digest(self):\n    keys = []\n    key = ('test', 'demo', None, bytearray(\"asd;as[d'as;djk;uyfl\", 'utf-8'))\n    rec = {\n        'name': 'name1',\n        'age': 1,\n    }\n    TestExistsMany.client.put(key, rec)\n    keys.append(key)\n    key = ('test', 'demo', None, bytearray(\"ase;as[d'as;djk;uyfl\", 'utf-8'))\n    rec = {\n        'name': 'name2',\n        'age': 2,\n    }\n    TestExistsMany.client.put(key, rec)\n    keys.append(key)\n    records = TestExistsMany.client.exists_many(keys)\n    for key in keys:\n        TestExistsMany.client.remove(key)\n    assert (type(records) == list)\n    assert (len(records) == 2)\n    i = 0\n    for x in records:\n        if i:\n            assert (x[0][3] == bytearray(b\"ase;as[d'as;djk;uyfl\"))\n        else:\n            assert (x[0][3] == bytearray(b\"asd;as[d'as;djk;uyfl\"))\n        i += 1\n", "label": 1}
{"function": "\n\ndef test_promotes(self):\n    p = Problem(root=Group())\n    p.root.add('comp', self.comp)\n    self.comp.add_param('xxyyzz', 0.0)\n    self.comp.add_param('foobar', 0.0)\n    self.comp.add_output('a:bcd:efg', (- 1))\n    self.comp.add_output('x_y_z', np.zeros(10))\n    self.comp._promotes = ('*',)\n    p.setup(check=False)\n    for name in self.comp._init_params_dict:\n        self.assertTrue(self.comp._promoted(name))\n    for name in self.comp._init_unknowns_dict:\n        self.assertTrue(self.comp._promoted(name))\n    self.assertFalse(self.comp._promoted('blah'))\n    self.comp._promotes = ('x*',)\n    p.setup(check=False)\n    for name in self.comp._init_params_dict:\n        if name.startswith('x'):\n            self.assertTrue(self.comp._promoted(name))\n        else:\n            self.assertFalse(self.comp._promoted(name))\n    for name in self.comp._init_unknowns_dict:\n        if name.startswith('x'):\n            self.assertTrue(self.comp._promoted(name))\n        else:\n            self.assertFalse(self.comp._promoted(name))\n    self.comp._promotes = ('*:efg',)\n    p.setup(check=False)\n    for name in self.comp._init_params_dict:\n        if name.endswith(':efg'):\n            self.assertTrue(self.comp._promoted(name))\n        else:\n            self.assertFalse(self.comp._promoted(name))\n    for name in self.comp._init_unknowns_dict:\n        if name.endswith(':efg'):\n            self.assertTrue(self.comp._promoted(name))\n        else:\n            self.assertFalse(self.comp._promoted(name))\n    try:\n        self.comp._promotes = '*'\n        self.comp._promoted('xxyyzz')\n    except Exception as err:\n        self.assertEqual(text_type(err), \"'comp' promotes must be specified as a list, tuple or other iterator of strings, but '*' was specified\")\n", "label": 1}
{"function": "\n\ndef fit_inside(image, shape):\n    '\\n    Scales image down to fit inside shape preserves proportions of image\\n\\n    Parameters\\n    ----------\\n    image : WRITEME\\n    shape : WRITEME\\n\\n    Returns\\n    -------\\n    WRITEME\\n    '\n    assert (len(image.shape) == 3)\n    assert (len(shape) == 2)\n    if ((image.shape[0] <= shape[0]) and (image.shape[1] <= shape[1])):\n        return image.copy()\n    row_ratio = (float(image.shape[0]) / float(shape[0]))\n    col_ratio = (float(image.shape[1]) / float(shape[1]))\n    if (row_ratio > col_ratio):\n        target_shape = [shape[0], min((image.shape[1] / row_ratio), shape[1])]\n    else:\n        target_shape = [min((image.shape[0] / col_ratio), shape[0]), shape[1]]\n    assert (target_shape[0] <= shape[0])\n    assert (target_shape[1] <= shape[1])\n    assert ((target_shape[0] == shape[0]) or (target_shape[1] == shape[1]))\n    rval = rescale(image, target_shape)\n    return rval\n", "label": 1}
{"function": "\n\ndef test_always_new(self):\n    params = dict(name='name', url='http://url.com/', paypal='paypal')\n    charity = forms.CharityForm(params).save()\n    for (k, v) in params.items():\n        assert (getattr(charity, k) == v)\n    assert charity.id\n    instance = Charity.objects.get(id=charity.id)\n    params['name'] = 'new'\n    new_charity = forms.CharityForm(params, instance=instance).save()\n    for (k, v) in params.items():\n        assert (getattr(new_charity, k) == v)\n    assert (new_charity.id != charity.id)\n", "label": 0}
{"function": "\n\ndef test_symbols():\n    sym1 = util.symbol('foo')\n    assert (sym1.name == 'foo')\n    sym2 = util.symbol('foo')\n    assert (sym1 is sym2)\n    assert (sym1 == sym2)\n    sym3 = util.symbol('bar')\n    assert (sym1 is not sym3)\n    assert (sym1 != sym3)\n    assert (repr(sym3) == 'bar')\n", "label": 0}
{"function": "\n\ndef _do_live_migration(self, context, dest, instance, block_migration, migration, migrate_data):\n    self._set_migration_status(migration, 'preparing')\n    got_migrate_data_object = isinstance(migrate_data, migrate_data_obj.LiveMigrateData)\n    if (not got_migrate_data_object):\n        migrate_data = migrate_data_obj.LiveMigrateData.detect_implementation(migrate_data)\n    try:\n        if (('block_migration' in migrate_data) and migrate_data.block_migration):\n            block_device_info = self._get_instance_block_device_info(context, instance)\n            disk = self.driver.get_instance_disk_info(instance, block_device_info=block_device_info)\n        else:\n            disk = None\n        migrate_data = self.compute_rpcapi.pre_live_migration(context, instance, block_migration, disk, dest, migrate_data)\n    except Exception:\n        with excutils.save_and_reraise_exception():\n            LOG.exception(_LE('Pre live migration failed at %s'), dest, instance=instance)\n            self._set_migration_status(migration, 'failed')\n            self._rollback_live_migration(context, instance, dest, block_migration, migrate_data)\n    self._set_migration_status(migration, 'running')\n    if migrate_data:\n        migrate_data.migration = migration\n    LOG.debug('live_migration data is %s', migrate_data)\n    try:\n        self.driver.live_migration(context, instance, dest, self._post_live_migration, self._rollback_live_migration, block_migration, migrate_data)\n    except Exception:\n        LOG.exception(_LE('Live migration failed.'), instance=instance)\n        with excutils.save_and_reraise_exception():\n            self._set_migration_status(migration, 'failed')\n", "label": 0}
{"function": "\n\ndef process_response(self, request, response):\n    if (not self.is_cms_request(request)):\n        return response\n    from django.utils.cache import add_never_cache_headers\n    if ((hasattr(request, 'toolbar') and request.toolbar.edit_mode) or (not all((ph.cache_placeholder for (ph, __) in getattr(request, 'placeholders', {\n        \n    }).values())))):\n        add_never_cache_headers(response)\n    if (hasattr(request, 'user') and request.user.is_staff and (response.status_code != 500)):\n        try:\n            pk = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE)).only('pk').order_by('-pk')[0].pk\n            if (hasattr(request, 'cms_latest_entry') and (request.cms_latest_entry != pk)):\n                log = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE))[0]\n                request.session['cms_log_latest'] = log.pk\n        except IndexError:\n            pass\n    return response\n", "label": 1}
{"function": "\n\ndef testParse(self):\n    s = 'foo = pkg_resources.tests.test_resources:TestEntryPoints [x]'\n    ep = EntryPoint.parse(s, self.dist)\n    self.assertfields(ep)\n    ep = EntryPoint.parse('bar baz=  spammity[PING]')\n    assert (ep.name == 'bar baz')\n    assert (ep.module_name == 'spammity')\n    assert (ep.attrs == ())\n    assert (ep.extras == ('ping',))\n    ep = EntryPoint.parse(' fizzly =  wocka:foo')\n    assert (ep.name == 'fizzly')\n    assert (ep.module_name == 'wocka')\n    assert (ep.attrs == ('foo',))\n    assert (ep.extras == ())\n    spec = 'html+mako = mako.ext.pygmentplugin:MakoHtmlLexer'\n    ep = EntryPoint.parse(spec)\n    assert (ep.name == 'html+mako')\n", "label": 1}
{"function": "\n\ndef build_routes(iface, **settings):\n    \"\\n    Build a route script for a network interface.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' ip.build_routes eth0 <settings>\\n    \"\n    template = 'rh6_route_eth.jinja'\n    try:\n        if (int(__grains__['osrelease'][0]) < 6):\n            template = 'route_eth.jinja'\n    except ValueError:\n        pass\n    log.debug(('Template name: ' + template))\n    iface = iface.lower()\n    opts = _parse_routes(iface, settings)\n    log.debug('Opts: \\n {0}'.format(opts))\n    try:\n        template = JINJA.get_template(template)\n    except jinja2.exceptions.TemplateNotFound:\n        log.error('Could not load template {0}'.format(template))\n        return ''\n    opts6 = []\n    opts4 = []\n    for route in opts['routes']:\n        ipaddr = route['ipaddr']\n        if salt.utils.validate.net.ipv6_addr(ipaddr):\n            opts6.append(route)\n        else:\n            opts4.append(route)\n    log.debug('IPv4 routes:\\n{0}'.format(opts4))\n    log.debug('IPv6 routes:\\n{0}'.format(opts6))\n    routecfg = template.render(routes=opts4)\n    routecfg6 = template.render(routes=opts6)\n    if settings['test']:\n        routes = _read_temp(routecfg)\n        routes.extend(_read_temp(routecfg6))\n        return routes\n    _write_file_iface(iface, routecfg, _RH_NETWORK_SCRIPT_DIR, 'route-{0}')\n    _write_file_iface(iface, routecfg6, _RH_NETWORK_SCRIPT_DIR, 'route6-{0}')\n    path = os.path.join(_RH_NETWORK_SCRIPT_DIR, 'route-{0}'.format(iface))\n    path6 = os.path.join(_RH_NETWORK_SCRIPT_DIR, 'route6-{0}'.format(iface))\n    routes = _read_file(path)\n    routes.extend(_read_file(path6))\n    return routes\n", "label": 0}
{"function": "\n\ndef createBytecode(irdata):\n    instrs = irdata.flat_instructions\n    (posd, end_pos) = _calcMinimumPositions(instrs)\n    bytecode = bytearray()\n    for ins in instrs:\n        if isinstance(ins, (ir.LazyJumpBase, ir.Switch)):\n            ins.calcBytecode(posd, irdata.labels)\n        bytecode += ins.bytecode\n    assert (len(bytecode) == end_pos)\n    prev_instr_map = dict(zip(instrs[1:], instrs))\n    packed_excepts = []\n    for (s, e, h, c) in irdata.excepts:\n        s = prev_instr_map.get(s, s)\n        s_off = posd[s]\n        e_off = posd[e]\n        h_off = posd[h]\n        assert (s_off <= e_off)\n        if (s_off < e_off):\n            packed_excepts.append(struct.pack('>HHHH', s_off, e_off, h_off, c))\n        else:\n            print('Skipping zero width exception!')\n            assert 0\n    return (bytes(bytecode), packed_excepts)\n", "label": 1}
{"function": "\n\n@register.simple_tag(takes_context=True)\ndef wagtailuserbar(context, position='bottom-right'):\n    try:\n        request = context['request']\n    except KeyError:\n        return ''\n    if (not request.user.has_perm('wagtailadmin.access_admin')):\n        return ''\n    page = get_page_instance(context)\n    if (page is None):\n        return ''\n    if (page.pk is None):\n        return ''\n    try:\n        revision_id = request.revision_id\n    except AttributeError:\n        revision_id = None\n    if (revision_id is None):\n        items = [AdminItem(), ExplorePageItem(Page.objects.get(id=page.id)), EditPageItem(Page.objects.get(id=page.id)), AddPageItem(Page.objects.get(id=page.id))]\n    else:\n        items = [AdminItem(), ExplorePageItem(PageRevision.objects.get(id=revision_id).page), EditPageItem(PageRevision.objects.get(id=revision_id).page), AddPageItem(PageRevision.objects.get(id=revision_id).page), ApproveModerationEditPageItem(PageRevision.objects.get(id=revision_id)), RejectModerationEditPageItem(PageRevision.objects.get(id=revision_id))]\n    for fn in hooks.get_hooks('construct_wagtail_userbar'):\n        fn(request, items)\n    rendered_items = [item.render(request) for item in items]\n    rendered_items = [item for item in rendered_items if item]\n    return render_to_string('wagtailadmin/userbar/base.html', {\n        'request': request,\n        'items': rendered_items,\n        'position': position,\n        'page': page,\n        'revision_id': revision_id,\n    })\n", "label": 1}
{"function": "\n\ndef extract_sparse(classifier, grayscale, *chs):\n    '\\n    Extract using sparse model (slower).\\n\\n    classifier: Caffe Classifier object\\n\\n    '\n    min_side = np.min(img.shape[:2])\n    max_side = np.max(img.shape[:2])\n    scale = min((MIN_SIDE / min_side), 1)\n    if ((max_side * scale) >= MAX_SIDE):\n        scale = (MAX_SIDE / max_side)\n    samples = classifier.blobs['centroids'].data.shape[1]\n    size = classifier.blobs['data'].data.shape[2]\n    centroids = np.zeros((1, samples, 2), dtype=np.float32)\n    full_shape = grayscale.shape[:2]\n    grayscale = resize_by_factor(grayscale, scale)\n    raw_shape = grayscale.shape[:2]\n    st0 = ((size - raw_shape[0]) // 2)\n    en0 = (st0 + raw_shape[0])\n    st1 = ((size - raw_shape[1]) // 2)\n    en1 = (st1 + raw_shape[1])\n    grayscale = image.center_crop_reflect(grayscale, (size, size))\n    data = grayscale[(np.newaxis, np.newaxis)]\n    print('data', data.shape)\n    shape = raw_img.shape[:2]\n    scaled_size = (size // HOLE)\n    scaled_shape = tuple([(shi // HOLE) for shi in raw_shape[:2]])\n    scaleds = ([None] * len(chs))\n    (img_ii, img_jj) = np.meshgrid(range(scaled_shape[0]), range(scaled_shape[1]))\n    ii = (((HOLE // 2) + st0) + (img_ii * HOLE))\n    jj = (((HOLE // 2) + st1) + (img_jj * HOLE))\n    indices = np.arange(0, ii.size, samples)\n    for chunk in indices:\n        chunk_slice = np.s_[chunk:(chunk + samples)]\n        ii_ = ii.ravel()[chunk_slice]\n        jj_ = jj.ravel()[chunk_slice]\n        centroids[0, :len(ii_), 0] = ii_\n        centroids[0, :len(jj_), 1] = jj_\n        ret = classifier.forward(data=grayscale, centroids=centroids)\n        ii_ = img_ii.ravel()[chunk_slice]\n        jj_ = img_jj.ravel()[chunk_slice]\n        for (i, ch) in enumerate(chs):\n            if (scaleds[i] is None):\n                scaleds[i] = np.zeros((scaled_shape + (ret[chs[i]].shape[(- 1)],)))\n            scaleds[i][(ii_, jj_)] = ret[ch][:len(ii_)]\n    scaled_combined = np.concatenate(scaleds, axis=(- 1))\n    full_combined = resize_by_factor(scaled_combined, (HOLE / scale))\n    full_combined = image.center_crop(full_combined, img.shape[:2])\n    assert (full_combined.shape[:2] == img.shape[:2])\n    combined = full_combined\n    res = []\n    cur = 0\n    for (i, ch) in enumerate(chs):\n        C = scaleds[i].shape[(- 1)]\n        res.append(combined[..., cur:(cur + C)])\n        cur += C\n    return tuple(res)\n", "label": 1}
{"function": "\n\ndef _run(self, thread):\n    if ((time.time() - self.heartbeat) >= 15):\n        self.heartbeat = time.time()\n        self._log_status('PUTS')\n    name = uuid.uuid4().hex\n    if self.object_sources:\n        source = random.choice(self.files)\n    elif (self.upper_object_size > self.lower_object_size):\n        source = SourceFile(random.randint(self.lower_object_size, self.upper_object_size))\n    else:\n        source = SourceFile(self.object_size)\n    device = random.choice(self.devices)\n    partition = str(random.randint(1, 3000))\n    container_name = random.choice(self.containers)\n    with self.connection() as conn:\n        try:\n            if self.use_proxy:\n                client.put_object(self.url, self.token, container_name, name, source, content_length=len(source), http_conn=conn)\n            else:\n                node = {\n                    'ip': self.ip,\n                    'port': self.port,\n                    'device': device,\n                }\n                direct_client.direct_put_object(node, partition, self.account, container_name, name, source, content_length=len(source))\n        except client.ClientException as e:\n            self.logger.debug(str(e))\n            self.failures += 1\n        else:\n            self.names.append((device, partition, name, container_name))\n    self.complete += 1\n", "label": 0}
{"function": "\n\n@transaction.atomic\n@login_required\ndef resend_confirmation(request):\n    try:\n        dom_req = RegistrationRequest.get_request_for_username(request.user.username)\n    except Exception:\n        dom_req = None\n    if (not dom_req):\n        inactive_domains_for_user = Domain.active_for_user(request.user, is_active=False)\n        if (len(inactive_domains_for_user) > 0):\n            for domain in inactive_domains_for_user:\n                domain.is_active = True\n                domain.save()\n        return redirect('domain_select')\n    context = get_domain_context()\n    if (request.method == 'POST'):\n        try:\n            send_domain_registration_email(dom_req.new_user_username, dom_req.domain, dom_req.activation_guid, request.user.get_full_name())\n        except Exception:\n            context.update({\n                'current_page': {\n                    'page_name': _('Oops!'),\n                },\n                'error_msg': _('There was a problem with your request'),\n                'error_details': sys.exc_info(),\n                'show_homepage_link': 1,\n            })\n            return render(request, 'error.html', context)\n        else:\n            context.update({\n                'requested_domain': dom_req.domain,\n                'current_page': {\n                    'page_name': 'Confirmation Email Sent',\n                },\n            })\n            return render(request, 'registration/confirmation_sent.html', context)\n    context.update({\n        'requested_domain': dom_req.domain,\n        'current_page': {\n            'page_name': _('Resend Confirmation Email'),\n        },\n    })\n    return render(request, 'registration/confirmation_resend.html', context)\n", "label": 1}
{"function": "\n\ndef route_url(route_name, request, *elements, **kw):\n    try:\n        reg = request.registry\n    except AttributeError:\n        reg = get_current_registry()\n    mapper = reg.getUtility(IRoutesMapper)\n    route = mapper.routes.get(route_name)\n    if (route and ('custom_url_generator' in route.__dict__)):\n        (route_name, request, elements, kw) = route.custom_url_generator(route_name, request, *elements, **kw)\n    anchor = ''\n    qs = ''\n    app_url = None\n    if ('_query' in kw):\n        qs = ('?' + urlencode(kw.pop('_query'), doseq=True))\n    if ('_anchor' in kw):\n        anchor = kw.pop('_anchor')\n        if isinstance(anchor, unicode):\n            anchor = anchor.encode('utf-8')\n        anchor = ('#' + anchor)\n    if ('_app_url' in kw):\n        app_url = kw.pop('_app_url')\n    path = mapper.generate(route_name, kw)\n    if elements:\n        suffix = _join_elements(elements)\n        if (not path.endswith('/')):\n            suffix = ('/' + suffix)\n    else:\n        suffix = ''\n    if (app_url is None):\n        app_url = request.application_url\n    return ((((app_url + path) + suffix) + qs) + anchor)\n", "label": 1}
{"function": "\n\ndef _update_branch(repo, branch, is_active=False):\n    'Update a single branch.'\n    print(INDENT2, 'Updating', (BOLD + branch.name), end=': ')\n    upstream = branch.tracking_branch()\n    if (not upstream):\n        print((YELLOW + 'skipped:'), 'no upstream is tracked.')\n        return\n    try:\n        branch.commit\n    except ValueError:\n        print((YELLOW + 'skipped:'), 'branch has no revisions.')\n        return\n    try:\n        upstream.commit\n    except ValueError:\n        print((YELLOW + 'skipped:'), 'upstream does not exist.')\n        return\n    base = repo.git.merge_base(branch.commit, upstream.commit)\n    if (repo.commit(base) == upstream.commit):\n        print((BLUE + 'up to date'), end='.\\n')\n        return\n    if is_active:\n        try:\n            repo.git.merge(upstream.name, ff_only=True)\n            print((GREEN + 'done'), end='.\\n')\n        except exc.GitCommandError as err:\n            msg = err.stderr\n            if (('local changes' in msg) and ('would be overwritten' in msg)):\n                print((YELLOW + 'skipped:'), 'uncommitted changes.')\n            else:\n                print((YELLOW + 'skipped:'), 'not possible to fast-forward.')\n    else:\n        status = repo.git.merge_base(branch.commit, upstream.commit, is_ancestor=True, with_extended_output=True, with_exceptions=False)[0]\n        if (status != 0):\n            print((YELLOW + 'skipped:'), 'not possible to fast-forward.')\n        else:\n            repo.git.branch(branch.name, upstream.name, force=True)\n            print((GREEN + 'done'), end='.\\n')\n", "label": 1}
{"function": "\n\ndef checkout(self, ref, *args, **kwargs):\n    result = self._repo.checkout(ref, *args, **kwargs)\n    self.ignore.update()\n    status = self._repo.status()\n    for (path, status) in iteritems(status):\n        if (status == GIT_STATUS_CURRENT):\n            continue\n        full_path = self._full_path(path)\n        if (path not in self._repo.index):\n            if (path not in self.ignore):\n                try:\n                    os.unlink(full_path)\n                except OSError:\n                    rmtree(full_path, onerror=(lambda function, fpath, excinfo: log.info(\"Repository: Checkout couldn't delete %s\", fpath)))\n            continue\n        stats = self.get_git_object_default_stats(ref, path)\n        current_stat = os.lstat(full_path)\n        if (stats['st_mode'] != current_stat.st_mode):\n            os.chmod(full_path, current_stat.st_mode)\n            self._repo.index.add(self._sanitize(path))\n    return result\n", "label": 0}
{"function": "\n\ndef delete_app_from_datastore(self, app, email=None):\n    ' Removes information about the named app from the datastore and, if\\n      necessary, the list of applications that this user owns.\\n\\n    Args:\\n      app: A str that corresponds to the appid of the app to delete.\\n      email: A str that indicates the e-mail address of the administrator of\\n        this application, or None if the currently logged-in user is the admin.\\n    Returns:\\n      A UserInfo object for the user with the specified e-mail address, or if\\n        None was provided, the currently logged in user.\\n    '\n    if (email is None):\n        user = users.get_current_user()\n        if (not user):\n            return None\n        email = user.email()\n    try:\n        app_status = self.get_by_id(AppStatus, app)\n        if app_status:\n            app_status.key.delete()\n        user_info = self.get_by_id(UserInfo, email)\n        if user_info:\n            if (app in user_info.owned_apps):\n                user_info.owned_apps.remove(app)\n                user_info.put()\n        return user_info\n    except Exception as err:\n        logging.exception(err)\n        return None\n", "label": 0}
{"function": "\n\ndef test_invalid_dict_fromjson(self):\n    jdict = 'invalid'\n    try:\n        parse(('{\"a\": \"%s\"}' % jdict), {\n            'a': DictType(str, str),\n        }, False)\n        assert False\n    except Exception as e:\n        assert isinstance(e, InvalidInput)\n        assert (e.fieldname == 'a')\n        assert (e.value == jdict)\n        assert (e.msg == ('Value not a valid dict: %s' % jdict))\n", "label": 0}
{"function": "\n\ndef add_image_cb(self, viewer, chname, image, image_info):\n    if (not self.gui_up):\n        return False\n    nothumb = image.get('nothumb', False)\n    if nothumb:\n        return\n    idx = image.get('idx', None)\n    path = image_info.path\n    if (path is not None):\n        path = os.path.abspath(path)\n    name = image_info.name\n    thumbname = name\n    self.logger.info(('making thumb for %s' % thumbname))\n    future = image_info.image_future\n    chinfo = self.fv.get_channelInfo(chname)\n    prefs = chinfo.settings\n    if (not prefs.get('genthumb', False)):\n        return\n    thumbkey = self.get_thumb_key(chname, name, path)\n    with self.thmblock:\n        if (thumbkey in self.thumbDict):\n            return\n    header = image.get_header()\n    metadata = {\n        \n    }\n    for kwd in self.keywords:\n        metadata[kwd] = header.get(kwd, 'N/A')\n    metadata[self.settings.get('mouseover_name_key', 'NAME')] = name\n    thumbpath = self.get_thumbpath(path)\n    with self.thmblock:\n        self.copy_attrs(chinfo.fitsimage)\n        self.thumb_generator.set_image(image)\n        imgwin = self.thumb_generator.get_image_as_widget()\n    label_length = self.settings.get('label_length', None)\n    label_cutoff = self.settings.get('label_cutoff', 'right')\n    if (label_length is not None):\n        thumbname = iohelper.shorten_name(thumbname, label_length, side=label_cutoff)\n    self.insert_thumbnail(imgwin, thumbkey, thumbname, chname, name, path, thumbpath, metadata, future)\n", "label": 1}
{"function": "\n\ndef update(self, *args, **kwargs):\n    '\\n        update() extends rather than replaces existing key lists.\\n        Also accepts keyword args.\\n        '\n    if (len(args) > 1):\n        raise TypeError(('update expected at most 1 arguments, got %d' % len(args)))\n    if args:\n        other_dict = args[0]\n        if isinstance(other_dict, MultiValueDict):\n            for (key, value_list) in other_dict.lists():\n                self.setlistdefault(key).extend(value_list)\n        else:\n            try:\n                for (key, value) in other_dict.items():\n                    self.setlistdefault(key).append(value)\n            except TypeError:\n                raise ValueError('MultiValueDict.update() takes either a MultiValueDict or dictionary')\n    for (key, value) in six.iteritems(kwargs):\n        self.setlistdefault(key).append(value)\n", "label": 1}
{"function": "\n\ndef DownloadActivityList(self, serviceRecord, exhaustive=False):\n    logger.debug('Checking motivato premium state')\n    self._applyPaymentState(serviceRecord)\n    logger.debug('Motivato DownloadActivityList')\n    session = self._get_session(record=serviceRecord)\n    activities = []\n    exclusions = []\n    self._rate_limit()\n    retried_auth = False\n    headers = {\n        \n    }\n    res = session.post((self._urlRoot + '/api/workouts/sync'), headers=headers)\n    if ((res.status_code == 403) and (not retried_auth)):\n        retried_auth = True\n        session = self._get_session(serviceRecord, skip_cache=True)\n    try:\n        respList = res.json()\n    except ValueError:\n        res_txt = res.text\n        raise APIException(('Parse failure in Motivato list resp: %s' % res.status_code))\n    for actInfo in respList:\n        if ('duration' in actInfo):\n            duration = self._durationToSeconds(actInfo['duration'])\n        else:\n            continue\n        activity = UploadedActivity()\n        if ('time_start' in actInfo['metas']):\n            startTimeStr = ((actInfo['training_at'] + ' ') + actInfo['metas']['time_start'])\n        else:\n            startTimeStr = (actInfo['training_at'] + ' 00:00:00')\n        activity.StartTime = self._parseDateTime(startTimeStr)\n        activity.EndTime = (self._parseDateTime(startTimeStr) + timedelta(seconds=duration))\n        activity.Type = self._reverseActivityMappings[actInfo['discipline_id']]\n        activity.Stats.TimerTime = ActivityStatistic(ActivityStatisticUnit.Seconds, value=duration)\n        if ('distance' in actInfo):\n            activity.Stats.Distance = ActivityStatistic(ActivityStatisticUnit.Kilometers, value=float(actInfo['distance']))\n        activity.ServiceData = {\n            'WorkoutID': int(actInfo['id']),\n        }\n        activity.CalculateUID()\n        logger.debug(('Generated UID %s' % activity.UID))\n        activities.append(activity)\n    return (activities, exclusions)\n", "label": 1}
{"function": "\n\ndef test_split(self):\n    from lasagne.layers import InputLayer, DenseLayer, get_all_layers\n    l1 = InputLayer((10, 20))\n    l2 = DenseLayer(l1, 30)\n    l3 = DenseLayer(l2, 40)\n    l4 = DenseLayer(l1, 50)\n    assert (get_all_layers(l3) == [l1, l2, l3])\n    assert (get_all_layers(l4) == [l1, l4])\n    assert (get_all_layers([l3, l4]) == [l1, l2, l3, l4])\n    assert (get_all_layers([l4, l3]) == [l1, l4, l2, l3])\n    assert (get_all_layers(l3, treat_as_input=[l2]) == [l2, l3])\n    assert (get_all_layers([l3, l4], treat_as_input=[l2]) == [l2, l3, l1, l4])\n", "label": 0}
{"function": "\n\ndef run(self):\n    res = None\n    os.system('clear')\n    while True:\n        try:\n            if (not self.server_version):\n                self.server_version = self.client.perform_request('version')\n                if (self.server_version.capabilities and ('async' in self.server_version.capabilities)):\n                    self.block = False\n                elif self.supports_blocking:\n                    self.block = True\n                else:\n                    raise BlockingNotSupportedError('Debugger requires blocking mode')\n            self.render()\n            if (not self.block):\n                done = False\n                while (not done):\n                    res = self.client.perform_request('version', block=True)\n                    if res.is_success:\n                        done = True\n        except ConnectionError as e:\n            try:\n                msg = e.message.args[1].strerror\n            except:\n                try:\n                    msg = e.message.args[0]\n                except:\n                    msg = str(e)\n            traceback.print_exc()\n            self.do_render(error='Error: {}'.format(msg))\n            self.server_version = None\n            time.sleep(1)\n", "label": 1}
{"function": "\n\ndef _get_path(path, key, name):\n    'Helper to get a dataset path'\n    if (path is None):\n        def_path = op.realpath(op.join(op.dirname(__file__), '..', '..', 'examples'))\n        if (get_config(key) is None):\n            key = 'MNE_DATA'\n        path = get_config(key, def_path)\n        if ((not op.exists(path)) or (not os.access(path, os.W_OK))):\n            try:\n                os.mkdir(path)\n            except OSError:\n                try:\n                    logger.info(('Checking for %s data in \"~/mne_data\"...' % name))\n                    path = op.join(op.expanduser('~'), 'mne_data')\n                    if (not op.exists(path)):\n                        logger.info(\"Trying to create '~/mne_data' in home directory\")\n                        os.mkdir(path)\n                except OSError:\n                    raise OSError((\"User does not have write permissions at '%s', try giving the path as an argument to data_path() where user has write permissions, for ex:data_path('/home/xyz/me2/')\" % path))\n    if (not isinstance(path, string_types)):\n        raise ValueError('path must be a string or None')\n    return path\n", "label": 1}
{"function": "\n\ndef test_database():\n    client = mock.MagicMock()\n    database = models.Database(client, 'sample_datasets', tables=['nasdaq', 'www_access'], count=12345, created_at='created_at', updated_at='updated_at', org_name='org_name', permission='administrator')\n    assert (database.org_name == 'org_name')\n    assert (database.permission == 'administrator')\n    assert (database.count == 12345)\n    assert (database.name == 'sample_datasets')\n    assert (database.tables() == ['nasdaq', 'www_access'])\n    assert (database.created_at == 'created_at')\n    assert (database.updated_at == 'updated_at')\n", "label": 1}
{"function": "\n\ndef create_cookie_jar(cookie_file=None):\n    'Return a cookie jar backed by cookie_file\\n\\n    If cooie_file is not provided, we will default it. If the\\n    cookie_file does not exist, we will create it with the proper\\n    permissions.\\n\\n    In the case where we default cookie_file, and it does not exist,\\n    we will attempt to copy the .post-review-cookies.txt file.\\n    '\n    home_path = get_home_path()\n    if (not cookie_file):\n        cookie_file = os.path.join(home_path, RBTOOLS_COOKIE_FILE)\n        post_review_cookies = os.path.join(home_path, '.post-review-cookies.txt')\n        if ((not os.path.isfile(cookie_file)) and os.path.isfile(post_review_cookies)):\n            try:\n                shutil.copyfile(post_review_cookies, cookie_file)\n                os.chmod(cookie_file, 384)\n            except IOError as e:\n                logging.warning(\"There was an error while copying post-review's cookies: %s\", e)\n    if (not os.path.isfile(cookie_file)):\n        try:\n            open(cookie_file, 'w').close()\n            os.chmod(cookie_file, 384)\n        except IOError as e:\n            logging.warning('There was an error while creating a cookie file: %s', e)\n    return (MozillaCookieJar(cookie_file), cookie_file)\n", "label": 0}
{"function": "\n\ndef get_token_from_post_data(self, data):\n    'Get a token response from POST data.\\n\\n        :param data: POST data containing authorization information.\\n        :type data: dict\\n        :rtype: requests.Response\\n        '\n    try:\n        for x in ['grant_type', 'client_id', 'client_secret']:\n            if (not data.get(x)):\n                raise TypeError('Missing required OAuth 2.0 POST param: {0}'.format(x))\n        if ('refresh_token' in data):\n            return self.refresh_token(**data)\n        for x in ['redirect_uri', 'code']:\n            if (not data.get(x)):\n                raise TypeError('Missing required OAuth 2.0 POST param: {0}'.format(x))\n        return self.get_token(**data)\n    except TypeError as exc:\n        self._handle_exception(exc)\n        return self._make_json_error_response('invalid_request')\n    except StandardError as exc:\n        self._handle_exception(exc)\n        return self._make_json_error_response('server_error')\n", "label": 1}
{"function": "\n\ndef main():\n    global remote_host, remote_port, remote_ssl\n    msg.LOG_LEVEL = msg.LOG_LEVELS['ERROR']\n    usage = 'Figure it out :P'\n    parser = optparse.OptionParser(usage=usage)\n    parser.add_option('--url', dest='url', default=None)\n    parser.add_option('--data', dest='data', default=None)\n    parser.add_option('--method', dest='method', default=None)\n    parser.add_option('--host', dest='host', default=None)\n    parser.add_option('--port', dest='port', default=None)\n    parser.add_option('--ssl', dest='ssl', default=None)\n    (options, args) = parser.parse_args()\n    if options.url:\n        data = None\n        err = False\n        if options.data:\n            data = json.loads(options.data)\n        try:\n            r = api.hit_url(options.host, options.url, data, options.method)\n        except HTTPError as e:\n            r = e\n        except URLError as e:\n            r = e\n            err = True\n        try:\n            print(r.code)\n        except Exception:\n            err = True\n        if err:\n            print(r.reason)\n        else:\n            print(r.read().decode('utf-8'))\n        sys.exit(err)\n    if (not options.host):\n        sys.exit(1)\n    remote_host = options.host\n    remote_port = (int(options.port) or remote_port)\n    remote_ssl = (bool(options.ssl) or remote_ssl)\n    proxy = Server()\n    (_, port) = reactor.reactor.listen(proxy, port=int(G.PROXY_PORT))\n\n    def on_ready():\n        print(('Now listening on <%s>' % port))\n        sys.stdout.flush()\n    utils.set_timeout(on_ready, 100)\n    try:\n        reactor.reactor.block()\n    except KeyboardInterrupt:\n        print('ciao')\n", "label": 1}
{"function": "\n\n@permission_required('core.manage_shop')\ndef manage_related_products_inline(request, product_id, as_string=False, template_name='manage/product/related_products_inline.html'):\n    'View which shows all related products for the product with the passed id.\\n    '\n    product = Product.objects.get(pk=product_id)\n    related_products = product.related_products.all()\n    related_products_ids = [p.id for p in related_products]\n    r = (request.POST if (request.method == 'POST') else request.GET)\n    s = request.session\n    if (r.get('keep-filters') or r.get('page')):\n        page = r.get('page', s.get('related_products', 1))\n        filter_ = r.get('filter', s.get('filter'))\n        category_filter = r.get('related_products_category_filter', s.get('related_products_category_filter'))\n    else:\n        page = r.get('page', 1)\n        filter_ = r.get('filter')\n        category_filter = r.get('related_products_category_filter')\n    s['related_products_page'] = page\n    s['filter'] = filter_\n    s['related_products_category_filter'] = category_filter\n    try:\n        s['related-products-amount'] = int(r.get('related-products-amount', s.get('related-products-amount')))\n    except TypeError:\n        s['related-products-amount'] = 25\n    filters = Q()\n    if filter_:\n        filters &= (Q(name__icontains=filter_) | Q(sku__icontains=filter_))\n        filters |= ((Q(sub_type=VARIANT) & Q(active_sku=False)) & Q(parent__sku__icontains=filter_))\n        filters |= ((Q(sub_type=VARIANT) & Q(active_name=False)) & Q(parent__name__icontains=filter_))\n    if category_filter:\n        if (category_filter == 'None'):\n            filters &= Q(categories=None)\n        elif (category_filter == 'All'):\n            pass\n        else:\n            category = lfs_get_object_or_404(Category, pk=category_filter)\n            categories = [category]\n            categories.extend(category.get_all_children())\n            filters &= Q(categories__in=categories)\n    products = Product.objects.filter(filters).exclude(pk__in=related_products_ids).exclude(pk=product.pk)\n    paginator = Paginator(products, s['related-products-amount'])\n    total = products.count()\n    try:\n        page = paginator.page(page)\n    except EmptyPage:\n        page = 0\n    result = render_to_string(template_name, RequestContext(request, {\n        'product': product,\n        'related_products': related_products,\n        'total': total,\n        'page': page,\n        'paginator': paginator,\n        'filter': filter_,\n    }))\n    if as_string:\n        return result\n    else:\n        return HttpResponse(json.dumps({\n            'html': [['#related-products-inline', result]],\n        }), content_type='application/json')\n", "label": 1}
{"function": "\n\ndef test_get_browsers():\n    browsers = utils.get_browsers()\n    browser_list = []\n    for browser in browsers:\n        if (browser[0] == 'Internet Explorer'):\n            assert (int(float(browser[1])) < 20)\n        browser_list.append(browser[0])\n    assert (len(browser_list) == 5)\n    assert ('Firefox' in browser_list)\n    assert ('Opera' in browser_list)\n    assert ('Chrome' in browser_list)\n    assert ('Internet Explorer' in browser_list)\n    assert ('Safari' in browser_list)\n    global browser_list\n", "label": 1}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01, gamma=10, cof=4000):\n    samplerate = self.audio.samplerate\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for y in super(Y3, self).walk(N, freq_base, freq_max, start, end, each=False, combine=False, gamma=gamma, cof=cof):\n        y[..., 1:] = np.diff(y, 1, 1)\n        if (not combine):\n            if each:\n                for v in y:\n                    (yield v)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\n@pytest.mark.slow\n@pytest.inlineCallbacks\ndef test_late_start(self):\n    self.q = Queue.Queue()\n\n    def cb(key, value):\n        self.q.put({\n            'key': key,\n            'value': value,\n        })\n\n    def started_cb(started):\n        self.q.put(started)\n    self.storage = storage.Storage()\n    self.storage.set('test', '1', 1, None)\n    self.storage.set('test', '2', 2, None)\n    self.storage.set('test', '3', 3, None)\n    assert ('test1' in self.storage.localstore)\n    assert ('test2' in self.storage.localstore)\n    assert ('test3' in self.storage.localstore)\n    (yield threads.defer_to_thread(self.storage.start, CalvinCB(started_cb)))\n    (yield threads.defer_to_thread(time.sleep, 2))\n    value = self.q.get(timeout=0.2)\n    assert value\n    assert ('test1' not in self.storage.localstore)\n    assert ('test2' not in self.storage.localstore)\n    assert ('test3' not in self.storage.localstore)\n    (yield threads.defer_to_thread(self.storage.get, 'test', '3', CalvinCB(func=cb)))\n    value = self.q.get(timeout=0.2)\n    assert (value['value'] == 3)\n    (yield threads.defer_to_thread(self.storage.stop))\n", "label": 1}
{"function": "\n\ndef generate_service(directory, name=None, url=None, plural=None):\n    if (not name):\n        name = raw_input('Service Name: ')\n    if (not url):\n        url = raw_input('Endpoint URL: ')\n    if (not plural):\n        plural = raw_input('Plural Form: ')\n    endpoints = []\n    service = os.path.join('app', 'services', (name.lower() + 'Service.js'))\n    with open(os.path.join(directory, 'assets', service), 'w') as f:\n        f.write(('app.service(\\'%sService\\', [\"$http\", \"$q\", function ($http, $q) {\\n    \\'use strict\\';\\n    return {\\n' % name.title()))\n        for method in _endpoints:\n            title = name.title()\n            if method['data']:\n                dataline = ',\\n                data: data'\n            else:\n                dataline = ''\n            if method['ext']:\n                extensions = method['ext']\n            else:\n                extensions = ''\n            if method['args']:\n                arguments = method['args']\n            else:\n                arguments = ''\n            if method['plural']:\n                title = plural.title()\n            serv = _template.substitute(name=method['name'], title=title, args=arguments, method=method['method'], url=url, ext=extensions, dataline=dataline)\n            endpoints.append(serv)\n        f.write(',\\n'.join(endpoints))\n        f.write('\\n\\n    };\\n}]);')\n", "label": 1}
{"function": "\n\ndef _from_hdf_view(h5, data, xfmname=None, vmin=None, vmax=None, **kwargs):\n    try:\n        basestring\n        strcls = (unicode, str)\n    except NameError:\n        strcls = str\n    if isinstance(data, strcls):\n        return _from_hdf_data(h5, data, xfmname=xfmname, vmin=vmin, vmax=vmax, **kwargs)\n    if (len(data) == 2):\n        dim1 = _from_hdf_data(h5, data[0], xfmname=xfmname[0])\n        dim2 = _from_hdf_data(h5, data[1], xfmname=xfmname[1])\n        cls = (Vertex2D if isinstance(dim1, Vertex) else Volume2D)\n        return cls(dim1, dim2, vmin=vmin[0], vmin2=vmin[1], vmax=vmax[0], vmax2=vmax[1], **kwargs)\n    elif (len(data) == 4):\n        (red, green, blue) = [_from_hdf_data(h5, d, xfmname=xfmname) for d in data[:3]]\n        alpha = None\n        if (data[3] is not None):\n            alpha = _from_hdf_data(h5, data[3], xfmname=xfmname)\n        cls = (VertexRGB if isinstance(red, Vertex) else VolumeRGB)\n        return cls(red, green, blue, alpha=alpha, **kwargs)\n    else:\n        raise ValueError('Invalid Dataview specification')\n", "label": 1}
{"function": "\n\ndef test_build_install(self):\n    fdir = self._create_formula_files(_F1)\n    self.client.run(['build', fdir])\n    pkgpath = self.ui._status[(- 1)].split()[(- 1)]\n    assert os.path.exists(pkgpath)\n    self.client.run(['local', 'install', pkgpath])\n    for (path, contents) in _F1['contents']:\n        path = os.path.join(__opts__['file_roots']['base'][0], _F1['definition']['name'], path)\n        assert os.path.exists(path)\n        assert (open(path, 'r').read() == contents)\n    self.client.run(['info', _F1['definition']['name']])\n    lines = self.ui._status[(- 1)].split('\\n')\n    for (key, line) in (('name', 'Name: {0}'), ('version', 'Version: {0}'), ('release', 'Release: {0}'), ('summary', 'Summary: {0}')):\n        assert (line.format(_F1['definition'][key]) in lines)\n    self.ui._error = []\n    self.client.run(['local', 'install', pkgpath])\n    assert (len(self.ui._error) > 0)\n    __opts__['force'] = True\n    self.ui._error = []\n    self.client.run(['local', 'install', pkgpath])\n    assert (len(self.ui._error) == 0)\n    __opts__['force'] = False\n", "label": 1}
{"function": "\n\ndef test_lazy_property():\n    poison = False\n\n    class Foo(object):\n\n        @util.lazy_property\n        def squiznart(self):\n            assert (not poison)\n            return 'abc'\n    assert (Foo.squiznart != 'abc')\n    assert hasattr(Foo.squiznart, '__get__')\n    f = Foo()\n    assert ('squiznart' not in f.__dict__)\n    assert (f.squiznart == 'abc')\n    assert (f.__dict__['squiznart'] == 'abc')\n    poison = True\n    assert (f.squiznart == 'abc')\n    new_foo = Foo()\n    assert_raises(AssertionError, getattr, new_foo, 'squiznart')\n    assert ('squiznart' not in new_foo.__dict__)\n", "label": 1}
{"function": "\n\ndef test_file():\n    nmeafile = pynmea2.NMEAFile(StringIO(TEST_DATA))\n    nmea_strings = nmeafile.read()\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    del nmeafile\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [_f.readline() for i in range(10)]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [s for s in _f]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [_f.next() for i in range(10)]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n", "label": 1}
{"function": "\n\ndef load(fips_dir, proj_dir, pattern):\n    \"load one or more matching configs from fips and current project dir\\n\\n    :param fips_dir:    absolute fips directory\\n    :param proj_dir:    absolute project directory\\n    :param pattern:     config name pattern (e.g. 'linux-make-*')\\n    :returns:   an array of loaded config objects\\n    \"\n    dirs = get_config_dirs(fips_dir, proj_dir)\n    configs = []\n    for curDir in dirs:\n        paths = glob.glob('{}/{}.yml'.format(curDir, pattern))\n        for path in paths:\n            try:\n                with open(path, 'r') as f:\n                    cfg = yaml.load(f)\n                (folder, fname) = os.path.split(path)\n                cfg['path'] = path\n                cfg['folder'] = folder\n                cfg['name'] = os.path.splitext(fname)[0]\n                if ('generator' not in cfg):\n                    cfg['generator'] = 'Default'\n                if ('generator-platform' not in cfg):\n                    cfg['generator-platform'] = None\n                if ('generator-toolset' not in cfg):\n                    cfg['generator-toolset'] = None\n                if ('defines' not in cfg):\n                    cfg['defines'] = None\n                configs.append(cfg)\n            except yaml.error.YAMLError as e:\n                log.error('YML parse error: {}', e.message)\n    return configs\n", "label": 1}
{"function": "\n\n@tornado.gen.coroutine\ndef disbatch(self):\n    '\\n        Disbatch all lowstates to the appropriate clients\\n        '\n    ret = []\n    for low in self.lowstate:\n        if (not self._verify_client(low)):\n            return\n        if ((self.token is not None) and ('token' not in low)):\n            low['token'] = self.token\n        if (not (('token' in low) or (('username' in low) and ('password' in low) and ('eauth' in low)))):\n            ret.append('Failed to authenticate')\n            break\n        try:\n            chunk_ret = (yield getattr(self, '_disbatch_{0}'.format(low['client']))(low))\n            ret.append(chunk_ret)\n        except EauthAuthenticationError as exc:\n            ret.append('Failed to authenticate')\n            break\n        except Exception as ex:\n            ret.append('Unexpected exception while handling request: {0}'.format(ex))\n            logger.error('Unexpected exception while handling request:', exc_info=True)\n    self.write(self.serialize({\n        'return': ret,\n    }))\n    self.finish()\n", "label": 1}
{"function": "\n\ndef super_len(o):\n    total_length = 0\n    current_position = 0\n    if hasattr(o, '__len__'):\n        total_length = len(o)\n    elif hasattr(o, 'len'):\n        total_length = o.len\n    elif hasattr(o, 'getvalue'):\n        total_length = len(o.getvalue())\n    elif hasattr(o, 'fileno'):\n        try:\n            fileno = o.fileno()\n        except io.UnsupportedOperation:\n            pass\n        else:\n            total_length = os.fstat(fileno).st_size\n            if ('b' not in o.mode):\n                warnings.warn(\"Requests has determined the content-length for this request using the binary size of the file: however, the file has been opened in text mode (i.e. without the 'b' flag in the mode). This may lead to an incorrect content-length. In Requests 3.0, support will be removed for files in text mode.\", FileModeWarning)\n    if hasattr(o, 'tell'):\n        try:\n            current_position = o.tell()\n        except (OSError, IOError):\n            current_position = total_length\n    return max(0, (total_length - current_position))\n", "label": 1}
{"function": "\n\n@dispatch(_strtypes)\ndef discover(s):\n    if (not s):\n        return null\n    for f in string_coercions:\n        try:\n            return discover(f(s))\n        except (ValueError, KeyError):\n            pass\n    if (s.isalpha() or s.isspace()):\n        return string\n    try:\n        d = dateparse(s)\n    except (ValueError, OverflowError):\n        pass\n    else:\n        return (date_ if is_zero_time(d.time()) else datetime_)\n    return string\n", "label": 1}
{"function": "\n\ndef get_image_dimensions(file_or_path, close=False):\n    '\\n    A modified version of ``django.core.files.images.get_image_dimensions``\\n    which accounts for Exif orientation.\\n\\n    '\n    p = ImageFile.Parser()\n    if hasattr(file_or_path, 'read'):\n        file = file_or_path\n        file_pos = file.tell()\n        file.seek(0)\n    else:\n        file = open(file_or_path, 'rb')\n        close = True\n    try:\n        chunk_size = 1024\n        while 1:\n            data = file.read(chunk_size)\n            if (not data):\n                break\n            try:\n                p.feed(data)\n            except zlib.error as e:\n                if e.args[0].startswith('Error -5'):\n                    pass\n                else:\n                    raise\n            if p.image:\n                return exif_aware_size(p.image)\n            chunk_size *= 2\n        return None\n    finally:\n        if close:\n            file.close()\n        else:\n            file.seek(file_pos)\n", "label": 1}
{"function": "\n\ndef run_as_initiator(self, terminate=(lambda : False)):\n    recv_timeout = (0.001 * (self.cfg['recv-lto'] + 10))\n    symm = 0\n    try:\n        pdu = self.collect(delay=0.01)\n        while (not terminate()):\n            if (pdu is None):\n                pdu = Symmetry()\n            pdu = self.exchange(pdu, recv_timeout)\n            if (pdu is None):\n                return self.terminate(reason='link disruption')\n            if (pdu == Disconnect(0, 0)):\n                return self.terminate(reason='remote choice')\n            symm = ((symm + 1) if (type(pdu) == Symmetry) else 0)\n            self.dispatch(pdu)\n            pdu = self.collect(delay=0.001)\n            if ((pdu is None) and (symm >= 10)):\n                pdu = self.collect(delay=0.05)\n        else:\n            self.terminate(reason='local choice')\n    except KeyboardInterrupt:\n        print\n        self.terminate(reason='local choice')\n        raise KeyboardInterrupt\n    except IOError:\n        self.terminate(reason='input/output error')\n        raise SystemExit\n    finally:\n        log.debug('llc run loop terminated on initiator')\n", "label": 1}
{"function": "\n\ndef test_unitroots():\n    assert (unitroots(1) == [1])\n    assert (unitroots(2) == [1, (- 1)])\n    (a, b, c) = unitroots(3)\n    assert (a == 1)\n    assert b.ae(((- 0.5) + 0.8660254037844386j))\n    assert c.ae(((- 0.5) - 0.8660254037844386j))\n    assert (unitroots(1, primitive=True) == [1])\n    assert (unitroots(2, primitive=True) == [(- 1)])\n    assert (unitroots(3, primitive=True) == unitroots(3)[1:])\n    assert (unitroots(4, primitive=True) == [j, (- j)])\n    assert (len(unitroots(17, primitive=True)) == 16)\n    assert (len(unitroots(16, primitive=True)) == 8)\n", "label": 1}
{"function": "\n\n@modify_site\ndef create_item(self, path):\n    'Creates a new Location object for the site.\\n\\n        The location path should be canonical and should not contain\\n        parts that are not used for access control (query, fragment,\\n        parameters). Location should not contain non-ascii characters.\\n\\n        Raises:\\n            ValidationError if the path is invalid or if a site\\n            already has a location with such path.\\n            LimitExceeded if the site defines a maximum number of\\n            locations and adding a new one would exceed this number.\\n        '\n    locations_limit = self.site.locations_limit\n    if ((locations_limit is not None) and (self.count() >= locations_limit)):\n        raise LimitExceeded('Locations limit exceeded')\n    if (not url_utils.is_canonical(path)):\n        raise ValidationError('Path should be absolute and normalized (starting with / without /../ or /./ or //).')\n    if (len(path) > self.PATH_LEN_LIMIT):\n        raise ValidationError('Path too long')\n    if url_utils.contains_fragment(path):\n        raise ValidationError(\"Path should not contain fragment ('#' part).\")\n    if url_utils.contains_query(path):\n        raise ValidationError(\"Path should not contain query ('?' part).\")\n    if url_utils.contains_params(path):\n        raise ValidationError(\"Path should not contain parameters (';' part).\")\n    try:\n        path.encode('ascii')\n    except UnicodeError:\n        raise ValidationError('Path should contain only ascii characters.')\n    if (self.get_unique((lambda item: (item.path == path))) is not None):\n        raise ValidationError('Location already exists.')\n    return self._do_create_item(path=path)\n", "label": 1}
{"function": "\n\ndef save_hashes(self, hashes):\n    try:\n        hashes_folder = join(self.cache_dir, self._cached_hash_folder)\n        try:\n            makedirs(hashes_folder)\n        except OSError as e:\n            if (e.errno != EEXIST):\n                LOG.error(str(e))\n                return\n        for file_path in iglob(join(hashes_folder, '*.json')):\n            try:\n                file_obj = open(file_path, 'rb')\n                hashes_meta = json_load(file_obj)\n                file_obj.close()\n                hashes_host = hashes_meta['host']\n                if (hashes_host == self.hub_pool.host):\n                    hashes.difference_update(hashes_meta['hashes'])\n            except (IOError, TypeError, ValueError, KeyError, AttributeError):\n                pass\n        if hashes:\n            try:\n                file_path = join(hashes_folder, ('%d.json' % long(time())))\n                file_obj = open(file_path, 'wb')\n                hashes_meta = {\n                    'version': 2,\n                    'host': self.hub_pool.host,\n                    'hashes': list(hashes),\n                }\n                json_dump(hashes_meta, file_obj, separators=(',', ':'))\n                file_obj.close()\n            except IOError:\n                pass\n    except Exception as e:\n        LOG.error(str(e))\n", "label": 1}
{"function": "\n\ndef _sunos_remotes_on(port, which_end):\n    \"\\n    SunOS specific helper function.\\n    Returns set of ipv4 host addresses of remote established connections\\n    on local or remote tcp port.\\n\\n    Parses output of shell 'netstat' to get connections\\n\\n    [root@salt-master ~]# netstat -f inet -n\\n    TCP: IPv4\\n       Local Address        Remote Address    Swind Send-Q Rwind Recv-Q    State\\n       -------------------- -------------------- ----- ------ ----- ------ -----------\\n       10.0.0.101.4505      10.0.0.1.45329       1064800      0 1055864      0 ESTABLISHED\\n       10.0.0.101.4505      10.0.0.100.50798     1064800      0 1055864      0 ESTABLISHED\\n    \"\n    remotes = set()\n    try:\n        data = subprocess.check_output(['netstat', '-f', 'inet', '-n'])\n    except subprocess.CalledProcessError:\n        log.error('Failed netstat')\n        raise\n    lines = salt.utils.to_str(data).split('\\n')\n    for line in lines:\n        if ('ESTABLISHED' not in line):\n            continue\n        chunks = line.split()\n        (local_host, local_port) = chunks[0].rsplit('.', 1)\n        (remote_host, remote_port) = chunks[1].rsplit('.', 1)\n        if ((which_end == 'remote_port') and (int(remote_port) != port)):\n            continue\n        if ((which_end == 'local_port') and (int(local_port) != port)):\n            continue\n        remotes.add(remote_host)\n    return remotes\n", "label": 1}
{"function": "\n\n@click.command()\n@click.option('--apikey', default=load_config_key, help='API key to use')\n@click.option('--list', 'listcodes', is_flag=True, help='List all valid team code/team name pairs')\n@click.option('--live', is_flag=True, help='Shows live scores from various leagues')\n@click.option('--use12hour', is_flag=True, default=False, help='Displays the time using 12 hour format instead of 24 (default).')\n@click.option('--standings', is_flag=True, help='Standings for a particular league')\n@click.option('--league', '-league', type=click.Choice(LEAGUE_IDS.keys()), help='Choose the league whose fixtures you want to see. See league codes listed in README.')\n@click.option('--players', is_flag=True, help='Shows players for a particular team')\n@click.option('--team', type=click.Choice(TEAM_NAMES.keys()), help='Choose the team whose fixtures you want to see. See team codes listed in README.')\n@click.option('--lookup', is_flag=True, help='Get team name from team code when used with --team command.')\n@click.option('--time', default=6, help='The number of days in the past for which you want to see the scores')\n@click.option('--upcoming', is_flag=True, default=False, help='Displays upcoming games when used with --time command.')\n@click.option('--stdout', 'output_format', flag_value='stdout', default=True, help='Print to stdout')\n@click.option('--csv', 'output_format', flag_value='csv', help='Output in CSV format')\n@click.option('--json', 'output_format', flag_value='json', help='Output in JSON format')\n@click.option('-o', '--output-file', default=None, help='Save output to a file (only if csv or json option is provided)')\ndef main(league, time, standings, team, live, use12hour, players, output_format, output_file, upcoming, lookup, listcodes, apikey):\n    'A CLI for live and past football scores from various football leagues'\n    global headers\n    headers = {\n        'X-Auth-Token': apikey,\n    }\n    try:\n        if ((output_format == 'stdout') and output_file):\n            raise IncorrectParametersException('Printing output to stdout and saving to a file are mutually exclusive')\n        writer = get_writer(output_format, output_file)\n        if listcodes:\n            list_team_codes()\n            return\n        if live:\n            get_live_scores(writer, use12hour)\n            return\n        if standings:\n            if (not league):\n                raise IncorrectParametersException('Please specify a league. Example --standings --league=EPL')\n            get_standings(league, writer)\n            return\n        if team:\n            if lookup:\n                map_team_id(team)\n                return\n            if players:\n                get_team_players(team, writer)\n                return\n            else:\n                get_team_scores(team, time, writer, upcoming, use12hour)\n                return\n        get_league_scores(league, time, writer, upcoming, use12hour)\n    except IncorrectParametersException as e:\n        click.secho(e.message, fg='red', bold=True)\n", "label": 1}
{"function": "\n\ndef parallel_map(function, *args, **kwargs):\n    \"Wrapper around IPython's map_sync() that defaults to map().\\n\\n    This might use IPython's parallel map_sync(), or the standard map()\\n    function if IPython cannot be used.\\n\\n    If the 'ask' keyword argument is true, the user will be prompted to start\\n    IPython engines, but the function will still default to map() if the user\\n    cancels.\\n    If the 'ipython' keyword argument is True, the function will return an\\n    additional boolean indicating whether this was computed through IPython\\n    (True) or with the default map() function (False).\\n    \"\n    say_ipython = kwargs.pop('ipython', False)\n    ask = kwargs.pop('ask', False)\n    if kwargs:\n        raise TypeError('map() got unexpected keyword arguments')\n    try:\n        import IPython.parallel\n    except ImportError:\n        (result, ipython) = (map(function, *args), False)\n    else:\n        from engine_manager import EngineManager\n        c = EngineManager.ensure_controller(connect_only=(not ask))\n        if ((c is not None) and (not c.ids)):\n            EngineManager.start_engines(prompt='A module is performing a parallelizable operation, however no IPython engines are running. Do you want to start some?')\n        if ((c is None) or (not c.ids)):\n            (result, ipython) = (map(function, *args), False)\n        else:\n            ldview = c.load_balanced_view()\n            (result, ipython) = (ldview.map_sync(function, *args), True)\n    if say_ipython:\n        return (result, ipython)\n    else:\n        return result\n", "label": 1}
{"function": "\n\ndef populate_functions(self, schema, filter_func):\n    'Returns a list of function names\\n\\n        filter_func is a function that accepts a FunctionMetadata namedtuple\\n        and returns a boolean indicating whether that function should be\\n        kept or discarded\\n        '\n    metadata = self.dbmetadata['functions']\n    if schema:\n        try:\n            return [func for (func, metas) in metadata[schema].items() for meta in metas if filter_func(meta)]\n        except KeyError:\n            return []\n    else:\n        return [func for schema in self.search_path for (func, metas) in metadata[schema].items() for meta in metas if filter_func(meta)]\n", "label": 1}
{"function": "\n\ndef ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None, ca_certs=None, server_hostname=None, ssl_version=None, ciphers=None, ssl_context=None, ca_cert_dir=None):\n    \"\\n    All arguments except for server_hostname, ssl_context, and ca_cert_dir have\\n    the same meaning as they do when using :func:`ssl.wrap_socket`.\\n\\n    :param server_hostname:\\n        When SNI is supported, the expected hostname of the certificate\\n    :param ssl_context:\\n        A pre-made :class:`SSLContext` object. If none is provided, one will\\n        be created using :func:`create_urllib3_context`.\\n    :param ciphers:\\n        A string of ciphers we wish the client to support. This is not\\n        supported on Python 2.6 as the ssl module does not support it.\\n    :param ca_cert_dir:\\n        A directory containing CA certificates in multiple separate files, as\\n        supported by OpenSSL's -CApath flag or the capath argument to\\n        SSLContext.load_verify_locations().\\n    \"\n    context = ssl_context\n    if (context is None):\n        context = create_urllib3_context(ssl_version, cert_reqs, ciphers=ciphers)\n    if (ca_certs or ca_cert_dir):\n        try:\n            context.load_verify_locations(ca_certs, ca_cert_dir)\n        except IOError as e:\n            raise SSLError(e)\n        except OSError as e:\n            if (e.errno == errno.ENOENT):\n                raise SSLError(e)\n            raise\n    if certfile:\n        context.load_cert_chain(certfile, keyfile)\n    if HAS_SNI:\n        return context.wrap_socket(sock, server_hostname=server_hostname)\n    return context.wrap_socket(sock)\n", "label": 1}
{"function": "\n\ndef make_object_map(table, headercolumn, ranges, colors, type, **kwargs):\n    filenames = False\n    for (key, value) in kwargs.iteritems():\n        if ('filenames' == key):\n            filenames = value\n    count = 0\n    dummy = make_dummy(table.columns.values.tolist(), type)\n    for row in ranges:\n        if (count == 0):\n            count = 1\n            oldrow = row\n            colorgenerator = yieldgen(colors)\n            colordict = {\n                \n            }\n        else:\n            temp = get_range(table, headercolumn, oldrow, row)\n            color = next(colorgenerator)\n            if (filenames == True):\n                filename = (color + '2.geojson')\n            else:\n                filename = (color + '.geojson')\n            try:\n                if (not (len(temp) == 0)):\n                    make_type(temp, filename, type)\n                    colordict[filename] = color\n                else:\n                    make_type(dummy, filename, type)\n                    colordict[filename] = color\n            except Exception:\n                make_type(dummy, filename, type)\n                colordict[filename] = color\n            oldrow = row\n    return colordict\n", "label": 1}
{"function": "\n\ndef bind_params(self, I, E, O, alpha):\n    assert (I.dtype == E.dtype)\n    if (not self.initialized):\n        self.initialized = True\n        self.autotune(I, E, O)\n    if ((O.dtype.type is not np.float32) or self.determ_size):\n        update_temp = self.lib.scratch_buffer(self.output_size)\n        self.convert_args = [update_temp, 'f4', O, self.determ_shape]\n        if self.trans_size:\n            input_temp = self.lib.scratch_buffer_offset(self.trans_size)\n    else:\n        update_temp = O.gpudata\n        self.convert_args = False\n        if self.trans_size:\n            input_temp = self.lib.scratch_buffer(self.trans_size)\n    if self.trans_size:\n        self.trans_args[2:5] = (self.lib.stream, input_temp, I.gpudata)\n    else:\n        input_temp = I.gpudata\n    if self.zero:\n        self.zero_args = [update_temp, 0, O.size, self.lib.stream]\n    self.kernel[3:8] = (self.lib.stream, update_temp, input_temp, E.gpudata, alpha)\n", "label": 1}
{"function": "\n\ndef unpack(ext, source, dest_path):\n    '\\n    Unpack the archive |source| to |dest_path|.\\n    Note: |source| can be a file handle or a path.\\n    |ext| contains the extension of the archive.\\n    '\n    if (ext != '.zip'):\n        close_source = False\n        try:\n            if isinstance(source, basestring):\n                source = open(source, 'rb')\n                close_source = True\n            if ((ext == '.tar.gz') or (ext == '.tgz')):\n                un_tar_directory(source, dest_path, 'gz')\n            elif (ext == '.tar.bz2'):\n                un_tar_directory(source, dest_path, 'bz2')\n            elif (ext == '.gz'):\n                with open(dest_path, 'wb') as f:\n                    shutil.copyfileobj(un_gzip_stream(source), f)\n            else:\n                raise UsageError('Not an archive.')\n        except (tarfile.TarError, IOError):\n            raise UsageError('Invalid archive upload.')\n        finally:\n            if close_source:\n                source.close()\n    else:\n        delete_source = False\n        try:\n            if (not isinstance(source, basestring)):\n                temp_path = (dest_path + '.zip')\n                with open(temp_path, 'wb') as f:\n                    shutil.copyfileobj(source, f)\n                source = temp_path\n                delete_source = True\n            exitcode = subprocess.call(['unzip', '-q', source, '-d', dest_path])\n            if (exitcode != 0):\n                raise UsageError('Invalid archive upload.')\n        finally:\n            if delete_source:\n                path_util.remove(source)\n", "label": 1}
{"function": "\n\ndef finish(self):\n    self.done = True\n    if (self.has_trailers and hasattr(self.fp, 'read_trailer_lines')):\n        self.trailers = {\n            \n        }\n        try:\n            for line in self.fp.read_trailer_lines():\n                if (line[0] in ntob(' \\t')):\n                    v = line.strip()\n                else:\n                    try:\n                        (k, v) = line.split(ntob(':'), 1)\n                    except ValueError:\n                        raise ValueError('Illegal header line.')\n                    k = k.strip().title()\n                    v = v.strip()\n                if (k in comma_separated_headers):\n                    existing = self.trailers.get(envname)\n                    if existing:\n                        v = ntob(', ').join((existing, v))\n                self.trailers[k] = v\n        except Exception:\n            e = sys.exc_info()[1]\n            if (e.__class__.__name__ == 'MaxSizeExceeded'):\n                raise cherrypy.HTTPError(413, ('Maximum request length: %r' % e.args[1]))\n            else:\n                raise\n", "label": 1}
{"function": "\n\ndef rotate_clip(data_np, theta_deg, rotctr_x=None, rotctr_y=None, out=None):\n    '\\n    Rotate numpy array `data_np` by `theta_deg` around rotation center\\n    (rotctr_x, rotctr_y).  If the rotation center is omitted it defaults\\n    to the center of the array.\\n\\n    No adjustment is done to the data array beforehand, so the result will\\n    be clipped according to the size of the array (the output array will be\\n    the same size as the input array).\\n    '\n    if (math.fmod(theta_deg, 360.0) == 0.0):\n        return data_np\n    (ht, wd) = data_np.shape[:2]\n    if (rotctr_x is None):\n        rotctr_x = (wd // 2)\n    if (rotctr_y is None):\n        rotctr_y = (ht // 2)\n    if have_opencv:\n        M = cv2.getRotationMatrix2D((rotctr_y, rotctr_x), theta_deg, 1)\n        if (out is not None):\n            out[:, :, ...] = cv2.warpAffine(data_np, M, (wd, ht))\n            newdata = out\n        else:\n            newdata = cv2.warpAffine(data_np, M, (wd, ht))\n            (new_ht, new_wd) = newdata.shape[:2]\n            assert ((wd == new_wd) and (ht == new_ht)), Exception(('rotated cutout is %dx%d original=%dx%d' % (new_wd, new_ht, wd, ht)))\n    else:\n        (yi, xi) = numpy.mgrid[0:ht, 0:wd]\n        xi -= rotctr_x\n        yi -= rotctr_y\n        cos_t = numpy.cos(numpy.radians(theta_deg))\n        sin_t = numpy.sin(numpy.radians(theta_deg))\n        if have_numexpr:\n            ap = ne.evaluate('(xi * cos_t) - (yi * sin_t) + rotctr_x')\n            bp = ne.evaluate('(xi * sin_t) + (yi * cos_t) + rotctr_y')\n        else:\n            ap = (((xi * cos_t) - (yi * sin_t)) + rotctr_x)\n            bp = (((xi * sin_t) + (yi * cos_t)) + rotctr_y)\n        numpy.rint(ap, out=ap)\n        ap = ap.astype('int')\n        ap.clip(0, (wd - 1), out=ap)\n        numpy.rint(bp, out=bp)\n        bp = bp.astype('int')\n        bp.clip(0, (ht - 1), out=bp)\n        if (out is not None):\n            out[:, :, ...] = data_np[(bp, ap)]\n            newdata = out\n        else:\n            newdata = data_np[(bp, ap)]\n            (new_ht, new_wd) = newdata.shape[:2]\n            assert ((wd == new_wd) and (ht == new_ht)), Exception(('rotated cutout is %dx%d original=%dx%d' % (new_wd, new_ht, wd, ht)))\n    return newdata\n", "label": 1}
{"function": "\n\ndef _import_arbitrary(self, records):\n    if (records == []):\n        return (pd.Series([]), pd.Series([]))\n    try:\n        sorted_records = sorted(records, key=(lambda x: x['start']))\n    except KeyError:\n        message = 'Records must all have a \"start\" key and an \"end\" key.'\n        raise ValueError(message)\n    start_datetimes = []\n    values = []\n    estimateds = []\n    previous_end_datetime = None\n    for record in sorted_records:\n        start = record.get('start')\n        end = record.get('end')\n        value = record.get('value')\n        estimated = record.get('estimated')\n        if ((start is None) or (end is None)):\n            message = 'Records must all have a \"start\" key and an \"end\" key.'\n            raise ValueError(message)\n        elif (start >= end):\n            message = 'Record start must be earlier than end:{} >= {}.'.format(start, end)\n            raise ValueError(message)\n        if ((previous_end_datetime is None) or (start == previous_end_datetime)):\n            start_datetimes.append(start)\n            values.append(value)\n            previous_end_datetime = end\n            estimateds.append(bool(estimated))\n        elif (start < previous_end_datetime):\n            message = 'Skipping overlapping record: start ({}) < previous end ({})'.format(start, previous_end_datetime)\n            warn(message)\n        else:\n            start_datetimes.append(previous_end_datetime)\n            values.append(np.nan)\n            estimateds.append(False)\n            start_datetimes.append(start)\n            values.append(value)\n            previous_end_datetime = end\n            estimateds.append(bool(estimated))\n    start_datetimes.append(previous_end_datetime)\n    values.append(np.nan)\n    estimateds.append(False)\n    dt_index = pd.DatetimeIndex(start_datetimes)\n    data = pd.Series(values, index=dt_index)\n    estimated = pd.Series(estimateds, index=dt_index)\n    return (data, estimated)\n", "label": 1}
{"function": "\n\ndef ipfs_add(self, album):\n    try:\n        album_dir = album.item_dir()\n    except AttributeError:\n        return False\n    try:\n        if album.ipfs:\n            self._log.debug('{0} already added', album_dir)\n            return False\n    except AttributeError:\n        pass\n    self._log.info('Adding {0} to ipfs', album_dir)\n    cmd = 'ipfs add -q -r'.split()\n    cmd.append(album_dir)\n    try:\n        output = util.command_output(cmd).split()\n    except (OSError, subprocess.CalledProcessError) as exc:\n        self._log.error('Failed to add {0}, error: {1}', album_dir, exc)\n        return False\n    length = len(output)\n    for (linenr, line) in enumerate(output):\n        line = line.strip()\n        if (linenr == (length - 1)):\n            self._log.info('album: {0}', line)\n            album.ipfs = line\n        else:\n            try:\n                item = album.items()[linenr]\n                self._log.info('item: {0}', line)\n                item.ipfs = line\n                item.store()\n            except IndexError:\n                pass\n    return True\n", "label": 1}
{"function": "\n\ndef write_pnm(file, width, height, pixels, meta):\n    'Write a Netpbm PNM/PAM file.\\n    '\n    bitdepth = meta['bitdepth']\n    maxval = ((2 ** bitdepth) - 1)\n    planes = meta['planes']\n    assert (planes in (1, 2, 3, 4))\n    if (planes in (1, 3)):\n        if (1 == planes):\n            fmt = 'P5'\n        else:\n            fmt = 'P6'\n        header = ('%s %d %d %d\\n' % (fmt, width, height, maxval))\n    if (planes in (2, 4)):\n        if (2 == planes):\n            tupltype = 'GRAYSCALE_ALPHA'\n        else:\n            tupltype = 'RGB_ALPHA'\n        header = ('P7\\nWIDTH %d\\nHEIGHT %d\\nDEPTH %d\\nMAXVAL %d\\nTUPLTYPE %s\\nENDHDR\\n' % (width, height, planes, maxval, tupltype))\n    file.write(header.encode('ascii'))\n    vpr = (planes * width)\n    fmt = ('>%d' % vpr)\n    if (maxval > 255):\n        fmt = (fmt + 'H')\n    else:\n        fmt = (fmt + 'B')\n    for row in pixels:\n        file.write(struct.pack(fmt, *row))\n    file.flush()\n", "label": 1}
{"function": "\n\ndef read(self, address, size):\n    if arbitrary(self, address):\n        raise ArbitraryRead(self, address)\n    as_ = concretise(self, address)\n    try:\n        if (len(as_) > 1):\n            e = None\n            value = bv.Symbol(size, unique_name('read'))\n            for a in as_:\n                v = None\n                for i in range(0, (size // 8)):\n                    if (v is None):\n                        v = self.memory.read_byte(self, (a.value + i))\n                    else:\n                        v = self.memory.read_byte(self, (a.value + i)).concatenate(v)\n                if (e is None):\n                    e = ((address == a) & (value == v))\n                else:\n                    e = (e | ((address == a) & (value == v)))\n            self.solver.add(e)\n        else:\n            value = self.memory.read_byte(self, as_[0].value)\n            for i in range(1, (size // 8)):\n                value = self.memory.read_byte(self, (as_[0].value + i)).concatenate(value)\n    except KeyError:\n        raise InvalidRead(self, address)\n    return value\n", "label": 1}
{"function": "\n\ndef download_instances(self, form_id, cursor=0, num_entries=100):\n    self.logger.debug(('Starting submissions download for %s' % form_id))\n    if (not self._get_response(self.submission_list_url, params={\n        'formId': form_id,\n        'numEntries': num_entries,\n        'cursor': cursor,\n    })):\n        self.logger.error(('Fetching %s formId: %s, cursor: %s' % (self.submission_list_url, form_id, cursor)))\n        return\n    response = self._current_response\n    self.logger.debug(('Fetching %s formId: %s, cursor: %s' % (self.submission_list_url, form_id, cursor)))\n    try:\n        xml_doc = clean_and_parse_xml(response.content)\n    except ExpatError:\n        return\n    instances = self.get_instances_uuids(xml_doc)\n    path = os.path.join(self.forms_path, form_id, 'instances')\n    for uuid in instances:\n        self.logger.debug(('Fetching %s %s submission' % (uuid, form_id)))\n        form_str = ('%(formId)s[@version=null and @uiVersion=null]/%(formId)s[@key=%(instanceId)s]' % {\n            'formId': form_id,\n            'instanceId': uuid,\n        })\n        instance_path = os.path.join(path, uuid.replace(':', ''), 'submission.xml')\n        if (not default_storage.exists(instance_path)):\n            if self._get_response(self.download_submission_url, params={\n                'formId': form_str,\n            }):\n                instance_res = self._current_response\n                content = instance_res.content.strip()\n                default_storage.save(instance_path, ContentFile(content))\n            else:\n                continue\n        else:\n            instance_res = default_storage.open(instance_path)\n            content = instance_res.read()\n        try:\n            instance_doc = clean_and_parse_xml(content)\n        except ExpatError:\n            continue\n        media_path = os.path.join(path, uuid.replace(':', ''))\n        self.download_media_files(instance_doc, media_path)\n        self.logger.debug(('Fetched %s %s submission' % (form_id, uuid)))\n    if xml_doc.getElementsByTagName('resumptionCursor'):\n        rs_node = xml_doc.getElementsByTagName('resumptionCursor')[0]\n        cursor = rs_node.childNodes[0].nodeValue\n        if (self.resumption_cursor != cursor):\n            self.resumption_cursor = cursor\n            self.download_instances(form_id, cursor)\n", "label": 1}
{"function": "\n\n@ensure_empty\n@ensure_visible\n@ensure_connected\ndef test_properties(editor):\n    mode = get_mode(editor)\n    mode.trigger_key = 'A'\n    assert (mode.trigger_key == 'A')\n    mode.trigger_length = 3\n    assert (mode.trigger_length == 3)\n    mode.trigger_symbols = ['.', '->']\n    assert (len(mode.trigger_symbols) == 2)\n    assert ('.' in mode.trigger_symbols)\n    assert ('->' in mode.trigger_symbols)\n    mode.show_tooltips = False\n    assert (mode.show_tooltips is False)\n    mode.case_sensitive = True\n    assert (mode.case_sensitive is True)\n    mode.trigger_key = 1\n", "label": 1}
{"function": "\n\ndef main():\n    '\\n    Start main program: Parse user arguments and take action\\n    '\n    parser = argparse.ArgumentParser(description='mx: Orchestrate tmux sessions and git projects')\n    parser.add_argument('action', type=str, nargs='?', default='start', choices=(WORKSPACE_COMMANDS + GIT_COMMANDS), help='an action for %(prog)s (default: %(default)s)')\n    parser.add_argument('session', type=str, nargs='?', help=\"session for %(prog)s to load (default: current directory's .mx.yml)\")\n    parser.add_argument('-c', '--config', type=str, default='.mx.yml', help='workspace yml config file (default: %(default)s)')\n    parser.add_argument('-v', action='version', version='%(prog)s {}'.format(__version__))\n    args = parser.parse_args()\n    cache_dir = os.environ.get('XDG_CACHE_HOME', os.path.join(os.environ.get('HOME'), '.cache'))\n    pool_dir = os.path.join(cache_dir, 'mx')\n    if args.session:\n        cfg_path = os.path.join(pool_dir, '{}.yml'.format(args.session))\n    else:\n        cfg_path = os.path.realpath(args.config)\n    log = Logger()\n    if (not os.path.isfile(cfg_path)):\n        if (args.action == 'init'):\n            schema = Workspace.initialize(os.getcwd())\n            with open(cfg_path, 'w') as cfg_file:\n                cfg_file.write(yaml.safe_dump(schema, default_flow_style=False))\n            args.action = 'status'\n        else:\n            log.echo('[red]ERROR: [reset]Unable to find [white]{}'.format(cfg_path))\n            sys.exit(2)\n    try:\n        with open(cfg_path, 'r') as stream:\n            config = yaml.load(stream)\n        run(config, args.action)\n        if (not os.path.isdir(pool_dir)):\n            os.makedirs(pool_dir)\n        link = os.path.join(pool_dir, '{}.yml'.format(config.get('name')))\n        if (not os.path.isfile(link)):\n            os.symlink(cfg_path, link)\n    except (WorkspaceException, TmuxException) as e:\n        if (hasattr(e, '__context__') and e.__context__):\n            log.echo(' -> {}'.format(e.__context__))\n        if hasattr(e, 'errors'):\n            log.echo('[red]{}: [reset]{}'.format(e.message, e.errors))\n        else:\n            log.echo('[red]Raw error: [reset]{}'.format(str(e)))\n        sys.exit(3)\n", "label": 1}
{"function": "\n\ndef run_wsgi(self):\n    headers_set = []\n    headers_sent = []\n\n    def write(data):\n        if (not headers_set):\n            raise AssertionError('write() before start_response()')\n        elif (not headers_sent):\n            buf = []\n            (status, response_headers) = headers_sent[:] = headers_set\n            (code, _, msg) = status.partition(' ')\n            buf.append(wsgi_to_bytes(('%s %d %s\\r\\n' % (self.protocol_version, int(code), msg))))\n            header_keys = set()\n            for (key, value) in response_headers:\n                buf.append(wsgi_to_bytes(('%s: %s\\r\\n' % (key, value))))\n                header_keys.add(key.lower())\n            if ('content-length' not in header_keys):\n                self.close_connection = True\n            if ('server' not in header_keys):\n                buf.append(wsgi_to_bytes(('Server: uvwsgi/%s\\r\\n' % __version__)))\n            if ('date' not in header_keys):\n                buf.append(wsgi_to_bytes(('Date: %s\\r\\n' % date_time_string())))\n            buf.append(b'\\r\\n')\n            self.connection.write(b''.join(buf))\n        assert (type(data) is bytes), 'applications must write bytes'\n        self.connection.write(data)\n\n    def start_response(status, response_headers, exc_info=None):\n        if exc_info:\n            try:\n                if headers_sent:\n                    reraise(*exc_info)\n            finally:\n                exc_info = None\n        elif headers_set:\n            raise AssertionError('Headers already set!')\n        headers_set[:] = [status, response_headers]\n        return write\n    env = self.connection.parser.get_wsgi_environ()\n    env['wsgi.version'] = (1, 0)\n    env['wsgi.url_scheme'] = 'http'\n    env['wsgi.input'] = self.body\n    env['wsgi.errors'] = ErrorStream(logger)\n    env['wsgi.multithread'] = False\n    env['wsgi.multiprocess'] = False\n    env['wsgi.run_once'] = False\n    app = self.connection.server.app\n    try:\n        app_response = app(env, start_response)\n    except Exception:\n        logger.exception('Running WSGI application')\n        if DEBUG:\n            response_body = traceback.format_exc()\n            response_headers = [('Content-Type', 'text/plain'), ('Content-Length', str(len(response_body)))]\n            start_response('500 Internal Server Error', response_headers, exc_info=sys.exc_info())\n            app_response = [response_body]\n        else:\n            self.connection.finish()\n            return\n    try:\n        for data in app_response:\n            write(data)\n        if (not headers_sent):\n            write(b'')\n    except Exception:\n        logger.exception('Running WSGI application')\n    finally:\n        if hasattr(app_response, 'close'):\n            app_response.close()\n    self.end()\n    if DEBUG:\n        status = headers_set[0]\n        self._log(int(status.split()[0]))\n", "label": 1}
{"function": "\n\ndef refresh(self):\n    if self.subscribed:\n        self.title = 'Subscribed projects'\n        if self.unreviewed:\n            self.title += ' with unreviewed changes'\n    else:\n        self.title = 'All projects'\n    self.app.status.update(title=self.title)\n    with self.app.db.getSession() as session:\n        i = 0\n        for project in session.getProjects(topicless=True, subscribed=self.subscribed, unreviewed=self.unreviewed):\n            i = self._projectRow(i, project, None)\n        for topic in session.getTopics():\n            i = self._topicRow(i, topic)\n            topic_unreviewed = 0\n            topic_open = 0\n            for project in topic.projects:\n                cache = self.app.project_cache.get(project)\n                topic_unreviewed += cache['unreviewed_changes']\n                topic_open += cache['open_changes']\n                if self.subscribed:\n                    if (not project.subscribed):\n                        continue\n                    if (self.unreviewed and (not cache['unreviewed_changes'])):\n                        continue\n                if (topic.key in self.open_topics):\n                    i = self._projectRow(i, project, topic)\n            topic_row = self.topic_rows.get(topic.key)\n            topic_row.update(topic, topic_unreviewed, topic_open)\n    while (i < len(self.listbox.body)):\n        current_row = self.listbox.body[i]\n        self._deleteRow(current_row)\n", "label": 1}
{"function": "\n\ndef test_getStats(self):\n    self.cov.start()\n    import testUnit.getStats, testUnit.getRanges\n    reload(testUnit.getRanges)\n    testUnit.getStats.foo()\n    testUnit.getRanges.foo()\n    self.cov.stop()\n    f = self.cov.fileDict[os.path.abspath('testUnit/getStats.py')]\n    assert (f.numExecutable == 4)\n    assert (f.numExecuted == 3)\n    assert (f.percentage == 75)\n    assert (f.notExecutedRanges == [5])\n    f = self.cov.fileDict[os.path.abspath('testUnit/getRanges.py')]\n    assert (f.numExecutable == 12)\n    assert (f.notExecutedRanges == [6, 9, 14])\n    assert (f.numExecuted == 9)\n    assert (f.percentage == 75)\n", "label": 1}
{"function": "\n\ndef test_json_conversion():\n    from commonast import Node, Assign, Name, BinOp, Bytes, Num\n    roota = Assign([Name('foo')], BinOp('Add', Name('a'), Num(3)))\n    rootb = Assign([Name('foo')], BinOp('Add', None, Num(3.2)))\n    rootc = Assign([Name('foo')], BinOp('Add', Bytes(b'xx'), Num(4j)))\n    for node1 in (roota, rootb, rootc):\n        js = node1.tojson()\n        node2 = Node.fromjson(js)\n        assert (js.count('BinOp') == 1)\n        assert (js.count('Num') == 1)\n        assert (node2.target_nodes[0].name == node1.target_nodes[0].name)\n        assert (node2.value_node.op == node1.value_node.op)\n        assert (node2.value_node.left_node == node1.value_node.left_node)\n        assert (node2.value_node.right_node.value == node1.value_node.right_node.value)\n        (node1 == node2)\n    assert (roota != rootb)\n    assert (roota != rootc)\n    with raises(ValueError):\n        (roota == 5)\n    assert (str(roota) == roota.tojson())\n    assert (len(repr(roota)) < 80)\n", "label": 1}
{"function": "\n\ndef encode(self, command, source, dest, pretend=False):\n    'Encode `source` to `dest` using command template `command`.\\n\\n        Raises `subprocess.CalledProcessError` if the command exited with a\\n        non-zero status code.\\n        '\n    assert isinstance(command, bytes)\n    assert isinstance(source, bytes)\n    assert isinstance(dest, bytes)\n    quiet = self.config['quiet'].get(bool)\n    if ((not quiet) and (not pretend)):\n        self._log.info('Encoding {0}', util.displayable_path(source))\n    args = shlex.split(command)\n    for (i, arg) in enumerate(args):\n        args[i] = Template(arg).safe_substitute({\n            b'source': source,\n            b'dest': dest,\n        })\n    if pretend:\n        self._log.info(' '.join(ui.decargs(args)))\n        return\n    try:\n        util.command_output(args)\n    except subprocess.CalledProcessError as exc:\n        self._log.info('Encoding {0} failed. Cleaning up...', util.displayable_path(source))\n        self._log.debug('Command {0} exited with status {1}', exc.cmd.decode('utf8', 'ignore'), exc.returncode)\n        util.remove(dest)\n        util.prune_dirs(os.path.dirname(dest))\n        raise\n    except OSError as exc:\n        raise ui.UserError(\"convert: could invoke '{0}': {1}\".format(' '.join(args), exc))\n    if ((not quiet) and (not pretend)):\n        self._log.info('Finished encoding {0}', util.displayable_path(source))\n", "label": 1}
{"function": "\n\ndef main():\n    server = sys.argv[1]\n    streams = None\n    while (not streams):\n        try:\n            streams = {row.name: row.position for row in replicate(server, {\n                'streams': '-1',\n            })['streams'].rows}\n        except requests.exceptions.ConnectionError as e:\n            time.sleep(0.1)\n    while True:\n        try:\n            results = replicate(server, streams)\n        except:\n            sys.stdout.write((('connection_lost(' + repr(streams)) + ')\\n'))\n            break\n        for update in results.values():\n            for row in update.rows:\n                sys.stdout.write((repr(row) + '\\n'))\n            streams[update.name] = update.position\n", "label": 1}
{"function": "\n\ndef test_actions_stepping_without_completion(self):\n    global rec, next_done\n    next_done = 0\n    node = CocosNode()\n    name1 = '1'\n    action1 = UAction(name1)\n    name2 = '2'\n    action2 = UAction(name2)\n    a1_copy = node.do(action1)\n    a2_copy = node.do(action2)\n    rec = []\n    dt = 0.1\n    node._step(dt)\n    recx = [e for e in rec if (e[0] == name1)]\n    assert (recx[0] == (name1, 'step', dt))\n    assert (len(recx) == 1)\n    recx = [e for e in rec if (e[0] == name2)]\n    assert (recx[0] == (name2, 'step', dt))\n    assert (len(recx) == 1)\n", "label": 1}
{"function": "\n\ndef load_dll_in_remote_process(target, dll_name):\n    rpeb = target.peb\n    if rpeb.Ldr:\n        modules = rpeb.modules\n        if any(((mod.name == dll_name) for mod in modules)):\n            dbgprint('DLL already present in target', 'DLLINJECT')\n            return True\n        k32 = [mod for mod in modules if (mod.name.lower() == 'kernel32.dll')]\n        if k32:\n            k32 = k32[0]\n            try:\n                load_libraryA = k32.pe.exports['LoadLibraryA']\n            except KeyError:\n                raise ValueError('Kernel32 have no export <LoadLibraryA> (wtf)')\n            with target.allocated_memory(4096) as addr:\n                target.write_memory(addr, (dll_name + '\\x00'))\n                t = target.create_thread(load_libraryA, addr)\n                t.wait()\n            dbgprint('DLL Injected via LoadLibray', 'DLLINJECT')\n            return True\n    if (target.bitness == 32):\n        return perform_manual_getproc_loadlib_32(target, dll_name)\n    return perform_manual_getproc_loadlib_64(target, dll_name)\n", "label": 1}
{"function": "\n\ndef test_play_with_one_card(self):\n    file = open('AllSets.enUS.json', 'r', encoding='UTF-8')\n    card_dict = json.load(file)\n    for card_set in ['Classic', 'Basic', 'Curse of Naxxramas', 'Goblins vs Gnomes', 'Blackrock Mountain']:\n        for card_info in card_dict[card_set]:\n            if (('collectible' in card_info) and card_info['collectible'] and (card_info['type'] != 'Hero')):\n                try:\n                    card = card_lookup(card_info['name'])\n                except KeyError:\n                    continue\n                game = generate_game_for(type(card), StonetuskBoar, PlayAndAttackAgent, DoNothingAgent)\n                try:\n                    while (not game.game_ended):\n                        game.play_single_turn()\n                except Exception as e:\n                    print(card)\n                    raise e\n        file.close()\n", "label": 1}
{"function": "\n\n@login_required()\ndef delete_assignment(request, course_id):\n    response_data = {\n        'status': 'failed',\n        'message': 'unknown error with deletion',\n    }\n    if request.is_ajax():\n        if (request.method == 'POST'):\n            assignment_id = int(request.POST['assignment_id'])\n            course = Course.objects.get(id=course_id)\n            student = Student.objects.get(user=request.user)\n            assignment = Assignment.objects.get(assignment_id=assignment_id)\n            try:\n                submission = AssignmentSubmission.objects.get(student=student, assignment=assignment)\n                submission.is_finished = False\n                submission.save()\n            except AssignmentSubmission.DoesNotExist:\n                return HttpResponse(json.dumps({\n                    'status': 'failed',\n                    'message': 'record does not exist',\n                }), content_type='application/json')\n            try:\n                e_submissions = EssaySubmission.objects.filter(question__assignment=assignment, student=student)\n                e_submissions.delete()\n            except EssayQuestion.DoesNotExist:\n                pass\n            try:\n                mc_submissions = MultipleChoiceSubmission.objects.filter(question__assignment=assignment, student=student)\n                mc_submissions.delete()\n            except MultipleChoiceSubmission.DoesNotExist:\n                pass\n            try:\n                tf_submissions = TrueFalseSubmission.objects.filter(question__assignment=assignment, student=student)\n                tf_submissions.delete()\n            except tf_submissions.DoesNotExist:\n                pass\n            try:\n                r_submissions = ResponseSubmission.objects.filter(question__assignment=assignment, student=student)\n                r_submissions.delete()\n            except ResponseQuestion.DoesNotExist:\n                pass\n            response_data = {\n                'status': 'success',\n                'message': 'assignment was deleted',\n            }\n    return HttpResponse(json.dumps(response_data), content_type='application/json')\n", "label": 1}
{"function": "\n\ndef newuser():\n    'Authenticate a new user for the first time.\\n\\n    Creates a file `.pingmeconfig` inside home folder.\\n\\n    Parameters\\n    ----------\\n    email : string\\n        Email address of the user (unverified)\\n    password : string\\n        Password of the user (Stored using hashlib)\\n    phone_numer = string\\n        Phone number including country code (India, by default)\\n    save_password = bool\\n        Prompt for password for each ping\\n\\n    '\n    EMAIL_REGEX = re.compile(\"^\\\\w+([-+.']\\\\w+)*@\\\\w+([-.]\\\\w+)*\\\\.\\\\w+([-.]\\\\w+)*$\")\n    sys.stdout.write('Email : ')\n    email = sys.stdin.readline()\n    while (not EMAIL_REGEX.match(email)):\n        sys.stderr.write('Wrong email address. Try again.\\n')\n        sys.stdout.write('Email : ')\n        email = sys.stdin.readline()\n    email = email.rstrip()\n    password = hashlib.md5(getpass.getpass().rstrip()).hexdigest()\n    while (password == 'd41d8cd98f00b204e9800998ecf8427e'):\n        sys.stderr.write('Invalid password. Try Again.\\n')\n        password = hashlib.md5(getpass.getpass().rstrip()).hexdigest()\n    repass = hashlib.md5(getpass.getpass('Re-enter : ').rstrip()).hexdigest()\n    while (password != repass):\n        sys.stderr.write('Password match failed. Try again.\\n')\n        password = hashlib.md5(getpass.getpass().rstrip()).hexdigest()\n        repass = hashlib.md5(getpass.getpass(('Re-enter : ' + '')).rstrip()).hexdigest()\n    while True:\n        try:\n            sys.stdout.write('Phone number : ')\n            read_number = sys.stdin.readline()\n            read_number = phonenumbers.parse(read_number, 'IN')\n            while (not phonenumbers.is_valid_number(read_number)):\n                sys.stderr.write('Phone number is invalid. Try again.\\n')\n                sys.stdout.write('Phone number : ')\n                read_number = sys.stdin.readline()\n                read_number = phonenumbers.parse(read_number, 'IN')\n            break\n        except Exception as e:\n            print(e)\n    number = str(read_number.national_number).rstrip()\n    country_code = str(read_number.country_code)\n    country_name = countrylist.code_to_country[('+' + country_code)]\n    save_password = 'YES'\n    sys.stdout.write('Prompt for password ? (y/N) : ')\n    opt = sys.stdin.readline()\n    if (opt.strip() == 'y'):\n        save_password = 'NO'\n    target = 'http://ping-me.himanshumishra.in/config/'\n    credentials = {\n        'email': email,\n        'password': password,\n        'phone': number,\n        'join_date': datetime.date.today(),\n        'os': sys.platform,\n        'country_code': country_code,\n        'country_name': country_name,\n        'phone_os': 'Unknown',\n    }\n    r = requests.post(target, data=credentials)\n    if (r.reason == 'OK'):\n        if (ast.literal_eval(r.text)['success'] == 'True'):\n            config_file = open((home + '/.pingmeconfig'), 'w+')\n            config_file.write((('[email]\\n\\t' + email) + '\\n'))\n            config_file.write((('[password]\\n\\t' + password) + '\\n'))\n            config_file.write((((((('[phone]\\n\\t' + country_code) + ' ') + number) + ' ') + country_name) + '\\n'))\n            config_file.write(((('[preference]\\n\\t' + 'SAVE_PASSWORD = ') + save_password) + '\\n'))\n            config_file.close()\n        else:\n            sys.stderr.write((('\\nERROR : ' + ast.literal_eval(r.text)['reason']) + '\\n'))\n    else:\n        sys.stderr.write('\\nERROR : Problem on the server. Contact sysadmin\\n')\n", "label": 1}
{"function": "\n\ndef test_uniform(self):\n    t = TDigest()\n    x = random.random(size=10000)\n    t.batch_update(x)\n    assert (abs((t.percentile(50) - 0.5)) < 0.02)\n    assert (abs((t.percentile(10) - 0.1)) < 0.01)\n    assert (abs((t.percentile(90) - 0.9)) < 0.01)\n    assert (abs((t.percentile(1) - 0.01)) < 0.005)\n    assert (abs((t.percentile(99) - 0.99)) < 0.005)\n    assert (abs((t.percentile(0.1) - 0.001)) < 0.001)\n    assert (abs((t.percentile(99.9) - 0.999)) < 0.001)\n", "label": 1}
{"function": "\n\ndef position2dict(pos):\n    'Convert CFFI position struct to a dict.'\n    assert (pos.unique_1 == pos.unique_2)\n    keys = ['usecs', 'frame_rate', 'frame']\n    if (pos.valid & _lib.JackPositionBBT):\n        keys += ['bar', 'beat', 'tick', 'bar_start_tick', 'beats_per_bar', 'beat_type', 'ticks_per_beat', 'beats_per_minute']\n    if (pos.valid & _lib.JackPositionTimecode):\n        keys += ['frame_time', 'next_time']\n    if (pos.valid & _lib.JackBBTFrameOffset):\n        keys += ['bbt_offset']\n    if (pos.valid & _lib.JackAudioVideoRatio):\n        keys += ['audio_frames_per_video_frame']\n    if (pos.valid & _lib.JackVideoFrameOffset):\n        keys += ['video_offset']\n    return dict(((k, getattr(pos, k)) for k in keys))\n", "label": 1}
{"function": "\n\ndef _fruchterman_reingold(A, dim=2, k=None, pos=None, fixed=None, iterations=50):\n    try:\n        import numpy as np\n    except ImportError:\n        raise ImportError('_fruchterman_reingold() requires numpy: http://scipy.org/ ')\n    try:\n        (nnodes, _) = A.shape\n    except AttributeError:\n        raise nx.NetworkXError('fruchterman_reingold() takes an adjacency matrix as input')\n    A = np.asarray(A)\n    if (pos == None):\n        pos = np.asarray(np.random.random((nnodes, dim)), dtype=A.dtype)\n    else:\n        pos = pos.astype(A.dtype)\n    if (k is None):\n        k = np.sqrt((1.0 / nnodes))\n    t = 0.1\n    dt = (t / float((iterations + 1)))\n    delta = np.zeros((pos.shape[0], pos.shape[0], pos.shape[1]), dtype=A.dtype)\n    for iteration in range(iterations):\n        for i in range(pos.shape[1]):\n            delta[:, :, i] = (pos[:, i, None] - pos[:, i])\n        distance = np.sqrt((delta ** 2).sum(axis=(- 1)))\n        distance = np.where((distance < 0.01), 0.01, distance)\n        displacement = np.transpose((np.transpose(delta) * (((k * k) / (distance ** 2)) - ((A * distance) / k)))).sum(axis=1)\n        length = np.sqrt((displacement ** 2).sum(axis=1))\n        length = np.where((length < 0.01), 0.1, length)\n        delta_pos = np.transpose(((np.transpose(displacement) * t) / length))\n        if (fixed is not None):\n            delta_pos[fixed] = 0.0\n        pos += delta_pos\n        t -= dt\n        pos = _rescale_layout(pos)\n    return pos\n", "label": 1}
{"function": "\n\ndef render_claims_to_pdf(request, slug, claim_group, deferred_awards):\n    'Currently hard-coded to print to Avery 22805 labels'\n    metrics = dict(page_width=(8.5 * inch), page_height=(11.0 * inch), top_margin=(0.5 * inch), left_margin=((25.0 / 32.0) * inch), qr_overlap=((1.0 / 32.0) * inch), padding=((1.0 / 16.0) * inch), horizontal_spacing=((5.0 / 16.0) * inch), vertical_spacing=((13.0 / 64.0) * inch), width=(1.5 * inch), height=(1.5 * inch))\n    debug = (request.GET.get('debug', False) is not False)\n    pagesize = (metrics['page_width'], metrics['page_height'])\n    cols = int(((metrics['page_width'] - metrics['left_margin']) / (metrics['width'] + metrics['horizontal_spacing'])))\n    rows = int(((metrics['page_height'] - metrics['top_margin']) / (metrics['height'] + metrics['vertical_spacing'])))\n    per_page = (cols * rows)\n    label_ct = len(deferred_awards)\n    page_ct = math.ceil((label_ct / per_page))\n    pages = [deferred_awards[x:(x + per_page)] for x in range(0, label_ct, per_page)]\n    response = HttpResponse(content_type='application/pdf; charset=utf-8')\n    if (not debug):\n        response['Content-Disposition'] = ('attachment; filename=\"%s-%s.pdf\"' % (slug.encode('utf-8', 'replace'), claim_group))\n    badge_img = None\n    fout = StringIO()\n    c = canvas.Canvas(fout, pagesize=pagesize)\n    for page in pages:\n        c.translate(metrics['left_margin'], (metrics['page_height'] - metrics['top_margin']))\n        for row in range(0, rows, 1):\n            c.translate(0.0, (0 - metrics['height']))\n            c.saveState()\n            for col in range(0, cols, 1):\n                try:\n                    da = page.pop(0)\n                except IndexError:\n                    continue\n                if (not badge_img):\n                    image_fin = da.badge.image.file\n                    image_fin.open()\n                    badge_img = ImageReader(StringIO(image_fin.read()))\n                c.saveState()\n                render_label(request, c, metrics, da, badge_img, debug)\n                c.restoreState()\n                dx = (metrics['width'] + metrics['horizontal_spacing'])\n                c.translate(dx, 0.0)\n            c.restoreState()\n            c.translate(0.0, (0 - metrics['vertical_spacing']))\n        c.showPage()\n    c.save()\n    response.write(fout.getvalue())\n    return response\n", "label": 1}
{"function": "\n\ndef submit_project_or_release(user, post_data, files):\n    'Registers/updates a project or release'\n    try:\n        project = Project.objects.get(name=post_data['name'])\n        if (project.owner != user):\n            return HttpResponseForbidden('That project is owned by someone else!')\n    except Project.DoesNotExist:\n        project = None\n    project_form = ProjectForm(post_data, instance=project)\n    if project_form.is_valid():\n        project = project_form.save(commit=False)\n        project.owner = user\n        project.save()\n        for c in post_data.getlist('classifiers'):\n            (classifier, created) = Classifier.objects.get_or_create(name=c)\n            project.classifiers.add(classifier)\n        if files:\n            allow_overwrite = getattr(settings, 'DJANGOPYPI_ALLOW_VERSION_OVERWRITE', False)\n            try:\n                release = Release.objects.get(version=post_data['version'], project=project, distribution=((UPLOAD_TO + '/') + files['distribution']._name))\n                if (not allow_overwrite):\n                    return HttpResponseForbidden((ALREADY_EXISTS_FMT % (release.filename, release)))\n            except Release.DoesNotExist:\n                release = None\n            release_form = ReleaseForm(post_data, files, instance=release)\n            if release_form.is_valid():\n                if (release and os.path.exists(release.distribution.path)):\n                    os.remove(release.distribution.path)\n                release = release_form.save(commit=False)\n                release.project = project\n                release.save()\n            else:\n                return HttpResponseBadRequest(('ERRORS: %s' % release_form.errors))\n    else:\n        return HttpResponseBadRequest(('ERRORS: %s' % project_form.errors))\n    return HttpResponse()\n", "label": 1}
{"function": "\n\ndef _authorize_user(self, username, key, req):\n    'Generates a new token and assigns it to a user.\\n\\n        username - string\\n        key - string API key\\n        req - wsgi.Request object\\n        '\n    ctxt = context.get_admin_context()\n    project_id = req.headers.get('X-Auth-Project-Id')\n    if (project_id is None):\n        user = self.auth.get_user_from_access_key(key)\n        projects = self.auth.get_projects(user.id)\n        if (not projects):\n            raise webob.exc.HTTPUnauthorized()\n        project_id = projects[0].id\n    try:\n        user = self.auth.get_user_from_access_key(key)\n    except exception.NotFound:\n        LOG.warn(_('User not found with provided API key.'))\n        user = None\n    if (user and (user.name == username)):\n        token_hash = hashlib.sha1(('%s%s%f' % (username, key, time.time()))).hexdigest()\n        token_dict = {\n            \n        }\n        token_dict['token_hash'] = token_hash\n        token_dict['cdn_management_url'] = ''\n        os_url = req.url\n        token_dict['server_management_url'] = os_url.strip('/')\n        version = common.get_version_from_href(os_url)\n        if (version == '1.1'):\n            token_dict['server_management_url'] += ('/' + project_id)\n        token_dict['storage_url'] = ''\n        token_dict['user_id'] = user.id\n        token = self.db.auth_token_create(ctxt, token_dict)\n        return (token, user)\n    elif (user and (user.name != username)):\n        msg = (_(\"Provided API key is valid, but not for user '%(username)s'\") % locals())\n        LOG.warn(msg)\n    return (None, None)\n", "label": 1}
{"function": "\n\ndef init(self):\n    with self.lock:\n        if self.inited:\n            return\n        files = os.listdir(self.dir_)\n        for fi in files:\n            if (fi == 'lock'):\n                continue\n            file_path = os.path.join(self.dir_, fi)\n            if ((not os.path.isfile(file_path)) or (LEGAL_STORE_FILE_REGEX.match(fi) is None)):\n                raise StoreNotSafetyShutdown('Store did not shutdown safety last time.')\n            else:\n                self.legal_files.append(file_path)\n        self.legal_files = sorted(self.legal_files, key=(lambda k: int(os.path.basename(k))))\n        if (len(self.legal_files) > 0):\n            read_file_handle = self.file_handles[READ_ENTRANCE] = open(self.legal_files[(- 1)], 'r+')\n            self.map_handles[READ_ENTRANCE] = mmap.mmap(read_file_handle.fileno(), self.store_file_size)\n            if (len(self.legal_files) == 1):\n                self.file_handles[WRITE_ENTRANCE] = self.file_handles[READ_ENTRANCE]\n                self.map_handles[WRITE_ENTRANCE] = self.map_handles[READ_ENTRANCE]\n            else:\n                write_file_handle = self.file_handles[WRITE_ENTRANCE] = open(self.legal_files[0], 'r+')\n                self.map_handles[WRITE_ENTRANCE] = mmap.mmap(write_file_handle.fileno(), self.store_file_size)\n        self.inited = True\n", "label": 1}
{"function": "\n\ndef send_ready(self, conn, count):\n    if (self.state == CLOSED):\n        self.logger.debug(('[%s] cannot send RDY (in state CLOSED)' % conn))\n        return\n    if (self.state == BACKOFF):\n        self.logger.debug(('[%s] cannot send RDY (in state BACKOFF)' % conn))\n        return\n    if ((self.state == THROTTLED) and self.total_in_flight_or_ready):\n        msg = '[%s] cannot send RDY (THROTTLED and %d in flight or ready)'\n        self.logger.debug((msg % (conn, self.total_in_flight_or_ready)))\n        return\n    if (not conn.is_connected):\n        self.logger.debug(('[%s] cannot send RDY (connection closed)' % conn))\n        return\n    total = ((self.total_ready_count - conn.ready_count) + count)\n    if (total > self.max_in_flight):\n        if (not (conn.ready_count or conn.in_flight)):\n            self.logger.debug(('[%s] sending later' % conn))\n            gevent.spawn_later(5, self.send_ready, conn, count)\n        return\n    self.logger.debug(('[%s] sending RDY %d' % (conn, count)))\n    try:\n        conn.ready(count)\n    except NSQSocketError as error:\n        self.logger.warn(('[%s] RDY %d failed (%r)' % (conn, count, error)))\n", "label": 1}
{"function": "\n\ndef applicationDataReceived(self, bytes):\n    '\\n        '\n    if (not self.interacting):\n        self.transport.write(bytes)\n        if (bytes in ('\\r', '\\n')):\n            self.transport.write('\\n')\n            pieces = self.cmdbuf.split(' ', 1)\n            self.cmdbuf = ''\n            (cmd, args) = (pieces[0], '')\n            if (len(pieces) > 1):\n                args = pieces[1]\n            try:\n                func = getattr(self, ('cmd_' + cmd))\n            except AttributeError:\n                self.transport.write('** Unknown command.\\r\\n')\n                return\n            func(args)\n        else:\n            self.cmdbuf += bytes\n    else:\n        for c in bytes:\n            if (ord(c) == 27):\n                self.interacting.terminal.delInteractor(self)\n                self.interacting = None\n                self.transport.write('\\r\\n** Interactive session closed.\\r\\n')\n                return\n        if (not self.readonly):\n            if (type(bytes) == type('')):\n                ttylog.ttylog_write(self.interacting.terminal.ttylog_file, len(bytes), ttylog.TYPE_INTERACT, time.time(), bytes)\n            for c in bytes:\n                recvline.HistoricRecvLine.keystrokeReceived(self.interacting, c, None)\n", "label": 1}
{"function": "\n\ndef _evaluate_condition(self, conditions, trait, at_init=False):\n    \" Evaluates a list of (eval, editor) pairs and sets a specified trait\\n        on each editor to reflect the Boolean value of the expression.\\n\\n        1) All conditions are evaluated\\n        2) The elements whose condition evaluates to False are updated\\n        3) The elements whose condition evaluates to True are updated\\n\\n        E.g., we first make invisible all elements for which 'visible_when'\\n        evaluates to False, and then we make visible the ones\\n        for which 'visible_when' is True. This avoids mutually exclusive\\n        elements to be visible at the same time, and thus making a dialog\\n        unnecessarily large.\\n\\n        The state of an editor is updated only when it changes, unless\\n        at_init is set to True.\\n\\n        Parameters\\n        ----------\\n        conditions : list of (str, Editor) tuple\\n            A list of tuples, each formed by 1) a string that contains a\\n            condition that evaluates to either True or False, and\\n            2) the editor whose state depends on the condition\\n\\n        trait : str\\n            The trait that is set by the condition.\\n            Either 'visible, 'enabled', or 'checked'.\\n\\n        at_init : bool\\n            If False, the state of an editor is set only when it changes\\n            (e.g., a visible element would not be updated to visible=True\\n            again). If True, the state is always updated (used at\\n            initialization).\\n        \"\n    context = self._get_context(self.context)\n    activate = []\n    deactivate = []\n    for (when, editor) in conditions:\n        try:\n            cond_value = eval(when, globals(), context)\n            editor_state = getattr(editor, trait)\n            if (cond_value and (at_init or (not editor_state))):\n                activate.append(editor)\n            if ((not cond_value) and (at_init or editor_state)):\n                deactivate.append(editor)\n        except Exception:\n            from traitsui.api import raise_to_debug\n            raise_to_debug()\n    for editor in deactivate:\n        setattr(editor, trait, False)\n    for editor in activate:\n        setattr(editor, trait, True)\n", "label": 1}
{"function": "\n\n@access.public\n@describeRoute(Description('Search for items that are entirely within either a GeoJSON polygon or a circular region.').param('field', 'Name of field containing GeoJSON on which to search.', required=True).param('geometry', 'Search query condition as a GeoJSON polygon.', required=False).param('center', 'Center of search radius as a GeoJSON point.', required=False).param('radius', 'Search radius in meters.', required=False, dataType='number').param('limit', 'Result set size limit (default=50).', required=False, dataType='integer').param('offset', 'Offset into result set (default=0).', required=False, dataType='integer').errorResponse().notes(\"Either parameter 'geometry' or both parameters 'center'  and 'radius' are required.\"))\ndef within(self, params):\n    \"\\n        Search for items that are entirely within either a GeoJSON polygon or a\\n        circular region. Either parameter 'geometry' or both parameters 'center'\\n        and 'radius' are required.\\n\\n        :param params: parameters to the API call, including 'field' and either\\n                       'geometry' or both 'center' and 'radius'.\\n        :type params: dict[str, unknown]\\n        :returns: filtered fields of the matching items with geospatial data\\n                 appended to the 'geo' field of each item.\\n        :rtype: list[dict[str, unknown]]\\n        :raise RestException: on malformed API call.\\n        \"\n    self.requireParams(('field',), params)\n    if ('geometry' in params):\n        try:\n            geometry = bson.json_util.loads(params['geometry'])\n            GeoJSON.to_instance(geometry, strict=True)\n            if (geometry['type'] != 'Polygon'):\n                raise ValueError\n        except (TypeError, ValueError):\n            raise RestException(\"Invalid GeoJSON passed as 'geometry' parameter.\")\n        condition = {\n            '$geometry': geometry,\n        }\n    elif (('center' in params) and ('radius' in params)):\n        try:\n            radius = (float(params['radius']) / self._RADIUS_OF_EARTH)\n            if (radius < 0.0):\n                raise ValueError\n        except ValueError:\n            raise RestException(\"Parameter 'radius' must be a number.\")\n        try:\n            center = bson.json_util.loads(params['center'])\n            GeoJSON.to_instance(center, strict=True)\n            if (center['type'] != 'Point'):\n                raise ValueError\n        except (TypeError, ValueError):\n            raise RestException(\"Invalid GeoJSON passed as 'center' parameter.\")\n        condition = {\n            '$centerSphere': [center['coordinates'], radius],\n        }\n    else:\n        raise RestException(\"Either parameter 'geometry' or both parameters 'center' and 'radius' are required.\")\n    if (params['field'][:3] == ('%s.' % GEOSPATIAL_FIELD)):\n        field = params['field'].strip()\n    else:\n        field = ('%s.%s' % (GEOSPATIAL_FIELD, params['field'].strip()))\n    (limit, offset, sort) = self.getPagingParameters(params, 'lowerName')\n    query = {\n        field: {\n            '$geoWithin': condition,\n        },\n    }\n    return self._find(query, limit, offset, sort)\n", "label": 1}
{"function": "\n\ndef _load_from_csv(self, reader, entity_type, source):\n    j = 0\n    for (i, line) in enumerate(reader):\n        (postcode_abbrev, (easting, northing)) = (line[0], line[10:12])\n        postcode_abbrev = postcode_abbrev.replace(' ', '')\n        if re.match('[A-Z][0-9]{2}[A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:2], postcode_abbrev[2:]))\n        elif re.match('[A-Z][0-9]{3}[A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:3], postcode_abbrev[3:]))\n        elif re.match('[A-Z]{2}[0-9]{2}[A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:3], postcode_abbrev[3:]))\n        elif re.match('[A-Z]{2}[0-9]{3}[A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:4], postcode_abbrev[4:]))\n        elif re.match('[A-Z][0-9][A-Z][0-9][A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:3], postcode_abbrev[3:]))\n        elif re.match('[A-Z]{2}[0-9][A-Z][0-9][A-Z]{2}', postcode_abbrev):\n            postcode = ('%s %s' % (postcode_abbrev[:4], postcode_abbrev[4:]))\n        else:\n            postcode = postcode_abbrev\n        try:\n            (easting, northing) = (int(easting), int(northing))\n        except ValueError:\n            continue\n        j += 1\n        try:\n            entity = Entity.objects.get(source=source, _identifiers__scheme='postcode', _identifiers__value=postcode_abbrev)\n        except Entity.DoesNotExist:\n            entity = Entity(source=source)\n        entity.location = Point(easting, northing, srid=27700)\n        entity.geometry = entity.location\n        entity.primary_type = entity_type\n        identifiers = {\n            'postcode': postcode_abbrev,\n            'postcode-canonical': postcode,\n        }\n        entity.save(identifiers=identifiers)\n        set_name_in_language(entity, 'en', title=postcode)\n        entity.all_types.add(entity_type)\n        entity.update_all_types_completion()\n", "label": 1}
{"function": "\n\ndef human_play(w, i, grid):\n    'Just ask for a move.'\n    plaint = ''\n    prompt = (whose_move(grid) + ' move? [1-9] ')\n    while True:\n        w.render_to_terminal(w.array_from_text((((view(grid) + '\\n\\n') + plaint) + prompt)))\n        key = c = i.next()\n        try:\n            move = int(key)\n        except ValueError:\n            pass\n        else:\n            if (1 <= move <= 9):\n                successor = apply_move(grid, from_human_move(move))\n                if successor:\n                    return successor\n        plaint = (\"Hey, that's illegal. Give me one of these digits:\\n\\n\" + ((grid_format % tuple(((move if apply_move(grid, from_human_move(move)) else '-') for move in range(1, 10)))) + '\\n\\n'))\n", "label": 1}
{"function": "\n\ndef parse_cluster_id(cluster_id):\n    try:\n        if isinstance(cluster_id, str):\n            pass\n        elif isinstance(cluster_id, int):\n            cluster_id = str(cluster_id)\n        elif isinstance(cluster_id, bytes):\n            cluster_id = cluster_id.decode('utf-8')\n        elif isinstance(cluster_id, OrientRecordLink):\n            cluster_id = cluster_id.get()\n        elif ((sys.version_info[0] < 3) and isinstance(cluster_id, unicode)):\n            cluster_id = cluster_id.encode('utf-8')\n        (_cluster_id, _position) = cluster_id.split(':')\n        if (_cluster_id[0] is '#'):\n            _cluster_id = _cluster_id[1:]\n    except (AttributeError, ValueError):\n        _cluster_id = cluster_id\n    return _cluster_id\n", "label": 1}
{"function": "\n\ndef decode(self, string):\n    (lat_min, latitude, lat_max) = ((- 90), 0, 90)\n    (long_min, longitude, long_max) = ((- 180), 0, 180)\n    odd_bit = False\n    for char in string:\n        try:\n            byte = self.DECODE_MAP[char]\n        except KeyError:\n            raise ValueError(('Invalid hash: unexpected character %r.' % (c,)))\n        else:\n            for bit in (16, 8, 4, 2, 1):\n                if odd_bit:\n                    if (byte & bit):\n                        lat_min = latitude\n                    else:\n                        lat_max = latitude\n                    latitude = ((lat_min + lat_max) / 2.0)\n                else:\n                    if (byte & bit):\n                        long_min = longitude\n                    else:\n                        long_max = longitude\n                    longitude = ((long_min + long_max) / 2.0)\n                odd_bit = (not odd_bit)\n    point = self.point_class((latitude, longitude))\n    point.error = ((lat_max - latitude), (long_max - longitude))\n    return point\n", "label": 1}
{"function": "\n\ndef http_statuscode_role(name, rawtext, text, lineno, inliner, options={\n    \n}, content=[]):\n    if text.isdigit():\n        code = int(text)\n        try:\n            status = HTTP_STATUS_CODES[code]\n        except KeyError:\n            msg = inliner.reporter.error(('%d is invalid HTTP status code' % code), lineno=lineno)\n            prb = inliner.problematic(rawtext, rawtext, msg)\n            return ([prb], [msg])\n    else:\n        try:\n            (code, status) = re.split('\\\\s', text.strip(), 1)\n            code = int(code)\n        except ValueError:\n            msg = inliner.reporter.error(('HTTP status code must be an integer (e.g. `200`) or start with an integer (e.g. `200 OK`); %r is invalid' % text), line=lineno)\n            prb = inliner.problematic(rawtext, rawtext, msg)\n            return ([prb], [msg])\n    nodes.reference(rawtext)\n    if (code == 226):\n        url = 'http://www.ietf.org/rfc/rfc3229.txt'\n    if (code == 418):\n        url = 'http://www.ietf.org/rfc/rfc2324.txt'\n    if (code == 449):\n        url = 'http://msdn.microsoft.com/en-us/library/dd891478(v=prot.10).aspx'\n    elif (code in HTTP_STATUS_CODES):\n        url = ('http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.' + ('%d.%d' % ((code // 100), (1 + (code % 100)))))\n    else:\n        url = ''\n    set_classes(options)\n    node = nodes.reference(rawtext, ('%d %s' % (code, status)), refuri=url, **options)\n    return ([node], [])\n", "label": 1}
{"function": "\n\n@classmethod\ndef _iter_packed_refs(cls, repo):\n    'Returns an iterator yielding pairs of sha1/path pairs (as bytes) for the corresponding refs.\\n        :note: The packed refs file will be kept open as long as we iterate'\n    try:\n        with open(cls._get_packed_refs_path(repo), 'rt') as fp:\n            for line in fp:\n                line = line.strip()\n                if (not line):\n                    continue\n                if line.startswith('#'):\n                    if (line.startswith('# pack-refs with:') and (not line.endswith('peeled'))):\n                        raise TypeError(('PackingType of packed-Refs not understood: %r' % line))\n                    continue\n                if (line[0] == '^'):\n                    continue\n                (yield tuple(line.split(' ', 1)))\n    except (OSError, IOError):\n        return\n", "label": 1}
{"function": "\n\ndef create_zabbix_data(self, iaas_instance, openstack_instance, zabbix_settings, options):\n    self.stdout.write((' Migrating zabbix data for instance %s' % iaas_instance))\n    from nodeconductor_zabbix.models import Template, Host, ITService, Trigger, ZabbixServiceProjectLink\n    backend = zabbix_settings.get_backend()\n    api = backend.api\n    old_client = self.get_old_zabbix_client()\n    try:\n        host_id = old_client.get_host(iaas_instance)['hostid']\n    except IndexError:\n        self.error(('  Zabbix host does not exist for instance %s (UUID: %s)' % (iaas_instance.name, iaas_instance.uuid)))\n        return\n    host_data = api.host.get(filter={\n        'hostid': host_id,\n    }, selectParentTemplates='', output='extend', selectGroups='extend')[0]\n    host_interface_data = api.hostinterface.get(filter={\n        'hostid': host_id,\n    }, output='extend')[0]\n    del host_interface_data['hostid']\n    del host_interface_data['interfaceid']\n    template_ids = [el['templateid'] for el in host_data['parentTemplates']]\n    try:\n        templates = [Template.objects.get(backend_id=template_id) for template_id in template_ids]\n    except Template.DoesNotExist:\n        self.error(('  NC Zabbix database does not have template with backend_id %s. Please pull it.' % template_ids))\n        return\n    try:\n        host_group_name = host_data['groups'][0]['name']\n    except (IndexError, KeyError):\n        host_group_name = ''\n    spl = ZabbixServiceProjectLink.objects.get(service__settings=zabbix_settings, project=openstack_instance.project)\n    self.stdout.write(('  [+] Host for instance %s.' % openstack_instance.name))\n    host = Host.objects.create(service_project_link=spl, scope=openstack_instance, visible_name=host_data['name'], name=host_data['host'], backend_id=host_id, state=Host.States.ONLINE, host_group_name=host_group_name, interface_parameters=host_interface_data)\n    host.templates.add(*templates)\n    service_name = old_client.get_service_name(iaas_instance)\n    try:\n        service_data = api.service.get(filter={\n            'name': service_name,\n        }, output='extend')[0]\n    except IndexError:\n        self.error(('  IT service for instance %s (UUID: %s) does not exist.' % (iaas_instance.name, iaas_instance.uuid)))\n        return\n    trigger_data = api.trigger.get(filter={\n        'triggerid': service_data['triggerid'],\n    }, output='extend')[0]\n    try:\n        trigger = Trigger.objects.get(name=trigger_data['description'], backend_id=trigger_data['templateid'])\n    except Trigger.DoesNotExist:\n        self.error(('  Trigger with name \"%s\" that belong to template with backend_id %s does not exist in NC database. Please pull it.' % (trigger_data['description'], trigger_data['templateid'])))\n    self.stdout.write(('  [+] IT Service for instance %s.' % openstack_instance.name))\n    ITService.objects.create(service_project_link=spl, host=host, is_main=True, algorithm=int(service_data['algorithm']), sort_order=int(service_data['sortorder']), agreed_sla=float(service_data['goodsla']), backend_trigger_id=service_data['triggerid'], state=ITService.States.ONLINE, backend_id=service_data['serviceid'], name=service_data['name'], trigger=trigger)\n    if (iaas_instance.type == iaas_instance.Services.PAAS):\n        self.stdout.write(('  [+] Installation state as monitoring item for %s.' % openstack_instance))\n        mapping = {\n            'NO DATA': 0,\n            'OK': 1,\n            'NOT OK': 0,\n        }\n        openstack_instance.monitoring_items.create(name=Host.MONITORING_ITEMS_CONFIGS[0]['monitoring_item_name'], value=mapping[iaas_instance.installation_state])\n", "label": 1}
{"function": "\n\ndef _receive_callback(self, message):\n    accept = self.accept\n    (on_m, channel, decoded) = (self.on_message, self.channel, None)\n    try:\n        m2p = getattr(channel, 'message_to_python', None)\n        if m2p:\n            message = m2p(message)\n        if (accept is not None):\n            message.accept = accept\n        if message.errors:\n            return message._reraise_error(self.on_decode_error)\n        decoded = (None if on_m else message.decode())\n    except Exception as exc:\n        if (not self.on_decode_error):\n            raise\n        self.on_decode_error(message, exc)\n    else:\n        return (on_m(message) if on_m else self.receive(decoded, message))\n", "label": 1}
{"function": "\n\ndef _close_database(self):\n    try:\n        funs = [conn.close for conn in db.connections]\n    except AttributeError:\n        if hasattr(db, 'close_old_connections'):\n            funs = [db.close_old_connections]\n        else:\n            funs = [db.close_connection]\n    for close in funs:\n        try:\n            close()\n        except DATABASE_ERRORS as exc:\n            str_exc = str(exc)\n            if (('closed' not in str_exc) and ('not connected' not in str_exc)):\n                raise\n", "label": 1}
{"function": "\n\ndef _get_weights(dist, weights):\n    \"Get the weights from an array of distances and a parameter ``weights``\\n\\n    Parameters\\n    ===========\\n    dist: ndarray\\n        The input distances\\n    weights: {'uniform', 'distance' or a callable}\\n        The kind of weighting used\\n\\n    Returns\\n    ========\\n    weights_arr: array of the same shape as ``dist``\\n        if ``weights == 'uniform'``, then returns None\\n    \"\n    if (weights in (None, 'uniform')):\n        return None\n    elif (weights == 'distance'):\n        if (dist.dtype is np.dtype(object)):\n            for (point_dist_i, point_dist) in enumerate(dist):\n                if (hasattr(point_dist, '__contains__') and (0.0 in point_dist)):\n                    dist[point_dist_i] = (point_dist == 0.0)\n                else:\n                    dist[point_dist_i] = (1.0 / point_dist)\n        else:\n            with np.errstate(divide='ignore'):\n                dist = (1.0 / dist)\n            inf_mask = np.isinf(dist)\n            inf_row = np.any(inf_mask, axis=1)\n            dist[inf_row] = inf_mask[inf_row]\n        return dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized: should be 'uniform', 'distance', or a callable function\")\n", "label": 1}
{"function": "\n\ndef decode(self, encoded_packet):\n    'Decode a transmitted package.'\n    b64 = False\n    if (not isinstance(encoded_packet, six.binary_type)):\n        encoded_packet = encoded_packet.encode('utf-8')\n    self.packet_type = six.byte2int(encoded_packet[0:1])\n    if (self.packet_type == 98):\n        self.binary = True\n        encoded_packet = encoded_packet[1:]\n        self.packet_type = six.byte2int(encoded_packet[0:1])\n        self.packet_type -= 48\n        b64 = True\n    elif (self.packet_type >= 48):\n        self.packet_type -= 48\n        self.binary = False\n    else:\n        self.binary = True\n    self.data = None\n    if (len(encoded_packet) > 1):\n        if self.binary:\n            if b64:\n                self.data = base64.b64decode(encoded_packet[1:])\n            else:\n                self.data = encoded_packet[1:]\n        else:\n            try:\n                self.data = self.json.loads(encoded_packet[1:].decode('utf-8'))\n            except ValueError:\n                self.data = encoded_packet[1:].decode('utf-8')\n", "label": 1}
{"function": "\n\ndef repeat_weekdays(self):\n    '\\n        Like self.repeat(), but used to repeat every weekday.\\n        '\n    try:\n        d = date(self.year, self.month, self.day)\n    except ValueError:\n        return self.count\n    if (self.count_first and (d <= self.end_repeat) and (d.weekday() not in (5, 6))):\n        self.count_it(d.day)\n    d += timedelta(days=1)\n    while ((d.month == self.month) and (d <= self.end_repeat)):\n        if (d.weekday() not in (5, 6)):\n            self.count_it(d.day)\n        d += timedelta(days=1)\n", "label": 1}
{"function": "\n\ndef domain_delete(domain, logger, filesystem):\n    'libvirt domain undefinition.\\n\\n    @raise: libvirt.libvirtError.\\n\\n    '\n    if (domain is not None):\n        try:\n            if domain.isActive():\n                domain.destroy()\n        except libvirt.libvirtError:\n            logger.exception('Unable to destroy the domain.')\n        try:\n            domain.undefine()\n        except libvirt.libvirtError:\n            logger.exception('Unable to undefine the domain.')\n        try:\n            if ((filesystem is not None) and os.path.exists(filesystem)):\n                shutil.rmtree(filesystem)\n        except Exception:\n            logger.exception('Unable to remove the shared folder.')\n", "label": 1}
{"function": "\n\ndef add_srs_entry(srs, auth_name='EPSG', auth_srid=None, ref_sys_name=None, database=None):\n    '\\n    This function takes a GDAL SpatialReference system and adds its information\\n    to the `spatial_ref_sys` table of the spatial backend.  Doing this enables\\n    database-level spatial transformations for the backend.  Thus, this utility\\n    is useful for adding spatial reference systems not included by default with\\n    the backend -- for example, the so-called \"Google Maps Mercator Projection\"\\n    is excluded in PostGIS 1.3 and below, and the following adds it to the\\n    `spatial_ref_sys` table:\\n\\n    >>> from django.contrib.gis.utils import add_srs_entry\\n    >>> add_srs_entry(900913)\\n\\n    Keyword Arguments:\\n     auth_name:\\n       This keyword may be customized with the value of the `auth_name` field.\\n       Defaults to \\'EPSG\\'.\\n\\n     auth_srid:\\n       This keyword may be customized with the value of the `auth_srid` field.\\n       Defaults to the SRID determined by GDAL.\\n\\n     ref_sys_name:\\n       For SpatiaLite users only, sets the value of the `ref_sys_name` field.\\n       Defaults to the name determined by GDAL.\\n\\n     database:\\n      The name of the database connection to use; the default is the value\\n      of `django.db.DEFAULT_DB_ALIAS` (at the time of this writing, it\\'s value\\n      is \\'default\\').\\n    '\n    from django.db import connections, DEFAULT_DB_ALIAS\n    if (not database):\n        database = DEFAULT_DB_ALIAS\n    connection = connections[database]\n    if (not hasattr(connection.ops, 'spatial_version')):\n        raise Exception('The `add_srs_entry` utility only works with spatial backends.')\n    if (connection.ops.oracle or connection.ops.mysql):\n        raise Exception('This utility does not support the Oracle or MySQL spatial backends.')\n    SpatialRefSys = connection.ops.spatial_ref_sys()\n    if (not isinstance(srs, SpatialReference)):\n        srs = SpatialReference(srs)\n    if (srs.srid is None):\n        raise Exception('Spatial reference requires an SRID to be compatible with the spatial backend.')\n    kwargs = {\n        'srid': srs.srid,\n        'auth_name': auth_name,\n        'auth_srid': (auth_srid or srs.srid),\n        'proj4text': srs.proj4,\n    }\n    if connection.ops.postgis:\n        kwargs['srtext'] = srs.wkt\n    if connection.ops.spatialite:\n        kwargs['ref_sys_name'] = (ref_sys_name or srs.name)\n    try:\n        sr = SpatialRefSys.objects.using(database).get(srid=srs.srid)\n    except SpatialRefSys.DoesNotExist:\n        sr = SpatialRefSys.objects.using(database).create(**kwargs)\n", "label": 1}
{"function": "\n\n@mock.patch.object(random, 'uniform')\ndef test_reroll(self, uniform):\n    'Even without a cookie, calling flag_is_active twice should return\\n        the same value.'\n    Flag.objects.create(name='myflag', percent='50.0')\n    request = get()\n    assert (not hasattr(request, 'waffles'))\n    uniform.return_value = '10'\n    assert waffle.flag_is_active(request, 'myflag')\n    assert hasattr(request, 'waffles')\n    assert ('myflag' in request.waffles)\n    assert request.waffles['myflag'][0]\n    uniform.return_value = '70'\n    assert waffle.flag_is_active(request, 'myflag')\n    assert request.waffles['myflag'][0]\n", "label": 1}
{"function": "\n\ndef interactive_running(args):\n    \"\\n    This function provides an interactive environment for running the system.\\n    It receives text from the standard input, tokenizes it, and calls the function\\n    given as a parameter to produce an answer.\\n    \\n    :param task: 'pos', 'srl' or 'dependency'\\n    :param use_tokenizer: whether to use built-in tokenizer\\n    \"\n    use_tokenizer = (not args.disable_tokenizer)\n    task_lower = args.task.lower()\n    if (task_lower == 'pos'):\n        tagger = nlpnet.taggers.POSTagger(language=args.lang)\n    elif (task_lower == 'srl'):\n        tagger = nlpnet.taggers.SRLTagger(language=args.lang)\n    elif (task_lower == 'dependency'):\n        tagger = nlpnet.taggers.DependencyParser(language=args.lang)\n    else:\n        raise ValueError(('Unknown task: %s' % args.task))\n    while True:\n        try:\n            text = raw_input()\n        except KeyboardInterrupt:\n            break\n        except EOFError:\n            break\n        if (type(text) is not unicode):\n            text = unicode(text, 'utf-8')\n        if use_tokenizer:\n            result = tagger.tag(text)\n        else:\n            tokens = text.split()\n            if (task_lower != 'dependency'):\n                result = [tagger.tag_tokens(tokens, True)]\n            else:\n                result = [tagger.tag_tokens(tokens)]\n        _print_tagged(result, task_lower)\n", "label": 1}
{"function": "\n\n@app.route('/v1/services/<id>', methods=['PUT'])\n@authnz.require_auth\n@authnz.require_csrf_token\ndef map_service_credentials(id):\n    data = request.get_json()\n    try:\n        _service = Service.get(id)\n        if (_service.data_type != 'service'):\n            msg = 'id provided is not a service.'\n            return (jsonify({\n                'error': msg,\n            }), 400)\n        revision = (_service.revision + 1)\n        _service_credential_ids = _service.credentials\n    except Service.DoesNotExist:\n        revision = 1\n        _service_credential_ids = []\n    if data.get('credentials'):\n        conflicts = _pair_key_conflicts_for_credentials(copy.deepcopy(data['credentials']))\n        if conflicts:\n            ret = {\n                'error': 'Conflicting key pairs in mapped service.',\n                'conflicts': conflicts,\n            }\n            return (jsonify(ret), 400)\n    if (revision == 1):\n        try:\n            keymanager.ensure_grants(id)\n        except keymanager.ServiceCreateGrantError:\n            msg = 'Failed to add grants for {0}.'.format(id)\n            logging.error(msg)\n    try:\n        Service(id='{0}-{1}'.format(id, revision), data_type='archive-service', credentials=data.get('credentials'), enabled=data.get('enabled'), revision=revision, modified_by=authnz.get_logged_in_user_email()).save(id__null=True)\n    except PutError as e:\n        logging.error(e)\n        return (jsonify({\n            'error': 'Failed to add service to archive.',\n        }), 500)\n    try:\n        service = Service(id=id, data_type='service', credentials=data['credentials'], enabled=data.get('enabled'), revision=revision, modified_by=authnz.get_logged_in_user_email())\n        service.save()\n    except PutError as e:\n        logging.error(e)\n        return (jsonify({\n            'error': 'Failed to update active service.',\n        }), 500)\n    added = list((set(service.credentials) - set(_service_credential_ids)))\n    removed = list((set(_service_credential_ids) - set(service.credentials)))\n    msg = 'Added credentials: {0}; Removed credentials {1}; Revision {2}'\n    msg = msg.format(added, removed, service.revision)\n    graphite.send_event([id], msg)\n    try:\n        credentials = _get_credentials(service.credentials)\n    except KeyError:\n        return (jsonify({\n            'error': 'Decryption error.',\n        }), 500)\n    return jsonify({\n        'id': service.id,\n        'credentials': credentials,\n        'revision': service.revision,\n        'enabled': service.enabled,\n        'modified_date': service.modified_date,\n        'modified_by': service.modified_by,\n    })\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse_change_desc(changedesc, changenum, allow_empty=False):\n    if (not changedesc):\n        return None\n    changeset = ChangeSet()\n    try:\n        changeset.changenum = int(changedesc['change'])\n    except ValueError:\n        changeset.changenum = changenum\n    changeset.username = changedesc['user']\n    try:\n        changeset.description = changedesc['desc'].decode('utf-8')\n    except UnicodeDecodeError:\n        changeset.description = changedesc['desc'].decode('utf-8', 'replace')\n    if (changedesc['status'] == 'pending'):\n        changeset.pending = True\n    try:\n        changeset.files = changedesc['depotFile']\n    except KeyError:\n        if (not allow_empty):\n            raise EmptyChangeSetError(changenum)\n    split = changeset.description.find('\\n\\n')\n    if ((split >= 0) and (split < 100)):\n        changeset.summary = changeset.description.split('\\n\\n', 1)[0].replace('\\n', ' ')\n    else:\n        changeset.summary = changeset.description.split('\\n', 1)[0]\n    return changeset\n", "label": 1}
{"function": "\n\ndef get_node(self, path):\n    if isinstance(path, unicode):\n        path = path.encode('utf-8')\n    path = self._fix_path(path)\n    if (not (path in self.nodes)):\n        try:\n            id_ = self._get_id_for_path(path)\n        except ChangesetError:\n            raise NodeDoesNotExistError((\"Cannot find one of parents' directories for a given path: %s\" % path))\n        _GL = (lambda m: (m and objects.S_ISGITLINK(m)))\n        if _GL(self._stat_modes.get(path)):\n            node = SubModuleNode(path, url=None, changeset=id_, alias=self.repository.alias)\n        else:\n            obj = self.repository._repo.get_object(id_)\n            if isinstance(obj, objects.Tree):\n                if (path == ''):\n                    node = RootNode(changeset=self)\n                else:\n                    node = DirNode(path, changeset=self)\n                node._tree = obj\n            elif isinstance(obj, objects.Blob):\n                node = FileNode(path, changeset=self)\n                node._blob = obj\n            else:\n                raise NodeDoesNotExistError((\"There is no file nor directory at the given path '%s' at revision %s\" % (path, self.short_id)))\n        self.nodes[path] = node\n    return self.nodes[path]\n", "label": 1}
{"function": "\n\ndef make_axis(self, ax, location, props):\n    'Given a mpl axes instance, returns a Bokeh LinearAxis object.'\n    tf = props['tickformat']\n    tv = props['tickvalues']\n    if (tf and any((isinstance(x, string_types) for x in tf))):\n        laxis = CategoricalAxis(axis_label=ax.get_label_text())\n        assert (np.min(tv) >= 0), 'Assuming categorical axis have positive-integer dump tick values'\n        offset = (np.min(tv) - 1)\n        rng = FactorRange(factors=[str(x) for x in tf], offset=offset)\n        if (location in ['above', 'below']):\n            self.plot.x_range = rng\n        else:\n            self.plot.y_range = rng\n    elif (props['scale'] == 'linear'):\n        laxis = LinearAxis(axis_label=ax.get_label_text())\n    elif (props['scale'] == 'date'):\n        laxis = DatetimeAxis(axis_label=ax.get_label_text())\n    self.plot.add_layout(laxis, location)\n    label = ax.get_label()\n    self.text_props(label, laxis, prefix='axis_label_')\n    if (props['nticks'] == 0):\n        laxis.major_tick_line_color = None\n        laxis.minor_tick_line_color = None\n        laxis.major_label_text_color = None\n    ticklabels = ax.get_ticklabels()\n    if ticklabels:\n        self.text_props(ticklabels[0], laxis, prefix='major_label_')\n    if self.xkcd:\n        laxis.axis_line_width = 3\n        laxis.axis_label_text_font = 'Comic Sans MS, Textile, cursive'\n        laxis.axis_label_text_font_style = 'bold'\n        laxis.axis_label_text_color = 'black'\n        laxis.major_label_text_font = 'Comic Sans MS, Textile, cursive'\n        laxis.major_label_text_font_style = 'bold'\n        laxis.major_label_text_color = 'black'\n    return laxis\n", "label": 1}
{"function": "\n\ndef _real_extract(self, url):\n    mobj = re.match(self._VALID_URL, url)\n    video_id = mobj.group('id')\n    if (video_id is not None):\n        all_info = self._download_xml(('http://www.nbcnews.com/id/%s/displaymode/1219' % video_id), video_id)\n        info = all_info.find('video')\n        return {\n            'id': video_id,\n            'title': info.find('headline').text,\n            'ext': 'flv',\n            'url': find_xpath_attr(info, 'media', 'type', 'flashVideo').text,\n            'description': compat_str(info.find('caption').text),\n            'thumbnail': find_xpath_attr(info, 'media', 'type', 'thumbnail').text,\n        }\n    else:\n        title = mobj.group('title')\n        webpage = self._download_webpage(url, title)\n        bootstrap_json = self._search_regex('var bootstrapJson = ({.+})\\\\s*$', webpage, 'bootstrap json', flags=re.MULTILINE)\n        bootstrap = json.loads(bootstrap_json)\n        info = bootstrap['results'][0]['video']\n        mpxid = info['mpxId']\n        base_urls = [info['fallbackPlaylistUrl'], info['associatedPlaylistUrl']]\n        for base_url in base_urls:\n            if (not base_url):\n                continue\n            playlist_url = (base_url + '?form=MPXNBCNewsAPI')\n            all_videos = self._download_json(playlist_url, title)['videos']\n            try:\n                info = next((v for v in all_videos if (v['mpxId'] == mpxid)))\n                break\n            except StopIteration:\n                continue\n        if (info is None):\n            raise ExtractorError('Could not find video in playlists')\n        return {\n            '_type': 'url',\n            'url': info['videoAssets'][(- 1)]['publicUrl'],\n            'ie_key': 'ThePlatform',\n        }\n", "label": 1}
{"function": "\n\ndef core_select_lib(category, llist, create_instance=False, base='kivy.core', basemodule=None):\n    if ('KIVY_DOC' in os.environ):\n        return\n    category = category.lower()\n    basemodule = (basemodule or category)\n    libs_ignored = []\n    errs = []\n    for (option, modulename, classname) in llist:\n        try:\n            try:\n                if (option not in kivy.kivy_options[category]):\n                    libs_ignored.append(modulename)\n                    Logger.debug('{0}: Provider <{1}> ignored by config'.format(category.capitalize(), option))\n                    continue\n            except KeyError:\n                pass\n            mod = __import__(name='{2}.{0}.{1}'.format(basemodule, modulename, base), globals=globals(), locals=locals(), fromlist=[modulename], level=0)\n            cls = mod.__getattribute__(classname)\n            Logger.info('{0}: Provider: {1}{2}'.format(category.capitalize(), option, ('({0} ignored)'.format(libs_ignored) if libs_ignored else '')))\n            if create_instance:\n                cls = cls()\n            return cls\n        except ImportError as e:\n            errs.append((option, e, sys.exc_info()[2]))\n            libs_ignored.append(modulename)\n            Logger.debug('{0}: Ignored <{1}> (import error)'.format(category.capitalize(), option))\n            Logger.trace('', exc_info=e)\n        except CoreCriticalException as e:\n            errs.append((option, e, sys.exc_info()[2]))\n            Logger.error('{0}: Unable to use {1}'.format(category.capitalize(), option))\n            Logger.error('{0}: The module raised an important error: {1!r}'.format(category.capitalize(), e.message))\n            raise\n        except Exception as e:\n            errs.append((option, e, sys.exc_info()[2]))\n            libs_ignored.append(modulename)\n            Logger.trace('{0}: Unable to use {1}'.format(category.capitalize(), option, category))\n            Logger.trace('', exc_info=e)\n    err = '\\n'.join(['{} - {}: {}\\n{}'.format(opt, e.__class__.__name__, e, ''.join(traceback.format_tb(tb))) for (opt, e, tb) in errs])\n    Logger.critical('{0}: Unable to find any valuable {0} provider at all!\\n{1}'.format(category.capitalize(), err))\n", "label": 1}
{"function": "\n\n@classmethod\ndef parse(cls, value):\n    '\\n        Parse this from a header value\\n        '\n    results = []\n    weak_results = []\n    while value:\n        if value.lower().startswith('w/'):\n            weak = True\n            value = value[2:]\n        else:\n            weak = False\n        if value.startswith('\"'):\n            try:\n                (etag, rest) = value[1:].split('\"', 1)\n            except ValueError:\n                etag = value.strip(' \",')\n                rest = ''\n            else:\n                rest = rest.strip(', ')\n        elif (',' in value):\n            (etag, rest) = value.split(',', 1)\n            rest = rest.strip()\n        else:\n            etag = value\n            rest = ''\n        if (etag == '*'):\n            return AnyETag\n        if etag:\n            if weak:\n                weak_results.append(etag)\n            else:\n                results.append(etag)\n        value = rest\n    return cls(results, weak_results)\n", "label": 1}
{"function": "\n\ndef check_field_spec(self, cls, model, flds, label):\n    '\\n        Validate the fields specification in `flds` from a ModelAdmin subclass\\n        `cls` for the `model` model. Use `label` for reporting problems to the user.\\n\\n        The fields specification can be a ``fields`` option or a ``fields``\\n        sub-option from a ``fieldsets`` option component.\\n        '\n    for fields in flds:\n        if (type(fields) != tuple):\n            fields = (fields,)\n        for field in fields:\n            if (field in cls.readonly_fields):\n                continue\n            try:\n                f = model._meta.get_field(field)\n            except models.FieldDoesNotExist:\n                continue\n            if (isinstance(f, models.ManyToManyField) and (not f.rel.through._meta.auto_created)):\n                raise ImproperlyConfigured((\"'%s.%s' can't include the ManyToManyField field '%s' because '%s' manually specifies a 'through' model.\" % (cls.__name__, label, field, field)))\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    maxExcLoc = (- 1)\n    maxException = None\n    for e in self.exprs:\n        try:\n            ret = e._parse(instring, loc, doActions)\n            return ret\n        except ParseException as err:\n            if (err.loc > maxExcLoc):\n                maxException = err\n                maxExcLoc = err.loc\n        except IndexError:\n            if (len(instring) > maxExcLoc):\n                maxException = ParseException(instring, len(instring), e.errmsg, self)\n                maxExcLoc = len(instring)\n    else:\n        if (maxException is not None):\n            raise maxException\n        else:\n            raise ParseException(instring, loc, 'no defined alternatives to match', self)\n", "label": 1}
{"function": "\n\ndef main():\n    o = ord('x')\n    assert (o == 120)\n    n = float('1.1')\n    assert (n == 1.1)\n    n = float('NaN')\n    print(n)\n    assert (isNaN(n) == True)\n    r = round(1.1234, 2)\n    print(r)\n    assert (str(r) == '1.12')\n    x = chr(120)\n    print(x)\n    assert (x == 'x')\n    r = round(100.001, 2)\n    assert (r == 100)\n    i = int(100.1)\n    assert (i == 100)\n    r = round(5.49)\n    assert (r == 5)\n    r = round(5.49, 1)\n    assert (r == 5.5)\n", "label": 1}
{"function": "\n\n@login_required()\n@require_http_methods(['POST'])\ndef save(request):\n    try:\n        policy = SchedulePolicy.objects.get(id=request.POST['id'])\n    except SchedulePolicy.DoesNotExist:\n        policy = SchedulePolicy()\n    policy.name = request.POST['name']\n    policy.repeat_times = request.POST['repeat']\n    try:\n        policy.save()\n    except IntegrityError:\n        messages.error(request, 'Schedule already exists')\n        if (int(request.POST['id']) > 0):\n            return HttpResponseRedirect(reverse('openduty.escalation.edit', None, [str(request.POST['id'])]))\n        else:\n            return HttpResponseRedirect(reverse('openduty.escalation.new'))\n    elements = request.POST.getlist('escalate_to[]')\n    try:\n        SchedulePolicyRule.objects.filter(schedule_policy=policy).delete()\n    except SchedulePolicyRule.DoesNotExist:\n        pass\n    for (idx, item) in enumerate(elements):\n        rule = SchedulePolicyRule()\n        rule.schedule_policy = policy\n        parts = item.split('|')\n        rule.escalate_after = 0\n        rule.position = (idx + 1)\n        if (parts[0] == 'user'):\n            rule.user_id = User.objects.get(id=parts[1])\n            rule.schedule = None\n        if (parts[0] == 'calendar'):\n            rule.schedule = Calendar.objects.get(id=parts[1])\n            rule.user_id = None\n        try:\n            rule.save()\n        except IntegrityError:\n            return HttpResponseRedirect(reverse('openduty.escalation.edit', None, [str(request.POST['id'])]))\n    return HttpResponseRedirect('/policies/')\n", "label": 1}
{"function": "\n\n@classmethod\ndef classpath_entries_contents(cls, classpath_entries):\n    'Provide a generator over the contents (classes/resources) of a classpath.\\n\\n    Subdirectories are included and differentiated via a trailing forward slash (for symmetry\\n    across ZipFile.namelist and directory walks).\\n\\n    :param classpath_entries: A sequence of classpath_entries. Non-jars/dirs are ignored.\\n    :returns: An iterator over all classpath contents, one directory, class or resource relative\\n              path per iteration step.\\n    :rtype: :class:`collections.Iterator` of string\\n    '\n    for entry in classpath_entries:\n        if cls.is_jar(entry):\n            with open_zip(entry, mode='r') as jar:\n                for name in jar.namelist():\n                    (yield name)\n        elif os.path.isdir(entry):\n\n            def rel_walk_name(abs_sub_dir, name):\n                return fast_relpath(os.path.join(abs_sub_dir, name), entry)\n            for (abs_sub_dir, dirnames, filenames) in safe_walk(entry):\n                for name in dirnames:\n                    (yield '{}/'.format(rel_walk_name(abs_sub_dir, name)))\n                for name in filenames:\n                    (yield rel_walk_name(abs_sub_dir, name))\n        else:\n            pass\n", "label": 1}
{"function": "\n\ndef default(self, obj, **kwargs):\n    if isinstance(obj.__class__, DeclarativeMeta):\n        fields = {\n            \n        }\n        for field in [x for x in dir(obj) if ((not x.startswith('_')) and (x != 'metadata'))]:\n            data = obj.__getattribute__(field)\n            try:\n                json.dumps(data)\n                fields[field] = data\n            except TypeError:\n                fields[field] = None\n        return fields\n    elif isinstance(obj, date):\n        return obj.isoformat()\n    return json.JSONEncoder.default(self, obj, **kwargs)\n", "label": 1}
{"function": "\n\ndef compress(self, node):\n    type = node.type\n    result = None\n    if (type in self.__simple):\n        result = type\n    elif (type in self.__prefixes):\n        if getattr(node, 'postfix', False):\n            result = (self.compress(node[0]) + self.__prefixes[node.type])\n        else:\n            result = (self.__prefixes[node.type] + self.compress(node[0]))\n    elif (type in self.__dividers):\n        first = self.compress(node[0])\n        second = self.compress(node[1])\n        divider = self.__dividers[node.type]\n        if (node.type not in ('plus', 'minus')):\n            result = ('%s%s%s' % (first, divider, second))\n        else:\n            result = first\n            if first.endswith(divider):\n                result += ' '\n            result += divider\n            if second.startswith(divider):\n                result += ' '\n            result += second\n    else:\n        try:\n            result = getattr(self, ('type_%s' % type))(node)\n        except KeyError:\n            print((\"Compressor does not support type '%s' from line %s in file %s\" % (type, node.line, node.getFileName())))\n            sys.exit(1)\n    if getattr(node, 'parenthesized', None):\n        return ('(%s)' % result)\n    else:\n        return result\n", "label": 1}
{"function": "\n\ndef extractObjectDefinition(self, objDefJson):\n    if (('objectId_' in objDefJson) or ('objectDefinition_' in objDefJson)):\n        return self.convertObjectArgs(objDefJson)\n    if (('type' not in objDefJson) or ('args' not in objDefJson)):\n        raise MalformedMessageException('Malformed object definition given')\n    objType = objDefJson['type']\n    objectArgs = objDefJson['args']\n    objectArgs = self.convertObjectArgs(objectArgs)\n    if (objType not in AllObjectClassesToExpose.classMap):\n        raise InvalidObjectDefinitionException('Unknown object type')\n    try:\n        objectCls = AllObjectClassesToExpose.classMap[objType]\n        result = objectCls(objectArgs)\n        if (not ComputedGraph.isLocation(result)):\n            result.__dict__['objectDefinition_'] = objDefJson\n        return result\n    except Exceptions.SubscribableWebObjectsException as e:\n        raise InvalidObjectDefinitionException(e.message)\n", "label": 1}
{"function": "\n\ndef _query(self, session, keyspace, count=12, consistency_level=ConsistencyLevel.ONE, use_prepared=False):\n    if use_prepared:\n        query_string = ('SELECT * FROM %s.cf WHERE k = ?' % keyspace)\n        if ((not self.prepared) or (self.prepared.query_string != query_string)):\n            self.prepared = session.prepare(query_string)\n            self.prepared.consistency_level = consistency_level\n        for i in range(count):\n            tries = 0\n            while True:\n                if (tries > 100):\n                    raise RuntimeError('Failed to execute query after 100 attempts: {0}'.format(self.prepared))\n                try:\n                    self.coordinator_stats.add_coordinator(session.execute_async(self.prepared.bind((0,))))\n                    break\n                except (OperationTimedOut, ReadTimeout, ReadFailure):\n                    (ex_type, ex, tb) = sys.exc_info()\n                    log.warn('{0}: {1} Backtrace: {2}'.format(ex_type.__name__, ex, traceback.extract_tb(tb)))\n                    del tb\n                    tries += 1\n    else:\n        routing_key = struct.pack('>i', 0)\n        for i in range(count):\n            ss = SimpleStatement(('SELECT * FROM %s.cf WHERE k = 0' % keyspace), consistency_level=consistency_level, routing_key=routing_key)\n            tries = 0\n            while True:\n                if (tries > 100):\n                    raise RuntimeError('Failed to execute query after 100 attempts: {0}'.format(ss))\n                try:\n                    self.coordinator_stats.add_coordinator(session.execute_async(ss))\n                    break\n                except (OperationTimedOut, ReadTimeout, ReadFailure):\n                    (ex_type, ex, tb) = sys.exc_info()\n                    log.warn('{0}: {1} Backtrace: {2}'.format(ex_type.__name__, ex, traceback.extract_tb(tb)))\n                    del tb\n                    tries += 1\n", "label": 1}
{"function": "\n\ndef set_etcd_facts_if_unset(facts):\n    '\\n    If using embedded etcd, loads the data directory from master-config.yaml.\\n\\n    If using standalone etcd, loads ETCD_DATA_DIR from etcd.conf.\\n\\n    If anything goes wrong parsing these, the fact will not be set.\\n    '\n    if (('master' in facts) and facts['master']['embedded_etcd']):\n        etcd_facts = (facts['etcd'] if ('etcd' in facts) else dict())\n        if ('etcd_data_dir' not in etcd_facts):\n            try:\n                master_cfg_path = os.path.join(facts['common']['config_base'], 'master/master-config.yaml')\n                master_cfg_f = open(master_cfg_path, 'r')\n                config = yaml.safe_load(master_cfg_f.read())\n                master_cfg_f.close()\n                etcd_facts['etcd_data_dir'] = config['etcdConfig']['storageDirectory']\n                facts['etcd'] = etcd_facts\n            except Exception:\n                pass\n    else:\n        etcd_facts = (facts['etcd'] if ('etcd' in facts) else dict())\n        try:\n            ini_str = ('[root]\\n' + open('/etc/etcd/etcd.conf', 'r').read())\n            ini_fp = StringIO.StringIO(ini_str)\n            config = ConfigParser.RawConfigParser()\n            config.readfp(ini_fp)\n            etcd_data_dir = config.get('root', 'ETCD_DATA_DIR')\n            if (etcd_data_dir.startswith('\"') and etcd_data_dir.endswith('\"')):\n                etcd_data_dir = etcd_data_dir[1:(- 1)]\n            etcd_facts['etcd_data_dir'] = etcd_data_dir\n            facts['etcd'] = etcd_facts\n        except Exception:\n            pass\n    return facts\n", "label": 1}
{"function": "\n\ndef post(self, slug, filename):\n    '\\n        Saves given contents to file to game folder.\\n        '\n    game = get_game_by_slug(slug)\n    if (not game):\n        self.set_status(404)\n        return self.finish({\n            'ok': False,\n            'msg': ('Game does not exist: %s' % slug),\n        })\n    if (not filename):\n        self.set_status(400)\n        return self.finish({\n            'ok': False,\n            'msg': 'Missing filename',\n        })\n    if ('..' in filename):\n        self.set_status(403)\n        return self.finish({\n            'ok': False,\n            'msg': 'Cannot write outside game folder',\n        })\n    content_type = self.request.headers.get('Content-Type', '')\n    if (content_type and ('application/x-www-form-urlencoded' in content_type)):\n        content = self.get_argument('content')\n        binary = False\n    else:\n        content = self.request.body\n        binary = True\n    self.request.body = None\n    self.request.arguments = None\n    file_path = path_join(get_absolute_path(game.get_path()), normpath(filename))\n    file_dir = dirname(file_path)\n    if (not create_dir(file_dir)):\n        LOG.error('Failed to create directory at \"%s\"', file_dir)\n        self.set_status(500)\n        return self.finish({\n            'ok': False,\n            'msg': 'Failed to create directory',\n        })\n    if content:\n        if (not binary):\n            try:\n                content = content.encode('utf-8')\n            except UnicodeEncodeError as e:\n                LOG.error('Failed to encode file contents: %s', str(e))\n                self.set_status(500)\n                return self.finish({\n                    'ok': False,\n                    'msg': 'Failed to encode file contents',\n                })\n        LOG.info('Writing file at \"%s\" (%d bytes)', file_path, len(content))\n    else:\n        LOG.info('Writing empty file at \"%s\"', file_path)\n    try:\n        file_obj = open(file_path, 'wb')\n        try:\n            file_obj.write(content)\n        finally:\n            file_obj.close()\n    except IOError as e:\n        LOG.error('Failed to write file at \"%s\": %s', file_path, str(e))\n        self.set_status(500)\n        return self.finish({\n            'ok': False,\n            'msg': 'Failed to write file',\n        })\n    return self.finish({\n        'ok': True,\n    })\n", "label": 1}
{"function": "\n\ndef rebalance(self):\n    \"Figure out which brokers and partitions we should be consuming from,\\n        based on the latest information about the other consumers and brokers\\n        that are present.\\n\\n        We registered for notifications from ZooKeeper whenever a broker or \\n        consumer enters or leaves the pool. But we usually only rebalance right\\n        before we're about to take an action like fetching.\\n\\n        The rebalancing algorithm is slightly different from that described in\\n        the design doc (mostly in the sense that the design doc algorithm will\\n        leave partitions unassigned if there's an uneven distributions). The \\n        idea is that we split the partitions as evently as possible, and if\\n        some consumers need to have more partitions than others, the extra \\n        partitions always go to the earlier consumers in the list. So you could\\n        have a distribution like 4-4-4-4 or 5-5-4-4, but never 4-4-4-5.\\n\\n        Rebalancing has special consequences if the Consumer is doing manual \\n        commits (autocommit=False):\\n\\n        1. This Consumer will keep using the in memory offset state for all \\n           BrokerPartitions that it was already following before the rebalance.\\n        2. The offset state for any new BrokerPartitions that this Consumer is\\n           responsible for after the rebalance will be read from ZooKeeper.\\n        3. For those BrokerPartitions that this Consumer was reading but is no\\n           longer responsible for after the rebalance, the offset state is \\n           simply discarded. It is not persisted to ZooKeeper.\\n        \\n        So there is no guarantee of single delivery in this circumstance. If \\n        BrokerPartition 1-0 shifts ownership from Consumer A to Consumer B in \\n        the rebalance, Consumer B will pick up from the last manual commit of \\n        Consumer A -- *not* the offset that Consumer A was at when the rebalance\\n        was triggered.\\n        \"\n    log.info(('Rebalance triggered for Consumer {0}, broker partitions ' + 'before rebalance: {1}').format(self.id, unicode(self)))\n    if (not self._rebalance_enabled):\n        log.info('Rebalancing disabled -- ignoring rebalance request')\n        return\n    all_topic_consumers = self._zk_util.consumer_ids_for(self.topic, self.consumer_group)\n    all_broker_partitions = self._zk_util.broker_partitions_for(self.topic)\n    try:\n        my_index = all_topic_consumers.index(self.id)\n    except ValueError:\n        msg_tmpl = ('This consumer ({0}) not found list of consumers ' + 'for this topic {1}: {2}')\n        raise ConsumerEntryNotFoundError(msg_tmpl.format(self.id, self.topic, all_topic_consumers))\n    bp_per_consumer = (len(all_broker_partitions) / len(all_topic_consumers))\n    consumers_with_extra = range((len(all_broker_partitions) % len(all_topic_consumers)))\n    num_parts = (bp_per_consumer + (1 if (my_index in consumers_with_extra) else 0))\n    if (my_index == 0):\n        start = 0\n    elif ((my_index - 1) in consumers_with_extra):\n        start = (my_index * (bp_per_consumer + 1))\n    else:\n        start = ((len(consumers_with_extra) * (bp_per_consumer + 1)) + ((my_index - len(consumers_with_extra)) * bp_per_consumer))\n    self._broker_partitions = all_broker_partitions[start:(start + num_parts)]\n    for bp in self._bps_to_next_offsets.keys():\n        if (bp not in self._broker_partitions):\n            del self._bps_to_next_offsets[bp]\n    for bp in self._broker_partitions:\n        self._bps_to_next_offsets.setdefault(bp, None)\n    broker_conn_info = frozenset(((bp.broker_id, bp.host, bp.port) for bp in self._broker_partitions))\n    self._connections = dict(((broker_id, Kafka(host, port)) for (broker_id, host, port) in broker_conn_info))\n    self._register_callbacks()\n    if self._all_callbacks_registered():\n        self._needs_rebalance = False\n    log.info('Rebalance finished for Consumer {0}: {1}'.format(self.id, unicode(self)))\n", "label": 1}
{"function": "\n\ndef perform_job(self, job):\n    ' Wraps a job.perform() call with timeout logic and exception handlers.\\n\\n            This is the first call happening inside the greenlet.\\n        '\n    if self.config['trace_memory']:\n        job.trace_memory_start()\n    set_current_job(job)\n    gevent_timeout = None\n    if job.timeout:\n        gevent_timeout = gevent.Timeout(job.timeout, TimeoutInterrupt(('Job exceeded maximum timeout value in greenlet (%d seconds).' % job.timeout)))\n        gevent_timeout.start()\n    try:\n        job.perform()\n    except RetryInterrupt:\n        self.log.error('Caught retry')\n        job.save_retry(sys.exc_info()[1])\n    except MaxRetriesInterrupt:\n        self.log.error('Max retries reached')\n        job._save_status('maxretries', exception=True)\n    except AbortInterrupt:\n        self.log.error('Caught abort')\n        job.save_abort()\n    except TimeoutInterrupt:\n        self.log.error(('Job timeouted after %s seconds' % job.timeout))\n        job._save_status('timeout', exception=True)\n    except JobInterrupt:\n        self.log.error('Job interrupted')\n        job._save_status('interrupt', exception=True)\n    except Exception:\n        self.log.error('Job failed')\n        job._save_status('failed', exception=True)\n    finally:\n        if gevent_timeout:\n            gevent_timeout.cancel()\n        set_current_job(None)\n        self.done_jobs += 1\n        if self.config['trace_memory']:\n            job.trace_memory_stop()\n", "label": 1}
{"function": "\n\n@defun\ndef ellipfun(ctx, kind, u=None, m=None, q=None, k=None, tau=None):\n    try:\n        S = jacobi_spec[kind]\n    except KeyError:\n        raise ValueError(\"First argument must be a two-character string containing 's', 'c', 'd' or 'n', e.g.: 'sn'\")\n    if (u is None):\n\n        def f(*args, **kwargs):\n            return ctx.ellipfun(kind, *args, **kwargs)\n        f.__name__ = kind\n        return f\n    prec = ctx.prec\n    try:\n        ctx.prec += 10\n        u = ctx.convert(u)\n        q = ctx.qfrom(m=m, q=q, k=k, tau=tau)\n        if (S is None):\n            v = (ctx.one + ((0 * q) * u))\n        elif (q == ctx.zero):\n            if (S[4] == '1'):\n                v = ctx.one\n            else:\n                v = getattr(ctx, S[4])(u)\n            v += ((0 * q) * u)\n        elif (q == ctx.one):\n            if (S[5] == '1'):\n                v = ctx.one\n            else:\n                v = getattr(ctx, S[5])(u)\n            v += ((0 * q) * u)\n        else:\n            t = (u / (ctx.jtheta(3, 0, q) ** 2))\n            v = ctx.one\n            for a in S[0]:\n                v *= ctx.jtheta(a, 0, q)\n            for b in S[1]:\n                v /= ctx.jtheta(b, 0, q)\n            for c in S[2]:\n                v *= ctx.jtheta(c, t, q)\n            for d in S[3]:\n                v /= ctx.jtheta(d, t, q)\n    finally:\n        ctx.prec = prec\n    return (+ v)\n", "label": 1}
{"function": "\n\ndef indexCube(nodes, gridSize, n=None):\n    \"\\n    Returns the index of nodes on the mesh.\\n\\n\\n    Input:\\n       nodes     - string of which nodes to return. e.g. 'ABCD'\\n       gridSize  - size of the nodal grid\\n       n         - number of nodes each i,j,k direction: [ni,nj,nk]\\n\\n\\n    Output:\\n       index  - index in the order asked e.g. 'ABCD' --> (A,B,C,D)\\n\\n    TWO DIMENSIONS::\\n\\n      node(i,j)          node(i,j+1)\\n           A -------------- B\\n           |                |\\n           |    cell(i,j)   |\\n           |        I       |\\n           |                |\\n          D -------------- C\\n      node(i+1,j)        node(i+1,j+1)\\n\\n\\n    THREE DIMENSIONS::\\n\\n            node(i,j,k+1)       node(i,j+1,k+1)\\n                E --------------- F\\n               /|               / |\\n              / |              /  |\\n             /  |             /   |\\n      node(i,j,k)         node(i,j+1,k)\\n           A -------------- B     |\\n           |    H ----------|---- G\\n           |   /cell(i,j)   |   /\\n           |  /     I       |  /\\n           | /              | /\\n           D -------------- C\\n      node(i+1,j,k)      node(i+1,j+1,k)\\n\\n    \"\n    assert (type(nodes) == str), \"Nodes must be a str variable: e.g. 'ABCD'\"\n    assert isinstance(gridSize, np.ndarray), 'Number of nodes must be an ndarray'\n    nodes = nodes.upper()\n    possibleNodes = ('ABCD' if (gridSize.size == 2) else 'ABCDEFGH')\n    for node in nodes:\n        assert (node in possibleNodes), (\"Nodes must be chosen from: '%s'\" % possibleNodes)\n    dim = gridSize.size\n    if (n is None):\n        n = (gridSize - 1)\n    if (dim == 2):\n        ij = ndgrid(np.arange(n[0]), np.arange(n[1]))\n        (i, j) = (ij[:, 0], ij[:, 1])\n    elif (dim == 3):\n        ijk = ndgrid(np.arange(n[0]), np.arange(n[1]), np.arange(n[2]))\n        (i, j, k) = (ijk[:, 0], ijk[:, 1], ijk[:, 2])\n    else:\n        raise Exception('Only 2 and 3 dimensions supported.')\n    nodeMap = {\n        'A': [0, 0, 0],\n        'B': [0, 1, 0],\n        'C': [1, 1, 0],\n        'D': [1, 0, 0],\n        'E': [0, 0, 1],\n        'F': [0, 1, 1],\n        'G': [1, 1, 1],\n        'H': [1, 0, 1],\n    }\n    out = ()\n    for node in nodes:\n        shift = nodeMap[node]\n        if (dim == 2):\n            out += (sub2ind(gridSize, np.c_[((i + shift[0]), (j + shift[1]))]).flatten(),)\n        elif (dim == 3):\n            out += (sub2ind(gridSize, np.c_[((i + shift[0]), (j + shift[1]), (k + shift[2]))]).flatten(),)\n    return out\n", "label": 1}
{"function": "\n\ndef read(self, size=(- 1)):\n    '\\n        read([size]) -> read at most size bytes, returned as a string.\\n\\n        If the size argument is negative or omitted, read until EOF is reached.\\n        Notice that when in non-blocking mode, less data than what was\\n        requested may be returned, even if no size parameter was given.\\n        '\n    if (self.left is not None):\n        size = min(size, self.left)\n    if self.closed:\n        raise ValueError('I/O operation on closed file')\n    if (size < 0):\n        return ''.join(self)\n    elif (not size):\n        chunk = ''\n    elif self.buf:\n        chunk = self.buf\n        self.buf = None\n    else:\n        try:\n            chunk = next(self.iterator)\n        except StopIteration:\n            return ''\n    if (len(chunk) > size):\n        self.buf = chunk[size:]\n        chunk = chunk[:size]\n    if (self.left is not None):\n        self.left -= len(chunk)\n    return chunk\n", "label": 1}
{"function": "\n\ndef _delete_node(self, nid):\n    ' Deletes a specified tree node and all its children.\\n        '\n    for cnid in self._nodes_for(nid):\n        self._delete_node(cnid)\n    pnid = nid.parent()\n    if ((pnid is not None) and (getattr(pnid, '_dummy', None) is nid)):\n        pnid.removeChild(nid)\n        del pnid._dummy\n        return\n    try:\n        (expanded, node, object) = self._get_node_data(nid)\n    except AttributeError:\n        pass\n    else:\n        id_object = id(object)\n        object_info = self._map[id_object]\n        for (i, info) in enumerate(object_info):\n            if (id(nid) == id(info[1])):\n                del object_info[i]\n                break\n        if (len(object_info) == 0):\n            self._remove_listeners(node, object)\n            del self._map[id_object]\n    if (pnid is None):\n        self._tree.takeTopLevelItem(self._tree.indexOfTopLevelItem(nid))\n    else:\n        pnid.removeChild(nid)\n    if ((self._editor is not None) and (id(nid) == id(self._editor._editor_nid))):\n        self._clear_editor()\n", "label": 1}
{"function": "\n\n@register_canonicalize('local_setsubtensor_of_allocs')\n@register_stabilize('local_setsubtensor_of_allocs')\n@gof.local_optimizer([IncSubtensor])\ndef local_setsubtensor_of_constants(node):\n    '\\n    SetSubtensor(x, x[idx], idx) -> x\\n\\n    when x is constant or alloc.\\n\\n    '\n    if (isinstance(node.op, IncSubtensor) and node.op.set_instead_of_inc):\n        x = node.inputs[0]\n        y = node.inputs[1]\n        replace_x = None\n        replace_y = None\n        try:\n            replace_x = get_scalar_constant_value(x)\n        except NotScalarConstantError:\n            pass\n        try:\n            replace_y = get_scalar_constant_value(y)\n        except NotScalarConstantError:\n            pass\n        if ((replace_x is not None) and (replace_y is not None) and (replace_x == replace_y)):\n            return [x]\n        else:\n            return False\n", "label": 1}
{"function": "\n\ndef begin_site(self):\n    for node in self.site.content.walk():\n        for resource in node.resources:\n            created = None\n            modified = None\n            try:\n                created = resource.meta.created\n                modified = resource.meta.modified\n            except AttributeError:\n                pass\n            if ((created != self.vcs_name) and (modified != self.vcs_name)):\n                continue\n            (date_created, date_modified) = self.get_dates(resource)\n            if (created == 'git'):\n                created = (date_created or datetime.utcfromtimestamp(os.path.getctime(resource.path)))\n                created = created.replace(tzinfo=None)\n                resource.meta.created = created\n            if (modified == 'git'):\n                modified = (date_modified or resource.source.last_modified)\n                modified = modified.replace(tzinfo=None)\n                resource.meta.modified = modified\n", "label": 1}
{"function": "\n\ndef test_repeating(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, repeat=True, _time_function=time)\n    time.time = 7\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 0)\n    assert (timer.sleep_time() == 3)\n    time.time = 34\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 6)\n    time.time = 40\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 2)\n    assert (timer.sleep_time() == 10)\n", "label": 1}
{"function": "\n\ndef request(self, method, path, body=None, headers={\n    \n}):\n    if (not self.port):\n        self.port = (443 if self.secure else 80)\n    if self.secure:\n        self.http_conn = httplib.HTTPSConnection(self.host, self.port, timeout=self.timeout)\n    else:\n        self.http_conn = httplib.HTTPConnection(self.host, self.port, timeout=self.timeout)\n    if self.authenticator:\n        self.authenticator.add_auth(headers)\n    self.http_conn.request(method, (self.root + path), body, headers=headers)\n    response = self.http_conn.getresponse()\n    body = response.read()\n    try:\n        data = json.loads(body)\n    except ValueError:\n        data = body\n    if (response.status == 403):\n        raise AuthenticationError()\n    elif (response.status == 500):\n        raise InternalServerError()\n    elif (response.status == 400):\n        raise BadRequestError(body)\n    elif (response.status == 404):\n        raise NotFoundError()\n    return (response, data)\n", "label": 1}
{"function": "\n\ndef user(self, login, password, firstName=None, lastName=None, email=None, admin=False):\n    if (self.module.params['state'] == 'present'):\n        for (var_name, var) in [('firstName', firstName), ('lastName', lastName), ('email', email)]:\n            if (var is None):\n                self.fail(\"{} must be set if state is 'present'\".format(var_name))\n        try:\n            ret = self.authenticate(username=login, password=password)\n            me = self.get('user/me')\n            updateable = ['firstName', 'lastName', 'email', 'admin']\n            passed_in = [firstName, lastName, email, admin]\n            if (set([(k, v) for (k, v) in me.items() if (k in updateable)]) ^ set(zip(updateable, passed_in))):\n                self.put('user/{}'.format(me['_id']), parameters={\n                    'login': login,\n                    'firstName': firstName,\n                    'lastName': lastName,\n                    'password': password,\n                    'email': email,\n                    'admin': ('true' if admin else 'false'),\n                })\n                self.changed = True\n        except AuthenticationError:\n            ret = self.post('user', parameters={\n                'login': login,\n                'firstName': firstName,\n                'lastName': lastName,\n                'password': password,\n                'email': email,\n                'admin': ('true' if admin else 'false'),\n            })\n            self.changed = True\n    elif (self.module.params['state'] == 'absent'):\n        ret = []\n        try:\n            ret = self.authenticate(username=login, password=password)\n            me = self.get('user/me')\n            self.delete('user/{}'.format(me['_id']))\n            self.changed = True\n        except AuthenticationError:\n            ret = []\n    return ret\n", "label": 1}
{"function": "\n\ndef do1(self, reml, irf, ds_ix):\n    if (irf and (ds_ix < 6)):\n        return\n    irfs = ('irf' if irf else 'drf')\n    meth = ('reml' if reml else 'ml')\n    rslt = R_Results(meth, irfs, ds_ix)\n    md = MixedLM(rslt.endog, rslt.exog_fe, rslt.groups, rslt.exog_re)\n    if (not irf):\n        if np.any((np.diag(rslt.cov_re_r) < 1e-05)):\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore')\n                mdf = md.fit(gtol=1e-07, reml=reml)\n        else:\n            mdf = md.fit(gtol=1e-07, reml=reml)\n    else:\n        k_fe = rslt.exog_fe.shape[1]\n        k_re = rslt.exog_re.shape[1]\n        free = MixedLMParams(k_fe, k_re, 0)\n        free.fe_params = np.ones(k_fe)\n        free.cov_re = np.eye(k_re)\n        free.vcomp = np.array([])\n        if np.any((np.diag(rslt.cov_re_r) < 1e-05)):\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore')\n                mdf = md.fit(reml=reml, gtol=1e-07, free=free)\n        else:\n            mdf = md.fit(reml=reml, gtol=1e-07, free=free)\n    assert_almost_equal(mdf.fe_params, rslt.coef, decimal=4)\n    assert_almost_equal(mdf.cov_re, rslt.cov_re_r, decimal=4)\n    assert_almost_equal(mdf.scale, rslt.scale_r, decimal=4)\n    k_fe = md.k_fe\n    assert_almost_equal(rslt.vcov_r, mdf.cov_params()[0:k_fe, 0:k_fe], decimal=3)\n    assert_almost_equal(mdf.llf, rslt.loglike[0], decimal=2)\n    if (not irf):\n        assert_almost_equal(mdf.random_effects[0], rslt.ranef_postmean, decimal=3)\n        assert_almost_equal(mdf.random_effects_cov[0], rslt.ranef_condvar, decimal=3)\n", "label": 1}
{"function": "\n\ndef assert_warns_message(warning_class, message, func, *args, **kw):\n    'Test that a certain warning occurs and with a certain message.\\n\\n    Parameters\\n    ----------\\n    warning_class : the warning class\\n        The class to test for, e.g. UserWarning.\\n\\n    message : str | callable\\n        The entire message or a substring to  test for. If callable,\\n        it takes a string as argument and will trigger an assertion error\\n        if it returns `False`.\\n\\n    func : callable\\n        Calable object to trigger warnings.\\n\\n    *args : the positional arguments to `func`.\\n\\n    **kw : the keyword arguments to `func`.\\n\\n    Returns\\n    -------\\n\\n    result : the return value of `func`\\n\\n    '\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            warnings.simplefilter('ignore', np.VisibleDeprecationWarning)\n        result = func(*args, **kw)\n        if (not (len(w) > 0)):\n            raise AssertionError(('No warning raised when calling %s' % func.__name__))\n        found = [issubclass(warning.category, warning_class) for warning in w]\n        if (not any(found)):\n            raise AssertionError(('No warning raised for %s with class %s' % (func.__name__, warning_class)))\n        message_found = False\n        for index in [i for (i, x) in enumerate(found) if x]:\n            msg = w[index].message\n            msg = str((msg.args[0] if hasattr(msg, 'args') else msg))\n            if callable(message):\n                check_in_message = message\n            else:\n                check_in_message = (lambda msg: (message in msg))\n            if check_in_message(msg):\n                message_found = True\n                break\n        if (not message_found):\n            raise AssertionError((\"Did not receive the message you expected ('%s') for <%s>, got: '%s'\" % (message, func.__name__, msg)))\n    return result\n", "label": 1}
{"function": "\n\ndef test_events():\n    event = test(1, 2, 3, foo='bar')\n    event.success = True\n    event.failure = False\n    event.test_meta = 1\n    id = 1\n    s = dump_event(event, id)\n    (event, id) = load_event(s)\n    assert hasattr(event, 'args')\n    assert hasattr(event, 'kwargs')\n    assert hasattr(event, 'success')\n    assert hasattr(event, 'failure')\n    assert hasattr(event, 'channels')\n    assert hasattr(event, 'notify')\n    assert hasattr(event, 'test_meta')\n", "label": 1}
{"function": "\n\n@contextmanager\ndef ensure_clean(filename=None, return_filelike=False):\n    'Gets a temporary path and agrees to remove on close.\\n\\n    Parameters\\n    ----------\\n    filename : str (optional)\\n        if None, creates a temporary file which is then removed when out of\\n        scope. if passed, creates temporary file with filename as ending.\\n    return_filelike : bool (default False)\\n        if True, returns a file-like which is *always* cleaned. Necessary for\\n        savefig and other functions which want to append extensions.\\n    '\n    filename = (filename or '')\n    fd = None\n    if return_filelike:\n        f = tempfile.TemporaryFile(suffix=filename)\n        try:\n            (yield f)\n        finally:\n            f.close()\n    else:\n        if len(os.path.dirname(filename)):\n            raise ValueError(\"Can't pass a qualified name to ensure_clean()\")\n        try:\n            (fd, filename) = tempfile.mkstemp(suffix=filename)\n        except UnicodeEncodeError:\n            import nose\n            raise nose.SkipTest('no unicode file names on this system')\n        try:\n            (yield filename)\n        finally:\n            try:\n                os.close(fd)\n            except Exception as e:\n                print((\"Couldn't close file descriptor: %d (file: %s)\" % (fd, filename)))\n            try:\n                if os.path.exists(filename):\n                    os.remove(filename)\n            except Exception as e:\n                print(('Exception on removing file: %s' % e))\n", "label": 1}
{"function": "\n\n@inlineCallbacks\ndef onJoin(self, details):\n    print('session attached')\n    for x in [2, 0, (- 2)]:\n        try:\n            res = (yield self.call('com.myapp.sqrt', x))\n        except Exception as e:\n            print('Error: {} {}'.format(e, e.args))\n        else:\n            print('Result: {}'.format(res))\n    for name in ['foo', 'a', ('*' * 11), 'Hello']:\n        try:\n            res = (yield self.call('com.myapp.checkname', name))\n        except ApplicationError as e:\n            print('Error: {} {} {} {}'.format(e, e.error, e.args, e.kwargs))\n        else:\n            print('Result: {}'.format(res))\n    self.define(AppError1)\n    try:\n        (yield self.call('com.myapp.compare', 3, 17))\n    except AppError1 as e:\n        print('Compare Error: {}'.format(e))\n    print('Exiting; we received only errors we expected.')\n    self.leave()\n", "label": 1}
{"function": "\n\ndef dataReceived(self, data):\n    chunk = StringIO()\n    for char in data:\n        if self.gotIAC:\n            if self.iacByte:\n                if (self.iacByte == SB):\n                    if (char == SE):\n                        self.iacSBchunk(chunk.getvalue())\n                        chunk = StringIO()\n                        del self.iacByte\n                        del self.gotIAC\n                    else:\n                        chunk.write(char)\n                else:\n                    try:\n                        getattr(self, ('iac_%s' % iacBytes[self.iacByte]))(char)\n                    except KeyError:\n                        pass\n                    del self.iacByte\n                    del self.gotIAC\n            else:\n                self.iacByte = char\n        elif (char == IAC):\n            c = chunk.getvalue()\n            if c:\n                why = self.processChunk(c)\n                if why:\n                    return why\n                chunk = StringIO()\n            self.gotIAC = 1\n        else:\n            chunk.write(char)\n    c = chunk.getvalue()\n    if c:\n        why = self.processChunk(c)\n        if why:\n            return why\n", "label": 1}
{"function": "\n\ndef _linkcode_resolve(domain, info, package, url_fmt, revision):\n    \"Determine a link to online source for a class/method/function\\n\\n    This is called by sphinx.ext.linkcode\\n\\n    An example with a long-untouched module that everyone has\\n    >>> _linkcode_resolve('py', {'module': 'tty',\\n    ...                          'fullname': 'setraw'},\\n    ...                   package='tty',\\n    ...                   url_fmt='http://hg.python.org/cpython/file/'\\n    ...                           '{revision}/Lib/{package}/{path}#L{lineno}',\\n    ...                   revision='xxxx')\\n    'http://hg.python.org/cpython/file/xxxx/Lib/tty/tty.py#L18'\\n    \"\n    if (revision is None):\n        return\n    if (domain not in ('py', 'pyx')):\n        return\n    if ((not info.get('module')) or (not info.get('fullname'))):\n        return\n    class_name = info['fullname'].split('.')[0]\n    if (type(class_name) != str):\n        class_name = class_name.encode('utf-8')\n    module = __import__(info['module'], fromlist=[class_name])\n    obj = attrgetter(info['fullname'])(module)\n    try:\n        fn = inspect.getsourcefile(obj)\n    except Exception:\n        fn = None\n    if (not fn):\n        try:\n            fn = inspect.getsourcefile(sys.modules[obj.__module__])\n        except Exception:\n            fn = None\n    if (not fn):\n        return\n    fn = os.path.relpath(fn, start=os.path.dirname(__import__(package).__file__))\n    try:\n        lineno = inspect.getsourcelines(obj)[1]\n    except Exception:\n        lineno = ''\n    return url_fmt.format(revision=revision, package=package, path=fn, lineno=lineno)\n", "label": 1}
{"function": "\n\ndef direct_payment(self, credit_card_details, options=None):\n    \"\\n            Function that implement Direct Payment functionality provided by eWay.\\n                (Reference: http://www.eway.com.au/developers/api/direct-payments)\\n\\n            Input Parameters:\\n                ( Please find the details here in this Gist: https://gist.github.com/08893221533daad49388 )\\n                credit_card   :    Customer Credit Card details\\n                options       :    Customer and Recurring Payment details\\n\\n            Output Paramters:\\n                status: 'SUCCESS' or 'FAILURE'\\n                response : eWay Response in Dictionary format.\\n        \"\n    error_response = {\n        \n    }\n    try:\n        if (options and options.get('customer_details', False) and options.get('payment_details', False)):\n            customer_details = options.get('customer_details')\n            payment_details = options.get('payment_details')\n        else:\n            error_response = {\n                'reason': 'Not enough information Available!',\n            }\n            raise\n        '\\n                # Validate Entered credit card details.\\n            '\n        credit_card = CreditCard(**credit_card_details)\n        is_valid = self.validate_card(credit_card)\n        if (not is_valid):\n            raise InvalidCard('Invalid Card')\n        '\\n                # Create direct payment details\\n            '\n        direct_payment_details = self.add_direct_payment_details(credit_card, customer_details, payment_details)\n        '\\n                Process Direct Payment.\\n            '\n        dpObj = DirectPaymentClient(self.direct_payment_url)\n        response = dpObj.process_direct_payment(direct_payment_details)\n        '\\n                Return value based on eWay Response\\n            '\n        eway_response = response.get('ewayResponse', None)\n        if (eway_response and (eway_response.get('ewayTrxnStatus', 'false').lower() == 'true')):\n            status = 'SUCCESS'\n        else:\n            status = 'FAILURE'\n    except Exception as e:\n        error_response['exception'] = e\n        return {\n            'status': 'FAILURE',\n            'response': error_response,\n        }\n    return {\n        'status': status,\n        'response': response,\n    }\n", "label": 1}
{"function": "\n\ndef dispatch(self, pdu):\n    if isinstance(pdu, Symmetry):\n        return\n    if isinstance(pdu, AggregatedFrame):\n        if ((pdu.dsap == 0) and (pdu.ssap == 0)):\n            [log.debug(('     ' + str(p))) for p in pdu]\n            [self.dispatch(p) for p in pdu]\n        return\n    if (isinstance(pdu, Connect) and (pdu.dsap == 1)):\n        addr = self.snl.get(pdu.sn)\n        if ((not addr) or (self.sap[addr] is None)):\n            log.debug(\"no service named '{0}'\".format(pdu.sn))\n            pdu = DisconnectedMode(pdu.ssap, 1, reason=2)\n            self.sap[1].dmpdu.append(pdu)\n            return\n        pdu = Connect(dsap=addr, ssap=pdu.ssap, rw=pdu.rw, miu=pdu.miu)\n    with self.lock:\n        sap = self.sap[pdu.dsap]\n        if sap:\n            sap.enqueue(pdu)\n            return\n    log.debug('discard PDU {0}'.format(str(pdu)))\n    return\n", "label": 1}
{"function": "\n\ndef test_process_info():\n    (m, ctl, config) = init()\n    m.load(config)\n    cmd = TestCommand('process_info', [1])\n    ctl.process_command(cmd)\n    m.stop()\n    m.run()\n    assert isinstance(cmd.result, dict)\n    assert ('info' in cmd.result)\n    info = cmd.result['info']\n    assert (info['name'] == 'default.dummy')\n    assert (info['active'] == True)\n    assert (info['pid'] == 1)\n    assert (info['cmd'] == config['cmd'])\n    assert (info['args'] == config['args'])\n", "label": 1}
{"function": "\n\n@doctest_depends_on(modules=('pyglet',))\ndef tangent_lines(self, p):\n    'Tangent lines between `p` and the ellipse.\\n\\n        If `p` is on the ellipse, returns the tangent line through point `p`.\\n        Otherwise, returns the tangent line(s) from `p` to the ellipse, or\\n        None if no tangent line is possible (e.g., `p` inside ellipse).\\n\\n        Parameters\\n        ==========\\n\\n        p : Point\\n\\n        Returns\\n        =======\\n\\n        tangent_lines : list with 1 or 2 Lines\\n\\n        Raises\\n        ======\\n\\n        NotImplementedError\\n            Can only find tangent lines for a point, `p`, on the ellipse.\\n\\n        See Also\\n        ========\\n\\n        sympy.geometry.point.Point, sympy.geometry.line.Line\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy import Point, Ellipse\\n        >>> e1 = Ellipse(Point(0, 0), 3, 2)\\n        >>> e1.tangent_lines(Point(3, 0))\\n        [Line(Point2D(3, 0), Point2D(3, -12))]\\n\\n        >>> # This will plot an ellipse together with a tangent line.\\n        >>> from sympy.plotting.pygletplot import PygletPlot as Plot\\n        >>> from sympy import Point, Ellipse\\n        >>> e = Ellipse(Point(0,0), 3, 2)\\n        >>> t = e.tangent_lines(e.random_point())\\n        >>> p = Plot()\\n        >>> p[0] = e # doctest: +SKIP\\n        >>> p[1] = t # doctest: +SKIP\\n\\n        '\n    p = Point(p)\n    if self.encloses_point(p):\n        return []\n    if (p in self):\n        delta = (self.center - p)\n        rise = ((self.vradius ** 2) * delta.x)\n        run = ((- (self.hradius ** 2)) * delta.y)\n        p2 = Point(simplify((p.x + run)), simplify((p.y + rise)))\n        return [Line(p, p2)]\n    else:\n        if (len(self.foci) == 2):\n            (f1, f2) = self.foci\n            maj = self.hradius\n            test = (((2 * maj) - Point.distance(f1, p)) - Point.distance(f2, p))\n        else:\n            test = (self.radius - Point.distance(self.center, p))\n        if (test.is_number and test.is_positive):\n            return []\n        (x, y) = (Dummy('x'), Dummy('y'))\n        eq = self.equation(x, y)\n        dydx = idiff(eq, y, x)\n        slope = Line(p, Point(x, y)).slope\n        tangent_points = solve([(slope - dydx), eq], [x, y])\n        if (len(tangent_points) == 1):\n            assert ((tangent_points[0][0] == p.x) or (tangent_points[0][1] == p.y))\n            return [Line(p, (p + Point(1, 0))), Line(p, (p + Point(0, 1)))]\n        return [Line(p, tangent_points[0]), Line(p, tangent_points[1])]\n", "label": 1}
{"function": "\n\ndef get_urls_from_handle(self):\n    ' Parse the xml file and generate the elements '\n    if self._validate:\n        schema = etree.XMLSchema(file=open(self.get_schema_path()))\n    else:\n        schema = None\n    context = etree.iterparse(self._handle, events=('end',), schema=schema)\n    element_data = {\n        \n    }\n    for (action, elem) in context:\n        tag = self._remove_ns(elem.tag)\n        if ((tag == 'url') and element_data):\n            try:\n                e = UrlSetElement(**element_data)\n                (yield e)\n            except ValueError:\n                element_data = {\n                    \n                }\n                continue\n        elif (tag in ['loc', 'lastmod', 'changefreq', 'priority']):\n            element_data[tag] = elem.text\n        while (elem.getprevious() is not None):\n            del elem.getparent()[0]\n    del context\n    del schema\n", "label": 1}
{"function": "\n\ndef as_path(tokeniser):\n    as_seq = []\n    as_set = []\n    value = tokeniser()\n    inset = False\n    try:\n        if (value == '['):\n            while True:\n                value = tokeniser()\n                if (value == ','):\n                    continue\n                if (value in ('(', '[')):\n                    inset = True\n                    while True:\n                        value = tokeniser()\n                        if (value == ')'):\n                            break\n                        as_set.append(ASN.from_string(value))\n                if (value == ')'):\n                    inset = False\n                    continue\n                if (value == ']'):\n                    if inset:\n                        inset = False\n                        continue\n                    break\n                as_seq.append(ASN.from_string(value))\n        else:\n            as_seq.append(ASN.from_string(value))\n    except ValueError:\n        raise ValueError('could not parse as-path')\n    return ASPath(as_seq, as_set)\n", "label": 1}
{"function": "\n\ndef test_show_license_info(self, runner):\n    all_the_licenses = {\n        'agpl-3.0': ('GNU Affero General Public License v3.0', 11),\n        'apache-2.0': ('Apache License 2.0', 10),\n        'artistic-2.0': ('Artistic License 2.0', 9),\n        'bsd-2-clause': ('BSD 2-clause \"Simplified\" License', 7),\n        'bsd-3-clause': ('BSD 3-clause \"New\" or \"Revised\" License', 8),\n        'cc0-1.0': ('Creative Commons Zero v1.0 Universal', 5),\n        'epl-1.0': ('Eclipse Public License 1.0', 9),\n        'gpl-2.0': ('GNU General Public License v2.0', 10),\n        'gpl-3.0': ('GNU General Public License v3.0', 10),\n        'isc': ('ISC license', 7),\n        'lgpl-2.1': ('GNU Lesser General Public License v2.1', 10),\n        'lgpl-3.0': ('GNU Lesser General Public License v3.0', 10),\n        'mit': ('MIT License', 7),\n        'mpl-2.0': ('Mozilla Public License 2.0', 10),\n        'unlicense': ('The Unlicense', 6),\n    }\n    for (short_name, fullname_and_rules_number) in all_the_licenses.items():\n        result = runner.invoke(info, [short_name])\n        (output, exit_code) = (result.output, result.exit_code)\n        rules = output.split('Forbidden\\n')[1].split('\\n')\n        flat_rules = sum([item.split() for item in rules], [])\n        (fullname, rules_number) = fullname_and_rules_number\n        assert (exit_code == 0)\n        assert ('</a>' not in output)\n        assert (fullname in output)\n        assert ('{0:<25}{1:<25}{2}'.format('Required', 'Permitted', 'Forbidden') in output)\n        assert (len(flat_rules) == rules_number)\n", "label": 1}
{"function": "\n\n@contextfilter\ndef httime(context, time, timeformat='TIME_FORMAT'):\n    ' Render time in the current locale '\n    if (not time):\n        return ''\n    lang = translation.get_language()\n    localeformat = timeformat\n    formatspath = getattr(settings, 'FORMAT_MODULE_PATH', 'treeio.formats')\n    try:\n        modulepath = (((formatspath + '.') + lang) + '.formats')\n        module = __import__(modulepath, fromlist=[str(modulepath)])\n        localeformat = getattr(module, timeformat, timeformat)\n    except ImportError:\n        pass\n    request = context['request']\n    user = None\n    if request.user.username:\n        try:\n            user = request.user.profile\n        except Exception:\n            pass\n    default_timezone = settings.HARDTREE_SERVER_DEFAULT_TIMEZONE\n    try:\n        conf = ModuleSetting.get('default_timezone')[0]\n        default_timezone = conf.value\n    except:\n        pass\n    try:\n        conf = ModuleSetting.get('default_timezone', user=user)[0]\n        default_timezone = conf.value\n    except Exception:\n        default_timezone = getattr(settings, 'HARDTREE_SERVER_TIMEZONE')[default_timezone][0]\n    all_timezones = getattr(settings, 'HARDTREE_SERVER_TIMEZONE', [(1, '(GMT-11:00) International Date Line West')])\n    title = all_timezones[int(default_timezone)][1]\n    GMT = title[4:10]\n    sign = GMT[0:1]\n    hours = int(GMT[1:3])\n    mins = int(GMT[4:6])\n    if (sign == '-'):\n        time = (time - timedelta(hours=hours, minutes=mins))\n    else:\n        time = (time + timedelta(hours=hours, minutes=mins))\n    result = djangotime(time, localeformat)\n    return Markup(result)\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('shuffle', [True, False])\ndef test_simple_x_and_y(self, BatchIterator, X, y, shuffle):\n    bi = BatchIterator(2, shuffle=shuffle)(X, y)\n    batches = list(bi)\n    assert (len(batches) == 10)\n    (X0, y0) = batches[0]\n    assert (X0.shape == (2, 10))\n    assert (y0.shape == (2,))\n    Xt = np.vstack((b[0] for b in batches))\n    yt = np.hstack((b[1] for b in batches))\n    assert (Xt.shape == X.shape)\n    assert (yt.shape == y.shape)\n    np.testing.assert_equal(Xt[:, 0], yt)\n    if (shuffle is False):\n        np.testing.assert_equal(X[:2], X0)\n        np.testing.assert_equal(y[:2], y0)\n", "label": 1}
{"function": "\n\ndef dump_things_worker(ckan, thing, arguments, stdin=None, stdout=None):\n    '\\n    a process that accepts names on stdin which are\\n    passed to the {thing}_show actions.  it produces lines of json\\n    which are the responses from each action call.\\n    '\n    if (stdin is None):\n        stdin = getattr(sys.stdin, 'buffer', sys.stdin)\n        try:\n            sys.stdin = open('/dev/tty', 'rb')\n        except IOError:\n            pass\n    if (stdout is None):\n        stdout = getattr(sys.stdout, 'buffer', sys.stdout)\n        sys.stdout = sys.stderr\n    thing_show = {\n        'datasets': 'package_show',\n        'groups': 'group_show',\n        'organizations': 'organization_show',\n        'users': 'user_show',\n        'related': 'related_show',\n    }[thing]\n\n    def reply(error, record=None):\n        '\\n        format messages to be sent back to parent process\\n        '\n        stdout.write((compact_json([datetime.now().isoformat(), error, record]) + b'\\n'))\n        stdout.flush()\n    for line in iter(stdin.readline, b''):\n        try:\n            name = json.loads(line.decode('utf-8'))\n        except UnicodeDecodeError as e:\n            reply('UnicodeDecodeError')\n            continue\n        try:\n            obj = ckan.call_action(thing_show, {\n                'id': name,\n                'include_datasets': False,\n                'include_password_hash': True,\n            })\n            reply(None, obj)\n        except NotFound:\n            reply('NotFound')\n        except NotAuthorized:\n            reply('NotAuthorized')\n", "label": 1}
{"function": "\n\ndef _logdetA(self, K, w, nargout):\n    '\\n        Compute the log determinant ldA and the inverse iA of a square nxn matrix\\n        A = eye(n) + K*diag(w) from its LU decomposition; for negative definite A, we\\n        return ldA = Inf. We also return mwiA = -diag(w)*inv(A).\\n        [ldA,iA,mwiA] = _logdetA(K,w)'\n    n = K.shape[0]\n    assert (K.shape[0] == K.shape[1])\n    A = (np.eye(n) + (K * np.tile(w.T, (n, 1))))\n    [L, U, P] = np.linalg.lu(A)\n    u = np.diag(U)\n    signU = np.prod(np.sign(u))\n    detP = 1\n    p = np.dot(P, np.array(list(range(n))).T)\n    for ii in range(n):\n        if (ii != p[ii]):\n            detP = (- detP)\n            j = [jj for (jj, val) in enumerate(p) if (val == ii)]\n            p[(ii, j[0])] = p[(j[0], ii)]\n    if (signU != detP):\n        ldA = np.inf\n    else:\n        ldA = np.log(np.abs(u)).sum()\n    if (nargout > 1):\n        iA = ((inv(U) * inv(L)) * P)\n        if (nargout > 2):\n            mwiA = ((- np.tile(w, (1, n))) * iA)\n            return (ldA, iA, mwiA)\n        else:\n            return (ldA, iA)\n    else:\n        return ldA\n", "label": 1}
{"function": "\n\ndef plot_bc(self, ftype=None, package=None, kper=0, color=None, plotAll=False, **kwargs):\n    \"\\n        Plot boundary conditions locations for a specific boundary\\n        type from a flopy model\\n\\n        Parameters\\n        ----------\\n        ftype : string\\n            Package name string ('WEL', 'GHB', etc.). (Default is None)\\n        package : flopy.modflow.Modflow package class instance\\n            flopy package class instance. (Default is None)\\n        kper : int\\n            Stress period to plot\\n        color : string\\n            matplotlib color string. (Default is None)\\n        plotAll : bool\\n            Boolean used to specify that boundary condition locations for all\\n            layers will be plotted on the current ModelMap layer.\\n            (Default is False)\\n        **kwargs : dictionary\\n            keyword arguments passed to matplotlib.collections.PatchCollection\\n\\n        Returns\\n        -------\\n        quadmesh : matplotlib.collections.QuadMesh\\n\\n        \"\n    if ('ax' in kwargs):\n        ax = kwargs.pop('ax')\n    else:\n        ax = self.ax\n    if (package is not None):\n        p = package\n    elif (self.model is not None):\n        if (ftype is None):\n            raise Exception('ftype not specified')\n        p = self.model.get_package(ftype)\n    else:\n        raise Exception('Cannot find package to plot')\n    try:\n        mflist = p.stress_period_data[kper]\n    except Exception as e:\n        raise Exception(('Not a list-style boundary package:' + str(e)))\n    if (mflist is None):\n        return None\n    nlay = self.model.nlay\n    plotarray = np.zeros((nlay, self.sr.nrow, self.sr.ncol), dtype=np.int)\n    if plotAll:\n        idx = [mflist['i'], mflist['j']]\n        pa = np.zeros((self.sr.nrow, self.sr.ncol), dtype=np.int)\n        pa[idx] = 1\n        for k in range(nlay):\n            plotarray[k, :, :] = pa.copy()\n    else:\n        idx = [mflist['k'], mflist['i'], mflist['j']]\n        plotarray[idx] = 1\n    plotarray = np.ma.masked_equal(plotarray, 0)\n    if (color is None):\n        if (ftype in bc_color_dict):\n            c = bc_color_dict[ftype]\n        else:\n            c = bc_color_dict['default']\n    else:\n        c = color\n    cmap = matplotlib.colors.ListedColormap(['0', c])\n    bounds = [0, 1, 2]\n    norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n    quadmesh = self.plot_array(plotarray, cmap=cmap, norm=norm, **kwargs)\n    return quadmesh\n", "label": 1}
{"function": "\n\ndef cancel_job(job_execution_id):\n    ctx = context.ctx()\n    job_execution = conductor.job_execution_get(ctx, job_execution_id)\n    if (job_execution.info['status'] in edp.JOB_STATUSES_TERMINATED):\n        LOG.info(_LI(\"Job execution is already finished and shouldn't be canceled\"))\n        return job_execution\n    cluster = conductor.cluster_get(ctx, job_execution.cluster_id)\n    if (cluster is None):\n        LOG.info(_LI('Can not cancel this job on a non-existant cluster.'))\n        return job_execution\n    engine = get_job_engine(cluster, job_execution)\n    if (engine is not None):\n        job_execution = conductor.job_execution_update(ctx, job_execution_id, {\n            'info': {\n                'status': edp.JOB_STATUS_TOBEKILLED,\n            },\n        })\n        timeout = CONF.job_canceling_timeout\n        s_time = timeutils.utcnow()\n        while (timeutils.delta_seconds(s_time, timeutils.utcnow()) < timeout):\n            if (job_execution.info['status'] not in edp.JOB_STATUSES_TERMINATED):\n                try:\n                    job_info = engine.cancel_job(job_execution)\n                except Exception as ex:\n                    job_info = None\n                    LOG.warning(_LW('Error during cancel of job execution: {error}').format(error=ex))\n                if (job_info is not None):\n                    job_execution = _write_job_status(job_execution, job_info)\n                    LOG.info(_LI('Job execution was canceled successfully'))\n                    return job_execution\n                context.sleep(3)\n                job_execution = conductor.job_execution_get(ctx, job_execution_id)\n                if (not job_execution):\n                    LOG.info(_LI('Job execution was deleted. Canceling current operation.'))\n                    return job_execution\n            else:\n                LOG.info(_LI('Job execution status: {status}').format(status=job_execution.info['status']))\n                return job_execution\n        else:\n            raise e.CancelingFailed((_('Job execution %s was not canceled') % job_execution.id))\n", "label": 1}
{"function": "\n\ndef _find_mac(command, args, hw_identifiers, get_index):\n    import os, shutil\n    executable = shutil.which(command)\n    if (executable is None):\n        path = os.pathsep.join(('/sbin', '/usr/sbin'))\n        executable = shutil.which(command, path=path)\n        if (executable is None):\n            return None\n    try:\n        cmd = ('LC_ALL=C %s %s 2>/dev/null' % (executable, args))\n        with os.popen(cmd) as pipe:\n            for line in pipe:\n                words = line.lower().split()\n                for i in range(len(words)):\n                    if (words[i] in hw_identifiers):\n                        try:\n                            return int(words[get_index(i)].replace(':', ''), 16)\n                        except (ValueError, IndexError):\n                            pass\n    except OSError:\n        pass\n", "label": 1}
{"function": "\n\ndef bandwidth_usage(instance_ref, audit_start, ignore_missing_network_data=True):\n    'Get bandwidth usage information for the instance for the\\n    specified audit period.\\n    '\n    admin_context = nova.context.get_admin_context(read_deleted='yes')\n\n    def _get_nwinfo_old_skool():\n        'Support for getting network info without objects.'\n        if (instance_ref.get('info_cache') and (instance_ref['info_cache'].get('network_info') is not None)):\n            cached_info = instance_ref['info_cache']['network_info']\n            if isinstance(cached_info, network_model.NetworkInfo):\n                return cached_info\n            return network_model.NetworkInfo.hydrate(cached_info)\n        try:\n            return network.API().get_instance_nw_info(admin_context, instance_ref)\n        except Exception:\n            try:\n                with excutils.save_and_reraise_exception():\n                    LOG.exception(_LE('Failed to get nw_info'), instance=instance_ref)\n            except Exception:\n                if ignore_missing_network_data:\n                    return\n                raise\n    if isinstance(instance_ref, obj_base.NovaObject):\n        nw_info = instance_ref.info_cache.network_info\n        if (nw_info is None):\n            nw_info = network_model.NetworkInfo()\n    else:\n        nw_info = _get_nwinfo_old_skool()\n    macs = [vif['address'] for vif in nw_info]\n    uuids = [instance_ref['uuid']]\n    bw_usages = objects.BandwidthUsageList.get_by_uuids(admin_context, uuids, audit_start)\n    bw = {\n        \n    }\n    for b in bw_usages:\n        if (b.mac in macs):\n            label = ('net-name-not-found-%s' % b.mac)\n            for vif in nw_info:\n                if (vif['address'] == b.mac):\n                    label = vif['network']['label']\n                    break\n            bw[label] = dict(bw_in=b.bw_in, bw_out=b.bw_out)\n    return bw\n", "label": 1}
{"function": "\n\ndef duration(text, loader_context):\n    cformat = loader_context.get('duration')\n    text_int = None\n    try:\n        text_int = int(text)\n    except ValueError:\n        pass\n    if (cformat == '%H:%M'):\n        if text_int:\n            text += ':00'\n    if (cformat == '%M'):\n        text = _breakdown_time_unit_overlap(text, 60)\n        cformat = '%H:%M'\n    if (cformat == '%M:%S'):\n        if text_int:\n            text += ':00'\n        text = _breakdown_time_unit_overlap(text, 60)\n        cformat = '%H:%M:%S'\n    if (cformat == '%S'):\n        if text_int:\n            if (text_int >= 3600):\n                hours_str = (str((text_int // 3600)) + ':')\n                secs_under_hour_str = str((text_int % 3600))\n                text = (hours_str + _breakdown_time_unit_overlap(secs_under_hour_str, 60))\n                cformat = '%H:%M:%S'\n            else:\n                text = _breakdown_time_unit_overlap(text, 60)\n                cformat = '%M:%S'\n    try:\n        duration = datetime.datetime.strptime(text, cformat)\n    except ValueError:\n        loader_context.get('spider').log('Duration could not be parsed (\"{t}\", Format string: \"{f}\")!'.format(t=text, f=cformat), logging.ERROR)\n        return None\n    return duration.strftime('%H:%M:%S')\n", "label": 1}
{"function": "\n\ndef test_by_cause(self):\n    project = self.create_project()\n    patch = self.create_patch(repository=project.repository)\n    source = self.create_source(project, patch=patch)\n    self.create_build(project)\n    push_build = self.create_build(project, source=source, cause=Cause.push)\n    push_build2 = self.create_build(project, source=source, cause=Cause.push)\n    snapshot_build = self.create_build(project, source=source, cause=Cause.snapshot)\n    snapshot_path = '/api/0/projects/{0}/builds/?cause=snapshot'.format(project.id.hex)\n    push_path = '/api/0/projects/{0}/builds/?cause=push'.format(project.id.hex)\n    resp = self.client.get(snapshot_path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == snapshot_build.id.hex)\n    resp = self.client.get(push_path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 2)\n    push_build_ids = [push_build.id.hex, push_build2.id.hex]\n    assert (data[0]['id'] in push_build_ids)\n    assert (data[1]['id'] in push_build_ids)\n", "label": 1}
{"function": "\n\ndef test_from_dataset():\n    '\\n    Tests whether it supports integer labels.\\n    '\n    rng = np.random.RandomState([1, 2, 3])\n    topo_view = rng.randn(12, 2, 3, 3)\n    y = rng.randint(0, 5, (12, 1))\n    d1 = DenseDesignMatrix(topo_view=topo_view)\n    slice_d = from_dataset(d1, 5)\n    assert (slice_d.X.shape[1] == d1.X.shape[1])\n    assert (slice_d.X.shape[0] == 5)\n    d2 = DenseDesignMatrix(topo_view=topo_view, y=y)\n    slice_d = from_dataset(d2, 5)\n    assert (slice_d.X.shape[1] == d2.X.shape[1])\n    assert (slice_d.X.shape[0] == 5)\n    assert (slice_d.y.shape[0] == 5)\n    x = topo_view.reshape(12, 18)\n    d3 = DenseDesignMatrix(X=x, y=y)\n    slice_d = from_dataset(d3, 5)\n    assert (slice_d.X.shape[1] == d3.X.shape[1])\n    assert (slice_d.X.shape[0] == 5)\n    assert (slice_d.y.shape[0] == 5)\n", "label": 1}
{"function": "\n\ndef generate_global_pillar_file(self, update_formulas=False):\n    from stackdio.api.formulas.models import FormulaVersion\n    from stackdio.api.formulas.tasks import update_formula\n    pillar_props = {\n        \n    }\n    accounts = set([host.cloud_image.account for host in self.hosts.all()])\n    global_formulas = []\n    for account in accounts:\n        global_formulas.extend(account.get_formulas())\n    if update_formulas:\n        for formula in global_formulas:\n            if formula.private_git_repo:\n                logger.debug('Skipping private formula: {0}'.format(formula.uri))\n                continue\n            try:\n                version = self.formula_versions.get(formula=formula).version\n            except FormulaVersion.DoesNotExist:\n                version = formula.default_version\n            update_formula.si(formula.id, None, version, raise_exception=False)()\n    for formula in set(global_formulas):\n        recursive_update(pillar_props, formula.properties)\n    for account in accounts:\n        recursive_update(pillar_props, account.global_orchestration_properties)\n    pillar_file_yaml = yaml.safe_dump(pillar_props, default_flow_style=False)\n    if (not self.global_pillar_file):\n        self.global_pillar_file.save('stack.global_pillar', ContentFile(pillar_file_yaml))\n    else:\n        with open(self.global_pillar_file.path, 'w') as f:\n            f.write(pillar_file_yaml)\n", "label": 1}
{"function": "\n\ndef activate(self, mac):\n    assert (type(mac) in (nfc.dep.Initiator, nfc.dep.Target))\n    self.mac = None\n    miu = self.cfg['recv-miu']\n    lto = self.cfg['send-lto']\n    wks = (1 + sum(sorted([(1 << sap) for sap in self.snl.values() if (sap < 15)])))\n    pax = ParameterExchange(version=(1, 1), miu=miu, lto=lto, wks=wks)\n    if (type(mac) == nfc.dep.Initiator):\n        gb = mac.activate(gbi=('Ffm' + pax.to_string()[2:]))\n        self.run = self.run_as_initiator\n        role = 'Initiator'\n    if (type(mac) == nfc.dep.Target):\n        gb = mac.activate(gbt=('Ffm' + pax.to_string()[2:]), wt=9)\n        self.run = self.run_as_target\n        role = 'Target'\n    if ((gb is not None) and gb.startswith('Ffm') and (len(gb) >= 6)):\n        info = ['LLCP Link established as NFC-DEP {0}'.format(role)]\n        info.append('Local LLCP Settings')\n        info.append('  LLCP Version: {0[0]}.{0[1]}'.format(pax.version))\n        info.append('  Link Timeout: {0} ms'.format(pax.lto))\n        info.append('  Max Inf Unit: {0} octet'.format(pax.miu))\n        info.append('  Service List: {0:016b}'.format(pax.wks))\n        pax = ProtocolDataUnit.from_string(('\\x00@' + str(gb[3:])))\n        info.append('Remote LLCP Settings')\n        info.append('  LLCP Version: {0[0]}.{0[1]}'.format(pax.version))\n        info.append('  Link Timeout: {0} ms'.format(pax.lto))\n        info.append('  Max Inf Unit: {0} octet'.format(pax.miu))\n        info.append('  Service List: {0:016b}'.format(pax.wks))\n        log.info('\\n'.join(info))\n        self.cfg['rcvd-ver'] = pax.version\n        self.cfg['send-miu'] = pax.miu\n        self.cfg['recv-lto'] = pax.lto\n        self.cfg['send-wks'] = pax.wks\n        self.cfg['send-lsc'] = pax.lsc\n        log.debug('llc cfg {0}'.format(self.cfg))\n        if ((type(mac) == nfc.dep.Initiator) and (mac.rwt is not None)):\n            max_rwt = ((4096 / 13560000.0) * (2 ** 10))\n            if (mac.rwt > max_rwt):\n                log.warning('NFC-DEP RWT {0:.3f} exceeds max {1:.3f} sec'.format(mac.rwt, max_rwt))\n        self.mac = mac\n    return bool(self.mac)\n", "label": 1}
{"function": "\n\ndef test_exists_many_with_non_existent_keys_in_middle(self):\n    self.keys.append(('test', 'demo', 'some_key'))\n    for i in range(15, 20):\n        key = ('test', 'demo', i)\n        rec = {\n            'name': ('name%s' % str(i)),\n            'age': i,\n        }\n        TestExistsMany.client.put(key, rec)\n        self.keys.append(key)\n    records = TestExistsMany.client.exists_many(self.keys)\n    for i in range(15, 20):\n        key = ('test', 'demo', i)\n        TestExistsMany.client.remove(key)\n    assert (type(records) == list)\n    assert (len(records) == 11)\n    assert (Counter([x[0][2] for x in records]) == Counter([0, 1, 2, 3, 4, 'some_key', 15, 16, 17, 18, 19]))\n    for x in records:\n        if (x[0][2] == 'some_key'):\n            assert (x[1] == None)\n", "label": 1}
{"function": "\n\n@fab.parallel\ndef destroy(leave_data=False, kill_delay=0):\n    'Uninstall Cassandra and clean up data and logs'\n    if leave_data:\n        fab.run('JAVA_HOME={java_home} {nodetool_cmd} drain'.format(java_home=config['java_home'], nodetool_cmd=_nodetool_cmd()), quiet=True)\n        fab.run('rm -rf {commitlog}/*'.format(commitlog=config['commitlog_directory']))\n    if kill_delay:\n        fab.run('killall java', quiet=True)\n        time.sleep(kill_delay)\n    fab.run('killall -9 java', quiet=True)\n    fab.run('pkill -f \"python.*fincore_capture\"', quiet=True)\n    fab.run('rm -rf fab/cassandra')\n    fab.run('rm -rf fab/dse')\n    fab.run('rm -rf fab/scripts')\n    fab.run('rm -f fab/nohup.log')\n    assert (type(config['data_file_directories']) == list)\n    for t in ([config['saved_caches_directory'], config['commitlog_directory'], config['flush_directory'], config['log_dir']] + config['data_file_directories']):\n        assert ((type(t) in (str, unicode)) and (len(t) > 1)), \"{t} doesn't look like a directory\".format(t=t)\n    if (not leave_data):\n        for d in config['data_file_directories']:\n            fab.run('rm -rf {data}/*'.format(data=d))\n        fab.run('rm -rf {saved_caches_directory}/*'.format(saved_caches_directory=config['saved_caches_directory']))\n        fab.run('rm -rf {commitlog}/*'.format(commitlog=config['commitlog_directory']))\n        fab.run('rm -rf {flushdir}/*'.format(flushdir=config['flush_directory']))\n        if config.get('hints_directory'):\n            fab.run('rm -rf {hints_directory}/*'.format(hints_directory=config.get('hints_directory')))\n    fab.run('rm -rf {log_dir}'.format(log_dir=config['log_dir']))\n    fab.run('rm -f /tmp/fincore.stats.log')\n", "label": 1}
{"function": "\n\ndef _get_asam_configuration(driver_url=''):\n    '\\n    Return the configuration read from the master configuration\\n    file or directory\\n    '\n    asam_config = (__opts__['asam'] if ('asam' in __opts__) else None)\n    if asam_config:\n        try:\n            for (asam_server, service_config) in six.iteritems(asam_config):\n                username = service_config.get('username', None)\n                password = service_config.get('password', None)\n                protocol = service_config.get('protocol', 'https')\n                port = service_config.get('port', 3451)\n                if ((not username) or (not password)):\n                    log.error('Username or Password has not been specified in the master configuration for {0}'.format(asam_server))\n                    return False\n                ret = {\n                    'platform_edit_url': '{0}://{1}:{2}/config/PlatformEdit.html'.format(protocol, asam_server, port),\n                    'platform_config_url': '{0}://{1}:{2}/config/PlatformConfig.html'.format(protocol, asam_server, port),\n                    'platformset_edit_url': '{0}://{1}:{2}/config/PlatformSetEdit.html'.format(protocol, asam_server, port),\n                    'platformset_config_url': '{0}://{1}:{2}/config/PlatformSetConfig.html'.format(protocol, asam_server, port),\n                    'username': username,\n                    'password': password,\n                }\n                if ((not driver_url) or (driver_url == asam_server)):\n                    return ret\n        except Exception as exc:\n            log.error('Exception encountered: {0}'.format(exc))\n            return False\n        if driver_url:\n            log.error('Configuration for {0} has not been specified in the master configuration'.format(driver_url))\n            return False\n    return False\n", "label": 1}
{"function": "\n\ndef create_margin(self, cli, window_render_info, width, height):\n    total_height = window_render_info.content_height\n    display_arrows = self.display_arrows(cli)\n    window_height = window_render_info.window_height\n    if display_arrows:\n        window_height -= 2\n    try:\n        items_per_row = (float(total_height) / min(total_height, window_height))\n    except ZeroDivisionError:\n        return []\n    else:\n\n        def is_scroll_button(row):\n            ' True if we should display a button on this row. '\n            current_row_middle = int(((row + 0.5) * items_per_row))\n            return (current_row_middle in window_render_info.displayed_lines)\n        result = []\n        if display_arrows:\n            result.extend([(Token.Scrollbar.Arrow, '^'), (Token.Scrollbar, '\\n')])\n        for i in range(window_height):\n            if is_scroll_button(i):\n                result.append((Token.Scrollbar.Button, ' '))\n            else:\n                result.append((Token.Scrollbar, ' '))\n            result.append((Token, '\\n'))\n        if display_arrows:\n            result.append((Token.Scrollbar.Arrow, 'v'))\n        return result\n", "label": 1}
{"function": "\n\ndef _register_rpcmethods(apps, restrict_introspection=False, dispatchers={\n    \n}):\n    '\\n    Scans the installed apps for methods with the rpcmethod decorator\\n    Adds these methods to the list of methods callable via RPC\\n    '\n    for appname in apps:\n        app = __import__(appname, globals(), locals(), ['*'])\n        for obj in dir(app):\n            method = getattr(app, obj)\n            if (callable(method) and getattr(method, 'is_rpcmethod', False)):\n                if (method.url_name not in dispatchers):\n                    dispatchers[method.url_name] = RPCDispatcher(method.url_name, restrict_introspection)\n                dispatchers[method.url_name].register_method(method, method.external_name)\n            elif isinstance(method, types.ModuleType):\n                try:\n                    _register_rpcmethods([('%s.%s' % (appname, obj))], restrict_introspection, dispatchers)\n                except ImportError:\n                    pass\n    return dispatchers\n", "label": 1}
{"function": "\n\ndef to_internal_value(self, data):\n    request = self.context.get('request', None)\n    try:\n        http_prefix = data.startswith(('http:', 'https:'))\n    except AttributeError:\n        self.fail('incorrect_type', data_type=type(data).__name__)\n    if http_prefix:\n        data = urlparse.urlparse(data).path\n        prefix = get_script_prefix()\n        if data.startswith(prefix):\n            data = ('/' + data[len(prefix):])\n    try:\n        match = resolve(data)\n    except Resolver404:\n        self.fail('no_match')\n    try:\n        expected_viewname = request.versioning_scheme.get_versioned_viewname(self.view_name, request)\n    except AttributeError:\n        expected_viewname = self.view_name\n    if (match.view_name != expected_viewname):\n        self.fail('incorrect_match')\n    try:\n        return self.get_object(match.view_name, match.args, match.kwargs)\n    except (ObjectDoesNotExist, TypeError, ValueError):\n        self.fail('does_not_exist')\n", "label": 1}
{"function": "\n\ndef load_backend(backend_name):\n    try:\n        return import_module(('%s.base' % backend_name))\n    except ImportError as e_user:\n        backend_dir = os.path.join(os.path.dirname(upath(__file__)), 'backends')\n        try:\n            builtin_backends = [name for (_, name, ispkg) in pkgutil.iter_modules([backend_dir]) if (ispkg and (name != 'dummy'))]\n        except EnvironmentError:\n            builtin_backends = []\n        if (backend_name not in [('django.db.backends.%s' % b) for b in builtin_backends]):\n            backend_reprs = map(repr, sorted(builtin_backends))\n            error_msg = (\"%r isn't an available database backend.\\nTry using 'django.db.backends.XXX', where XXX is one of:\\n    %s\\nError was: %s\" % (backend_name, ', '.join(backend_reprs), e_user))\n            raise ImproperlyConfigured(error_msg)\n        else:\n            raise\n", "label": 1}
{"function": "\n\ndef diff(self, other):\n    assert (self.__class__ == other.__class__)\n    if ((self._data is not None) and (self._data == other._data)):\n        return None\n    us = set(self.iterAll())\n    them = set(other.iterAll())\n    added = (us - them)\n    removed = (them - us)\n    if ((not added) and (not removed)):\n        return None\n    l = []\n    if (len(removed) >= (1 << 16)):\n        raise OverflowError\n    if (len(added) >= (1 << 16)):\n        raise OverflowError\n    l.append(struct.pack('!HH', len(removed), len(added)))\n    for (typeId, item) in itertools.chain(removed, added):\n        s = item.freeze()\n        if (len(s) >= (1 << 16)):\n            raise OverflowError\n        l.append(struct.pack('!BH', typeId, len(s)))\n        l.append(s)\n    return ''.join(l)\n", "label": 1}
{"function": "\n\ndef UpdateData(self, event=None, force=False):\n    self.timer_counter += 1\n    if ((self.mca is None) or self.needs_newplot):\n        self.show_mca()\n    self.mca.real_time = self.det.elapsed_real\n    if (force or self.det.needs_refresh):\n        self.det.needs_refresh = False\n        if (self.det_back > 0):\n            if (self.mca2 is None):\n                self.mca2 = self.det.get_mca(mca=self.det_back)\n            counts = self.det.get_array(mca=self.det_back)\n            energy = self.det.get_energy(mca=self.det_back)\n            try:\n                self.update_mca(counts, energy=energy, is_mca2=True, draw=False)\n            except ValueError:\n                pass\n        if (self.mca is None):\n            self.mca = self.det.get_mca(mca=self.det_fore)\n        counts = (self.det.get_array(mca=self.det_fore) * 1.0)\n        energy = self.det.get_energy(mca=self.det_fore)\n        if (max(counts) < 1.0):\n            counts = (0.0001 * np.ones(len(counts)))\n            counts[0] = 2.0\n        self.update_mca(counts, energy=energy)\n", "label": 1}
{"function": "\n\ndef handle_transfer(self, path, type, name=None, contents=None):\n    action = self.__action_for_transfer(path, type, contents)\n    if action.staging_needed:\n        local_action = action.staging_action_local\n        if local_action:\n            response = self.client.put_file(path, type, name=name, contents=contents, action_type=action.action_type)\n\n            def get_path():\n                return response['path']\n        else:\n            job_directory = self.client.job_directory\n            assert job_directory, ('job directory required for action %s' % action)\n            if (not name):\n                name = basename(path)\n            self.__add_remote_staging_input(action, name, type)\n\n            def get_path():\n                return job_directory.calculate_path(name, type)\n        register = (self.rewrite_paths or (type == 'tool'))\n        if register:\n            self.register_rewrite(path, get_path(), type, force=True)\n    elif self.rewrite_paths:\n        path_rewrite = action.path_rewrite(self.path_helper)\n        if path_rewrite:\n            self.register_rewrite(path, path_rewrite, type, force=True)\n", "label": 1}
{"function": "\n\ndef _write_resources(self, f, config=None):\n    '\\n        Write resource requirements (memory, threads, wall time)\\n        to file ``f``\\n\\n        '\n    if (config is None):\n        c = self.config\n    else:\n        c = config\n    if (c.parallel.environment == 'qsub'):\n        conf = c.get_key('parallel.resources.memory', default=False)\n        if conf:\n            mem_gb = (conf / 1000.0)\n            f.write('#$ -l mem_total={:.1f}G\\n'.format(mem_gb))\n        conf = c.get_key('parallel.resources.threads', default=False)\n        if conf:\n            try:\n                penv = c.get_key('parallel.parallel_env')\n            except KeyError:\n                raise KeyError('Must specify parallel_env for threads >1 and qsub.')\n            f.write('#$ -pe {} {}\\n'.format(penv, conf))\n    elif (c.parallel.environment == 'bsub'):\n        conf = c.get_key('parallel.resources.memory', default=False)\n        if conf:\n            f.write('#BSUB -R \"rusage[mem={:.0f}]\"\\n'.format(conf))\n        conf = c.get_key('parallel.resources.threads', default=False)\n        if conf:\n            f.write('#BSUB -n {:.0f}\\n'.format(conf))\n        conf = c.get_key('parallel.resources.wall_time', default=False)\n        if conf:\n            f.write('#BSUB -W {:.0f}\\n'.format(conf))\n", "label": 1}
{"function": "\n\ndef registration_endpoint(self, **kwargs):\n    '\\n\\n        :param request: The request\\n        :param authn: Client authentication information\\n        :param kwargs: extra keyword arguments\\n        :return: A Response instance\\n        '\n    _request = RegistrationRequest().deserialize(kwargs['request'], 'json')\n    try:\n        _request.verify(keyjar=self.keyjar)\n    except InvalidRedirectUri as err:\n        msg = ClientRegistrationError(error='invalid_redirect_uri', error_description=('%s' % err))\n        return BadRequest(msg.to_json(), content='application/json')\n    except (MissingPage, VerificationError) as err:\n        msg = ClientRegistrationError(error='invalid_client_metadata', error_description=('%s' % err))\n        return BadRequest(msg.to_json(), content='application/json')\n    if self.authn_at_registration:\n        try:\n            self.verify_client(kwargs['environ'], _request, self.authn_at_registration)\n        except (AuthnFailure, UnknownAssertionType):\n            return Unauthorized()\n    client_restrictions = {\n        \n    }\n    if ('parsed_software_statement' in _request):\n        for ss in _request['parsed_software_statement']:\n            client_restrictions.update(self.consume_software_statement(ss))\n        del _request['software_statement']\n        del _request['parsed_software_statement']\n    try:\n        client_id = self.create_new_client(_request, client_restrictions)\n    except CapabilitiesMisMatch as err:\n        msg = ClientRegistrationError(error='invalid_client_metadata', error_description=('%s' % err))\n        return BadRequest(msg.to_json(), content='application/json')\n    except RestrictionError as err:\n        msg = ClientRegistrationError(error='invalid_client_metadata', error_description=('%s' % err))\n        return BadRequest(msg.to_json(), content='application/json')\n    return self.client_info(client_id)\n", "label": 1}
{"function": "\n\ndef dispy_send_file(path, timeout=MsgTimeout):\n    \"Computations may use this function to send files back to the client.\\n\\n    If the computations have small amount of data to be sent back to\\n    the client, then the return value can be used for that\\n    purpose. However, if (temporary) result is stored in file(s), then\\n    those file(s) can be sent back to the client.\\n\\n    File at given 'path' is sent to the client, which saves the file\\n    with the same path under its working directory. If multiple jobs\\n    on different nodes send files, care must be taken to use different\\n    paths so files sent by one job don't overwrite files sent by other\\n    jobs.\\n\\n    If file size exceeds 'MaxFileSize' bytes, this function returns -1,\\n    without sending it.\\n\\n    'timeout' is seconds for socket connection/messages; i.e., if\\n    there is no I/O on socket (to client), this call fails. Default\\n    value for it is MsgTimeout (5) seconds.\\n\\n    Return value of 0 indicates successfull transfer.\\n    \"\n    path = os.path.expanduser(path)\n    xf = _XferFile(path, os.stat(path))\n    if (MaxFileSize and (xf.stat_buf.st_size > MaxFileSize)):\n        return (- 1)\n    xf.name = os.path.splitdrive(path)[1]\n    if xf.name.startswith(os.sep):\n        xf.name = xf.name[len(os.sep):]\n    dispy_job_reply = __dispy_job_info.job_reply\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock = AsyncSocket(sock, blocking=True, keyfile=__dispy_job_keyfile, certfile=__dispy_job_certfile)\n    sock.settimeout(timeout)\n    try:\n        sock.connect(__dispy_job_info.reply_addr)\n        sock.send_msg(('FILEXFER:'.encode() + serialize(xf)))\n        sock.send_msg(serialize(dispy_job_reply))\n        recvd = sock.recv_msg()\n        recvd = unserialize(recvd)\n        fd = open(path, 'rb')\n        sent = 0\n        while (sent == recvd):\n            data = fd.read(1024000)\n            if (not data):\n                break\n            sock.sendall(data)\n            sent += len(data)\n            recvd = sock.recv_msg()\n            recvd = unserialize(recvd)\n        fd.close()\n        assert (recvd == xf.stat_buf.st_size)\n    except:\n        print(('Could not transfer file \"%s\": %s' % (path, traceback.format_exc())))\n        return (- 1)\n    else:\n        return 0\n    finally:\n        sock.close()\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    if options.get('param'):\n        for item in options['param']:\n            if ('=' in item):\n                (key, val) = item.split('=')\n            else:\n                (key, val) = (item, True)\n            options[key] = val\n    if (not args):\n        args = []\n    for app in settings.INSTALLED_APPS:\n        if (args and (app not in args)):\n            continue\n        try:\n            app_path = import_module(app).__path__\n        except AttributeError:\n            continue\n        try:\n            imp.find_module('sampledata', app_path)\n        except ImportError:\n            continue\n        module = import_module(('%s.sampledata' % app))\n        if hasattr(module, 'generate_sampledata'):\n            self.stdout.write(('Generating sample data from %s...\\n' % app))\n            module.generate_sampledata(options)\n    self.stdout.write('Done!\\n')\n", "label": 1}
{"function": "\n\ndef retrieve_follow_organization(self, follow_organization_id, voter_id, organization_id):\n    error_result = False\n    exception_does_not_exist = False\n    exception_multiple_object_returned = False\n    follow_organization_on_stage = FollowOrganization()\n    follow_organization_on_stage_id = 0\n    try:\n        if (follow_organization_id > 0):\n            follow_organization_on_stage = FollowOrganization.objects.get(id=follow_organization_id)\n            follow_organization_on_stage_id = organization_id.id\n        elif ((voter_id > 0) and (organization_id > 0)):\n            follow_organization_on_stage = FollowOrganization.objects.get(voter_id=voter_id, organization_id=organization_id)\n            follow_organization_on_stage_id = follow_organization_on_stage.id\n    except FollowOrganization.MultipleObjectsReturned as e:\n        handle_record_found_more_than_one_exception(e, logger=logger)\n        error_result = True\n        exception_multiple_object_returned = True\n    except FollowOrganization.DoesNotExist:\n        error_result = True\n        exception_does_not_exist = True\n    follow_organization_on_stage_found = (True if (follow_organization_on_stage_id > 0) else False)\n    results = {\n        'success': (True if follow_organization_on_stage_found else False),\n        'follow_organization_found': follow_organization_on_stage_found,\n        'follow_organization_id': follow_organization_on_stage_id,\n        'follow_organization': follow_organization_on_stage,\n        'is_following': follow_organization_on_stage.is_following(),\n        'is_not_following': follow_organization_on_stage.is_not_following(),\n        'is_ignoring': follow_organization_on_stage.is_ignoring(),\n        'error_result': error_result,\n        'DoesNotExist': exception_does_not_exist,\n        'MultipleObjectsReturned': exception_multiple_object_returned,\n    }\n    return results\n", "label": 1}
{"function": "\n\ndef update_pick(self, serialnum, objlist, qs, x1, y1, wd, ht, pickobj, msg):\n    if (serialnum != self.get_serial()):\n        return\n    try:\n        image = self.fitsimage.get_image()\n        point = pickobj.objects[1]\n        text = pickobj.objects[2]\n        text.text = self._textlabel\n        if (msg is not None):\n            raise Exception(msg)\n        if self.show_candidates:\n            for obj in objlist:\n                tag = self.canvas.add(self.dc.Point((x1 + obj.objx), (y1 + obj.objy), 5, linewidth=1, color=self.candidate_color), tagpfx='peak')\n        qs.x += x1\n        qs.y += y1\n        qs.objx += x1\n        qs.objy += y1\n        obj_x = qs.objx\n        obj_y = qs.objy\n        fwhm = qs.fwhm\n        (fwhm_x, fwhm_y) = (qs.fwhm_x, qs.fwhm_y)\n        (point.x, point.y) = (obj_x, obj_y)\n        text.color = 'cyan'\n        self.last_rpt = self._make_report(image, qs)\n        d = self.last_rpt\n        if self.do_record:\n            self.add_pick_cb()\n        self.wdetail.fwhm_x.set_text(('%.3f' % fwhm_x))\n        self.wdetail.fwhm_y.set_text(('%.3f' % fwhm_y))\n        self.wdetail.fwhm.set_text(('%.3f' % fwhm))\n        self.wdetail.object_x.set_text(('%.3f' % d.x))\n        self.wdetail.object_y.set_text(('%.3f' % d.y))\n        self.wdetail.sky_level.set_text(('%.3f' % qs.skylevel))\n        self.wdetail.background.set_text(('%.3f' % qs.background))\n        self.wdetail.brightness.set_text(('%.3f' % qs.brightness))\n        self.wdetail.ra.set_text(d.ra_txt)\n        self.wdetail.dec.set_text(d.dec_txt)\n        self.wdetail.equinox.set_text(str(d.equinox))\n        self.wdetail.star_size.set_text(('%.3f' % d.starsize))\n        self.w.btn_sky_cut.set_enabled(True)\n        self.w.btn_bright_cut.set_enabled(True)\n        i1 = (point.x - x1)\n        j1 = (point.y - y1)\n        self.pickcenter.x = i1\n        self.pickcenter.y = j1\n        self.pickcenter.color = 'cyan'\n        self.pick_qs = qs\n        self.pickimage.panset_xy(i1, j1)\n        point.color = 'cyan'\n        self.update_status('Done')\n        self.plot_panx = (float(i1) / wd)\n        self.plot_pany = (float(j1) / ht)\n        if self.have_mpl:\n            self.plot_contours(image)\n            self.plot_fwhm(qs, image)\n            self.plot_radial(qs, image)\n    except Exception as e:\n        errmsg = ('Error calculating quality metrics: %s' % str(e))\n        self.logger.error(errmsg)\n        self.fv.show_error(errmsg, raisetab=False)\n        for key in ('sky_level', 'background', 'brightness', 'star_size', 'fwhm_x', 'fwhm_y'):\n            self.wdetail[key].set_text('')\n        self.wdetail.fwhm.set_text('Failed')\n        self.w.btn_sky_cut.set_enabled(False)\n        self.w.btn_bright_cut.set_enabled(False)\n        self.pick_qs = None\n        text.color = 'red'\n        self.plot_panx = self.plot_pany = 0.5\n    self.w.btn_intr_eval.set_enabled(False)\n    self.pickimage.redraw(whence=3)\n    self.canvas.redraw(whence=3)\n    self.fv.showStatus('Click left mouse button to reposition pick')\n    return True\n", "label": 1}
{"function": "\n\ndef test_easy_file(self):\n    cmd = ('%s %s -r 10 -o %s' % (fq_pgm, self.in_fqfn, self.out_fqfn))\n    r = envoy.run(cmd)\n    p_recs = []\n    for rec in fileinput.input(self.out_fqfn):\n        p_recs.append(rec)\n    fileinput.close()\n    assert (len(p_recs) == 4)\n    assert p_recs[0].strip().startswith('field_0')\n    assert p_recs[0].strip().endswith('10')\n    assert p_recs[1].strip().startswith('field_1')\n    assert p_recs[1].strip().endswith('A')\n    assert p_recs[2].strip().startswith('field_2')\n    assert p_recs[2].strip().endswith('B')\n    assert p_recs[3].strip().startswith('field_3')\n    assert p_recs[3].strip().endswith('C')\n", "label": 1}
{"function": "\n\ndef get(self, author_id):\n    authors = Author.find(author_id, get_current_user())\n    if ((not authors) and (author_id == 'me')):\n        return ('Either there is no current user or you are not in the\\n              author table', 401)\n    elif (not authors):\n        return ('author not found', 404)\n    try:\n        author_email = authors[0].email\n        request = PhabricatorClient()\n        request.connect()\n        user_info = request.call('user.query', {\n            'emails': [author_email],\n        })\n        if (not user_info):\n            return (('phabricator: %s not found' % author_email), 404)\n        author_phid = user_info[0]['phid']\n        diff_info = request.call('differential.query', {\n            'authors': [author_phid],\n            'status': 'status-open',\n        })\n        diff_info.sort(key=(lambda k: ((- 1) * int(k['dateModified']))))\n    except requests.exceptions.ConnectionError:\n        return ('Unable to connect to Phabricator', 503)\n    if (not diff_info):\n        return self.respond([])\n    rows = list(db.session.query(PhabricatorDiff, Build).join(Build, (Build.source_id == PhabricatorDiff.source_id)).filter(PhabricatorDiff.revision_id.in_([d['id'] for d in diff_info])))\n    serialized_builds = zip(self.serialize([row.Build for row in rows]), [row.PhabricatorDiff for row in rows])\n    builds_map = defaultdict(list)\n    for (build, phabricator_diff) in serialized_builds:\n        builds_map[str(phabricator_diff.revision_id)].append(build)\n    for d in diff_info:\n        d['builds'] = builds_map[str(d['id'])]\n    return self.respond(diff_info)\n", "label": 1}
{"function": "\n\ndef reload(self):\n    old_address = self.cfg.address\n    for k in self.cfg.env:\n        if (k in self.cfg.env_orig):\n            os.environ[k] = self.cfg.env_orig[k]\n        else:\n            try:\n                del os.environ[k]\n            except KeyError:\n                pass\n    self.app.reload()\n    self.setup(self.app)\n    self.log.reopen_files()\n    if (old_address != self.cfg.address):\n        [l.close() for l in self.LISTENERS]\n        self.LISTENERS = create_sockets(self.cfg, self.log)\n        self.log.info('Listening at: %s', ','.join(str(self.LISTENERS)))\n    self.cfg.on_reload(self)\n    if (self.pidfile is not None):\n        self.pidfile.unlink()\n    if (self.cfg.pidfile is not None):\n        self.pidfile = Pidfile(self.cfg.pidfile)\n        self.pidfile.create(self.pid)\n    util._setproctitle(('master [%s]' % self.proc_name))\n    for i in range(self.cfg.workers):\n        self.spawn_worker()\n    self.manage_workers()\n", "label": 1}
{"function": "\n\ndef get_ordering_field_columns(self):\n    '\\n        Returns an OrderedDict of ordering field column numbers and asc/desc\\n        '\n    ordering = self._get_default_ordering()\n    ordering_fields = OrderedDict()\n    if (ORDER_VAR not in self.params):\n        for field in ordering:\n            if field.startswith('-'):\n                field = field[1:]\n                order_type = 'desc'\n            else:\n                order_type = 'asc'\n            for (index, attr) in enumerate(self.list_display):\n                if (self.get_ordering_field(attr) == field):\n                    ordering_fields[index] = order_type\n                    break\n    else:\n        for p in self.params[ORDER_VAR].split('.'):\n            (none, pfx, idx) = p.rpartition('-')\n            try:\n                idx = int(idx)\n            except ValueError:\n                continue\n            ordering_fields[idx] = ('desc' if (pfx == '-') else 'asc')\n    return ordering_fields\n", "label": 1}
{"function": "\n\ndef fetch_info(log, items, write):\n    'Get data from AcousticBrainz for the items.\\n    '\n\n    def get_value(*map_path):\n        try:\n            return reduce(operator.getitem, map_path, data)\n        except KeyError:\n            log.debug('Invalid Path: {}', map_path)\n    for item in items:\n        if item.mb_trackid:\n            log.info('getting data for: {}', item)\n            urls = [generate_url(item.mb_trackid, path) for path in LEVELS]\n            log.debug('fetching URLs: {}', ' '.join(urls))\n            try:\n                res = [requests.get(url) for url in urls]\n            except requests.RequestException as exc:\n                log.info('request error: {}', exc)\n                continue\n            if any(((r.status_code == 404) for r in res)):\n                log.info('recording ID {} not found', item.mb_trackid)\n                continue\n            try:\n                data = res[0].json()\n                data.update(res[1].json())\n            except ValueError:\n                log.debug('Invalid Response: {} & {}', [r.text for r in res])\n            item.danceable = get_value('highlevel', 'danceability', 'all', 'danceable')\n            item.gender = get_value('highlevel', 'gender', 'value')\n            item.genre_rosamerica = get_value('highlevel', 'genre_rosamerica', 'value')\n            item.mood_acoustic = get_value('highlevel', 'mood_acoustic', 'all', 'acoustic')\n            item.mood_aggressive = get_value('highlevel', 'mood_aggressive', 'all', 'aggressive')\n            item.mood_electronic = get_value('highlevel', 'mood_electronic', 'all', 'electronic')\n            item.mood_happy = get_value('highlevel', 'mood_happy', 'all', 'happy')\n            item.mood_party = get_value('highlevel', 'mood_party', 'all', 'party')\n            item.mood_relaxed = get_value('highlevel', 'mood_relaxed', 'all', 'relaxed')\n            item.mood_sad = get_value('highlevel', 'mood_sad', 'all', 'sad')\n            item.rhythm = get_value('highlevel', 'ismir04_rhythm', 'value')\n            item.tonal = get_value('highlevel', 'tonal_atonal', 'all', 'tonal')\n            item.voice_instrumental = get_value('highlevel', 'voice_instrumental', 'value')\n            item.average_loudness = get_value('lowlevel', 'average_loudness')\n            item.chords_changes_rate = get_value('tonal', 'chords_changes_rate')\n            item.chords_key = get_value('tonal', 'chords_key')\n            item.chords_number_rate = get_value('tonal', 'chords_number_rate')\n            item.chords_scale = get_value('tonal', 'chords_scale')\n            item.initial_key = '{} {}'.format(get_value('tonal', 'key_key'), get_value('tonal', 'key_scale'))\n            item.key_strength = get_value('tonal', 'key_strength')\n            item.store()\n            if write:\n                item.try_write()\n", "label": 1}
{"function": "\n\ndef test_getitem_slice():\n    random.seed(0)\n    slt = SortedList(load=17)\n    lst = list()\n    for rpt in range(100):\n        val = random.random()\n        slt.add(val)\n        lst.append(val)\n    lst.sort()\n    assert all(((slt[start:] == lst[start:]) for start in [(- 75), (- 25), 0, 25, 75]))\n    assert all(((slt[:stop] == lst[:stop]) for stop in [(- 75), (- 25), 0, 25, 75]))\n    assert all(((slt[::step] == lst[::step]) for step in [(- 5), (- 1), 1, 5]))\n    assert all(((slt[start:stop] == lst[start:stop]) for start in [(- 75), (- 25), 0, 25, 75] for stop in [(- 75), (- 25), 0, 25, 75]))\n    assert all(((slt[:stop:step] == lst[:stop:step]) for stop in [(- 75), (- 25), 0, 25, 75] for step in [(- 5), (- 1), 1, 5]))\n    assert all(((slt[start::step] == lst[start::step]) for start in [(- 75), (- 25), 0, 25, 75] for step in [(- 5), (- 1), 1, 5]))\n    assert all(((slt[start:stop:step] == lst[start:stop:step]) for start in [(- 75), (- 25), 0, 25, 75] for stop in [(- 75), (- 25), 0, 25, 75] for step in [(- 5), (- 1), 1, 5]))\n", "label": 1}
{"function": "\n\ndef __init__(self, device, args):\n    super(MTDMotionEventProvider, self).__init__(device, args)\n    self._device = None\n    self.input_fn = None\n    self.default_ranges = dict()\n    args = args.split(',')\n    if (not args):\n        Logger.error('MTD: No filename pass to MTD configuration')\n        Logger.error('MTD: Use /dev/input/event0 for example')\n        return None\n    self.input_fn = args[0]\n    Logger.info(('MTD: Read event from <%s>' % self.input_fn))\n    for arg in args[1:]:\n        if (arg == ''):\n            continue\n        arg = arg.split('=')\n        if (len(arg) != 2):\n            err = ('MTD: Bad parameter %s: Not in key=value format' % arg)\n            Logger.error(err)\n            continue\n        (key, value) = arg\n        if (key not in MTDMotionEventProvider.options):\n            Logger.error(('MTD: unknown %s option' % key))\n            continue\n        try:\n            self.default_ranges[key] = int(value)\n        except ValueError:\n            err = ('MTD: invalid value %s for option %s' % (key, value))\n            Logger.error(err)\n            continue\n        Logger.info(('MTD: Set custom %s to %d' % (key, int(value))))\n    if ('rotation' not in self.default_ranges):\n        self.default_ranges['rotation'] = 0\n    elif (self.default_ranges['rotation'] not in (0, 90, 180, 270)):\n        Logger.error('HIDInput: invalid rotation value ({})'.format(self.default_ranges['rotation']))\n        self.default_ranges['rotation'] = 0\n", "label": 1}
{"function": "\n\ndef run(self, edit):\n\n    def update_ordered_list(lines):\n        new_lines = []\n        next_num = None\n        kind = (lambda a: a)\n        for line in lines:\n            match = ORDER_LIST_PATTERN.match(line)\n            if (not match):\n                new_lines.append(line)\n                continue\n            new_line = (((match.group(1) + (kind(next_num) or match.group(2))) + match.group(3)) + match.group(4))\n            new_lines.append(new_line)\n            if (not next_num):\n                try:\n                    next_num = int(match.group(2))\n                    kind = str\n                except ValueError:\n                    next_num = ord(match.group(2))\n                    kind = chr\n            next_num += 1\n        return new_lines\n\n    def update_roman_list(lines):\n        new_lines = []\n        next_num = None\n        kind = (lambda a: a)\n        for line in lines:\n            match = ROMAN_PATTERN.match(line)\n            if (not match):\n                new_lines.append(line)\n                continue\n            new_line = (((match.group(1) + (kind(next_num) or match.group(2))) + match.group(3)) + match.group(4))\n            new_lines.append(new_line)\n            if (not next_num):\n                actual = match.group(2)\n                next_num = from_roman(actual.upper())\n                if (actual == actual.lower()):\n                    kind = (lambda a: to_roman(a).lower())\n                else:\n                    kind = to_roman\n            next_num += 1\n        return new_lines\n    for region in self.view.sel():\n        line_region = self.view.line(region)\n        before_point_region = sublime.Region(line_region.a, region.a)\n        before_point_content = self.view.substr(before_point_region)\n        folded = False\n        for i in self.view.folded_regions():\n            if i.contains(before_point_region):\n                self.view.insert(edit, region.a, '\\n')\n                folded = True\n        if folded:\n            break\n        match = EMPTY_LIST_PATTERN.match(before_point_content)\n        if match:\n            insert_text = ((match.group(1) + re.sub('\\\\S', ' ', str(match.group(2)))) + match.group(3))\n            self.view.erase(edit, before_point_region)\n            self.view.insert(edit, line_region.a, insert_text)\n            break\n        match = ROMAN_PATTERN.match(before_point_content)\n        if match:\n            actual = match.group(2)\n            next_num = to_roman((from_roman(actual.upper()) + 1))\n            if (actual == actual.lower()):\n                next_num = next_num.lower()\n            insert_text = ((match.group(1) + next_num) + match.group(3))\n            self.view.insert(edit, region.a, ('\\n' + insert_text))\n            pos = self.view.sel()[0].a\n            (region, lines, indent) = self.get_block_bounds()\n            new_list = update_roman_list(lines)\n            self.view.replace(edit, region, ('\\n'.join(new_list) + '\\n'))\n            self.view.sel().clear()\n            self.view.sel().add(sublime.Region(pos, pos))\n            self.view.show(pos)\n            break\n        match = ORDER_LIST_PATTERN.match(before_point_content)\n        if match:\n            try:\n                next_num = str((int(match.group(2)) + 1))\n            except ValueError:\n                next_num = chr((ord(match.group(2)) + 1))\n            insert_text = ((match.group(1) + next_num) + match.group(3))\n            self.view.insert(edit, region.a, ('\\n' + insert_text))\n            pos = self.view.sel()[0].a\n            (region, lines, indent) = self.get_block_bounds()\n            new_list = update_ordered_list(lines)\n            self.view.replace(edit, region, ('\\n'.join(new_list) + '\\n'))\n            self.view.sel().clear()\n            self.view.sel().add(sublime.Region(pos, pos))\n            self.view.show(pos)\n            break\n        match = UNORDER_LIST_PATTERN.match(before_point_content)\n        if match:\n            insert_text = (match.group(1) + match.group(2))\n            self.view.insert(edit, region.a, ('\\n' + insert_text))\n            break\n        match = NONLIST_PATTERN.match(before_point_content)\n        if match:\n            insert_text = (match.group(1) + match.group(2))\n            self.view.insert(edit, region.a, ('\\n' + insert_text))\n            break\n        self.view.insert(edit, region.a, ('\\n' + re.sub('\\\\S+\\\\s*', '', before_point_content)))\n    self.adjust_view()\n", "label": 1}
{"function": "\n\ndef reap_workers(self):\n    '        Reap workers to avoid zombie processes\\n        '\n    try:\n        while True:\n            (wpid, status) = os.waitpid((- 1), os.WNOHANG)\n            if (not wpid):\n                break\n            if (self.reexec_pid == wpid):\n                self.reexec_pid = 0\n            else:\n                exitcode = (status >> 8)\n                if (exitcode == self.WORKER_BOOT_ERROR):\n                    reason = 'Worker failed to boot.'\n                    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n                if (exitcode == self.APP_LOAD_ERROR):\n                    reason = 'App failed to load.'\n                    raise HaltServer(reason, self.APP_LOAD_ERROR)\n                worker = self.WORKERS.pop(wpid, None)\n                if (not worker):\n                    continue\n                worker.tmp.close()\n    except OSError as e:\n        if (e.errno != errno.ECHILD):\n            raise\n", "label": 1}
{"function": "\n\ndef __negotiatesocks4(self, destaddr, destport):\n    '__negotiatesocks4(self,destaddr,destport)\\n        Negotiates a connection through a SOCKS4 server.\\n        '\n    rmtrslv = False\n    try:\n        ipaddr = socket.inet_aton(destaddr)\n    except socket.error:\n        if self.__proxy[3]:\n            ipaddr = struct.pack('BBBB', 0, 0, 0, 1)\n            rmtrslv = True\n        else:\n            ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))\n    req = (struct.pack('>BBH', 4, 1, destport) + ipaddr)\n    if (self.__proxy[4] != None):\n        req = (req + self.__proxy[4])\n    req = (req + chr(0).encode())\n    if rmtrslv:\n        req = ((req + destaddr) + chr(0).encode())\n    self.sendall(req)\n    resp = self.__recvall(8)\n    if (resp[0:1] != chr(0).encode()):\n        self.close()\n        raise GeneralProxyError((1, _generalerrors[1]))\n    if (resp[1:2] != chr(90).encode()):\n        self.close()\n        if (ord(resp[1:2]) in (91, 92, 93)):\n            self.close()\n            raise Socks4Error((ord(resp[1:2]), _socks4errors[(ord(resp[1:2]) - 90)]))\n        else:\n            raise Socks4Error((94, _socks4errors[4]))\n    self.__proxysockname = (socket.inet_ntoa(resp[4:]), struct.unpack('>H', resp[2:4])[0])\n    if (rmtrslv != None):\n        self.__proxypeername = (socket.inet_ntoa(ipaddr), destport)\n    else:\n        self.__proxypeername = (destaddr, destport)\n", "label": 1}
{"function": "\n\ndef _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):\n    '\\n        Returns context_lines before and after lineno from file.\\n        Returns (pre_context_lineno, pre_context, context_line, post_context).\\n        '\n    source = None\n    if ((loader is not None) and hasattr(loader, 'get_source')):\n        try:\n            source = loader.get_source(module_name)\n        except ImportError:\n            pass\n        if (source is not None):\n            source = source.splitlines()\n    if (source is None):\n        try:\n            with open(filename, 'rb') as fp:\n                source = fp.read().splitlines()\n        except (OSError, IOError):\n            pass\n    if (source is None):\n        return (None, [], None, [])\n    if isinstance(source[0], six.binary_type):\n        encoding = 'ascii'\n        for line in source[:2]:\n            match = re.search(b'coding[:=]\\\\s*([-\\\\w.]+)', line)\n            if match:\n                encoding = match.group(1).decode('ascii')\n                break\n        source = [six.text_type(sline, encoding, 'replace') for sline in source]\n    lower_bound = max(0, (lineno - context_lines))\n    upper_bound = (lineno + context_lines)\n    pre_context = source[lower_bound:lineno]\n    context_line = source[lineno]\n    post_context = source[(lineno + 1):upper_bound]\n    return (lower_bound, pre_context, context_line, post_context)\n", "label": 1}
{"function": "\n\ndef test_block_locator_hashes():\n    for size in [100, 500, 720, 1000]:\n        headers = make_headers(size)\n        nodes = [(i, headers[i].hash(), i) for i in range(size)]\n        bcv = BlockChainView(nodes)\n        blh = bcv.block_locator_hashes()\n        tuples = [bcv.tuple_for_hash(h) for h in blh]\n        indices = [t[0] for t in tuples]\n        indices.sort()\n        assert (indices == sorted(bcv.key_index_generator()))\n        bcv.winnow()\n        blh = bcv.block_locator_hashes()\n        tuples = [bcv.tuple_for_hash(h) for h in blh]\n        indices = [t[0] for t in tuples]\n        indices.sort()\n        assert (indices == list((t[0] for t in bcv.node_tuples)))\n", "label": 1}
{"function": "\n\ndef _apply(self, data):\n    '\\n        Update an instance from supplied data.\\n\\n        All fields marked required=True MUST be provided.\\n        All fields omitted will have their default used, if provided.\\n        '\n    errors = defaultdict(list)\n    for name in self._field_names:\n        required = getattr(self._fields[name], 'required', True)\n        default = getattr(self._fields[name], 'default', NOT_PROVIDED)\n        try:\n            value = data[name]\n        except KeyError:\n            if required:\n                if (default is NOT_PROVIDED):\n                    errors[name].append(ValidationError('This field is required'))\n                    continue\n            elif (default is NOT_PROVIDED):\n                continue\n            value = default\n            if callable(value):\n                value = value()\n        try:\n            setattr(self, name, value)\n        except ValidationError as e:\n            errors[name].append(e)\n    self._errors = dict(errors)\n    self._clean(data, full=True)\n    if self._errors:\n        raise ValidationError(self._errors)\n    return self._obj\n", "label": 1}
{"function": "\n\ndef depart_code_latex(self, node):\n    'Some changes to code blocks.\\n\\n    * Remove the frame (by changing Verbatim -> OriginalVerbatim)\\n    * Add empty lines before and after the code\\n    * Add prompt to the first line, empty space to the following lines\\n\\n    '\n    lines = self.body[(- 1)].split('\\n')\n    out = []\n    assert (lines[0] == '')\n    out.append(lines[0])\n    assert lines[1].startswith('\\\\begin{Verbatim}')\n    out.append(lines[1].replace('Verbatim', 'OriginalVerbatim'))\n    code_lines = ((([''] * node.get('empty-lines-before', 0)) + lines[2:(- 2)]) + ([''] * node.get('empty-lines-after', 0)))\n    prompt = node.get('latex_prompt')\n    color = ('nbsphinxin' if prompt.startswith('In') else 'nbsphinxout')\n    prefix = ((((('\\\\textcolor{' + color) + '}{') + prompt) + '}') if prompt else '')\n    for line in code_lines[:1]:\n        out.append((prefix + line))\n    prefix = (' ' * len(prompt))\n    for line in code_lines[1:]:\n        out.append((prefix + line))\n    assert lines[(- 2)].startswith('\\\\end{Verbatim}')\n    out.append(lines[(- 2)].replace('Verbatim', 'OriginalVerbatim'))\n    assert (lines[(- 1)] == '')\n    out.append(lines[(- 1)])\n    self.body[(- 1)] = '\\n'.join(out)\n", "label": 1}
{"function": "\n\ndef test_base():\n\n    class ExampleWorker(BaseWorker):\n        qinput = Queue('test_queue')\n        qoutput = Queue('test_output_queue')\n\n        def run(self, job):\n            for x in range(3):\n                (yield (str(job) * (x + 1)))\n    w = ExampleWorker()\n    qi = w.qinput\n    qo = w.qoutput\n    qi.clear()\n    qo.clear()\n    test_jobs = ['foo', 'bar', 'baz']\n    for x in test_jobs:\n        qi.send(x)\n    jobs_run = []\n    for job in w:\n        if (job is None):\n            break\n        jobs_run.append(job)\n    assert (jobs_run == test_jobs)\n    assert (len(qo) == (3 * len(test_jobs)))\n    assert ('foo' in qo)\n    assert ('foofoo' in qo)\n    assert ('bar' in qo)\n    assert ('bazbazbaz' in qo)\n    assert (len(qo) == 0)\n", "label": 1}
{"function": "\n\ndef train_loop():\n    graph_generated = False\n    while True:\n        while data_q.empty():\n            time.sleep(0.1)\n        inp = data_q.get()\n        if (inp == 'end'):\n            res_q.put('end')\n            break\n        elif (inp == 'train'):\n            res_q.put('train')\n            model.train = True\n            continue\n        elif (inp == 'val'):\n            res_q.put('val')\n            serializers.save_hdf5(args.out, model)\n            serializers.save_hdf5(args.outstate, optimizer)\n            model.train = False\n            continue\n        volatile = ('off' if model.train else 'on')\n        x = chainer.Variable(xp.asarray(inp[0]), volatile=volatile)\n        t = chainer.Variable(xp.asarray(inp[1]), volatile=volatile)\n        if model.train:\n            optimizer.update(model, x, t)\n            if (not graph_generated):\n                with open('graph.dot', 'w') as o:\n                    o.write(computational_graph.build_computational_graph((model.loss,)).dump())\n                print('generated graph', file=sys.stderr)\n                graph_generated = True\n        else:\n            model(x, t)\n        res_q.put((float(model.loss.data), float(model.accuracy.data)))\n        del x, t\n", "label": 1}
{"function": "\n\ndef login(self, request):\n    '\\n        Displays the login form for the given HttpRequest.\\n        '\n    from django.contrib.auth.models import User\n    if (not request.POST.has_key(LOGIN_FORM_KEY)):\n        if request.POST:\n            message = _('Please log in again, because your session has expired.')\n        else:\n            message = ''\n        return self.display_login_form(request, message)\n    if (not request.session.test_cookie_worked()):\n        message = _(\"Looks like your browser isn't configured to accept cookies. Please enable cookies, reload this page, and try again.\")\n        return self.display_login_form(request, message)\n    else:\n        request.session.delete_test_cookie()\n    username = request.POST.get('username', None)\n    password = request.POST.get('password', None)\n    user = authenticate(username=username, password=password)\n    if (user is None):\n        message = ERROR_MESSAGE\n        if ((username is not None) and ('@' in username)):\n            try:\n                user = User.objects.get(email=username)\n            except (User.DoesNotExist, User.MultipleObjectsReturned):\n                pass\n            else:\n                if user.check_password(password):\n                    message = (_(\"Your e-mail address is not your username. Try '%s' instead.\") % user.username)\n        return self.display_login_form(request, message)\n    elif (user.is_active and user.is_staff):\n        login(request, user)\n        return http.HttpResponseRedirect(request.get_full_path())\n    else:\n        return self.display_login_form(request, ERROR_MESSAGE)\n", "label": 1}
{"function": "\n\ndef transform(self, data):\n    transforms = []\n    in_fenced_code_block = False\n    linenum = 0\n    for line in data:\n        if fencedcodere.search(line):\n            if in_fenced_code_block:\n                in_fenced_code_block = False\n            else:\n                in_fenced_code_block = True\n        if ((not in_fenced_code_block) and (not codere.search(line))):\n            match = youtube_url_re.search(line)\n            if match:\n                url = match.group(1)\n                image_url = ('http://img.youtube.com/vi/%s/0.jpg' % url)\n                video_url = ('http://www.youtube.com/watch?v=%s' % url)\n                processed_image_dir = os.path.join('images', 'youtube')\n                processed_image_path = os.path.join(processed_image_dir, ('%s.png' % url))\n                if (not os.path.isfile(processed_image_path)):\n                    try:\n                        import Image\n                        from tempfile import NamedTemporaryFile\n                        if (not os.path.exists(processed_image_dir)):\n                            os.makedirs(processed_image_dir)\n                        screenshot_img = NamedTemporaryFile(suffix='.jpg')\n                        button_img = NamedTemporaryFile(suffix='.png')\n                        urllib.urlretrieve(image_url, screenshot_img.name)\n                        urllib.urlretrieve(play_button_url, button_img.name)\n                        background = Image.open(screenshot_img.name)\n                        foreground = Image.open(button_img.name)\n                        background.paste(foreground, (90, 65), foreground)\n                        background.save(processed_image_path)\n                    except Exception as e:\n                        print(('Unable to add play button to YouTube screenshot (%s). Using the screenshot on its own instead.' % e))\n                image_link = ('[![Link to Youtube video](%s)](%s)\\n' % (processed_image_path, video_url))\n                transforms.append(Transform(linenum, 'swap', image_link))\n        linenum += 1\n    return transforms\n", "label": 1}
{"function": "\n\ndef run_typed_fn(fn, args, backend=None):\n    actual_types = tuple((type_conv.typeof(arg) for arg in args))\n    expected_types = fn.input_types\n    assert (actual_types == expected_types), ('Arg type mismatch, expected %s but got %s' % (expected_types, actual_types))\n    if (backend is None):\n        backend = config.backend\n    if (backend == 'c'):\n        return c_backend.run(fn, args)\n    elif (backend == 'openmp'):\n        return openmp_backend.run(fn, args)\n    elif (backend == 'cuda'):\n        from .. import cuda_backend\n        return cuda_backend.run(fn, args)\n    elif (backend == 'llvm'):\n        from ..llvm_backend.llvm_context import global_context\n        from ..llvm_backend import generic_value_to_python\n        from ..llvm_backend import ctypes_to_generic_value, compile_fn\n        lowered_fn = pipeline.lowering.apply(fn)\n        llvm_fn = compile_fn(lowered_fn).llvm_fn\n        ctypes_inputs = [t.from_python(v) for (v, t) in zip(args, expected_types)]\n        gv_inputs = [ctypes_to_generic_value(cv, t) for (cv, t) in zip(ctypes_inputs, expected_types)]\n        exec_engine = global_context.exec_engine\n        gv_return = exec_engine.run_function(llvm_fn, gv_inputs)\n        return generic_value_to_python(gv_return, fn.return_type)\n    elif (backend == 'interp'):\n        from .. import interp\n        fn = pipeline.loopify(fn)\n        return interp.eval_fn(fn, args)\n    else:\n        assert False, ('Unknown backend %s' % backend)\n", "label": 1}
{"function": "\n\ndef create(self, req, body):\n    'Creates a new volume type.'\n    context = req.environ['nova.context']\n    if ((not body) or (body == '')):\n        return faults.Fault(exc.HTTPUnprocessableEntity())\n    vol_type = body.get('volume_type', None)\n    if ((vol_type is None) or (vol_type == '')):\n        return faults.Fault(exc.HTTPUnprocessableEntity())\n    name = vol_type.get('name', None)\n    specs = vol_type.get('extra_specs', {\n        \n    })\n    if ((name is None) or (name == '')):\n        return faults.Fault(exc.HTTPUnprocessableEntity())\n    try:\n        volume_types.create(context, name, specs)\n        vol_type = volume_types.get_volume_type_by_name(context, name)\n    except quota.QuotaError as error:\n        self._handle_quota_error(error)\n    except exception.NotFound:\n        return faults.Fault(exc.HTTPNotFound())\n    return {\n        'volume_type': vol_type,\n    }\n", "label": 1}
{"function": "\n\ndef test_raise_statement_regular_expression():\n    candidates_ok = [\"some text # raise Exception, 'text'\", \"raise ValueError('text') # raise Exception, 'text'\", \"raise ValueError('text')\", 'raise ValueError', \"raise ValueError('text')\", \"raise ValueError('text') #,\", '\\'\"\"\"This function will raise ValueError, except when it doesn\\'t\"\"\"', \"raise (ValueError('text')\"]\n    str_candidates_fail = [\"raise 'exception'\", \"raise 'Exception'\", 'raise \"exception\"', 'raise \"Exception\"', \"raise 'ValueError'\"]\n    gen_candidates_fail = [\"raise Exception('text') # raise Exception, 'text'\", \"raise Exception('text')\", 'raise Exception', \"raise Exception('text')\", \"raise Exception('text') #,\", \"raise Exception, 'text'\", \"raise Exception, 'text' # raise Exception('text')\", \"raise Exception, 'text' # raise Exception, 'text'\", \">>> raise Exception, 'text'\", \">>> raise Exception, 'text' # raise Exception('text')\", \">>> raise Exception, 'text' # raise Exception, 'text'\"]\n    old_candidates_fail = [\"raise Exception, 'text'\", \"raise Exception, 'text' # raise Exception('text')\", \"raise Exception, 'text' # raise Exception, 'text'\", \">>> raise Exception, 'text'\", \">>> raise Exception, 'text' # raise Exception('text')\", \">>> raise Exception, 'text' # raise Exception, 'text'\", \"raise ValueError, 'text'\", \"raise ValueError, 'text' # raise Exception('text')\", \"raise ValueError, 'text' # raise Exception, 'text'\", \">>> raise ValueError, 'text'\", \">>> raise ValueError, 'text' # raise Exception('text')\", \">>> raise ValueError, 'text' # raise Exception, 'text'\", 'raise(ValueError,', 'raise (ValueError,', 'raise( ValueError,', 'raise ( ValueError,', 'raise(ValueError ,', 'raise (ValueError ,', 'raise( ValueError ,', 'raise ( ValueError ,']\n    for c in candidates_ok:\n        assert (str_raise_re.search(_with_space(c)) is None), c\n        assert (gen_raise_re.search(_with_space(c)) is None), c\n        assert (old_raise_re.search(_with_space(c)) is None), c\n    for c in str_candidates_fail:\n        assert (str_raise_re.search(_with_space(c)) is not None), c\n    for c in gen_candidates_fail:\n        assert (gen_raise_re.search(_with_space(c)) is not None), c\n    for c in old_candidates_fail:\n        assert (old_raise_re.search(_with_space(c)) is not None), c\n", "label": 1}
{"function": "\n\ndef str_extractall(arr, pat, flags=0):\n    '\\n    For each subject string in the Series, extract groups from all\\n    matches of regular expression pat. When each subject string in the\\n    Series has exactly one match, extractall(pat).xs(0, level=\\'match\\')\\n    is the same as extract(pat).\\n\\n    .. versionadded:: 0.18.0\\n\\n    Parameters\\n    ----------\\n    pat : string\\n        Regular expression pattern with capturing groups\\n    flags : int, default 0 (no flags)\\n        re module flags, e.g. re.IGNORECASE\\n\\n    Returns\\n    -------\\n    A DataFrame with one row for each match, and one column for each\\n    group. Its rows have a MultiIndex with first levels that come from\\n    the subject Series. The last level is named \\'match\\' and indicates\\n    the order in the subject. Any capture group names in regular\\n    expression pat will be used for column names; otherwise capture\\n    group numbers will be used.\\n\\n    See Also\\n    --------\\n    extract : returns first match only (not all matches)\\n\\n    Examples\\n    --------\\n    A pattern with one group will return a DataFrame with one column.\\n    Indices with no matches will not appear in the result.\\n\\n    >>> s = Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\\n    >>> s.str.extractall(\"[ab](\\\\d)\")\\n             0\\n      match\\n    A 0      1\\n      1      2\\n    B 0      1\\n\\n    Capture group names are used for column names of the result.\\n\\n    >>> s.str.extractall(\"[ab](?P<digit>\\\\d)\")\\n            digit\\n      match\\n    A 0         1\\n      1         2\\n    B 0         1\\n\\n    A pattern with two groups will return a DataFrame with two columns.\\n\\n    >>> s.str.extractall(\"(?P<letter>[ab])(?P<digit>\\\\d)\")\\n            letter digit\\n      match\\n    A 0          a     1\\n      1          a     2\\n    B 0          b     1\\n\\n    Optional groups that do not match are NaN in the result.\\n\\n    >>> s.str.extractall(\"(?P<letter>[ab])?(?P<digit>\\\\d)\")\\n            letter digit\\n      match\\n    A 0          a     1\\n      1          a     2\\n    B 0          b     1\\n    C 0        NaN     1\\n\\n    '\n    from pandas import DataFrame, MultiIndex\n    regex = re.compile(pat, flags=flags)\n    if (regex.groups == 0):\n        raise ValueError('pattern contains no capture groups')\n    names = dict(zip(regex.groupindex.values(), regex.groupindex.keys()))\n    columns = [names.get((1 + i), i) for i in range(regex.groups)]\n    match_list = []\n    index_list = []\n    for (subject_key, subject) in arr.iteritems():\n        if isinstance(subject, compat.string_types):\n            try:\n                key_list = list(subject_key)\n            except TypeError:\n                key_list = [subject_key]\n            for (match_i, match_tuple) in enumerate(regex.findall(subject)):\n                na_tuple = [(np.NaN if (group == '') else group) for group in match_tuple]\n                match_list.append(na_tuple)\n                result_key = tuple((key_list + [match_i]))\n                index_list.append(result_key)\n    if (0 < len(index_list)):\n        index = MultiIndex.from_tuples(index_list, names=(arr.index.names + ['match']))\n    else:\n        index = None\n    result = DataFrame(match_list, index, columns)\n    return result\n", "label": 1}
{"function": "\n\ndef handle_noargs(self, **options):\n    from django.db.models.loading import get_models\n    loaded_models = get_models()\n    use_plain = options.get('plain', False)\n    try:\n        if use_plain:\n            raise ImportError\n        self.run_shell()\n    except ImportError:\n        import code\n        imported_objects = {\n            \n        }\n        try:\n            import readline\n        except ImportError:\n            pass\n        else:\n            import rlcompleter\n            readline.set_completer(rlcompleter.Completer(imported_objects).complete)\n            readline.parse_and_bind('tab:complete')\n        if (not use_plain):\n            pythonrc = os.environ.get('PYTHONSTARTUP')\n            if (pythonrc and os.path.isfile(pythonrc)):\n                try:\n                    execfile(pythonrc)\n                except NameError:\n                    pass\n            import user\n        code.interact(local=imported_objects)\n", "label": 1}
{"function": "\n\n@expose(hide=True)\ndef debug_php7(self):\n    'Start/Stop PHP debug'\n    if ((self.app.pargs.php7 == 'on') and (not self.app.pargs.site_name)):\n        if (EEVariables.ee_platform_codename != 'trusty'):\n            Log.error(self, 'PHP 7.0 not supported.')\n        if (not EEShellExec.cmd_exec(self, 'sed -n \"/upstream php7{/,/}/p \" /etc/nginx/conf.d/upstream.conf | grep 9170')):\n            Log.info(self, 'Enabling PHP 7.0 debug')\n            nc = NginxConfig()\n            nc.loadf('/etc/nginx/conf.d/upstream.conf')\n            nc.set([('upstream', 'php'), 'server'], '127.0.0.1:9170')\n            if os.path.isfile('/etc/nginx/common/wpfc-hhvm.conf'):\n                nc.set([('upstream', 'hhvm'), 'server'], '127.0.0.1:9170')\n            nc.savef('/etc/nginx/conf.d/upstream.conf')\n            EEFileUtils.searchreplace(self, '/etc/php/7.0/mods-available/xdebug.ini', ';zend_extension', 'zend_extension')\n            config = configparser.ConfigParser()\n            config.read('/etc/php/7.0/fpm/pool.d/debug.conf')\n            config['debug']['slowlog'] = '/var/log/php/7.0/slow.log'\n            config['debug']['request_slowlog_timeout'] = '10s'\n            with open('/etc/php/7.0/fpm/pool.d/debug.conf', encoding='utf-8', mode='w') as confifile:\n                Log.debug(self, 'Writting debug.conf configuration into /etc/php/7.0/fpm/pool.d/debug.conf')\n                config.write(confifile)\n            self.trigger_php = True\n            self.trigger_nginx = True\n        else:\n            Log.info(self, 'PHP debug is already enabled')\n        self.msg = (self.msg + ['/var/log/php/7.0/slow.log'])\n    elif ((self.app.pargs.php7 == 'off') and (not self.app.pargs.site_name)):\n        if EEShellExec.cmd_exec(self, ' sed -n \"/upstream php {/,/}/p\" /etc/nginx/conf.d/upstream.conf | grep 9170'):\n            Log.info(self, 'Disabling PHP 7.0 debug')\n            nc = NginxConfig()\n            nc.loadf('/etc/nginx/conf.d/upstream.conf')\n            nc.set([('upstream', 'php'), 'server'], '127.0.0.1:9070')\n            if os.path.isfile('/etc/nginx/common/wpfc-hhvm.conf'):\n                nc.set([('upstream', 'hhvm'), 'server'], '127.0.0.1:8000')\n            nc.savef('/etc/nginx/conf.d/upstream.conf')\n            EEFileUtils.searchreplace(self, '/etc/php/7.0/mods-available/xdebug.ini', 'zend_extension', ';zend_extension')\n            self.trigger_php = True\n            self.trigger_nginx = True\n        else:\n            Log.info(self, 'PHP 7.0 debug is already disabled')\n", "label": 1}
{"function": "\n\ndef __init__(self, device, args):\n    super(HIDInputMotionEventProvider, self).__init__(device, args)\n    global Window, Keyboard\n    if (Window is None):\n        from kivy.core.window import Window\n    if (Keyboard is None):\n        from kivy.core.window import Keyboard\n    self.input_fn = None\n    self.default_ranges = dict()\n    args = args.split(',')\n    if (not args):\n        Logger.error('HIDInput: Filename missing in configuration')\n        Logger.error('HIDInput: Use /dev/input/event0 for example')\n        return None\n    self.input_fn = args[0]\n    Logger.info(('HIDInput: Read event from <%s>' % self.input_fn))\n    for arg in args[1:]:\n        if (arg == ''):\n            continue\n        arg = arg.split('=')\n        if (len(arg) != 2):\n            Logger.error(('HIDInput: invalid parameter %s, not in key=value format.' % arg))\n            continue\n        (key, value) = arg\n        if (key not in HIDInputMotionEventProvider.options):\n            Logger.error(('HIDInput: unknown %s option' % key))\n            continue\n        try:\n            self.default_ranges[key] = int(value)\n        except ValueError:\n            err = ('HIDInput: invalid value \"%s\" for \"%s\"' % (key, value))\n            Logger.error(err)\n            continue\n        Logger.info(('HIDInput: Set custom %s to %d' % (key, int(value))))\n    if ('rotation' not in self.default_ranges):\n        self.default_ranges['rotation'] = 0\n    elif (self.default_ranges['rotation'] not in (0, 90, 180, 270)):\n        Logger.error('HIDInput: invalid rotation value ({})'.format(self.default_ranges['rotation']))\n        self.default_ranges['rotation'] = 0\n", "label": 1}
{"function": "\n\ndef inner_run(self, *args, **options):\n    key_file = options.get('key')\n    cert_file = options.get('certificate')\n    self.check_certs(key_file, cert_file)\n    from django.conf import settings\n    from django.utils import translation\n    threading = options.get('use_threading')\n    shutdown_message = options.get('shutdown_message', '')\n    quit_command = (((sys.platform == 'win32') and 'CTRL-BREAK') or 'CONTROL-C')\n    self.stdout.write('Validating models...\\n\\n')\n    self.check(display_num_errors=True)\n    self.stdout.write(('%(started_at)s\\nDjango version %(version)s, using settings %(settings)r\\nStarting development server at https://%(addr)s:%(port)s/\\nUsing SSL certificate: %(cert)s\\nUsing SSL key: %(key)s\\nQuit the server with %(quit_command)s.\\n' % {\n        'started_at': datetime.now().strftime('%B %d, %Y - %X'),\n        'version': self.get_version(),\n        'settings': settings.SETTINGS_MODULE,\n        'addr': ((self._raw_ipv6 and ('[%s]' % self.addr)) or self.addr),\n        'port': self.port,\n        'quit_command': quit_command,\n        'cert': cert_file,\n        'key': key_file,\n    }))\n    translation.activate(settings.LANGUAGE_CODE)\n    try:\n        handler = self.get_handler(*args, **options)\n        server = SecureHTTPServer((self.addr, int(self.port)), WSGIRequestHandler, cert_file, key_file)\n        server.set_app(handler)\n        server.serve_forever()\n    except WSGIServerException:\n        e = sys.exc_info()[1]\n        ERRORS = {\n            13: \"You don't have permission to access that port.\",\n            98: 'That port is already in use.',\n            99: \"That IP address can't be assigned-to.\",\n        }\n        try:\n            error_text = ERRORS[e.args[0].args[0]]\n        except (AttributeError, KeyError):\n            error_text = str(e)\n        self.stderr.write(('Error: %s' % error_text))\n        os._exit(1)\n    except KeyboardInterrupt:\n        if shutdown_message:\n            self.stdout.write(shutdown_message)\n        sys.exit(0)\n", "label": 1}
{"function": "\n\ndef test_ddt_class_decorator(self):\n    'Classes with DDT-decorated functions have ddt.ddt class decorator.\\n\\n        '\n    errors = []\n    for (dirname, dirnames, filenames) in os.walk(self.tests_path):\n        for filename in filenames:\n            if (not (filename.startswith('test_') and filename.endswith('.py'))):\n                continue\n            filename = os.path.relpath(os.path.join(dirname, filename))\n            with open(filename, 'rb') as fh:\n                try:\n                    tree = ast.parse(fh.read(), filename)\n                except TypeError as err:\n                    errors.append({\n                        'message': str(err),\n                        'filename': filename,\n                        'lineno': (- 1),\n                    })\n            visitor = DDTDecoratorChecker()\n            visitor.visit(tree)\n            errors.extend((dict(filename=filename, **error) for error in visitor.errors.values()))\n    if errors:\n        msg = ['']\n        for error in errors:\n            msg.extend([('Errors at %(filename)s line %(lineno)d: %(message)s' % {\n                'message': error['message'],\n                'filename': error['filename'],\n                'lineno': error['lineno'],\n            }), ''])\n        self.fail('\\n'.join(msg))\n", "label": 1}
{"function": "\n\ndef load_module(self, name):\n    if (not name.startswith(self._vendor_pkg)):\n        raise ImportError((\"Cannot import %s, must be a subpackage of '%s'.\" % (name, self._vendor_name)))\n    if (not ((name == self._vendor_name) or any((name.startswith(pkg) for pkg in self._vendor_pkgs)))):\n        raise ImportError(('Cannot import %s, must be one of %s.' % (name, self._vendor_pkgs)))\n    if (name in sys.modules):\n        return sys.modules[name]\n    try:\n        real_meta_path = sys.meta_path[:]\n        try:\n            sys.meta_path = [m for m in sys.meta_path if (not isinstance(m, VendorAlias))]\n            __import__(name)\n            module = sys.modules[name]\n        finally:\n            for m in sys.meta_path:\n                if (m not in real_meta_path):\n                    real_meta_path.append(m)\n            sys.meta_path = real_meta_path\n    except ImportError:\n        real_name = name[len(self._vendor_pkg):]\n        try:\n            __import__(real_name)\n            module = sys.modules[real_name]\n        except ImportError:\n            raise ImportError((\"No module named '%s'\" % (name,)))\n    sys.modules[name] = module\n    return module\n", "label": 1}
{"function": "\n\ndef arg_of_sigmoid(Y_hat):\n    '\\n    Given the output of a call to theano.tensor.nnet.sigmoid,\\n    returns the argument to the sigmoid (by tracing the Theano\\n    graph).\\n\\n    Parameters\\n    ----------\\n    Y_hat : Variable\\n        T.nnet.sigmoid(Z)\\n\\n    Returns\\n    -------\\n    Z : Variable\\n        The variable that was passed to T.nnet.sigmoid to create `Y_hat`.\\n        Raises an error if `Y_hat` is not actually the output of a theano\\n        sigmoid.\\n    '\n    assert hasattr(Y_hat, 'owner')\n    owner = Y_hat.owner\n    assert (owner is not None)\n    op = owner.op\n    if isinstance(op, Print):\n        assert (len(owner.inputs) == 1)\n        (Y_hat,) = owner.inputs\n        owner = Y_hat.owner\n        op = owner.op\n    success = False\n    if isinstance(op, T.Elemwise):\n        if isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid):\n            success = True\n    if (not success):\n        raise TypeError(((('Expected Y_hat to be the output of a sigmoid, but it appears to be the output of ' + str(op)) + ' of type ') + str(type(op))))\n    (z,) = owner.inputs\n    assert (z.ndim == 2)\n    return z\n", "label": 1}
{"function": "\n\ndef test_round(self):\n    result = self.alltypes.double_col.round()\n    assert isinstance(result, ir.Int64Array)\n    assert (result.op().args[1] is None)\n    result = self.alltypes.double_col.round(2)\n    assert isinstance(result, ir.DoubleArray)\n    assert (result.op().args[1] == 2)\n    result = self.alltypes.int_col.round(2)\n    assert isinstance(result, ir.DoubleArray)\n    dec = self.lineitem.l_extendedprice\n    result = dec.round()\n    assert isinstance(result, ir.DecimalArray)\n    result = dec.round(2)\n    assert isinstance(result, ir.DecimalArray)\n    result = ibis.literal(1.2345).round()\n    assert isinstance(result, ir.Int64Scalar)\n", "label": 1}
{"function": "\n\ndef test_genetate_with_more_than_one_resource(self):\n    api = APISpecification(version='v1', base_url='http://api.globo.com')\n    api.add_resource(Resource('dogs', paths=[Path('/dogs', methods=[Method('PUT')])]))\n    api.add_resource(Resource('cats', paths=[Path('/cats', methods=[Method('PUT')])]))\n    result = self.gen(api)\n    doc = ElementTree.fromstring(result)\n    assert doc.tag.endswith('application')\n    resource = doc.getchildren()[0]\n    resources = resource.getchildren()\n    assert (len(resources) == 2)\n    assert resources[0].tag.endswith('resource')\n    assert (resources[0].get('path') == '/dogs')\n    method = resources[0].getchildren()[0]\n    assert method.tag.endswith('method')\n    assert (method.get('name') == 'PUT')\n    assert resources[1].tag.endswith('resource')\n    assert (resources[1].get('path') == '/cats')\n    method = resources[1].getchildren()[0]\n    assert method.tag.endswith('method')\n    assert (method.get('name') == 'PUT')\n", "label": 1}
{"function": "\n\ndef _copy_item(item, source, target, target_user, target_folder_id, relationships, copy_dir):\n    itemid = item['id']\n    item_dir = os.path.join(copy_dir, itemid)\n    os.makedirs(item_dir)\n    try:\n        target_item = _select_properties(item, ITEM_COPY_PROPERTIES)\n        data_file = None\n        if (item['type'] in TEXT_BASED_ITEM_TYPES):\n            text = source.item_data(itemid)\n            if (text and (len(text) > 0)):\n                target_item['text'] = text\n        elif (item['type'] in FILE_BASED_ITEM_TYPES):\n            data_file = source.item_datad(itemid, item_dir, item.get('name'))\n        thumbnail_file = None\n        if ('thumbnail' in item):\n            thumbnail_file = source.item_thumbnaild(itemid, item_dir, unicode_to_ascii(item['thumbnail']))\n        metadata_file = source.item_metadatad(itemid, item_dir)\n        target_itemid = target.add_item(target_item, data_file, thumbnail_file, metadata_file, target_user, unicode_to_ascii(target_folder_id))\n        if target_itemid:\n            _log.info(((((('Copied item ' + itemid) + ' in source portal ') + 'to ') + target_itemid) + ' in target portal'))\n            copied_items = dict({\n                itemid: target_itemid,\n            })\n            if relationships:\n                related_items = _copy_relationships(copied_items, source, target, target_user, target_folder_id, relationships, copy_dir)\n                copied_items.update(related_items)\n            return (target_itemid, copied_items)\n        else:\n            _log.warning(((('Item ' + itemid) + ' was not copied ') + 'to target portal'))\n    except IOError as e:\n        _log.warning(((((('Item ' + itemid) + ' was not copied to target portal ') + '(') + str(e)) + ')'))\n    finally:\n        if clean_temp_files:\n            shutil.rmtree(item_dir)\n    return (None, None)\n", "label": 1}
{"function": "\n\ndef code_match(code, select, ignore):\n    if ignore:\n        assert (not isinstance(ignore, unicode))\n        for ignored_code in [c.strip() for c in ignore]:\n            if mutual_startswith(code.lower(), ignored_code.lower()):\n                return False\n    if select:\n        assert (not isinstance(select, unicode))\n        for selected_code in [c.strip() for c in select]:\n            if mutual_startswith(code.lower(), selected_code.lower()):\n                return True\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_logging_level(self):\n    rebase_environment('app_without_logging_configuration')\n    command = StubbedSearchCommand()\n    warning = logging.getLevelName(logging.WARNING)\n    notset = logging.getLevelName(logging.NOTSET)\n    logging.root.setLevel(logging.WARNING)\n    self.assertEqual(command.logging_level, warning)\n    self.assertEquals(warning, command.logging_level)\n    for level in logging._levelNames:\n        if (type(level) is int):\n            command.logging_level = level\n            level_name = logging.getLevelName(level)\n            self.assertEquals(command.logging_level, (warning if (level_name == notset) else level_name))\n        else:\n            level_name = logging.getLevelName(logging.getLevelName(level))\n            for variant in (level, level.lower(), level.capitalize()):\n                command.logging_level = variant\n                self.assertEquals(command.logging_level, (warning if (level_name == notset) else level_name))\n    for level in (999, 999.999):\n        command.logging_level = level\n        self.assertEqual(command.logging_level, 'Level 999')\n    current_value = command.logging_level\n    try:\n        command.logging_level = 'foo'\n    except ValueError:\n        pass\n    except BaseException as e:\n        self.fail('Expected ValueError, but {} was raised'.format(type(e)))\n    else:\n        self.fail('Expected ValueError, but logging_level={}'.format(command.logging_level))\n    self.assertEqual(command.logging_level, current_value)\n", "label": 1}
{"function": "\n\ndef test_issue_6746():\n    y = Symbol('y')\n    n = Symbol('n')\n    assert (manualintegrate((y ** x), x) == Piecewise((x, Eq(log(y), 0)), (((y ** x) / log(y)), True)))\n    assert (manualintegrate((y ** (n * x)), x) == Piecewise((x, Eq(n, 0)), ((Piecewise(((n * x), Eq(log(y), 0)), (((y ** (n * x)) / log(y)), True)) / n), True)))\n    assert (manualintegrate(exp((n * x)), x) == Piecewise((x, Eq(n, 0)), ((exp((n * x)) / n), True)))\n    y = Symbol('y', positive=True)\n    assert (manualintegrate(((y + 1) ** x), x) == (((y + 1) ** x) / log((y + 1))))\n    y = Symbol('y', zero=True)\n    assert (manualintegrate(((y + 1) ** x), x) == x)\n    y = Symbol('y')\n    n = Symbol('n', nonzero=True)\n    assert (manualintegrate((y ** (n * x)), x) == (Piecewise(((n * x), Eq(log(y), 0)), (((y ** (n * x)) / log(y)), True)) / n))\n    y = Symbol('y', positive=True)\n    assert (manualintegrate(((y + 1) ** (n * x)), x) == (((y + 1) ** (n * x)) / (n * log((y + 1)))))\n    a = Symbol('a', negative=True)\n    assert (manualintegrate((1 / (a + (b * (x ** 2)))), x) == Integral((1 / (a + (b * (x ** 2)))), x))\n", "label": 1}
{"function": "\n\ndef _parseAddr(addr, lookup=True):\n    if (lookup and any(((ch not in IP4Range._IPREMOVE) for ch in addr))):\n        try:\n            addr = socket.gethostbyname(addr)\n        except socket.error:\n            raise ValueError('Invalid Hostname as argument.')\n    naddr = 0\n    for (naddrpos, part) in enumerate(addr.split('.')):\n        if (naddrpos >= 4):\n            raise ValueError('Address contains more than four parts.')\n        try:\n            if (not part):\n                part = 0\n            else:\n                part = int(part)\n            if (not (0 <= part < 256)):\n                raise ValueError\n        except ValueError:\n            raise ValueError('Address part out of range.')\n        naddr <<= 8\n        naddr += part\n    return (naddr, (naddrpos + 1))\n", "label": 1}
{"function": "\n\ndef replace_all_validate(self, fgraph, replacements, reason=None, verbose=None):\n    chk = fgraph.checkpoint()\n    if (verbose is None):\n        verbose = config.optimizer_verbose\n    for (r, new_r) in replacements:\n        try:\n            fgraph.replace(r, new_r, reason=reason, verbose=False)\n        except Exception as e:\n            msg = str(e)\n            s1 = 'The type of the replacement must be the same'\n            s2 = 'does not belong to this FunctionGraph'\n            s3 = 'maximum recursion depth exceeded'\n            if (s3 in msg):\n                e.args += ('Please, report this to theano-dev mailing list. As a temporary work around, you can raise Python stack limit with: import sys; sys.setrecursionlimit(10000)',)\n                raise\n            elif ((s1 not in msg) and (s2 not in msg)):\n                out = sys.stderr\n                print('<<!! BUG IN FGRAPH.REPLACE OR A LISTENER !!>>', type(e), e, reason, file=out)\n            fgraph.revert(chk)\n            raise\n    try:\n        fgraph.validate()\n    except Exception as e:\n        fgraph.revert(chk)\n        if verbose:\n            print(('validate failed on node %s.\\n Reason: %s, %s' % (r, reason, e)))\n        raise\n    if verbose:\n        print(reason, r, new_r)\n    return chk\n", "label": 1}
{"function": "\n\ndef do_attribute_consuming_service(conf, spsso):\n    service_description = service_name = None\n    requested_attributes = []\n    acs = conf.attribute_converters\n    req = conf.getattr('required_attributes', 'sp')\n    if req:\n        requested_attributes.extend(do_requested_attribute(req, acs, is_required='true'))\n    opt = conf.getattr('optional_attributes', 'sp')\n    if opt:\n        requested_attributes.extend(do_requested_attribute(opt, acs))\n    try:\n        if conf.description:\n            try:\n                (text, lang) = conf.description\n            except ValueError:\n                text = conf.description\n                lang = 'en'\n            service_description = [md.ServiceDescription(text=text, lang=lang)]\n    except KeyError:\n        pass\n    try:\n        if conf.name:\n            try:\n                (text, lang) = conf.name\n            except ValueError:\n                text = conf.name\n                lang = 'en'\n            service_name = [md.ServiceName(text=text, lang=lang)]\n    except KeyError:\n        pass\n    if requested_attributes:\n        if (not service_name):\n            service_name = [md.ServiceName(text='', lang='en')]\n        ac_serv = md.AttributeConsumingService(index='1', service_name=service_name, requested_attribute=requested_attributes)\n        if service_description:\n            ac_serv.service_description = service_description\n        spsso.attribute_consuming_service = [ac_serv]\n", "label": 1}
{"function": "\n\ndef dump_video(self, filename, meta_data):\n    '\\n        Dump a video from videolectures.net\\n        '\n    if os.path.exists(filename):\n        if ((self.opts.overwrite is True) and (self.opts.resume is False)):\n            os.remove(filename)\n        elif ((self.opts.overwrite is False) and (self.opts.resume is False)):\n            self.error(DownloadError, ('ERROR: file already exists. ' + 'remove it or use `overwrite`'))\n    try:\n        stdout = open(os.path.devnull, 'w')\n        subprocess.call(['rtmpdump', '-h'], stdout=stdout, stderr=subprocess.STDOUT)\n    except (OSError, IOError):\n        print('ERROR: rtmpdump could not be run. check the binary path.')\n        sys.exit(1)\n    finally:\n        stdout.close()\n    basic_args = ['rtmpdump', '-q', '-r', meta_data['streamer'], '-y', meta_data['source'], '-a', 'vod', '-o', filename]\n    if (self.opts.resume is True):\n        basic_args.append('-e')\n    retval = subprocess.Popen(basic_args)\n    while True:\n        if (retval.poll() is not None):\n            break\n        if (not os.path.exists(filename)):\n            continue\n        previous_size = os.path.getsize(filename)\n        display_size = _convert_display_size(previous_size)\n        self._to_stdout(self._CLEAR_STDOUT, skip_eol=True)\n        self._to_stdout('\\r[rtmpdump] {0}'.format(display_size), skip_eol=True)\n        time.sleep(2.0)\n    if (retval.wait() == 0):\n        current_size = os.path.getsize(filename)\n        display_size = _convert_display_size(current_size)\n        self._to_stdout(self._CLEAR_STDOUT, skip_eol=True)\n        self._to_stdout('\\r[rtmpdump] {0}'.format(display_size))\n        return True\n    else:\n        self.error(DownloadError, ('ERROR: download may be incomplete.' + ' rtmpdump exited with code 1 or 2'))\n        return False\n", "label": 1}
{"function": "\n\ndef get_form_instance(**kwargs):\n    'Returns form instance'\n    callee = type(inspect.currentframe().f_back.f_locals['self']).__name__\n    operation = ('create' if ('Create' in callee) else 'update')\n    try:\n        field_config = activity_config(*[get_request_params(key, **kwargs) for key in ('app_name', 'model_name')])\n        fields = [field for field in field_config if (operation in field_config[field])]\n    except KeyError:\n        fields = [field for field in (field.name for field in get_model(**kwargs)().class_meta.get_fields()) if (field not in ['id', 'task', 'task_id', 'last_updated', 'creation_date'])]\n    return modelform_factory(get_model(**kwargs), fields=fields)\n", "label": 1}
{"function": "\n\ndef expand_to_line(string, startIndex, endIndex):\n    linebreakRe = re.compile('\\\\n')\n    spacesAndTabsRe = re.compile('([ \\\\t]+)')\n    searchIndex = (startIndex - 1)\n    while True:\n        if (searchIndex < 0):\n            newStartIndex = (searchIndex + 1)\n            break\n        char = string[searchIndex:(searchIndex + 1)]\n        if linebreakRe.match(char):\n            newStartIndex = (searchIndex + 1)\n            break\n        else:\n            searchIndex -= 1\n    searchIndex = endIndex\n    while True:\n        if (searchIndex > (len(string) - 1)):\n            newEndIndex = searchIndex\n            break\n        char = string[searchIndex:(searchIndex + 1)]\n        if linebreakRe.match(char):\n            newEndIndex = searchIndex\n            break\n        else:\n            searchIndex += 1\n    s = string[newStartIndex:newEndIndex]\n    r = spacesAndTabsRe.match(s)\n    if (r and (r.end() <= startIndex)):\n        newStartIndex = (newStartIndex + r.end())\n    try:\n        if ((startIndex == newStartIndex) and (endIndex == newEndIndex)):\n            return None\n        else:\n            return utils.create_return_obj(newStartIndex, newEndIndex, string, 'line')\n    except NameError:\n        return None\n", "label": 1}
{"function": "\n\ndef _remotes_on(port, which_end):\n    '\\n    Return a set of ip addrs active tcp connections\\n    '\n    port = int(port)\n    ret = set()\n    proc_available = False\n    for statf in ['/proc/net/tcp', '/proc/net/tcp6']:\n        if os.path.isfile(statf):\n            proc_available = True\n            with salt.utils.fopen(statf, 'rb') as fp_:\n                for line in fp_:\n                    if line.strip().startswith('sl'):\n                        continue\n                    iret = _parse_tcp_line(line)\n                    sl = next(iter(iret))\n                    if (iret[sl][which_end] == port):\n                        ret.add(iret[sl]['remote_addr'])\n    if (not proc_available):\n        if salt.utils.is_sunos():\n            return _sunos_remotes_on(port, which_end)\n        if salt.utils.is_freebsd():\n            return _freebsd_remotes_on(port, which_end)\n        if salt.utils.is_netbsd():\n            return _netbsd_remotes_on(port, which_end)\n        if salt.utils.is_openbsd():\n            return _openbsd_remotes_on(port, which_end)\n        if salt.utils.is_windows():\n            return _windows_remotes_on(port, which_end)\n        return _linux_remotes_on(port, which_end)\n    return ret\n", "label": 1}
{"function": "\n\ndef _do_import_tags(self, project_ids, collected_tags):\n    tags_to_import_now = set([])\n    tags_to_import_after = set([])\n    for tag in collected_tags:\n        if self._is_prefix_of_any_other_tag(tag, collected_tags):\n            tags_to_import_after.add(tag)\n        else:\n            tags_to_import_now.add(tag)\n    for project_id in project_ids:\n        for (issue_id, tags) in self._get_issue_tags(project_id):\n            yt_issue_id = ('%s-%s' % (project_id, issue_id))\n            for tag in tags:\n                if (tag in tags_to_import_now):\n                    try:\n                        self._target.executeCommand(yt_issue_id, ('tag ' + tag))\n                    except YouTrackException:\n                        print(('Failed to import tag for issue [%s]' % yt_issue_id))\n    if len(tags_to_import_after):\n        self._do_import_tags(project_ids, tags_to_import_after)\n", "label": 1}
{"function": "\n\n@classmethod\ndef apply_changes(cls, theme, msg, filters, ui_theme, command):\n    'Update theme.  Set the theme, then get the next one in line.'\n    debug_log(('apply_changes(\\n    theme=%s\\n    msg=%s,\\n    filters=%s,\\n    ui_theme=%s,\\n    command=%s\\n)' % (theme, msg, filters, ui_theme, command)))\n    if (cls.next_change is not None):\n        cls.current_time = cls.next_change.time\n    cls.current_theme = theme\n    cls.current_ui_theme = ui_theme\n    cls.current_msg = msg\n    cls.current_filters = filters\n    if (filters is not None):\n        if is_tweakable():\n            debug_log('Using Theme Tweaker to adjust file!')\n            sublime.run_command('theme_tweaker_custom', {\n                'theme': theme,\n                'filters': filters,\n            })\n            if (ui_theme is not None):\n                cls.set_theme(None, ui_theme)\n        else:\n            debug_log('ThemeTweaker is not installed :(')\n            cls.set_theme(theme, ui_theme)\n    else:\n        cls.set_theme(theme, ui_theme)\n    try:\n        if (command is not None):\n            command.run()\n    except Exception as e:\n        log(('Command %s failed!' % str(command)))\n        log(('\\n%s' % str(e)))\n    if ((msg is not None) and isinstance(msg, str)):\n        sublime.set_timeout((lambda m=msg: display_message(m)), 3000)\n", "label": 1}
{"function": "\n\ndef start(self, isMaster, environ={\n    \n}):\n    if self.started:\n        return\n    logger.debug('start env in %s: %s %s', os.getpid(), isMaster, environ)\n    self.isMaster = isMaster\n    if isMaster:\n        roots = conf.DPARK_WORK_DIR\n        if isinstance(roots, str):\n            roots = roots.split(',')\n        name = ('%s-%s' % (socket.gethostname(), uuid.uuid4()))\n        self.workdir = [os.path.join(root, name) for root in roots]\n        try:\n            for d in self.workdir:\n                util.mkdir_p(d)\n        except OSError as e:\n            if environ.get('is_local', False):\n                raise e\n        self.environ['SERVER_URI'] = ('file://' + self.workdir[0])\n        self.environ['WORKDIR'] = self.workdir\n        self.environ['COMPRESS'] = util.COMPRESS\n    else:\n        self.environ.update(environ)\n        if (self.environ['COMPRESS'] != util.COMPRESS):\n            raise Exception(('no %s available' % self.environ['COMPRESS']))\n    self.ctx = zmq.Context()\n    from dpark.tracker import TrackerServer, TrackerClient\n    if isMaster:\n        self.trackerServer = TrackerServer()\n        self.trackerServer.start()\n        addr = self.trackerServer.addr\n        env.register('TrackerAddr', addr)\n    else:\n        addr = env.get('TrackerAddr')\n    self.trackerClient = TrackerClient(addr)\n    from dpark.cache import CacheTracker\n    self.cacheTracker = CacheTracker()\n    from dpark.shuffle import LocalFileShuffle, MapOutputTracker\n    LocalFileShuffle.initialize(isMaster)\n    self.mapOutputTracker = MapOutputTracker()\n    from dpark.shuffle import ParallelShuffleFetcher\n    self.shuffleFetcher = ParallelShuffleFetcher(2)\n    from dpark.broadcast import start_manager\n    start_manager(isMaster)\n    self.started = True\n    logger.debug('env started')\n", "label": 1}
{"function": "\n\ndef parse_body_arguments(content_type, body, arguments, files, headers=None):\n    'Parses a form request body.\\n\\n    Supports ``application/x-www-form-urlencoded`` and\\n    ``multipart/form-data``.  The ``content_type`` parameter should be\\n    a string and ``body`` should be a byte string.  The ``arguments``\\n    and ``files`` parameters are dictionaries that will be updated\\n    with the parsed contents.\\n    '\n    if (headers and ('Content-Encoding' in headers)):\n        gen_log.warning('Unsupported Content-Encoding: %s', headers['Content-Encoding'])\n        return\n    if content_type.startswith('application/x-www-form-urlencoded'):\n        try:\n            uri_arguments = parse_qs_bytes(native_str(body), keep_blank_values=True)\n        except Exception as e:\n            gen_log.warning('Invalid x-www-form-urlencoded body: %s', e)\n            uri_arguments = {\n                \n            }\n        for (name, values) in uri_arguments.items():\n            if values:\n                arguments.setdefault(name, []).extend(values)\n    elif content_type.startswith('multipart/form-data'):\n        fields = content_type.split(';')\n        for field in fields:\n            (k, sep, v) = field.strip().partition('=')\n            if ((k == 'boundary') and v):\n                parse_multipart_form_data(utf8(v), body, arguments, files)\n                break\n        else:\n            gen_log.warning('Invalid multipart/form-data')\n", "label": 1}
{"function": "\n\n@require_POST\n@anonymous_csrf\ndef watch_question(request, question_id):\n    'Start watching a question for replies or solution.'\n    question = get_object_or_404(Question, pk=question_id, is_spam=False)\n    form = WatchQuestionForm(request.user, request.POST)\n    msg = None\n    if form.is_valid():\n        user_or_email = (request.user if request.user.is_authenticated() else form.cleaned_data['email'])\n        try:\n            if (form.cleaned_data['event_type'] == 'reply'):\n                QuestionReplyEvent.notify(user_or_email, question)\n            else:\n                QuestionSolvedEvent.notify(user_or_email, question)\n            statsd.incr('questions.watches.new')\n        except ActivationRequestFailed:\n            msg = _('Could not send a message to that email address.')\n    if request.is_ajax():\n        if form.is_valid():\n            msg = (msg or (_('You will be notified of updates by email.') if request.user.is_authenticated() else _('You should receive an email shortly to confirm your subscription.')))\n            return HttpResponse(json.dumps({\n                'message': msg,\n            }))\n        if request.POST.get('from_vote'):\n            tmpl = 'questions/includes/question_vote_thanks.html'\n        else:\n            tmpl = 'questions/includes/email_subscribe.html'\n        html = render_to_string(tmpl, context={\n            'question': question,\n            'watch_form': form,\n        }, request=request)\n        return HttpResponse(json.dumps({\n            'html': html,\n        }))\n    if msg:\n        messages.add_message(request, messages.ERROR, msg)\n    return HttpResponseRedirect(question.get_absolute_url())\n", "label": 1}
{"function": "\n\ndef Run(self):\n    'The main run method of the client.\\n\\n    This method does not normally return. Only if there have been more than\\n    Client.connection_error_limit failures, the method returns and allows the\\n    client to exit.\\n    '\n    while True:\n        if (self.http_manager.consecutive_connection_errors > config_lib.CONFIG['Client.connection_error_limit']):\n            return\n        self.client_worker.SendNannyMessage()\n        now = time.time()\n        if (now > (self.last_foreman_check + config_lib.CONFIG['Client.foreman_check_frequency'])):\n            try:\n                self.client_worker.SendReply(rdf_protodict.DataBlob(), session_id=rdfvalue.FlowSessionID(flow_name='Foreman'), priority=rdf_flows.GrrMessage.Priority.LOW_PRIORITY, require_fastpoll=False, blocking=False)\n                self.last_foreman_check = now\n            except Queue.Full:\n                pass\n        try:\n            self.RunOnce()\n        except Exception:\n            logging.warn('Uncaught exception caught: %s', traceback.format_exc())\n            if flags.FLAGS.debug:\n                pdb.post_mortem()\n        if (self.client_worker.MemoryExceeded() and (not self.client_worker.IsActive()) and (self.client_worker.InQueueSize() == 0) and (self.client_worker.OutQueueSize() == 0)):\n            logging.warning('Memory exceeded - exiting.')\n            self.client_worker.SendClientAlert('Memory limit exceeded, exiting.')\n            self.client_worker.MemoryExceeded = (lambda : True)\n            self.RunOnce()\n            sys.exit((- 1))\n        self.timer.Wait()\n", "label": 1}
{"function": "\n\ndef permits(self, context, principals, permission):\n    'Returns True or False depending if the token with the specified\\n        principals has access to the given permission.\\n        '\n    principals = set(principals)\n    permissions_required = VIEWS_PERMISSIONS_REQUIRED[permission]\n    current_permissions = set()\n    if principals.intersection(self.model_creators):\n        current_permissions.add('create_model')\n    if principals.intersection(self.token_creators):\n        current_permissions.add('create_token')\n    if principals.intersection(self.token_managers):\n        current_permissions.add('manage_token')\n    model_id = context.model_id\n    if (model_id is not None):\n        try:\n            model_permissions = context.db.get_model_permissions(model_id)\n        except backend_exceptions.ModelNotFound:\n            model_permissions = {\n                \n            }\n            if (permission != 'post_model'):\n                return True\n        finally:\n            for (perm_name, credentials_ids) in iteritems(model_permissions):\n                if principals.intersection(credentials_ids):\n                    current_permissions.add(perm_name)\n    record_id = context.record_id\n    if (record_id is not None):\n        try:\n            authors = context.db.get_record_authors(model_id, record_id)\n        except backend_exceptions.RecordNotFound:\n            authors = []\n        finally:\n            if (not principals.intersection(authors)):\n                current_permissions -= AUTHORS_PERMISSIONS\n    logger.debug('Current permissions: %s', current_permissions)\n    context.request.permissions = current_permissions\n    context.request.principals = principals\n    return permissions_required.matches(current_permissions)\n", "label": 1}
{"function": "\n\ndef acquire(self, blocking=True, timeout=None):\n    \"\\n        Acquire the lock. By defaults blocks and waits forever.\\n\\n        :param blocking: Block until lock is obtained or return immediately.\\n        :type blocking: bool\\n        :param timeout: Don't wait forever to acquire the lock.\\n        :type timeout: float or None\\n\\n        :returns: Was the lock acquired?\\n        :rtype: bool\\n\\n        :raises: :exc:`~kazoo.exceptions.LockTimeout` if the lock\\n                 wasn't acquired within `timeout` seconds.\\n\\n        .. versionadded:: 1.1\\n            The timeout option.\\n        \"\n\n    def _acquire_lock():\n        got_it = self._lock.acquire(False)\n        if (not got_it):\n            raise ForceRetryError()\n        return True\n    retry = self._retry.copy()\n    retry.deadline = timeout\n    locked = self._lock.acquire(False)\n    if ((not locked) and (not blocking)):\n        return False\n    if (not locked):\n        try:\n            locked = retry(_acquire_lock)\n        except RetryFailedError:\n            return False\n    already_acquired = self.is_acquired\n    try:\n        gotten = False\n        try:\n            gotten = retry(self._inner_acquire, blocking=blocking, timeout=timeout)\n        except RetryFailedError:\n            pass\n        except KazooException:\n            exc_info = sys.exc_info()\n            if (not already_acquired):\n                self._best_effort_cleanup()\n                self.cancelled = False\n            six.reraise(exc_info[0], exc_info[1], exc_info[2])\n        if gotten:\n            self.is_acquired = gotten\n        if ((not gotten) and (not already_acquired)):\n            self._best_effort_cleanup()\n        return gotten\n    finally:\n        self._lock.release()\n", "label": 1}
{"function": "\n\ndef set_dns(name, dnsservers=None, searchdomains=None, path=None):\n    '\\n    .. versionchanged:: 2015.5.0\\n        The ``dnsservers`` and ``searchdomains`` parameters can now be passed\\n        as a comma-separated list.\\n\\n    Update /etc/resolv.confo\\n\\n    path\\n\\n        path to the container parent\\n        default: /var/lib/lxc (system default)\\n\\n        .. versionadded:: 2015.8.0\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt myminion lxc.set_dns ubuntu \"[\\'8.8.8.8\\', \\'4.4.4.4\\']\"\\n\\n    '\n    if (dnsservers is None):\n        dnsservers = ['8.8.8.8', '4.4.4.4']\n    elif (not isinstance(dnsservers, list)):\n        try:\n            dnsservers = dnsservers.split(',')\n        except AttributeError:\n            raise SaltInvocationError(\"Invalid input for 'dnsservers' parameter\")\n    if (searchdomains is None):\n        searchdomains = []\n    elif (not isinstance(searchdomains, list)):\n        try:\n            searchdomains = searchdomains.split(',')\n        except AttributeError:\n            raise SaltInvocationError(\"Invalid input for 'searchdomains' parameter\")\n    dns = ['nameserver {0}'.format(x) for x in dnsservers]\n    dns.extend(['search {0}'.format(x) for x in searchdomains])\n    dns = ('\\n'.join(dns) + '\\n')\n    rstr = __salt__['test.rand_str']()\n    script = '/sbin/{0}_dns.sh'.format(rstr)\n    DNS_SCRIPT = '\\n'.join(['#!/usr/bin/env bash', 'if [ -h /etc/resolv.conf ];then', ' if [ \"x$(readlink /etc/resolv.conf)\" = \"x../run/resolvconf/resolv.conf\" ];then', '  if [ ! -d /run/resolvconf/ ];then', '   mkdir -p /run/resolvconf', '  fi', '  cat > /etc/resolvconf/resolv.conf.d/head <<EOF', dns, 'EOF', '', ' fi', 'fi', 'cat > /etc/resolv.conf <<EOF', dns, 'EOF', ''])\n    result = run_all(name, 'tee {0}'.format(script), path=path, stdin=DNS_SCRIPT, python_shell=True)\n    if (result['retcode'] == 0):\n        result = run_all(name, 'sh -c \"chmod +x {0};{0}\"'.format(script), path=path, python_shell=True)\n    run_all(name, 'sh -c \\'if [ -f \"{0}\" ];then rm -f \"{0}\";fi\\''.format(script), path=path, python_shell=True)\n    if (result['retcode'] != 0):\n        error = \"Unable to write to /etc/resolv.conf in container '{0}'\".format(name)\n        if result['stderr']:\n            error += ': {0}'.format(result['stderr'])\n        raise CommandExecutionError(error)\n    return True\n", "label": 1}
{"function": "\n\ndef parse_one_cond(tokens, name, context):\n    ((first, pos), tokens) = (tokens[0], tokens[1:])\n    content = []\n    if first.endswith(':'):\n        first = first[:(- 1)]\n    if first.startswith('if '):\n        part = ('if', pos, first[3:].lstrip(), content)\n    elif first.startswith('elif '):\n        part = ('elif', pos, first[5:].lstrip(), content)\n    elif (first == 'else'):\n        part = ('else', pos, None, content)\n    else:\n        assert 0, ('Unexpected token %r at %s' % (first, pos))\n    while 1:\n        if (not tokens):\n            raise TemplateError('No {{endif}}', position=pos, name=name)\n        if (isinstance(tokens[0], tuple) and ((tokens[0][0] == 'endif') or tokens[0][0].startswith('elif ') or (tokens[0][0] == 'else'))):\n            return (part, tokens)\n        (next_chunk, tokens) = parse_expr(tokens, name, context)\n        content.append(next_chunk)\n", "label": 1}
{"function": "\n\ndef _sparse_fruchterman_reingold(A, dim=2, k=None, pos=None, fixed=None, iterations=50):\n    try:\n        import numpy as np\n    except ImportError:\n        raise ImportError('_sparse_fruchterman_reingold() requires numpy: http://scipy.org/ ')\n    try:\n        (nnodes, _) = A.shape\n    except AttributeError:\n        raise nx.NetworkXError('fruchterman_reingold() takes an adjacency matrix as input')\n    try:\n        from scipy.sparse import spdiags, coo_matrix\n    except ImportError:\n        raise ImportError('_sparse_fruchterman_reingold() scipy numpy: http://scipy.org/ ')\n    try:\n        A = A.tolil()\n    except:\n        A = coo_matrix(A).tolil()\n    if (pos == None):\n        pos = np.asarray(np.random.random((nnodes, dim)), dtype=A.dtype)\n    else:\n        pos = pos.astype(A.dtype)\n    if (fixed == None):\n        fixed = []\n    if (k is None):\n        k = np.sqrt((1.0 / nnodes))\n    t = 0.1\n    dt = (t / float((iterations + 1)))\n    displacement = np.zeros((dim, nnodes))\n    for iteration in range(iterations):\n        displacement *= 0\n        for i in range(A.shape[0]):\n            if (i in fixed):\n                continue\n            delta = (pos[i] - pos).T\n            distance = np.sqrt((delta ** 2).sum(axis=0))\n            distance = np.where((distance < 0.01), 0.01, distance)\n            Ai = np.asarray(A.getrowview(i).toarray())\n            displacement[:, i] += (delta * (((k * k) / (distance ** 2)) - ((Ai * distance) / k))).sum(axis=1)\n        length = np.sqrt((displacement ** 2).sum(axis=0))\n        length = np.where((length < 0.01), 0.1, length)\n        pos += ((displacement * t) / length).T\n        t -= dt\n        pos = _rescale_layout(pos)\n    return pos\n", "label": 1}
{"function": "\n\n@execute_count(3)\ndef test_iteration(self):\n    ' Tests that iterating over a query set pulls back all of the expected results '\n    q = TestModel.objects(test_id=0)\n    compare_set = set([(0, 5), (1, 10), (2, 15), (3, 20)])\n    for t in q:\n        val = (t.attempt_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n    q = TestModel.objects(attempt_id=3).allow_filtering()\n    assert (len(q) == 3)\n    compare_set = set([(0, 20), (1, 20), (2, 75)])\n    for t in q:\n        val = (t.test_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n    q = TestModel.objects((TestModel.attempt_id == 3)).allow_filtering()\n    assert (len(q) == 3)\n    compare_set = set([(0, 20), (1, 20), (2, 75)])\n    for t in q:\n        val = (t.test_id, t.expected_result)\n        assert (val in compare_set)\n        compare_set.remove(val)\n    assert (len(compare_set) == 0)\n", "label": 1}
{"function": "\n\n@prevent_invalid_dir\ndef global_incremental_search(self, should_open=True):\n    '\\n        Incremental search.\\n        '\n    query = ''\n    should_create_on_enter = False\n    (V + 'echohl None')\n    (V + 'echo \">> \"')\n    while True:\n        raw_char = vim.eval('getchar()')\n        if (raw_char in ('13', '27')):\n            if (raw_char == '13'):\n                if should_create_on_enter:\n                    if (should_open == True):\n                        self.open(first_line=query)\n                    else:\n                        path = join(get_save_dir(), (PadInfo([query]).id + get_setting('default_file_extension')))\n                        with open(path, 'w') as new_note:\n                            new_note.write(query)\n                    (V + 'echohl None')\n                else:\n                    self.display(query, True)\n            (V + 'redraw!')\n            break\n        else:\n            try:\n                int(raw_char)\n                last_char = vim.eval((('nr2char(' + raw_char) + ')'))\n                query = (query + last_char)\n            except:\n                keycode = unicode(raw_char, errors='ignore')\n                if (keycode == 'kb'):\n                    query = query[:(- len(last_char))]\n        pad_files = self.collator.get_filelist(query)\n        if (pad_files != []):\n            info = ''\n            (V + 'echohl None')\n            should_create_on_enter = False\n        else:\n            info = '[NEW] '\n            (V + 'echohl WarningMsg')\n            should_create_on_enter = True\n        (V + 'redraw')\n        (V + ((('echo \">> ' + info) + query) + '\"'))\n", "label": 1}
{"function": "\n\ndef __init__(self, num=None, m=None, szx=None, rawbytes=[]):\n    if rawbytes:\n        assert (num == None)\n        assert (m == None)\n        assert (szx == None)\n    else:\n        assert (num != None)\n        assert (m != None)\n        assert (szx != None)\n    coapOption.__init__(self, d.OPTION_NUM_BLOCK2)\n    if num:\n        self.num = num\n        self.m = m\n        self.szx = szx\n    elif (len(rawbytes) == 1):\n        self.num = ((rawbytes[0] >> 4) & 15)\n        self.m = ((rawbytes[0] >> 3) & 1)\n        self.szx = ((rawbytes[0] >> 0) & 7)\n    elif (len(rawbytes) == 2):\n        self.num = ((rawbytes[0] << 8) | ((rawbytes[1] >> 4) & 15))\n        self.m = ((rawbytes[1] >> 3) & 1)\n        self.szx = ((rawbytes[1] >> 0) & 7)\n    elif (len(rawbytes) == 3):\n        self.num = (((rawbytes[0] << 16) | (rawbytes[1] << 8)) | ((rawbytes[2] >> 4) & 15))\n        self.m = ((rawbytes[2] >> 3) & 1)\n        self.szx = ((rawbytes[2] >> 0) & 7)\n    else:\n        raise ValueError('unexpected Block2 len={0}'.format(len(rawbytes)))\n", "label": 1}
{"function": "\n\n@require_POST\ndef handle_bounce(request):\n    '\\n    Handle a bounced email via an SNS webhook.\\n\\n    Parse the bounced message and send the appropriate signal.\\n    For bounce messages the bounce_received signal is called.\\n    For complaint messages the complaint_received signal is called.\\n    See: http://docs.aws.amazon.com/sns/latest/gsg/json-formats.html#http-subscription-confirmation-json\\n    See: http://docs.amazonwebservices.com/ses/latest/DeveloperGuide/NotificationsViaSNS.html\\n    \\n    In addition to email bounce requests this endpoint also supports the SNS\\n    subscription confirmation request. This request is sent to the SNS\\n    subscription endpoint when the subscription is registered.\\n    See: http://docs.aws.amazon.com/sns/latest/gsg/Subscribe.html\\n\\n    For the format of the SNS subscription confirmation request see this URL:\\n    http://docs.aws.amazon.com/sns/latest/gsg/json-formats.html#http-subscription-confirmation-json\\n    \\n    SNS message signatures are verified by default. This funcionality can\\n    be disabled by setting AWS_SES_VERIFY_BOUNCE_SIGNATURES to False.\\n    However, this is not recommended.\\n    See: http://docs.amazonwebservices.com/sns/latest/gsg/SendMessageToHttp.verify.signature.html\\n    '\n    if hasattr(request, 'body'):\n        raw_json = request.body\n    else:\n        raw_json = request.raw_post_data\n    try:\n        notification = json.loads(raw_json)\n    except ValueError as e:\n        logger.warning('Recieved bounce with bad JSON: \"%s\"', e)\n        return HttpResponseBadRequest()\n    if (settings.VERIFY_BOUNCE_SIGNATURES and (not utils.verify_bounce_message(notification))):\n        logger.info('Recieved unverified notification: Type: %s', notification.get('Type'), extra={\n            'notification': notification,\n        })\n        return HttpResponse()\n    if (notification.get('Type') in ('SubscriptionConfirmation', 'UnsubscribeConfirmation')):\n        logger.info('Recieved subscription confirmation: TopicArn: %s', notification.get('TopicArn'), extra={\n            'notification': notification,\n        })\n        subscribe_url = notification.get('SubscribeURL')\n        try:\n            urlopen(subscribe_url).read()\n        except URLError as e:\n            logger.error('Could not confirm subscription: \"%s\"', e, extra={\n                'notification': notification,\n            }, exc_info=True)\n    elif (notification.get('Type') == 'Notification'):\n        try:\n            message = json.loads(notification['Message'])\n        except ValueError as e:\n            logger.warning('Recieved bounce with bad JSON: \"%s\"', e, extra={\n                'notification': notification,\n            })\n        else:\n            mail_obj = message.get('mail')\n            notification_type = message.get('notificationType')\n            if (notification_type == 'Bounce'):\n                bounce_obj = message.get('bounce', {\n                    \n                })\n                feedback_id = bounce_obj.get('feedbackId')\n                bounce_type = bounce_obj.get('bounceType')\n                bounce_subtype = bounce_obj.get('bounceSubType')\n                logger.info('Recieved bounce notification: feedbackId: %s, bounceType: %s, bounceSubType: %s', feedback_id, bounce_type, bounce_subtype, extra={\n                    'notification': notification,\n                })\n                signals.bounce_received.send(sender=handle_bounce, mail_obj=mail_obj, bounce_obj=bounce_obj, raw_message=raw_json)\n            elif (notification_type == 'Complaint'):\n                complaint_obj = message.get('complaint', {\n                    \n                })\n                feedback_id = complaint_obj.get('feedbackId')\n                feedback_type = complaint_obj.get('complaintFeedbackType')\n                logger.info('Recieved complaint notification: feedbackId: %s, feedbackType: %s', feedback_id, feedback_type, extra={\n                    'notification': notification,\n                })\n                signals.complaint_received.send(sender=handle_bounce, mail_obj=mail_obj, complaint_obj=complaint_obj, raw_message=raw_json)\n            else:\n                logger.warning('Recieved unknown notification', extra={\n                    'notification': notification,\n                })\n    else:\n        logger.info('Recieved unknown notification type: %s', notification.get('Type'), extra={\n            'notification': notification,\n        })\n    return HttpResponse()\n", "label": 1}
{"function": "\n\ndef test_interprocess_lock(self):\n    lock_file = os.path.join(self.lock_dir, 'processlock')\n    pid = os.fork()\n    if pid:\n        start = time.time()\n        while (not os.path.exists(lock_file)):\n            if ((time.time() - start) > 5):\n                self.fail('Timed out waiting for child to grab lock')\n            time.sleep(0)\n        lock1 = lockutils.InterProcessLock('foo')\n        lock1.lockfile = open(lock_file, 'w')\n        while ((time.time() - start) < 5):\n            try:\n                lock1.trylock()\n                lock1.unlock()\n                time.sleep(0)\n            except IOError:\n                break\n        else:\n            self.fail('Never caught expected lock exception')\n        os.kill(pid, signal.SIGKILL)\n    else:\n        try:\n            lock2 = lockutils.InterProcessLock('foo')\n            lock2.lockfile = open(lock_file, 'w')\n            have_lock = False\n            while (not have_lock):\n                try:\n                    lock2.trylock()\n                    have_lock = True\n                except IOError:\n                    pass\n        finally:\n            time.sleep(0.5)\n            os._exit(0)\n", "label": 1}
{"function": "\n\ndef seek(self, offset, whence=0):\n    assert (whence in [0, 1, 2])\n    if (whence == 2):\n        if (offset < 0):\n            raise ValueError('negative seek offset')\n        to_read = None\n    else:\n        if (whence == 0):\n            if (offset < 0):\n                raise ValueError('negative seek offset')\n            dest = offset\n        else:\n            pos = self.__pos\n            if (pos < offset):\n                raise ValueError('seek to before start of file')\n            dest = (pos + offset)\n        end = len_of_seekable(self.__cache)\n        to_read = (dest - end)\n        if (to_read < 0):\n            to_read = 0\n    if (to_read != 0):\n        self.__cache.seek(0, 2)\n        if (to_read is None):\n            assert (whence == 2)\n            self.__cache.write(self.wrapped.read())\n            self.read_complete = True\n            self.__pos = (self.__cache.tell() - offset)\n        else:\n            data = self.wrapped.read(to_read)\n            if (not data):\n                self.read_complete = True\n            else:\n                self.__cache.write(data)\n            self.__pos = dest\n    else:\n        self.__pos = dest\n", "label": 1}
{"function": "\n\n@login_required\ndef rule_details(request, testplan_id, rule_id):\n    try:\n        testplan = TestPlan(auth_token=request.user.password).get(testplan_id)\n        rule = Rule(testplan_id, auth_token=request.user.password).get(rule_id)\n    except UnauthorizedException:\n        logger.warning('User unauthorized. Signing out...')\n        return signout(request)\n    except NotFoundException:\n        logger.warning('The requested Rule \"%s\" does not exists in the given Test Plan \"%s\"', rule_id, testplan_id)\n        return render(request, '404.html')\n    except Exception as inst:\n        logger.error('Unexpected exception', exc_info=True)\n        messages.error(request, (inst.message if inst.message else 'Unexpected error'))\n        return HttpResponseRedirect(reverse('testplan_list'))\n    initial_data = _rule_to_post_data(rule)\n    form = RuleRequestForm((request.POST or None), initial=initial_data)\n    if form.is_valid():\n        try:\n            Rule(testplan_id, auth_token=request.user.password).update(rule['id'], form.cleaned_data)\n            messages.success(request, 'The rule was updated successfully.')\n            return HttpResponseRedirect(reverse('rule_details', args=(str(testplan_id), str(rule_id))))\n        except UnauthorizedException:\n            logger.warning('User unauthorized. Signing out...')\n            return signout(request)\n        except Exception as inst:\n            logger.error('Unexpected exception', exc_info=True)\n            messages.error(request, (inst.message if inst.message else 'Unexpected error'))\n            return HttpResponseRedirect(reverse('rule_details', args=(str(testplan_id), str(rule_id))))\n    data = {\n        'testplan': testplan,\n        'rule': rule,\n        'form': form,\n    }\n    return render(request, 'rules/rule_details.html', data)\n", "label": 1}
{"function": "\n\n@tornado.gen.coroutine\ndef handle_message(self, stream, header, payload):\n    '\\n        Handle incoming messages from underylying tcp streams\\n        '\n    try:\n        payload = self._decode_payload(payload)\n    except Exception:\n        stream.write(salt.transport.frame.frame_msg('bad load', header=header))\n        raise tornado.gen.Return()\n    if ((not isinstance(payload, dict)) or (not isinstance(payload.get('load'), dict))):\n        (yield stream.write(salt.transport.frame.frame_msg('payload and load must be a dict', header=header)))\n        raise tornado.gen.Return()\n    if ((payload['enc'] == 'clear') and (payload.get('load', {\n        \n    }).get('cmd') == '_auth')):\n        (yield stream.write(salt.transport.frame.frame_msg(self._auth(payload['load']), header=header)))\n        raise tornado.gen.Return()\n    try:\n        (ret, req_opts) = (yield self.payload_handler(payload))\n    except Exception as e:\n        stream.write('Some exception handling minion payload')\n        log.error('Some exception handling a payload from minion', exc_info=True)\n        stream.close()\n        raise tornado.gen.Return()\n    req_fun = req_opts.get('fun', 'send')\n    if (req_fun == 'send_clear'):\n        stream.write(salt.transport.frame.frame_msg(ret, header=header))\n    elif (req_fun == 'send'):\n        stream.write(salt.transport.frame.frame_msg(self.crypticle.dumps(ret), header=header))\n    elif (req_fun == 'send_private'):\n        stream.write(salt.transport.frame.frame_msg(self._encrypt_private(ret, req_opts['key'], req_opts['tgt']), header=header))\n    else:\n        log.error('Unknown req_fun {0}'.format(req_fun))\n        stream.write('Server-side exception handling payload')\n        stream.close()\n    raise tornado.gen.Return()\n", "label": 1}
{"function": "\n\ndef data_get(target, key, default=None):\n    '\\n    Get an item from a list, a dict or an object using \"dot\" notation.\\n\\n    :param target: The target element\\n    :type target: list or dict or object\\n\\n    :param key: The key to get\\n    :type key: string or list\\n\\n    :param default: The default value\\n    :type default: mixed\\n\\n    :rtype: mixed\\n    '\n    from ..support import Collection\n    if (key is None):\n        return target\n    if (not isinstance(key, list)):\n        key = key.split('.')\n    for segment in key:\n        if isinstance(target, (list, tuple)):\n            try:\n                target = target[segment]\n            except IndexError:\n                return value(default)\n        elif isinstance(target, dict):\n            try:\n                target = target[segment]\n            except IndexError:\n                return value(default)\n        elif isinstance(target, Collection):\n            try:\n                target = target[segment]\n            except IndexError:\n                return value(default)\n        else:\n            try:\n                target = getattr(target, segment)\n            except AttributeError:\n                return value(default)\n    return target\n", "label": 1}
{"function": "\n\ndef __handle_system_exit(self):\n    code = None\n    try:\n        if ((self.exception is None) or (self.exception.code is None)):\n            code = 0\n        else:\n            code = int(self.exception.code)\n    except ValueError:\n        code = 1\n    except AttributeError:\n        try:\n            code = int(self.exception)\n        except ValueError:\n            code = 1\n    except:\n        logger.exception(('Unknown exception: %s' % str(self.exception)))\n    if (code is not None):\n        if (code is not 0):\n            self._log_exception()\n        logger.debug('Application exiting with status %d', code)\n    else:\n        self._log_exception()\n    sys.exit(code)\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('case', [QtCore.Qt.CaseSensitive, QtCore.Qt.CaseInsensitive])\ndef test_subsequence_completer(case):\n    completer = SubsequenceCompleter()\n    words = ['actionA', 'actionB', 'setMySuperAction', 'geTToolTip', 'setStatusTip', 'seTToolTip']\n    model = QtGui.QStandardItemModel()\n    for word in words:\n        item = QtGui.QStandardItem()\n        item.setData(word, QtCore.Qt.DisplayRole)\n        model.appendRow(item)\n    completer.setCaseSensitivity(case)\n    completer.setModel(model)\n    if (case == QtCore.Qt.CaseInsensitive):\n        completer.setCompletionPrefix('tip')\n        completer.update_model()\n        assert (completer.completionCount() == 3)\n        completer.setCompletionPrefix('settip')\n        completer.update_model()\n        assert (completer.completionCount() == 2)\n        completer.update_model()\n        completer.setCompletionPrefix('action')\n        completer.update_model()\n        assert (completer.completionCount() == 3)\n    else:\n        completer.setCompletionPrefix('tip')\n        completer.update_model()\n        assert (completer.completionCount() == 1)\n        completer.setCompletionPrefix('Tip')\n        completer.update_model()\n        assert (completer.completionCount() == 3)\n        completer.setCompletionPrefix('setTip')\n        completer.update_model()\n        assert (completer.completionCount() == 1)\n        completer.setCompletionPrefix('action')\n        completer.update_model()\n        assert (completer.completionCount() == 2)\n", "label": 1}
{"function": "\n\n@property\ndef next_time(self):\n    '\\n        Return the OccurringRule or RecurringRule with the closest `dt_start` from now.\\n        '\n    now = timezone.now()\n    recurring_start = occurring_start = None\n    try:\n        occurring_rule = self.occurring_rule\n    except OccurringRule.DoesNotExist:\n        pass\n    else:\n        if (occurring_rule and (occurring_rule.dt_start > now)):\n            occurring_start = (occurring_rule.dt_start, occurring_rule)\n    rrules = self.recurring_rules.filter(finish__gt=now)\n    recurring_starts = [(rule.dt_start, rule) for rule in rrules if (rule.dt_start is not None)]\n    recurring_starts.sort(key=itemgetter(0))\n    try:\n        recurring_start = recurring_starts[0]\n    except IndexError:\n        pass\n    starts = [i for i in (recurring_start, occurring_start) if (i is not None)]\n    starts.sort(key=itemgetter(0))\n    try:\n        return starts[0][1]\n    except IndexError:\n        return None\n", "label": 1}
{"function": "\n\ndef _strip(self):\n    '\\n        Strip cards specific to a certain kind of header.\\n\\n        Strip cards like ``SIMPLE``, ``BITPIX``, etc. so the rest of\\n        the header can be used to reconstruct another kind of header.\\n        '\n    if ('NAXIS' in self):\n        naxis = self['NAXIS']\n    else:\n        naxis = 0\n    if ('TFIELDS' in self):\n        tfields = self['TFIELDS']\n    else:\n        tfields = 0\n    for idx in range(naxis):\n        try:\n            del self[('NAXIS' + str((idx + 1)))]\n        except KeyError:\n            pass\n    for name in ('TFORM', 'TSCAL', 'TZERO', 'TNULL', 'TTYPE', 'TUNIT', 'TDISP', 'TDIM', 'THEAP', 'TBCOL'):\n        for idx in range(tfields):\n            try:\n                del self[(name + str((idx + 1)))]\n            except KeyError:\n                pass\n    for name in ('SIMPLE', 'XTENSION', 'BITPIX', 'NAXIS', 'EXTEND', 'PCOUNT', 'GCOUNT', 'GROUPS', 'BSCALE', 'BZERO', 'TFIELDS'):\n        try:\n            del self[name]\n        except KeyError:\n            pass\n", "label": 1}
{"function": "\n\ndef test_init(self, dataFrame):\n    filterString = 'Foo < 10'\n    datasearch = DataSearch('Test', filterString)\n    assert (datasearch._filterString == filterString)\n    assert isinstance(datasearch._dataFrame, pandas.DataFrame)\n    assert (datasearch.name == 'Test')\n    datasearch = DataSearch('Test2')\n    assert (datasearch._filterString == '')\n    assert isinstance(datasearch._dataFrame, pandas.DataFrame)\n    assert (datasearch.name == 'Test2')\n    datasearch = DataSearch('Test3', dataFrame=dataFrame)\n    assert (datasearch._filterString == '')\n    assert isinstance(datasearch._dataFrame, pandas.DataFrame)\n    assert (datasearch.name == 'Test3')\n    assert (len(datasearch._dataFrame.index) == 3)\n", "label": 1}
{"function": "\n\ndef check_char_lookup(self, lookup):\n    lname = ('field__' + lookup)\n    mymodel = CharSetModel.objects.create(field={'mouldy', 'rotten'})\n    mouldy = CharSetModel.objects.filter(**{\n        lname: 'mouldy',\n    })\n    assert (mouldy.count() == 1)\n    assert (mouldy[0] == mymodel)\n    rotten = CharSetModel.objects.filter(**{\n        lname: 'rotten',\n    })\n    assert (rotten.count() == 1)\n    assert (rotten[0] == mymodel)\n    clean = CharSetModel.objects.filter(**{\n        lname: 'clean',\n    })\n    assert (clean.count() == 0)\n    with pytest.raises(ValueError):\n        list(CharSetModel.objects.filter(**{\n            lname: {'a', 'b'},\n        }))\n    both = CharSetModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) & Q(**{\n        lname: 'rotten',\n    })))\n    assert (both.count() == 1)\n    assert (both[0] == mymodel)\n    either = CharSetModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) | Q(**{\n        lname: 'clean',\n    })))\n    assert (either.count() == 1)\n    not_clean = CharSetModel.objects.exclude(**{\n        lname: 'clean',\n    })\n    assert (not_clean.count() == 1)\n    not_mouldy = CharSetModel.objects.exclude(**{\n        lname: 'mouldy',\n    })\n    assert (not_mouldy.count() == 0)\n", "label": 1}
{"function": "\n\ndef _write_todo(self, commits, *args, **kwargs):\n    todo_file = os.path.join(self.repo.git_dir, REBASE_EDITOR_TODO)\n    if os.path.exists(todo_file):\n        os.remove(todo_file)\n    if (not os.path.exists(os.path.dirname(todo_file))):\n        os.mkdir(os.path.dirname(todo_file))\n    onto = kwargs.get('onto', None)\n    for (idx, arg) in enumerate(args):\n        if arg.startswith('--onto'):\n            onto = (arg[7:] or args[(idx + 1)])\n            break\n    root = None\n    with codecs.open(todo_file, 'w', 'utf-8') as todo:\n        for commit in commits:\n            if (not root):\n                root = commit.parents[0]\n            subject = commit.message.splitlines()[0]\n            todo.write(('pick %s %s\\n' % (self._shorten(commit), subject)))\n        if (not root):\n            todo.write('noop\\n')\n        todo.write((TODO_EPILOGUE % {\n            'shortrevisions': ('%s..%s' % (self._shorten(root), self._shorten(commit))),\n            'shortonto': self._shorten((onto or root)),\n        }))\n    return todo_file\n", "label": 1}
{"function": "\n\ndef run_do(self, action):\n    ' Perform an api call with given parameters. '\n    catch = action.pop('catch', None)\n    self.assertEquals(1, len(action))\n    (method, args) = list(action.items())[0]\n    api = self.client\n    for m in method.split('.'):\n        self.assertTrue(hasattr(api, m))\n        api = getattr(api, m)\n    for k in PARAMS_RENAMES:\n        if (k in args):\n            args[PARAMS_RENAMES[k]] = args.pop(k)\n    for k in args:\n        args[k] = self._resolve(args[k])\n    try:\n        self.last_response = api(**args)\n    except Exception as e:\n        if (not catch):\n            raise\n        self.run_catch(catch, e)\n    else:\n        if catch:\n            raise AssertionError(('Failed to catch %r in %r.' % (catch, self.last_response)))\n", "label": 1}
{"function": "\n\ndef test_private_project(self):\n    cache_id = 'qwerty'\n    cached_image = 'kozmic-cache/{}'.format(cache_id)\n    try:\n        for image_data in docker.images(cached_image):\n            for repo_tag in image_data['RepoTags']:\n                if repo_tag.startswith(cached_image):\n                    docker.remove_image(image_data['Id'])\n                    break\n    except:\n        pass\n    assert (not docker.images(cached_image))\n    build = factories.BuildFactory.create(project=self.project, gh_commit_sha=self.prev_head_sha)\n    hook_call = factories.HookCallFactory.create(hook=self.hook, build=build)\n    job = self._do_job(hook_call)\n    assert (job.return_code == 0)\n    assert (job.stdout == 'Pulling \"{}\" Docker image...\\ninstalled!\\nit works\\nYEAH\\n'.format(self.hook.docker_image))\n    assert docker.images(cached_image)\n    build = factories.BuildFactory.create(project=self.project, gh_commit_sha=self.head_sha)\n    hook_call = factories.HookCallFactory.create(hook=self.hook, build=build)\n    job = self._do_job(hook_call)\n    assert (job.return_code == 0)\n    assert (job.stdout == 'Pulling \"{}\" Docker image...\\nSkipping install script as tracked files did not change...\\nit works\\nYEAH\\n'.format(self.hook.docker_image))\n", "label": 1}
{"function": "\n\ndef _traverse_repos(self, callback, repo_name=None):\n    '\\n        Traverse through all repo files and apply the functionality provided in\\n        the callback to them\\n        '\n    repo_files = []\n    if os.path.exists(self.opts['spm_repos_config']):\n        repo_files.append(self.opts['spm_repos_config'])\n    for (dirpath, dirnames, filenames) in os.walk('{0}.d'.format(self.opts['spm_repos_config'])):\n        for repo_file in filenames:\n            if (not repo_file.endswith('.repo')):\n                continue\n            repo_files.append(repo_file)\n    if (not os.path.exists(self.opts['spm_cache_dir'])):\n        os.makedirs(self.opts['spm_cache_dir'])\n    for repo_file in repo_files:\n        repo_path = '{0}.d/{1}'.format(self.opts['spm_repos_config'], repo_file)\n        with salt.utils.fopen(repo_path) as rph:\n            repo_data = yaml.safe_load(rph)\n            for repo in repo_data:\n                if (repo_data[repo].get('enabled', True) is False):\n                    continue\n                if ((repo_name is not None) and (repo != repo_name)):\n                    continue\n                callback(repo, repo_data[repo])\n", "label": 1}
{"function": "\n\ndef ListNames(self):\n    'List the names of all keys and values.'\n    if (not self.IsDirectory()):\n        return\n    if (self.hive is None):\n        for name in dir(_winreg):\n            if name.startswith('HKEY_'):\n                (yield name)\n        return\n    try:\n        with OpenKey(self.hive, self.local_path) as key:\n            (self.number_of_keys, self.number_of_values, self.last_modified) = QueryInfoKey(key)\n            self.last_modified = ((self.last_modified / 10000000) - WIN_UNIX_DIFF_MSECS)\n            for i in range(self.number_of_keys):\n                try:\n                    (yield EnumKey(key, i))\n                except exceptions.WindowsError:\n                    pass\n            for i in range(self.number_of_values):\n                try:\n                    (name, unused_value, unused_value_type) = EnumValue(key, i)\n                    (yield name)\n                except exceptions.WindowsError:\n                    pass\n    except exceptions.WindowsError as e:\n        raise IOError(('Unable to list key %s: %s' % (self.key_name, e)))\n", "label": 1}
{"function": "\n\ndef from_request(self, request, create=False):\n    'Get the contact from the session, else look up using the logged-in\\n        user. Create an unsaved new contact if `create` is true.\\n\\n        Returns:\\n        - Contact object or None\\n        '\n    contact = None\n    if request.user.is_authenticated():\n        try:\n            contact = Contact.objects.get(user=request.user.id)\n            request.session[CUSTOMER_ID] = contact.id\n        except Contact.DoesNotExist:\n            pass\n    else:\n        create = False\n    if request.session.get(CUSTOMER_ID):\n        try:\n            contactBySession = Contact.objects.get(id=request.session[CUSTOMER_ID])\n            if (contact is None):\n                contact = contactBySession\n            elif (contact != contactBySession):\n                log.debug(('CURIOUS: The user authenticated as %r (contact id:%r) and a session as %r (contact id:%r)' % (contact.user.get_full_name(), contact.id, Contact.objects.get(id=request.session[CUSTOMER_ID]).full_name, request.session[CUSTOMER_ID])))\n                log.debug('Deleting the session contact.')\n                del request.session[CUSTOMER_ID]\n        except Contact.DoesNotExist:\n            log.debug((\"This user has a session stored customer id (%r) which doesn't exist anymore. Removing it from the session.\" % request.session[CUSTOMER_ID]))\n            del request.session[CUSTOMER_ID]\n    if (contact is None):\n        if create:\n            contact = Contact(user=request.user)\n        else:\n            raise Contact.DoesNotExist()\n    return contact\n", "label": 1}
{"function": "\n\ndef test_index():\n    slt = SortedList(range(100), load=17)\n    for val in range(100):\n        assert (val == slt.index(val))\n    assert (slt.index(99, 0, 1000) == 99)\n    slt = SortedList((0 for rpt in range(100)), load=17)\n    for start in range(100):\n        for stop in range(start, 100):\n            assert (slt.index(0, start, (stop + 1)) == start)\n    for start in range(100):\n        assert (slt.index(0, (- (100 - start))) == start)\n    assert (slt.index(0, (- 1000)) == 0)\n", "label": 1}
{"function": "\n\ndef test_do_complicated(self):\n    ' Instantiate services from advanced YAML file '\n    config = get_config_yaml()\n    rslvr = resolver.Resolver(config)\n    services = rslvr.do()\n    assert isinstance(services['foo'], Foo)\n    assert isinstance(services['bar'], Bar)\n    assert isinstance(services['baz'], Baz)\n    assert isinstance(services['qux'], Qux)\n    assert isinstance(services['wobble'], Wobble)\n    assert isinstance(services['spam'], Spam)\n    spam = services['spam']\n    self.assertEquals(spam.ham, 'ham!')\n    self.assertEquals(spam.eggs, 'eggz')\n    wobble = services['wobble']\n    assert isinstance(wobble.foo, Foo)\n    assert isinstance(wobble.bar, Bar)\n    assert isinstance(wobble.baz, Baz)\n    assert isinstance(wobble.spam, Spam)\n", "label": 1}
{"function": "\n\ndef create_generator_test(self):\n    'Test the parameter generation function.'\n    precision = (10 ** (- 2))\n    values_one = [(i * precision) for i in xrange(1, 100)]\n    values_two = [(i * precision) for i in xrange(201)]\n    generator_one = GridSearch(SMAPE, precision=(- 2))._generate_next_parameter_value('parameter_one', self.bfm)\n    generator_two = GridSearch(SMAPE, precision=(- 2))._generate_next_parameter_value('parameter_two', self.bfm)\n    generated_one = [val for val in generator_one]\n    generated_two = [val for val in generator_two]\n    assert (len(values_one) == len(generated_one))\n    assert (len(values_two) == len(generated_two))\n    for idx in xrange(len(values_one)):\n        value = str(values_one[idx])[:12]\n        assert (str(value) == str(generated_one[idx])[:len(value)])\n    for idx in xrange(len(values_two)):\n        value = str(values_two[idx])[:12]\n        assert (str(value) == str(generated_two[idx])[:len(value)])\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    startLoc = loc\n    instrlen = len(instring)\n    expr = self.expr\n    failParse = False\n    while (loc <= instrlen):\n        try:\n            if self.failOn:\n                try:\n                    self.failOn.tryParse(instring, loc)\n                except ParseBaseException:\n                    pass\n                else:\n                    failParse = True\n                    raise ParseException(instring, loc, ('Found expression ' + str(self.failOn)))\n                failParse = False\n            if (self.ignoreExpr is not None):\n                while 1:\n                    try:\n                        loc = self.ignoreExpr.tryParse(instring, loc)\n                    except ParseBaseException:\n                        break\n            expr._parse(instring, loc, doActions=False, callPreParse=False)\n            skipText = instring[startLoc:loc]\n            if self.includeMatch:\n                (loc, mat) = expr._parse(instring, loc, doActions, callPreParse=False)\n                if mat:\n                    skipRes = ParseResults(skipText)\n                    skipRes += mat\n                    return (loc, [skipRes])\n                else:\n                    return (loc, [skipText])\n            else:\n                return (loc, [skipText])\n        except (ParseException, IndexError):\n            if failParse:\n                raise\n            else:\n                loc += 1\n    raise ParseException(instring, loc, self.errmsg, self)\n", "label": 1}
{"function": "\n\ndef make_app(root, **kw):\n    '\\n    Utility for creating the Pecan application object.  This function should\\n    generally be called from the ``setup_app`` function in your project\\'s\\n    ``app.py`` file.\\n\\n    :param root: A string representing a root controller object (e.g.,\\n                 \"myapp.controller.root.RootController\")\\n    :param static_root: The relative path to a directory containing static\\n                        files.  Serving static files is only enabled when\\n                        debug mode is set.\\n    :param debug: A flag to enable debug mode.  This enables the debug\\n                  middleware and serving static files.\\n    :param wrap_app: A function or middleware class to wrap the Pecan app.\\n                     This must either be a wsgi middleware class or a\\n                     function that returns a wsgi application. This wrapper\\n                     is applied first before wrapping the application in\\n                     other middlewares such as Pecan\\'s debug middleware.\\n                     This should be used if you want to use middleware to\\n                     perform authentication or intercept all requests before\\n                     they are routed to the root controller.\\n    :param logging: A dictionary used to configure logging.  This uses\\n                    ``logging.config.dictConfig``.\\n\\n    All other keyword arguments are passed in to the Pecan app constructor.\\n\\n    :returns: a ``Pecan`` object.\\n    '\n    logging = kw.get('logging', {\n        \n    })\n    debug = kw.get('debug', False)\n    if logging:\n        if debug:\n            try:\n                from logging import captureWarnings\n                captureWarnings(True)\n                warnings.simplefilter('default', DeprecationWarning)\n            except ImportError:\n                pass\n        if isinstance(logging, Config):\n            logging = logging.to_dict()\n        if ('version' not in logging):\n            logging['version'] = 1\n        load_logging_config(logging)\n    app = Pecan(root, **kw)\n    wrap_app = kw.get('wrap_app', None)\n    if wrap_app:\n        app = wrap_app(app)\n    errors = kw.get('errors', getattr(conf.app, 'errors', {\n        \n    }))\n    if errors:\n        app = middleware.errordocument.ErrorDocumentMiddleware(app, errors)\n    app = middleware.recursive.RecursiveMiddleware(app)\n    static_root = kw.get('static_root', None)\n    if debug:\n        debug_kwargs = getattr(conf, 'debug', {\n            \n        })\n        debug_kwargs.setdefault('context_injectors', []).append((lambda environ: {\n            'request': environ.get('pecan.locals', {\n                \n            }).get('request'),\n        }))\n        app = DebugMiddleware(app, **debug_kwargs)\n        if static_root:\n            app = middleware.static.StaticFileMiddleware(app, static_root)\n    elif static_root:\n        warnings.warn('`static_root` is only used when `debug` is True, ignoring', RuntimeWarning)\n    if hasattr(conf, 'requestviewer'):\n        warnings.warn(''.join(['`pecan.conf.requestviewer` is deprecated.  To apply the ', '`RequestViewerHook` to your application, add it to ', \"`pecan.conf.app.hooks` or manually in your project's `app.py` \", 'file.']), DeprecationWarning)\n    return app\n", "label": 1}
{"function": "\n\ndef wait(self, timeout=None):\n    \"Must be used with 'yield' as 'yield cv.wait()'.\\n        \"\n    coro = AsynCoro.cur_coro()\n    if (self._owner != coro):\n        raise RuntimeError(('\"%s\"/%s: invalid lock release - owned by \"%s\"/%s' % (coro._name, coro._id, self._owner._name, self._owner._id)))\n    assert (self._depth > 0)\n    depth = self._depth\n    self._depth = 0\n    self._owner = None\n    if self._waitlist:\n        wake = self._waitlist.pop(0)\n        wake._proceed_(True)\n    self._notifylist.append(coro)\n    start = _time()\n    if ((yield coro._await_(timeout)) is None):\n        try:\n            self._notifylist.remove(coro)\n        except ValueError:\n            pass\n        raise StopIteration(False)\n    while (self._owner is not None):\n        self._waitlist.insert(0, coro)\n        if (timeout is not None):\n            timeout -= (_time() - start)\n            if (timeout <= 0):\n                raise StopIteration(False)\n            start = _time()\n        if ((yield coro._await_(timeout)) is None):\n            try:\n                self._waitlist.remove(coro)\n            except ValueError:\n                pass\n            raise StopIteration(False)\n    assert (self._depth == 0)\n    self._owner = coro\n    self._depth = depth\n    raise StopIteration(True)\n", "label": 1}
{"function": "\n\ndef filter_by(lookup_dict, grain='os_family', merge=None, default='default', base=None):\n    '\\n    .. versionadded:: 0.17.0\\n\\n    Look up the given grain in a given dictionary for the current OS and return\\n    the result\\n\\n    Although this may occasionally be useful at the CLI, the primary intent of\\n    this function is for use in Jinja to make short work of creating lookup\\n    tables for OS-specific data. For example:\\n\\n    .. code-block:: jinja\\n\\n        {% set apache = salt[\\'grains.filter_by\\']({\\n            \\'Debian\\': {\\'pkg\\': \\'apache2\\', \\'srv\\': \\'apache2\\'},\\n            \\'RedHat\\': {\\'pkg\\': \\'httpd\\', \\'srv\\': \\'httpd\\'},\\n        }, default=\\'Debian\\') %}\\n\\n        myapache:\\n          pkg.installed:\\n            - name: {{ apache.pkg }}\\n          service.running:\\n            - name: {{ apache.srv }}\\n\\n    Values in the lookup table may be overridden by values in Pillar. An\\n    example Pillar to override values in the example above could be as follows:\\n\\n    .. code-block:: yaml\\n\\n        apache:\\n          lookup:\\n            pkg: apache_13\\n            srv: apache\\n\\n    The call to ``filter_by()`` would be modified as follows to reference those\\n    Pillar values:\\n\\n    .. code-block:: jinja\\n\\n        {% set apache = salt[\\'grains.filter_by\\']({\\n            ...\\n        }, merge=salt[\\'pillar.get\\'](\\'apache:lookup\\')) %}\\n\\n\\n    :param lookup_dict: A dictionary, keyed by a grain, containing a value or\\n        values relevant to systems matching that grain. For example, a key\\n        could be the grain for an OS and the value could the name of a package\\n        on that particular OS.\\n    :param grain: The name of a grain to match with the current system\\'s\\n        grains. For example, the value of the \"os_family\" grain for the current\\n        system could be used to pull values from the ``lookup_dict``\\n        dictionary.\\n    :param merge: A dictionary to merge with the results of the grain selection\\n        from ``lookup_dict``. This allows Pillar to override the values in the\\n        ``lookup_dict``. This could be useful, for example, to override the\\n        values for non-standard package names such as when using a different\\n        Python version from the default Python version provided by the OS\\n        (e.g., ``python26-mysql`` instead of ``python-mysql``).\\n    :param default: default lookup_dict\\'s key used if the grain does not exists\\n        or if the grain value has no match on lookup_dict.  If unspecified\\n        the value is \"default\".\\n\\n        .. versionadded:: 2014.1.0\\n\\n    :param base: A lookup_dict key to use for a base dictionary.  The\\n        grain-selected ``lookup_dict`` is merged over this and then finally\\n        the ``merge`` dictionary is merged.  This allows common values for\\n        each case to be collected in the base and overridden by the grain\\n        selection dictionary and the merge dictionary.  Default is unset.\\n\\n        .. versionadded:: 2015.5.0\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt \\'*\\' grains.filter_by \\'{Debian: Debheads rule, RedHat: I love my hat}\\'\\n        # this one will render {D: {E: I, G: H}, J: K}\\n        salt \\'*\\' grains.filter_by \\'{A: B, C: {D: {E: F,G: H}}}\\' \\'xxx\\' \\'{D: {E: I},J: K}\\' \\'C\\'\\n        # next one renders {A: {B: G}, D: J}\\n        salt \\'*\\' grains.filter_by \\'{default: {A: {B: C}, D: E}, F: {A: {B: G}}, H: {D: I}}\\' \\'xxx\\' \\'{D: J}\\' \\'F\\' \\'default\\'\\n        # next same as above when default=\\'H\\' instead of \\'F\\' renders {A: {B: C}, D: J}\\n    '\n    ret = lookup_dict.get(salt.utils.traverse_dict_and_list(__grains__, grain, None), lookup_dict.get(default, None))\n    if (base and (base in lookup_dict)):\n        base_values = lookup_dict[base]\n        if (ret is None):\n            ret = base_values\n        elif isinstance(base_values, collections.Mapping):\n            if (not isinstance(ret, collections.Mapping)):\n                raise SaltException('filter_by default and look-up values must both be dictionaries.')\n            ret = salt.utils.dictupdate.update(copy.deepcopy(base_values), ret)\n    if merge:\n        if (not isinstance(merge, collections.Mapping)):\n            raise SaltException('filter_by merge argument must be a dictionary.')\n        if (ret is None):\n            ret = merge\n        else:\n            salt.utils.dictupdate.update(ret, copy.deepcopy(merge))\n    return ret\n", "label": 1}
{"function": "\n\ndef test_link_str():\n    'Links should have sensible names'\n    d = Data(x=[1, 2, 3], y=[2, 3, 4])\n    x = d.id['x']\n    y = d.id['y']\n    assert (str((x + y)) == '(x + y)')\n    assert (str((x - y)) == '(x - y)')\n    assert (str((x * y)) == '(x * y)')\n    assert (str((x / y)) == '(x / y)')\n    assert (str((x ** y)) == '(x ** y)')\n    assert (str((x ** 3)) == '(x ** 3)')\n    assert (str((3 + (x * y))) == '(3 + (x * y))')\n    assert (str(((x + x) + y)) == '((x + x) + y)')\n    assert (repr((x + y)) == '<BinaryComponentLink: (x + y)>')\n", "label": 1}
{"function": "\n\ndef test_automatic_symbols():\n    if (not readline):\n        return None\n    app = init_ipython_session()\n    app.run_cell('from sympy import *')\n    enable_automatic_symbols(app)\n    symbol = 'verylongsymbolname'\n    assert (symbol not in app.user_ns)\n    app.run_cell(('a = %s' % symbol), True)\n    assert (symbol not in app.user_ns)\n    app.run_cell(('a = type(%s)' % symbol), True)\n    assert (app.user_ns['a'] == Symbol)\n    app.run_cell((\"%s = Symbol('%s')\" % (symbol, symbol)), True)\n    assert (symbol in app.user_ns)\n    app.run_cell('a = all == __builtin__.all', True)\n    assert ('all' not in app.user_ns)\n    assert (app.user_ns['a'] is True)\n    app.run_cell('import sympy')\n    app.run_cell('a = factorial == sympy.factorial', True)\n    assert (app.user_ns['a'] is True)\n", "label": 1}
{"function": "\n\ndef do_idpsso_descriptor(conf, cert=None):\n    idpsso = md.IDPSSODescriptor()\n    idpsso.protocol_support_enumeration = samlp.NAMESPACE\n    endps = conf.getattr('endpoints', 'idp')\n    if endps:\n        for (endpoint, instlist) in do_endpoints(endps, ENDPOINTS['idp']).items():\n            setattr(idpsso, endpoint, instlist)\n    _do_nameid_format(idpsso, conf, 'idp')\n    scopes = conf.getattr('scope', 'idp')\n    if scopes:\n        if (idpsso.extensions is None):\n            idpsso.extensions = md.Extensions()\n        for scope in scopes:\n            mdscope = shibmd.Scope()\n            mdscope.text = scope\n            mdscope.regexp = 'false'\n            idpsso.extensions.add_extension_element(mdscope)\n    ui_info = conf.getattr('ui_info', 'idp')\n    if ui_info:\n        if (idpsso.extensions is None):\n            idpsso.extensions = md.Extensions()\n        idpsso.extensions.add_extension_element(do_uiinfo(ui_info))\n    if cert:\n        idpsso.key_descriptor = do_key_descriptor(cert)\n    for key in ['want_authn_requests_signed']:\n        try:\n            val = conf.getattr(key, 'idp')\n            if (val is None):\n                setattr(idpsso, key, DEFAULT[key])\n            else:\n                setattr(idpsso, key, ('%s' % val).lower())\n        except KeyError:\n            setattr(idpsso, key, DEFAULTS[key])\n    return idpsso\n", "label": 1}
{"function": "\n\ndef _Dynamic_Put(self, put_request, put_response):\n    if put_request.has_transaction():\n        entities = put_request.entity_list()\n        requires_id = (lambda x: ((x.id() == 0) and (not x.has_name())))\n        new_ents = [e for e in entities if requires_id(e.key().path().element_list()[(- 1)])]\n        id_request = datastore_pb.PutRequest()\n        txid = put_request.transaction().handle()\n        txdata = self.__transactions[txid]\n        assert (txdata.thread_id == thread.get_ident()), 'Transactions are single-threaded.'\n        if new_ents:\n            for ent in new_ents:\n                e = id_request.add_entity()\n                e.mutable_key().CopyFrom(ent.key())\n                e.mutable_entity_group()\n            id_response = datastore_pb.PutResponse()\n            if txdata.is_xg:\n                rpc_name = 'GetIDsXG'\n            else:\n                rpc_name = 'GetIDs'\n            super(RemoteDatastoreStub, self).MakeSyncCall('remote_datastore', rpc_name, id_request, id_response)\n            assert (id_request.entity_size() == id_response.key_size())\n            for (key, ent) in zip(id_response.key_list(), new_ents):\n                ent.mutable_key().CopyFrom(key)\n                ent.mutable_entity_group().add_element().CopyFrom(key.path().element(0))\n        for entity in entities:\n            txdata.entities[entity.key().Encode()] = (entity.key(), entity)\n            put_response.add_key().CopyFrom(entity.key())\n    else:\n        super(RemoteDatastoreStub, self).MakeSyncCall('datastore_v3', 'Put', put_request, put_response)\n", "label": 1}
{"function": "\n\ndef __init__(self, request, procedure, match=None, invoke=None):\n    '\\n\\n        :param request: The WAMP request ID of this request.\\n        :type request: int\\n        :param procedure: The WAMP or application URI of the RPC endpoint provided.\\n        :type procedure: unicode\\n        :param match: The procedure matching policy to be used for the registration.\\n        :type match: unicode\\n        :param invoke: The procedure invocation policy to be used for the registration.\\n        :type invoke: unicode\\n        '\n    assert (type(request) in six.integer_types)\n    assert (type(procedure) == six.text_type)\n    assert ((match is None) or (type(match) == six.text_type))\n    assert ((match is None) or (match in [Register.MATCH_EXACT, Register.MATCH_PREFIX, Register.MATCH_WILDCARD]))\n    assert ((invoke is None) or (type(invoke) == six.text_type))\n    assert ((invoke is None) or (invoke in [Register.INVOKE_SINGLE, Register.INVOKE_FIRST, Register.INVOKE_LAST, Register.INVOKE_ROUNDROBIN, Register.INVOKE_RANDOM]))\n    Message.__init__(self)\n    self.request = request\n    self.procedure = procedure\n    self.match = (match or Register.MATCH_EXACT)\n    self.invoke = (invoke or Register.INVOKE_SINGLE)\n", "label": 1}
{"function": "\n\ndef attribute(tokeniser):\n    start = tokeniser()\n    if (start != '['):\n        raise ValueError('invalid attribute, does not starts with [')\n    code = tokeniser().lower()\n    if (not code.startswith('0x')):\n        raise ValueError('invalid attribute, code is not 0x hexadecimal')\n    try:\n        code = int(code, 16)\n    except ValueError:\n        raise ValueError('invalid attribute, code is not 0x hexadecimal')\n    flag = tokeniser().lower()\n    if (not flag.startswith('0x')):\n        raise ValueError('invalid attribute, flag is not 0x hexadecimal')\n    try:\n        flag = int(flag, 16)\n    except ValueError:\n        raise ValueError('invalid attribute, flag is not 0x hexadecimal')\n    data = tokeniser().lower()\n    if (not data.startswith('0x')):\n        raise ValueError('invalid attribute, data is not 0x hexadecimal')\n    if (len(data) % 2):\n        raise ValueError('invalid attribute, data is not 0x hexadecimal')\n    data = ''.join((chr(int(data[_:(_ + 2)], 16)) for _ in range(2, len(data), 2)))\n    end = tokeniser()\n    if (end != ']'):\n        raise ValueError('invalid attribute, does not ends with ]')\n    return GenericAttribute(code, flag, data)\n", "label": 1}
{"function": "\n\ndef CreateIssue(self, diffbase, description):\n    'Creates a new code review issue.\\n\\n    Args:\\n      diffbase: string containing the diffbase.\\n      description: string containing the description.\\n\\n    Returns:\\n      An integer containing the code review number or None.\\n    '\n    reviewers = list(self._REVIEWERS)\n    reviewers_cc = list(self._REVIEWERS_CC)\n    try:\n        reviewers.remove(self._email_address)\n    except ValueError:\n        pass\n    try:\n        reviewers_cc.remove(self._email_address)\n    except ValueError:\n        pass\n    reviewers = ','.join(reviewers)\n    reviewers_cc = ','.join(reviewers_cc)\n    command = '{0:s} {1:s} --oauth2'.format(sys.executable, self._upload_py_path)\n    if self._no_browser:\n        command = '{0:s} --no_oauth2_webbrowser'.format(command)\n    command = '{0:s} --send_mail -r {1:s} --cc {2:s} -t \"{3:s}\" -y -- {4:s}'.format(command, reviewers, reviewers_cc, description, diffbase)\n    if self._no_browser:\n        print('Upload server: codereview.appspot.com (change with -s/--server)\\nGo to the following link in your browser:\\n\\n    https://codereview.appspot.com/get-access-token\\n\\nand copy the access token.\\n\\n')\n        print('Enter access token:', end=' ')\n        sys.stdout.flush()\n    (exit_code, output, _) = self.RunCommand(command)\n    print(output)\n    if (exit_code != 0):\n        return\n    issue_url_line_start = 'Issue created. URL: http://codereview.appspot.com/'\n    for line in output.split(b'\\n'):\n        if (issue_url_line_start in line):\n            (_, _, issue_number) = line.rpartition(issue_url_line_start)\n            try:\n                return int(issue_number, 10)\n            except ValueError:\n                pass\n", "label": 1}
{"function": "\n\ndef _add_subtree(self, level, shadow, parent_item, key, node):\n    if (level >= self.levels):\n        values = [('' if (_key == 'icon') else str(node[_key])) for _key in self.datakeys]\n        try:\n            bnch = shadow[key]\n            item = bnch.item\n        except KeyError:\n            item = QtGui.QTreeWidgetItem(parent_item, values)\n            if (level == 1):\n                parent_item.addTopLevelItem(item)\n            else:\n                parent_item.addChild(item)\n            shadow[key] = Bunch.Bunch(node=node, item=item, terminal=True)\n            if ('icon' in node):\n                i = self.datakeys.index('icon')\n                item.setIcon(i, node['icon'])\n            item.setFlags((item.flags() & (~ QtCore.Qt.ItemIsEditable)))\n    else:\n        try:\n            bnch = shadow[key]\n            item = bnch.item\n            d = bnch.node\n        except KeyError:\n            item = QtGui.QTreeWidgetItem(parent_item, [str(key)])\n            if (level == 1):\n                parent_item.addTopLevelItem(item)\n            else:\n                parent_item.addChild(item)\n            d = {\n                \n            }\n            shadow[key] = Bunch.Bunch(node=d, item=item, terminal=False)\n        for key in node:\n            self._add_subtree((level + 1), d, item, key, node[key])\n", "label": 1}
{"function": "\n\ndef get(self, element):\n    'Retrieve multiple elements.\\n\\n        The elements are specified by (nodes|ways|relations) parameter\\n        to the request, as a comma separated list of element IDs.\\n        '\n    if (element not in [C.NODES, C.WAYS, C.RELATIONS]):\n        raise tornado.web.HTTPError(500)\n    if (element == C.NODES):\n        namespace = C.NODE\n    elif (element == C.WAYS):\n        namespace = C.WAY\n    elif (element == C.RELATIONS):\n        namespace = C.RELATION\n    else:\n        assert False, (\"Unexpected element '%s'\" % element)\n    params = self.get_argument(element, None)\n    if (not params):\n        raise tornado.web.HTTPError(400)\n    osm = new_osm_response()\n    for (st, r) in self.datastore.fetch_keys(namespace, params.split(',')):\n        if st:\n            r.build_response(osm)\n    self.set_header(C.CONTENT_TYPE, C.TEXT_XML)\n    self.write(response_to_xml(osm))\n", "label": 1}
{"function": "\n\ndef raw_call(self, method, path, content=None):\n    'Sign a given query and return its result.\\n\\n        :param method: the HTTP method of the request (get/post/put/delete)\\n        :param path: the url you want to request\\n        :param content: the object you want to send in your request\\n         (will be automatically serialized to JSON)\\n\\n        :raises APIError: Error send by api\\n        '\n    target_url = (self.base_url + path)\n    now = str((int(time.time()) + self.time_delta()))\n    body = ''\n    if content:\n        body = json.dumps(content)\n    if (not self.consumer_key):\n        raise BadParametersError(msg='Cannot call API withoutConsumer Key')\n    s1 = hashlib.sha1()\n    s1.update('+'.join([self.application_secret, self.consumer_key, method.upper(), target_url, body, now]).encode())\n    sig = ('$1$' + s1.hexdigest())\n    query_headers = {\n        'X-Ra-Application': self.application_key,\n        'X-Ra-Timestamp': now,\n        'X-Ra-Consumer': self.consumer_key,\n        'X-Ra-Signature': sig,\n        'Content-type': 'application/json',\n    }\n    req = getattr(requests, method.lower())\n    result = req(target_url, headers=query_headers, data=body)\n    if result.text:\n        try:\n            json_result = json.loads(result.text)\n        except ValueError:\n            raise APIError('API response is not valid')\n    else:\n        json_result = {\n            \n        }\n    if (result.status_code == 404):\n        raise ResourceNotFoundError(msg=json_result.get('message'))\n    if (result.status_code == 400):\n        raise BadParametersError(msg=json_result.get('message'))\n    if (result.status_code == 409):\n        raise ResourceAlreadyExistsError(msg=json_result.get('message'))\n    if (result.status_code == 0):\n        raise NetworkError()\n    if ((result.status_code < 100) or (result.status_code >= 300)):\n        raise APIError(msg=json_result.get('message'))\n    return json_result\n", "label": 1}
{"function": "\n\ndef find_argumented_step_fixture_name(name, type_, fixturemanager, request=None):\n    'Find argumented step fixture name.'\n    for (fixturename, fixturedefs) in list(fixturemanager._arg2fixturedefs.items()):\n        for fixturedef in fixturedefs:\n            parser = getattr(fixturedef.func, 'parser', None)\n            match = (parser.is_matching(name) if parser else None)\n            if match:\n                converters = getattr(fixturedef.func, 'converters', {\n                    \n                })\n                for (arg, value) in parser.parse_arguments(name).items():\n                    if (arg in converters):\n                        value = converters[arg](value)\n                    if request:\n                        inject_fixture(request, arg, value)\n                parser_name = get_step_fixture_name(parser.name, type_)\n                if request:\n                    try:\n                        request.getfuncargvalue(parser_name)\n                    except python.FixtureLookupError:\n                        continue\n                return parser_name\n", "label": 1}
{"function": "\n\ndef norm(ctx, x, p=2):\n    \"\\n        Gives the entrywise `p`-norm of an iterable *x*, i.e. the vector norm\\n        `\\\\left(\\\\sum_k |x_k|^p\\\\right)^{1/p}`, for any given `1 \\\\le p \\\\le \\\\infty`.\\n\\n        Special cases:\\n\\n        If *x* is not iterable, this just returns ``absmax(x)``.\\n\\n        ``p=1`` gives the sum of absolute values.\\n\\n        ``p=2`` is the standard Euclidean vector norm.\\n\\n        ``p=inf`` gives the magnitude of the largest element.\\n\\n        For *x* a matrix, ``p=2`` is the Frobenius norm.\\n        For operator matrix norms, use :func:`~mpmath.mnorm` instead.\\n\\n        You can use the string 'inf' as well as float('inf') or mpf('inf')\\n        to specify the infinity norm.\\n\\n        **Examples**\\n\\n            >>> from mpmath import *\\n            >>> mp.dps = 15; mp.pretty = False\\n            >>> x = matrix([-10, 2, 100])\\n            >>> norm(x, 1)\\n            mpf('112.0')\\n            >>> norm(x, 2)\\n            mpf('100.5186549850325')\\n            >>> norm(x, inf)\\n            mpf('100.0')\\n\\n        \"\n    try:\n        iter(x)\n    except TypeError:\n        return ctx.absmax(x)\n    if (type(p) is not int):\n        p = ctx.convert(p)\n    if (p == ctx.inf):\n        return max((ctx.absmax(i) for i in x))\n    elif (p == 1):\n        return ctx.fsum(x, absolute=1)\n    elif (p == 2):\n        return ctx.sqrt(ctx.fsum(x, absolute=1, squared=1))\n    elif (p > 1):\n        return ctx.nthroot(ctx.fsum(((abs(i) ** p) for i in x)), p)\n    else:\n        raise ValueError('p has to be >= 1')\n", "label": 1}
{"function": "\n\ndef expect_loop(self, timeout=(- 1)):\n    'Blocking expect'\n    spawn = self.spawn\n    if (timeout is not None):\n        end_time = (time.time() + timeout)\n    try:\n        incoming = spawn.buffer\n        spawn.buffer = spawn.string_type()\n        while True:\n            idx = self.new_data(incoming)\n            if (idx is not None):\n                return idx\n            if ((timeout is not None) and (timeout < 0)):\n                return self.timeout()\n            incoming = spawn.read_nonblocking(spawn.maxread, timeout)\n            time.sleep(0.0001)\n            if (timeout is not None):\n                timeout = (end_time - time.time())\n    except EOF as e:\n        return self.eof(e)\n    except TIMEOUT as e:\n        return self.timeout(e)\n    except:\n        self.errored()\n        raise\n", "label": 1}
{"function": "\n\ndef run(self, edit, command_line=''):\n    if (not command_line):\n        raise ValueError('no command line passed; that seems wrong')\n    parsed = parse_command_line(command_line)\n    pattern = parsed.command.pattern\n    replacement = parsed.command.replacement\n    count = parsed.command.count\n    flags = parsed.command.flags\n    if (not pattern):\n        pattern = ExSubstitute.last_pattern\n        replacement = ExSubstitute.last_replacement\n        flags = []\n        count = 0\n    if (not pattern):\n        sublime.status_message('Vintageous: no previous pattern available')\n        print('Vintageous: no previous pattern available')\n        return\n    ExSubstitute.last_pattern = pattern\n    ExSubstitute.last_replacement = replacement\n    ExSubstitute.last_flags = flags\n    computed_flags = re.MULTILINE\n    computed_flags |= (re.IGNORECASE if ('i' in flags) else 0)\n    try:\n        compiled_rx = re.compile(pattern, flags=computed_flags)\n    except Exception as e:\n        sublime.status_message((\"Vintageous: bad pattern '%s'\" % (e.message, pattern)))\n        print((\"Vintageous [regex error]: %s ... in pattern '%s'\" % (e.message, pattern)))\n        return\n    replace_count = (0 if (flags and ('g' in flags)) else 1)\n    target_region = parsed.line_range.resolve(self.view)\n    if ('c' in flags):\n        self.replace_confirming(edit, pattern, compiled_rx, replacement, replace_count, target_region)\n        return\n    line_text = self.view.substr(target_region)\n    new_text = re.sub(compiled_rx, replacement, line_text, count=replace_count)\n    self.view.replace(edit, target_region, new_text)\n", "label": 1}
{"function": "\n\n@register.simple_tag\ndef render_case(case, options):\n    \"\\n    Uses options since Django 1.3 doesn't seem to support templatetag kwargs.\\n    Change to kwargs when we're on a version of Django that does.\\n    \"\n    from corehq.apps.hqwebapp.templatetags.proptable_tags import get_tables_as_rows, get_default_definition\n    wrapped_case = get_wrapped_case(case)\n    timezone = options.get('timezone', pytz.utc)\n    timezone = timezone.localize(datetime.datetime.utcnow()).tzinfo\n    _get_tables_as_rows = partial(get_tables_as_rows, timezone=timezone)\n    display = (options.get('display') or wrapped_case.get_display_config())\n    show_transaction_export = (options.get('show_transaction_export') or False)\n    get_case_url = options['get_case_url']\n    data = copy.deepcopy(wrapped_case.to_full_dict())\n    default_properties = _get_tables_as_rows(data, display)\n    dynamic_data = wrapped_case.dynamic_properties()\n    for section in display:\n        for row in section['layout']:\n            for item in row:\n                dynamic_data.pop(item.get('expr'), None)\n    if dynamic_data:\n        dynamic_keys = sorted(dynamic_data.keys())\n        definition = get_default_definition(dynamic_keys, num_columns=DYNAMIC_CASE_PROPERTIES_COLUMNS)\n        dynamic_properties = _get_tables_as_rows(dynamic_data, definition)\n    else:\n        dynamic_properties = None\n    the_time_is_now = datetime.datetime.utcnow()\n    tz_offset_ms = (int(timezone.utcoffset(the_time_is_now).total_seconds()) * 1000)\n    tz_abbrev = timezone.localize(the_time_is_now).tzname()\n\n    def _product_name(product_id):\n        try:\n            return SQLProduct.objects.get(product_id=product_id).name\n        except SQLProduct.DoesNotExist:\n            return _('Unknown Product (\"{}\")').format(product_id)\n    ledgers = get_current_ledger_transactions(case.case_id)\n    for (section, product_map) in ledgers.items():\n        product_tuples = sorted(((_product_name(product_id), product_map[product_id]) for product_id in product_map))\n        ledgers[section] = product_tuples\n    return render_to_string('case/partials/single_case.html', {\n        'default_properties': default_properties,\n        'default_properties_options': {\n            'style': 'table',\n        },\n        'dynamic_properties': dynamic_properties,\n        'dynamic_properties_options': {\n            'style': 'table',\n        },\n        'case': wrapped_case.case,\n        'case_actions': mark_safe(json.dumps(wrapped_case.actions())),\n        'timezone': timezone,\n        'tz_abbrev': tz_abbrev,\n        'case_hierarchy_options': {\n            'show_view_buttons': True,\n            'get_case_url': get_case_url,\n            'timezone': timezone,\n        },\n        'ledgers': ledgers,\n        'timezone_offset': tz_offset_ms,\n        'show_transaction_export': show_transaction_export,\n        'xform_api_url': reverse('single_case_forms', args=[case.domain, case.case_id]),\n    })\n", "label": 1}
{"function": "\n\ndef os_installed(host_id, clusterhosts_ready, clusters_os_ready, username=None):\n    'Callback when os is installed.\\n\\n    :param host_id: host that os is installed.\\n    :type host_id: integer\\n    :param clusterhosts_ready: the clusterhosts that should trigger ready.\\n    :param clusters_os_ready: the cluster that should trigger os ready.\\n\\n    .. note::\\n        The function should be called out of database session.\\n    '\n    with util.lock('serialized_action') as lock:\n        if (not lock):\n            raise Exception('failed to acquire lock to do the post action after os installation')\n        logging.info('os installed on host %s with cluster host ready %s cluster os ready %s', host_id, clusterhosts_ready, clusters_os_ready)\n        if username:\n            user = user_db.get_user_object(username)\n        else:\n            user = None\n        os_installed_triggered = False\n        for (cluster_id, clusterhost_ready) in clusterhosts_ready.items():\n            if ((not clusterhost_ready) and os_installed_triggered):\n                continue\n            cluster_info = util.ActionHelper.get_cluster_info(cluster_id, user)\n            adapter_id = cluster_info[const.ADAPTER_ID]\n            adapter_info = util.ActionHelper.get_adapter_info(adapter_id, cluster_id, user)\n            hosts_info = util.ActionHelper.get_hosts_info(cluster_id, [host_id], user)\n            deploy_manager = DeployManager(adapter_info, cluster_info, hosts_info)\n            if (not os_installed_triggered):\n                deploy_manager.os_installed()\n                util.ActionHelper.host_ready(host_id, True, user)\n                os_installed_triggered = True\n            if clusterhost_ready:\n                util.ActionHelper.cluster_host_ready(cluster_id, host_id, False, user)\n            if util.ActionHelper.is_cluster_os_ready(cluster_id, user):\n                logging.info('deploy_manager begin cluster_os_installed')\n                deploy_manager.cluster_os_installed()\n", "label": 1}
{"function": "\n\n@responses.activate\ndef test_download_file(tempdir, mock_urls):\n    path = op.join(tempdir, 'test')\n    (param, url_data, url_checksum) = mock_urls\n    (data_here, data_valid, checksum_here, checksum_valid) = param\n    assert_succeeds = (data_here and data_valid and ((checksum_here == checksum_valid) or ((not checksum_here) and checksum_valid)))\n    download_succeeds = (assert_succeeds or (data_here and ((not data_valid) and (not checksum_here))))\n    if download_succeeds:\n        data = _dl(path)\n    else:\n        with raises(Exception):\n            data = _dl(path)\n    if assert_succeeds:\n        _check(data)\n", "label": 1}
{"function": "\n\ndef _yearly(self):\n    num = self.num\n    year = self.now.year\n    if (self.event.l_start_date > self.now):\n        year = self.event.l_start_date.year\n    elif (self.now.month > self.event.l_start_date.month):\n        year += 1\n    elif (self.now.month == self.event.l_start_date.month):\n        if ((self.now.day > self.event.l_start_date.day) or ((self.now.day == self.event.l_start_date.day) and (self.now.time() > self.event.l_start_date.time()))):\n            year += 1\n    month = self.event.l_start_date.month\n    day = self.event.l_start_date.day\n    while num:\n        try:\n            start = make_aware(datetime(year, month, day), get_default_timezone())\n            start_ = date(year, month, day)\n        except ValueError:\n            year += 1\n            continue\n        if self.we_should_stop(start, start_):\n            return\n        self.events.append((start, self.event))\n        year += 1\n        num -= 1\n", "label": 1}
{"function": "\n\ndef test_options_setting_as_property():\n    help_string = 'Some Tool\\n\\nUsage: tools --name <input>\\n\\nOptions:\\n    -n, --name <name>    The input\\n    '\n    opts = Options.from_docopt(help_string)\n    assert (opts is not None)\n    opts['name'].set('Test')\n    assert (opts['name'].get() == 'Test')\n    assert (opts.name.get() == 'Test')\n    assert (opts.name == 'Test')\n    opts['name'] = 'Test2'\n    assert (opts['name'].get() == 'Test2')\n    assert (opts.name.get() == 'Test2')\n    assert (opts.name == 'Test2')\n    opts.name = 'Test3'\n    assert (opts['name'].get() == 'Test3')\n    assert (opts.name.get() == 'Test3')\n    assert (opts.name == 'Test3')\n", "label": 1}
{"function": "\n\ndef step(self, dependency_states, step_context):\n    dep_values = []\n    dependencies = [step_context.select_node(selector, self.subject, self.variants) for selector in self.clause]\n    for (dep_select, dep_key) in zip(self.clause, dependencies):\n        dep_state = dependency_states.get(dep_key, None)\n        if ((dep_state is None) or (type(dep_state) == Waiting)):\n            return Waiting(dependencies)\n        elif (type(dep_state) == Return):\n            dep_values.append(dep_state.value)\n        elif (type(dep_state) == Noop):\n            if dep_select.optional:\n                dep_values.append(None)\n            else:\n                return Noop('Was missing (at least) input {}.'.format(dep_key))\n        elif (type(dep_state) == Throw):\n            return dep_state\n        else:\n            State.raise_unrecognized(dep_state)\n    try:\n        return Return(self.func(*dep_values))\n    except Exception as e:\n        return Throw(e)\n", "label": 1}
{"function": "\n\ndef __init__(self, test, host, output_format=None):\n    '\\n\\n        :rtype : object\\n        '\n    self.passed = 0\n    self.failed = 0\n    self.duration_ms = 0\n    try:\n        validictory.validate(test, SCHEMA)\n        LOG.debug('Valid schema')\n    except ValueError as error:\n        LOG.error('Error, invalid test format: {0}.  Tests now use v0.2 format. v0.1 branch is still available.'.format(error))\n        raise\n    try:\n        self.port = test['port']\n    except (AttributeError, KeyError):\n        print('Warning: No port definition found in the first test, using port 80 as default.')\n        try:\n            if (test['protocol'] == 'https'):\n                self.port = 443\n            else:\n                self.port = 80\n        except (AttributeError, KeyError):\n            self.port = 80\n    self.output = Output(output_format=output_format)\n    LOG.debug('Test: {0}'.format(test))\n    test_defaults = dict(inputs=dict(allow_redirects=False, timeout=30), method='get', outcomes=dict(expect_status_code=200, colour_output=True))\n    host_overrides = get_host_overrides.get_host_overrides(host, self.port)\n    if (host_overrides['hostname'] is not None):\n        self.host = host_overrides['hostname']\n    else:\n        self.host = host\n    intermediate_dict = deepupdate(test_defaults, host_overrides)\n    final_dict = deepupdate(intermediate_dict, test)\n    if (('tcp_test' in test) and test['tcp_test']):\n        tcptest.tcp_test(self.host, self.port)\n    try:\n        verify = final_dict['inputs']['verify']\n    except (AttributeError, KeyError):\n        verify = None\n    try:\n        proto = final_dict['protocol']\n    except (AttributeError, KeyError):\n        proto = None\n    (self.verify, self.verify_specified) = get_verify.get_verify(verify, proto)\n    self.test = deepcopy(final_dict)\n    LOG.debug('Test with defaults: {0}'.format(self.test))\n    if ('verify' in self.test['inputs']):\n        del self.test['inputs']['verify']\n    self.inputs = deepcopy(self.test['inputs'])\n    request_url_format = '{protocol}://{host}:{port}{uri}'\n    try:\n        self.inputs['url'] = request_url_format.format(protocol=self.test['protocol'], host=self.host, port=self.test['port'], uri=self.test['uri'])\n    except KeyError:\n        self.inputs['url'] = request_url_format.format(protocol=self.test['protocol'], host=self.host, port=self.test['port'], uri='')\n    LOG.debug('Testing {0}'.format(self.inputs['url']))\n    self.output.append(('-' * OUTPUT_WIDTH))\n    this_name = (('{0:^' + str(OUTPUT_WIDTH)) + '}').format(self.test['name'][:OUTPUT_WIDTH])\n    self.output.append(this_name)\n    this_url = (('{0:^' + str(OUTPUT_WIDTH)) + '}').format(self.inputs['url'][:OUTPUT_WIDTH])\n    self.output.append(this_url)\n    self.output.append(self.__repr__())\n    self.output.append(('-' * OUTPUT_WIDTH))\n    self.output.append(self.inputs, sec='request_inputs')\n    self.run()\n", "label": 1}
{"function": "\n\ndef get(self):\n    del self.response.headers['Content-Type']\n    stub = _get_channel_stub()\n    command = self.request.get('command', None)\n    token = self.request.get('channel', None)\n    if ((command is None) or (token is None)):\n        self.response.status = 400\n        return\n    if (command == 'connect'):\n        try:\n            stub.connect_channel(token)\n        except channel_service_stub.InvalidTokenError:\n            self.response.status = INVALID_TOKEN_STATUS\n            return\n        except channel_service_stub.TokenTimedOutError:\n            self.response.status = TOKEN_TIMED_OUT_STATUS\n            return\n        self.response.headers['Content-Type'] = 'text/plain'\n        self.response.out.write('1')\n    elif (command == 'poll'):\n        try:\n            message = stub.connect_and_pop_first_message(token)\n        except channel_service_stub.InvalidTokenError:\n            self.response.status = INVALID_TOKEN_STATUS\n            return\n        except channel_service_stub.TokenTimedOutError:\n            self.response.status = TOKEN_TIMED_OUT_STATUS\n            return\n        if (message is not None):\n            self.response.headers['Content-Type'] = 'application/json'\n            self.response.out.write(message)\n    else:\n        self.response.status = 400\n", "label": 1}
{"function": "\n\ndef __init__(self):\n    self._logger = logging.getLogger(__name__)\n    if (not os.path.exists(jasperpath.CONFIG_PATH)):\n        try:\n            os.makedirs(jasperpath.CONFIG_PATH)\n        except OSError:\n            self._logger.error(\"Could not create config dir: '%s'\", jasperpath.CONFIG_PATH, exc_info=True)\n            raise\n    if (not os.access(jasperpath.CONFIG_PATH, os.W_OK)):\n        self._logger.critical(('Config dir %s is not writable. Jasper ' + \"won't work correctly.\"), jasperpath.CONFIG_PATH)\n    old_configfile = os.path.join(jasperpath.LIB_PATH, 'profile.yml')\n    new_configfile = jasperpath.config('profile.yml')\n    if os.path.exists(old_configfile):\n        if os.path.exists(new_configfile):\n            self._logger.warning((\"Deprecated profile file found: '%s'. \" + 'Please remove it.'), old_configfile)\n        else:\n            self._logger.warning((\"Deprecated profile file found: '%s'. \" + \"Trying to copy it to new location '%s'.\"), old_configfile, new_configfile)\n            try:\n                shutil.copy2(old_configfile, new_configfile)\n            except shutil.Error:\n                self._logger.error(('Unable to copy config file. ' + 'Please copy it manually.'), exc_info=True)\n                raise\n    self._logger.debug(\"Trying to read config file: '%s'\", new_configfile)\n    try:\n        with open(new_configfile, 'r') as f:\n            self.config = yaml.safe_load(f)\n    except OSError:\n        self._logger.error(\"Can't open config file: '%s'\", new_configfile)\n        raise\n    try:\n        stt_engine_slug = self.config['stt_engine']\n    except KeyError:\n        stt_engine_slug = 'sphinx'\n        logger.warning(('stt_engine not specified in profile, defaulting ' + \"to '%s'\"), stt_engine_slug)\n    stt_engine_class = stt.get_engine_by_slug(stt_engine_slug)\n    try:\n        slug = self.config['stt_passive_engine']\n        stt_passive_engine_class = stt.get_engine_by_slug(slug)\n    except KeyError:\n        stt_passive_engine_class = stt_engine_class\n    try:\n        tts_engine_slug = self.config['tts_engine']\n    except KeyError:\n        tts_engine_slug = tts.get_default_engine_slug()\n        logger.warning(('tts_engine not specified in profile, defaulting ' + \"to '%s'\"), tts_engine_slug)\n    tts_engine_class = tts.get_engine_by_slug(tts_engine_slug)\n    self.mic = Mic(tts_engine_class.get_instance(), stt_passive_engine_class.get_passive_instance(), stt_engine_class.get_active_instance())\n", "label": 1}
{"function": "\n\ndef _recv(self, which, maxsize):\n    (conn, maxsize) = self.get_conn_maxsize(which, maxsize)\n    if (conn is None):\n        return None\n    try:\n        x = msvcrt.get_osfhandle(conn.fileno())\n        (read, nAvail, nMessage) = PeekNamedPipe(x, 0)\n        if (maxsize < nAvail):\n            nAvail = maxsize\n        if (nAvail > 0):\n            (errCode, read) = ReadFile(x, nAvail, None)\n    except ValueError:\n        return self._close(which)\n    except (subprocess.pywintypes.error, Exception) as why:\n        if (why.args[0] in (109, errno.ESHUTDOWN)):\n            return self._close(which)\n        raise\n    getattr(self, '{0}_buff'.format(which)).write(read)\n    getattr(self, '_{0}_logger'.format(which)).debug(read.rstrip())\n    if self.stream_stds:\n        getattr(sys, which).write(read)\n    if self.universal_newlines:\n        read = self._translate_newlines(read)\n    return read\n", "label": 1}
{"function": "\n\ndef test_use():\n    assert (use(0, expand) == 0)\n    f = ((((x + y) ** 2) * x) + 1)\n    assert (use(f, expand, level=0) == ((((x ** 3) + ((2 * (x ** 2)) * y)) + (x * (y ** 2))) + (+ 1)))\n    assert (use(f, expand, level=1) == ((((x ** 3) + ((2 * (x ** 2)) * y)) + (x * (y ** 2))) + (+ 1)))\n    assert (use(f, expand, level=2) == (1 + (x * ((((2 * x) * y) + (x ** 2)) + (y ** 2)))))\n    assert (use(f, expand, level=3) == ((((x + y) ** 2) * x) + 1))\n    f = ((((x ** 2) + 1) ** 2) - 1)\n    kwargs = {\n        'gaussian': True,\n    }\n    assert (use(f, factor, level=0, kwargs=kwargs) == ((x ** 2) * ((x ** 2) + 2)))\n    assert (use(f, factor, level=1, kwargs=kwargs) == ((((x + I) ** 2) * ((x - I) ** 2)) - 1))\n    assert (use(f, factor, level=2, kwargs=kwargs) == ((((x + I) ** 2) * ((x - I) ** 2)) - 1))\n    assert (use(f, factor, level=3, kwargs=kwargs) == ((((x ** 2) + 1) ** 2) - 1))\n", "label": 1}
{"function": "\n\ndef encode_7or8bit(msg):\n    'Set the Content-Transfer-Encoding header to 7bit or 8bit.'\n    orig = msg.get_payload()\n    if (orig is None):\n        msg['Content-Transfer-Encoding'] = '7bit'\n        return\n    try:\n        if isinstance(orig, str):\n            orig.encode('ascii')\n        else:\n            orig.decode('ascii')\n    except UnicodeError:\n        charset = msg.get_charset()\n        output_cset = (charset and charset.output_charset)\n        if (output_cset and output_cset.lower().startswith('iso-2022-')):\n            msg['Content-Transfer-Encoding'] = '7bit'\n        else:\n            msg['Content-Transfer-Encoding'] = '8bit'\n    else:\n        msg['Content-Transfer-Encoding'] = '7bit'\n    if (not isinstance(orig, str)):\n        msg.set_payload(orig.decode('ascii', 'surrogateescape'))\n", "label": 1}
{"function": "\n\ndef print_info(db_type, db_file, swift_dir='/etc/swift', stale_reads_ok=False):\n    if (db_type not in ('account', 'container')):\n        print('Unrecognized DB type: internal error')\n        raise InfoSystemExit()\n    if ((not os.path.exists(db_file)) or (not db_file.endswith('.db'))):\n        print(\"DB file doesn't exist\")\n        raise InfoSystemExit()\n    if (not db_file.startswith(('/', './'))):\n        db_file = ('./' + db_file)\n    if (db_type == 'account'):\n        broker = AccountBroker(db_file, stale_reads_ok=stale_reads_ok)\n        datadir = ABDATADIR\n    else:\n        broker = ContainerBroker(db_file, stale_reads_ok=stale_reads_ok)\n        datadir = CBDATADIR\n    try:\n        info = broker.get_info()\n    except sqlite3.OperationalError as err:\n        if ('no such table' in str(err)):\n            print(('Does not appear to be a DB of type \"%s\": %s' % (db_type, db_file)))\n            raise InfoSystemExit()\n        raise\n    account = info['account']\n    container = (info['container'] if (db_type == 'container') else None)\n    print_db_info_metadata(db_type, info, broker.metadata)\n    try:\n        ring = Ring(swift_dir, ring_name=db_type)\n    except Exception:\n        ring = None\n    else:\n        print_ring_locations(ring, datadir, account, container)\n", "label": 1}
{"function": "\n\ndef generate_separated_multinomial_weights(A, C):\n    'Generates a set of multinomial weights B, where sum(abs(B-A)) = C\\n\\t\\tInputs:\\n\\t\\t\\tA: a list of multinomial weights\\n\\t\\t\\tC: A float, 0 <= C <= 1\\n\\t'\n    if (not isinstance(A, list)):\n        raise TypeError('A should be a list')\n    if (not (math.fabs((1 - sum(A))) < 1e-07)):\n        raise ValueError('A must sum to 1.')\n    if ((C > 1.0) or (C < 0)):\n        raise ValueError('0 <= C <= 1')\n    if (C == 0.0):\n        return A\n    idx = [i[0] for i in sorted(enumerate(A), key=(lambda x: x[1]))]\n    A_sum = [A[i] for i in idx]\n    A = numpy.array(A)\n    A_sum = numpy.cumsum(numpy.array(A_sum))\n    B = numpy.copy(A)\n    t = numpy.nonzero((A_sum >= 0.5))[0][0]\n    err_up = idx[:t]\n    err_dn = idx[t:]\n    upper_bounds = (1.0 - B)\n    upper_bounds[err_dn] = 0\n    lower_bounds = numpy.copy(B)\n    lower_bounds[err_up] = 0\n    for _ in range(int((C * 10.0))):\n        ups = numpy.nonzero((upper_bounds >= 0.05))[0]\n        move_up = ups[random.randrange(len(ups))]\n        B[move_up] += 0.05\n        upper_bounds[move_up] -= 0.05\n        dns = numpy.nonzero((lower_bounds >= 0.05))[0]\n        if (len(dns) == 0):\n            maxdex = numpy.argmin(lower_bounds)\n            B[maxdex] = 0\n            B /= numpy.sum(B)\n            break\n        move_down = dns[random.randrange(len(dns))]\n        B[move_down] -= 0.05\n        lower_bounds[move_down] -= 0.05\n    assert (math.fabs((1 - numpy.sum(B))) < 1e-07)\n    return B.tolist()\n", "label": 1}
{"function": "\n\ndef prepare_source_fields_default(self, request):\n    source_type_choices = []\n    self.fields['availability_zone'].choices = self.availability_zones(request)\n    try:\n        available = api.cinder.VOLUME_STATE_AVAILABLE\n        snapshots = cinder.volume_snapshot_list(request, search_opts=dict(status=available))\n        if snapshots:\n            source_type_choices.append(('snapshot_source', _('Snapshot')))\n            choices = ([('', _('Choose a snapshot'))] + [(s.id, s) for s in snapshots])\n            self.fields['snapshot_source'].choices = choices\n        else:\n            del self.fields['snapshot_source']\n    except Exception:\n        exceptions.handle(request, _('Unable to retrieve volume snapshots.'))\n    images = utils.get_available_images(request, request.user.tenant_id)\n    if images:\n        source_type_choices.append(('image_source', _('Image')))\n        choices = [('', _('Choose an image'))]\n        for image in images:\n            image.bytes = image.size\n            image.size = functions.bytes_to_gigabytes(image.bytes)\n            choices.append((image.id, image))\n        self.fields['image_source'].choices = choices\n    else:\n        del self.fields['image_source']\n    volumes = self.get_volumes(request)\n    if volumes:\n        source_type_choices.append(('volume_source', _('Volume')))\n        choices = [('', _('Choose a volume'))]\n        for volume in volumes:\n            choices.append((volume.id, volume))\n        self.fields['volume_source'].choices = choices\n    else:\n        del self.fields['volume_source']\n    if source_type_choices:\n        choices = ([('no_source_type', _('No source, empty volume'))] + source_type_choices)\n        self.fields['volume_source_type'].choices = choices\n    else:\n        del self.fields['volume_source_type']\n", "label": 1}
{"function": "\n\ndef build_graph(layout_str, slanted):\n    \"\\n    builds an adjacency graph as a dictionary: {character: [adjacent_characters]}.\\n    adjacent characters occur in a clockwise order.\\n    for example:\\n    * on qwerty layout, 'g' maps to ['fF', 'tT', 'yY', 'hH', 'bB', 'vV']\\n    * on keypad layout, '7' maps to [None, None, None, '=', '8', '5', '4', None]\\n    \"\n    position_table = {\n        \n    }\n    tokens = layout_str.split()\n    token_size = len(tokens[0])\n    x_unit = (token_size + 1)\n    adjacency_func = (get_slanted_adjacent_coords if slanted else get_aligned_adjacent_coords)\n    assert all(((len(token) == token_size) for token in tokens)), ('token length mismatch:\\n ' + layout_str)\n    for (y, line) in enumerate(layout_str.split('\\n')):\n        slant = ((y - 1) if slanted else 0)\n        for token in line.split():\n            (x, remainder) = divmod((line.index(token) - slant), x_unit)\n            assert (remainder == 0), ('unexpected x offset for %s in:\\n%s' % (token, layout_str))\n            position_table[(x, y)] = token\n    adjacency_graph = {\n        \n    }\n    for ((x, y), chars) in position_table.iteritems():\n        for char in chars:\n            adjacency_graph[char] = []\n            for coord in adjacency_func(x, y):\n                adjacency_graph[char].append(position_table.get(coord, None))\n    return adjacency_graph\n", "label": 1}
{"function": "\n\ndef _detect_image_library():\n    global Image\n    global _imaging\n    global ImageFile\n    if (Image is not None):\n        return (Image, _imaging, ImageFile)\n    PIL_imaging = False\n    try:\n        from PIL import Image as PILImage\n    except ImportError as err:\n        try:\n            import Image as PILImage\n        except ImportError as err:\n            raise ImproperlyConfigured((_('Neither Pillow nor PIL could be imported: %s') % err))\n    if hasattr(PILImage, 'alpha_composite'):\n        PIL_imaging = False\n    else:\n        import platform\n        if (platform.python_implementation().lower() == 'cpython'):\n            try:\n                from PIL import _imaging as PIL_imaging\n            except ImportError:\n                try:\n                    import _imaging as PIL_imaging\n                except ImportError as err:\n                    raise ImproperlyConfigured((_(\"The '_imaging' module for the PIL could not be imported: %s\") % err))\n    try:\n        from PIL import ImageFile as PILImageFile\n    except ImportError:\n        import ImageFile as PILImageFile\n    if (PIL_imaging is not False):\n        warnings.warn(('Support for the PIL will be removed in Django 1.8. Please ' + 'uninstall it & install Pillow instead.'), PendingDeprecationWarning)\n    return (PILImage, PIL_imaging, PILImageFile)\n", "label": 1}
{"function": "\n\ndef is_cached_response_fresh(self, cachedresponse, request):\n    cc = self._parse_cachecontrol(cachedresponse)\n    ccreq = self._parse_cachecontrol(request)\n    if ((b'no-cache' in cc) or (b'no-cache' in ccreq)):\n        return False\n    now = time()\n    freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n    currentage = self._compute_current_age(cachedresponse, request, now)\n    reqmaxage = self._get_max_age(ccreq)\n    if (reqmaxage is not None):\n        freshnesslifetime = min(freshnesslifetime, reqmaxage)\n    if (currentage < freshnesslifetime):\n        return True\n    if ((b'max-stale' in ccreq) and (b'must-revalidate' not in cc)):\n        staleage = ccreq[b'max-stale']\n        if (staleage is None):\n            return True\n        try:\n            if (currentage < (freshnesslifetime + max(0, int(staleage)))):\n                return True\n        except ValueError:\n            pass\n    self._set_conditional_validators(request, cachedresponse)\n    return False\n", "label": 1}
{"function": "\n\ndef write_document_file(doc, record_module=None, create_init=None):\n    newdoc = doc.as_dict(no_nulls=True)\n    for df in doc.meta.get_table_fields():\n        for d in newdoc.get(df.fieldname):\n            for fieldname in frappe.model.default_fields:\n                if (fieldname in d):\n                    del d[fieldname]\n    module = (record_module or get_module_name(doc))\n    if (create_init is None):\n        create_init = (doc.doctype in lower_case_files_for)\n    folder = create_folder(module, doc.doctype, doc.name, create_init)\n    fname = (((doc.doctype in lower_case_files_for) and scrub(doc.name)) or doc.name)\n    with open(os.path.join(folder, (fname + '.json')), 'w+') as txtfile:\n        txtfile.write(frappe.as_json(newdoc))\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Main master loop.'\n    self.start()\n    util._setproctitle(('master [%s]' % self.proc_name))\n    try:\n        self.manage_workers()\n        while True:\n            sig = (self.SIG_QUEUE.pop(0) if len(self.SIG_QUEUE) else None)\n            if (sig is None):\n                self.sleep()\n                self.murder_workers()\n                self.manage_workers()\n                continue\n            if (sig not in self.SIG_NAMES):\n                self.log.info('Ignoring unknown signal: %s', sig)\n                continue\n            signame = self.SIG_NAMES.get(sig)\n            handler = getattr(self, ('handle_%s' % signame), None)\n            if (not handler):\n                self.log.error('Unhandled signal: %s', signame)\n                continue\n            self.log.info('Handling signal: %s', signame)\n            handler()\n            self.wakeup()\n    except StopIteration:\n        self.halt()\n    except KeyboardInterrupt:\n        self.halt()\n    except HaltServer as inst:\n        self.halt(reason=inst.reason, exit_status=inst.exit_status)\n    except SystemExit:\n        raise\n    except Exception:\n        self.log.info('Unhandled exception in main loop:\\n%s', traceback.format_exc())\n        self.stop(False)\n        if (self.pidfile is not None):\n            self.pidfile.unlink()\n        sys.exit((- 1))\n", "label": 1}
{"function": "\n\ndef update_editable(self, obtain=True):\n    if (not self.link):\n        logger.debug('Cannot update repository at %s; repository location is unknown', self.source_dir)\n        return\n    assert self.editable\n    assert self.source_dir\n    if (self.link.scheme == 'file'):\n        return\n    assert ('+' in self.link.url), ('bad url: %r' % self.link.url)\n    if (not self.update):\n        return\n    (vc_type, url) = self.link.url.split('+', 1)\n    backend = vcs.get_backend(vc_type)\n    if backend:\n        vcs_backend = backend(self.link.url)\n        if obtain:\n            vcs_backend.obtain(self.source_dir)\n        else:\n            vcs_backend.export(self.source_dir)\n    else:\n        assert 0, ('Unexpected version control type (in %s): %s' % (self.link, vc_type))\n", "label": 1}
{"function": "\n\ndef _get_fetch_info_from_stderr(self, proc, progress):\n    output = IterableList('name')\n    fetch_info_lines = list()\n    cmds = (set(PushInfo._flag_map.keys()) & set(FetchInfo._flag_map.keys()))\n    progress_handler = progress.new_message_handler()\n    for line in proc.stderr.readlines():\n        line = line.decode(defenc)\n        for pline in progress_handler(line):\n            if (line.startswith('fatal:') or line.startswith('error:')):\n                raise GitCommandError((('Error when fetching: %s' % line),), 2)\n            for cmd in cmds:\n                if ((len(line) > 1) and (line[0] == ' ') and (line[1] == cmd)):\n                    fetch_info_lines.append(line)\n                    continue\n    finalize_process(proc)\n    fp = open(join(self.repo.git_dir, 'FETCH_HEAD'), 'rb')\n    fetch_head_info = [l.decode(defenc) for l in fp.readlines()]\n    fp.close()\n    l_fil = len(fetch_info_lines)\n    l_fhi = len(fetch_head_info)\n    assert (l_fil >= l_fhi), ('len(%s) <= len(%s)' % (l_fil, l_fhi))\n    output.extend((FetchInfo._from_line(self.repo, err_line, fetch_line) for (err_line, fetch_line) in zip(fetch_info_lines, fetch_head_info)))\n    return output\n", "label": 1}
{"function": "\n\n@non_atomic_requests\ndef guid_search(request, api_version, guids):\n    lang = request.LANG\n\n    def guid_search_cache_key(guid):\n        key = ('guid_search:%s:%s:%s' % (api_version, lang, guid))\n        return hashlib.md5(smart_str(key)).hexdigest()\n    guids = ([g.strip() for g in guids.split(',')] if guids else [])\n    addons_xml = cache.get_many([guid_search_cache_key(g) for g in guids])\n    dirty_keys = set()\n    for g in guids:\n        key = guid_search_cache_key(g)\n        if (key not in addons_xml):\n            dirty_keys.add(key)\n            try:\n                addon = Addon.objects.get(guid=g, disabled_by_user=False, status__in=SEARCHABLE_STATUSES)\n            except Addon.DoesNotExist:\n                addons_xml[key] = ''\n            else:\n                addon_xml = render_xml_to_string(request, 'legacy_api/includes/addon.xml', {\n                    'addon': addon,\n                    'api_version': api_version,\n                    'api': legacy_api,\n                })\n                addons_xml[key] = addon_xml\n    cache.set_many(dict(((k, v) for (k, v) in addons_xml.iteritems() if (k in dirty_keys))))\n    compat = CompatOverride.objects.filter(guid__in=guids).transform(CompatOverride.transformer)\n    addons_xml = [v for v in addons_xml.values() if v]\n    return render_xml(request, 'legacy_api/search.xml', {\n        'addons_xml': addons_xml,\n        'total': len(addons_xml),\n        'compat': compat,\n        'api_version': api_version,\n        'api': legacy_api,\n    })\n", "label": 1}
{"function": "\n\n@Xephyr(True, ShConfig())\ndef test_findNode(self):\n    self.sh = libqtile.sh.QSh(self.c)\n    n = self.sh._findNode(self.sh.current, 'layout')\n    assert (n.path == 'layout')\n    assert n.parent\n    n = self.sh._findNode(n, '0')\n    assert (n.path == 'layout[0]')\n    n = self.sh._findNode(n, '..')\n    assert (n.path == 'layout')\n    n = self.sh._findNode(n, '0', '..')\n    assert (n.path == 'layout')\n    n = self.sh._findNode(n, '..', 'layout', 0)\n    assert (n.path == 'layout[0]')\n    assert (not self.sh._findNode(n, 'wibble'))\n    assert (not self.sh._findNode(n, '..', '0', 'wibble'))\n", "label": 1}
{"function": "\n\ndef convert_to_datetime(input, tz, arg_name):\n    '\\n    Converts the given object to a timezone aware datetime object.\\n\\n    If a timezone aware datetime object is passed, it is returned unmodified.\\n    If a native datetime object is passed, it is given the specified timezone.\\n    If the input is a string, it is parsed as a datetime with the given timezone.\\n\\n    Date strings are accepted in three different forms: date only (Y-m-d), date with time\\n    (Y-m-d H:M:S) or with date+time with microseconds (Y-m-d H:M:S.micro).\\n\\n    :param str|datetime input: the datetime or string to convert to a timezone aware datetime\\n    :param datetime.tzinfo tz: timezone to interpret ``input`` in\\n    :param str arg_name: the name of the argument (used in an error message)\\n    :rtype: datetime\\n\\n    '\n    if (input is None):\n        return\n    elif isinstance(input, datetime):\n        datetime_ = input\n    elif isinstance(input, date):\n        datetime_ = datetime.combine(input, time())\n    elif isinstance(input, six.string_types):\n        m = _DATE_REGEX.match(input)\n        if (not m):\n            raise ValueError('Invalid date string')\n        values = [(k, int((v or 0))) for (k, v) in m.groupdict().items()]\n        values = dict(values)\n        datetime_ = datetime(**values)\n    else:\n        raise TypeError(('Unsupported type for %s: %s' % (arg_name, input.__class__.__name__)))\n    if (datetime_.tzinfo is not None):\n        return datetime_\n    if (tz is None):\n        raise ValueError(('The \"tz\" argument must be specified if %s has no timezone information' % arg_name))\n    if isinstance(tz, six.string_types):\n        tz = timezone(tz)\n    try:\n        return tz.localize(datetime_, is_dst=None)\n    except AttributeError:\n        raise TypeError('Only pytz timezones are supported (need the localize() and normalize() methods)')\n", "label": 1}
{"function": "\n\ndef test_make_undirecred_copy(self, huang_darwiche_dag):\n    ug = make_undirected_copy(huang_darwiche_dag)\n    nodes = dict([(node.name, node) for node in ug.nodes])\n    assert (set(nodes['f_a'].neighbours) == set([nodes['f_b'], nodes['f_c']]))\n    assert (set(nodes['f_b'].neighbours) == set([nodes['f_a'], nodes['f_d']]))\n    assert (set(nodes['f_c'].neighbours) == set([nodes['f_a'], nodes['f_e'], nodes['f_g']]))\n    assert (set(nodes['f_d'].neighbours) == set([nodes['f_b'], nodes['f_f']]))\n    assert (set(nodes['f_e'].neighbours) == set([nodes['f_c'], nodes['f_f'], nodes['f_h']]))\n    assert (set(nodes['f_f'].neighbours) == set([nodes['f_d'], nodes['f_e']]))\n    assert (set(nodes['f_g'].neighbours) == set([nodes['f_c'], nodes['f_h']]))\n    assert (set(nodes['f_h'].neighbours) == set([nodes['f_e'], nodes['f_g']]))\n", "label": 1}
{"function": "\n\ndef resolveAxesStructure(view, viewTblELR):\n    if isinstance(viewTblELR, (ModelEuTable, ModelTable)):\n        table = viewTblELR\n        for rel in view.modelXbrl.relationshipSet((XbrlConst.tableBreakdown, XbrlConst.tableBreakdownMMDD, XbrlConst.tableBreakdown201305, XbrlConst.tableBreakdown201301, XbrlConst.tableAxis2011)).fromModelObject(table):\n            view.axisSubtreeRelSet = view.modelXbrl.relationshipSet((XbrlConst.tableBreakdownTree, XbrlConst.tableBreakdownTreeMMDD, XbrlConst.tableBreakdownTree201305, XbrlConst.tableDefinitionNodeSubtree, XbrlConst.tableDefinitionNodeSubtreeMMDD, XbrlConst.tableDefinitionNodeSubtree201305, XbrlConst.tableDefinitionNodeSubtree201301, XbrlConst.tableAxisSubtree2011), rel.linkrole)\n            return resolveTableAxesStructure(view, table, view.modelXbrl.relationshipSet((XbrlConst.tableBreakdown, XbrlConst.tableBreakdownMMDD, XbrlConst.tableBreakdown201305, XbrlConst.tableBreakdown201301, XbrlConst.tableAxis2011), rel.linkrole))\n        return (None, None, None, None)\n    tblAxisRelSet = view.modelXbrl.relationshipSet(XbrlConst.euTableAxis, viewTblELR)\n    if (len(tblAxisRelSet.modelRelationships) > 0):\n        view.axisSubtreeRelSet = view.modelXbrl.relationshipSet(XbrlConst.euAxisMember, viewTblELR)\n    else:\n        tblAxisRelSet = view.modelXbrl.relationshipSet((XbrlConst.tableBreakdown, XbrlConst.tableBreakdownMMDD, XbrlConst.tableBreakdown201305, XbrlConst.tableBreakdown201301, XbrlConst.tableAxis2011), viewTblELR)\n        view.axisSubtreeRelSet = view.modelXbrl.relationshipSet((XbrlConst.tableBreakdownTree, XbrlConst.tableBreakdownTreeMMDD, XbrlConst.tableBreakdownTree201305, XbrlConst.tableDefinitionNodeSubtree, XbrlConst.tableDefinitionNodeSubtreeMMDD, XbrlConst.tableDefinitionNodeSubtree201305, XbrlConst.tableDefinitionNodeSubtree201301, XbrlConst.tableAxisSubtree2011), viewTblELR)\n    if ((tblAxisRelSet is None) or (len(tblAxisRelSet.modelRelationships) == 0)):\n        view.modelXbrl.modelManager.addToLog(_('no table relationships for {0}').format(viewTblELR))\n        return (None, None, None, None)\n    modelRoleTypes = view.modelXbrl.roleTypes.get(viewTblELR)\n    if ((modelRoleTypes is not None) and (len(modelRoleTypes) > 0)):\n        view.roledefinition = modelRoleTypes[0].definition\n        if ((view.roledefinition is None) or (view.roledefinition == '')):\n            view.roledefinition = os.path.basename(viewTblELR)\n    try:\n        for table in tblAxisRelSet.rootConcepts:\n            return resolveTableAxesStructure(view, table, tblAxisRelSet)\n    except ResolutionException as ex:\n        view.modelXbrl.error(ex.code, ex.message, exc_info=True, **ex.kwargs)\n    return (None, None, None, None)\n", "label": 1}
{"function": "\n\ndef _do_request(self, method, path, params=None, data=None):\n    'Query Marathon server.'\n    headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json',\n    }\n    response = None\n    servers = list(self.servers)\n    while (servers and (response is None)):\n        server = servers.pop(0)\n        url = ''.join([server.rstrip('/'), path])\n        try:\n            response = requests.request(method, url, params=params, data=data, headers=headers, auth=self.auth, timeout=self.timeout)\n            marathon.log.info('Got response from %s', server)\n        except requests.exceptions.RequestException as e:\n            marathon.log.error('Error while calling %s: %s', url, e.message)\n    if (response is None):\n        raise MarathonError('No remaining Marathon servers to try')\n    if (response.status_code >= 500):\n        marathon.log.error('Got HTTP {code}: {body}'.format(code=response.status_code, body=response.text))\n        raise InternalServerError(response)\n    elif (response.status_code >= 400):\n        marathon.log.error('Got HTTP {code}: {body}'.format(code=response.status_code, body=response.text))\n        if (response.status_code == 404):\n            raise NotFoundError(response)\n        else:\n            raise MarathonHttpError(response)\n    elif (response.status_code >= 300):\n        marathon.log.warn('Got HTTP {code}: {body}'.format(code=response.status_code, body=response.text))\n    else:\n        marathon.log.debug('Got HTTP {code}: {body}'.format(code=response.status_code, body=response.text))\n    return response\n", "label": 1}
{"function": "\n\ndef functions(hfiles):\n    lines = []\n    for (fn, path) in hfiles.items():\n        with open(path) as f:\n            cont = ''\n            for ln in f:\n                if (cont == ','):\n                    lines.append(ln)\n                    cont = ''\n                if (cont in BLOCKS):\n                    lines.append(ln)\n                    if (BLOCKS[cont] in ln):\n                        cont = ''\n                if (not (ln.startswith('NN_EXPORT') or ln.startswith('typedef'))):\n                    continue\n                lines.append(ln)\n                cont = ln.strip()[(- 1)]\n    return ''.join(((ln[10:] if ln.startswith('NN_') else ln) for ln in lines))\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    if (not MEDIA_DEV_MODE):\n        return\n    _refresh_dev_names()\n    if (not request.path.startswith(DEV_MEDIA_URL)):\n        return\n    filename = request.path[len(DEV_MEDIA_URL):]\n    try:\n        backend = _backend_mapping[filename]\n    except KeyError:\n        raise Http404(('The mediagenerator could not find the media file \"%s\"' % filename))\n    (content, mimetype) = backend.get_dev_output(filename)\n    if (not mimetype):\n        mimetype = 'application/octet-stream'\n    if isinstance(content, unicode):\n        content = content.encode('utf-8')\n    if (mimetype.startswith('text/') or (mimetype in TEXT_MIME_TYPES)):\n        mimetype += '; charset=utf-8'\n    response = HttpResponse(content, content_type=mimetype)\n    response['Content-Length'] = len(content)\n    if ((response['Content-Type'] != 'text/cache-manifest') and (response.status_code == 200)):\n        patch_cache_control(response, public=True, max_age=self.MAX_AGE)\n        response['Expires'] = http_date((time.time() + self.MAX_AGE))\n    return response\n", "label": 1}
{"function": "\n\ndef _caas_2_2_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_server_antiAffinityRule_ALLFILTERS(self, method, url, body, headers):\n    (_, params) = url.split('?')\n    parameters = params.split('&')\n    for parameter in parameters:\n        (key, value) = parameter.split('=')\n        if (key == 'id'):\n            assert (value == 'FAKE_ID')\n        elif (key == 'state'):\n            assert (value == 'FAKE_STATE')\n        elif (key == 'pageSize'):\n            assert (value == '250')\n        elif (key == 'networkDomainId'):\n            pass\n        else:\n            raise ValueError('Could not find in url parameters {0}:{1}'.format(key, value))\n    body = self.fixtures.load('server_antiAffinityRule_list.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 1}
{"function": "\n\ndef get(self, album, extra):\n    if (not album.mb_releasegroupid):\n        return\n    response = self.request((self.API_ALBUMS + album.mb_releasegroupid), headers={\n        'api-key': self.PROJECT_KEY,\n        'client-key': self.client_key,\n    })\n    try:\n        data = response.json()\n    except ValueError:\n        self._log.debug('fanart.tv: error loading response: {}', response.text)\n        return\n    if (('status' in data) and (data['status'] == 'error')):\n        if ('not found' in data['error message'].lower()):\n            self._log.debug('fanart.tv: no image found')\n        elif ('api key' in data['error message'].lower()):\n            self._log.warning('fanart.tv: Invalid API key given, please enter a valid one in your config file.')\n        else:\n            self._log.debug('fanart.tv: error on request: {}', data['error message'])\n        return\n    matches = []\n    for (mbid, art) in data.get('albums', dict()).items():\n        if ((album.mb_releasegroupid == mbid) and ('albumcover' in art)):\n            matches.extend(art['albumcover'])\n        else:\n            self._log.debug('fanart.tv: unexpected mb_releasegroupid in response!')\n    matches.sort(key=(lambda x: x['likes']), reverse=True)\n    for item in matches:\n        (yield self._candidate(url=item['url'], match=Candidate.MATCH_EXACT, size=(1000, 1000)))\n", "label": 1}
{"function": "\n\ndef to_xml(self, include_namespaces=True, include_schemalocs=False, ns_dict=None, schemaloc_dict=None, pretty=True, auto_namespace=True, encoding='utf-8'):\n    'Serializes a :class:`Entity` instance to an XML string.\\n\\n        The default character encoding is ``utf-8`` and can be set via the\\n        `encoding` parameter. If `encoding` is ``None``, a unicode string\\n        is returned.\\n\\n        Args:\\n            auto_namespace: Automatically discover and export XML namespaces\\n                for a STIX :class:`Entity` instance.\\n            include_namespaces: Export namespace definitions in the output\\n                XML. Default is ``True``.\\n            include_schemalocs: Export ``xsi:schemaLocation`` attribute\\n                in the output document. This will attempt to associate\\n                namespaces declared in the STIX document with schema locations.\\n                If a namespace cannot be resolved to a schemaLocation, a\\n                Python warning will be raised. Schemalocations will only be\\n                exported if `include_namespaces` is also ``True``.\\n            ns_dict: Dictionary of XML definitions (namespace is key, alias is\\n                value) to include in the exported document. This must be\\n                passed in if `auto_namespace` is ``False``.\\n            schemaloc_dict: Dictionary of XML ``namespace: schema location``\\n                mappings to include in the exported document. These will\\n                only be included if `auto_namespace` is ``False``.\\n            pretty: Pretty-print the XML.\\n            encoding: The output character encoding. Default is ``utf-8``. If\\n                `encoding` is set to ``None``, a unicode string is returned.\\n\\n        Returns:\\n            An XML string for this\\n            :class:`Entity` instance. Default character encoding is ``utf-8``.\\n\\n        '\n    from .utils import nsparser\n    parser = nsparser.NamespaceParser()\n    if auto_namespace:\n        ns_info = nsparser.NamespaceInfo()\n    else:\n        ns_info = None\n    obj = self.to_obj(ns_info=ns_info)\n    if ((not auto_namespace) and (not ns_dict)):\n        raise Exception('Auto-namespacing was disabled but ns_dict was empty or missing.')\n    if auto_namespace:\n        ns_info.finalize(ns_dict=ns_dict, schemaloc_dict=schemaloc_dict)\n        obj_ns_dict = ns_info.binding_namespaces\n    else:\n        ns_info = nsparser.NamespaceInfo()\n        ns_info.finalized_namespaces = (ns_dict or {\n            \n        })\n        ns_info.finalized_schemalocs = (schemaloc_dict or {\n            \n        })\n        obj_ns_dict = dict(itertools.chain(ns_dict.iteritems(), nsparser.DEFAULT_STIX_NAMESPACES.iteritems()))\n    namespace_def = ''\n    if include_namespaces:\n        xmlns = parser.get_xmlns_str(ns_info.finalized_namespaces)\n        namespace_def += ('\\n\\t' + xmlns)\n    if (include_schemalocs and include_namespaces):\n        schemaloc = parser.get_schemaloc_str(ns_info.finalized_schemalocs)\n        namespace_def += ('\\n\\t' + schemaloc)\n    if (not pretty):\n        namespace_def = namespace_def.replace('\\n\\t', ' ')\n    with save_encoding(encoding):\n        sio = StringIO.StringIO()\n        obj.export(sio.write, 0, obj_ns_dict, pretty_print=pretty, namespacedef_=namespace_def)\n    s = unicode(sio.getvalue())\n    if encoding:\n        return s.encode(encoding)\n    return s\n", "label": 1}
{"function": "\n\ndef _call(self, environ, start_response):\n    if environ['wsgi.input']:\n        environ['wsgi.input'] = GreenStream(environ['wsgi.input'])\n    response = None\n    try:\n        for middleware in self.middleware:\n            response = wait(middleware(environ, start_response))\n            if (response is not None):\n                break\n        if (response is None):\n            raise Http404\n    except Exception as exc:\n        response = wait(handle_wsgi_error(environ, exc))\n    if (isinstance(response, WsgiResponse) and (not response.started)):\n        for middleware in self.response_middleware:\n            response = (wait(middleware(environ, response)) or response)\n        response.start(start_response)\n    return response\n", "label": 1}
{"function": "\n\ndef load_hashes(self, project):\n    hashes = set()\n    try:\n        hashes_folder = join(self.cache_dir, self._cached_hash_folder)\n        stale_time = long((time() - self._cached_hash_ttl))\n        for file_path in iglob(join(hashes_folder, '*.json')):\n            delete_file = True\n            try:\n                file_time = long(splitext(basename(file_path))[0])\n                if (stale_time < file_time):\n                    file_obj = open(file_path, 'rb')\n                    hashes_meta = json_load(file_obj)\n                    file_obj.close()\n                    hashes_version = hashes_meta.get('version', 0)\n                    if (2 <= hashes_version):\n                        cached_hashes = hashes_meta.get('hashes', None)\n                        if cached_hashes:\n                            delete_file = False\n                            hashes_host = hashes_meta.get('host', None)\n                            if (hashes_host == self.hub_pool.host):\n                                hashes.update(cached_hashes)\n            except (TypeError, ValueError):\n                pass\n            if delete_file:\n                LOG.info('Deleting stale cache file: %s', file_path)\n                remove(file_path)\n    except (IOError, error):\n        pass\n    except Exception as e:\n        LOG.error(str(e))\n    hashes.update(self.request_hashes(project))\n    return hashes\n", "label": 1}
{"function": "\n\ndef mintegrate(m, var, l1, l2, mname, sufix, norm=False, do_simplify=False, conds='none'):\n    print('\\tstarting integration of {mname} over {var}'.format(mname=mname, var=var))\n    m = Matrix(m)\n    sympy.var('AAA')\n    if norm:\n        subs = {\n            var: (((l2 - l1) * AAA) + l1),\n        }\n        m = m.subs(subs)\n        m *= (l2 - l1)\n    for ((i, j), ki) in np.ndenumerate(m):\n        tmp = '{mname}_{sufix}[{i}, {j}] over {var}'.format(mname=mname, sufix=sufix, i=i, j=j, var=var)\n        try:\n            if norm:\n                ki = integrate(ki, (AAA, 0, 1), conds=conds)\n            else:\n                ki = integrate(ki, (var, l1, l2), conds=conds)\n            print('\\tfinished integrate {tmp}'.format(tmp=tmp))\n        except:\n            print('\\t\\tintegrate() failed for {tmp}'.format(tmp=tmp))\n        m[(i, j)] = ki\n    for ((i, j), ki) in np.ndenumerate(m):\n        tmp = '{mname}_{sufix}[{i}, {j}] over {var}'.format(mname=mname, sufix=sufix, i=i, j=j, var=var)\n        try:\n            if do_simplify:\n                ki = simplify(ki)\n            else:\n                ki = trigsimp(ki)\n        except:\n            print('\\t\\ttrigsimp failed {tmp}'.format(tmp=tmp))\n            ki = simplify(ki)\n        print('\\tfinished simplify {tmp}'.format(tmp=tmp))\n        m[(i, j)] = ki\n    filename = 'print_{mname}_{sufix}_over_{var}.txt'.format(mname=mname, sufix=sufix, var=var)\n    with open(filename, 'w') as f:\n\n        def myprint(sth):\n            lines.append((str(sth).strip() + '\\n'))\n        myprint(((('matrix ' + mname) + ' in file ') + filename))\n        for ((i, j), v) in np.ndenumerate(m):\n            if v:\n                myprint((mname + '[{0},{1}] = {2}'.format(i, j, v)))\n    return m\n", "label": 1}
{"function": "\n\ndef visit_Attribute(self, attr):\n    if (not isinstance(attr.ctx, ast.Load)):\n        return self.generic_visit(attr)\n    (source_explanation, source_result) = self.visit(attr.value)\n    explanation = ('%s.%s' % (source_explanation, attr.attr))\n    source = ('__exprinfo_expr.%s' % (attr.attr,))\n    co = self._compile(source)\n    try:\n        try:\n            result = self.frame.eval(co, __exprinfo_expr=source_result)\n        except AttributeError:\n            if ((not attr.attr.startswith('__')) or attr.attr.endswith('__')):\n                raise\n            source = \"getattr(__exprinfo_expr.__class__, '__name__', '')\"\n            co = self._compile(source)\n            class_name = self.frame.eval(co, __exprinfo_expr=source_result)\n            mangled_attr = (('_' + class_name) + attr.attr)\n            source = ('__exprinfo_expr.%s' % (mangled_attr,))\n            co = self._compile(source)\n            result = self.frame.eval(co, __exprinfo_expr=source_result)\n    except Exception:\n        raise Failure(explanation)\n    explanation = ('%s\\n{%s = %s.%s\\n}' % (self.frame.repr(result), self.frame.repr(result), source_explanation, attr.attr))\n    source = \"%r in getattr(__exprinfo_expr, '__dict__', {})\"\n    source = (source % (attr.attr,))\n    co = self._compile(source)\n    try:\n        from_instance = self.frame.eval(co, __exprinfo_expr=source_result)\n    except Exception:\n        from_instance = None\n    if ((from_instance is None) or self.frame.is_true(from_instance)):\n        rep = self.frame.repr(result)\n        pattern = '%s\\n{%s = %s\\n}'\n        explanation = (pattern % (rep, rep, explanation))\n    return (explanation, result)\n", "label": 1}
{"function": "\n\n@property\ndef range(self):\n    try:\n        value = self.env['HTTP_RANGE']\n        if ('=' in value):\n            (unit, sep, req_range) = value.partition('=')\n        else:\n            msg = \"The value must be prefixed with a range unit, e.g. 'bytes='\"\n            raise HTTPInvalidHeader(msg, 'Range')\n    except KeyError:\n        return None\n    if (',' in req_range):\n        msg = 'The value must be a continuous range.'\n        raise HTTPInvalidHeader(msg, 'Range')\n    try:\n        (first, sep, last) = req_range.partition('-')\n        if (not sep):\n            raise ValueError()\n        if first:\n            return (int(first), int((last or (- 1))))\n        elif last:\n            return ((- int(last)), (- 1))\n        else:\n            msg = 'The range offsets are missing.'\n            raise HTTPInvalidHeader(msg, 'Range')\n    except ValueError:\n        href = 'http://goo.gl/zZ6Ey'\n        href_text = 'HTTP/1.1 Range Requests'\n        msg = 'It must be a range formatted according to RFC 7233.'\n        raise HTTPInvalidHeader(msg, 'Range', href=href, href_text=href_text)\n", "label": 1}
{"function": "\n\ndef _GetGridParams(self, chart):\n    'Collect params related to grid lines.'\n    x = 0\n    y = 0\n    if chart.bottom.grid_spacing:\n        assert (chart.bottom.min is not None)\n        assert (chart.bottom.max is not None)\n        total = float((chart.bottom.max - chart.bottom.min))\n        x = ((100 * chart.bottom.grid_spacing) / total)\n    if chart.left.grid_spacing:\n        assert (chart.left.min is not None)\n        assert (chart.left.max is not None)\n        total = float((chart.left.max - chart.left.min))\n        y = ((100 * chart.left.grid_spacing) / total)\n    if (x or y):\n        return dict(grid=('%.3g,%.3g,1,0' % (x, y)))\n    return {\n        \n    }\n", "label": 1}
{"function": "\n\ndef process(self, event):\n    filename = event.format(self.filename)\n    if (self.max_size and os.path.exists(filename) and (os.path.getsize(filename) > self.max_size)):\n        self._rotate(filename, self.last_event, event)\n    if (('timestamp' in self.filename) and (self.last_filename != filename)):\n        if self.last_filename:\n            self._rotate(self.last_filename, self.last_event, event)\n        self.last_filename = filename\n    self.last_event = event\n    dirname = os.path.dirname(filename)\n    if (dirname and (not os.path.exists(dirname))):\n        os.makedirs(dirname)\n    with file(filename, 'a') as fout:\n        d = event.to_json()\n        self.logger.debug(('Writing to %s: %s' % (filename, d)))\n        ((print >> fout), d)\n", "label": 1}
{"function": "\n\ndef __init__(self, fname=None, is_restart=False):\n    if (not fname):\n        config_directory = os.path.expandvars('$XDG_CONFIG_HOME')\n        if (config_directory == '$XDG_CONFIG_HOME'):\n            config_directory = os.path.expanduser('~/.config')\n        fname = os.path.join(config_directory, 'qtile', 'config.py')\n    from .resources import default_config\n    if (fname == 'default'):\n        config = default_config\n    elif os.path.isfile(fname):\n        try:\n            sys.path.insert(0, os.path.dirname(fname))\n            config = __import__(os.path.basename(fname)[:(- 3)])\n        except Exception as v:\n            tb = traceback.format_exc()\n            if is_restart:\n                logger.warning('Caught exception in configuration:\\n\\n%s\\n\\nQtile restarted with default config', tb)\n                config = None\n            else:\n                raise ConfigError(tb)\n    else:\n        config = None\n    config_options = ['keys', 'mouse', 'groups', 'dgroups_key_binder', 'dgroups_app_rules', 'follow_mouse_focus', 'focus_on_window_activation', 'cursor_warp', 'layouts', 'floating_layout', 'screens', 'main', 'auto_fullscreen', 'widget_defaults', 'bring_front_click', 'wmname']\n    for option in config_options:\n        if hasattr(config, option):\n            v = getattr(config, option)\n        else:\n            v = getattr(default_config, option)\n        if (not hasattr(self, option)):\n            setattr(self, option, v)\n", "label": 1}
{"function": "\n\ndef validate_protected_resource_request(self, uri, http_method='GET', body=None, headers=None, realms=None):\n    'Create a request token response, with a new request token if valid.\\n\\n        :param uri: The full URI of the token request.\\n        :param http_method: A valid HTTP verb, i.e. GET, POST, PUT, HEAD, etc.\\n        :param body: The request body as a string.\\n        :param headers: The request headers as a dict.\\n        :param realms: A list of realms the resource is protected under.\\n                       This will be supplied to the ``validate_realms``\\n                       method of the request validator.\\n        :returns: A tuple of 2 elements.\\n                  1. True if valid, False otherwise.\\n                  2. An oauthlib.common.Request object.\\n        '\n    try:\n        request = self._create_request(uri, http_method, body, headers)\n    except errors.OAuth1Error:\n        return (False, None)\n    try:\n        self._check_transport_security(request)\n        self._check_mandatory_parameters(request)\n    except errors.OAuth1Error:\n        return (False, request)\n    if (not request.resource_owner_key):\n        return (False, request)\n    if (not self.request_validator.check_access_token(request.resource_owner_key)):\n        return (False, request)\n    if (not self.request_validator.validate_timestamp_and_nonce(request.client_key, request.timestamp, request.nonce, request, access_token=request.resource_owner_key)):\n        return (False, request)\n    valid_client = self.request_validator.validate_client_key(request.client_key, request)\n    if (not valid_client):\n        request.client_key = self.request_validator.dummy_client\n    valid_resource_owner = self.request_validator.validate_access_token(request.client_key, request.resource_owner_key, request)\n    if (not valid_resource_owner):\n        request.resource_owner_key = self.request_validator.dummy_access_token\n    valid_realm = self.request_validator.validate_realms(request.client_key, request.resource_owner_key, request, uri=request.uri, realms=realms)\n    valid_signature = self._check_signature(request)\n    request.validator_log['client'] = valid_client\n    request.validator_log['resource_owner'] = valid_resource_owner\n    request.validator_log['realm'] = valid_realm\n    request.validator_log['signature'] = valid_signature\n    v = all((valid_client, valid_resource_owner, valid_realm, valid_signature))\n    if (not v):\n        log.info('[Failure] request verification failed.')\n        log.info('Valid client: %s', valid_client)\n        log.info('Valid token: %s', valid_resource_owner)\n        log.info('Valid realm: %s', valid_realm)\n        log.info('Valid signature: %s', valid_signature)\n    return (v, request)\n", "label": 1}
{"function": "\n\n@defun_wrapped\ndef expint(ctx, n, z):\n    if (ctx.isint(n) and ctx._is_real_type(z)):\n        try:\n            return ctx._expint_int(n, z)\n        except NotImplementedError:\n            pass\n    if (ctx.isnan(n) or ctx.isnan(z)):\n        return (z * n)\n    if (z == ctx.inf):\n        return (1 / z)\n    if (z == 0):\n        if (ctx.re(n) <= 1):\n            return type(z)(ctx.inf)\n        else:\n            return (ctx.one / (n - 1))\n    if (n == 0):\n        return (ctx.exp((- z)) / z)\n    if (n == (- 1)):\n        return ((ctx.exp((- z)) * (z + 1)) / (z ** 2))\n    return ((z ** (n - 1)) * ctx.gammainc((1 - n), z))\n", "label": 1}
{"function": "\n\ndef __init__(self, name, sdb, cdb, authn_broker, authz, client_authn, symkey='', urlmap=None, iv=0, default_scope='', ca_bundle=None, seed=b'', client_authn_methods=None, authn_at_registration='', client_info_url='', secret_lifetime=86400, jwks_uri='', keyjar=None, capabilities=None, verify_ssl=True, baseurl='', hostname='', config=None, behavior=None, lifetime_policy=None, **kwargs):\n    if (not name.endswith('/')):\n        name += '/'\n    provider.Provider.__init__(self, name, sdb, cdb, authn_broker, authz, client_authn, symkey, urlmap, iv, default_scope, ca_bundle)\n    self.endp.extend([RegistrationEndpoint, ClientInfoEndpoint, RevocationEndpoint, IntrospectionEndpoint])\n    self.client_authn_methods = client_authn_methods\n    if authn_at_registration:\n        if (authn_at_registration not in client_authn_methods):\n            raise UnknownAuthnMethod(authn_at_registration)\n    self.authn_at_registration = authn_at_registration\n    self.seed = seed\n    self.client_info_url = client_info_url\n    self.secret_lifetime = secret_lifetime\n    self.jwks_uri = jwks_uri\n    self.verify_ssl = verify_ssl\n    try:\n        self.scopes = kwargs['scopes']\n    except KeyError:\n        self.scopes = ['offline_access']\n    self.keyjar = keyjar\n    if (self.keyjar is None):\n        self.keyjar = KeyJar(verify_ssl=self.verify_ssl)\n    if capabilities:\n        self.capabilities = self.provider_features(provider_config=capabilities)\n    else:\n        self.capabilities = self.provider_features()\n    self.baseurl = baseurl\n    self.hostname = (hostname or socket.gethostname())\n    self.kid = {\n        'sig': {\n            \n        },\n        'enc': {\n            \n        },\n    }\n    self.config = (config or {\n        \n    })\n    self.behavior = (behavior or {\n        \n    })\n    self.token_policy = {\n        'access_token': {\n            \n        },\n        'refresh_token': {\n            \n        },\n    }\n    if (lifetime_policy is None):\n        self.lifetime_policy = {\n            'access_token': {\n                'code': 600,\n                'token': 120,\n                'implicit': 120,\n                'authorization_code': 600,\n                'client_credentials': 600,\n                'password': 600,\n            },\n            'refresh_token': {\n                'code': 3600,\n                'token': 3600,\n                'implicit': 3600,\n                'authorization_code': 3600,\n                'client_credentials': 3600,\n                'password': 3600,\n            },\n        }\n    else:\n        self.lifetime_policy = lifetime_policy\n    self.token_handler = TokenHandler(self.baseurl, self.token_policy, keyjar=self.keyjar)\n", "label": 1}
{"function": "\n\ndef get_insert_default(self, column):\n    if (column.primary_key and (column is column.table._autoincrement_column)):\n        if (column.server_default and column.server_default.has_argument):\n            return self._execute_scalar(('select %s' % column.server_default.arg), column.type)\n        elif ((column.default is None) or (column.default.is_sequence and column.default.optional)):\n            try:\n                seq_name = column._postgresql_seq_name\n            except AttributeError:\n                tab = column.table.name\n                col = column.name\n                tab = tab[0:(29 + max(0, (29 - len(col))))]\n                col = col[0:(29 + max(0, (29 - len(tab))))]\n                name = ('%s_%s_seq' % (tab, col))\n                column._postgresql_seq_name = seq_name = name\n            sch = column.table.schema\n            if (sch is not None):\n                exc = ('select nextval(\\'\"%s\".\"%s\"\\')' % (sch, seq_name))\n            else:\n                exc = ('select nextval(\\'\"%s\"\\')' % (seq_name,))\n            return self._execute_scalar(exc, column.type)\n    return super(PGExecutionContext, self).get_insert_default(column)\n", "label": 1}
{"function": "\n\ndef create_client(self, **kwargs):\n    '\\n        Do an instantiation of a client instance\\n\\n        :param: Keyword arguments\\n            Keys are:\\n                srv_discovery_url\\n                client_info\\n                client_registration\\n                provider_info\\n                behaviour\\n        :return: client instance\\n        '\n    _key_set = set(list(kwargs.keys()))\n    try:\n        _verify_ssl = kwargs['verify_ssl']\n    except KeyError:\n        _verify_ssl = self.verify_ssl\n    else:\n        _key_set.discard('verify_ssl')\n    _client = self.client_cls(client_authn_method=CLIENT_AUTHN_METHOD, behaviour=kwargs['behaviour'], verify_ssl=_verify_ssl)\n    _key_set.discard('behaviour')\n    for param in ['allow']:\n        try:\n            setattr(_client, param, kwargs[param])\n        except KeyError:\n            pass\n        else:\n            _key_set.discard(param)\n    if (_key_set == {'client_info', 'srv_discovery_url'}):\n        _client.provider_config(kwargs['srv_discovery_url'])\n        _client.register(_client.provider_info['registration_endpoint'], **kwargs['client_info'])\n        self.get_path(kwargs['client_info']['redirect_uris'], kwargs['srv_discovery_url'])\n    elif (_key_set == {'provider_info', 'client_info'}):\n        _client.handle_provider_config(ProviderConfigurationResponse(**kwargs['provider_info']), kwargs['provider_info']['issuer'])\n        _client.register(_client.provider_info['registration_endpoint'], **kwargs['client_info'])\n        self.get_path(kwargs['client_info']['redirect_uris'], kwargs['provider_info']['issuer'])\n    elif (_key_set == {'provider_info', 'client_registration'}):\n        _client.handle_provider_config(ProviderConfigurationResponse(**kwargs['provider_info']), kwargs['provider_info']['issuer'])\n        _client.store_registration_info(ClientInfoResponse(**kwargs['client_registration']))\n        self.get_path(kwargs['client_info']['redirect_uris'], kwargs['provider_info']['issuer'])\n    elif (_key_set == {'srv_discovery_url', 'client_registration'}):\n        _client.provider_config(kwargs['srv_discovery_url'])\n        _client.store_registration_info(ClientInfoResponse(**kwargs['client_registration']))\n        self.get_path(kwargs['client_registration']['redirect_uris'], kwargs['srv_discovery_url'])\n    else:\n        raise Exception('Configuration error ?')\n    return client\n", "label": 1}
{"function": "\n\ndef __init__(self, which_norb, which_set, image_dtype='uint8'):\n    \"\\n        Reads the specified NORB dataset from a memmap cache.\\n        Creates this cache first, if necessary.\\n\\n        Parameters\\n        ----------\\n\\n        which_norb : str\\n            Valid values: 'big' or 'small'.\\n            Chooses between the (big) 'NORB dataset', and the 'Small NORB\\n            dataset'.\\n\\n        which_set : str\\n            Valid values: 'test', 'train', or 'both'.\\n            Chooses between the testing set or the training set. If 'both',\\n            the two datasets will be stacked together (testing data in the\\n            first N rows, then training data).\\n\\n        image_dtype : str, or numpy.dtype\\n            The dtype to store image data as in the memmap cache.\\n            Default is uint8, which is what the original NORB files use.\\n        \"\n    if (which_norb not in ('big', 'small')):\n        raise ValueError((\"Expected which_norb argument to be either 'big' or 'small', not '%s'\" % str(which_norb)))\n    if (which_set not in ('test', 'train', 'both')):\n        raise ValueError((\"Expected which_set argument to be either 'test' or 'train', not '%s'.\" % str(which_set)))\n    image_dtype = numpy.dtype(image_dtype)\n    self.label_index_to_name = ('category', 'instance', 'elevation', 'azimuth', 'lighting condition')\n    if (which_norb == 'big'):\n        self.label_index_to_name = (self.label_index_to_name + ('horizontal shift', 'vertical shift', 'lumination change', 'contrast', 'object scale', 'rotation'))\n    self.label_name_to_index = {\n        \n    }\n    for (index, name) in enumerate(self.label_index_to_name):\n        self.label_name_to_index[name] = index\n    self.label_to_value_funcs = (get_category_value, get_instance_value, get_elevation_value, get_azimuth_value, get_lighting_value)\n    if (which_norb == 'big'):\n        self.label_to_value_funcs = (self.label_to_value_funcs + (get_horizontal_shift_value, get_vertical_shift_value, get_lumination_change_value, get_contrast_change_value, get_scale_change_value, get_rotation_change_value))\n    image_length = (96 if (which_norb == 'small') else 108)\n\n    def read_norb_files(norb_files, output):\n        '\\n            Reads the contents of a list of norb files into a matrix.\\n            Data is assumed to be in row-major order.\\n            '\n\n        def read_norb_file(norb_file_path, debug=False):\n            '\\n                Returns the numbers in a single NORB file as a 1-D ndarray.\\n\\n                Parameters\\n                ----------\\n\\n                norb_file_path : str\\n                  A NORB file from which to read.\\n                  Can be uncompressed (*.mat) or compressed (*.mat.gz).\\n\\n                debug : bool\\n                  Set to True if you want debug printfs.\\n                '\n            if (not (norb_file_path.endswith('.mat') or norb_file_path.endswith('.mat.gz'))):\n                raise ValueError((\"Expected norb_file_path to end in either '.mat' or '.mat.gz'. Instead got '%s'\" % norb_file_path))\n            if (not os.path.isfile(norb_file_path)):\n                raise IOError((\"Could not find NORB file '%s' in expected directory '%s'.\" % reversed(os.path.split(norb_file_path))))\n            file_handle = (gzip.open(norb_file_path) if norb_file_path.endswith('.mat.gz') else open(norb_file_path))\n\n            def readNums(file_handle, num_type, count):\n                '\\n                    Reads some numbers from a file and returns them as a\\n                    numpy.ndarray.\\n\\n                    Parameters\\n                    ----------\\n\\n                    file_handle : file handle\\n                      The file handle from which to read the numbers.\\n\\n                    num_type : str, numpy.dtype\\n                      The dtype of the numbers.\\n\\n                    count : int\\n                      Reads off this many numbers.\\n                    '\n                num_bytes = (count * numpy.dtype(num_type).itemsize)\n                string = file_handle.read(num_bytes)\n                return numpy.fromstring(string, dtype=num_type)\n            (elem_type, elem_size, _num_dims, shape, num_elems) = read_header(file_handle, debug)\n            del _num_dims\n            beginning = file_handle.tell()\n            result = None\n            if isinstance(file_handle, (gzip.GzipFile, bz2.BZ2File)):\n                result = readNums(file_handle, elem_type, (num_elems * elem_size)).reshape(shape)\n            else:\n                result = numpy.fromfile(file_handle, dtype=elem_type, count=num_elems).reshape(shape)\n            return result\n        row_index = 0\n        for norb_file in norb_files:\n            print(('copying NORB file %s' % os.path.split(norb_file)[1]))\n            norb_data = read_norb_file(norb_file)\n            norb_data = norb_data.reshape((- 1), output.shape[1])\n            end_row = (row_index + norb_data.shape[0])\n            output[row_index:end_row, :] = norb_data\n            row_index = end_row\n        assert (end_row == output.shape[0])\n    if (which_norb == 'small'):\n        training_set_size = 24300\n        testing_set_size = 24300\n    else:\n        assert (which_norb == 'big')\n        num_rows_per_file = 29160\n        training_set_size = (num_rows_per_file * 10)\n        testing_set_size = (num_rows_per_file * 2)\n\n    def load_images(which_norb, which_set, dtype):\n        \"\\n            Reads image data from memmap disk cache, if available. If not, then\\n            first builds the memmap file from the NORB files.\\n\\n            Parameters\\n            ----------\\n            which_norb : str\\n            'big' or 'small'.\\n\\n            which_set : str\\n            'test', 'train', or 'both'.\\n\\n            dtype : numpy.dtype\\n            The dtype of the image memmap cache file. If a\\n            cache of this dtype doesn't exist, it will be created.\\n            \"\n        assert (type(dtype) == numpy.dtype)\n        memmap_path = get_memmap_path(which_norb, ('images_%s' % str(dtype)))\n        row_size = (2 * (image_length ** 2))\n        shape = ((training_set_size + testing_set_size), row_size)\n\n        def make_memmap():\n            dat_files = get_norb_file_paths(which_norb, 'both', 'dat')\n            memmap_dir = os.path.split(memmap_path)[0]\n            if (not os.path.isdir(memmap_dir)):\n                os.mkdir(memmap_dir)\n            print(('Allocating memmap file %s' % memmap_path))\n            writeable_memmap = numpy.memmap(filename=memmap_path, dtype=dtype, mode='w+', shape=shape)\n            read_norb_files(dat_files, writeable_memmap)\n        if (not os.path.isfile(memmap_path)):\n            print('Caching images to memmap file. This will only be done once.')\n            make_memmap()\n        images = numpy.memmap(filename=memmap_path, dtype=dtype, mode='r', shape=shape)\n        if (which_set == 'train'):\n            images = images[:training_set_size, :]\n        elif (which_set == 'test'):\n            images = images[training_set_size:, :]\n        return images\n\n    def load_labels(which_norb, which_set):\n        '\\n            Reads label data (both category and info data) from memmap disk\\n            cache, if available. If not, then first builds the memmap file from\\n            the NORB files.\\n            '\n        memmap_path = get_memmap_path(which_norb, 'labels')\n        dtype = numpy.dtype('int32')\n        row_size = (5 if (which_norb == 'small') else 11)\n        shape = ((training_set_size + testing_set_size), row_size)\n\n        def make_memmap():\n            (cat_files, info_files) = [get_norb_file_paths(which_norb, 'both', x) for x in ('cat', 'info')]\n            memmap_dir = os.path.split(memmap_path)[0]\n            if (not os.path.isdir(memmap_dir)):\n                os.mkdir(memmap_dir)\n            print(\"allocating labels' memmap...\")\n            writeable_memmap = numpy.memmap(filename=memmap_path, dtype=dtype, mode='w+', shape=shape)\n            print('... done.')\n            cat_memmap = writeable_memmap[:, :1]\n            info_memmap = writeable_memmap[:, 1:]\n            for (norb_files, memmap) in safe_zip((cat_files, info_files), (cat_memmap, info_memmap)):\n                read_norb_files(norb_files, memmap)\n        if (not os.path.isfile(memmap_path)):\n            print(('Caching images to memmap file %s.\\nThis will only be done once.' % memmap_path))\n            make_memmap()\n        labels = numpy.memmap(filename=memmap_path, dtype=dtype, mode='r', shape=shape)\n        if (which_set == 'train'):\n            labels = labels[:training_set_size, :]\n        elif (which_set == 'test'):\n            labels = labels[training_set_size:, :]\n        return labels\n\n    def get_norb_dir(which_norb):\n        datasets_dir = os.getenv('PYLEARN2_DATA_PATH')\n        if (datasets_dir is None):\n            raise RuntimeError(\"Please set the 'PYLEARN2_DATA_PATH' environment variable to tell pylearn2 where the datasets are.\")\n        if (not os.path.isdir(datasets_dir)):\n            raise IOError((\"The PYLEARN2_DATA_PATH directory (%s) doesn't exist.\" % datasets_dir))\n        return os.path.join(datasets_dir, ('norb' if (which_norb == 'big') else 'norb_small'))\n    norb_dir = get_norb_dir(which_norb)\n\n    def get_memmap_path(which_norb, file_basename):\n        assert (which_norb in ('big', 'small'))\n        assert ((file_basename == 'labels') or file_basename.startswith('images')), file_basename\n        memmap_dir = os.path.join(norb_dir, 'memmaps_of_original')\n        return os.path.join(memmap_dir, ('%s.npy' % file_basename))\n\n    def get_norb_file_paths(which_norb, which_set, norb_file_type):\n        \"\\n            Returns a list of paths for a given norb file type.\\n\\n            For example,\\n\\n                get_norb_file_paths('big', 'test', 'cat')\\n\\n            Will return the category label files ('cat') for the big NORB\\n            dataset's test set.\\n            \"\n        assert (which_set in ('train', 'test', 'both'))\n        if (which_set == 'both'):\n            return (get_norb_file_paths(which_norb, 'train', norb_file_type) + get_norb_file_paths(which_norb, 'test', norb_file_type))\n        norb_file_types = ('cat', 'dat', 'info')\n        if (norb_file_type not in norb_file_types):\n            raise ValueError((\"Expected norb_file_type to be one of %s, but it was '%s'\" % (str(norb_file_types), norb_file_type)))\n        instance_list = ('01235' if (which_set == 'test') else '46789')\n        if (which_norb == 'small'):\n            templates = [('smallnorb-5x%sx9x18x6x2x96x96-%sing-%%s.mat' % (instance_list, which_set))]\n        else:\n            numbers = range(1, (3 if (which_set == 'test') else 11))\n            templates = [('norb-5x%sx9x18x6x2x108x108-%sing-%02d-%%s.mat' % (instance_list, which_set, n)) for n in numbers]\n        original_files_dir = os.path.join(norb_dir, 'original')\n        return [os.path.join(original_files_dir, (t % norb_file_type)) for t in templates]\n\n    def make_view_converter(which_norb, which_set):\n        image_length = (96 if (which_norb == 'small') else 108)\n        datum_shape = (2, image_length, image_length, 1)\n        axes = ('b', 's', 0, 1, 'c')\n        return StereoViewConverter(datum_shape, axes)\n    images = load_images(which_norb, which_set, image_dtype)\n    labels = load_labels(which_norb, which_set)\n    view_converter = make_view_converter(which_norb, which_set)\n    super(NORB, self).__init__(X=images, y=labels, y_labels=(numpy.max(labels) + 1), view_converter=view_converter)\n    self.X_memmap_info = None\n    self.y_memmap_info = None\n", "label": 1}
{"function": "\n\n@locations_access_required\ndef non_administrative_locations_for_select2(request, domain):\n    id = request.GET.get('id')\n    query = request.GET.get('name', '').lower()\n    if id:\n        try:\n            loc = SQLLocation.objects.get(location_id=id)\n            if (loc.domain != domain):\n                raise SQLLocation.DoesNotExist()\n        except SQLLocation.DoesNotExist:\n            return json_response({\n                'message': ('no location with id %s found' % id),\n            }, status_code=404)\n        else:\n            return json_response(loc_to_payload(loc))\n    locs = []\n    user = request.couch_user\n    user_loc = user.get_sql_location(domain)\n    if user_can_edit_any_location(user, request.project):\n        locs = SQLLocation.objects.filter(domain=domain, location_type__administrative=False)\n    elif user_loc:\n        locs = user_loc.get_descendants(include_self=True, location_type__administrative=False)\n    if ((locs != []) and query):\n        locs = locs.filter(name__icontains=query)\n    return json_response(map(loc_to_payload, locs[:10]))\n", "label": 1}
{"function": "\n\ndef run(self):\n    ' Runs generator and executes tasks\\n        '\n    gen = self.gen\n    try:\n        task = next(gen)\n    except StopIteration:\n        return\n    while True:\n        try:\n            if isinstance(task, (list, tuple)):\n                assert len(task), 'Empty tasks sequence'\n                first_task = task[0]\n                if isinstance(first_task, ProcessTask):\n                    task = MultiProcessTask(task)\n                elif (GTask and isinstance(first_task, GTask)):\n                    task = MultiGTask(task)\n                else:\n                    task = MultiTask(task)\n            with task.executor_class(task.max_workers) as executor:\n                if isinstance(task, MultiTask):\n                    task = self._execute_multi_task(gen, executor, task)\n                else:\n                    task = self._execute_single_task(gen, executor, task)\n        except StopIteration:\n            break\n        except ReturnResult as e:\n            gen.close()\n            return e.result\n", "label": 1}
{"function": "\n\ndef addtoken(self, type, value, context):\n    'Add a token; return True iff this is the end of the program.'\n    ilabel = self.classify(type, value, context)\n    while True:\n        (dfa, state, node) = self.stack[(- 1)]\n        (states, first) = dfa\n        arcs = states[state]\n        for (i, newstate) in arcs:\n            (t, v) = self.grammar.labels[i]\n            if (ilabel == i):\n                assert (t < 256)\n                self.shift(type, value, newstate, context)\n                state = newstate\n                while (states[state] == [(0, state)]):\n                    self.pop()\n                    if (not self.stack):\n                        return True\n                    (dfa, state, node) = self.stack[(- 1)]\n                    (states, first) = dfa\n                return False\n            elif (t >= 256):\n                itsdfa = self.grammar.dfas[t]\n                (itsstates, itsfirst) = itsdfa\n                if (ilabel in itsfirst):\n                    self.push(t, self.grammar.dfas[t], newstate, context)\n                    break\n        else:\n            if ((0, state) in arcs):\n                self.pop()\n                if (not self.stack):\n                    raise ParseError('too much input', type, value, context)\n            else:\n                raise ParseError('bad input', type, value, context)\n", "label": 1}
{"function": "\n\ndef get_argv():\n    args = [sys.argv[0], 'tests', '--verbosity', '2']\n    attr_conditions = []\n    if ('force-all' in sys.argv):\n        return args\n    if ('cover' in sys.argv):\n        args += ['--with-coverage', '--cover-html']\n    try:\n        __import__('numpy')\n    except ImportError:\n        attr_conditions.append('not requires_numpy')\n    if (not PY2):\n        attr_conditions.append('not py2_only')\n    if PYPY:\n        attr_conditions.append('not no_pypy')\n    if ('fast' in sys.argv):\n        attr_conditions.append('not slow')\n    if ('no-internet' in sys.argv):\n        attr_conditions.append('not requires_internet')\n    attr_conditions.append('not skip')\n    attr_expression = ' and '.join(attr_conditions)\n    if attr_expression:\n        args.extend(['-A', attr_expression])\n    return args\n", "label": 1}
{"function": "\n\ndef __call__(self, request):\n    \"\\n        This function is called when a new request is received, and uses the\\n        method :meth:`Application.route_request` to select and execute the\\n        proper request handler, and then the method\\n        :meth:`Application.parse_output` to process the handler's output.\\n        \"\n    Application.current_app = self\n    self.request = request\n    try:\n        request.auto_finish = True\n        result = self.route_request(request)\n        if request.auto_finish:\n            self.parse_output(result)\n    except Exception as err:\n        try:\n            (body, status, headers) = self.handle_500(request, err)\n        except Exception:\n            log.exception(('There was a problem handling a request, and a problem running Application.handle_500 for %r.' % self))\n            (body, status, headers) = error(500, request=request)\n        if (not ('Content-Length' in headers)):\n            headers['Content-Length'] = len(body)\n        request.send_status(status)\n        request.send_headers(headers)\n        request.write(body)\n        request.finish()\n    finally:\n        if hasattr(request, '_hooks'):\n            hks = request._hooks.get('request_teardown')\n            if hks:\n                for hf in hks:\n                    try:\n                        hf(request)\n                    except Exception as err:\n                        log.exception('There was a problem handling a request teardown hook for: %r', request)\n        if hasattr(request, '_converted_match'):\n            del request._converted_match\n        Application.current_app = None\n        self.request = None\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_2d_time():\n    ' reading/writing of 2D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_2d', 'time_2d.sec'))\n    assert (data.shape == (332, 1500))\n    assert (np.abs((data[(0, 1)].real - 360.07)) <= 0.01)\n    assert (np.abs((data[(0, 1)].imag - (- 223.2))) <= 0.01)\n    assert (np.abs((data[(10, 18)].real - 17.93)) <= 0.01)\n    assert (np.abs((data[(10, 18)].imag - (- 67.2))) <= 0.01)\n    assert (dic['sw'][1] == 50000.0)\n    assert (dic['sf'][1] == 125.69)\n    assert (dic['ppm'][1] == 55.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 50.65)\n    assert (dic['ppm'][0] == 120.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef downloadManga(self):\n    print('Parsing XML File...')\n    if self.verbose_FLAG:\n        print(('XML Path = %s' % self.xmlfile_path))\n    dom = minidom.parse(self.xmlfile_path)\n    threadPool = []\n    self.options.auto = True\n    SetOutputPathToName_Flag = False\n    if (self.options.outputDir == 'DEFAULT_VALUE'):\n        SetOutputPathToName_Flag = True\n    for node in dom.getElementsByTagName('MangaSeries'):\n        seriesOptions = copy.copy(self.options)\n        seriesOptions.manga = getText(node.getElementsByTagName('name')[0])\n        seriesOptions.site = getText(node.getElementsByTagName('HostSite')[0])\n        try:\n            lastDownloaded = getText(node.getElementsByTagName('LastChapterDownloaded')[0])\n        except IndexError:\n            lastDownloaded = ''\n        try:\n            download_path = getText(node.getElementsByTagName('downloadPath')[0])\n        except IndexError:\n            download_path = ('./' + fixFormatting(seriesOptions.manga, seriesOptions.spaceToken))\n        if ((self.options.downloadPath != 'DEFAULT_VALUE') and (not os.path.isabs(download_path))):\n            download_path = os.path.join(self.options.downloadPath, download_path)\n        seriesOptions.downloadPath = download_path\n        seriesOptions.lastDownloaded = lastDownloaded\n        if SetOutputPathToName_Flag:\n            seriesOptions.outputDir = download_path\n        threadPool.append(SiteParserThread(seriesOptions, dom, node))\n    for thread in threadPool:\n        thread.start()\n        thread.join()\n    backupFileName = (self.xmlfile_path + '_bak')\n    os.rename(self.xmlfile_path, backupFileName)\n    f = open(self.xmlfile_path, 'w')\n    outputStr = '\\n'.join([line for line in dom.toprettyxml().split('\\n') if line.strip()])\n    outputStr = outputStr.encode('utf-8')\n    f.write(outputStr)\n    os.remove(backupFileName)\n", "label": 1}
{"function": "\n\ndef test_constant_output(self):\n    f = theano.function([], theano.tensor.constant([4]))\n    out = f()\n    assert (out == 4).all()\n    out[0] = 3\n    out2 = f()\n    assert (out2 is not out)\n    assert (out2 == 4).all()\n    f = theano.function([], Out(theano.tensor.constant([4]), borrow=True))\n    out = f()\n    assert (out == 4).all()\n    out[0] = 3\n    out2 = f()\n    if isinstance(theano.compile.mode.get_default_mode(), theano.compile.DebugMode):\n        assert (out2 == 4).all()\n    else:\n        assert (out2 is out)\n        assert (out2 == 3).all()\n", "label": 1}
{"function": "\n\ndef run(self):\n    with warnings.catch_warnings():\n        warnings.simplefilter('error')\n        try:\n            LOG.debug('Verify is a: {0}, with value: {1}'.format(type(self.verify), self.verify))\n            start = int(round((time.time() * 1000)))\n            if (self.test['protocol'] != 'tcp'):\n                self.req = getattr(requests, self.test['method'].lower())(verify=self.verify, **self.inputs)\n            else:\n                tcptest.tcp_test(self.host, self.port)\n            end = int(round((time.time() * 1000)))\n            self.duration_ms = (end - start)\n        except (RuntimeWarning, requests.exceptions.SSLError):\n            warnings.simplefilter('ignore')\n            start = int(round((time.time() * 1000)))\n            try:\n                self.req = getattr(requests, self.test['method'].lower())(verify=self.verify, **self.inputs)\n            except requests.exceptions.SSLError:\n                (message, status) = self.fail_test(\"Certificate verify failed and not ignored by inputs['verify']\")\n                self.add_output('SSLVerify', message, status)\n                return\n            end = int(round((time.time() * 1000)))\n            self.duration_ms = (end - start)\n            if (not self.verify_specified):\n                (message, status) = self.fail_test(\"Insecure request not ignored by inputs['verify']\")\n                self.add_output('SecureRequest', message, status)\n            elif self.verify:\n                (message, status) = self.fail_test('Insecure request made')\n                self.add_output('SecureRequest', message, status)\n            else:\n                (message, status) = self.warn_test('Insecure request made and ignored')\n                self.add_output('SecureRequest', message, status)\n    if (self.test['protocol'] != 'tcp'):\n        self.output.append(dict(self.req.headers), sec='response_headers')\n        self.output.append(self.req.status_code, sec='response_status_code')\n        if ('show_body' in self.test):\n            try:\n                req_content = self.req.content.decode()\n            except UnicodeDecodeError:\n                req_content = self.req.content\n            self.output.append(req_content)\n        for plugin_info in manager.getAllPlugins():\n            for outcome in self.test['outcomes']:\n                if (plugin_info.name == outcome):\n                    manager.activatePluginByName(plugin_info.name)\n                    (message, status) = plugin_info.plugin_object.run(self)\n                    self.add_output(plugin_info.name, message, status)\n                    manager.deactivatePluginByName(plugin_info.name)\n", "label": 1}
{"function": "\n\ndef html_visit_bokeh_plot(self, node):\n    env = self.builder.env\n    dest_dir = join(self.builder.outdir, node['relpath'])\n    if (settings.docs_cdn() == 'local'):\n        resources = Resources(mode='server', root_url='/en/latest/')\n    else:\n        resources = Resources(mode='cdn')\n    try:\n        if ('path' in node):\n            path = node['path']\n            filename = ('bokeh-plot-%s.js' % hashlib.md5(path.encode('utf-8')).hexdigest())\n            dest_path = join(dest_dir, filename)\n            tmpdir = join(env.bokeh_plot_tmpdir, node['relpath'])\n            if (not exists(tmpdir)):\n                makedirs(tmpdir)\n            cached_path = join(tmpdir, filename)\n            if (out_of_date(path, cached_path) or (not exists((cached_path + '.script')))):\n                self.builder.app.verbose((\"generating new plot for '%s'\" % path))\n                plot = _render_plot(node['source'], node.get('symbol'))\n                (js, script) = autoload_static(plot, resources, filename)\n                with open(cached_path, 'w') as f:\n                    f.write(js)\n                with open((cached_path + '.script'), 'w') as f:\n                    f.write(script)\n            else:\n                self.builder.app.verbose((\"using cached plot for '%s'\" % path))\n                script = open((cached_path + '.script'), 'r').read()\n            if (not exists(dest_dir)):\n                makedirs(dest_dir)\n            copy(cached_path, dest_path)\n        else:\n            filename = (node['target_id'] + '.js')\n            if (not exists(dest_dir)):\n                makedirs(dest_dir)\n            dest_path = join(dest_dir, filename)\n            plot = _render_plot(node['source'], None)\n            (js, script) = autoload_static(plot, resources, filename)\n            self.builder.app.verbose(('saving inline plot at: %s' % dest_path))\n            with open(dest_path, 'w') as f:\n                f.write(js)\n        html = SCRIPT_TEMPLATE.render(script=script)\n        self.body.append(html)\n    except Exception:\n        err_node = nodes.error(None, nodes.paragraph(text=('Unable to generate Bokeh plot at %s:%d:' % (node['rst_source'], node['rst_lineno']))), nodes.paragraph(text=str(sys.exc_info()[1])))\n        node.children.append(err_node)\n        raise nodes.SkipDeparture\n    else:\n        raise nodes.SkipNode\n", "label": 1}
{"function": "\n\ndef assert_warns_message(warning_class, message, func, *args, **kw):\n    'Test that a certain warning occurs and with a certain message.\\n\\n    Parameters\\n    ----------\\n    warning_class : the warning class\\n        The class to test for, e.g. UserWarning.\\n\\n    message : str | callable\\n        The entire message or a substring to  test for. If callable,\\n        it takes a string as argument and will trigger an assertion error\\n        if it returns `False`.\\n\\n    func : callable\\n        Calable object to trigger warnings.\\n\\n    *args : the positional arguments to `func`.\\n\\n    **kw : the keyword arguments to `func`.\\n\\n    Returns\\n    -------\\n\\n    result : the return value of `func`\\n\\n    '\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            warnings.simplefilter('ignore', np.VisibleDeprecationWarning)\n        result = func(*args, **kw)\n        if (not (len(w) > 0)):\n            raise AssertionError(('No warning raised when calling %s' % func.__name__))\n        found = [(warning.category is warning_class) for warning in w]\n        if (not any(found)):\n            raise AssertionError(('No warning raised for %s with class %s' % (func.__name__, warning_class)))\n        message_found = False\n        for index in [i for (i, x) in enumerate(found) if x]:\n            msg = w[index].message\n            msg = str((msg.args[0] if hasattr(msg, 'args') else msg))\n            if callable(message):\n                check_in_message = message\n            else:\n                check_in_message = (lambda msg: (message in msg))\n            if check_in_message(msg):\n                message_found = True\n                break\n        if (not message_found):\n            raise AssertionError((\"Did not receive the message you expected ('%s') for <%s>.\" % (message, func.__name__)))\n    return result\n", "label": 1}
{"function": "\n\n@register.simple_tag\ndef bmffield(field):\n    if isinstance(field.field, forms.models.ModelMultipleChoiceField):\n        return field.as_widget(attrs={\n            'class': 'form-control',\n        })\n    elif isinstance(field.field, forms.models.ModelChoiceField):\n        model = field.field.choices.queryset.model\n        if hasattr(model, '_bmfmeta'):\n            value = field.value()\n            if value:\n                try:\n                    text = field.field.choices.queryset.get(pk=field.value())\n                except field.field.choices.queryset.model.DoesNotExist:\n                    text = None\n                    value = None\n            else:\n                text = None\n            if field.field.widget.attrs.get('readonly', False):\n                data = ('<p class=\"form-control-static\">%s</p>' % (text or ('<i>%s</i>' % _('empty'))))\n                data += field.as_hidden(attrs={\n                    'autocomplete': 'off',\n                    'value': value,\n                })\n                return data\n            else:\n                data = '<div class=\"input-group\" data-bmf-autocomplete=\"1\">'\n                data += field.as_text(attrs={\n                    'class': 'form-control',\n                    'id': ('%s-value' % field.auto_id),\n                    'placeholder': (text or ''),\n                    'autocomplete': 'off',\n                    'name': '',\n                })\n                data += '</div>'\n                data += field.as_hidden(attrs={\n                    'autocomplete': 'off',\n                })\n                return data\n        else:\n            return field.as_widget(attrs={\n                'class': 'form-control',\n            })\n    elif isinstance(field.field, forms.DateTimeField):\n        data = '<div class=\"input-group\" data-bmf-calendar=\"dt\">'\n        data += field.as_widget(attrs={\n            'class': 'form-control',\n            'autocomplete': 'off',\n        })\n        data += '</div>'\n        return data\n    elif isinstance(field.field, forms.DateField):\n        data = '<div class=\"input-group\" data-bmf-calendar=\"d\">'\n        data += field.as_widget(attrs={\n            'class': 'form-control',\n            'autocomplete': 'off',\n        })\n        data += '</div>'\n        return data\n    elif isinstance(field.field, forms.TimeField):\n        data = '<div class=\"input-group\" data-bmf-calendar=\"t\">'\n        data += field.as_widget(attrs={\n            'class': 'form-control',\n            'autocomplete': 'off',\n        })\n        data += '</div>'\n        return data\n    else:\n        return field.as_widget(attrs={\n            'class': 'form-control',\n        })\n", "label": 1}
{"function": "\n\ndef usage(self):\n    print('Usage:')\n    print('    buildozer [--profile <name>] [--verbose] [target] <command>...')\n    print('    buildozer --version')\n    print('')\n    print('Available targets:')\n    targets = list(self.targets())\n    for (target, m) in targets:\n        try:\n            doc = m.__doc__.strip().splitlines()[0].strip()\n        except Exception:\n            doc = '<no description>'\n        print('  {0:<18} {1}'.format(target, doc))\n    print('')\n    print('Global commands (without target):')\n    cmds = [x for x in dir(self) if x.startswith('cmd_')]\n    for cmd in cmds:\n        name = cmd[4:]\n        meth = getattr(self, cmd)\n        if (not meth.__doc__):\n            continue\n        doc = [x for x in meth.__doc__.strip().splitlines()][0].strip()\n        print('  {0:<18} {1}'.format(name, doc))\n    print('')\n    print('Target commands:')\n    print('  clean      Clean the target environment')\n    print('  update     Update the target dependencies')\n    print('  debug      Build the application in debug mode')\n    print('  release    Build the application in release mode')\n    print('  deploy     Deploy the application on the device')\n    print('  run        Run the application on the device')\n    print('  serve      Serve the bin directory via SimpleHTTPServer')\n    for (target, m) in targets:\n        mt = m.get_target(self)\n        commands = mt.get_custom_commands()\n        if (not commands):\n            continue\n        print('')\n        print('Target \"{0}\" commands:'.format(target))\n        for (command, doc) in commands:\n            if (not doc):\n                continue\n            doc = doc.strip().splitlines()[0].strip()\n            print('  {0:<18} {1}'.format(command, doc))\n    print('')\n", "label": 1}
{"function": "\n\ndef get_boot_device(self, task):\n    \"Get the current boot device for the task's node.\\n\\n        Returns the current boot device of the node.\\n\\n        :param task: a task from TaskManager.\\n        :raises: MissingParameterValue if required IPMI parameters\\n            are missing.\\n        :raises: IPMIFailure on an error from pyghmi.\\n        :returns: a dictionary containing:\\n\\n            :boot_device: the boot device, one of\\n                :mod:`ironic.common.boot_devices` or None if it is unknown.\\n            :persistent: Whether the boot device will persist to all\\n                future boots or not, None if it is unknown.\\n\\n        \"\n    driver_info = task.node.driver_info\n    driver_internal_info = task.node.driver_internal_info\n    if (driver_info.get('ipmi_force_boot_device', False) and driver_internal_info.get('persistent_boot_device') and driver_internal_info.get('is_next_boot_persistent', True)):\n        return {\n            'boot_device': driver_internal_info['persistent_boot_device'],\n            'persistent': True,\n        }\n    driver_info = _parse_driver_info(task.node)\n    response = {\n        'boot_device': None,\n    }\n    try:\n        ipmicmd = ipmi_command.Command(bmc=driver_info['address'], userid=driver_info['username'], password=driver_info['password'])\n        ret = ipmicmd.get_bootdev()\n        if ('error' in ret):\n            raise pyghmi_exception.IpmiException(ret['error'])\n    except pyghmi_exception.IpmiException as e:\n        LOG.error(_LE('IPMI get boot device failed for node %(node_id)s with the following error: %(error)s'), {\n            'node_id': driver_info['uuid'],\n            'error': e,\n        })\n        raise exception.IPMIFailure(cmd=e)\n    response['persistent'] = ret.get('persistent')\n    bootdev = ret.get('bootdev')\n    if bootdev:\n        response['boot_device'] = next((dev for (dev, hdev) in _BOOT_DEVICES_MAP.items() if (hdev == bootdev)), None)\n    return response\n", "label": 1}
{"function": "\n\ndef test_connection_switch_workload():\n    qname = 'swapworkloadq'\n    first = disq.Disque(port=7711, record_job_origin=True)\n    for _ in range(90):\n        first.addjob(qname, 'foobar')\n    second = disq.Disque(port=7712, record_job_origin=True)\n    for _ in range(10):\n        second.addjob(qname, 'foobar')\n    (conn, node) = second._get_connection('GETJOB')\n    assert (node == second.default_node)\n    second._release_connection(conn, node)\n    for _ in range(100):\n        second.getjob(qname)\n    assert (first.default_node in second._job_score.keys())\n    (conn, node) = second._get_connection('GETJOB')\n    assert (node != second.default_node)\n    assert (node == first.default_node)\n    second._release_connection(conn, node)\n    (conn, node) = second._get_connection('ADDJOB')\n    assert (node == second.default_node)\n    assert (node != first.default_node)\n    second._release_connection(conn, node)\n", "label": 1}
{"function": "\n\ndef __call__(self, module):\n    self.module = module\n    super(GirderClientModule, self).__init__(**{p: self.module.params[p] for p in ['host', 'port', 'apiRoot', 'scheme', 'dryrun', 'blacklist'] if (module.params[p] is not None)})\n    if (self.module.params['username'] is not None):\n        try:\n            self.authenticate(username=self.module.params['username'], password=self.module.params['password'])\n        except AuthenticationError:\n            self.fail('Could not Authenticate!')\n    elif (self.module.params['token'] is not None):\n        self.token = self.module.params['token']\n    elif (self.module.params['user'] is None):\n        self.fail('Must pass in either username & password, or a valid girder_client token')\n    self.message['token'] = self.token\n    for method in self.required_one_of:\n        if (self.module.params[method] is not None):\n            self.__process(method)\n            self.exit()\n    self.fail('Could not find executable method!')\n", "label": 1}
{"function": "\n\ndef publish_test(self, target, artifacts, pushdb_files, extra_options=None, extra_config=None, extra_env=None, expected_primary_artifact_count=1, success_expected=True, assert_publish_config_contents=False):\n    \"Tests that publishing the given target results in the expected output.\\n\\n    :param target: Target to test.\\n    :param artifacts: A map from directories to a list of expected filenames.\\n    :param pushdb_files: list of pushdb files that would be created if this weren't a local publish\\n    :param extra_options: Extra command-line options to the pants run.\\n    :param extra_config: Extra pants.ini configuration for the pants run.\\n    :param expected_primary_artifact_count: Number of artifacts we expect to be published.\\n    :param extra_env: Extra environment variables for the pants run.\\n    :param assert_publish_config_contents: Test the contents of the generated ivy and pom file.\\n           If set to True, compares the generated ivy.xml and pom files in\\n           tests/python/pants_test/tasks/jar_publish_resources/<package_name>/<artifact_name>/\\n    \"\n    with temporary_dir() as publish_dir:\n        options = ['--local={}'.format(publish_dir), '--no-dryrun', '--force']\n        if extra_options:\n            options.extend(extra_options)\n        pants_run = self.run_pants(((['publish.jar'] + options) + [target]), config=extra_config, extra_env=extra_env)\n        if success_expected:\n            self.assert_success(pants_run, \"'pants goal publish' expected success, but failed instead.\")\n        else:\n            self.assert_failure(pants_run, \"'pants goal publish' expected failure, but succeeded instead.\")\n            return\n        for pushdb_file in pushdb_files:\n            pushdb_dir = os.path.dirname(os.path.join(self.pushdb_root, pushdb_file))\n            self.assertTrue(os.path.exists(pushdb_dir))\n        for pushdb_file in pushdb_files:\n            self.assertFalse(os.path.exists(os.path.join(self.pushdb_root, pushdb_file)))\n        for (directory, artifact_list) in artifacts.items():\n            for artifact in artifact_list:\n                artifact_path = os.path.join(publish_dir, directory, artifact)\n                self.assertTrue(os.path.exists(artifact_path))\n                if assert_publish_config_contents:\n                    if (artifact.endswith('xml') or artifact.endswith('pom')):\n                        self.compare_file_contents(artifact_path, directory)\n", "label": 1}
{"function": "\n\ndef test_parser_with_syntax_errors(broken_status):\n\n    def catch_syntax_error(seq):\n        datafile = broken_status.join(('%d.txt' % seq))\n        parser = LogParser.fromstring(datafile.read())\n        with raises(ParsingError) as error:\n            parser.parse()\n        return error\n    error = catch_syntax_error(0)\n    assert (not error.value.args[0].startswith('expected list'))\n    assert (not error.value.args[0].startswith('expected 2-tuple'))\n    assert error.value.args[0].endswith('got end of input')\n    error = catch_syntax_error(1)\n    assert (not error.value.args[0].startswith('expected list'))\n    assert (not error.value.args[0].startswith('expected 2-tuple'))\n    assert error.value.args[0].endswith(('got %r' % 'BrokenVPN CLIENT LIST'))\n    error = catch_syntax_error(2)\n    assert (error.value.args[0] == 'expected list but got end of input')\n    error = catch_syntax_error(3)\n    assert (error.value.args[0] == 'expected 2-tuple but got end of input')\n    error = catch_syntax_error(4)\n    assert (error.value.args[0] == ('expected 2-tuple but got %r' % 'Updated,Yo,Hoo'))\n    error = catch_syntax_error(5)\n    assert (error.value.args[0] == ('expected 2-tuple starting with %r' % 'Updated'))\n    error = catch_syntax_error(6)\n    assert (error.value.args[0] == ('expected list but got %r' % 'YO TABLE'))\n", "label": 1}
{"function": "\n\ndef run(self, args):\n    '\\n        Run the SPM command\\n        '\n    command = args[0]\n    try:\n        if (command == 'install'):\n            self._install(args)\n        elif (command == 'local'):\n            self._local(args)\n        elif (command == 'remove'):\n            self._remove(args)\n        elif (command == 'build'):\n            self._build(args)\n        elif (command == 'update_repo'):\n            self._download_repo_metadata(args)\n        elif (command == 'create_repo'):\n            self._create_repo(args)\n        elif (command == 'files'):\n            self._list_files(args)\n        elif (command == 'info'):\n            self._info(args)\n        else:\n            raise SPMInvocationError(\"Invalid command '{0}'\".format(command))\n    except SPMException as exc:\n        self.ui.error(str(exc))\n", "label": 1}
{"function": "\n\ndef _init_cgroups(self):\n    '\\n        This function initializes the cgroups for the limitations and measurements.\\n        '\n    self.cgroups = find_my_cgroups()\n    for subsystem in self._cgroup_subsystems:\n        self.cgroups.require_subsystem(subsystem)\n        if (subsystem not in self.cgroups):\n            sys.exit('Required cgroup subsystem \"{}\" is missing.'.format(subsystem))\n    self.cgroups.require_subsystem(CPUACCT)\n    if (CPUACCT not in self.cgroups):\n        logging.warning('Without cpuacct cgroups, cputime measurement and limit might not work correctly if subprocesses are started.')\n    self.cgroups.require_subsystem(FREEZER)\n    if (FREEZER not in self.cgroups):\n        if (self._user is not None):\n            sys.exit((('Cannot reliably kill sub-processes without freezer cgroup,' + ' this is necessary if --user is specified.') + ' Please enable this cgroup or do not specify --user.'))\n        else:\n            logging.warning('Cannot reliably kill sub-processes without freezer cgroup.')\n    self.cgroups.require_subsystem(MEMORY)\n    if (MEMORY not in self.cgroups):\n        logging.warning('Cannot measure memory consumption without memory cgroup.')\n    elif (systeminfo.has_swap() and (not self.cgroups.has_value(MEMORY, 'memsw.max_usage_in_bytes'))):\n        logging.warning('Kernel misses feature for accounting swap memory, but machine has swap. Memory usage may be measured inaccurately. Please set swapaccount=1 on your kernel command line or disable swap with \"sudo swapoff -a\".')\n    self.cgroups.require_subsystem(CPUSET)\n    self.cpus = None\n    self.memory_nodes = None\n    if (CPUSET in self.cgroups):\n        try:\n            self.cpus = util.parse_int_list(self.cgroups.get_value(CPUSET, 'cpus'))\n        except ValueError as e:\n            logging.warning('Could not read available CPU cores from kernel: %s', e.strerror)\n        logging.debug('List of available CPU cores is %s.', self.cpus)\n        try:\n            self.memory_nodes = util.parse_int_list(self.cgroups.get_value(CPUSET, 'mems'))\n        except ValueError as e:\n            logging.warning('Could not read available memory nodes from kernel: %s', e.strerror)\n        logging.debug('List of available memory nodes is %s.', self.memory_nodes)\n", "label": 1}
{"function": "\n\ndef send(func, *args, **kwargs):\n    \"\\n    Send a specific function to the mine.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' mine.send network.ip_addrs eth0\\n        salt '*' mine.send eth0_ip_addrs mine_function=network.ip_addrs eth0\\n    \"\n    kwargs = salt.utils.clean_kwargs(**kwargs)\n    mine_func = kwargs.pop('mine_function', func)\n    if (mine_func not in __salt__):\n        return False\n    data = {\n        \n    }\n    arg_data = salt.utils.arg_lookup(__salt__[mine_func])\n    func_data = copy.deepcopy(kwargs)\n    for (ind, _) in enumerate(arg_data.get('args', [])):\n        try:\n            func_data[arg_data['args'][ind]] = args[ind]\n        except IndexError:\n            pass\n    f_call = salt.utils.format_call(__salt__[mine_func], func_data, expected_extra_kws=MINE_INTERNAL_KEYWORDS)\n    for arg in args:\n        if (arg not in f_call['args']):\n            f_call['args'].append(arg)\n    try:\n        if ('kwargs' in f_call):\n            data[func] = __salt__[mine_func](*f_call['args'], **f_call['kwargs'])\n        else:\n            data[func] = __salt__[mine_func](*f_call['args'])\n    except Exception as exc:\n        log.error('Function {0} in mine.send failed to execute: {1}'.format(mine_func, exc))\n        return False\n    if (__opts__['file_client'] == 'local'):\n        old = __salt__['data.get']('mine_cache')\n        if isinstance(old, dict):\n            old.update(data)\n            data = old\n        return __salt__['data.update']('mine_cache', data)\n    load = {\n        'cmd': '_mine',\n        'data': data,\n        'id': __opts__['id'],\n    }\n    return _mine_send(load, __opts__)\n", "label": 1}
{"function": "\n\ndef __init__(self, request, args=None, kwargs=None, payload=None, progress=None, enc_algo=None, enc_key=None, enc_serializer=None):\n    '\\n\\n        :param request: The WAMP request ID of the original call.\\n        :type request: int\\n        :param args: Positional values for application-defined event payload.\\n           Must be serializable using any serializers in use.\\n        :type args: list or tuple or None\\n        :param kwargs: Keyword values for application-defined event payload.\\n           Must be serializable using any serializers in use.\\n        :type kwargs: dict or None\\n        :param payload: Alternative, transparent payload. If given, `args` and `kwargs` must be left unset.\\n        :type payload: unicode or bytes\\n        :param progress: If ``True``, this result is a progressive invocation result, and subsequent\\n           results (or a final error) will follow.\\n        :type progress: bool or None\\n        :param enc_algo: If using payload encryption, the algorithm used (currently, only \"cryptobox\" is valid).\\n        :type enc_algo: unicode\\n        :param enc_key: If using payload encryption, the message encryption key.\\n        :type enc_key: unicode or binary\\n        :param enc_serializer: If using payload encryption, the encrypted payload object serializer.\\n        :type enc_serializer: unicode\\n        '\n    assert (type(request) in six.integer_types)\n    assert ((args is None) or (type(args) in [list, tuple]))\n    assert ((kwargs is None) or (type(kwargs) == dict))\n    assert ((payload is None) or (type(payload) in [six.text_type, six.binary_type]))\n    assert ((payload is None) or ((payload is not None) and (args is None) and (kwargs is None)))\n    assert ((progress is None) or (type(progress) == bool))\n    assert ((enc_algo is None) or (enc_algo in [PAYLOAD_ENC_CRYPTO_BOX]))\n    assert ((enc_key is None) or (type(enc_key) in [six.text_type, six.binary_type]))\n    assert ((enc_serializer is None) or (enc_serializer in ['json', 'msgpack', 'cbor', 'ubjson']))\n    assert (((enc_algo is None) and (enc_key is None) and (enc_serializer is None)) or ((enc_algo is not None) and (payload is not None)))\n    Message.__init__(self)\n    self.request = request\n    self.args = args\n    self.kwargs = kwargs\n    self.payload = payload\n    self.progress = progress\n    self.enc_algo = enc_algo\n    self.enc_key = enc_key\n    self.enc_serializer = enc_serializer\n", "label": 1}
{"function": "\n\n@property\ndef kwargs(self):\n    kwargs = {\n        \n    }\n    kwargs_started = False\n    for (param_name, param) in self._signature.parameters.items():\n        if (not kwargs_started):\n            if ((param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY)) or param._partial_kwarg):\n                kwargs_started = True\n            elif (param_name not in self.arguments):\n                kwargs_started = True\n                continue\n        if (not kwargs_started):\n            continue\n        try:\n            arg = self.arguments[param_name]\n        except KeyError:\n            pass\n        else:\n            if (param.kind == _VAR_KEYWORD):\n                kwargs.update(arg)\n            else:\n                kwargs[param_name] = arg\n    return kwargs\n", "label": 1}
{"function": "\n\ndef fails(self):\n    validate_data = getattr(self, 'validate_data')\n    for k in validate_data:\n        rules = validate_data.get(k, None)\n        if (not rules):\n            raise InvalidValidateDataError()\n        else:\n            rules_list = rules.split('|')\n            for rule in rules_list:\n                rule_origin = rule\n                rule = rule.split(':')[0]\n                if ((rule is None) or (rule == '')):\n                    raise InvalidRuleNameError(rule=rule)\n                rule_validator = import_module('.rules', package='django_laravel_validator')\n                try:\n                    regex = getattr(rule_validator, rule.upper())\n                except AttributeError:\n                    raise InvalidRuleNameError(rule=rule)\n                if (rule.upper() in WITH_PARAMETERS_VALIDATOR):\n                    rule_args = format_args_split(rule_origin)\n                    regex = regex(rule_args)\n                else:\n                    regex = regex()\n                try:\n                    setattr(regex, 'validator_instance', self)\n                    regex(self.data.get(k, None))\n                except ValidationError as e:\n                    error_message = error_message_generate(k, rule, self.message, e)\n                    error_dict = self.error_list.get(k)\n                    error_dict.update({\n                        rule: error_message,\n                    })\n                    self.error_list.get(k).update(error_dict)\n                    self.validate_flag = False\n    check = getattr(self, 'check', None)\n    if (check and callable(check) and self.validate_flag):\n        check()\n    return check_errors(self.error_list, self.error_list_ext)\n", "label": 1}
{"function": "\n\ndef _check_chain(r, chain):\n    '\\n    WRITEME\\n\\n    '\n    chain = list(reversed(chain))\n    while chain:\n        elem = chain.pop()\n        if (elem is None):\n            if (r.owner is not None):\n                return False\n        elif (r.owner is None):\n            return False\n        elif isinstance(elem, op.Op):\n            if (not (r.owner.op == elem)):\n                return False\n        else:\n            try:\n                if (issubclass(elem, op.Op) and (not isinstance(r.owner.op, elem))):\n                    return False\n            except TypeError:\n                return False\n        if chain:\n            r = r.owner.inputs[chain.pop()]\n    return (r is not None)\n", "label": 1}
{"function": "\n\ndef _validate(response):\n    status_code_to_exc = {\n        400: InvalidRequestError,\n        401: NotAuthorizedError,\n        402: MessageRejectedError,\n        403: NotAuthorizedError,\n        404: NotFoundError,\n        405: MethodNotSupportedError,\n        409: ConflictError,\n        429: SendingQuotaExceededError,\n        500: ServerError,\n        503: ServiceUnavailableError,\n        504: ServerTimeoutError,\n    }\n    request = response.request\n    url = request.url\n    status_code = response.status_code\n    data = request.body\n    if DEBUG:\n        print('{} {} ({}) => {}: {}'.format(request.method, url, data, status_code, response.text))\n    try:\n        data = (json.loads(data) if data else None)\n    except (ValueError, TypeError):\n        pass\n    if (status_code == 200):\n        return response\n    elif (status_code in status_code_to_exc):\n        cls = status_code_to_exc[status_code]\n        try:\n            response = json.loads(response.text)\n            kwargs = dict(url=url, status_code=status_code, data=data)\n            for key in ['message', 'server_error']:\n                if (key in response):\n                    kwargs[key] = response[key]\n            raise cls(**kwargs)\n        except (ValueError, TypeError):\n            raise cls(url=url, status_code=status_code, data=data, message='Malformed')\n    else:\n        raise APIClientError(url=url, status_code=status_code, data=data, message='Unknown status code.')\n", "label": 1}
{"function": "\n\ndef stop_workflow(workflow_dict, task=None):\n    LOG.info('Running undo...')\n    if ('steps' not in workflow_dict):\n        return False\n    if ('exceptions' not in workflow_dict):\n        workflow_dict['exceptions'] = {\n            \n        }\n        workflow_dict['exceptions']['traceback'] = []\n        workflow_dict['exceptions']['error_codes'] = []\n    workflow_dict['total_steps'] = len(workflow_dict['steps'])\n    if ('step_counter' not in workflow_dict):\n        workflow_dict['step_counter'] = len(workflow_dict['steps'])\n    workflow_dict['msgs'] = []\n    workflow_dict['created'] = False\n    try:\n        for step in workflow_dict['steps'][::(- 1)]:\n            my_class = import_by_path(step)\n            my_instance = my_class()\n            time_now = str(time.strftime('%m/%d/%Y %H:%M:%S'))\n            msg = ('\\n%s - Rollback Step %i of %i - %s' % (time_now, workflow_dict['step_counter'], workflow_dict['total_steps'], str(my_instance)))\n            LOG.info(msg)\n            workflow_dict['step_counter'] -= 1\n            if task:\n                workflow_dict['msgs'].append(msg)\n                task.update_details(persist=True, details=msg)\n            my_instance.undo(workflow_dict)\n            if task:\n                task.update_details(persist=True, details='DONE!')\n        return True\n    except Exception as e:\n        LOG.info('Exception: {}'.format(e))\n        if ((not workflow_dict['exceptions']['error_codes']) or (not workflow_dict['exceptions']['traceback'])):\n            traceback = full_stack()\n            workflow_dict['exceptions']['error_codes'].append(DBAAS_0001)\n            workflow_dict['exceptions']['traceback'].append(traceback)\n        LOG.warn('\\n'.join((': '.join(error) for error in workflow_dict['exceptions']['error_codes'])))\n        LOG.warn('\\nException Traceback\\n'.join(workflow_dict['exceptions']['traceback']))\n        return False\n", "label": 1}
{"function": "\n\ndef run(self, variables=None):\n    'Run script.'\n    with self._lock:\n        if (self._cur == (- 1)):\n            self._log('Running script')\n            self._cur = 0\n        self._remove_listener()\n        for (cur, action) in islice(enumerate(self.sequence), self._cur, None):\n            if (CONF_DELAY in action):\n\n                def script_delay(now):\n                    'Called after delay is done.'\n                    self._delay_listener = None\n                    self.run(variables)\n                self._delay_listener = track_point_in_utc_time(self.hass, script_delay, (date_util.utcnow() + action[CONF_DELAY]))\n                self._cur = (cur + 1)\n                if self._change_listener:\n                    self._change_listener()\n                return\n            elif (CONF_CONDITION in action):\n                if (not self._check_condition(action, variables)):\n                    break\n            elif (CONF_EVENT in action):\n                self._fire_event(action)\n            else:\n                self._call_service(action, variables)\n        self._cur = (- 1)\n        self.last_action = None\n        if self._change_listener:\n            self._change_listener()\n", "label": 1}
{"function": "\n\ndef computeExpressionCall(self, call_node, call_args, call_kw, constraint_collection):\n    constraint_collection.onExceptionRaiseExit(BaseException)\n    if ((call_kw is not None) and ((not call_kw.isExpressionConstantRef()) or (call_kw.getConstant() != {\n        \n    }))):\n        return (call_node, None, None)\n    if (call_kw is not None):\n        return (call_node, None, None)\n    if (call_args is None):\n        args_tuple = ()\n    elif (call_args.isExpressionConstantRef() or call_args.isExpressionMakeTuple()):\n        args_tuple = call_args.getIterationValues()\n    else:\n        assert False, call_args\n        return (call_node, None, None)\n    function_body = self.getFunctionRef().getFunctionBody()\n    call_spec = function_body.getParameters()\n    try:\n        args_dict = matchCall(func_name=self.getName(), args=call_spec.getArgumentNames(), star_list_arg=call_spec.getStarListArgumentName(), star_dict_arg=call_spec.getStarDictArgumentName(), num_defaults=call_spec.getDefaultCount(), positional=args_tuple, pairs=())\n        values = [args_dict[name] for name in call_spec.getParameterNames()]\n        result = ExpressionFunctionCall(function=self, values=values, source_ref=call_node.getSourceReference())\n        return (result, 'new_statements', (\"Replaced call to created function body '%s' with direct function call.\" % self.getName()))\n    except TooManyArguments as e:\n        result = wrapExpressionWithSideEffects(new_node=makeRaiseExceptionReplacementExpressionFromInstance(expression=call_node, exception=e.getRealException()), old_node=call_node, side_effects=call_node.extractPreCallSideEffects())\n        return (result, 'new_raise', (\"Replaced call to created function body '%s' to argument error\" % self.getName()))\n", "label": 1}
{"function": "\n\ndef test_sharding(self):\n    tests = ['foo/bar.py', 'foo/baz.py', 'foo.bar.test_biz', 'foo.bar.test_buz']\n    test_weights = {\n        ('foo', 'bar'): 50,\n        ('foo', 'baz'): 15,\n        ('foo', 'bar', 'test_biz'): 10,\n        ('foo', 'bar', 'test_buz'): 200,\n    }\n    avg_test_time = (sum(test_weights.values()) / len(test_weights))\n    groups = TestsExpander.shard_tests(tests, 2, test_weights, avg_test_time)\n    assert (len(groups) == 2)\n    groups.sort()\n    assert (groups[0] == (78, ['foo/bar.py', 'foo/baz.py', 'foo.bar.test_biz']))\n    assert (groups[1] == (201, ['foo.bar.test_buz']))\n    groups = TestsExpander.shard_tests(tests, 3, test_weights, avg_test_time)\n    assert (len(groups) == 3)\n    groups.sort()\n    assert (groups[0] == (27, ['foo/baz.py', 'foo.bar.test_biz']))\n    assert (groups[1] == (51, ['foo/bar.py']))\n    assert (groups[2] == (201, ['foo.bar.test_buz']))\n    groups = TestsExpander.shard_tests(tests, (len(tests) * 2), test_weights, avg_test_time)\n    assert (len(groups) == len(tests))\n", "label": 1}
{"function": "\n\ndef hilite(self):\n    '\\n        Pass code to the [Pygments](http://pygments.pocoo.org/) highliter with\\n        optional line numbers. The output should then be styled with css to\\n        your liking. No styles are applied by default - only styling hooks\\n        (i.e.: <span class=\"k\">).\\n\\n        returns : A string of html.\\n\\n        '\n    self.src = self.src.strip('\\n')\n    if (self.lang is None):\n        self._getLang()\n    if pygments:\n        try:\n            lexer = get_lexer_by_name(self.lang)\n        except ValueError:\n            try:\n                if self.guess_lang:\n                    lexer = guess_lexer(self.src)\n                else:\n                    lexer = TextLexer()\n            except ValueError:\n                lexer = TextLexer()\n        formatter = HtmlFormatter(linenos=self.linenums, cssclass=self.css_class, style=self.style, noclasses=self.noclasses)\n        return highlight(self.src, lexer, formatter)\n    else:\n        txt = self.src.replace('&', '&amp;')\n        txt = txt.replace('<', '&lt;')\n        txt = txt.replace('>', '&gt;')\n        txt = txt.replace('\"', '&quot;')\n        classes = []\n        if self.lang:\n            classes.append(('language-%s' % self.lang))\n        if self.linenums:\n            classes.append('linenums')\n        class_str = ''\n        if classes:\n            class_str = (' class=\"%s\"' % ' '.join(classes))\n        return ('<pre class=\"%s\"><code%s>%s</code></pre>\\n' % (self.css_class, class_str, txt))\n", "label": 1}
{"function": "\n\ndef _format_int(format, subtype, endian):\n    'Return numeric ID for given format|subtype|endian combo.'\n    result = _check_format(format)\n    if (subtype is None):\n        subtype = default_subtype(format)\n        if (subtype is None):\n            raise TypeError('No default subtype for major format {0!r}'.format(format))\n    elif (not isinstance(subtype, (_unicode, str))):\n        raise TypeError('Invalid subtype: {0!r}'.format(subtype))\n    try:\n        result |= _subtypes[subtype.upper()]\n    except KeyError:\n        raise ValueError('Unknown subtype: {0!r}'.format(subtype))\n    if (endian is None):\n        endian = 'FILE'\n    elif (not isinstance(endian, (_unicode, str))):\n        raise TypeError('Invalid endian-ness: {0!r}'.format(endian))\n    try:\n        result |= _endians[endian.upper()]\n    except KeyError:\n        raise ValueError('Unknown endian-ness: {0!r}'.format(endian))\n    info = _ffi.new('SF_INFO*')\n    info.format = result\n    info.channels = 1\n    if (_snd.sf_format_check(info) == _snd.SF_FALSE):\n        raise ValueError('Invalid combination of format, subtype and endian')\n    return result\n", "label": 1}
{"function": "\n\ndef match_transform(self, e):\n    '\\n        Check to see if the event e follows one of the verb transformation patterns\\n        specified at the bottom of the Verb Dictionary file.\\n        \\n        If the transformation is present, adjust the event accordingly. \\n        If no transformation is present, check if the event is of the form:\\n        \\n                    a ( b . Q ) P , where Q is not a top-level verb. \\n           \\n            and then convert this to ( a b P+Q )\\n        \\n        Otherwise, return the event as-is.\\n        \\n        Parameters\\n        -----------\\n        e: tuple\\n           Event to be transformed\\n        \\n        Returns\\n        -------\\n        t: list of tuples\\n           List of modified events, since multiple events can come from one single event\\n        '\n\n    def recurse(pdict, event, a2v={\n        \n    }, v2a={\n        \n    }):\n        path = pdict\n        if isinstance(pdict, list):\n            line = pdict[1]\n            path = pdict[0]\n            verb = (utilities.convert_code(path[2])[0] if (not (path[2] == 'Q')) else v2a['Q'])\n            if isinstance(v2a[path[1]], tuple):\n                results = []\n                for item in v2a[path[1]]:\n                    results.append((list(v2a[path[0]]), item, verb))\n                return (results, line)\n            return ([(list(v2a[path[0]]), v2a[path[1]], verb)], line)\n        if isinstance(event, tuple):\n            actor = (None if (not event[0]) else tuple(event[0]))\n            masks = filter((lambda a: (a in pdict)), [event[2], (event[2] - (event[2] % 16)), (event[2] - (event[2] % 256)), (event[2] - (event[2] % 4096))])\n            if masks:\n                path = pdict[masks[0]]\n            elif ((- 1) in pdict):\n                v2a['Q'] = event[2]\n                path = pdict[(- 1)]\n            else:\n                return False\n        else:\n            actor = event\n        if (actor in a2v):\n            actor = a2v[actor]\n        if (not actor):\n            actor = '_'\n        if (actor in path):\n            return recurse(path[actor], event[1], a2v, v2a)\n        elif (not (actor == '_')):\n            for var in sorted(path.keys())[::(- 1)]:\n                if (var in v2a):\n                    continue\n                if (not (var == '.')):\n                    v2a[var] = actor\n                    a2v[actor] = var\n                return recurse(path[var], event[1], a2v, v2a)\n        return False\n    try:\n        t = recurse(PETRglobals.VerbDict['transformations'], e)\n        if t:\n            return t\n        elif (e[0] and e[2] and isinstance(e[1], tuple) and e[1][0] and (not (e[1][2] / (16 ** 3)))):\n            if isinstance(e[1][0], list):\n                results = []\n                for item in e[1][0]:\n                    event = (e[0], item, utilities.combine_code(e[1][2], e[2]))\n                    results.append(event)\n                return results\n            event = (e[0], e[1][0], utilities.combine_code(e[2], e[1][2]))\n            return [event]\n    except Exception as ex:\n        pass\n    return [e]\n", "label": 1}
{"function": "\n\ndef _parse_using_etree(self, xml_input):\n    from xml.etree.cElementTree import iterparse\n    parser = iterparse(self._to_stream(xml_input), events=('start', 'end'))\n    self.startDocument()\n    try:\n        for (action, element) in parser:\n            name = element.tag\n            if (action == 'start'):\n                if (name in XmlPropertyListParser.START_CALLBACKS):\n                    XmlPropertyListParser.START_CALLBACKS[name](self, element.tag, element.attrib)\n            elif (action == 'end'):\n                if (name in XmlPropertyListParser.END_CALLBACKS):\n                    XmlPropertyListParser.END_CALLBACKS[name](self, name)\n                if (name in XmlPropertyListParser.PARSE_CALLBACKS):\n                    XmlPropertyListParser.PARSE_CALLBACKS[name](self, name, (element.text or ''))\n                element.clear()\n    except SyntaxError as e:\n        raise PropertyListParseError(e)\n    self.endDocument()\n    return self.__plist\n", "label": 1}
{"function": "\n\ndef is_binary_file(file):\n    try:\n        f = open(file)\n        buf = f.read()\n        f.close()\n    except OSError:\n        return False\n    try:\n        ubuf = unicode(buf, 'utf-8')\n        return False\n    except UnicodeDecodeError:\n        pass\n    treshold = (len(buf) / 5)\n    binary_chars = 0\n    for c in buf:\n        oc = ord(c)\n        if ((oc > 127) or ((oc < 31) and (oc != '\\r') and (oc != '\\n'))):\n            binary_chars += 1\n            if (binary_chars > treshold):\n                return True\n    return False\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    script = options.get('script')\n    if (not script):\n        if len(args):\n            script = args[0]\n        else:\n            raise CommandError('You must provide a script path or directory containing scripts.')\n    if (not os.path.exists(script)):\n        raise CommandError('{0} does not exist.'.format(script))\n    group = options.get('group', 'Wooey Scripts')\n    scripts = ([os.path.join(script, i) for i in os.listdir(script)] if os.path.isdir(script) else [script])\n    converted = 0\n    for script in scripts:\n        if (script.endswith('.pyc') or ('__init__' in script)):\n            continue\n        if script.endswith('.py'):\n            sys.stdout.write('Converting {}\\n'.format(script))\n            with open(script, 'r') as f:\n                script = default_storage.save(os.path.join(wooey_settings.WOOEY_SCRIPT_DIR, os.path.split(script)[1]), File(f))\n                if wooey_settings.WOOEY_EPHEMERAL_FILES:\n                    local_storage = get_storage(local=True)\n                    local_storage.save(os.path.join(wooey_settings.WOOEY_SCRIPT_DIR, os.path.split(script)[1]), File(f))\n            res = add_wooey_script(script_path=script, group=group)\n            if res['valid']:\n                converted += 1\n    sys.stdout.write('Converted {} scripts\\n'.format(converted))\n", "label": 1}
{"function": "\n\ndef _handle_client_requests_and_responses(self, remote_client):\n    assert self.nitro_socket\n    queues = [remote_client.incoming]\n    try:\n        while True:\n            (evt, value) = diesel.first(waits=queues, sleep=self.timeout)\n            if (evt is remote_client.incoming):\n                assert isinstance(value, Message)\n                remote_client.async_frame = value.orig_frame\n                resp = self.handle_client_packet(value.data, remote_client.context)\n                if resp:\n                    if isinstance(resp, basestring):\n                        output = [resp]\n                    else:\n                        output = iter(resp)\n                    for part in output:\n                        msg = Message(value.orig_frame, remote_client.identity, self.serialize_message(remote_client.identity, part))\n                        self.outgoing.put(msg)\n            elif (evt == 'sleep'):\n                break\n    finally:\n        self._cleanup_client(remote_client)\n", "label": 1}
{"function": "\n\ndef list(self, href):\n    \"\\n        list of files and directories at remote server\\n        :param href: remote folder\\n        :return: list(folders, files) and list(None,None) if folder doesn't exist\\n        \"\n    for iTry in range(TRYINGS):\n        logger.info((u('list(%s): %s') % (iTry, href)))\n        folders = None\n        files = None\n        try:\n            href = os.path.join(u('/'), _(href))\n            conn = self.getConnection()\n            conn.request('PROPFIND', _encode_utf8(href), u(''), self.getHeaders())\n            response = conn.getresponse()\n            checkResponse(response)\n            data = response.read()\n            if (data == b('list: folder was not found')):\n                return (folders, files)\n            elif (data == b('You are not authorized to see this!')):\n                return (folders, files)\n            else:\n                try:\n                    dom = xml.dom.minidom.parseString(data)\n                    responces = dom.getElementsByTagNameNS('DAV:', 'response')\n                    folders = {\n                        \n                    }\n                    files = {\n                        \n                    }\n                    for dom in responces:\n                        response = RemoteObject(dom, self, href)\n                        if (response.href != href):\n                            if response.isFolder():\n                                folders[response.href] = response\n                            else:\n                                files[response.href] = response\n                except xml.parsers.expat.ExpatError:\n                    e = sys.exc_info()[1]\n                    logger.exception(e)\n            return (folders, files)\n        except ConnectionException:\n            raise\n        except Exception:\n            e = sys.exc_info()[1]\n            logger.exception(e)\n        return (folders, files)\n", "label": 1}
{"function": "\n\ndef paintEvent(self, event):\n    super(FoldingPanel, self).paintEvent(event)\n    painter = QtGui.QPainter(self)\n    if (self._mouse_over_line is not None):\n        block = self.editor.document().findBlockByNumber(self._mouse_over_line)\n        try:\n            self._draw_fold_region_background(block, painter)\n        except ValueError:\n            pass\n    for (top_position, line_number, block) in self.editor.visible_blocks:\n        if TextBlockHelper.is_fold_trigger(block):\n            collapsed = TextBlockHelper.is_collapsed(block)\n            mouse_over = (self._mouse_over_line == line_number)\n            self._draw_fold_indicator(top_position, mouse_over, collapsed, painter)\n            if collapsed:\n                for deco in self._block_decos:\n                    if (deco.block == block):\n                        break\n                else:\n                    self._add_fold_decoration(block, FoldScope(block))\n            else:\n                for deco in self._block_decos:\n                    if (deco.block == block):\n                        self._block_decos.remove(deco)\n                        self.editor.decorations.remove(deco)\n                        del deco\n                        break\n", "label": 1}
{"function": "\n\ndef _PromptUserForEncryptedVolumeCredential(self, scan_context, locked_scan_node, output_writer):\n    'Prompts the user to provide a credential for an encrypted volume.\\n\\n    Args:\\n      scan_context: the source scanner context (instance of\\n                    SourceScannerContext).\\n      locked_scan_node: the locked scan node (instance of SourceScanNode).\\n      output_writer: the output writer (instance of StdoutWriter).\\n    '\n    credentials = credentials_manager.CredentialsManager.GetCredentials(locked_scan_node.path_spec)\n    if (locked_scan_node.type_indicator == definitions.TYPE_INDICATOR_BDE):\n        output_writer.WriteLine('Found a BitLocker encrypted volume.')\n    else:\n        output_writer.WriteLine('Found an encrypted volume.')\n    credentials_list = list(credentials.CREDENTIALS)\n    credentials_list.append('skip')\n    output_writer.WriteLine('Supported credentials:')\n    output_writer.WriteLine('')\n    for (index, name) in enumerate(credentials_list):\n        output_writer.WriteLine('  {0:d}. {1:s}'.format(index, name))\n    output_writer.WriteLine('')\n    result = False\n    while (not result):\n        output_writer.WriteString('Select a credential to unlock the volume: ')\n        input_line = sys.stdin.readline()\n        input_line = input_line.strip()\n        if (input_line in credentials_list):\n            credential_identifier = input_line\n        else:\n            try:\n                credential_identifier = int(input_line, 10)\n                credential_identifier = credentials_list[credential_identifier]\n            except (IndexError, ValueError):\n                output_writer.WriteLine('Unsupported credential: {0:s}'.format(input_line))\n                continue\n        if (credential_identifier == 'skip'):\n            break\n        getpass_string = 'Enter credential data: '\n        if (sys.platform.startswith('win') and (sys.version_info[0] < 3)):\n            getpass_string = self._EncodeString(getpass_string)\n        credential_data = getpass.getpass(getpass_string)\n        output_writer.WriteLine('')\n        result = self._source_scanner.Unlock(scan_context, locked_scan_node.path_spec, credential_identifier, credential_data)\n        if (not result):\n            output_writer.WriteLine('Unable to unlock volume.')\n            output_writer.WriteLine('')\n", "label": 1}
{"function": "\n\ndef format_email_address(self, email_type, emails=None):\n    '\\n        returns email headers with email information.\\n        :param email_type: One of: from|to|cc|bcc\\n        :param emails: A list of email address or list/tuple of (name, email) pairs.\\n        :return: the email header\\n        '\n    emails = (emails or self.recipients or [])\n    header = self.get_header(email_type)\n    for (i, address) in enumerate(emails):\n        if (i > 0):\n            header.append(',', 'us-ascii')\n        if isinstance(address, basestring):\n            try:\n                header.append(address, charset='us-ascii')\n            except UnicodeError:\n                header.append(address, charset=self.charset)\n        elif (type(address) in (tuple, list)):\n            (_name, _address) = address\n            try:\n                _name.encode('us-ascii')\n                formatted_address = formataddr(address)\n                header.append(formatted_address, charset='us-ascii')\n            except UnicodeError:\n                header.append(_name)\n                header.append('<{0}>'.format(_address), charset='us-ascii')\n    return header\n", "label": 1}
{"function": "\n\ndef check_host(netloc, username=None, password=None, namespace=None):\n    '\\n    Checks if we can connect to a host with a known key.\\n\\n    This will raise an exception if we cannot connect to the host. The\\n    exception will be one of BadHostKeyError, UnknownHostKeyError, or\\n    SCMError.\\n    '\n    from django.conf import settings\n    client = SSHClient(namespace=namespace)\n    client.set_missing_host_key_policy(RaiseUnknownHostKeyPolicy())\n    kwargs = {\n        \n    }\n    if (':' in netloc):\n        (hostname, port) = netloc.split(':')\n        try:\n            port = int(port)\n        except ValueError:\n            raise SSHInvalidPortError(port)\n    else:\n        hostname = netloc\n        port = SSH_PORT\n    if getattr(settings, 'RUNNING_TEST', False):\n        client.set_missing_host_key_policy(paramiko.WarningPolicy())\n        kwargs['allow_agent'] = False\n    try:\n        client.connect(hostname, port, username=username, password=password, pkey=client.get_user_key(), **kwargs)\n    except paramiko.BadHostKeyException as e:\n        raise BadHostKeyError(e.hostname, e.key, e.expected_key)\n    except paramiko.AuthenticationException as e:\n        allowed_types = getattr(e, 'allowed_types', [])\n        if ('publickey' in allowed_types):\n            key = client.get_user_key()\n        else:\n            key = None\n        raise SSHAuthenticationError(allowed_types=allowed_types, user_key=key)\n    except paramiko.SSHException as e:\n        msg = six.text_type(e)\n        if (msg == 'No authentication methods available'):\n            raise SSHAuthenticationError\n        else:\n            raise SSHError(msg)\n", "label": 1}
{"function": "\n\ndef test_summary():\n    t = Symbol('t', 'var * {id: int32, name: string, amount: int32}')\n    s = summary(total=t.amount.sum(), num=t.id.count())\n    assert (s.dshape == dshape('{num: int32, total: int64}'))\n    assert hash(s)\n    assert eval(str(s)).isidentical(s)\n    assert ('summary(' in str(s))\n    assert ('total=' in str(s))\n    assert ('num=' in str(s))\n    assert (str(t.amount.sum()) in str(s))\n    assert (not summary(total=t.amount.sum())._child.isidentical(t.amount.sum()))\n    assert iscollection(summary(total=(t.amount.sum() + 1))._child.dshape)\n", "label": 1}
{"function": "\n\ndef __init__(self, prog=None, usage=None, description=None, epilog=None, version=None, parents=[], formatter_class=HelpFormatter, prefix_chars='-', fromfile_prefix_chars=None, argument_default=None, conflict_handler='error', add_help=True):\n    if (version is not None):\n        import warnings\n        warnings.warn('The \"version\" argument to ArgumentParser is deprecated. Please use \"add_argument(..., action=\\'version\\', version=\"N\", ...)\" instead', DeprecationWarning)\n    superinit = super(ArgumentParser, self).__init__\n    superinit(description=description, prefix_chars=prefix_chars, argument_default=argument_default, conflict_handler=conflict_handler)\n    if (prog is None):\n        prog = _os.path.basename(_sys.argv[0])\n    self.prog = prog\n    self.usage = usage\n    self.epilog = epilog\n    self.version = version\n    self.formatter_class = formatter_class\n    self.fromfile_prefix_chars = fromfile_prefix_chars\n    self.add_help = add_help\n    add_group = self.add_argument_group\n    self._positionals = add_group(_('positional arguments'))\n    self._optionals = add_group(_('optional arguments'))\n    self._subparsers = None\n\n    def identity(string):\n        return string\n    self.register('type', None, identity)\n    if ('-' in prefix_chars):\n        default_prefix = '-'\n    else:\n        default_prefix = prefix_chars[0]\n    if self.add_help:\n        self.add_argument((default_prefix + 'h'), ((default_prefix * 2) + 'help'), action='help', default=SUPPRESS, help=_('show this help message and exit'))\n    if self.version:\n        self.add_argument((default_prefix + 'v'), ((default_prefix * 2) + 'version'), action='version', default=SUPPRESS, version=self.version, help=_(\"show program's version number and exit\"))\n    for parent in parents:\n        self._add_container_actions(parent)\n        try:\n            defaults = parent._defaults\n        except AttributeError:\n            pass\n        else:\n            self._defaults.update(defaults)\n", "label": 1}
{"function": "\n\ndef _collect_all_modules(self):\n    'Collect modules, so we can handle imports later'\n    for pkgroot in self.path:\n        pkgroot = os.path.abspath(pkgroot)\n        has_followlinks = (sys.version_info >= (2, 6))\n        if has_followlinks:\n            allfiles = os.walk(pkgroot, followlinks=True)\n        else:\n            allfiles = os.walk(pkgroot)\n        for (root, dirs, files) in allfiles:\n            if ('__init__.py' in files):\n                files.remove('__init__.py')\n                if (root != pkgroot):\n                    files.insert(0, '__init__.py')\n            elif (root != pkgroot):\n                dirs[:] = []\n                continue\n            for filename in files:\n                if (not filename.endswith('.py')):\n                    continue\n                path = os.path.join(root, filename)\n                if (not has_followlinks):\n                    path = os.path.abspath(path)\n                module_path = path[(len(pkgroot) + len(os.sep)):]\n                if (os.path.basename(module_path) == '__init__.py'):\n                    module_name = os.path.dirname(module_path)\n                else:\n                    module_name = module_path[:(- 3)]\n                assert ('.' not in module_name), ('Invalid module file name: %s' % module_path)\n                module_name = module_name.replace(os.sep, '.')\n                self._collected.setdefault(module_name, path)\n", "label": 1}
{"function": "\n\ndef enable(**kwargs):\n    \"\\n    Enable all beacons on the minion\\n\\n    :return:                Boolean and status message on success or failure of enable.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' beacons.enable\\n    \"\n    ret = {\n        'comment': [],\n        'result': True,\n    }\n    if (('test' in kwargs) and kwargs['test']):\n        ret['comment'] = 'Beacons would be enabled.'\n    else:\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({\n                'func': 'enable',\n            }, 'manage_beacons')\n            if res:\n                event_ret = eventer.get_event(tag='/salt/minion/minion_beacons_enabled_complete', wait=30)\n                if (event_ret and event_ret['complete']):\n                    beacons = event_ret['beacons']\n                    if (('enabled' in beacons) and beacons['enabled']):\n                        ret['result'] = True\n                        ret['comment'] = 'Enabled beacons on minion.'\n                    else:\n                        ret['result'] = False\n                        ret['comment'] = 'Failed to enable beacons on minion.'\n                    return ret\n        except KeyError:\n            ret['comment'] = 'Event module not available. Beacons enable job failed.'\n    return ret\n", "label": 1}
{"function": "\n\ndef parse_if_needed(self, force=False):\n    if ((not self.filename) or (not self.exists)):\n        return False\n    changed = False\n    modtime = os.path.getmtime(self.filename)\n    if (force or (modtime != self._lastmtime)):\n        self.parser = UnicodeConfigParser()\n        try:\n            if (not self.parser.read(self.filename)):\n                raise TracError(_(\"Error reading '%(file)s', make sure it is readable.\", file=self.filename))\n        except ParsingError as e:\n            raise TracError(e)\n        self._lastmtime = modtime\n        self._pristine_parser = copy.deepcopy(self.parser)\n        changed = True\n    if changed:\n        self.parents = self._get_parents()\n    else:\n        for parent in self.parents:\n            changed |= parent.parse_if_needed(force=force)\n    if changed:\n        self._sections = {\n            \n        }\n    return changed\n", "label": 1}
{"function": "\n\ndef test_schema():\n    client = mock.MagicMock()\n    job = models.Job(client, 'job_id', 'type', 'query', status='status', url='url', debug='debug', start_at='start_at', end_at='end_at', cpu_time='cpu_time', result_size='result_size', result='result', result_url='result_url', hive_result_schema=[['_c1', 'string'], ['_c2', 'bigint']], priority='UNKNOWN', retry_limit='retry_limit', org_name='org_name', database='database')\n    assert (job.id == 'job_id')\n    assert (job.job_id == 'job_id')\n    assert (job.type == 'type')\n    assert (job.result_url == 'result_url')\n    assert (job.priority == 'UNKNOWN')\n    assert (job.retry_limit == 'retry_limit')\n    assert (job.org_name == 'org_name')\n    assert (job.database == 'database')\n    assert (job.result_schema == [['_c1', 'string'], ['_c2', 'bigint']])\n", "label": 1}
{"function": "\n\ndef test_make_moralized_copy(self, huang_darwiche_dag):\n    gu = make_undirected_copy(huang_darwiche_dag)\n    gm = make_moralized_copy(gu, huang_darwiche_dag)\n    nodes = dict([(node.name, node) for node in gm.nodes])\n    assert (set(nodes['f_a'].neighbours) == set([nodes['f_b'], nodes['f_c']]))\n    assert (set(nodes['f_b'].neighbours) == set([nodes['f_a'], nodes['f_d']]))\n    assert (set(nodes['f_c'].neighbours) == set([nodes['f_a'], nodes['f_e'], nodes['f_g']]))\n    assert (set(nodes['f_d'].neighbours) == set([nodes['f_b'], nodes['f_f'], nodes['f_e']]))\n    assert (set(nodes['f_e'].neighbours) == set([nodes['f_c'], nodes['f_f'], nodes['f_h'], nodes['f_d'], nodes['f_g']]))\n    assert (set(nodes['f_f'].neighbours) == set([nodes['f_d'], nodes['f_e']]))\n    assert (set(nodes['f_g'].neighbours) == set([nodes['f_c'], nodes['f_h'], nodes['f_e']]))\n    assert (set(nodes['f_h'].neighbours) == set([nodes['f_e'], nodes['f_g']]))\n", "label": 1}
{"function": "\n\n@classmethod\ndef parser(cls, buf):\n    assert (cls._ALL_PACK_LEN == len(buf))\n    offset = 0\n    (subtype, version) = struct.unpack_from(cls._HLEN_PACK_STR, buf, offset)\n    assert (SLOW_SUBTYPE_LACP == subtype)\n    assert (cls.LACP_VERSION_NUMBER == version)\n    offset += cls._HLEN_PACK_LEN\n    (actor_tag, actor_length, actor_system_priority, actor_system, actor_key, actor_port_priority, actor_port, actor_state) = struct.unpack_from(cls._ACTPRT_INFO_PACK_STR, buf, offset)\n    assert (cls.LACP_TLV_TYPE_ACTOR == actor_tag)\n    assert (cls._ACTPRT_INFO_PACK_LEN == actor_length)\n    offset += cls._ACTPRT_INFO_PACK_LEN\n    actor_state_activity = ((actor_state >> 0) & 1)\n    actor_state_timeout = ((actor_state >> 1) & 1)\n    actor_state_aggregation = ((actor_state >> 2) & 1)\n    actor_state_synchronization = ((actor_state >> 3) & 1)\n    actor_state_collecting = ((actor_state >> 4) & 1)\n    actor_state_distributing = ((actor_state >> 5) & 1)\n    actor_state_defaulted = ((actor_state >> 6) & 1)\n    actor_state_expired = ((actor_state >> 7) & 1)\n    (partner_tag, partner_length, partner_system_priority, partner_system, partner_key, partner_port_priority, partner_port, partner_state) = struct.unpack_from(cls._ACTPRT_INFO_PACK_STR, buf, offset)\n    assert (cls.LACP_TLV_TYPE_PARTNER == partner_tag)\n    assert (cls._ACTPRT_INFO_PACK_LEN == partner_length)\n    offset += cls._ACTPRT_INFO_PACK_LEN\n    partner_state_activity = ((partner_state >> 0) & 1)\n    partner_state_timeout = ((partner_state >> 1) & 1)\n    partner_state_aggregation = ((partner_state >> 2) & 1)\n    partner_state_synchronization = ((partner_state >> 3) & 1)\n    partner_state_collecting = ((partner_state >> 4) & 1)\n    partner_state_distributing = ((partner_state >> 5) & 1)\n    partner_state_defaulted = ((partner_state >> 6) & 1)\n    partner_state_expired = ((partner_state >> 7) & 1)\n    (collector_tag, collector_length, collector_max_delay) = struct.unpack_from(cls._COL_INFO_PACK_STR, buf, offset)\n    assert (cls.LACP_TLV_TYPE_COLLECTOR == collector_tag)\n    assert (cls._COL_INFO_PACK_LEN == collector_length)\n    offset += cls._COL_INFO_PACK_LEN\n    (terminator_tag, terminator_length) = struct.unpack_from(cls._TRM_PACK_STR, buf, offset)\n    assert (cls.LACP_TLV_TYPE_TERMINATOR == terminator_tag)\n    assert (0 == terminator_length)\n    return (cls(version, actor_system_priority, addrconv.mac.bin_to_text(actor_system), actor_key, actor_port_priority, actor_port, actor_state_activity, actor_state_timeout, actor_state_aggregation, actor_state_synchronization, actor_state_collecting, actor_state_distributing, actor_state_defaulted, actor_state_expired, partner_system_priority, addrconv.mac.bin_to_text(partner_system), partner_key, partner_port_priority, partner_port, partner_state_activity, partner_state_timeout, partner_state_aggregation, partner_state_synchronization, partner_state_collecting, partner_state_distributing, partner_state_defaulted, partner_state_expired, collector_max_delay), None, buf[lacp._ALL_PACK_LEN:])\n", "label": 1}
{"function": "\n\ndef iterantijoin(left, right, lkey, rkey):\n    lit = iter(left)\n    rit = iter(right)\n    lhdr = next(lit)\n    rhdr = next(rit)\n    (yield tuple(lhdr))\n    lkind = asindices(lhdr, lkey)\n    rkind = asindices(rhdr, rkey)\n    lgetk = comparable_itemgetter(*lkind)\n    rgetk = comparable_itemgetter(*rkind)\n    lgit = itertools.groupby(lit, key=lgetk)\n    rgit = itertools.groupby(rit, key=rgetk)\n    lrowgrp = []\n    (lkval, rkval) = (Comparable(None), Comparable(None))\n    try:\n        (lkval, lrowgrp) = next(lgit)\n        (rkval, _) = next(rgit)\n        while True:\n            if (lkval < rkval):\n                for row in lrowgrp:\n                    (yield tuple(row))\n                (lkval, lrowgrp) = next(lgit)\n            elif (lkval > rkval):\n                (rkval, _) = next(rgit)\n            else:\n                (lkval, lrowgrp) = next(lgit)\n                (rkval, _) = next(rgit)\n    except StopIteration:\n        pass\n    if (lkval > rkval):\n        for row in lrowgrp:\n            (yield tuple(row))\n    for (lkval, lrowgrp) in lgit:\n        for row in lrowgrp:\n            (yield tuple(row))\n", "label": 1}
{"function": "\n\ndef get_job(self, queues, timeout=None, count=None, nohang=False, withcounters=False):\n    '\\n        Return some number of jobs from specified queues.\\n\\n        GETJOB [NOHANG] [TIMEOUT <ms-timeout>] [COUNT <count>] [WITHCOUNTERS] FROM\\n            queue1 queue2 ... queueN\\n\\n        :param queues: name of queues\\n\\n        :returns: list of tuple(job_id, queue_name, job), tuple(job_id, queue_name, job, nacks, additional_deliveries) or empty list\\n        :rtype: list\\n        '\n    assert queues\n    command = ['GETJOB']\n    if nohang:\n        command += ['NOHANG']\n    if timeout:\n        command += ['TIMEOUT', timeout]\n    if count:\n        command += ['COUNT', count]\n    if withcounters:\n        command += ['WITHCOUNTERS']\n    command += (['FROM'] + queues)\n    results = self.execute_command(*command)\n    if (not results):\n        return []\n    if withcounters:\n        return [(job_id, queue_name, job, nacks, additional_deliveries) for (job_id, queue_name, job, _, nacks, _, additional_deliveries) in results]\n    else:\n        return [(job_id, queue_name, job) for (job_id, queue_name, job) in results]\n", "label": 1}
{"function": "\n\ndef clean(self):\n    super(BaseTranslationFormSet, self).clean()\n    master = self.instance\n    stashed = get_cached_translation(master)\n    for form in self.forms:\n        set_cached_translation(master, form.instance)\n        exclusions = form._get_validation_exclusions()\n        exclusions.extend((f.name for f in master._meta.fields))\n        try:\n            master.clean()\n        except ValidationError as e:\n            form._update_errors(e)\n    set_cached_translation(master, stashed)\n    forms_to_delete = self.deleted_forms\n    provided = [form for form in self.forms if (((getattr(form.instance, 'pk', None) is not None) or form.has_changed()) and (not (form in forms_to_delete)))]\n    if (len(provided) < 1):\n        raise ValidationError(_('At least one translation must be provided'), code='notranslation')\n", "label": 1}
{"function": "\n\ndef append(self, s, charset=None, errors='strict'):\n    \"Append a string to the MIME header.\\n\\n        Optional charset, if given, should be a Charset instance or the name\\n        of a character set (which will be converted to a Charset instance).  A\\n        value of None (the default) means that the charset given in the\\n        constructor is used.\\n\\n        s may be a byte string or a Unicode string.  If it is a byte string\\n        (i.e. isinstance(s, str) is false), then charset is the encoding of\\n        that byte string, and a UnicodeError will be raised if the string\\n        cannot be decoded with that charset.  If s is a Unicode string, then\\n        charset is a hint specifying the character set of the characters in\\n        the string.  In either case, when producing an RFC 2822 compliant\\n        header using RFC 2047 rules, the string will be encoded using the\\n        output codec of the charset.  If the string cannot be encoded to the\\n        output codec, a UnicodeError will be raised.\\n\\n        Optional `errors' is passed as the errors argument to the decode\\n        call if s is a byte string.\\n        \"\n    if (charset is None):\n        charset = self._charset\n    elif (not isinstance(charset, Charset)):\n        charset = Charset(charset)\n    if (not isinstance(s, str)):\n        input_charset = (charset.input_codec or 'us-ascii')\n        if (input_charset == _charset.UNKNOWN8BIT):\n            s = s.decode('us-ascii', 'surrogateescape')\n        else:\n            s = s.decode(input_charset, errors)\n    output_charset = (charset.output_codec or 'us-ascii')\n    if (output_charset != _charset.UNKNOWN8BIT):\n        try:\n            s.encode(output_charset, errors)\n        except UnicodeEncodeError:\n            if (output_charset != 'us-ascii'):\n                raise\n            charset = UTF8\n    self._chunks.append((s, charset))\n", "label": 1}
{"function": "\n\ndef user_update(request, user, **data):\n    manager = keystoneclient(request, admin=True).users\n    error = None\n    if (not keystone_can_edit_user()):\n        raise keystone_exceptions.ClientException(405, _('Identity service does not allow editing user data.'))\n    if (VERSIONS.active < 3):\n        try:\n            user = manager.update(user, **data)\n        except keystone_exceptions.Conflict:\n            raise exceptions.Conflict()\n        except Exception:\n            error = exceptions.handle(request, ignore=True)\n        if ('project' in data):\n            project = data.pop('project')\n            try:\n                user_update_tenant(request, user, project)\n                user.tenantId = project\n            except Exception:\n                error = exceptions.handle(request, ignore=True)\n            user_roles = roles_for_user(request, user, project)\n            if (not user_roles):\n                messages.warning(request, (_('User %s has no role defined for that project.') % data.get('name', None)))\n        if (error is not None):\n            raise error\n    else:\n        try:\n            user = manager.update(user, **data)\n        except keystone_exceptions.Conflict:\n            raise exceptions.Conflict()\n", "label": 1}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01, gamma=10, cof=4000):\n    ''\n    samplerate = self.audio.samplerate\n    level = 6\n    vpre = np.zeros((1, N))\n    ncof = min(0.99, ((float(cof) * 2) / samplerate))\n    (b, a) = sig.butter(6, ncof)\n    zi = np.zeros((N, level))\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for y in super(Y2, self).walk(N, freq_base, freq_max, start, end, each=False, combine=False):\n        yg = self.g(np.diff(np.append(vpre, y, 0), 1, 0), gamma)\n        (ygt, zi) = sig.lfilter(b, a, yg.T, zi=zi)\n        if (not combine):\n            if each:\n                for v in ygt.T:\n                    (yield v)\n            else:\n                (yield ygt.T)\n        else:\n            Y = np.append(Y, ygt.T, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\ndef _show_placeholder_for_page(context, placeholder_name, page_lookup, lang=None, site=None, cache_result=True):\n    '\\n    Shows the content of a page with a placeholder name and given lookup\\n    arguments in the given language.\\n    This is useful if you want to have some more or less static content that is\\n    shared among many pages, such as a footer.\\n\\n    See _get_page_by_untyped_arg() for detailed information on the allowed types\\n    and their interpretation for the page_lookup argument.\\n    '\n    validate_placeholder_name(placeholder_name)\n    request = context.get('request', False)\n    site_id = get_site_id(site)\n    if (not request):\n        return {\n            'content': '',\n        }\n    if (lang is None):\n        lang = get_language_from_request(request)\n    if cache_result:\n        cached_value = get_placeholder_page_cache(page_lookup, lang, site_id, placeholder_name)\n        if cached_value:\n            restore_sekizai_context(context, cached_value['sekizai'])\n            return {\n                'content': mark_safe(cached_value['content']),\n            }\n    page = _get_page_by_untyped_arg(page_lookup, request, site_id)\n    if (not page):\n        return {\n            'content': '',\n        }\n    try:\n        placeholder = page.placeholders.get(slot=placeholder_name)\n    except PlaceholderModel.DoesNotExist:\n        if settings.DEBUG:\n            raise\n        return {\n            'content': '',\n        }\n    watcher = Watcher(context)\n    content = render_placeholder(placeholder, context, placeholder_name, lang=lang, use_cache=cache_result)\n    changes = watcher.get_changes()\n    if cache_result:\n        set_placeholder_page_cache(page_lookup, lang, site_id, placeholder_name, {\n            'content': content,\n            'sekizai': changes,\n        })\n    if content:\n        return {\n            'content': mark_safe(content),\n        }\n    return {\n        'content': '',\n    }\n", "label": 1}
{"function": "\n\ndef test_locked(self):\n    user = self.create_user()\n    repo = self.create_repo()\n    app = self.create_app(repository=repo)\n    self.create_taskconfig(app=app)\n    task = self.create_task(app=app, user=user, status=TaskStatus.pending)\n    deploy = self.create_deploy(app=app, task=task)\n    result = serialize(deploy)\n    assert (result['id'] == str(deploy.id))\n    assert (result['status'] == 'pending')\n    assert (result['ref'] == task.ref)\n    assert (result['sha'] == task.sha)\n    assert (result['environment'] == deploy.environment)\n    assert (result['number'] == deploy.number)\n    assert (result['app']['id'] == str(app.id))\n    assert (result['app']['name'] == app.name)\n", "label": 1}
{"function": "\n\n@register.as_tag\ndef keywords_for(*args):\n    '\\n    Return a list of ``Keyword`` objects for the given model instance\\n    or a model class. In the case of a model class, retrieve all\\n    keywords for all instances of the model and apply a ``weight``\\n    attribute that can be used to create a tag cloud.\\n    '\n    if isinstance(args[0], Model):\n        obj = args[0]\n        if hasattr(obj, 'get_content_model'):\n            obj = (obj.get_content_model() or obj)\n        keywords_name = obj.get_keywordsfield_name()\n        keywords_queryset = getattr(obj, keywords_name).all()\n        prefetched = getattr(obj, '_prefetched_objects_cache', {\n            \n        })\n        if (keywords_name not in prefetched):\n            keywords_queryset = keywords_queryset.select_related('keyword')\n        return [assigned.keyword for assigned in keywords_queryset]\n    try:\n        (app_label, model) = args[0].split('.', 1)\n    except ValueError:\n        return []\n    content_type = ContentType.objects.get(app_label=app_label, model=model)\n    assigned = AssignedKeyword.objects.filter(content_type=content_type)\n    keywords = Keyword.objects.filter(assignments__in=assigned)\n    keywords = keywords.annotate(item_count=Count('assignments'))\n    if (not keywords):\n        return []\n    counts = [keyword.item_count for keyword in keywords]\n    (min_count, max_count) = (min(counts), max(counts))\n    factor = (settings.TAG_CLOUD_SIZES - 1.0)\n    if (min_count != max_count):\n        factor /= (max_count - min_count)\n    for kywd in keywords:\n        kywd.weight = (int(round(((kywd.item_count - min_count) * factor))) + 1)\n    return keywords\n", "label": 1}
{"function": "\n\n@app.task(bind=True, default_retry_delay=RETRY_DELAY, max_retries=None, ignore_result=True)\ndef task_index_document(self, document_id):\n    try:\n        rebuild_lock = Lock.acquire_lock('document_indexing_task_do_rebuild_all_indexes')\n    except LockError as exception:\n        raise self.retry(exc=exception)\n    else:\n        try:\n            lock = Lock.acquire_lock(('document_indexing_task_update_index_document_%d' % document_id))\n        except LockError as exception:\n            raise self.retry(exc=exception)\n        else:\n            try:\n                document = Document.objects.get(pk=document_id)\n            except Document.DoesNotExist:\n                pass\n            else:\n                try:\n                    IndexInstanceNode.objects.index_document(document)\n                except OperationalError as exception:\n                    logger.warning('Operational error while trying to index document: %s; %s', document, exception)\n                    lock.release()\n                    raise self.retry(exc=exception)\n                else:\n                    lock.release()\n            finally:\n                lock.release()\n        finally:\n            rebuild_lock.release()\n", "label": 1}
{"function": "\n\ndef __init__(self, start, saturate, decay_factor):\n    if isinstance(decay_factor, str):\n        decay_factor = float(decay_factor)\n    if isinstance(start, str):\n        start = float(start)\n    if isinstance(saturate, str):\n        saturate = float(saturate)\n    assert isinstance(decay_factor, float)\n    assert isinstance(start, (py_integer_types, py_float_types))\n    assert isinstance(saturate, (py_integer_types, py_float_types))\n    assert (saturate > start)\n    assert (start > 0)\n    self.__dict__.update(locals())\n    del self.self\n    self._count = 0\n", "label": 1}
{"function": "\n\ndef _on_frame_start(self, data):\n    (header, payloadlen) = struct.unpack('BB', data)\n    self._final_frame = (header & 128)\n    reserved_bits = (header & 112)\n    self._frame_opcode = (header & 15)\n    self._frame_opcode_is_control = (self._frame_opcode & 8)\n    if reserved_bits:\n        self._abort()\n        return\n    self._masked_frame = bool((payloadlen & 128))\n    payloadlen = (payloadlen & 127)\n    if (self._frame_opcode_is_control and (payloadlen >= 126)):\n        self._abort()\n        return\n    try:\n        if (payloadlen < 126):\n            self._frame_length = payloadlen\n            if self._masked_frame:\n                self.stream.read_bytes(4, self._on_masking_key)\n            else:\n                self.stream.read_bytes(self._frame_length, self._on_frame_data)\n        elif (payloadlen == 126):\n            self.stream.read_bytes(2, self._on_frame_length_16)\n        elif (payloadlen == 127):\n            self.stream.read_bytes(8, self._on_frame_length_64)\n    except StreamClosedError:\n        self._abort()\n", "label": 1}
{"function": "\n\ndef startall(self, wait=False, **kwdargs):\n    \"Start all of the threads in the thread pool.  If _wait_ is True\\n        then don't return until all threads are up and running.  Any extra\\n        keyword arguments are passed to the worker thread constructor.\\n        \"\n    self.logger.debug('startall called')\n    with self.regcond:\n        while (self.status != 'down'):\n            if ((self.status in ('start', 'up')) or self.ev_quit.isSet()):\n                self.logger.error('ignoring duplicate request to start thread pool')\n                return\n            self.logger.debug(('waiting for threads: count=%d' % self.runningcount))\n            self.regcond.wait()\n        if self.ev_quit.isSet():\n            return\n        self.runningcount = 0\n        self.status = 'start'\n        self.workers = []\n        if wait:\n            tpool = self\n        else:\n            tpool = None\n        self.logger.debug('starting threads in thread pool')\n        for i in range(self.numthreads):\n            t = self.workerClass(self.queue, logger=self.logger, ev_quit=self.ev_quit, tpool=tpool, **kwdargs)\n            self.workers.append(t)\n            t.start()\n        if wait:\n            while ((self.status != 'up') and (not self.ev_quit.isSet())):\n                self.logger.debug(('waiting for threads: count=%d' % self.runningcount))\n                self.regcond.wait()\n        self.logger.debug('startall done')\n", "label": 1}
{"function": "\n\ndef test_adult():\n    '\\n    Tests if it will work correctly for train and test set.\\n    '\n    skip_if_no_data()\n    adult_train = adult(which_set='train')\n    assert (adult_train.X >= 0.0).all()\n    assert (adult_train.y.dtype == bool)\n    assert (adult_train.X.shape == (30162, 104))\n    assert (adult_train.y.shape == (30162, 1))\n    adult_test = adult(which_set='test')\n    assert (adult_test.X >= 0.0).all()\n    assert (adult_test.y.dtype == bool)\n    assert (adult_test.X.shape == (15060, 103))\n    assert (adult_test.y.shape == (15060, 1))\n", "label": 1}
{"function": "\n\n@is_view('virt')\ndef links(request, hosts):\n    try:\n        fqdn = request.GET['fqdn']\n    except KeyError:\n        return None\n    current_node = None\n    for host in hosts:\n        if (fqdn == host['fqdn']):\n            current_node = host\n            break\n        for node in host.get('virtualization', {\n            \n        }).get('guests', []):\n            if (fqdn == node['fqdn']):\n                current_node = node\n                break\n        if current_node:\n            break\n    if current_node:\n        try:\n            links = current_node['kitchen']['data']['links']\n        except KeyError:\n            return None\n        for link in links:\n            if (link.get('title') == 'monitoring'):\n                return redirect(link['url'])\n        else:\n            return None\n", "label": 1}
{"function": "\n\n@execute_count(14)\ndef test_token_function(self):\n    ' Tests that token functions work properly '\n    assert (TokenTestModel.objects().count() == 0)\n    for i in range(10):\n        TokenTestModel.create(key=i, val=i)\n    assert (TokenTestModel.objects().count() == 10)\n    seen_keys = set()\n    last_token = None\n    for instance in TokenTestModel.objects().limit(5):\n        last_token = instance.key\n        seen_keys.add(last_token)\n    assert (len(seen_keys) == 5)\n    for instance in TokenTestModel.objects(pk__token__gt=functions.Token(last_token)):\n        seen_keys.add(instance.key)\n    assert (len(seen_keys) == 10)\n    assert all([(i in seen_keys) for i in range(10)])\n", "label": 1}
{"function": "\n\ndef test_where():\n    (X, Y) = (Die('X'), Die('Y'))\n    Z = Normal('Z', 0, 1)\n    assert (where(((Z ** 2) <= 1)).set == Interval((- 1), 1))\n    assert (where(((Z ** 2) <= 1)).as_boolean() == Interval((- 1), 1).as_relational(Z.symbol))\n    assert (where(And((X > Y), (Y > 4))).as_boolean() == And(Eq(X.symbol, 6), Eq(Y.symbol, 5)))\n    assert (len(where((X < 3)).set) == 2)\n    assert (1 in where((X < 3)).set)\n    (X, Y) = (Normal('X', 0, 1), Normal('Y', 0, 1))\n    assert (where(And(((X ** 2) <= 1), (X >= 0))).set == Interval(0, 1))\n    XX = given(X, And(((X ** 2) <= 1), (X >= 0)))\n    assert (XX.pspace.domain.set == Interval(0, 1))\n    assert (XX.pspace.domain.as_boolean() == And((0 <= X.symbol), ((X.symbol ** 2) <= 1), ((- oo) < X.symbol), (X.symbol < oo)))\n    with raises(TypeError):\n        XX = given(X, (X + 3))\n", "label": 1}
{"function": "\n\ndef _populate_config_from_old_location(self, conf):\n    if (('rate_limit_after_segment' in conf) or ('rate_limit_segments_per_sec' in conf) or ('max_get_time' in conf) or ('__file__' not in conf)):\n        return\n    cp = ConfigParser()\n    if os.path.isdir(conf['__file__']):\n        read_conf_dir(cp, conf['__file__'])\n    else:\n        cp.read(conf['__file__'])\n    try:\n        pipe = cp.get('pipeline:main', 'pipeline')\n    except (NoSectionError, NoOptionError):\n        return\n    proxy_name = pipe.rsplit(None, 1)[(- 1)]\n    proxy_section = ('app:' + proxy_name)\n    for setting in ('rate_limit_after_segment', 'rate_limit_segments_per_sec', 'max_get_time'):\n        try:\n            conf[setting] = cp.get(proxy_section, setting)\n        except (NoSectionError, NoOptionError):\n            pass\n", "label": 1}
{"function": "\n\ndef _ensure_features(dry_run, verbose, apps):\n    '\\n    Ensures that all the Features necessary for the plans are created.\\n    '\n    Feature = apps.get_model('accounting', 'Feature')\n    if verbose:\n        logger.info('Ensuring Features')\n    edition_to_features = defaultdict(list)\n    for edition in EDITIONS:\n        for feature_type in FEATURE_TYPES:\n            feature = Feature(name=('%s %s' % (feature_type, edition)), feature_type=feature_type)\n            if (edition == SoftwarePlanEdition.ENTERPRISE):\n                feature.name = ('Dimagi Only %s' % feature.name)\n            if dry_run:\n                logger.info(('[DRY RUN] Creating Feature: %s' % feature))\n            else:\n                try:\n                    feature = Feature.objects.get(name=feature.name)\n                    if verbose:\n                        logger.info((\"Feature '%s' already exists. Using existing feature to add rate.\" % feature.name))\n                except ObjectDoesNotExist:\n                    feature.save()\n                    if verbose:\n                        logger.info(('Creating Feature: %s' % feature))\n            edition_to_features[edition].append(feature)\n    return edition_to_features\n", "label": 1}
{"function": "\n\ndef cache_cleaned(name=None, user=None):\n    '\\n    Ensure that the given package is not cached.\\n\\n    If no package is specified, this ensures the entire cache is cleared.\\n\\n    name\\n        The name of the package to remove from the cache, or None for all packages\\n\\n    user\\n        The user to run NPM with\\n    '\n    ret = {\n        'name': name,\n        'result': None,\n        'comment': '',\n        'changes': {\n            \n        },\n    }\n    specific_pkg = None\n    try:\n        cached_pkgs = __salt__['npm.cache_list'](path=name, runas=user)\n    except (CommandExecutionError, CommandNotFoundError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Error looking up cached {0}: {1}'.format((name or 'packages'), err)\n        return ret\n    if name:\n        all_cached_pkgs = __salt__['npm.cache_list'](path=None, runas=user)\n        cache_root_path = all_cached_pkgs[0]\n        specific_pkg = '{0}/{1}/'.format(cache_root_path, name)\n        if (specific_pkg not in cached_pkgs):\n            ret['result'] = True\n            ret['comment'] = 'Package {0} is not in the cache'.format(name)\n            return ret\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Cached {0} set to be removed'.format((name or 'packages'))\n        return ret\n    if __salt__['npm.cache_clean'](path=name, runas=user):\n        ret['result'] = True\n        ret['changes'][(name or 'cache')] = 'Removed'\n        ret['comment'] = 'Cached {0} successfully removed'.format((name or 'packages'))\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Error cleaning cached {0}'.format((name or 'packages'))\n    return ret\n", "label": 1}
{"function": "\n\ndef _generate_all_urls(self):\n    '\\n        Run all generators and yield (url, enpoint) tuples.\\n        '\n    script_name = self._script_name()\n    url_encoding = self.app.url_map.charset\n    url_generators = list(self.url_generators)\n    url_generators += [self.url_for_logger.iter_calls]\n    with self.app.test_request_context(base_url=(script_name or None)):\n        for generator in url_generators:\n            for generated in generator():\n                if isinstance(generated, basestring):\n                    url = generated\n                    endpoint = None\n                else:\n                    if isinstance(generated, Mapping):\n                        values = generated\n                        endpoint = generator.__name__\n                    else:\n                        (endpoint, values) = generated\n                    url = url_for(endpoint, **values)\n                    assert url.startswith(script_name), ('url_for returned an URL %r not starting with script_name %r. Bug in Werkzeug?' % (url, script_name))\n                    url = url[len(script_name):]\n                url = unquote(url)\n                parsed_url = urlsplit(url)\n                if (parsed_url.scheme or parsed_url.netloc):\n                    raise ValueError(('External URLs not supported: ' + url))\n                url = parsed_url.path\n                if (not isinstance(url, unicode)):\n                    url = url.decode(url_encoding)\n                (yield (url, endpoint))\n", "label": 1}
{"function": "\n\n@sensitive_variables('context')\ndef handle(self, request, context):\n    custom_script = context.get('script_data', '')\n    dev_mapping_1 = None\n    dev_mapping_2 = None\n    image_id = ''\n    source_type = context.get('source_type', None)\n    if (source_type in ['image_id', 'instance_snapshot_id']):\n        image_id = context['source_id']\n    elif (source_type in ['volume_id', 'volume_snapshot_id']):\n        try:\n            if api.nova.extension_supported('BlockDeviceMappingV2Boot', request):\n                volume_source_id = context['source_id'].split(':')[0]\n                device_name = (context.get('device_name', '').strip() or None)\n                dev_source_type_mapping = {\n                    'volume_id': 'volume',\n                    'volume_snapshot_id': 'snapshot',\n                }\n                dev_mapping_2 = [{\n                    'device_name': device_name,\n                    'source_type': dev_source_type_mapping[source_type],\n                    'destination_type': 'volume',\n                    'delete_on_termination': int(bool(context['delete_on_terminate'])),\n                    'uuid': volume_source_id,\n                    'boot_index': '0',\n                    'volume_size': context['volume_size'],\n                }]\n            else:\n                dev_mapping_1 = {\n                    context['device_name']: ('%s::%s' % (context['source_id'], int(bool(context['delete_on_terminate'])))),\n                }\n        except Exception:\n            msg = _('Unable to retrieve extensions information')\n            exceptions.handle(request, msg)\n    elif (source_type == 'volume_image_id'):\n        device_name = (context.get('device_name', '').strip() or None)\n        dev_mapping_2 = [{\n            'device_name': device_name,\n            'source_type': 'image',\n            'destination_type': 'volume',\n            'delete_on_termination': int(bool(context['delete_on_terminate'])),\n            'uuid': context['source_id'],\n            'boot_index': '0',\n            'volume_size': context['volume_size'],\n        }]\n    avail_zone = context.get('availability_zone', None)\n    try:\n        instance_count = int(context['count'])\n        count = 1\n        while (count <= instance_count):\n            if (instance_count == 1):\n                instance_name = context['name']\n            else:\n                instance_name = (context['name'] + str(count))\n            nics = []\n            for ptg_id in context['group_id']:\n                ep = client.pt_create(request, policy_target_group_id=ptg_id, name=(instance_name[:41] + '_gbpui'))\n                nics.append({\n                    'port-id': ep.port_id,\n                })\n            api.nova.server_create(request, instance_name, image_id, context['flavor'], context['keypair_id'], normalize_newlines(custom_script), security_groups=None, block_device_mapping=dev_mapping_1, block_device_mapping_v2=dev_mapping_2, nics=nics, availability_zone=avail_zone, instance_count=1, admin_pass=context['admin_pass'], disk_config=context.get('disk_config'), config_drive=context.get('config_drive'))\n            count += 1\n        return True\n    except Exception:\n        error = _('Unable to launch member %(count)s with name %(name)s')\n        msg = (error % {\n            'count': count,\n            'name': instance_name,\n        })\n        LOG.error(msg)\n        u = 'horizon:project:policytargets:policy_targetdetails'\n        policy_target_id = self.request.path.split('/')[(- 2)]\n        redirect = reverse(u, kwargs={\n            'policy_target_id': policy_target_id,\n        })\n        exceptions.handle(request, msg, redirect=redirect)\n        return False\n", "label": 1}
{"function": "\n\ndef post(self, request_id, group_id=None, name=None):\n    group = Group.get(self.session, group_id, name)\n    if (not group):\n        return self.notfound()\n    members = group.my_members()\n    my_role = self.current_user.my_role(members)\n    if (my_role not in ('manager', 'owner', 'np-owner')):\n        return self.forbidden()\n    request = self.session.query(Request).filter_by(id=request_id).scalar()\n    if (not request):\n        return self.notfound()\n    form = GroupRequestModifyForm(self.request.arguments)\n    form.status.choices = self._get_choices(request.status)\n    updates = request.my_status_updates()\n    if (not form.validate()):\n        return self.render('group-request-update.html', group=group, request=request, members=members, form=form, alerts=self.get_form_alerts(form.errors), statuses=REQUEST_STATUS_CHOICES, updates=updates)\n    if (form.data['status'] != 'cancelled'):\n        fail_message = 'This join is denied with this role at this time.'\n        try:\n            user_can_join = assert_can_join(request.requesting, request.get_on_behalf(), role=request.edge.role)\n        except UserNotAuditor as e:\n            user_can_join = False\n            fail_message = e\n        if (not user_can_join):\n            return self.render('group-request-update.html', group=group, request=request, members=members, form=form, statuses=REQUEST_STATUS_CHOICES, updates=updates, alerts=[Alert('danger', fail_message, 'Audit Policy Enforcement')])\n    request.update_status(self.current_user, form.data['status'], form.data['reason'])\n    self.session.commit()\n    AuditLog.log(self.session, self.current_user.id, 'update_request', 'Updated request to status: {}'.format(form.data['status']), on_group_id=group.id, on_user_id=request.requester.id)\n    edge = self.session.query(GroupEdge).filter_by(id=request.edge_id).one()\n    if (form.data['status'] == 'actioned'):\n        send_email(self.session, [request.requester.name], 'Added to group: {}'.format(group.groupname), 'request_actioned', settings, {\n            'group': group.name,\n            'actioned_by': self.current_user.name,\n            'reason': form.data['reason'],\n            'expiration': edge.expiration,\n            'role': edge.role,\n        })\n    elif (form.data['status'] == 'cancelled'):\n        send_email(self.session, [request.requester.name], 'Request to join cancelled: {}'.format(group.groupname), 'request_cancelled', settings, {\n            'group': group.name,\n            'cancelled_by': self.current_user.name,\n            'reason': form.data['reason'],\n            'expiration': edge.expiration,\n            'role': edge.role,\n        })\n    if form.data['redirect_aggregate']:\n        return self.redirect('/user/requests')\n    else:\n        return self.redirect('/groups/{}/requests'.format(group.name))\n", "label": 1}
{"function": "\n\ndef _load_extensions(self):\n    'Dynamically load all the extensions .'\n    extensions = {\n        \n    }\n    abs_path = os.path.dirname(os.path.abspath(__file__))\n    extdirs = [e for e in os.listdir(abs_path) if ((not e.startswith('tests')) and (not e.endswith('.pyc')) and (not e.endswith('.py')))]\n    for e in extdirs:\n        log.info(e)\n        extpath = ((abs_path + '/') + e)\n        ext_files = [f for f in os.listdir(extpath) if (f.endswith('.py') and (not f.startswith('__init__')))]\n        for f in ext_files:\n            log.info(f)\n            ext_name = ((('toscaparser/extensions/' + e) + '/') + f.strip('.py'))\n            ext_name = ext_name.replace('/', '.')\n            try:\n                extinfo = importlib.import_module(ext_name)\n                version = getattr(extinfo, 'VERSION')\n                defs_file = ((extpath + '/') + getattr(extinfo, 'DEFS_FILE'))\n                sections = getattr(extinfo, 'SECTIONS', ())\n                extensions[version] = {\n                    'sections': sections,\n                    'defs_file': defs_file,\n                }\n            except ImportError:\n                raise ToscaExtImportError(ext_name=ext_name)\n            except AttributeError:\n                attrs = ', '.join(REQUIRED_ATTRIBUTES)\n                raise ToscaExtAttributeError(ext_name=ext_name, attrs=attrs)\n    return extensions\n", "label": 1}
{"function": "\n\ndef test_query_count_on_activate(self):\n    '\\n        Count number of queries needed for activation of complex run.\\n\\n        Queries explained:\\n        ------------------\\n\\n        Query 1: Get the environment ids from this run\\n\\n            \"SELECT `environments_environment`.`id` FROM\\n            `environments_environment` INNER JOIN\\n            `execution_run_environments` ON (`environments_environment`.`id`\\n             = `execution_run_environments`.`environment_id`) WHERE (\\n             `environments_environment`.`deleted_on` IS NULL AND\\n             `execution_run_environments`.`run_id` = 1 )\",\\n\\n        Query 2: Get the caseversion ids that SHOULD be included in this run,\\n            in order\\n\\n            \"SELECT DISTINCT cv.id as id\\n            FROM execution_run as r\\n                INNER JOIN execution_runsuite as rs\\n                    ON rs.run_id = r.id\\n                INNER JOIN library_suitecase as sc\\n                    ON rs.suite_id = sc.suite_id\\n                INNER JOIN library_suite as s\\n                    ON sc.suite_id = s.id\\n                INNER JOIN library_caseversion as cv\\n                    ON cv.case_id = sc.case_id\\n                    AND cv.productversion_id = r.productversion_id\\n                INNER JOIN library_caseversion_environments as cve\\n                    ON cv.id = cve.caseversion_id\\n            WHERE cv.status = \\'active\\'\\n                AND s.status = \\'active\\'\\n                AND rs.run_id = 1\\n                AND cve.environment_id IN (1,2,3,4)\\n            ORDER BY rs.order, sc.order\\n            \",\\n\\n        Query 3-6: Get all the runcaseversions that are not in the result of\\n         Query 2\\n            to be used for delete. and then delete them.\\n\\n            \"SELECT `execution_runcaseversion`.`id`,\\n            `execution_runcaseversion`.`created_on`,\\n            `execution_runcaseversion`.`created_by_id`,\\n            `execution_runcaseversion`.`modified_on`,\\n            `execution_runcaseversion`.`modified_by_id`,\\n            `execution_runcaseversion`.`deleted_on`,\\n            `execution_runcaseversion`.`deleted_by_id`,\\n            `execution_runcaseversion`.`cc_version`,\\n            `execution_runcaseversion`.`run_id`, `execution_runcaseversion`\\n            .`caseversion_id`, `execution_runcaseversion`.`order` FROM\\n            `execution_runcaseversion` WHERE (`execution_runcaseversion`\\n            .`deleted_on` IS NULL AND `execution_runcaseversion`.`run_id` =\\n            1  AND NOT (`execution_runcaseversion`.`caseversion_id` IN (2,\\n            3, 4, 5, 6, 7))) ORDER BY `execution_runcaseversion`.`order` ASC\",\\n\\n            \"SELECT `execution_result`.`id`, `execution_result`\\n            .`created_on`, `execution_result`.`created_by_id`,\\n            `execution_result`.`modified_on`,\\n            `execution_result`.`modified_by_id`,\\n            `execution_result`.`deleted_on`, `execution_result`\\n            .`deleted_by_id`, `execution_result`.`cc_version`,\\n            `execution_result`.`tester_id`, `execution_result`\\n            .`runcaseversion_id`, `execution_result`.`environment_id`,\\n            `execution_result`.`status`, `execution_result`.`comment`,\\n            `execution_result`.`is_latest`, `execution_result`.`review`,\\n            `execution_result`.`reviewed_by_id` FROM `execution_result`\\n            WHERE `execution_result`.`runcaseversion_id` IN (1)\",\\n\\n            \"DELETE FROM `execution_runcaseversion_environments` WHERE\\n            `runcaseversion_id` IN (1)\",\\n\\n            \"DELETE FROM `execution_runcaseversion` WHERE `id` IN (1)\",\\n\\n        Query 7: find duplicates in the existing rcvs, if they exist\\n\\n            SELECT `execution_runcaseversion`.`caseversion_id`,\\n            COUNT(`execution_runcaseversion`.`caseversion_id`) AS\\n            `num_records` FROM `execution_runcaseversion` WHERE (\\n            `execution_runcaseversion`.`deleted_on` IS NULL AND\\n            `execution_runcaseversion`.`run_id` = 8 ) GROUP BY\\n            `execution_runcaseversion`.`caseversion_id` HAVING COUNT(\\n            `execution_runcaseversion`.`caseversion_id`) > 1  ORDER BY\\n            `execution_runcaseversion`.`order` ASC\\n\\n        Query 8: Get existing runcaseversions with the caseversion ids so we\\n         can use\\n            Them to build the new RunCaseVersion objects we will only be\\n            updated\\n            with order in the bulk create.\\n\\n            \"SELECT `execution_runcaseversion`.`id`,\\n            `execution_runcaseversion`.`caseversion_id` FROM\\n            `execution_runcaseversion` WHERE (`execution_runcaseversion`\\n            .`deleted_on` IS NULL AND `execution_runcaseversion`.`run_id` =\\n            1 ) ORDER BY `execution_runcaseversion`.`order` ASC\",\\n\\n        Query 9: update order on existing rcvs\\n\\n            \"UPDATE `execution_runcaseversion` SET `modified_on` =\\n            \\'2013-03-15 01:00:08\\', `modified_by_id` = NULL, `order` = 4,\\n            `cc_version` = `execution_runcaseversion`.`cc_version` + 1 WHERE\\n             (`execution_runcaseversion`.`deleted_on` IS NULL AND\\n             `execution_runcaseversion`.`run_id` = 8  AND\\n             `execution_runcaseversion`.`caseversion_id` = 16 )\",\\n\\n        Query 10: bulk insert for RunCaseVersions\\n\\n            \"INSERT INTO `execution_runcaseversion` (`created_on`,\\n            `created_by_id`, `modified_on`, `modified_by_id`, `deleted_on`,\\n            `deleted_by_id`, `cc_version`, `run_id`, `caseversion_id`,\\n            `order`) VALUES (\\'2013-03-15 01:00:08\\', NULL,\\n            \\'2013-03-15 01:00:08\\', NULL, NULL, NULL, 0, 8, 13, 1),\\n            (\\'2013-03-15 01:00:08\\', NULL, \\'2013-03-15 01:00:08\\', NULL, NULL,\\n             NULL, 0, 8, 14, 2), (\\'2013-03-15 01:00:08\\', NULL,\\n             \\'2013-03-15 01:00:08\\', NULL, NULL, NULL, 0, 8, 15, 3),\\n             (\\'2013-03-15 01:00:08\\', NULL, \\'2013-03-15 01:00:08\\', NULL, NULL,\\n             NULL, 0, 8, 17, 5), (\\'2013-03-15 01:00:08\\', NULL,\\n             \\'2013-03-15 01:00:08\\', NULL, NULL, NULL, 0, 8, 18, 6)\"\\n\\n        Query 11: In order to add the runcaseversion_environment records,\\n            we need to have all the relevant runcaseversions and prefetch the\\n            environments for the caseversions\\n\\n            \"SELECT `execution_runcaseversion`.`id`,\\n            `execution_runcaseversion`.`created_on`,\\n            `execution_runcaseversion`.`created_by_id`,\\n            `execution_runcaseversion`.`modified_on`,\\n            `execution_runcaseversion`.`modified_by_id`,\\n            `execution_runcaseversion`.`deleted_on`,\\n            `execution_runcaseversion`.`deleted_by_id`,\\n            `execution_runcaseversion`.`cc_version`,\\n            `execution_runcaseversion`.`run_id`, `execution_runcaseversion`\\n            .`caseversion_id`, `execution_runcaseversion`.`order`,\\n            `library_caseversion`.`id`, `library_caseversion`.`created_on`,\\n            `library_caseversion`.`created_by_id`, `library_caseversion`\\n            .`modified_on`, `library_caseversion`.`modified_by_id`,\\n            `library_caseversion`.`deleted_on`, `library_caseversion`\\n            .`deleted_by_id`, `library_caseversion`.`cc_version`,\\n            `library_caseversion`.`status`, `library_caseversion`\\n            .`productversion_id`, `library_caseversion`.`case_id`,\\n            `library_caseversion`.`name`, `library_caseversion`\\n            .`description`, `library_caseversion`.`latest`,\\n            `library_caseversion`.`envs_narrowed` FROM\\n            `execution_runcaseversion` INNER JOIN `library_caseversion` ON (\\n            `execution_runcaseversion`.`caseversion_id` =\\n            `library_caseversion`.`id`) WHERE (`execution_runcaseversion`\\n            .`deleted_on` IS NULL AND `execution_runcaseversion`.`run_id` =\\n            1 ) ORDER BY `execution_runcaseversion`.`order` ASC\",\\n\\n        Query 12: This is the prefetch_related query used with Query 9.  Django\\n            makes a separate query and links them in-memory.\\n\\n            \"SELECT (`library_caseversion_environments`.`caseversion_id`) AS\\n             `_prefetch_related_val`, `environments_environment`.`id`,\\n             `environments_environment`.`created_on`,\\n             `environments_environment`.`created_by_id`,\\n             `environments_environment`.`modified_on`,\\n             `environments_environment`.`modified_by_id`,\\n             `environments_environment`.`deleted_on`,\\n             `environments_environment`.`deleted_by_id`,\\n             `environments_environment`.`cc_version`,\\n             `environments_environment`.`profile_id` FROM\\n             `environments_environment` INNER JOIN\\n             `library_caseversion_environments` ON (\\n             `environments_environment`.`id` =\\n             `library_caseversion_environments`.`environment_id`) WHERE (\\n             `environments_environment`.`deleted_on` IS NULL AND\\n             `library_caseversion_environments`.`caseversion_id` IN (2, 3,\\n             4, 5, 6, 7))\",\\n\\n        Query 13: runcaseversion_environments that already existed that\\n        pertain to\\n            the runcaseversions that are still relevant.\\n\\n            \"SELECT `execution_runcaseversion_environments`\\n            .`runcaseversion_id`, `execution_runcaseversion_environments`\\n            .`environment_id` FROM `execution_runcaseversion_environments`\\n            WHERE `execution_runcaseversion_environments`\\n            .`runcaseversion_id` IN (3, 4, 5, 2, 6, 7)\",\\n\\n        Query 14: Get the environments for this run so we can find the\\n        intersection\\n            with the caseversions.\\n\\n            \"SELECT `environments_environment`.`id` FROM\\n            `environments_environment` INNER JOIN\\n            `execution_run_environments` ON (`environments_environment`.`id`\\n             = `execution_run_environments`.`environment_id`) WHERE (\\n             `environments_environment`.`deleted_on` IS NULL AND\\n             `execution_run_environments`.`run_id` = 1 )\",\\n\\n        Query 15: Find the runcaseversion_environments that are no longer\\n        relevant.\\n\\n            \"SELECT `execution_runcaseversion_environments`.`id`,\\n            `execution_runcaseversion_environments`.`runcaseversion_id`,\\n            `execution_runcaseversion_environments`.`environment_id` FROM\\n            `execution_runcaseversion_environments`\\n            WHERE ((`execution_runcaseversion_environments`.`runcaseversion_id`\\n            = 2  AND `execution_runcaseversion_environments`.`environment_id`\\n            = 5 ))\",\\n\\n        Query 16: Delete the runcaseversion_environments that pertained to the\\n            caseversion that are no longer relevant.\\n\\n            \"DELETE FROM `execution_runcaseversion_environments` WHERE `id`\\n            IN (9)\",\\n\\n        Query 17: Bulk insert of runcaseversion_environment mappings.\\n\\n            \"INSERT INTO `execution_runcaseversion_environments`\\n            (`runcaseversion_id`, `environment_id`) VALUES (7, 3), (5, 4),\\n            (3, 1), (3, 3), (6, 4), (7, 4), (5, 2), (6, 1), (4, 4), (3, 2),\\n            (7, 1), (6, 3), (6, 2), (4, 3), (4, 2), (3, 4), (5, 1), (4, 1),\\n            (7, 2), (5, 3)\",\\n\\n        Query 18: Update the test run to make it active.\\n\\n            \"UPDATE `execution_run` SET `created_on` = \\'2012-11-20 00:11:25\\',\\n            `created_by_id` = NULL, `modified_on` = \\'2012-11-20 00:11:25\\',\\n            `modified_by_id` = NULL, `deleted_on` = NULL, `deleted_by_id` =\\n            NULL, `cc_version` = 1, `has_team` = 0, `status` = \\'active\\',\\n            `productversion_id` = 1, `name` = \\'Test Run\\', `description` = \\'\\',\\n            `start` = \\'2012-11-19\\', `end` = NULL, `build` = NULL,\\n            `is_series` = 0, `series_id` = NULL\\n            WHERE (`execution_run`.`deleted_on` IS NULL\\n            AND `execution_run`.`id` = 1\\n            AND `execution_run`.`cc_version` = 0 )\"\\n\\n        '\n    r = self.F.RunFactory.create(productversion=self.pv8)\n    old_cv = self.F.CaseVersionFactory.create(name=\"I shouldn't be here\", productversion=self.pv8, status='active')\n    self.F.RunCaseVersionFactory(run=r, caseversion=old_cv)\n    ts = self.F.SuiteFactory.create(product=self.p, status='active')\n    self.F.RunSuiteFactory.create(suite=ts, run=r)\n    cv_needed = []\n    for num in range(6):\n        cv = self.F.CaseVersionFactory.create(name=('casey' + str(num)), productversion=self.pv8, status='active')\n        self.F.SuiteCaseFactory.create(suite=ts, case=cv.case)\n        cv_needed.append(cv)\n    existing_rcv = self.F.RunCaseVersionFactory(run=r, caseversion=cv_needed[3], order=0)\n    old_env = self.F.EnvironmentFactory.create_set(['OS', 'Browser'], ['Atari', 'RS-232'])[0]\n    self.F.model.RunCaseVersion.environments.through(runcaseversion=existing_rcv, environment=old_env).save()\n    from django.conf import settings\n    from django.db import connection\n    settings.DEBUG = True\n    connection.queries = []\n    try:\n        with self.assertNumQueries(17):\n            r.activate()\n        selects = [x['sql'] for x in connection.queries if x['sql'].startswith('SELECT')]\n        inserts = [x['sql'] for x in connection.queries if x['sql'].startswith('INSERT')]\n        updates = [x['sql'] for x in connection.queries if x['sql'].startswith('UPDATE')]\n        deletes = [x['sql'] for x in connection.queries if x['sql'].startswith('DELETE')]\n        self.assertEqual(len(selects), 10)\n        self.assertEqual(len(inserts), 2)\n        self.assertEqual(len(updates), 2)\n        self.assertEqual(len(deletes), 3)\n    except AssertionError as e:\n        raise e\n    finally:\n        settings.DEBUG = False\n    self.refresh(r)\n    self.assertEqual(r.runcaseversions.count(), 6)\n    self.assertEqual(self.F.model.RunCaseVersion.environments.through.objects.count(), 24)\n    self.assertEqual(self.F.model.RunCaseVersion.objects.filter(run=r, caseversion=old_cv).count(), 0)\n", "label": 1}
{"function": "\n\ndef test_shared_mutable(self):\n    bval = numpy.arange(5)\n    b = shared(bval)\n    b_out = (b * 2)\n    assert (b.get_value(borrow=True) is not bval)\n    bval = data_of(b)\n    f = pfunc([], [b_out], mode='FAST_RUN')\n    assert (f() == (numpy.arange(5) * 2)).all()\n    assert numpy.all((b.get_value(borrow=True) == numpy.arange(5)))\n    f = pfunc([], [b_out], updates=[(b, b_out)], mode='FAST_RUN')\n    assert (f() == (numpy.arange(5) * 2)).all()\n    assert (b.get_value(borrow=True) == (numpy.arange(5) * 2)).all()\n    assert (bval == (numpy.arange(5) * 2)).all()\n    bval = numpy.arange(5)\n    b.set_value(bval, borrow=True)\n    bval = data_of(b)\n    f = pfunc([], [b_out], updates=[(b, (b_out + 3))], mode='FAST_RUN')\n    assert (f() == (numpy.arange(5) * 2)).all()\n    assert (b.get_value(borrow=True) == ((numpy.arange(5) * 2) + 3)).all()\n    assert (not (bval == numpy.arange(5)).all())\n    assert (not (bval == b.get_value(borrow=True)).all())\n", "label": 1}
{"function": "\n\ndef MagneticDipoleVectorPotential(srcLoc, obsLoc, component, moment=1.0, dipoleMoment=(0.0, 0.0, 1.0), mu=mu_0):\n    \"\\n        Calculate the vector potential of a set of magnetic dipoles\\n        at given locations 'ref. <http://en.wikipedia.org/wiki/Dipole#Magnetic_vector_potential>'\\n\\n        :param numpy.ndarray srcLoc: Location of the source(s) (x, y, z)\\n        :param numpy.ndarray,SimPEG.Mesh obsLoc: Where the potentials will be calculated (x, y, z) or a SimPEG Mesh\\n        :param str,list component: The component to calculate - 'x', 'y', or 'z' if an array, or grid type if mesh, can be a list\\n        :param numpy.ndarray dipoleMoment: The vector dipole moment\\n        :rtype: numpy.ndarray\\n        :return: The vector potential each dipole at each observation location\\n    \"\n    if (type(component) in [list, tuple]):\n        out = range(len(component))\n        for (i, comp) in enumerate(component):\n            out[i] = MagneticDipoleVectorPotential(srcLoc, obsLoc, comp, dipoleMoment=dipoleMoment)\n        return np.concatenate(out)\n    if isinstance(obsLoc, Mesh.BaseMesh):\n        mesh = obsLoc\n        assert (component in ['Ex', 'Ey', 'Ez', 'Fx', 'Fy', 'Fz']), \"Components must be in: ['Ex','Ey','Ez','Fx','Fy','Fz']\"\n        return MagneticDipoleVectorPotential(srcLoc, getattr(mesh, ('grid' + component)), component[1], dipoleMoment=dipoleMoment)\n    if (component == 'x'):\n        dimInd = 0\n    elif (component == 'y'):\n        dimInd = 1\n    elif (component == 'z'):\n        dimInd = 2\n    else:\n        raise ValueError('Invalid component')\n    srcLoc = np.atleast_2d(srcLoc)\n    obsLoc = np.atleast_2d(obsLoc)\n    dipoleMoment = np.atleast_2d(dipoleMoment)\n    nEdges = obsLoc.shape[0]\n    nSrc = srcLoc.shape[0]\n    m = np.array(dipoleMoment).repeat(nEdges, axis=0)\n    A = np.empty((nEdges, nSrc))\n    for i in range(nSrc):\n        dR = (obsLoc - srcLoc[(i, np.newaxis)].repeat(nEdges, axis=0))\n        mCr = np.cross(m, dR)\n        r = np.sqrt((dR ** 2).sum(axis=1))\n        A[:, i] = (((+ (mu / (4 * pi))) * mCr[:, dimInd]) / (r ** 3))\n    if (nSrc == 1):\n        return A.flatten()\n    return A\n", "label": 1}
{"function": "\n\ndef __init__(self, **kwargs):\n    kwargs = copy.deepcopy(kwargs)\n    self.name = kwargs.pop('name', None)\n    path = get_root_path(kwargs.get('path', None))\n    self.data = []\n    if self.name:\n        self.path = os.path.join(path, self.name, 'config')\n        if os.path.isfile(self.path):\n            with salt.utils.fopen(self.path) as fhr:\n                for line in fhr.readlines():\n                    match = self.pattern.findall(line.strip())\n                    if match:\n                        self.data.append((match[0][0], match[0][(- 1)]))\n                    match = self.non_interpretable_pattern.findall(line.strip())\n                    if match:\n                        self.data.append(('', match[0][0]))\n    else:\n        self.path = None\n\n    def _replace(key, val):\n        if val:\n            self._filter_data(key)\n            self.data.append((key, val))\n    default_data = _get_lxc_default_data(**kwargs)\n    for (key, val) in six.iteritems(default_data):\n        _replace(key, val)\n    old_net = self._filter_data('lxc.network')\n    net_datas = _network_conf(conf_tuples=old_net, **kwargs)\n    if net_datas:\n        for row in net_datas:\n            self.data.extend(list(row.items()))\n    for idx in ['lxc.cgroup.memory.limit_in_bytes']:\n        if (not default_data.get(idx)):\n            self._filter_data(idx)\n", "label": 1}
{"function": "\n\ndef _caas_2_2_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_tag_tagKey_ALLFILTERS(self, method, url, body, headers):\n    (_, params) = url.split('?')\n    parameters = params.split('&')\n    for parameter in parameters:\n        (key, value) = parameter.split('=')\n        if (key == 'id'):\n            assert (value == 'fake_id')\n        elif (key == 'name'):\n            assert (value == 'fake_name')\n        elif (key == 'valueRequired'):\n            assert (value == 'false')\n        elif (key == 'displayOnReport'):\n            assert (value == 'false')\n        elif (key == 'pageSize'):\n            assert (value == '250')\n        else:\n            raise ValueError('Could not find in url parameters {0}:{1}'.format(key, value))\n    body = self.fixtures.load('tag_tagKey_list.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 1}
{"function": "\n\n@staticmethod\ndef load(filename, **kwargs):\n    if (filename[:8] == 'atlas://'):\n        rfn = filename[8:]\n        try:\n            (rfn, uid) = rfn.rsplit('/', 1)\n        except ValueError:\n            raise ValueError(('Image: Invalid %s name for atlas' % filename))\n        atlas = Cache.get('kv.atlas', rfn)\n        if atlas:\n            texture = atlas[uid]\n            fn = ('atlas://%s/%s' % (rfn, uid))\n            cid = '{}|{:d}|{:d}'.format(fn, False, 0)\n            Cache.append('kv.texture', cid, texture)\n            return Image(texture)\n        afn = rfn\n        if (not afn.endswith('.atlas')):\n            afn += '.atlas'\n        afn = resource_find(afn)\n        if (not afn):\n            raise Exception(('Unable to found %r atlas' % afn))\n        atlas = Atlas(afn)\n        Cache.append('kv.atlas', rfn, atlas)\n        for (nid, texture) in atlas.textures.items():\n            fn = ('atlas://%s/%s' % (rfn, nid))\n            cid = '{}|{:d}|{:d}'.format(fn, False, 0)\n            Cache.append('kv.texture', cid, texture)\n        return Image(atlas[uid])\n    ext = filename.split('.')[(- 1)].lower()\n    if filename.startswith(('http://', 'https://')):\n        ext = ext.split('?')[0]\n    filename = resource_find(filename)\n    if (ext == 'zip'):\n        return ImageLoader.zip_loader(filename)\n    else:\n        im = None\n        for loader in ImageLoader.loaders:\n            if (ext not in loader.extensions()):\n                continue\n            Logger.debug(('Image%s: Load <%s>' % (loader.__name__[11:], filename)))\n            im = loader(filename, **kwargs)\n            break\n        if (im is None):\n            raise Exception(('Unknown <%s> type, no loader found.' % ext))\n        return im\n", "label": 1}
{"function": "\n\ndef test_meta_save_read(self):\n    name = 'my_data.txt'\n    self.ofs.put_stream(self.bucket_name, name, self._makefp(), params={\n        'hello': 'world',\n        'foo': 'bar',\n    })\n    meta = self.ofs.get_metadata(self.bucket_name, name)\n    assert ('_owner' in meta), meta\n    assert ('_creation_time' in meta), meta\n    assert ('_last_modified' in meta), meta\n    assert ('_checksum' in meta), meta\n    assert ('_format' in meta), meta\n    assert ('_bucket' in meta), meta\n    assert ('_label' in meta), meta\n    assert ('_content_length' in meta), meta\n    assert (meta['hello'] == 'world'), meta['hello']\n    assert (meta['foo'] == 'bar'), meta['bar']\n", "label": 1}
{"function": "\n\ndef test_graph_equality_edges(self):\n    '\\n        Graph equality test. This one checks edge equality. \\n        '\n    gr = graph()\n    gr.add_nodes([0, 1, 2, 3, 4])\n    gr.add_edge((0, 1), wt=1)\n    gr.add_edge((0, 2), wt=2)\n    gr.add_edge((1, 2), wt=3)\n    gr.add_edge((3, 4), wt=4)\n    gr2 = deepcopy(gr)\n    gr3 = deepcopy(gr)\n    gr3.del_edge((0, 2))\n    gr4 = deepcopy(gr)\n    gr4.add_edge((2, 4))\n    gr5 = deepcopy(gr)\n    gr5.del_edge((0, 2))\n    gr5.add_edge((2, 4))\n    gr6 = deepcopy(gr)\n    gr6.del_edge((0, 2))\n    gr6.add_edge((0, 2), wt=10)\n    assert (gr == gr2)\n    assert (gr2 == gr)\n    assert (gr != gr3)\n    assert (gr3 != gr)\n    assert (gr != gr4)\n    assert (gr4 != gr)\n    assert (gr != gr5)\n    assert (gr5 != gr)\n    assert (gr != gr6)\n    assert (gr6 != gr)\n", "label": 1}
{"function": "\n\ndef mergeOtherPDS(self, other_db_name, other_db_path='./', update='error', status_interval=5):\n    '\\n            update determines the update scheme\\n                error : raise error when key exists\\n                ignore: do nothing when key exists, keep old value\\n                update: update value when key exists with value from otherData \\n        '\n    error = False\n    if (update == 'error'):\n        error = True\n    elif (update == 'ignore'):\n        ignore = True\n    elif (update == 'update'):\n        ignore = False\n    else:\n        raise TypeError(\"update must be one of the following: 'error', 'ignore', 'update'\")\n    transfered = 0\n    ignored = 0\n    if (status_interval == 0):\n        PB = progress.ProgressSilentDummy\n    else:\n        PB = progress.ProgressBarFancy\n    with PersistentDataStructure(name=other_db_name, path=other_db_path, verbose=self.verbose) as otherData:\n        c = progress.UnsignedIntValue(val=0)\n        m = progress.UnsignedIntValue(val=len(otherData))\n        with PB(count=c, max_count=m, verbose=self.verbose, interval=status_interval) as pb:\n            pb.start()\n            for k in otherData:\n                if (k in self):\n                    if error:\n                        raise KeyError('merge error, key already found in PDS')\n                    elif ignore:\n                        if (self.verbose > 1):\n                            print('ignore key', k)\n                        ignored += 1\n                        continue\n                    else:\n                        if (self.verbose > 1):\n                            print('replace key', k)\n                        del self[k]\n                value = otherData[k]\n                try:\n                    self[k] = value\n                    transfered += 1\n                finally:\n                    if isinstance(value, PersistentDataStructure):\n                        value.close()\n                with c.get_lock():\n                    c.value += 1\n                sys.stdout.flush()\n        print('merge summary:')\n        print('   transfered values:', transfered)\n        print('      ignored values:', ignored)\n", "label": 1}
{"function": "\n\ndef _cull(self):\n    if (int(self._num_entries) < self._max_entries):\n        return\n    try:\n        filelist = sorted(os.listdir(self._dir))\n    except (IOError, OSError):\n        return\n    if (self._cull_frequency == 0):\n        doomed = filelist\n    else:\n        doomed = [os.path.join(self._dir, k) for (i, k) in enumerate(filelist) if ((i % self._cull_frequency) == 0)]\n    for topdir in doomed:\n        try:\n            for (root, _, files) in os.walk(topdir):\n                for f in files:\n                    self._delete(os.path.join(root, f))\n        except (IOError, OSError):\n            pass\n", "label": 1}
{"function": "\n\ndef test_schema_recursive(schema_app):\n    app_client = schema_app.app.test_client()\n    headers = {\n        'Content-type': 'application/json',\n    }\n    valid_object = {\n        'children': [{\n            'children': [],\n        }, {\n            'children': [{\n                'children': [],\n            }],\n        }, {\n            'children': [],\n        }],\n    }\n    invalid_object = {\n        'children': [42],\n    }\n    wrong_type = app_client.post('/v1.0/test_schema_recursive', headers=headers, data=json.dumps(42))\n    assert (wrong_type.status_code == 400)\n    assert (wrong_type.content_type == 'application/problem+json')\n    wrong_type_response = json.loads(wrong_type.data.decode())\n    assert (wrong_type_response['title'] == 'Bad Request')\n    assert wrong_type_response['detail'].startswith(\"42 is not of type 'object'\")\n    wrong_items = app_client.post('/v1.0/test_schema_recursive', headers=headers, data=json.dumps(invalid_object))\n    assert (wrong_items.status_code == 400)\n    assert (wrong_items.content_type == 'application/problem+json')\n    wrong_items_response = json.loads(wrong_items.data.decode())\n    assert (wrong_items_response['title'] == 'Bad Request')\n    assert wrong_items_response['detail'].startswith(\"42 is not of type 'object'\")\n    right_type = app_client.post('/v1.0/test_schema_recursive', headers=headers, data=json.dumps(valid_object))\n    assert (right_type.status_code == 200)\n", "label": 1}
{"function": "\n\ndef _updateRegistration(self, fd):\n    try:\n        fileno = (fd.fileno() if (not isinstance(fd, int)) else fd)\n        self._poller.unregister(fileno)\n    except (SocketError, IOError, ValueError) as e:\n        if (e.args[0] == EBADF):\n            keys = [k for (k, v) in list(self._map.items()) if (v == fd)]\n            for key in keys:\n                del self._map[key]\n    mask = 0\n    if (fd in self._read):\n        mask = (mask | select.EPOLLIN)\n    if (fd in self._write):\n        mask = (mask | select.EPOLLOUT)\n    if mask:\n        self._poller.register(fd, mask)\n        self._map[fileno] = fd\n    else:\n        super(EPoll, self).discard(fd)\n", "label": 1}
{"function": "\n\n@testing.requires.predictable_gc\ndef test_gced_delete_on_rollback(self):\n    (User, users) = (self.classes.User, self.tables.users)\n    s = self.session()\n    u1 = User(name='ed')\n    s.add(u1)\n    s.commit()\n    s.delete(u1)\n    u1_state = attributes.instance_state(u1)\n    assert (u1_state in s.identity_map.all_states())\n    assert (u1_state in s._deleted)\n    s.flush()\n    assert (u1_state not in s.identity_map.all_states())\n    assert (u1_state not in s._deleted)\n    del u1\n    gc_collect()\n    assert (u1_state.obj() is None)\n    s.rollback()\n    assert (u1_state not in s.identity_map.all_states())\n    u1 = s.query(User).filter_by(name='ed').one()\n    assert (u1_state not in s.identity_map.all_states())\n    assert (s.scalar(users.count()) == 1)\n    s.delete(u1)\n    s.flush()\n    assert (s.scalar(users.count()) == 0)\n    s.commit()\n", "label": 1}
{"function": "\n\ndef _parse_settings_bond_2(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond2.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '2',\n    }\n    valid = ['list of ips (up to 16)']\n    if ('arp_ip_target' in opts):\n        if isinstance(opts['arp_ip_target'], list):\n            if (1 <= len(opts['arp_ip_target']) <= 16):\n                bond.update({\n                    'arp_ip_target': '',\n                })\n                for ip in opts['arp_ip_target']:\n                    if (len(bond['arp_ip_target']) > 0):\n                        bond['arp_ip_target'] = ((bond['arp_ip_target'] + ',') + ip)\n                    else:\n                        bond['arp_ip_target'] = ip\n            else:\n                _raise_error_iface(iface, 'arp_ip_target', valid)\n        else:\n            _raise_error_iface(iface, 'arp_ip_target', valid)\n    else:\n        _raise_error_iface(iface, 'arp_ip_target', valid)\n    if ('arp_interval' in opts):\n        try:\n            int(opts['arp_interval'])\n            bond.update({\n                'arp_interval': opts['arp_interval'],\n            })\n        except ValueError:\n            _raise_error_iface(iface, 'arp_interval', ['integer'])\n    else:\n        _log_default_iface(iface, 'arp_interval', bond_def['arp_interval'])\n        bond.update({\n            'arp_interval': bond_def['arp_interval'],\n        })\n    if ('primary' in opts):\n        bond.update({\n            'primary': opts['primary'],\n        })\n    if ('hashing-algorithm' in opts):\n        valid = ['layer2', 'layer2+3', 'layer3+4']\n        if (opts['hashing-algorithm'] in valid):\n            bond.update({\n                'xmit_hash_policy': opts['hashing-algorithm'],\n            })\n        else:\n            _raise_error_iface(iface, 'hashing-algorithm', valid)\n    return bond\n", "label": 1}
{"function": "\n\ndef _run(name, cmd, output=None, no_start=False, preserve_state=True, stdin=None, python_shell=True, output_loglevel='debug', use_vt=False, path=None, ignore_retcode=False, chroot_fallback=None, keep_env='http_proxy,https_proxy,no_proxy'):\n    '\\n    Common logic for lxc.run functions\\n\\n    path\\n        path to the container parent\\n        default: /var/lib/lxc (system default)\\n\\n        .. versionadded:: 2015.8.0\\n\\n    '\n    orig_state = state(name, path=path)\n    try:\n        if attachable(name, path=path):\n            ret = __salt__['container_resource.run'](name, cmd, path=path, container_type=__virtualname__, exec_driver=EXEC_DRIVER, output=output, no_start=no_start, stdin=stdin, python_shell=python_shell, output_loglevel=output_loglevel, ignore_retcode=ignore_retcode, use_vt=use_vt, keep_env=keep_env)\n        else:\n            if (not chroot_fallback):\n                raise CommandExecutionError('{0} is not attachable.'.format(name))\n            rootfs = info(name, path=path).get('rootfs')\n            __context__['cmd.run_chroot.func'] = __salt__['cmd.run']\n            ret = __salt__['cmd.run_chroot'](rootfs, cmd, stdin=stdin, python_shell=python_shell, output_loglevel=output_loglevel, ignore_retcode=ignore_retcode)\n    except Exception:\n        raise\n    finally:\n        new_state = state(name, path=path)\n        if preserve_state:\n            if ((orig_state == 'stopped') and (new_state != 'stopped')):\n                stop(name, path=path)\n            elif ((orig_state == 'frozen') and (new_state != 'frozen')):\n                freeze(name, start=True, path=path)\n    if (output in (None, 'all')):\n        return ret\n    else:\n        return ret[output]\n", "label": 1}
{"function": "\n\ndef get_thumbnail(self, file_, geometry_string, **options):\n    '\\n        Returns thumbnail as an ImageFile instance for file with geometry and\\n        options given. First it will try to get it from the key value store,\\n        secondly it will create it.\\n        '\n    logger.debug(text_type('Getting thumbnail for file [%s] at [%s]'), file_, geometry_string)\n    if file_:\n        source = ImageFile(file_)\n    elif settings.THUMBNAIL_DUMMY:\n        return DummyImageFile(geometry_string)\n    else:\n        return None\n    if settings.THUMBNAIL_PRESERVE_FORMAT:\n        options.setdefault('format', self._get_format(source))\n    for (key, value) in self.default_options.items():\n        options.setdefault(key, value)\n    for (key, attr) in self.extra_options:\n        value = getattr(settings, attr)\n        if (value != getattr(default_settings, attr)):\n            options.setdefault(key, value)\n    name = self._get_thumbnail_filename(source, geometry_string, options)\n    thumbnail = ImageFile(name, default.storage)\n    cached = default.kvstore.get(thumbnail)\n    if cached:\n        return cached\n    if (settings.THUMBNAIL_FORCE_OVERWRITE or (not thumbnail.exists())):\n        try:\n            source_image = default.engine.get_image(source)\n        except IOError as e:\n            logger.exception(e)\n            if settings.THUMBNAIL_DUMMY:\n                return DummyImageFile(geometry_string)\n            else:\n                logger.warn(text_type('Remote file [%s] at [%s] does not exist'), file_, geometry_string)\n                return thumbnail\n        image_info = default.engine.get_image_info(source_image)\n        options['image_info'] = image_info\n        size = default.engine.get_image_size(source_image)\n        source.set_size(size)\n        try:\n            self._create_thumbnail(source_image, geometry_string, options, thumbnail)\n            self._create_alternative_resolutions(source_image, geometry_string, options, thumbnail.name)\n        finally:\n            default.engine.cleanup(source_image)\n    default.kvstore.get_or_set(source)\n    default.kvstore.set(thumbnail, source)\n    return thumbnail\n", "label": 1}
{"function": "\n\ndef _mp_consume(client, group, topic, message_queue, size, events, **consumer_options):\n    '\\n    A child process worker which consumes messages based on the\\n    notifications given by the controller process\\n\\n    NOTE: Ideally, this should have been a method inside the Consumer\\n    class. However, multiprocessing module has issues in windows. The\\n    functionality breaks unless this function is kept outside of a class\\n    '\n    interval = 1\n    while (not events.exit.is_set()):\n        try:\n            client.reinit()\n            consumer = SimpleConsumer(client, group, topic, auto_commit=False, auto_commit_every_n=None, auto_commit_every_t=None, **consumer_options)\n            consumer.provide_partition_info()\n            while True:\n                events.start.wait()\n                if events.exit.is_set():\n                    break\n                count = 0\n                message = consumer.get_message()\n                if message:\n                    while True:\n                        try:\n                            message_queue.put(message, timeout=FULL_QUEUE_WAIT_TIME_SECONDS)\n                            break\n                        except queue.Full:\n                            if events.exit.is_set():\n                                break\n                    count += 1\n                    if (count == size.value):\n                        events.pause.wait()\n                else:\n                    time.sleep(NO_MESSAGES_WAIT_TIME_SECONDS)\n            consumer.stop()\n        except KafkaError as e:\n            log.error(('Problem communicating with Kafka (%s), retrying in %d seconds...' % (e, interval)))\n            time.sleep(interval)\n            interval = ((interval * 2) if ((interval * 2) < MAX_BACKOFF_SECONDS) else MAX_BACKOFF_SECONDS)\n", "label": 1}
{"function": "\n\ndef float_layout(context, box, containing_block, device_size, absolute_boxes, fixed_boxes):\n    'Set the width and position of floating ``box``.'\n    from .blocks import block_container_layout\n    from .inlines import inline_replaced_box_width_height\n    resolve_percentages(box, (containing_block.width, containing_block.height))\n    resolve_position_percentages(box, (containing_block.width, containing_block.height))\n    if (box.margin_left == 'auto'):\n        box.margin_left = 0\n    if (box.margin_right == 'auto'):\n        box.margin_right = 0\n    if (box.margin_top == 'auto'):\n        box.margin_top = 0\n    if (box.margin_bottom == 'auto'):\n        box.margin_bottom = 0\n    clearance = get_clearance(context, box)\n    if (clearance is not None):\n        box.position_y += clearance\n    if isinstance(box, boxes.BlockReplacedBox):\n        inline_replaced_box_width_height(box, device_size=None)\n    elif (box.width == 'auto'):\n        float_width(box, context, containing_block)\n    if box.is_table_wrapper:\n        table_wrapper_width(context, box, (containing_block.width, containing_block.height))\n    if isinstance(box, boxes.BlockBox):\n        context.create_block_formatting_context()\n        (box, _, _, _, _) = block_container_layout(context, box, max_position_y=float('inf'), skip_stack=None, device_size=device_size, page_is_empty=False, absolute_boxes=absolute_boxes, fixed_boxes=fixed_boxes, adjoining_margins=None)\n        list_marker_layout(context, box)\n        context.finish_block_formatting_context(box)\n    else:\n        assert isinstance(box, boxes.BlockReplacedBox)\n    box = find_float_position(context, box, containing_block)\n    context.excluded_shapes.append(box)\n    return box\n", "label": 1}
{"function": "\n\ndef _execute_multi_gen_task(self, gen, executor, task):\n    unfinished = set((executor.submit(t) for t in task.tasks))\n    while unfinished:\n        if (not task.wait(executor, unfinished, self.engine.pool_timeout)):\n            self.engine.update_gui()\n        done = set((f for f in unfinished if f.done()))\n        for f in done:\n            try:\n                result = f.result()\n            except Exception:\n                if (not task.skip_errors):\n                    raise\n            else:\n                (yield result)\n        unfinished.difference_update(done)\n", "label": 1}
{"function": "\n\ndef get_aggregation(self, using):\n    '\\n        Returns the dictionary with the values of the existing aggregations.\\n        '\n    if (not self.aggregate_select):\n        return {\n            \n        }\n    if (self.group_by is not None):\n        from django.db.models.sql.subqueries import AggregateQuery\n        query = AggregateQuery(self.model)\n        obj = self.clone()\n        for (alias, aggregate) in self.aggregate_select.items():\n            if aggregate.is_summary:\n                query.aggregate_select[alias] = aggregate\n                del obj.aggregate_select[alias]\n        try:\n            query.add_subquery(obj, using)\n        except EmptyResultSet:\n            return dict(((alias, None) for alias in query.aggregate_select))\n    else:\n        query = self\n        self.select = []\n        self.default_cols = False\n        self.extra = {\n            \n        }\n        self.remove_inherited_models()\n    query.clear_ordering(True)\n    query.clear_limits()\n    query.select_for_update = False\n    query.select_related = False\n    query.related_select_cols = []\n    query.related_select_fields = []\n    result = query.get_compiler(using).execute_sql(SINGLE)\n    if (result is None):\n        result = [None for q in query.aggregate_select.items()]\n    return dict([(alias, self.resolve_aggregate(val, aggregate, connection=connections[using])) for ((alias, aggregate), val) in zip(query.aggregate_select.items(), result)])\n", "label": 1}
{"function": "\n\ndef rs_sin(p, x, prec):\n    \"\\n    Sine of a series\\n\\n    Return the series expansion of the sin of ``p``, about 0.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import QQ\\n    >>> from sympy.polys.rings import ring\\n    >>> from sympy.polys.ring_series import rs_sin\\n    >>> R, x, y = ring('x, y', QQ)\\n    >>> rs_sin(x + x*y, x, 4)\\n    -1/6*x**3*y**3 - 1/2*x**3*y**2 - 1/2*x**3*y - 1/6*x**3 + x*y + x\\n    >>> rs_sin(x**QQ(3, 2) + x*y**QQ(7, 5), x, 4)\\n    -1/2*x**(7/2)*y**(14/5) - 1/6*x**3*y**(21/5) + x**(3/2) + x*y**(7/5)\\n\\n    See Also\\n    ========\\n\\n    sin\\n    \"\n    if rs_is_puiseux(p, x):\n        return rs_puiseux(rs_sin, p, x, prec)\n    R = x.ring\n    if (not p):\n        return R(0)\n    c = _get_constant_term(p, x)\n    if c:\n        if (R.domain is EX):\n            c_expr = c.as_expr()\n            (t1, t2) = (sin(c_expr), cos(c_expr))\n        elif isinstance(c, PolyElement):\n            try:\n                c_expr = c.as_expr()\n                (t1, t2) = (R(sin(c_expr)), R(cos(c_expr)))\n            except ValueError:\n                R = R.add_gens([sin(c_expr), cos(c_expr)])\n                p = p.set_ring(R)\n                x = x.set_ring(R)\n                c = c.set_ring(R)\n                (t1, t2) = (R(sin(c_expr)), R(cos(c_expr)))\n        else:\n            try:\n                (t1, t2) = (R(sin(c)), R(cos(c)))\n            except ValueError:\n                raise DomainError(\"The given series can't be expanded in this domain.\")\n        p1 = (p - c)\n        return ((rs_sin(p1, x, prec) * t2) + (rs_cos(p1, x, prec) * t1))\n    if ((len(p) > 20) and (R.ngens == 1)):\n        t = rs_tan((p / 2), x, prec)\n        t2 = rs_square(t, x, prec)\n        p1 = rs_series_inversion((1 + t2), x, prec)\n        return rs_mul(p1, (2 * t), x, prec)\n    one = R(1)\n    n = 1\n    c = [0]\n    for k in range(2, (prec + 2), 2):\n        c.append((one / n))\n        c.append(0)\n        n *= ((- k) * (k + 1))\n    return rs_series_from_list(p, c, x, prec)\n", "label": 1}
{"function": "\n\ndef load(self, formatter=None):\n    try:\n        fd = open(self.filepath, 'r')\n    except IOError:\n        if (self.strict is True):\n            raise\n        else:\n            return\n    (_, file_extension) = os.path.splitext(self.filepath)\n    if (file_extension.lower() in JSON_EXTENSIONS):\n        import json\n        self.data = dict(((self.format(k, formatter), v) for (k, v) in json.load(fd).items()))\n    elif (file_extension.lower() in YAML_EXTENSIONS):\n        from yaml import load as yload, dump as ydump\n        try:\n            from yaml import CLoader as Loader\n        except ImportError:\n            from yaml import Loader\n        self.data = dict(((self.format(k, formatter), v) for (k, v) in yload(fd, Loader=Loader).items()))\n    elif (file_extension.lower() in PYTHON_EXTENSIONS):\n        mod = imp.load_source('mod', self.filepath)\n        self.data = dict(((self.format(k, formatter), v) for (k, v) in vars(mod).items() if k.isupper()))\n    else:\n        raise ValueError('Unhandled file extension {0}'.format(file_extension))\n    fd.close()\n", "label": 1}
{"function": "\n\ndef initialize_connection(self):\n    'Initialize a socket to a Chromecast, retrying as necessary.'\n    tries = self.tries\n    if (self.socket is not None):\n        self.socket.close()\n        self.socket = None\n    for callback in self._request_callbacks.values():\n        callback['event'].set()\n    self.app_namespaces = []\n    self.destination_id = None\n    self.session_id = None\n    self._request_id = 0\n    self._request_callbacks = {\n        \n    }\n    self._open_channels = []\n    self.connecting = True\n    retry_log_fun = self.logger.exception\n    while ((not self.stop.is_set()) and ((tries is None) or (tries > 0))):\n        try:\n            self.socket = ssl.wrap_socket(socket.socket())\n            self.socket.settimeout(self.timeout)\n            self._report_connection_status(ConnectionStatus(CONNECTION_STATUS_CONNECTING, NetworkAddress(self.host, self.port)))\n            self.socket.connect((self.host, self.port))\n            self.connecting = False\n            self._force_recon = False\n            self._report_connection_status(ConnectionStatus(CONNECTION_STATUS_CONNECTED, NetworkAddress(self.host, self.port)))\n            self.receiver_controller.update_status()\n            self.heartbeat_controller.ping()\n            self.heartbeat_controller.reset()\n            self.logger.debug('Connected!')\n            break\n        except socket.error:\n            self.connecting = True\n            if self.stop.is_set():\n                self.logger.exception('Failed to connect, aborting due to stop signal.')\n                raise ChromecastConnectionError('Failed to connect')\n            self._report_connection_status(ConnectionStatus(CONNECTION_STATUS_FAILED, NetworkAddress(self.host, self.port)))\n            retry_log_fun('Failed to connect, retrying in %fs', self.retry_wait)\n            retry_log_fun = self.logger.debug\n            time.sleep(self.retry_wait)\n            if tries:\n                tries -= 1\n    else:\n        self.stop.set()\n        self.logger.error('Failed to connect. No retries.')\n        raise ChromecastConnectionError('Failed to connect')\n", "label": 1}
{"function": "\n\ndef file_update_many(fh, points):\n    if LOCK:\n        fcntl.flock(fh.fileno(), fcntl.LOCK_EX)\n    header = __readHeader(fh)\n    now = int(time.time())\n    archives = iter(header['archives'])\n    currentArchive = next(archives)\n    currentPoints = []\n    for point in points:\n        age = (now - point[0])\n        while (currentArchive['retention'] < age):\n            if currentPoints:\n                currentPoints.reverse()\n                __archive_update_many(fh, header, currentArchive, currentPoints)\n                currentPoints = []\n            try:\n                currentArchive = next(archives)\n            except StopIteration:\n                currentArchive = None\n                break\n        if (not currentArchive):\n            break\n        currentPoints.append(point)\n    if (currentArchive and currentPoints):\n        currentPoints.reverse()\n        __archive_update_many(fh, header, currentArchive, currentPoints)\n    if AUTOFLUSH:\n        fh.flush()\n        os.fsync(fh.fileno())\n", "label": 1}
{"function": "\n\ndef test_exact_sqrts():\n    for i in range(20000):\n        assert (sqrt(mpf((i * i))) == i)\n    random.seed(1)\n    for prec in [100, 300, 1000, 10000]:\n        mp.dps = prec\n        for i in range(20):\n            A = random.randint((10 ** ((prec // 2) - 2)), (10 ** ((prec // 2) - 1)))\n            assert (sqrt(mpf((A * A))) == A)\n    mp.dps = 15\n    for i in range(100):\n        for a in [1, 8, 25, 112307]:\n            assert (sqrt(mpf(((a * a), (2 * i)))) == mpf((a, i)))\n            assert (sqrt(mpf(((a * a), ((- 2) * i)))) == mpf((a, (- i))))\n", "label": 1}
{"function": "\n\ndef _fetch_remotes(remotes, prune):\n    'Fetch a list of remotes, displaying progress info along the way.'\n\n    def _get_name(ref):\n        'Return the local name of a remote or tag reference.'\n        return (ref.remote_head if isinstance(ref, RemoteRef) else ref.name)\n    info = [('NEW_HEAD', 'new branch', 'new branches'), ('NEW_TAG', 'new tag', 'new tags'), ('FAST_FORWARD', 'branch update', 'branch updates')]\n    up_to_date = ((BLUE + 'up to date') + RESET)\n    for remote in remotes:\n        print(INDENT2, 'Fetching', (BOLD + remote.name), end='')\n        if (not remote.config_reader.has_option('fetch')):\n            print(':', (YELLOW + 'skipped:'), 'no configured refspec.')\n            continue\n        try:\n            results = remote.fetch(progress=_ProgressMonitor(), prune=prune)\n        except exc.GitCommandError as err:\n            msg = err.command[0].replace('Error when fetching: ', '')\n            if (not msg.endswith('.')):\n                msg += '.'\n            print(':', (RED + 'error:'), msg)\n            return\n        except AssertionError:\n            print(':', (RED + 'error:'), 'something went wrong in GitPython,', 'but the fetch might have been successful.')\n            return\n        rlist = []\n        for (attr, singular, plural) in info:\n            names = [_get_name(res.ref) for res in results if (res.flags & getattr(res, attr))]\n            if names:\n                desc = (singular if (len(names) == 1) else plural)\n                colored = ((GREEN + desc) + RESET)\n                rlist.append('{0} ({1})'.format(colored, ', '.join(names)))\n        print(':', ((', '.join(rlist) if rlist else up_to_date) + '.'))\n", "label": 1}
{"function": "\n\ndef conv3d(x, kernel, strides=(1, 1, 1), border_mode='valid', dim_ordering='th', volume_shape=None, filter_shape=None):\n    '\\n    Run on cuDNN if available.\\n    border_mode: string, \"same\" or \"valid\".\\n    '\n    if (dim_ordering not in {'th', 'tf'}):\n        raise Exception(('Unknown dim_ordering ' + str(dim_ordering)))\n    if (border_mode not in {'same', 'valid'}):\n        raise Exception(('Invalid border mode: ' + str(border_mode)))\n    if (dim_ordering == 'tf'):\n        x = x.dimshuffle((0, 4, 1, 2, 3))\n        kernel = kernel.dimshuffle((4, 3, 0, 1, 2))\n        if volume_shape:\n            volume_shape = (volume_shape[0], volume_shape[4], volume_shape[1], volume_shape[2], volume_shape[3])\n        if filter_shape:\n            filter_shape = (filter_shape[4], filter_shape[3], filter_shape[0], filter_shape[1], filter_shape[2])\n    if (border_mode == 'same'):\n        assert (strides == (1, 1, 1))\n        pad_dim1 = (kernel.shape[2] - 1)\n        pad_dim2 = (kernel.shape[3] - 1)\n        pad_dim3 = (kernel.shape[4] - 1)\n        output_shape = (x.shape[0], x.shape[1], (x.shape[2] + pad_dim1), (x.shape[3] + pad_dim2), (x.shape[4] + pad_dim3))\n        output = T.zeros(output_shape)\n        indices = (slice(None), slice(None), slice((pad_dim1 // 2), (x.shape[2] + (pad_dim1 // 2))), slice((pad_dim2 // 2), (x.shape[3] + (pad_dim2 // 2))), slice((pad_dim3 // 2), (x.shape[4] + (pad_dim3 // 2))))\n        x = T.set_subtensor(output[indices], x)\n        border_mode = 'valid'\n    border_mode_3d = (border_mode, border_mode, border_mode)\n    conv_out = conv3d2d.conv3d(signals=x.dimshuffle(0, 2, 1, 3, 4), filters=kernel.dimshuffle(0, 2, 1, 3, 4), border_mode=border_mode_3d)\n    conv_out = conv_out.dimshuffle(0, 2, 1, 3, 4)\n    if (strides != (1, 1, 1)):\n        conv_out = conv_out[:, :, ::strides[0], ::strides[1], ::strides[2]]\n    if (dim_ordering == 'tf'):\n        conv_out = conv_out.dimshuffle((0, 2, 3, 4, 1))\n    return conv_out\n", "label": 1}
{"function": "\n\ndef mksls(src, dst=None):\n    '\\n    Convert a preseed file to an SLS file\\n    '\n    ps_opts = {\n        \n    }\n    with salt.utils.fopen(src, 'r') as fh_:\n        for line in fh_:\n            if line.startswith('#'):\n                continue\n            if (not line.strip()):\n                continue\n            comps = shlex.split(line)\n            if (comps[0] not in ps_opts.keys()):\n                ps_opts[comps[0]] = {\n                    \n                }\n            cmds = comps[1].split('/')\n            pointer = ps_opts[comps[0]]\n            for cmd in cmds:\n                pointer = pointer.setdefault(cmd, {\n                    \n                })\n            pointer['type'] = comps[2]\n            if (len(comps) > 3):\n                pointer['argument'] = comps[3]\n    sls = {\n        \n    }\n    sls[ps_opts['d-i']['languagechooser']['language-name-fb']['argument']] = {\n        'locale': ['system'],\n    }\n    sls[ps_opts['d-i']['kbd-chooser']['method']['argument']] = {\n        'keyboard': ['system'],\n    }\n    timezone = ps_opts['d-i']['time']['zone']['argument']\n    sls[timezone] = {\n        'timezone': ['system'],\n    }\n    if (ps_opts['d-i']['tzconfig']['gmt']['argument'] == 'true'):\n        sls[timezone]['timezone'].append('utc')\n    if ('netcfg' in ps_opts['d-i'].keys()):\n        iface = ps_opts['d-i']['netcfg']['choose_interface']['argument']\n        sls[iface] = {\n            \n        }\n        sls[iface]['enabled'] = True\n        if (ps_opts['d-i']['netcfg']['confirm_static'] == 'true'):\n            sls[iface]['proto'] = 'static'\n        elif (ps_opts['d-i']['netcfg']['disable_dhcp'] == 'false'):\n            sls[iface]['proto'] = 'dhcp'\n        sls[iface]['netmask'] = ps_opts['d-i']['netcfg']['get_netmask']['argument']\n        sls[iface]['domain'] = ps_opts['d-i']['netcfg']['get_domain']['argument']\n        sls[iface]['gateway'] = ps_opts['d-i']['netcfg']['get_gateway']['argument']\n        sls[iface]['hostname'] = ps_opts['d-i']['netcfg']['get_hostname']['argument']\n        sls[iface]['ipaddress'] = ps_opts['d-i']['netcfg']['get_ipaddress']['argument']\n        sls[iface]['nameservers'] = ps_opts['d-i']['netcfg']['get_nameservers']['argument']\n    if (dst is not None):\n        with salt.utils.fopen(dst, 'w') as fh_:\n            fh_.write(yaml.safe_dump(sls, default_flow_style=False))\n    else:\n        return yaml.safe_dump(sls, default_flow_style=False)\n", "label": 1}
{"function": "\n\ndef test_content():\n    for f in formats:\n        try:\n            raise_error()\n        except:\n            result = format(f)\n            print(result)\n            assert ('test_object' in result)\n            assert ('http://whatever.com' in result)\n            assert ('This is some supplemental information' in result)\n            assert ('raise_error' in result)\n            assert ('call_error' in result)\n            assert ('5' in result)\n            assert ('test_content' in result)\n        else:\n            assert 0\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse(wmsg):\n    '\\n        Verifies and parses an unserialized raw message into an actual WAMP message instance.\\n\\n        :param wmsg: The unserialized raw message.\\n        :type wmsg: list\\n\\n        :returns: An instance of this class.\\n        '\n    assert ((len(wmsg) > 0) and (wmsg[0] == Register.MESSAGE_TYPE))\n    if (len(wmsg) != 4):\n        raise ProtocolError('invalid message length {0} for REGISTER'.format(len(wmsg)))\n    request = check_or_raise_id(wmsg[1], \"'request' in REGISTER\")\n    options = check_or_raise_extra(wmsg[2], \"'options' in REGISTER\")\n    match = Register.MATCH_EXACT\n    invoke = Register.INVOKE_SINGLE\n    if ('match' in options):\n        option_match = options['match']\n        if (type(option_match) != six.text_type):\n            raise ProtocolError(\"invalid type {0} for 'match' option in REGISTER\".format(type(option_match)))\n        if (option_match not in [Register.MATCH_EXACT, Register.MATCH_PREFIX, Register.MATCH_WILDCARD]):\n            raise ProtocolError(\"invalid value {0} for 'match' option in REGISTER\".format(option_match))\n        match = option_match\n    if (match == Register.MATCH_EXACT):\n        allow_empty_components = False\n        allow_last_empty = False\n    elif (match == Register.MATCH_PREFIX):\n        allow_empty_components = False\n        allow_last_empty = True\n    elif (match == Register.MATCH_WILDCARD):\n        allow_empty_components = True\n        allow_last_empty = False\n    else:\n        raise Exception('logic error')\n    procedure = check_or_raise_uri(wmsg[3], \"'procedure' in REGISTER\", allow_empty_components=allow_empty_components, allow_last_empty=allow_last_empty)\n    if ('invoke' in options):\n        option_invoke = options['invoke']\n        if (type(option_invoke) != six.text_type):\n            raise ProtocolError(\"invalid type {0} for 'invoke' option in REGISTER\".format(type(option_invoke)))\n        if (option_invoke not in [Register.INVOKE_SINGLE, Register.INVOKE_FIRST, Register.INVOKE_LAST, Register.INVOKE_ROUNDROBIN, Register.INVOKE_RANDOM]):\n            raise ProtocolError(\"invalid value {0} for 'invoke' option in REGISTER\".format(option_invoke))\n        invoke = option_invoke\n    obj = Register(request, procedure, match=match, invoke=invoke)\n    return obj\n", "label": 1}
{"function": "\n\ndef describe(Bucket, region=None, key=None, keyid=None, profile=None):\n    '\\n    Given a bucket name describe its properties.\\n\\n    Returns a dictionary of interesting properties.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt myminion boto_s3_bucket.describe mybucket\\n\\n    '\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        result = {\n            \n        }\n        for (key, query) in {\n            'ACL': conn.get_bucket_acl,\n            'CORS': conn.get_bucket_cors,\n            'LifecycleConfiguration': conn.get_bucket_lifecycle_configuration,\n            'Location': conn.get_bucket_location,\n            'Logging': conn.get_bucket_logging,\n            'NotificationConfiguration': conn.get_bucket_notification_configuration,\n            'Policy': conn.get_bucket_policy,\n            'Replication': conn.get_bucket_replication,\n            'RequestPayment': conn.get_bucket_request_payment,\n            'Versioning': conn.get_bucket_versioning,\n            'Website': conn.get_bucket_website,\n        }.iteritems():\n            try:\n                data = query(Bucket=Bucket)\n            except ClientError as e:\n                if (e.response.get('Error', {\n                    \n                }).get('Code') in ('NoSuchLifecycleConfiguration', 'NoSuchCORSConfiguration', 'NoSuchBucketPolicy', 'NoSuchWebsiteConfiguration', 'ReplicationConfigurationNotFoundError', 'NoSuchTagSet')):\n                    continue\n                raise\n            if ('ResponseMetadata' in data):\n                del data['ResponseMetadata']\n            result[key] = data\n        tags = {\n            \n        }\n        try:\n            data = conn.get_bucket_tagging(Bucket=Bucket)\n            for tagdef in data.get('TagSet'):\n                tags[tagdef.get('Key')] = tagdef.get('Value')\n        except ClientError as e:\n            if (not (e.response.get('Error', {\n                \n            }).get('Code') == 'NoSuchTagSet')):\n                raise\n        if tags:\n            result['Tagging'] = tags\n        return {\n            'bucket': result,\n        }\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if (e.response.get('Error', {\n            \n        }).get('Code') == 'NoSuchBucket'):\n            return {\n                'bucket': None,\n            }\n        return {\n            'error': __utils__['boto3.get_error'](e),\n        }\n", "label": 1}
{"function": "\n\ndef base_definition_post_save(sender, instance, created, raw, **kwargs):\n    declared_fields = instance.get_declared_fields()\n    if declared_fields:\n        model_class = instance.model_def.model_class().render_state()\n        opts = model_class._meta\n        if created:\n            add_columns = popattr(instance._state, '_add_columns', True)\n            if add_columns:\n                auto_pk = isinstance(opts.pk, models.AutoField)\n                for field in declared_fields:\n                    field.model = model_class\n                    remote_field = get_remote_field(field)\n                    if (auto_pk and remote_field and remote_field.parent_link):\n                        auto_pk = False\n                        field.primary_key = True\n                        perform_ddl('alter_field', model_class, opts.pk, field, strict=True)\n                    else:\n                        perform_ddl('add_field', model_class, field)\n        else:\n            for field in declared_fields:\n                try:\n                    old_field = opts.get_field(field.name)\n                except FieldDoesNotExist:\n                    perform_ddl('add_field', model_class, field)\n                else:\n                    perform_ddl('alter_field', model_class, old_field, field, strict=True)\n", "label": 1}
{"function": "\n\ndef take_using_weights(items, weights):\n    \"\\n    Generator that keeps yielding items from the items list, in proportion to\\n    their weight. For instance::\\n\\n        # Getting the first 70 items from this generator should have yielded 10\\n        # times A, 20 times B and 40 times C, all distributed equally..\\n        take_using_weights(['A', 'B', 'C'], [5, 10, 20])\\n\\n    :param items: List of items to take from.\\n    :param weights: Integers representing the weight. (Numbers have to be\\n                    integers, not floats.)\\n    \"\n    assert isinstance(items, list)\n    assert isinstance(weights, list)\n    assert all((isinstance(i, int) for i in weights))\n    assert (len(items) == len(weights))\n    assert (len(items) > 0)\n    already_taken = [0 for i in items]\n    item_count = len(items)\n    max_weight = max(weights)\n    i = 0\n    while True:\n        adding = True\n        while adding:\n            adding = False\n            for (item_i, item, weight) in zip(range(item_count), items, weights):\n                if (already_taken[item_i] < ((i * weight) / float(max_weight))):\n                    (yield item)\n                    already_taken[item_i] += 1\n                    adding = True\n        i += 1\n", "label": 1}
{"function": "\n\ndef __call__(self, clf, X, y, sample_weight=None):\n    'Evaluate decision function output for X relative to y_true.\\n\\n        Parameters\\n        ----------\\n        clf : object\\n            Trained classifier to use for scoring. Must have either a\\n            decision_function method or a predict_proba method; the output of\\n            that is used to compute the score.\\n\\n        X : array-like or sparse matrix\\n            Test data that will be fed to clf.decision_function or\\n            clf.predict_proba.\\n\\n        y : array-like\\n            Gold standard target values for X. These must be class labels,\\n            not decision function values.\\n\\n        sample_weight : array-like, optional (default=None)\\n            Sample weights.\\n\\n        Returns\\n        -------\\n        score : float\\n            Score function applied to prediction of estimator on X.\\n        '\n    y_type = type_of_target(y)\n    if (y_type not in ('binary', 'multilabel-indicator')):\n        raise ValueError('{0} format is not supported'.format(y_type))\n    if is_regressor(clf):\n        y_pred = clf.predict(X)\n    else:\n        try:\n            y_pred = clf.decision_function(X)\n            if isinstance(y_pred, list):\n                y_pred = np.vstack((p for p in y_pred)).T\n        except (NotImplementedError, AttributeError):\n            y_pred = clf.predict_proba(X)\n            if (y_type == 'binary'):\n                y_pred = y_pred[:, 1]\n            elif isinstance(y_pred, list):\n                y_pred = np.vstack([p[:, (- 1)] for p in y_pred]).T\n    if (sample_weight is not None):\n        return (self._sign * self._score_func(y, y_pred, sample_weight=sample_weight, **self._kwargs))\n    else:\n        return (self._sign * self._score_func(y, y_pred, **self._kwargs))\n", "label": 1}
{"function": "\n\ndef __init__(self, arg1, shape=None, dtype=None, copy=False):\n    _data_matrix.__init__(self)\n    if isinstance(arg1, tuple):\n        if isshape(arg1):\n            (M, N) = arg1\n            self.shape = (M, N)\n            idx_dtype = get_index_dtype(maxval=max(M, N))\n            self.row = np.array([], dtype=idx_dtype)\n            self.col = np.array([], dtype=idx_dtype)\n            self.data = np.array([], getdtype(dtype, default=float))\n            self.has_canonical_format = True\n        else:\n            try:\n                (obj, (row, col)) = arg1\n            except (TypeError, ValueError):\n                raise TypeError('invalid input format')\n            if (shape is None):\n                if ((len(row) == 0) or (len(col) == 0)):\n                    raise ValueError('cannot infer dimensions from zero sized index arrays')\n                M = (np.max(row) + 1)\n                N = (np.max(col) + 1)\n                self.shape = (M, N)\n            else:\n                (M, N) = shape\n                self.shape = (M, N)\n            idx_dtype = get_index_dtype(maxval=max(self.shape))\n            self.row = np.array(row, copy=copy, dtype=idx_dtype)\n            self.col = np.array(col, copy=copy, dtype=idx_dtype)\n            self.data = np.array(obj, copy=copy)\n            self.has_canonical_format = False\n    elif isspmatrix(arg1):\n        if (isspmatrix_coo(arg1) and copy):\n            self.row = arg1.row.copy()\n            self.col = arg1.col.copy()\n            self.data = arg1.data.copy()\n            self.shape = arg1.shape\n        else:\n            coo = arg1.tocoo()\n            self.row = coo.row\n            self.col = coo.col\n            self.data = coo.data\n            self.shape = coo.shape\n        self.has_canonical_format = False\n    else:\n        M = np.atleast_2d(np.asarray(arg1))\n        if (M.ndim != 2):\n            raise TypeError('expected dimension <= 2 array or matrix')\n        else:\n            self.shape = M.shape\n        (self.row, self.col) = M.nonzero()\n        self.data = M[(self.row, self.col)]\n        self.has_canonical_format = True\n    if (dtype is not None):\n        self.data = self.data.astype(dtype, copy=False)\n    self._check()\n", "label": 1}
{"function": "\n\ndef _restore_reduce(self, obj):\n    '\\n        Supports restoring with all elements of __reduce__ as per pep 307.\\n        Assumes that iterator items (the last two) are represented as lists\\n        as per pickler implementation.\\n        '\n    reduce_val = obj[tags.REDUCE]\n    (f, args, state, listitems, dictitems) = map(self._restore, reduce_val)\n    if ((f == tags.NEWOBJ) or (f.__name__ == '__newobj__')):\n        cls = args[0]\n        stage1 = cls.__new__(cls, *args[1:])\n    else:\n        stage1 = f(*args)\n    if state:\n        try:\n            stage1.__setstate__(state)\n        except AttributeError:\n            try:\n                stage1.__dict__.update(state)\n            except AttributeError:\n                for (k, v) in state.items():\n                    setattr(stage1, k, v)\n    if listitems:\n        try:\n            stage1.extend(listitems)\n        except AttributeError:\n            for x in listitems:\n                stage1.append(x)\n    if dictitems:\n        for (k, v) in dictitems:\n            stage1.__setitem__(k, v)\n    self._mkref(stage1)\n    return stage1\n", "label": 1}
{"function": "\n\ndef _ProcessCurlRequests(multi, requests):\n    'cURL request processor.\\n\\n  This generator yields a tuple once for every completed request, successful or\\n  not. The first value in the tuple is the handle, the second an error message\\n  or C{None} for successful requests.\\n\\n  @type multi: C{pycurl.CurlMulti}\\n  @param multi: cURL multi object\\n  @type requests: sequence\\n  @param requests: cURL request handles\\n\\n  '\n    for curl in requests:\n        multi.add_handle(curl)\n    while True:\n        (ret, active) = multi.perform()\n        assert (ret in (pycurl.E_MULTI_OK, pycurl.E_CALL_MULTI_PERFORM))\n        if (ret == pycurl.E_CALL_MULTI_PERFORM):\n            continue\n        while True:\n            (remaining_messages, successful, failed) = multi.info_read()\n            for curl in successful:\n                multi.remove_handle(curl)\n                (yield (curl, None))\n            for (curl, errnum, errmsg) in failed:\n                multi.remove_handle(curl)\n                (yield (curl, ('Error %s: %s' % (errnum, errmsg))))\n            if (remaining_messages == 0):\n                break\n        if (active == 0):\n            break\n        multi.select(1.0)\n", "label": 1}
{"function": "\n\ndef _process_scenario(data, pos):\n    main_area = charts.MainStackedAreaChart(data['info'])\n    main_hist = charts.MainHistogramChart(data['info'])\n    main_stat = charts.MainStatsTable(data['info'])\n    load_profile = charts.LoadProfileChart(data['info'])\n    atomic_pie = charts.AtomicAvgChart(data['info'])\n    atomic_area = charts.AtomicStackedAreaChart(data['info'])\n    atomic_hist = charts.AtomicHistogramChart(data['info'])\n    errors = []\n    output_errors = []\n    additive_output_charts = []\n    complete_output = []\n    for (idx, itr) in enumerate(data['iterations']):\n        if itr['error']:\n            (typ, msg, trace) = itr['error']\n            errors.append({\n                'iteration': idx,\n                'type': typ,\n                'message': msg,\n                'traceback': trace,\n            })\n        for (i, additive) in enumerate(itr['output']['additive']):\n            try:\n                additive_output_charts[i].add_iteration(additive['data'])\n            except IndexError:\n                chart_cls = plugin.Plugin.get(additive['chart_plugin'])\n                chart = chart_cls(data['info'], title=additive['title'], description=additive.get('description', ''), label=additive.get('label', ''), axis_label=additive.get('axis_label', 'Iteration sequence number'))\n                chart.add_iteration(additive['data'])\n                additive_output_charts.append(chart)\n        complete_charts = []\n        for complete in itr['output']['complete']:\n            complete_chart = dict(complete)\n            chart_cls = plugin.Plugin.get(complete_chart.pop('chart_plugin'))\n            complete_chart['widget'] = chart_cls.widget\n            complete_charts.append(complete_chart)\n        complete_output.append(complete_charts)\n        for chart in (main_area, main_hist, main_stat, load_profile, atomic_pie, atomic_area, atomic_hist):\n            chart.add_iteration(itr)\n    kw = data['key']['kw']\n    (cls, method) = data['key']['name'].split('.')\n    additive_output = [chart.render() for chart in additive_output_charts]\n    iterations_count = data['info']['iterations_count']\n    return {\n        'cls': cls,\n        'met': method,\n        'pos': str(pos),\n        'name': (method + ((pos and (' [%d]' % (pos + 1))) or '')),\n        'runner': kw['runner']['type'],\n        'config': json.dumps({\n            data['key']['name']: [kw],\n        }, indent=2),\n        'iterations': {\n            'iter': main_area.render(),\n            'pie': [('success', (data['info']['iterations_count'] - len(errors))), ('errors', len(errors))],\n            'histogram': main_hist.render(),\n        },\n        'load_profile': load_profile.render(),\n        'atomic': {\n            'histogram': atomic_hist.render(),\n            'iter': atomic_area.render(),\n            'pie': atomic_pie.render(),\n        },\n        'table': main_stat.render(),\n        'additive_output': additive_output,\n        'complete_output': complete_output,\n        'output_errors': output_errors,\n        'errors': errors,\n        'load_duration': data['info']['load_duration'],\n        'full_duration': data['info']['full_duration'],\n        'sla': data['sla'],\n        'sla_success': all([s['success'] for s in data['sla']]),\n        'iterations_count': iterations_count,\n    }\n", "label": 1}
{"function": "\n\ndef connect(self, receiver, sender=ANY, weak=True):\n    'Connect *receiver* to signal events send by *sender*.\\n\\n        :param receiver: A callable.  Will be invoked by :meth:`send`.\\n          Will be invoked with `sender=` as a named argument and any\\n          \\\\*\\\\*kwargs that were provided to a call to :meth:`send`.\\n\\n        :param sender: Any object or :attr:`Signal.ANY`.  Restricts\\n          notifications to *receiver* to only those :meth:`send`\\n          emissions sent by *sender*.  If ``ANY``, the receiver will\\n          always be notified.  A *receiver* may be connected to\\n          multiple *sender* on the same Signal.  Defaults to ``ANY``.\\n\\n        :param weak: If true, the Signal will hold a weakref to\\n          *receiver* and automatically disconnect when *receiver* goes\\n          out of scope or is garbage collected.  Defaults to True.\\n\\n        '\n    receiver_id = _hashable_identity(receiver)\n    if weak:\n        receiver_ref = weakrefs.reference(receiver, self._cleanup_receiver)\n        receiver_ref.receiver_id = receiver_id\n    else:\n        receiver_ref = receiver\n    sender_id = (ANY_ID if (sender is ANY) else _hashable_identity(sender))\n    self.receivers.setdefault(receiver_id, receiver_ref)\n    self._by_sender[sender_id].add(receiver_id)\n    self._by_receiver[receiver_id].add(sender_id)\n    del receiver_ref\n    if ((sender is not ANY) and (sender_id not in self._weak_senders)):\n        try:\n            sender_ref = weakrefs.reference(sender, self._cleanup_sender)\n            sender_ref.sender_id = sender_id\n        except TypeError:\n            pass\n        else:\n            self._weak_senders.setdefault(sender_id, sender_ref)\n            del sender_ref\n    if (receiver_connected.receivers and (self is not receiver_connected)):\n        try:\n            receiver_connected.send(self, receiver_arg=receiver, sender_arg=sender, weak_arg=weak)\n        except:\n            self.disconnect(receiver, sender)\n            raise\n    return receiver\n", "label": 1}
{"function": "\n\ndef _parse_param_line(self, line):\n    ' Parse a single param line. '\n    value = line.strip('\\n \\t')\n    if (len(value) > 0):\n        i = Input()\n        if (value.find('#') != (- 1)):\n            (value, extra_attributes) = value.split('#')\n            try:\n                extra_attributes = eval(extra_attributes)\n            except SyntaxError:\n                raise InputException('Incorrectly formatted input for {0}!'.format(value))\n            if (not isinstance(extra_attributes, dict)):\n                raise InputException('Incorrectly formatted input for {0}!'.format(value))\n            if ('prompt' in extra_attributes):\n                i.prompt = extra_attributes['prompt']\n            if ('help' in extra_attributes):\n                i.help = extra_attributes['help']\n            if ('type' in extra_attributes):\n                i.in_type = extra_attributes['type']\n                if (i.in_type.find('/') != (- 1)):\n                    (i.in_type, i.out_type) = i.in_type.split('/')\n            if ('cast' in extra_attributes):\n                i.out_type = extra_attributes['cast']\n        if (value.find('==') != (- 1)):\n            (value, default) = value.split('==')\n            i.default = default\n        if value.endswith('?'):\n            value = value[:(- 1)]\n            i.is_secret = True\n        return (value, i)\n    return None\n", "label": 1}
{"function": "\n\ndef get_fn_name(self, expr, compiler_kwargs={\n    \n}, attributes=[], inline=True):\n    if (expr.__class__ is TypedFn):\n        fn = expr\n    elif (expr.__class__ is Closure):\n        fn = expr.fn\n    else:\n        assert isinstance(expr.type, (FnT, ClosureT)), ('Expected function or closure, got %s : %s' % (expr, expr.type))\n        fn = expr.type.fn\n    compiler = self.__class__(module_entry=False, **compiler_kwargs)\n    compiled = compiler.compile_flat_source(fn, attributes=attributes, inline=inline)\n    if (compiled.sig not in self.extra_function_signatures):\n        for decl in compiled.declarations:\n            self.add_decl(decl)\n        self.extra_objects.update(compiled.extra_objects)\n        for extra_sig in compiled.extra_function_signatures:\n            if (extra_sig not in self.extra_function_signatures):\n                self.extra_function_signatures.append(extra_sig)\n                self.extra_functions[extra_sig] = compiled.extra_functions[extra_sig]\n        self.extra_function_signatures.append(compiled.sig)\n        self.extra_functions[compiled.sig] = compiled.src\n    for link_flag in compiler.extra_link_flags:\n        if (link_flag not in self.extra_link_flags):\n            self.extra_link_flags.append(link_flag)\n    for compile_flag in compiler.extra_compile_flags:\n        if (compile_flag not in self.extra_compile_flags):\n            self.extra_compile_flags.append(compile_flag)\n    return compiled.name\n", "label": 1}
{"function": "\n\ndef _load_manifest_interpret_source(manifest, source, username=None, password=None, verify_certificate=True, do_inherit=True):\n    ' Interpret the <source>, and load the results into <manifest> '\n    try:\n        if isinstance(source, string_types):\n            if source.startswith('http'):\n                _load_manifest_from_url(manifest, source, verify_certificate=verify_certificate, username=username, password=password)\n            else:\n                _load_manifest_from_file(manifest, source)\n            if (not manifest.has_option('config', 'source')):\n                manifest.set('config', 'source', str(source))\n        else:\n            manifest.readfp(source)\n        if (manifest.has_option('config', 'extends') and do_inherit):\n            parent_manifest = configparser.RawConfigParser()\n            _load_manifest_interpret_source(parent_manifest, manifest.get('config', 'extends'), username=username, password=password, verify_certificate=verify_certificate)\n            for s in parent_manifest.sections():\n                for (k, v) in parent_manifest.items(s):\n                    if (not manifest.has_option(s, k)):\n                        manifest.set(s, k, v)\n    except configparser.Error:\n        logger.debug('', exc_info=True)\n        error_message = sys.exc_info()[1]\n        raise ManifestException('Unable to parse manifest!: {0}'.format(error_message))\n", "label": 1}
{"function": "\n\ndef handle(self):\n    ' Handle log requests until connection closed. '\n    makeLogRecord = logging.makeLogRecord\n    logger = logging.getLogger('remote')\n    conn = self.connection\n    recv = conn.recv\n    peer = None\n    while True:\n        try:\n            data = recv(4)\n        except Exception:\n            return\n        if (len(data) < 4):\n            break\n        if (peer is None):\n            (host, port) = conn.getpeername()\n            try:\n                (host, aliases, addrs) = socket.gethostbyaddr(host)\n            except Exception as exc:\n                logger.debug('gethostbyaddr(%s) failed: %s', host, (str(exc) or repr(exc)))\n            peer = ('%s:%s' % (host, port))\n            logger.info('New logging connection from %s', peer)\n        slen = unpack('>L', data)[0]\n        data = recv(slen)\n        slen -= len(data)\n        msg = data\n        while slen:\n            data = recv(slen)\n            slen -= len(data)\n            msg = ''.join((msg, data))\n        try:\n            obj = loads(msg)\n            record = makeLogRecord(obj)\n        except Exception as exc:\n            logger.exception(\"Can't process log request from %s: %s\", peer, exc)\n        else:\n            prefix = record.prefix\n            name = record.name\n            if (name != prefix):\n                record.name = ('[%s] %s' % (prefix, name))\n            else:\n                record.name = ('[%s]' % prefix)\n            logger.handle(record)\n    conn.close()\n    if (peer is not None):\n        logger.info('Logging connection from %s closed', peer)\n", "label": 1}
{"function": "\n\ndef test_draw_component_model_hyperparameters_single(self):\n    draw_list = self.component_class.draw_hyperparameters(self.X)\n    assert (type(draw_list) is list)\n    assert (type(draw_list[0]) is dict)\n    draw = draw_list[0]\n    assert (type(draw) is dict)\n    for (key, value) in six.iteritems(draw):\n        assert (key in ['mu', 'nu', 'r', 's'])\n        assert ((type(value) is float) or (type(value) is numpy.float64))\n        assert (not math.isnan(value))\n        assert (not math.isinf(value))\n        if (key in ['nu', 's', 'r']):\n            assert (value > 0.0)\n", "label": 1}
{"function": "\n\ndef real_download(self, filename, info_dict):\n    man_url = info_dict['url']\n    requested_bitrate = info_dict.get('tbr')\n    self.to_screen('[download] Downloading f4m manifest')\n    manifest = self.ydl.urlopen(man_url).read()\n    self.report_destination(filename)\n    http_dl = HttpQuietDownloader(self.ydl, {\n        'continuedl': True,\n        'quiet': True,\n        'noprogress': True,\n        'ratelimit': self.params.get('ratelimit', None),\n        'test': self.params.get('test', False),\n    })\n    doc = etree.fromstring(manifest)\n    formats = [(int(f.attrib.get('bitrate', (- 1))), f) for f in doc.findall(_add_ns('media'))]\n    if (requested_bitrate is None):\n        formats = sorted(formats, key=(lambda f: f[0]))\n        (rate, media) = formats[(- 1)]\n    else:\n        (rate, media) = list(filter((lambda f: (int(f[0]) == requested_bitrate)), formats))[0]\n    base_url = compat_urlparse.urljoin(man_url, media.attrib['url'])\n    bootstrap_node = doc.find(_add_ns('bootstrapInfo'))\n    if (bootstrap_node.text is None):\n        bootstrap_url = compat_urlparse.urljoin(base_url, bootstrap_node.attrib['url'])\n        bootstrap = self.ydl.urlopen(bootstrap_url).read()\n    else:\n        bootstrap = base64.b64decode(bootstrap_node.text)\n    metadata_node = media.find(_add_ns('metadata'))\n    if (metadata_node is not None):\n        metadata = base64.b64decode(metadata_node.text)\n    else:\n        metadata = None\n    boot_info = read_bootstrap_info(bootstrap)\n    fragments_list = build_fragments_list(boot_info)\n    if self.params.get('test', False):\n        fragments_list = fragments_list[:1]\n    total_frags = len(fragments_list)\n    akamai_pv = xpath_text(doc, _add_ns('pv-2.0'))\n    tmpfilename = self.temp_name(filename)\n    (dest_stream, tmpfilename) = sanitize_open(tmpfilename, 'wb')\n    write_flv_header(dest_stream)\n    write_metadata_tag(dest_stream, metadata)\n    state = {\n        'downloaded_bytes': 0,\n        'frag_counter': 0,\n    }\n    start = time.time()\n\n    def frag_progress_hook(status):\n        frag_total_bytes = status.get('total_bytes', 0)\n        estimated_size = (state['downloaded_bytes'] + ((total_frags - state['frag_counter']) * frag_total_bytes))\n        if (status['status'] == 'finished'):\n            state['downloaded_bytes'] += frag_total_bytes\n            state['frag_counter'] += 1\n            progress = self.calc_percent(state['frag_counter'], total_frags)\n            byte_counter = state['downloaded_bytes']\n        else:\n            frag_downloaded_bytes = status['downloaded_bytes']\n            byte_counter = (state['downloaded_bytes'] + frag_downloaded_bytes)\n            frag_progress = self.calc_percent(frag_downloaded_bytes, frag_total_bytes)\n            progress = self.calc_percent(state['frag_counter'], total_frags)\n            progress += (frag_progress / float(total_frags))\n        eta = self.calc_eta(start, time.time(), estimated_size, byte_counter)\n        self.report_progress(progress, format_bytes(estimated_size), status.get('speed'), eta)\n    http_dl.add_progress_hook(frag_progress_hook)\n    frags_filenames = []\n    for (seg_i, frag_i) in fragments_list:\n        name = ('Seg%d-Frag%d' % (seg_i, frag_i))\n        url = (base_url + name)\n        if akamai_pv:\n            url += ('?' + akamai_pv.strip(';'))\n        frag_filename = ('%s-%s' % (tmpfilename, name))\n        success = http_dl.download(frag_filename, {\n            'url': url,\n        })\n        if (not success):\n            return False\n        with open(frag_filename, 'rb') as down:\n            down_data = down.read()\n            reader = FlvReader(down_data)\n            while True:\n                (_, box_type, box_data) = reader.read_box_info()\n                if (box_type == b'mdat'):\n                    dest_stream.write(box_data)\n                    break\n        frags_filenames.append(frag_filename)\n    dest_stream.close()\n    self.report_finish(format_bytes(state['downloaded_bytes']), (time.time() - start))\n    self.try_rename(tmpfilename, filename)\n    for frag_file in frags_filenames:\n        os.remove(frag_file)\n    fsize = os.path.getsize(encodeFilename(filename))\n    self._hook_progress({\n        'downloaded_bytes': fsize,\n        'total_bytes': fsize,\n        'filename': filename,\n        'status': 'finished',\n    })\n    return True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _find_new_(classdict, member_type, first_enum):\n    'Returns the __new__ to be used for creating the enum members.\\n\\n            classdict: the class dictionary given to __new__\\n            member_type: the data type whose __new__ will be used by default\\n            first_enum: enumeration to check for an overriding __new__\\n\\n            '\n    __new__ = classdict.get('__new__', None)\n    if __new__:\n        return (None, True, True)\n    N__new__ = getattr(None, '__new__')\n    O__new__ = getattr(object, '__new__')\n    if (Enum is None):\n        E__new__ = N__new__\n    else:\n        E__new__ = Enum.__dict__['__new__']\n    for method in ('__member_new__', '__new__'):\n        for possible in (member_type, first_enum):\n            try:\n                target = possible.__dict__[method]\n            except (AttributeError, KeyError):\n                target = getattr(possible, method, None)\n            if (target not in [None, N__new__, O__new__, E__new__]):\n                if (method == '__member_new__'):\n                    classdict['__new__'] = target\n                    return (None, False, True)\n                if isinstance(target, staticmethod):\n                    target = target.__get__(member_type)\n                __new__ = target\n                break\n        if (__new__ is not None):\n            break\n    else:\n        __new__ = object.__new__\n    if (__new__ is object.__new__):\n        use_args = False\n    else:\n        use_args = True\n    return (__new__, False, use_args)\n", "label": 1}
{"function": "\n\ndef test_newaxis(self):\n    '\\n        newaxis support comes from logic in the __getitem__ of TensorType\\n        Variables, which currently inserts dimshuffle to get the right number\\n        of dimensions, and adjusts the slice tuple accordingly.\\n\\n        So testing is done via square-bracket notation rather than direct\\n        interaction with the Subtensor Op (which has no support of its own for\\n        newaxis).\\n        '\n    newaxis = numpy.newaxis\n    n = self.shared(numpy.arange(24, dtype=self.dtype).reshape((2, 3, 4)))\n    assert (n.ndim == 3)\n    n4 = n[newaxis, :, :, :]\n    assert (n4.broadcastable == (True, False, False, False)), n4\n    n4 = n[:, newaxis, :, :]\n    assert (n4.broadcastable == (False, True, False, False)), n4\n    n4 = n[:, :, newaxis, :]\n    assert (n4.broadcastable == (False, False, True, False)), n4\n    n4 = n[:, :, :, newaxis]\n    assert (n4.broadcastable == (False, False, False, True)), n4\n    n3 = n.flatten()[newaxis, :, newaxis]\n    assert (n3.broadcastable == (True, False, True)), n3\n    s = cscalar()\n    s1 = s[newaxis]\n    assert (s1.broadcastable == (True,)), s1\n    (vs1, vn3, vn4) = theano.function([s], [s1, n3, n4])((- 2.0))\n    assert numpy.all((vs1 == [(- 2.0)]))\n    assert numpy.all((vn3 == numpy.arange(24)[newaxis, :, newaxis]))\n    assert numpy.all((vn4 == numpy.arange(24).reshape((2, 3, 4))[:, :, :, newaxis]))\n", "label": 1}
{"function": "\n\ndef test_panzoom_zoom_mouse(qtbot, canvas_pz, panzoom):\n    c = canvas_pz\n    pz = panzoom\n    press = MouseEvent(type='mouse_press', pos=(50.0, 50.0))\n    c.events.mouse_move(pos=(0.0, 0.0), button=2, last_event=press, press_event=press)\n    assert (pz.pan[0] < 0)\n    assert (pz.pan[1] < 0)\n    assert (pz.zoom[0] < 1)\n    assert (pz.zoom[1] > 1)\n    pz.reset()\n    size = np.asarray(c.size)\n    c.events.mouse_wheel(pos=(size / 2.0), delta=(0.0, 1.0))\n    assert (pz.pan == [0, 0])\n    assert (pz.zoom[0] > 1)\n    assert (pz.zoom[1] > 1)\n    pz.reset()\n    c.events.mouse_wheel(pos=(0.0, 0.0), delta=(0.0, 1.0), modifiers=(keys.CONTROL,))\n    assert (pz.pan == [0, 0])\n    assert (pz.zoom == [1, 1])\n    pz.reset()\n", "label": 1}
{"function": "\n\ndef send_buffer(buf, sock, host_id):\n    'Utility function that sends the buffer into the provided socket.\\n    The buffer itself will slowly clear out and is modified in place.\\n    '\n    try:\n        timeout = sock.gettimeout()\n        sock.setblocking(False)\n        try:\n            for (idx, item) in enumerate(buf):\n                sent = 0\n                while 1:\n                    try:\n                        sent = sock.send(item)\n                    except IOError as e:\n                        if (e.errno == errno.EAGAIN):\n                            continue\n                        elif (e.errno == errno.EWOULDBLOCK):\n                            break\n                        raise\n                    break\n                if (sent < len(item)):\n                    buf[:(idx + 1)] = [item[sent:]]\n                    break\n            else:\n                del buf[:]\n        finally:\n            sock.settimeout(timeout)\n    except IOError as e:\n        if isinstance(e, socket.timeout):\n            raise TimeoutError(('Timeout writing to socket (host %s)' % host_id))\n        raise ConnectionError(('Error while writing to socket (host %s): %s' % (host_id, e)))\n", "label": 1}
{"function": "\n\ndef set_by_index(dest_gpu, ind, src_gpu, ind_which='dest'):\n    \"\\n    Set values in a GPUArray by index.\\n\\n    Parameters\\n    ----------\\n    dest_gpu : pycuda.gpuarray.GPUArray\\n        GPUArray instance to modify.\\n    ind : pycuda.gpuarray.GPUArray or numpy.ndarray\\n        1D array of element indices to set. Must have an integer dtype.\\n    src_gpu : pycuda.gpuarray.GPUArray\\n        GPUArray instance from which to set values.\\n    ind_which : str\\n        If set to 'dest', set the elements in `dest_gpu` with indices `ind`\\n        to the successive values in `src_gpu`; the lengths of `ind` and\\n        `src_gpu` must be equal. If set to 'src', set the\\n        successive values in `dest_gpu` to the values in `src_gpu` with indices\\n        `ind`; the lengths of `ind` and `dest_gpu` must be equal.\\n\\n    Examples\\n    --------\\n    >>> import pycuda.gpuarray as gpuarray\\n    >>> import pycuda.autoinit\\n    >>> import numpy as np\\n    >>> import misc\\n    >>> dest_gpu = gpuarray.to_gpu(np.arange(5, dtype=np.float32))\\n    >>> ind = gpuarray.to_gpu(np.array([0, 2, 4]))\\n    >>> src_gpu = gpuarray.to_gpu(np.array([1, 1, 1], dtype=np.float32))\\n    >>> misc.set_by_index(dest_gpu, ind, src_gpu, 'dest')\\n    >>> np.allclose(dest_gpu.get(), np.array([1, 1, 1, 3, 1], dtype=np.float32))\\n    True\\n    >>> dest_gpu = gpuarray.to_gpu(np.zeros(3, dtype=np.float32))\\n    >>> ind = gpuarray.to_gpu(np.array([0, 2, 4]))\\n    >>> src_gpu = gpuarray.to_gpu(np.arange(5, dtype=np.float32))\\n    >>> misc.set_by_index(dest_gpu, ind, src_gpu)\\n    >>> np.allclose(dest_gpu.get(), np.array([0, 2, 4], dtype=np.float32))\\n    True\\n\\n    Notes\\n    -----\\n    Only supports 1D index arrays.\\n\\n    May not be efficient for certain index patterns because of lack of inability\\n    to coalesce memory operations.\\n    \"\n    assert (len(np.shape(ind)) == 1)\n    assert (dest_gpu.dtype == src_gpu.dtype)\n    assert issubclass(ind.dtype.type, numbers.Integral)\n    N = len(ind)\n    if (N == 0):\n        return\n    if (ind_which == 'dest'):\n        assert (N == len(src_gpu))\n    elif (ind_which == 'src'):\n        assert (N == len(dest_gpu))\n    else:\n        raise ValueError('invalid value for `ind_which`')\n    if (not isinstance(ind, gpuarray.GPUArray)):\n        ind = gpuarray.to_gpu(ind)\n    try:\n        func = set_by_index.cache[(dest_gpu.dtype, ind.dtype, ind_which)]\n    except KeyError:\n        data_ctype = tools.dtype_to_ctype(dest_gpu.dtype)\n        ind_ctype = tools.dtype_to_ctype(ind.dtype)\n        v = '{data_ctype} *dest, {ind_ctype} *ind, {data_ctype} *src'.format(data_ctype=data_ctype, ind_ctype=ind_ctype)\n        if (ind_which == 'dest'):\n            func = elementwise.ElementwiseKernel(v, 'dest[ind[i]] = src[i]')\n        else:\n            func = elementwise.ElementwiseKernel(v, 'dest[i] = src[ind[i]]')\n        set_by_index.cache[(dest_gpu.dtype, ind.dtype, ind_which)] = func\n    func(dest_gpu, ind, src_gpu, range=slice(0, N, 1))\n", "label": 1}
{"function": "\n\ndef md5sum(filename, use_sudo=False):\n    '\\n    Compute the MD5 sum of a file.\\n    '\n    func = ((use_sudo and run_as_root) or run)\n    with settings(hide('running', 'stdout', 'stderr', 'warnings'), warn_only=True):\n        if exists('/usr/bin/md5sum'):\n            res = func(('/usr/bin/md5sum %(filename)s' % locals()))\n        elif exists('/sbin/md5'):\n            res = func(('/sbin/md5 -r %(filename)s' % locals()))\n        elif exists('/opt/local/gnu/bin/md5sum'):\n            res = func(('/opt/local/gnu/bin/md5sum %(filename)s' % locals()))\n        elif exists('/opt/local/bin/md5sum'):\n            res = func(('/opt/local/bin/md5sum %(filename)s' % locals()))\n        else:\n            md5sum = func('which md5sum')\n            md5 = func('which md5')\n            if exists(md5sum):\n                res = func(('%(md5sum)s %(filename)s' % locals()))\n            elif exists(md5):\n                res = func(('%(md5)s %(filename)s' % locals()))\n            else:\n                abort('No MD5 utility was found on this system.')\n    if res.succeeded:\n        parts = res.split()\n        _md5sum = (((len(parts) > 0) and parts[0]) or None)\n    else:\n        warn(res)\n        _md5sum = None\n    return _md5sum\n", "label": 1}
{"function": "\n\ndef _GetAxisParams(self, chart):\n    'Collect params related to our various axes (x, y, right-hand).'\n    axis_types = []\n    axis_ranges = []\n    axis_labels = []\n    axis_label_positions = []\n    axis_label_gridlines = []\n    mark_length = max(self._width, self._height)\n    for (i, axis_pair) in enumerate((a for a in chart._GetAxes() if a[1].labels)):\n        (axis_type_code, axis) = axis_pair\n        axis_types.append(axis_type_code)\n        if ((axis.min is not None) or (axis.max is not None)):\n            assert (axis.min is not None)\n            assert (axis.max is not None)\n            axis_ranges.append(('%s,%s,%s' % (i, axis.min, axis.max)))\n        (labels, positions) = self._GetAxisLabelsAndPositions(axis, chart)\n        if labels:\n            axis_labels.append(('%s:' % i))\n            axis_labels.extend(labels)\n        if positions:\n            positions = ([i] + list(positions))\n            axis_label_positions.append(','.join((str(x) for x in positions)))\n        if axis.label_gridlines:\n            axis_label_gridlines.append(('%d,%d' % (i, (- mark_length))))\n    return util.JoinLists(axis_type=axis_types, axis_range=axis_ranges, axis_label=axis_labels, axis_position=axis_label_positions, axis_tick_marks=axis_label_gridlines)\n", "label": 1}
{"function": "\n\ndef matchreport(self, inamepart='', names='pytest_runtest_logreport pytest_collectreport', when=None):\n    ' return a testreport whose dotted import path matches '\n    l = []\n    for rep in self.getreports(names=names):\n        try:\n            if ((not when) and (rep.when != 'call') and rep.passed):\n                continue\n        except AttributeError:\n            pass\n        if (when and (getattr(rep, 'when', None) != when)):\n            continue\n        if ((not inamepart) or (inamepart in rep.nodeid.split('::'))):\n            l.append(rep)\n    if (not l):\n        raise ValueError(('could not find test report matching %r: no test reports at all!' % (inamepart,)))\n    if (len(l) > 1):\n        raise ValueError(('found 2 or more testreports matching %r: %s' % (inamepart, l)))\n    return l[0]\n", "label": 1}
{"function": "\n\ndef test_caching_sorting(self):\n    'Tests if Bayesian Average sorting works correctly.'\n    r = _make_backdated_revision(90)\n    for x in range(0, 26):\n        _add_vote_in_past(r, 1, 3)\n    for x in range(0, 76):\n        _add_vote_in_past(r, 0, 3)\n    r2 = _make_backdated_revision(90)\n    for x in range(0, 61):\n        _add_vote_in_past(r2, 1, 3)\n    for x in range(0, 181):\n        _add_vote_in_past(r2, 0, 3)\n    r3 = _make_backdated_revision(90)\n    for x in range(0, 31):\n        _add_vote_in_past(r3, 1, 3)\n    for x in range(0, 91):\n        _add_vote_in_past(r3, 0, 3)\n    cache_most_unhelpful_kb_articles()\n    eq_(3, self.redis.llen(self.REDIS_KEY))\n    result = self.redis.lrange(self.REDIS_KEY, 0, 3)\n    assert (('%d::%.1f:' % (r2.document.id, 242.0)) in result[0])\n    assert (('%d::%.1f:' % (r3.document.id, 122.0)) in result[1])\n    assert (('%d::%.1f:' % (r.document.id, 102.0)) in result[2])\n", "label": 1}
{"function": "\n\ndef load(self, config_file, apikey, appkey):\n    config = configparser.ConfigParser()\n    if ((apikey is not None) and (appkey is not None)):\n        self['apikey'] = apikey\n        self['appkey'] = appkey\n    else:\n        if os.access(config_file, os.F_OK):\n            config.read(config_file)\n            if (not config.has_section('Connection')):\n                report_errors({\n                    'errors': [('%s has no [Connection] section' % config_file)],\n                })\n        else:\n            try:\n                response = ''\n                while (response.strip().lower() not in ['y', 'n']):\n                    response = get_input(('%s does not exist. Would you like to create it? [Y/n] ' % config_file))\n                    if (response.strip().lower() in ['', 'y', 'yes']):\n                        apikey = get_input('What is your api key? (Get it here: https://app.datadoghq.com/account/settings#api) ')\n                        appkey = get_input('What is your application key? (Generate one here: https://app.datadoghq.com/account/settings#api) ')\n                        config.add_section('Connection')\n                        config.set('Connection', 'apikey', apikey)\n                        config.set('Connection', 'appkey', appkey)\n                        f = open(config_file, 'w')\n                        config.write(f)\n                        f.close()\n                        print(('Wrote %s' % config_file))\n                    elif (response.strip().lower() == 'n'):\n                        print_err('Exiting\\n')\n                        sys.exit(1)\n            except KeyboardInterrupt:\n                print_err('\\nExiting')\n                sys.exit(1)\n        self['apikey'] = config.get('Connection', 'apikey')\n        self['appkey'] = config.get('Connection', 'appkey')\n    assert ((self['apikey'] is not None) and (self['appkey'] is not None))\n", "label": 1}
{"function": "\n\ndef test_factor_nc():\n    (x, y) = symbols('x,y')\n    k = symbols('k', integer=True)\n    (n, m, o) = symbols('n,m,o', commutative=False)\n    from sympy.core.function import _mexpand\n    e = (x * ((1 + y) ** 2))\n    assert (_mexpand(e) == ((x + ((x * 2) * y)) + (x * (y ** 2))))\n\n    def factor_nc_test(e):\n        ex = _mexpand(e)\n        assert ex.is_Add\n        f = factor_nc(ex)\n        assert ((not f.is_Add) and (_mexpand(f) == ex))\n    factor_nc_test((x * (1 + y)))\n    factor_nc_test((n * (x + 1)))\n    factor_nc_test((n * (x + m)))\n    factor_nc_test(((x + m) * n))\n    factor_nc_test((((n * m) * ((x * o) + ((n * o) * m))) * n))\n    s = Sum(x, (x, 1, 2))\n    factor_nc_test((x * (1 + s)))\n    factor_nc_test(((x * (1 + s)) * s))\n    factor_nc_test((x * (1 + sin(s))))\n    factor_nc_test(((1 + n) ** 2))\n    factor_nc_test((((x + n) * (x + m)) * (x + y)))\n    factor_nc_test((x * ((n * m) + 1)))\n    factor_nc_test((x * ((n * m) + x)))\n    factor_nc_test((x * (((x * n) * m) + 1)))\n    factor_nc_test(((x * n) * ((x * m) + 1)))\n    factor_nc_test((x * ((m * n) + ((x * n) * m))))\n    factor_nc_test(((n * (1 - m)) * (n ** 2)))\n    factor_nc_test(((n + m) ** 2))\n    factor_nc_test(((n - m) * ((n + m) ** 2)))\n    factor_nc_test((((n + m) ** 2) * (n - m)))\n    factor_nc_test((((m - n) * ((n + m) ** 2)) * (n - m)))\n    assert (factor_nc((n * (n + (n * m)))) == ((n ** 2) * (1 + m)))\n    assert (factor_nc((m * ((m * n) + ((n * m) * (n ** 2))))) == ((m * (m + ((n * m) * n))) * n))\n    eq = ((m * sin(n)) - (sin(n) * m))\n    assert (factor_nc(eq) == eq)\n    from sympy.physics.secondquant import Commutator\n    from sympy import factor\n    eq = (1 + (x * Commutator(m, n)))\n    assert (factor_nc(eq) == eq)\n    eq = ((x * Commutator(m, n)) + ((x * Commutator(m, o)) * Commutator(m, n)))\n    assert (factor(eq) == ((x * (1 + Commutator(m, o))) * Commutator(m, n)))\n    assert (((2 * n) + (2 * m)).factor() == (2 * (n + m)))\n    assert (factor_nc(((n ** k) + (n ** (k + 1)))) == ((n ** k) * (1 + n)))\n    assert (factor_nc((((m * n) ** k) + ((m * n) ** (k + 1)))) == ((1 + (m * n)) * ((m * n) ** k)))\n    assert (factor_nc(((- n) * ((2 * (x ** 2)) + (2 * x)))) == ((((- 2) * n) * x) * (x + 1)))\n", "label": 1}
{"function": "\n\ndef test_Domain_unify_algebraic():\n    sqrt5 = QQ.algebraic_field(sqrt(5))\n    sqrt7 = QQ.algebraic_field(sqrt(7))\n    sqrt57 = QQ.algebraic_field(sqrt(5), sqrt(7))\n    assert (sqrt5.unify(sqrt7) == sqrt57)\n    assert (sqrt5.unify(sqrt5[(x, y)]) == sqrt5[(x, y)])\n    assert (sqrt5[(x, y)].unify(sqrt5) == sqrt5[(x, y)])\n    assert (sqrt5.unify(sqrt5.frac_field(x, y)) == sqrt5.frac_field(x, y))\n    assert (sqrt5.frac_field(x, y).unify(sqrt5) == sqrt5.frac_field(x, y))\n    assert (sqrt5.unify(sqrt7[(x, y)]) == sqrt57[(x, y)])\n    assert (sqrt5[(x, y)].unify(sqrt7) == sqrt57[(x, y)])\n    assert (sqrt5.unify(sqrt7.frac_field(x, y)) == sqrt57.frac_field(x, y))\n    assert (sqrt5.frac_field(x, y).unify(sqrt7) == sqrt57.frac_field(x, y))\n", "label": 1}
{"function": "\n\n@contextlib.contextmanager\ndef bundle_package(self, cache=True):\n    'Makes the pybundle archive (that :program:`pip` can take to\\n        install) with completely resolved dependencies.  It yields triple\\n        of package name, filename of the pybundle archive, and its full\\n        path. ::\\n\\n            with build.bundle_package() as (package, filename, path):\\n                sftp.put(path, filename)\\n\\n        :param cache: whether to cache the package file or not.\\n                      ``True`` by default\\n        :type cache: :class:`bool`\\n\\n        '\n    asuka_logger = self.get_logger('bundle_package')\n    if (not getattr(type(self), 'initialized', False)):\n        type(self).initialized = True\n        logger.consumers.extend([(Logger.FATAL, asuka_logger.critical), (Logger.ERROR, asuka_logger.error), (Logger.WARN, asuka_logger.warn), (Logger.NOTIFY, asuka_logger.info), (Logger.INFO, asuka_logger.info), (Logger.DEBUG, asuka_logger.debug), (Logger.VERBOSE_DEBUG, asuka_logger.debug)])\n        vcs.register(Git)\n    main_parser = create_main_parser()\n    bundle = commands['bundle'](main_parser)\n    with self.archive_package() as (package_name, filename, filepath):\n        bundle_filename = (package_name + '.pybundle')\n        if cache:\n            cache_dir_path = os.path.join(tempfile.gettempdir(), 'asuka-pybundle-cache')\n            if (not os.path.isdir(cache_dir_path)):\n                os.makedirs(cache_dir_path)\n            cache_path = os.path.join(cache_dir_path, bundle_filename)\n            if os.path.isfile(cache_path):\n                asuka_logger.info('cache exists: %s, skipping pybundle...', cache_path)\n                (yield (package_name, bundle_filename, cache_path))\n                return\n        tempdir = tempfile.gettempdir()\n        bundle_path = os.path.join(os.path.dirname(filepath), bundle_filename)\n        asuka_logger.info('pybundle_path = %r', bundle_path)\n        options = optparse.Values()\n        options.editables = []\n        options.requirements = []\n        options.find_links = []\n        options.index_url = PYPI_INDEX_URLS[0]\n        options.extra_index_urls = PYPI_INDEX_URLS[1:]\n        options.no_index = False\n        options.use_mirrors = False\n        options.mirrors = True\n        options.build_dir = os.path.join(tempdir, 'asuka-dist-build-bundle')\n        options.target_dir = None\n        options.download_dir = None\n        options.download_cache = os.path.join(tempdir, 'asuka-dist-download-cache')\n        options.src_dir = backup_dir(src_prefix, '-bundle')\n        options.upgrade = False\n        options.force_reinstall = False\n        options.ignore_dependencies = False\n        options.no_install = True\n        options.no_download = False\n        options.install_options = []\n        options.global_options = []\n        options.use_user_site = False\n        options.as_egg = False\n        asuka_logger.debug('start: pip bundle %s %s', bundle_path, filepath)\n        retrial = 0\n        while 1:\n            try:\n                shutil.rmtree(options.build_dir)\n            except (OSError, IOError):\n                pass\n            try:\n                bundle.run(options, [bundle_path, filepath])\n            except PipError as e:\n                asuka_logger.exception(e)\n                retrial += 1\n                if (retrial < 3):\n                    asuka_logger.error('retry pip bundle after %d second(s)... (%d)', retrial, (retrial ** 2))\n                    options.index_url = PYPI_INDEX_URLS[retrial]\n                    options.extra_index_urls = PYPI_INDEX_URLS[(retrial + 1):]\n                    time.sleep((retrial ** 2))\n                    continue\n                raise\n            finally:\n                if os.path.isdir(options.build_dir):\n                    shutil.rmtree(options.build_dir)\n            break\n        asuka_logger.debug('end: pip bundle %s %s', bundle_path, filepath)\n        if cache:\n            asuka_logger.info('save pybundle cache %s...', cache_path)\n            shutil.copyfile(bundle_path, cache_path)\n        (yield (package_name, os.path.basename(bundle_path), bundle_path))\n", "label": 1}
{"function": "\n\ndef _plot_topo_onpick(event, show_func):\n    'Onpick callback that shows a single channel in a new figure'\n    orig_ax = event.inaxes\n    if ((event.inaxes is None) or ((not hasattr(orig_ax, '_mne_ch_idx')) and (not hasattr(orig_ax, '_mne_axs')))):\n        return\n    import matplotlib.pyplot as plt\n    try:\n        if hasattr(orig_ax, '_mne_axs'):\n            (x, y) = (event.xdata, event.ydata)\n            for ax in orig_ax._mne_axs:\n                if ((x >= ax.pos[0]) and (y >= ax.pos[1]) and (x <= (ax.pos[0] + ax.pos[2])) and (y <= (ax.pos[1] + ax.pos[3]))):\n                    orig_ax = ax\n                    break\n            else:\n                return\n        ch_idx = orig_ax._mne_ch_idx\n        face_color = orig_ax._mne_ax_face_color\n        (fig, ax) = plt.subplots(1)\n        plt.title(orig_ax._mne_ch_name)\n        ax.set_axis_bgcolor(face_color)\n        show_func(plt, ch_idx)\n    except Exception as err:\n        print(err)\n        raise\n", "label": 1}
{"function": "\n\ndef quad(ctx, f, *points, **kwargs):\n    '\\n        Computes a single, double or triple integral over a given\\n        1D interval, 2D rectangle, or 3D cuboid. A basic example::\\n\\n            >>> from mpmath import *\\n            >>> mp.dps = 15; mp.pretty = True\\n            >>> quad(sin, [0, pi])\\n            2.0\\n\\n        A basic 2D integral::\\n\\n            >>> f = lambda x, y: cos(x+y/2)\\n            >>> quad(f, [-pi/2, pi/2], [0, pi])\\n            4.0\\n\\n        **Interval format**\\n\\n        The integration range for each dimension may be specified\\n        using a list or tuple. Arguments are interpreted as follows:\\n\\n        ``quad(f, [x1, x2])`` -- calculates\\n        `\\\\int_{x_1}^{x_2} f(x) \\\\, dx`\\n\\n        ``quad(f, [x1, x2], [y1, y2])`` -- calculates\\n        `\\\\int_{x_1}^{x_2} \\\\int_{y_1}^{y_2} f(x,y) \\\\, dy \\\\, dx`\\n\\n        ``quad(f, [x1, x2], [y1, y2], [z1, z2])`` -- calculates\\n        `\\\\int_{x_1}^{x_2} \\\\int_{y_1}^{y_2} \\\\int_{z_1}^{z_2} f(x,y,z)\\n        \\\\, dz \\\\, dy \\\\, dx`\\n\\n        Endpoints may be finite or infinite. An interval descriptor\\n        may also contain more than two points. In this\\n        case, the integration is split into subintervals, between\\n        each pair of consecutive points. This is useful for\\n        dealing with mid-interval discontinuities, or integrating\\n        over large intervals where the function is irregular or\\n        oscillates.\\n\\n        **Options**\\n\\n        :func:`~mpmath.quad` recognizes the following keyword arguments:\\n\\n        *method*\\n            Chooses integration algorithm (described below).\\n        *error*\\n            If set to true, :func:`~mpmath.quad` returns `(v, e)` where `v` is the\\n            integral and `e` is the estimated error.\\n        *maxdegree*\\n            Maximum degree of the quadrature rule to try before\\n            quitting.\\n        *verbose*\\n            Print details about progress.\\n\\n        **Algorithms**\\n\\n        Mpmath presently implements two integration algorithms: tanh-sinh\\n        quadrature and Gauss-Legendre quadrature. These can be selected\\n        using *method=\\'tanh-sinh\\'* or *method=\\'gauss-legendre\\'* or by\\n        passing the classes *method=TanhSinh*, *method=GaussLegendre*.\\n        The functions :func:`~mpmath.quadts` and :func:`~mpmath.quadgl` are also available\\n        as shortcuts.\\n\\n        Both algorithms have the property that doubling the number of\\n        evaluation points roughly doubles the accuracy, so both are ideal\\n        for high precision quadrature (hundreds or thousands of digits).\\n\\n        At high precision, computing the nodes and weights for the\\n        integration can be expensive (more expensive than computing the\\n        function values). To make repeated integrations fast, nodes\\n        are automatically cached.\\n\\n        The advantages of the tanh-sinh algorithm are that it tends to\\n        handle endpoint singularities well, and that the nodes are cheap\\n        to compute on the first run. For these reasons, it is used by\\n        :func:`~mpmath.quad` as the default algorithm.\\n\\n        Gauss-Legendre quadrature often requires fewer function\\n        evaluations, and is therefore often faster for repeated use, but\\n        the algorithm does not handle endpoint singularities as well and\\n        the nodes are more expensive to compute. Gauss-Legendre quadrature\\n        can be a better choice if the integrand is smooth and repeated\\n        integrations are required (e.g. for multiple integrals).\\n\\n        See the documentation for :class:`TanhSinh` and\\n        :class:`GaussLegendre` for additional details.\\n\\n        **Examples of 1D integrals**\\n\\n        Intervals may be infinite or half-infinite. The following two\\n        examples evaluate the limits of the inverse tangent function\\n        (`\\\\int 1/(1+x^2) = \\\\tan^{-1} x`), and the Gaussian integral\\n        `\\\\int_{\\\\infty}^{\\\\infty} \\\\exp(-x^2)\\\\,dx = \\\\sqrt{\\\\pi}`::\\n\\n            >>> mp.dps = 15\\n            >>> quad(lambda x: 2/(x**2+1), [0, inf])\\n            3.14159265358979\\n            >>> quad(lambda x: exp(-x**2), [-inf, inf])**2\\n            3.14159265358979\\n\\n        Integrals can typically be resolved to high precision.\\n        The following computes 50 digits of `\\\\pi` by integrating the\\n        area of the half-circle defined by `x^2 + y^2 \\\\le 1`,\\n        `-1 \\\\le x \\\\le 1`, `y \\\\ge 0`::\\n\\n            >>> mp.dps = 50\\n            >>> 2*quad(lambda x: sqrt(1-x**2), [-1, 1])\\n            3.1415926535897932384626433832795028841971693993751\\n\\n        One can just as well compute 1000 digits (output truncated)::\\n\\n            >>> mp.dps = 1000\\n            >>> 2*quad(lambda x: sqrt(1-x**2), [-1, 1])  #doctest:+ELLIPSIS\\n            3.141592653589793238462643383279502884...216420198\\n\\n        Complex integrals are supported. The following computes\\n        a residue at `z = 0` by integrating counterclockwise along the\\n        diamond-shaped path from `1` to `+i` to `-1` to `-i` to `1`::\\n\\n            >>> mp.dps = 15\\n            >>> chop(quad(lambda z: 1/z, [1,j,-1,-j,1]))\\n            (0.0 + 6.28318530717959j)\\n\\n        **Examples of 2D and 3D integrals**\\n\\n        Here are several nice examples of analytically solvable\\n        2D integrals (taken from MathWorld [1]) that can be evaluated\\n        to high precision fairly rapidly by :func:`~mpmath.quad`::\\n\\n            >>> mp.dps = 30\\n            >>> f = lambda x, y: (x-1)/((1-x*y)*log(x*y))\\n            >>> quad(f, [0, 1], [0, 1])\\n            0.577215664901532860606512090082\\n            >>> +euler\\n            0.577215664901532860606512090082\\n\\n            >>> f = lambda x, y: 1/sqrt(1+x**2+y**2)\\n            >>> quad(f, [-1, 1], [-1, 1])\\n            3.17343648530607134219175646705\\n            >>> 4*log(2+sqrt(3))-2*pi/3\\n            3.17343648530607134219175646705\\n\\n            >>> f = lambda x, y: 1/(1-x**2 * y**2)\\n            >>> quad(f, [0, 1], [0, 1])\\n            1.23370055013616982735431137498\\n            >>> pi**2 / 8\\n            1.23370055013616982735431137498\\n\\n            >>> quad(lambda x, y: 1/(1-x*y), [0, 1], [0, 1])\\n            1.64493406684822643647241516665\\n            >>> pi**2 / 6\\n            1.64493406684822643647241516665\\n\\n        Multiple integrals may be done over infinite ranges::\\n\\n            >>> mp.dps = 15\\n            >>> print(quad(lambda x,y: exp(-x-y), [0, inf], [1, inf]))\\n            0.367879441171442\\n            >>> print(1/e)\\n            0.367879441171442\\n\\n        For nonrectangular areas, one can call :func:`~mpmath.quad` recursively.\\n        For example, we can replicate the earlier example of calculating\\n        `\\\\pi` by integrating over the unit-circle, and actually use double\\n        quadrature to actually measure the area circle::\\n\\n            >>> f = lambda x: quad(lambda y: 1, [-sqrt(1-x**2), sqrt(1-x**2)])\\n            >>> quad(f, [-1, 1])\\n            3.14159265358979\\n\\n        Here is a simple triple integral::\\n\\n            >>> mp.dps = 15\\n            >>> f = lambda x,y,z: x*y/(1+z)\\n            >>> quad(f, [0,1], [0,1], [1,2], method=\\'gauss-legendre\\')\\n            0.101366277027041\\n            >>> (log(3)-log(2))/4\\n            0.101366277027041\\n\\n        **Singularities**\\n\\n        Both tanh-sinh and Gauss-Legendre quadrature are designed to\\n        integrate smooth (infinitely differentiable) functions. Neither\\n        algorithm copes well with mid-interval singularities (such as\\n        mid-interval discontinuities in `f(x)` or `f\\'(x)`).\\n        The best solution is to split the integral into parts::\\n\\n            >>> mp.dps = 15\\n            >>> quad(lambda x: abs(sin(x)), [0, 2*pi])   # Bad\\n            3.99900894176779\\n            >>> quad(lambda x: abs(sin(x)), [0, pi, 2*pi])  # Good\\n            4.0\\n\\n        The tanh-sinh rule often works well for integrands having a\\n        singularity at one or both endpoints::\\n\\n            >>> mp.dps = 15\\n            >>> quad(log, [0, 1], method=\\'tanh-sinh\\')  # Good\\n            -1.0\\n            >>> quad(log, [0, 1], method=\\'gauss-legendre\\')  # Bad\\n            -0.999932197413801\\n\\n        However, the result may still be inaccurate for some functions::\\n\\n            >>> quad(lambda x: 1/sqrt(x), [0, 1], method=\\'tanh-sinh\\')\\n            1.99999999946942\\n\\n        This problem is not due to the quadrature rule per se, but to\\n        numerical amplification of errors in the nodes. The problem can be\\n        circumvented by temporarily increasing the precision::\\n\\n            >>> mp.dps = 30\\n            >>> a = quad(lambda x: 1/sqrt(x), [0, 1], method=\\'tanh-sinh\\')\\n            >>> mp.dps = 15\\n            >>> +a\\n            2.0\\n\\n        **Highly variable functions**\\n\\n        For functions that are smooth (in the sense of being infinitely\\n        differentiable) but contain sharp mid-interval peaks or many\\n        \"bumps\", :func:`~mpmath.quad` may fail to provide full accuracy. For\\n        example, with default settings, :func:`~mpmath.quad` is able to integrate\\n        `\\\\sin(x)` accurately over an interval of length 100 but not over\\n        length 1000::\\n\\n            >>> quad(sin, [0, 100]); 1-cos(100)   # Good\\n            0.137681127712316\\n            0.137681127712316\\n            >>> quad(sin, [0, 1000]); 1-cos(1000)   # Bad\\n            -37.8587612408485\\n            0.437620923709297\\n\\n        One solution is to break the integration into 10 intervals of\\n        length 100::\\n\\n            >>> quad(sin, linspace(0, 1000, 10))   # Good\\n            0.437620923709297\\n\\n        Another is to increase the degree of the quadrature::\\n\\n            >>> quad(sin, [0, 1000], maxdegree=10)   # Also good\\n            0.437620923709297\\n\\n        Whether splitting the interval or increasing the degree is\\n        more efficient differs from case to case. Another example is the\\n        function `1/(1+x^2)`, which has a sharp peak centered around\\n        `x = 0`::\\n\\n            >>> f = lambda x: 1/(1+x**2)\\n            >>> quad(f, [-100, 100])   # Bad\\n            3.64804647105268\\n            >>> quad(f, [-100, 100], maxdegree=10)   # Good\\n            3.12159332021646\\n            >>> quad(f, [-100, 0, 100])   # Also good\\n            3.12159332021646\\n\\n        **References**\\n\\n        1. http://mathworld.wolfram.com/DoubleIntegral.html\\n\\n        '\n    rule = kwargs.get('method', 'tanh-sinh')\n    if (type(rule) is str):\n        if (rule == 'tanh-sinh'):\n            rule = ctx._tanh_sinh\n        elif (rule == 'gauss-legendre'):\n            rule = ctx._gauss_legendre\n        else:\n            raise ValueError(('unknown quadrature rule: %s' % rule))\n    else:\n        rule = rule(ctx)\n    verbose = kwargs.get('verbose')\n    dim = len(points)\n    orig = prec = ctx.prec\n    epsilon = (ctx.eps / 8)\n    m = (kwargs.get('maxdegree') or rule.guess_degree(prec))\n    points = [ctx._as_points(p) for p in points]\n    try:\n        ctx.prec += 20\n        if (dim == 1):\n            (v, err) = rule.summation(f, points[0], prec, epsilon, m, verbose)\n        elif (dim == 2):\n            (v, err) = rule.summation((lambda x: rule.summation((lambda y: f(x, y)), points[1], prec, epsilon, m)[0]), points[0], prec, epsilon, m, verbose)\n        elif (dim == 3):\n            (v, err) = rule.summation((lambda x: rule.summation((lambda y: rule.summation((lambda z: f(x, y, z)), points[2], prec, epsilon, m)[0]), points[1], prec, epsilon, m)[0]), points[0], prec, epsilon, m, verbose)\n        else:\n            raise NotImplementedError('quadrature must have dim 1, 2 or 3')\n    finally:\n        ctx.prec = orig\n    if kwargs.get('error'):\n        return ((+ v), err)\n    return (+ v)\n", "label": 1}
{"function": "\n\n@tornado.gen.coroutine\ndef handle_message(self, stream, payload):\n    '\\n        Handle incoming messages from underylying TCP streams\\n\\n        :stream ZMQStream stream: A ZeroMQ stream.\\n        See http://zeromq.github.io/pyzmq/api/generated/zmq.eventloop.zmqstream.html\\n\\n        :param dict payload: A payload to process\\n        '\n    try:\n        payload = self.serial.loads(payload[0])\n        payload = self._decode_payload(payload)\n    except Exception as exc:\n        log.error('Bad load from minion: %s: %s', type(exc).__name__, exc)\n        stream.send(self.serial.dumps('bad load'))\n        raise tornado.gen.Return()\n    if ((not isinstance(payload, dict)) or (not isinstance(payload.get('load'), dict))):\n        log.error('payload and load must be a dict. Payload was: {0} and load was {1}'.format(payload, payload.get('load')))\n        stream.send(self.serial.dumps('payload and load must be a dict'))\n        raise tornado.gen.Return()\n    if ((payload['enc'] == 'clear') and (payload.get('load', {\n        \n    }).get('cmd') == '_auth')):\n        stream.send(self.serial.dumps(self._auth(payload['load'])))\n        raise tornado.gen.Return()\n    try:\n        (ret, req_opts) = (yield self.payload_handler(payload))\n    except Exception as e:\n        stream.send('Some exception handling minion payload')\n        log.error('Some exception handling a payload from minion', exc_info=True)\n        raise tornado.gen.Return()\n    req_fun = req_opts.get('fun', 'send')\n    if (req_fun == 'send_clear'):\n        stream.send(self.serial.dumps(ret))\n    elif (req_fun == 'send'):\n        stream.send(self.serial.dumps(self.crypticle.dumps(ret)))\n    elif (req_fun == 'send_private'):\n        stream.send(self.serial.dumps(self._encrypt_private(ret, req_opts['key'], req_opts['tgt'])))\n    else:\n        log.error('Unknown req_fun {0}'.format(req_fun))\n        stream.send('Server-side exception handling payload')\n    raise tornado.gen.Return()\n", "label": 1}
{"function": "\n\ndef install_app(apppath, appname, apptype, projectdir):\n    'Copies the app into the project directory'\n    copy_functions = {\n        'file': fromfilesys,\n        'git': fromgit,\n        'hg': fromhg,\n    }\n    try:\n        cf = copy_functions[apptype]\n    except KeyError:\n        raise CopyError('App type not supported')\n    dst = os.path.join(projectdir, APPDIR, appname)\n    cf(apppath, dst)\n    bare = (- 1)\n    if os.path.exists(os.path.join(dst, 'metadata.json')):\n        bare = True\n    if os.path.exists(os.path.join(dst, appname, 'metadata.json')):\n        bare = False\n    if (bare == (- 1)):\n        if os.path.exists(os.path.join(dst, 'models.py')):\n            bare = True\n        if os.path.exists(os.path.join(dst, appname, 'models.py')):\n            bare = False\n    if (bare == (- 1)):\n        if os.path.exists(os.path.join(dst, 'views.py')):\n            bare = True\n        if os.path.exists(os.path.join(dst, appname, 'views.py')):\n            bare = False\n    if (bare == (- 1)):\n        raise CopyError('Could not detect repository format. Please make sure your application has one of these files: metadata.json, models.py, or views.py')\n    if bare:\n        os.symlink(dst, os.path.join(projectdir, appname))\n    else:\n        os.symlink(os.path.join(dst, appname), os.path.join(projectdir, appname))\n", "label": 1}
{"function": "\n\ndef get_token_info(self, authn, req, endpoint):\n    '\\n\\n        :param authn:\\n        :param req:\\n        :return:\\n        '\n    try:\n        client_id = self.client_authn(self, req, authn)\n    except FailedAuthentication as err:\n        err = TokenErrorResponse(error='unauthorized_client', error_description=('%s' % err))\n        return Response(err.to_json(), content='application/json', status='401 Unauthorized')\n    logger.debug('{}: {} requesting {}'.format(endpoint, client_id, req.to_dict()))\n    try:\n        token_type = req['token_type_hint']\n    except KeyError:\n        try:\n            _info = self.sdb.token_factory['access_token'].info(req['token'])\n        except KeyError:\n            try:\n                _info = self.sdb.token_factory['refresh_token'].get_info(req['token'])\n            except KeyError:\n                raise\n            else:\n                token_type = 'refresh_token'\n        else:\n            token_type = 'access_token'\n    else:\n        try:\n            _info = self.sdb.token_factory[token_type].get_info(req['token'])\n        except KeyError:\n            raise\n    if (not self.token_access(endpoint, client_id, _info)):\n        return BadRequest()\n    return (client_id, token_type, _info)\n", "label": 1}
{"function": "\n\ndef make_modules_and_forms_row(row_type, sheet_name, languages, case_labels, media_image, media_audio, unique_id):\n    '\\n    assemble the various pieces of data that make up a row in the\\n    \"Modules_and_forms\" sheet into a single row (a flat tuple).\\n\\n    This function is meant as the single point of truth for the\\n    column ordering of Modules_and_forms\\n\\n    '\n    assert (row_type is not None)\n    assert (sheet_name is not None)\n    assert isinstance(languages, list)\n    assert isinstance(case_labels, list)\n    assert isinstance(media_image, list)\n    assert isinstance(media_audio, list)\n    assert isinstance(unique_id, basestring)\n    return [(item if (item is not None) else '') for item in ((((([row_type, sheet_name] + languages) + case_labels) + media_image) + media_audio) + [unique_id])]\n", "label": 1}
{"function": "\n\ndef put_team(self, org, team_name, read_only, members):\n    \"Create a team in a github organization.\\n\\n        Utilize\\n        https://developer.github.com/v3/orgs/teams/#list-teams,\\n        https://developer.github.com/v3/orgs/teams/#create-team,\\n        https://developer.github.com/v3/orgs/teams/#list-team-members,\\n        https://developer.github.com/v3/orgs/teams/#add-team-membership,\\n        and\\n        https://developer.github.com/v3/orgs/teams/#remove-team-membership.\\n        to create a team and/or replace an existing team's membership\\n        with the ``members`` list.\\n\\n        Args:\\n            org (str): Organization to create the repo in.\\n            team_name (str): Name of team to create.\\n            read_only (bool): If false, read/write, if true read_only.\\n            members (list): List of github usernames to add to the\\n                              team. If none, membership changes won't occur\\n\\n        Raises:\\n            GitHubUnknownError\\n            requests.RequestException\\n\\n        Returns:\\n            dict: The team dictionary\\n                (https://developer.github.com/v3/orgs/teams/#response-1)\\n\\n        \"\n    try:\n        team_dict = self._find_team(org, team_name)\n    except GitHubNoTeamFound:\n        team_dict = self._create_team(org, team_name, read_only)\n    if (members is None):\n        return team_dict\n    members_url = '{url}teams/{id}/members'.format(url=self.api_url, id=team_dict['id'])\n    existing_members = self._get_all(members_url)\n    existing_members = [x['login'] for x in existing_members]\n    remove_members = dict([(x, False) for x in existing_members if (x not in members)])\n    add_members = dict([(x, True) for x in members if (x not in existing_members)])\n    membership_dict = dict(chain(remove_members.items(), add_members.items()))\n    for (member, add) in membership_dict.items():\n        url = '{url}teams/{id}/memberships/{member}'.format(url=self.api_url, id=team_dict['id'], member=member)\n        if add:\n            response = self.session.put(url)\n        else:\n            response = self.session.delete(url)\n        if (response.status_code not in [200, 204]):\n            raise GitHubUnknownError('Failed to add or remove {0}. Got: {1}'.format(member, response.text))\n    return team_dict\n", "label": 1}
{"function": "\n\n@signalcommand\ndef handle_noargs(self, **options):\n    if (not settings.DEBUG):\n        raise CommandError('Only available in debug mode')\n    try:\n        from django.contrib.auth import get_user_model\n    except ImportError:\n        from django_extensions.future_1_5 import get_user_model\n    from django.contrib.auth.models import Group\n    email = options.get('default_email', DEFAULT_FAKE_EMAIL)\n    include_regexp = options.get('include_regexp', None)\n    exclude_regexp = options.get('exclude_regexp', None)\n    include_groups = options.get('include_groups', None)\n    exclude_groups = options.get('exclude_groups', None)\n    no_admin = options.get('no_admin', False)\n    no_staff = options.get('no_staff', False)\n    User = get_user_model()\n    users = User.objects.all()\n    if no_admin:\n        users = users.exclude(is_superuser=True)\n    if no_staff:\n        users = users.exclude(is_staff=True)\n    if exclude_groups:\n        groups = Group.objects.filter(name__in=exclude_groups.split(','))\n        if groups:\n            users = users.exclude(groups__in=groups)\n        else:\n            raise CommandError(('No group matches filter: %s' % exclude_groups))\n    if include_groups:\n        groups = Group.objects.filter(name__in=include_groups.split(','))\n        if groups:\n            users = users.filter(groups__in=groups)\n        else:\n            raise CommandError(('No groups matches filter: %s' % include_groups))\n    if exclude_regexp:\n        users = users.exclude(username__regex=exclude_regexp)\n    if include_regexp:\n        users = users.filter(username__regex=include_regexp)\n    for user in users:\n        user.email = (email % {\n            'username': user.username,\n            'first_name': user.first_name,\n            'last_name': user.last_name,\n        })\n        user.save()\n    print(('Changed %d emails' % users.count()))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef get_layout(name):\n    '\\n            Check whether the current theme has a custom layout for this\\n            class, and if so, store it in current.layouts\\n\\n            @param: the name of the custom layout\\n            @return: the layout or None if not present\\n        '\n    if hasattr(current, 'layouts'):\n        layouts = current.layouts\n    else:\n        layouts = {\n            \n        }\n    if (layouts is False):\n        return None\n    if (name in layouts):\n        return layouts[name]\n    application = current.request.application\n    settings = current.deployment_settings\n    theme = settings.get_theme()\n    theme_location = current.response.s3.theme_location\n    if theme_location:\n        theme_location = ('%s.' % theme_location[:(- 1)])\n    package = ('applications.%s.modules.templates.%s%s.layouts' % (application, theme_location, theme))\n    try:\n        override = getattr(__import__(package, fromlist=[name]), name)\n    except ImportError:\n        current.layouts = False\n        return None\n    except AttributeError:\n        override = None\n    if (override and hasattr(override, 'layout') and (type(override.layout) == type((lambda : None)))):\n        layout = override.layout\n    else:\n        layout = None\n    layouts[name] = layout\n    current.layouts = layouts\n    return layout\n", "label": 1}
{"function": "\n\ndef calculate(self):\n    'Search for PEM encoded RSA keys.'\n    mem = utils.load_as(self._config, astype='physical')\n    addrs = list(mem.get_available_addresses())\n    assert (len(addrs) == 1), 'Physical memory is fragmented'\n    (mem_start, mem_size) = addrs[0]\n    for offset in xrange(0, mem_size, CHUNK_SIZE):\n        chunk = mem.zread(offset, CHUNK_SIZE)\n        if ((START_MARKER in chunk) and (END_MARKER in chunk)):\n            key = []\n            in_key = False\n            for line in chunk.splitlines():\n                if ((START_MARKER in line) and (not in_key)):\n                    in_key = True\n                    key.append(line)\n                elif ((END_MARKER in line) and in_key):\n                    in_key = False\n                    key.append(line)\n                elif in_key:\n                    key.append(line)\n            if (len(key) != 0):\n                (yield '\\n'.join(key))\n", "label": 1}
{"function": "\n\ndef get_last_activity(self, diffsets=None, reviews=None):\n    'Returns the last public activity information on the review request.\\n\\n        This will return the last object updated, along with the timestamp\\n        of that object. It can be used to judge whether something on a\\n        review request has been made public more recently.\\n        '\n    timestamp = self.last_updated\n    updated_object = self\n    if ((not diffsets) and self.repository_id):\n        latest_diffset = self.get_latest_diffset()\n        diffsets = []\n        if latest_diffset:\n            diffsets.append(latest_diffset)\n    if diffsets:\n        for diffset in diffsets:\n            if (diffset.timestamp >= timestamp):\n                timestamp = diffset.timestamp\n                updated_object = diffset\n    if (not reviews):\n        try:\n            reviews = [self.reviews.filter(public=True).latest()]\n        except ObjectDoesNotExist:\n            reviews = []\n    for review in reviews:\n        if (review.public and (review.timestamp >= timestamp)):\n            timestamp = review.timestamp\n            updated_object = review\n    return (timestamp, updated_object)\n", "label": 1}
{"function": "\n\ndef _make_submission(self, path, username=None, add_uuid=False, forced_submission_time=None, client=None, media_file=None, auth=None):\n    self.factory = APIRequestFactory()\n    if (auth is None):\n        auth = DigestAuth(self.profile_data['username'], self.profile_data['password1'])\n    tmp_file = None\n    if add_uuid:\n        path = self._add_uuid_to_submission_xml(path, self.xform)\n    with open(path) as f:\n        post_data = {\n            'xml_submission_file': f,\n        }\n        if (media_file is not None):\n            post_data['media_file'] = media_file\n        if (username is None):\n            username = self.user.username\n        url_prefix = (('%s/' % username) if username else '')\n        url = ('/%ssubmission' % url_prefix)\n        request = self.factory.post(url, post_data)\n        request.user = authenticate(username=auth.username, password=auth.password)\n        self.response = submission(request, username=username)\n        if (auth and (self.response.status_code == 401)):\n            request.META.update(auth(request.META, self.response))\n            self.response = submission(request, username=username)\n    if forced_submission_time:\n        instance = Instance.objects.order_by('-pk').all()[0]\n        instance.date_created = forced_submission_time\n        instance.save()\n        instance.parsed_instance.save()\n    if add_uuid:\n        os.unlink(tmp_file.name)\n", "label": 1}
{"function": "\n\ndef test_schema_map(schema_app):\n    app_client = schema_app.app.test_client()\n    headers = {\n        'Content-type': 'application/json',\n    }\n    valid_object = {\n        'foo': {\n            'image_version': 'string',\n        },\n        'bar': {\n            'image_version': 'string',\n        },\n    }\n    invalid_object = {\n        'foo': 42,\n    }\n    wrong_type = app_client.post('/v1.0/test_schema_map', headers=headers, data=json.dumps(42))\n    assert (wrong_type.status_code == 400)\n    assert (wrong_type.content_type == 'application/problem+json')\n    wrong_type_response = json.loads(wrong_type.data.decode())\n    assert (wrong_type_response['title'] == 'Bad Request')\n    assert wrong_type_response['detail'].startswith(\"42 is not of type 'object'\")\n    wrong_items = app_client.post('/v1.0/test_schema_map', headers=headers, data=json.dumps(invalid_object))\n    assert (wrong_items.status_code == 400)\n    assert (wrong_items.content_type == 'application/problem+json')\n    wrong_items_response = json.loads(wrong_items.data.decode())\n    assert (wrong_items_response['title'] == 'Bad Request')\n    assert wrong_items_response['detail'].startswith(\"42 is not of type 'object'\")\n    right_type = app_client.post('/v1.0/test_schema_map', headers=headers, data=json.dumps(valid_object))\n    assert (right_type.status_code == 200)\n", "label": 1}
{"function": "\n\ndef perform_action(self, request, queryset, action):\n    method = getattr(self._model, action, None)\n    if (not method):\n        self.message_user(request, 'Illegal action.', level=messages.ERROR)\n        return\n    exist_failed_changes = False\n    exist_failed_actions = False\n    failed_changes = []\n    failed_actions = []\n    results = []\n    for entry in queryset:\n        try:\n            result = method(entry)\n            if result:\n                results.append(result)\n            entry.save()\n            LogEntry.objects.log_action(user_id=request.user.id, content_type_id=ContentType.objects.get_for_model(entry).pk, object_id=entry.id, object_repr=unicode(entry), action_flag=CHANGE, change_message='{action} action initiated by user.'.format(action=action.replace('_', ' ').strip().capitalize()))\n        except TransitionNotAllowed:\n            exist_failed_changes = True\n            failed_changes.append(entry.number)\n        except ValueError as error:\n            exist_failed_actions = True\n            failed_actions.append(error.message)\n    if exist_failed_actions:\n        msg = '\\n'.join(failed_actions)\n        self.message_user(request, msg, level=messages.ERROR)\n    if exist_failed_changes:\n        failed_ids = ' '.join(map(str, failed_changes))\n        msg = 'The state change failed for {model_name}(s) with numbers: {ids}'.format(model_name=self._model_name.lower(), ids=failed_ids)\n        self.message_user(request, msg, level=messages.ERROR)\n    if ((not exist_failed_actions) and (not exist_failed_changes)):\n        qs_count = queryset.count()\n        if (action == 'clone_into_draft'):\n            results = ', '.join((result.series_number for result in results))\n            msg = 'Successfully cloned {count} {model_name}(s) into {results}.'.format(model_name=self._model_name.lower(), count=qs_count, results=results)\n        else:\n            msg = 'Successfully changed {count} {model_name}(s).'.format(model_name=self._model_name.lower(), count=qs_count)\n        self.message_user(request, msg)\n", "label": 1}
{"function": "\n\ndef pathquery(pson, path, separator='.', missing=None, iterate=True):\n    if (isinstance(path, str) or isinstance(path, unicode)):\n        path = pathparser(path, separator=separator)\n    counter = 0\n    for token in path:\n        if ((type(pson) == dict) and pson.has_key(token)):\n            pson = pson[token]\n        elif (type(pson) == list):\n            try:\n                if (int(token) < len(pson)):\n                    pson = pson[int(token)]\n                else:\n                    return missing\n            except ValueError:\n                if iterate:\n                    return [pathquery(x, path[counter:]) for x in pson]\n                return missing\n        else:\n            return missing\n        counter += 1\n    return pson\n", "label": 1}
{"function": "\n\ndef resolve_url(to, *args, **kwargs):\n    \"\\n    Return a URL appropriate for the arguments passed.\\n    The arguments could be:\\n        * A model: the model's `get_absolute_url()` function will be called.\\n        * A view name, possibly with arguments: `urlresolvers.reverse()` will\\n          be used to reverse-resolve the name.\\n        * A URL, which will be returned as-is.\\n    \"\n    from compat import six, force_text\n    if hasattr(to, 'get_absolute_url'):\n        return to.get_absolute_url()\n    if isinstance(to, Promise):\n        to = force_text(to)\n    if isinstance(to, six.string_types):\n        if any((to.startswith(path) for path in ('./', '../'))):\n            return to\n    try:\n        return urlresolvers.reverse(to, args=args, kwargs=kwargs)\n    except urlresolvers.NoReverseMatch:\n        if callable(to):\n            raise\n        if (('/' not in to) and ('.' not in to)):\n            raise\n    return to\n", "label": 1}
{"function": "\n\ndef findAllPlugins(curConfig=None):\n    '\\n    Walks the plugins directories to find all of the plugins. If the plugin has\\n    a plugin.json file, this reads that file to determine dependencies.\\n    '\n    allPlugins = {\n        \n    }\n    for entry_point in iter_entry_points(group='girder.plugin'):\n        allPlugins[entry_point.name] = {\n            'name': entry_point.name,\n            'description': '',\n            'version': '',\n            'dependencies': set(),\n        }\n        allPlugins[entry_point.name].update(getattr(entry_point.load(), 'config', {\n            \n        }))\n    pluginDirs = getPluginDirs(curConfig)\n    if (not pluginDirs):\n        print(TerminalColor.warning('Plugin directory not found.'))\n        return allPlugins\n    for pluginDir in pluginDirs:\n        dirs = [dir for dir in os.listdir(pluginDir) if os.path.isdir(os.path.join(pluginDir, dir))]\n        for plugin in dirs:\n            data = {\n                \n            }\n            configJson = os.path.join(pluginDir, plugin, 'plugin.json')\n            configYml = os.path.join(pluginDir, plugin, 'plugin.yml')\n            if os.path.isfile(configJson):\n                with open(configJson) as conf:\n                    try:\n                        data = json.load(conf)\n                    except ValueError as e:\n                        print(TerminalColor.error(('ERROR: Plugin \"%s\": plugin.json is not valid JSON.' % plugin)))\n                        print(e)\n            elif os.path.isfile(configYml):\n                with open(configYml) as conf:\n                    try:\n                        data = yaml.safe_load(conf)\n                    except yaml.YAMLError as e:\n                        print(TerminalColor.error(('ERROR: Plugin \"%s\": plugin.yml is not valid YAML.' % plugin)))\n                        print(e)\n            allPlugins[plugin] = {\n                'name': data.get('name', plugin),\n                'description': data.get('description', ''),\n                'version': data.get('version', ''),\n                'dependencies': set(data.get('dependencies', [])),\n            }\n    return allPlugins\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    if isinstance(key, tuple):\n        (i, j) = key\n        try:\n            (i, j) = self.key2ij(key)\n            return self._smat.get((i, j), S.Zero)\n        except (TypeError, IndexError):\n            if isinstance(i, slice):\n                i = list(range(self.rows))[i]\n            elif is_sequence(i):\n                pass\n            else:\n                if (i >= self.rows):\n                    raise IndexError('Row index out of bounds')\n                i = [i]\n            if isinstance(j, slice):\n                j = list(range(self.cols))[j]\n            elif is_sequence(j):\n                pass\n            else:\n                if (j >= self.cols):\n                    raise IndexError('Col index out of bounds')\n                j = [j]\n            return self.extract(i, j)\n    if isinstance(key, slice):\n        (lo, hi) = key.indices(len(self))[:2]\n        L = []\n        for i in range(lo, hi):\n            (m, n) = divmod(i, self.cols)\n            L.append(self._smat.get((m, n), S.Zero))\n        return L\n    (i, j) = divmod(a2idx(key, len(self)), self.cols)\n    return self._smat.get((i, j), S.Zero)\n", "label": 1}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    if (not view.match_selector(locations[0], 'source.cfscript.cfc - text - meta - string - comment')):\n        return []\n    if (not SETTINGS.get('component_method_completions')):\n        return\n    _completions = []\n    try:\n        cfc_region = view.find_by_selector('meta.component-operator.extends.value.cfscript')[0]\n    except IndexError:\n        cfc_region = ''\n    if len(cfc_region):\n        extendspath = view.substr(cfc_region).replace('.', '/')\n        this_file = view.file_name()\n        dir_len = this_file.rfind('/')\n        if (not (dir_len > 0)):\n            dir_len = this_file.rfind('\\\\')\n        this_dir = this_file[:(dir_len + 1)]\n        cfc_file = ((this_dir + extendspath) + '.cfc')\n        if (not os.path.isfile(cfc_file)):\n            for folder in sublime.active_window().folders():\n                if os.path.isfile((((folder + '/') + extendspath) + '.cfc')):\n                    cfc_file = (((folder + '/') + extendspath) + '.cfc')\n                    break\n        try:\n            add_methods(cfc_file, view.substr(cfc_region).split('.')[(- 1)])\n        except UnboundLocalError:\n            pass\n        except IOError:\n            pass\n    add_methods(view.file_name(), 'this')\n    _completions.extend(completions)\n    del completions[:]\n    return _completions\n", "label": 1}
{"function": "\n\ndef _connect_put_node(self, nodes, part, path, headers, logger_thread_locals):\n    '\\n        Make a connection for a replicated object.\\n\\n        Connects to the first working node that it finds in node_iter\\n        and sends over the request headers. Returns an HTTPConnection\\n        object to handle the rest of the streaming.\\n        '\n    self.app.logger.thread_locals = logger_thread_locals\n    for node in nodes:\n        try:\n            start_time = time.time()\n            with ConnectionTimeout(self.app.conn_timeout):\n                conn = http_connect(node['ip'], node['port'], node['device'], part, 'PUT', path, headers)\n            self.app.set_node_timing(node, (time.time() - start_time))\n            with Timeout(self.app.node_timeout):\n                resp = conn.getexpect()\n            if (resp.status == HTTP_CONTINUE):\n                conn.resp = None\n                conn.node = node\n                return conn\n            elif (is_success(resp.status) or (resp.status in (HTTP_CONFLICT, HTTP_UNPROCESSABLE_ENTITY))):\n                conn.resp = resp\n                conn.node = node\n                return conn\n            elif ((headers['If-None-Match'] is not None) and (resp.status == HTTP_PRECONDITION_FAILED)):\n                conn.resp = resp\n                conn.node = node\n                return conn\n            elif (resp.status == HTTP_INSUFFICIENT_STORAGE):\n                self.app.error_limit(node, _('ERROR Insufficient Storage'))\n            elif is_server_error(resp.status):\n                self.app.error_occurred(node, (_('ERROR %(status)d Expect: 100-continue From Object Server') % {\n                    'status': resp.status,\n                }))\n        except (Exception, Timeout):\n            self.app.exception_occurred(node, _('Object'), (_('Expect: 100-continue on %s') % path))\n", "label": 1}
{"function": "\n\ndef run_subcommand(self, argv):\n    subcommand = self.command_manager.find_command(argv)\n    (cmd_factory, cmd_name, sub_argv) = subcommand\n    cmd = cmd_factory(self, self.options)\n    err = None\n    result = 1\n    try:\n        self.prepare_to_run_command(cmd)\n        full_name = (cmd_name if self.interactive_mode else ' '.join([self.NAME, cmd_name]))\n        cmd_parser = cmd.get_parser(full_name)\n        return run_command(cmd, cmd_parser, sub_argv)\n    except Exception as err:\n        if (self.options.verbose_level >= self.DEBUG_LEVEL):\n            self.log.exception(unicode(err))\n        else:\n            self.log.error(unicode(err))\n        try:\n            self.clean_up(cmd, result, err)\n        except Exception as err2:\n            if (self.options.verbose_level >= self.DEBUG_LEVEL):\n                self.log.exception(unicode(err2))\n            else:\n                self.log.error(_('Could not clean up: %s'), unicode(err2))\n        if (self.options.verbose_level >= self.DEBUG_LEVEL):\n            raise\n    else:\n        try:\n            self.clean_up(cmd, result, None)\n        except Exception as err3:\n            if (self.options.verbose_level >= self.DEBUG_LEVEL):\n                self.log.exception(unicode(err3))\n            else:\n                self.log.error(_('Could not clean up: %s'), unicode(err3))\n    return result\n", "label": 1}
{"function": "\n\ndef test_remove_action(self):\n    global rec, next_done\n    next_done = 0\n    node = CocosNode()\n    name1 = '1'\n    action1 = UAction(name1)\n    name2 = '2'\n    action2 = UAction(name2)\n    a1_copy = node.do(action1)\n    a2_copy = node.do(action2)\n    assert (len(node.actions) == 2)\n    rec = []\n    node.remove_action(a1_copy)\n    recx = [e for e in rec if (e[0] == name1)]\n    assert (recx[0] == (name1, 'stop'))\n    assert (len(recx) == 1)\n    rec = []\n    dt = 0.1\n    node._step(dt)\n    assert (len(node.actions) == 1)\n    assert (a2_copy in node.actions)\n    recx = [e for e in rec if (e[0] == name1)]\n    assert (len(recx) == 0)\n", "label": 1}
{"function": "\n\ndef check_api_doc(basename, verbose, only_documented, has_submodules):\n    module_name = basename.replace('_', '.')\n    try:\n        module = __import__(module_name, globals(), {\n            \n        }, ['__all__'])\n    except ImportError as e:\n        print(('Skipping %s (%s)' % (basename, e)))\n        return\n    all = getattr(module, '__all__', None)\n    if (not all):\n        print((\"Warning: %s doesn't define __all__, using exported symbols.\" % module_name))\n        all = get_default_symbols(module, only_documented, has_submodules)\n    no_apidoc = getattr(module, '__no_apidoc__', None)\n    if no_apidoc:\n        if isinstance(no_apidoc, basestring):\n            no_apidoc = [s.strip() for s in no_apidoc.split()]\n        all = list((set(all) - set(no_apidoc)))\n    (symbols, keywords) = get_sphinx_documented_symbols((basename + '.rst'))\n    for symbol in sorted(all):\n        if (symbol in symbols):\n            if verbose:\n                print((' - OK %14s :: %s' % (keywords[symbols.index(symbol)], symbol)))\n        else:\n            value = getattr(module, symbol)\n            cls = getattr(value, '__class__', None)\n            keyword = 'data'\n            if (cls.__name__ in ('function', 'instancemethod')):\n                keyword = 'function'\n            elif (cls.__name__ == 'module'):\n                keyword = 'module'\n            else:\n                keyword = 'class'\n            print((' * .. %14s :: %s' % (('auto' + keyword), symbol)))\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef querySummary(self, header, connection_ids=None, global_reservation_ids=None, request_info=None):\n    log.msg(('QuerySummary request from %s. CID: %s. GID: %s' % (header.requester_nsa, connection_ids, global_reservation_ids)), system=LOG_SYSTEM)\n    try:\n        if connection_ids:\n            conns = (yield database.ServiceConnection.find(where=['requester_nsa = ? AND connection_id IN ?', header.requester_nsa, tuple(connection_ids)]))\n        elif global_reservation_ids:\n            conns = (yield database.ServiceConnection.find(where=['requester_nsa = ? AND global_reservation_ids IN ?', header.requester_nsa, tuple(global_reservation_ids)]))\n        else:\n            conns = (yield database.ServiceConnection.find(where=['requester_nsa = ?', header.requester_nsa]))\n        reservations = []\n        for c in conns:\n            source_stp = nsa.STP(c.source_network, c.source_port, c.source_label)\n            dest_stp = nsa.STP(c.dest_network, c.dest_port, c.dest_label)\n            schedule = nsa.Schedule(c.start_time, c.end_time)\n            sd = nsa.Point2PointService(source_stp, dest_stp, c.bandwidth, cnt.BIDIRECTIONAL, False, None)\n            criteria = nsa.QueryCriteria(c.revision, schedule, sd)\n            sub_conns = (yield self.getSubConnectionsByConnectionKey(c.id))\n            if (len(sub_conns) == 0):\n                data_plane_status = (False, 0, False)\n            else:\n                aggr_active = all([sc.data_plane_active for sc in sub_conns])\n                aggr_version = (max([sc.data_plane_version for sc in sub_conns]) or 0)\n                aggr_consistent = all([sc.data_plane_consistent for sc in sub_conns])\n                data_plane_status = (aggr_active, aggr_version, aggr_consistent)\n            states = (c.reservation_state, c.provision_state, c.lifecycle_state, data_plane_status)\n            notification_id = self.getNotificationId()\n            result_id = 0\n            ci = nsa.ConnectionInfo(c.connection_id, c.global_reservation_id, c.description, cnt.EVTS_AGOLE, [criteria], self.nsa_.urn(), c.requester_nsa, states, notification_id, result_id)\n            reservations.append(ci)\n        self.parent_requester.querySummaryConfirmed(header, reservations)\n    except Exception as e:\n        log.msg(('Error during querySummary request: %s' % str(e)), system=LOG_SYSTEM)\n        raise e\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _correct_signature_from_indices(data, indices, free, dum, inverse=False):\n    '\\n        Utility function to correct the values inside the components data\\n        ndarray according to whether indices are covariant or contravariant.\\n\\n        It uses the metric matrix to lower values of covariant indices.\\n        '\n    numpy = import_module('numpy')\n    for (i, indx) in enumerate(indices):\n        if ((not indx.is_up) and (not inverse)):\n            data = _TensorDataLazyEvaluator._flip_index_by_metric(data, indx._tensortype.data, i)\n        elif ((not indx.is_up) and inverse):\n            data = _TensorDataLazyEvaluator._flip_index_by_metric(data, _TensorDataLazyEvaluator.inverse_matrix(indx._tensortype.data), i)\n    if (len(dum) > 0):\n        axes1 = []\n        axes2 = []\n        for (i, indx1) in enumerate(indices):\n            try:\n                nd = indices[:i].index((- indx1))\n            except ValueError:\n                continue\n            axes1.append(nd)\n            axes2.append(i)\n        for (ax1, ax2) in zip(axes1, axes2):\n            data = numpy.trace(data, axis1=ax1, axis2=ax2)\n    return data\n", "label": 1}
{"function": "\n\ndef _do_update_node(self, node_id, values):\n    with _session_for_write():\n        query = model_query(models.Node)\n        query = add_identity_filter(query, node_id)\n        try:\n            ref = query.with_lockmode('update').one()\n        except NoResultFound:\n            raise exception.NodeNotFound(node=node_id)\n        if (values.get('instance_uuid') and ref.instance_uuid):\n            raise exception.NodeAssociated(node=ref.uuid, instance=ref.instance_uuid)\n        if ('provision_state' in values):\n            values['provision_updated_at'] = timeutils.utcnow()\n            if (values['provision_state'] == states.INSPECTING):\n                values['inspection_started_at'] = timeutils.utcnow()\n                values['inspection_finished_at'] = None\n            elif ((ref.provision_state == states.INSPECTING) and (values['provision_state'] == states.MANAGEABLE)):\n                values['inspection_finished_at'] = timeutils.utcnow()\n                values['inspection_started_at'] = None\n            elif ((ref.provision_state == states.INSPECTING) and (values['provision_state'] == states.INSPECTFAIL)):\n                values['inspection_started_at'] = None\n        ref.update(values)\n    return ref\n", "label": 1}
{"function": "\n\ndef test_mergeComps_K5_D3_withELBO_kA0(self, K=5, D=3):\n    SS = self.makeSuffStatBagAndFillWithOnes(K, D)\n    self.addELBOtoSuffStatBag(SS, K)\n    SS.mergeComps(0, 1)\n    (s, N, x, xxT) = self.getExpectedMergedFields(K, D)\n    assert (SS.K == (K - 1))\n    assert (SS._ELBOTerms.K == (K - 1))\n    assert (SS._MergeTerms.K == (K - 1))\n    assert np.allclose(SS.s, s)\n    assert np.allclose(SS.N, N)\n    assert np.allclose(SS.x, x)\n    assert np.allclose(SS.xxT, xxT)\n    assert np.allclose(SS.getELBOTerm('Elogz'), [2.0, 1, 1, 1])\n    assert np.allclose(SS.getELBOTerm('Econst'), 1.0)\n    assert np.all(np.isnan(SS._MergeTerms.Elogz[0, 1:]))\n    assert np.all(np.isnan(SS._MergeTerms.Elogz[:0, 0]))\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@gof.local_optimizer([T.add])\ndef local_IncSubtensor_serialize(node):\n    \"\\n    When using Subtensor, gradient graphs can be ugly.\\n\\n    If we ask for grad(f(a[0]), a), we are going to get something like\\n\\n        IncSubtensor(Elemwise{second}(a, 0), g(f(a[0])), [0])\\n\\n    This might be ugly, but at least it's as fast as you could want.\\n    If we ask for grad(f(a[0], a[1], a[2]), a), it's much worse...\\n\\n        Elemwise{Add}\\n            IncSubtensor(Elemwise{second}(a, 0), g(f(a[0])), [0])\\n            IncSubtensor(Elemwise{second}(a, 0), g(f(a[1])), [1])\\n            IncSubtensor(Elemwise{second}(a, 0), g(f(a[2])), [2])\\n\\n    This is much worse because this time we have to produce 3 matrices\\n    the size of 'a', just so we can add them together.\\n\\n    This Op rearranges IncSubtensor's that all work on the same\\n    initial argument (here, Elemwise{second}(a,0)) into a chain.  The\\n    advantage of the chain structure is that each one can be optimized\\n    later in the pipeline to operate inplace.\\n\\n    Ideally, the op will do something like this:\\n\\n    #\\n    #  add(x, incsubtensor(b, c), incsubtensor(b, d))\\n    #  -> incsubtensor(incsubtensor(add(x,b,b), c), d)\\n\\n    \"\n\n    def movable(i):\n        return (i.owner and isinstance(i.owner.op, (IncSubtensor, AdvancedIncSubtensor1, AdvancedIncSubtensor)) and (i.type == o_type) and (len(i.clients) == 1) and (not i.owner.op.set_instead_of_inc))\n    if (node.op == T.add):\n        o_type = node.outputs[0].type\n        movable_inputs = [i for i in node.inputs if movable(i)]\n        if movable_inputs:\n            new_inputs = ([i for i in node.inputs if (not movable(i))] + [mi.owner.inputs[0] for mi in movable_inputs])\n            new_add = T.add(*new_inputs)\n            copy_stack_trace(node.outputs[0], new_add)\n            tip = new_add\n            for mi in movable_inputs:\n                assert (tip.type == o_type)\n                assert (tip.type == mi.owner.inputs[0].type)\n                tip = mi.owner.op(tip, *mi.owner.inputs[1:])\n                copy_stack_trace((node.outputs + mi.owner.outputs), tip)\n            return [tip]\n", "label": 1}
{"function": "\n\ndef filter_scanline(type, line, fo, prev=None):\n    'Apply a scanline filter to a scanline.  `type` specifies the\\n    filter type (0 to 4); `line` specifies the current (unfiltered)\\n    scanline as a sequence of bytes; `prev` specifies the previous\\n    (unfiltered) scanline as a sequence of bytes. `fo` specifies the\\n    filter offset; normally this is size of a pixel in bytes (the number\\n    of bytes per sample times the number of channels), but when this is\\n    < 1 (for bit depths < 8) then the filter offset is 1.\\n    '\n    assert (0 <= type < 5)\n    out = array('B', [type])\n\n    def sub():\n        ai = (- fo)\n        for x in line:\n            if (ai >= 0):\n                x = ((x - line[ai]) & 255)\n            out.append(x)\n            ai += 1\n\n    def up():\n        for (i, x) in enumerate(line):\n            x = ((x - prev[i]) & 255)\n            out.append(x)\n\n    def average():\n        ai = (- fo)\n        for (i, x) in enumerate(line):\n            if (ai >= 0):\n                x = ((x - ((line[ai] + prev[i]) >> 1)) & 255)\n            else:\n                x = ((x - (prev[i] >> 1)) & 255)\n            out.append(x)\n            ai += 1\n\n    def paeth():\n        ai = (- fo)\n        for (i, x) in enumerate(line):\n            a = 0\n            b = prev[i]\n            c = 0\n            if (ai >= 0):\n                a = line[ai]\n                c = prev[ai]\n            p = ((a + b) - c)\n            pa = abs((p - a))\n            pb = abs((p - b))\n            pc = abs((p - c))\n            if ((pa <= pb) and (pa <= pc)):\n                Pr = a\n            elif (pb <= pc):\n                Pr = b\n            else:\n                Pr = c\n            x = ((x - Pr) & 255)\n            out.append(x)\n            ai += 1\n    if (not prev):\n        if (type == 2):\n            type = 0\n        elif (type == 3):\n            prev = ([0] * len(line))\n        elif (type == 4):\n            type = 1\n    if (type == 0):\n        out.extend(line)\n    elif (type == 1):\n        sub()\n    elif (type == 2):\n        up()\n    elif (type == 3):\n        average()\n    else:\n        paeth()\n    return out\n", "label": 1}
{"function": "\n\ndef zmq_device(self):\n    '\\n        Multiprocessing target for the zmq queue device\\n        '\n    self.__setup_signals()\n    salt.utils.appendproctitle('MWorkerQueue')\n    self.context = zmq.Context(self.opts['worker_threads'])\n    self.uri = 'tcp://{interface}:{ret_port}'.format(**self.opts)\n    self.clients = self.context.socket(zmq.ROUTER)\n    if ((self.opts['ipv6'] is True) and hasattr(zmq, 'IPV4ONLY')):\n        self.clients.setsockopt(zmq.IPV4ONLY, 0)\n    if (HAS_ZMQ_MONITOR and self.opts['zmq_monitor']):\n        import threading\n        self._monitor = ZeroMQSocketMonitor(self.clients)\n        t = threading.Thread(target=self._monitor.start_poll)\n        t.start()\n    self.workers = self.context.socket(zmq.DEALER)\n    if (self.opts.get('ipc_mode', '') == 'tcp'):\n        self.w_uri = 'tcp://127.0.0.1:{0}'.format(self.opts.get('tcp_master_workers', 4515))\n    else:\n        self.w_uri = 'ipc://{0}'.format(os.path.join(self.opts['sock_dir'], 'workers.ipc'))\n    log.info('Setting up the master communication server')\n    self.clients.bind(self.uri)\n    self.workers.bind(self.w_uri)\n    while True:\n        if (self.clients.closed or self.workers.closed):\n            break\n        try:\n            zmq.device(zmq.QUEUE, self.clients, self.workers)\n        except zmq.ZMQError as exc:\n            if (exc.errno == errno.EINTR):\n                continue\n            raise exc\n        except (KeyboardInterrupt, SystemExit):\n            break\n", "label": 1}
{"function": "\n\ndef _parse_settings_bond_4(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond4.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '4',\n    }\n    for binding in ['miimon', 'downdelay', 'updelay', 'lacp_rate', 'ad_select']:\n        if (binding in opts):\n            if (binding == 'lacp_rate'):\n                if (opts[binding] == 'fast'):\n                    opts.update({\n                        binding: '1',\n                    })\n                if (opts[binding] == 'slow'):\n                    opts.update({\n                        binding: '0',\n                    })\n                valid = ['fast', '1', 'slow', '0']\n            else:\n                valid = ['integer']\n            try:\n                int(opts[binding])\n                bond.update({\n                    binding: opts[binding],\n                })\n            except ValueError:\n                _raise_error_iface(iface, binding, valid)\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({\n                binding: bond_def[binding],\n            })\n    if ('use_carrier' in opts):\n        if (opts['use_carrier'] in _CONFIG_TRUE):\n            bond.update({\n                'use_carrier': '1',\n            })\n        elif (opts['use_carrier'] in _CONFIG_FALSE):\n            bond.update({\n                'use_carrier': '0',\n            })\n        else:\n            valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({\n            'use_carrier': bond_def['use_carrier'],\n        })\n    if ('hashing-algorithm' in opts):\n        valid = ['layer2', 'layer2+3', 'layer3+4']\n        if (opts['hashing-algorithm'] in valid):\n            bond.update({\n                'xmit_hash_policy': opts['hashing-algorithm'],\n            })\n        else:\n            _raise_error_iface(iface, 'hashing-algorithm', valid)\n    return bond\n", "label": 1}
{"function": "\n\ndef write_heading(self, heading, level=0, index_id=None, label=None):\n    assert self.stream\n    assert heading, 'Heading should not be empty'\n    assert (0 <= level < len(self.heading_styles))\n    if (level >= len(self.heading_styles)):\n        level = (len(self.heading_styles) - 1)\n    heading_size = len(heading)\n    heading_style = self.heading_styles[level]\n    if ((level == 0) and (heading_size < 70)):\n        heading_size = 70\n    separator = (heading_style * heading_size)\n    if index_id:\n        if isinstance(index_id, (list, tuple)):\n            index_id = ', '.join(index_id)\n        self.stream.write(('.. index:: %s\\n\\n' % index_id))\n    if label:\n        self.stream.write(('.. _%s:\\n\\n' % label))\n    if (level == 0):\n        self.stream.write(('%s\\n' % separator))\n    self.stream.write(('%s\\n' % heading))\n    self.stream.write(('%s\\n' % separator))\n    self.stream.write('\\n')\n", "label": 1}
{"function": "\n\ndef run(self, tree):\n    'Apply inline patterns to a parsed Markdown tree.\\n\\n        Iterate over ElementTree, find elements with inline tag, apply inline\\n        patterns and append newly created Elements to tree.  If you don\\'t\\n        want process your data with inline paterns, instead of normal string,\\n        use subclass AtomicString:\\n\\n            node.text = markdown.AtomicString(\"data won\\'t be processed with inline patterns\")\\n\\n        Arguments:\\n\\n        * markdownTree: ElementTree object, representing Markdown tree.\\n\\n        Returns: ElementTree object with applied inline patterns.\\n\\n        '\n    self.stashed_nodes = {\n        \n    }\n    stack = [tree]\n    while stack:\n        currElement = stack.pop()\n        insertQueue = []\n        for child in currElement.getchildren():\n            if (child.text and (not isinstance(child.text, markdown.AtomicString))):\n                text = child.text\n                child.text = None\n                lst = self.__processPlaceholders(self.__handleInline(text), child)\n                stack += lst\n                insertQueue.append((child, lst))\n            if child.getchildren():\n                stack.append(child)\n        for (element, lst) in insertQueue:\n            if element.text:\n                element.text = markdown.inlinepatterns.handleAttributes(element.text, element)\n            i = 0\n            for newChild in lst:\n                if newChild.tail:\n                    newChild.tail = markdown.inlinepatterns.handleAttributes(newChild.tail, element)\n                if newChild.text:\n                    newChild.text = markdown.inlinepatterns.handleAttributes(newChild.text, newChild)\n                element.insert(i, newChild)\n                i += 1\n    return tree\n", "label": 1}
{"function": "\n\ndef folder(self, name, parentId, parentType, description=None, public=True, folders=None, access=None, debug=False):\n    ret = {\n        \n    }\n    assert (parentType in ['collection', 'folder', 'user']), 'parentType must be collection or folder'\n    r = FolderResource(self, parentType, parentId)\n    valid_fields = [('name', name), ('description', description), ('parentType', parentType), ('parentId', parentId)]\n    if (self.module.params['state'] == 'present'):\n        if r.name_exists(name):\n            ret = r.update_by_name(name, {k: v for (k, v) in valid_fields if (v is not None)})\n        else:\n            valid_fields = (valid_fields + [('public', public)])\n            ret = r.create({k: v for (k, v) in valid_fields if (v is not None)})\n        if (folders is not None):\n            self._process_folders(folders, ret['_id'], 'folder')\n        if (access is not None):\n            _id = ret['_id']\n            ret['access'] = self._access(r, access, _id, public=public)\n    elif (self.module.params['state'] == 'absent'):\n        ret = r.delete_by_name(name)\n    return ret\n", "label": 1}
{"function": "\n\ndef apply_func(self, filters, type, args, kwargs=None, cache_key=None):\n    'Apply a filter that is not a \"stream in, stream out\" transform (i.e.\\n        like the input() and output() filter methods).  Instead, the filter\\n        method is given the arguments in ``args`` and should then produce an\\n        output stream. This is used, e.g., for the concat() and open() filter\\n        methods.\\n\\n        Only one such filter can run per operation.\\n\\n        ``cache_key`` may be a list of additional values to use as the cache\\n        key, in addition to the default key (the filter and arguments).\\n        '\n    assert (type in self.VALID_FUNCS)\n    log.debug('Need to run method \"%s\" of one of the filters (%s) with args=%s, kwargs=%s', type, filters, args, kwargs)\n    filters = [f for f in filters if getattr(f, type, None)]\n    if (not filters):\n        log.debug(('No filters have a \"%s\" method' % type))\n        raise NoFilters()\n    if (len(filters) > 1):\n        raise MoreThanOneFilterError(('These filters cannot be combined: %s' % ', '.join([f.name for f in filters])), filters)\n    kwargs_final = self.kwargs.copy()\n    kwargs_final.update((kwargs or {\n        \n    }))\n\n    def func():\n        filter = filters[0]\n        out = StringIO('')\n        log.debug('Running method \"%s\" of %s with args=%s, kwargs=%s', type, filter, args, kwargs)\n        getattr(filter, type)(out, *args, **kwargs_final)\n        return out\n    additional_cache_keys = []\n    if kwargs_final:\n        for filter in filters:\n            additional_cache_keys += filter.get_additional_cache_keys(**kwargs_final)\n    key = ('hunk', args, tuple(filters), type, (cache_key or []), additional_cache_keys)\n    return self._wrap_cache(key, func)\n", "label": 1}
{"function": "\n\n@app.route('/checks/id/<checkid>/owner', methods=['GET', 'POST', 'DELETE'])\n@login_required\ndef check_owner(checkid):\n    'claim or unclaim a given check'\n    if (request.method == 'GET'):\n        check = r.table('checks').get(checkid).run(rdb.conn)\n        if check:\n            check['id'] = str(check['id'])\n            return jsonify({\n                'check': check,\n            })\n        else:\n            abort(404)\n    elif (request.method == 'POST'):\n        if (not request.json):\n            abort(400)\n        try:\n            if request.json.get('owner'):\n                q = r.table('checks').get(checkid).update({\n                    'owner': str(request.json['owner']),\n                }).run(rdb.conn)\n            else:\n                abort(400)\n            if (q['replaced'] != 0):\n                return jsonify({\n                    'success': True,\n                })\n            else:\n                abort(404)\n        except Exception as err:\n            logger.error(err)\n            abort(400)\n    elif (request.method == 'DELETE'):\n        try:\n            q = r.table('checks').get(checkid).update({\n                'owner': '',\n            }).run(rdb.conn)\n            if (q['replaced'] != 0):\n                return jsonify({\n                    'success': True,\n                })\n            else:\n                abort(404)\n        except Exception as err:\n            logger.error(err)\n            abort(400)\n", "label": 1}
{"function": "\n\ndef run(cosmo, data, command_line):\n    '\\n    Sample with the CosmoHammer\\n\\n    '\n    parameter_names = data.get_mcmc_parameters(['varying'])\n    (is_flat, is_bound) = sampler.check_flat_bound_priors(data.mcmc_parameters, parameter_names)\n    if (not is_flat):\n        raise io_mp.ConfigurationError(('The Cosmo Hammer is only available with flat ' + 'priors. Sorry!'))\n    if (not is_bound):\n        raise io_mp.ConfigurationError((('The Cosmo Hammer is only available for bound ' + 'parameters. Set reasonable bounds for them in the \".param\"') + 'file.'))\n    params = []\n    for parameter in parameter_names:\n        params.append(data.mcmc_parameters[parameter]['initial'])\n    params = np.array(params)\n    CH_folder = os.path.join(command_line.folder, CH_subfolder)\n    if (not os.path.exists(CH_folder)):\n        os.makedirs(CH_folder)\n    chain = LikelihoodComputationChain(min=params[:, 1], max=params[:, 2])\n    chain.addCoreModule(data)\n    chain.addCoreModule(cosmo)\n    for likelihood in data.lkl.itervalues():\n        chain.addLikelihoodModule(likelihood)\n    chain_name = [a for a in command_line.folder.split(os.path.sep) if a][(- 1)]\n    file_prefix = os.path.join(command_line.folder, CH_subfolder, chain_name)\n    data.CH_arguments = {\n        \n    }\n    for arg in CH_user_arguments:\n        value = getattr(command_line, (CH_prefix + arg))\n        if (value != (- 1)):\n            data.CH_arguments[arg] = value\n    with open((file_prefix + name_arguments), 'w') as arg_file:\n        for arg in data.CH_arguments:\n            arg_file.write((' = '.join([str(arg), str(data.CH_arguments[arg])]) + '\\n'))\n    derived_util = DerivedUtil(file_prefix)\n    try:\n        num_threads = int(os.environ['OMP_NUM_THREADS'])\n    except KeyError:\n        warnings.warn('The environment variable OMP_NUM_THREADS is not set. To run the Cosmo Hammer meaningfully, you should better set it to something! Defaulting to 1 for now.')\n        num_threads = 1\n    sampler_hammer = CosmoHammerSampler(params=params, likelihoodComputationChain=chain, filePrefix=file_prefix, walkersRatio=50, burninIterations=10, sampleIterations=30, storageUtil=derived_util, threadCount=num_threads, **data.CH_arguments)\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG)\n    logging.getLogger().addHandler(console_handler)\n    sampler_hammer.startSampling()\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_2d_time_lowmem():\n    ' low memory reading/writing of 2D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read_lowmem(os.path.join(DATA_DIR, 'rnmrtk_2d', 'time_2d.sec'))\n    assert (data.shape == (332, 1500))\n    assert (np.abs((data[(0, 1)].real - 360.07)) <= 0.01)\n    assert (np.abs((data[(0, 1)].imag - (- 223.2))) <= 0.01)\n    assert (np.abs((data[(10, 18)].real - 17.93)) <= 0.01)\n    assert (np.abs((data[(10, 18)].imag - (- 67.2))) <= 0.01)\n    assert (dic['sw'][1] == 50000.0)\n    assert (dic['sf'][1] == 125.69)\n    assert (dic['ppm'][1] == 55.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 50.65)\n    assert (dic['ppm'][0] == 120.0)\n    lowmem_write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef run(self, edit, reverse=False):\n    for region in self.view.sel():\n        if (region.a != region.b):\n            continue\n        line = self.view.line(region)\n        line_content = self.view.substr(line)\n        new_line = line_content\n        m = self.line_pattern_re.match(new_line)\n        if (not m):\n            return\n        tab_str = (self.view.settings().get('tab_size', 4) * ' ')\n        sep_str = (' ' if m.group(4) else '')\n        prev_line = self.view.line(sublime.Region((line.begin() - 1), (line.begin() - 1)))\n        prev_line_content = self.view.substr(prev_line)\n        prev_prev_line = self.view.line(sublime.Region((prev_line.begin() - 1), (prev_line.begin() - 1)))\n        prev_prev_line_content = self.view.substr(prev_prev_line)\n        if (not reverse):\n            new_line = self.bullet_pattern_re.sub(((tab_str + sep_str) + '\\\\1'), new_line)\n            if prev_line_content:\n                new_line = ('\\n' + new_line)\n        else:\n            if (not new_line.startswith(tab_str)):\n                continue\n            new_line = re.sub(((tab_str + sep_str) + self.bullet_pattern), '\\\\1', new_line)\n            if prev_line_content:\n                new_line = ('\\n' + new_line)\n            else:\n                prev_spaces = self.spaces_re.match(prev_prev_line_content).group(0)\n                spaces = self.spaces_re.match(new_line).group(0)\n                if (prev_spaces == spaces):\n                    line = sublime.Region((line.begin() - 1), line.end())\n        endings = ['.', ')']\n        if self.view.settings().get('list_indent_auto_switch_bullet', True):\n            bullets = self.view.settings().get('list_indent_bullets', ['*', '-', '+'])\n\n            def change_bullet(m):\n                bullet = m.group(1)\n                try:\n                    return bullets[((bullets.index(bullet) + (1 if (not reverse) else (- 1))) % len(bullets))]\n                except ValueError:\n                    pass\n                n = m.group(2)\n                ending = endings[((endings.index(m.group(4)) + (1 if (not reverse) else (- 1))) % len(endings))]\n                if n.isdigit():\n                    return ('${1:a}' + ending)\n                elif (n != '#'):\n                    return ('${1:0}' + ending)\n                return (m.group(2) + ending)\n            new_line = self.bullet_pattern_re.sub(change_bullet, new_line)\n        self.view.replace(edit, line, '')\n        self.view.run_command('insert_snippet', {\n            'contents': new_line,\n        })\n", "label": 1}
{"function": "\n\ndef getDerMatrix(self, x=None, z=None, mode=None, der=None):\n    self.checkInputGetDerMatrix(x, z, mode, der)\n    if (not (x is None)):\n        assert (x.shape[1] == 1), 'periodic covariance can only be used for 1d data'\n    if (not (z is None)):\n        assert (z.shape[1] == 1), 'periodic covariance can only be used for 1d data'\n    ell = np.exp(self.hyp[0])\n    p = np.exp(self.hyp[1])\n    sf2 = np.exp((2.0 * self.hyp[2]))\n    if (mode == 'self_test'):\n        (nn, D) = z.shape\n        A = np.zeros((nn, 1))\n    elif (mode == 'train'):\n        A = np.sqrt(spdist.cdist(x, x, 'sqeuclidean'))\n    elif (mode == 'cross'):\n        A = np.sqrt(spdist.cdist(x, z, 'sqeuclidean'))\n    A = ((np.pi * A) / p)\n    if (der == 0):\n        A = old_div(np.sin(A), ell)\n        A = (A * A)\n        A = (((4.0 * sf2) * np.exp(((- 2.0) * A))) * A)\n    elif (der == 1):\n        R = old_div(np.sin(A), ell)\n        A = ((((((4 * sf2) / ell) * np.exp((((- 2.0) * R) * R))) * R) * np.cos(A)) * A)\n    elif (der == 2):\n        A = old_div(np.sin(A), ell)\n        A = (A * A)\n        A = ((2.0 * sf2) * np.exp(((- 2.0) * A)))\n    else:\n        raise Exception('Wrong derivative index in covPeriodic')\n    return A\n", "label": 1}
{"function": "\n\ndef _sparse_fruchterman_reingold(A, k=None, pos=None, fixed=None, iterations=50, dim=2):\n    try:\n        import numpy as np\n    except ImportError:\n        m = '_sparse_fruchterman_reingold() requires numpy: http://scipy.org/'\n        raise ImportError(m)\n    try:\n        (nnodes, _) = A.shape\n    except AttributeError:\n        msg = 'fruchterman_reingold() takes an adjacency matrix as input'\n        raise nx.NetworkXError(msg)\n    try:\n        from scipy.sparse import spdiags, coo_matrix\n    except ImportError:\n        msg = '_sparse_fruchterman_reingold() scipy numpy: http://scipy.org/ '\n        raise ImportError(msg)\n    try:\n        A = A.tolil()\n    except:\n        A = coo_matrix(A).tolil()\n    if (pos is None):\n        pos = np.asarray(np.random.random((nnodes, dim)), dtype=A.dtype)\n    else:\n        pos = pos.astype(A.dtype)\n    if (fixed is None):\n        fixed = []\n    if (k is None):\n        k = np.sqrt((1.0 / nnodes))\n    t = 0.1\n    dt = (t / float((iterations + 1)))\n    displacement = np.zeros((dim, nnodes))\n    for iteration in range(iterations):\n        displacement *= 0\n        for i in range(A.shape[0]):\n            if (i in fixed):\n                continue\n            delta = (pos[i] - pos).T\n            distance = np.sqrt((delta ** 2).sum(axis=0))\n            distance = np.where((distance < 0.01), 0.01, distance)\n            Ai = np.asarray(A.getrowview(i).toarray())\n            displacement[:, i] += (delta * (((k * k) / (distance ** 2)) - ((Ai * distance) / k))).sum(axis=1)\n        length = np.sqrt((displacement ** 2).sum(axis=0))\n        length = np.where((length < 0.01), 0.1, length)\n        pos += ((displacement * t) / length).T\n        t -= dt\n    return pos\n", "label": 1}
{"function": "\n\ndef visit_Index(self, expr):\n    arr = self.visit_expr(expr.value)\n    if isinstance(expr.index.type, ScalarT):\n        index_exprs = [expr.index]\n    else:\n        assert isinstance(expr.index.type, TupleT), ('Unexpected index %s : %s' % (expr.index, expr.index.type))\n        if isinstance(expr.index, Tuple):\n            index_exprs = expr.index.elts\n        else:\n            index_exprs = [TupleProj(expr.index, i, type=t) for (i, t) in enumerate(expr.index.type.elt_types)]\n    assert all((isinstance(idx_expr.type, ScalarT) for idx_expr in index_exprs)), ('Expected all indices to be scalars but got %s' % (index_exprs,))\n    indices = [self.visit_expr(idx_expr) for idx_expr in index_exprs]\n    if isinstance(expr.value.type, PtrT):\n        assert (len(indices) == 1), (\"Can't index into pointer using %d indices (%s)\" % (len(indices), index_exprs))\n        raw_ptr = ('%s.raw_ptr' % arr)\n        offset = indices[0]\n    else:\n        assert isinstance(expr.value.type, ArrayT)\n        offset = self.fresh_var('int64_t', 'offset', ('%s.offset' % arr))\n        for (i, idx) in enumerate(indices):\n            stride = ('%s.strides[%d]' % (arr, i))\n            self.append(('%s += %s * %s;' % (offset, idx, stride)))\n        raw_ptr = ('%s.data.raw_ptr' % arr)\n    return ('%s[%s]' % (raw_ptr, offset))\n", "label": 1}
{"function": "\n\ndef info(name):\n    \"\\n    Return user information\\n\\n    :param str name:\\n        Username for which to display information\\n\\n    :returns:\\n        A dictionary containing user information\\n            - fullname\\n            - username\\n            - SID\\n            - passwd (will always return None)\\n            - comment (same as description, left here for backwards compatibility)\\n            - description\\n            - active\\n            - logonscript\\n            - profile\\n            - home\\n            - homedrive\\n            - groups\\n            - password_changed\\n            - successful_logon_attempts\\n            - failed_logon_attempts\\n            - last_logon\\n            - account_disabled\\n            - account_locked\\n            - password_never_expires\\n            - disallow_change_password\\n            - gid\\n    :rtype: dict\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' user.info jsnuffy\\n    \"\n    ret = {\n        \n    }\n    items = {\n        \n    }\n    try:\n        items = win32net.NetUserGetInfo(None, name, 4)\n    except win32net.error:\n        pass\n    if items:\n        groups = []\n        try:\n            groups = win32net.NetUserGetLocalGroups(None, name)\n        except win32net.error:\n            pass\n        ret['fullname'] = items['full_name']\n        ret['name'] = items['name']\n        ret['uid'] = win32security.ConvertSidToStringSid(items['user_sid'])\n        ret['passwd'] = items['password']\n        ret['comment'] = items['comment']\n        ret['description'] = items['comment']\n        ret['active'] = (not bool((items['flags'] & win32netcon.UF_ACCOUNTDISABLE)))\n        ret['logonscript'] = items['script_path']\n        ret['profile'] = items['profile']\n        ret['failed_logon_attempts'] = items['bad_pw_count']\n        ret['successful_logon_attempts'] = items['num_logons']\n        secs = (time.mktime(datetime.now().timetuple()) - items['password_age'])\n        ret['password_changed'] = datetime.fromtimestamp(secs).strftime('%Y-%m-%d %H:%M:%S')\n        if (items['last_logon'] == 0):\n            ret['last_logon'] = 'Never'\n        else:\n            ret['last_logon'] = datetime.fromtimestamp(items['last_logon']).strftime('%Y-%m-%d %H:%M:%S')\n        ret['expiration_date'] = datetime.fromtimestamp(items['acct_expires']).strftime('%Y-%m-%d %H:%M:%S')\n        ret['expired'] = (items['password_expired'] == 1)\n        if (not ret['profile']):\n            ret['profile'] = _get_userprofile_from_registry(name, ret['uid'])\n        ret['home'] = items['home_dir']\n        ret['homedrive'] = items['home_dir_drive']\n        if (not ret['home']):\n            ret['home'] = ret['profile']\n        ret['groups'] = groups\n        if ((items['flags'] & win32netcon.UF_DONT_EXPIRE_PASSWD) == 0):\n            ret['password_never_expires'] = False\n        else:\n            ret['password_never_expires'] = True\n        if ((items['flags'] & win32netcon.UF_ACCOUNTDISABLE) == 0):\n            ret['account_disabled'] = False\n        else:\n            ret['account_disabled'] = True\n        if ((items['flags'] & win32netcon.UF_LOCKOUT) == 0):\n            ret['account_locked'] = False\n        else:\n            ret['account_locked'] = True\n        if ((items['flags'] & win32netcon.UF_PASSWD_CANT_CHANGE) == 0):\n            ret['disallow_change_password'] = False\n        else:\n            ret['disallow_change_password'] = True\n        ret['gid'] = ''\n        return ret\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef killJobs(self):\n    killList = list()\n    while True:\n        try:\n            jobId = self.killQueue.get(block=False)\n        except Empty:\n            break\n        else:\n            killList.append(jobId)\n    if (not killList):\n        return False\n    for jobID in list(killList):\n        if (jobID in self.runningJobs):\n            logger.debug('Killing job: %s', jobID)\n            subprocess.check_call(['qdel', self.getSgeID(jobID)])\n        else:\n            if (jobID in self.waitingJobs):\n                self.waitingJobs.remove(jobID)\n            self.killedJobsQueue.put(jobID)\n            killList.remove(jobID)\n    while killList:\n        for jobID in list(killList):\n            if (self.getJobExitCode(self.sgeJobIDs[jobID]) is not None):\n                logger.debug('Adding jobID %s to killedJobsQueue', jobID)\n                self.killedJobsQueue.put(jobID)\n                killList.remove(jobID)\n                self.forgetJob(jobID)\n        if (len(killList) > 0):\n            logger.warn(\"Some jobs weren't killed, trying again in %is.\", sleepSeconds)\n            time.sleep(sleepSeconds)\n    return True\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('prefix', ('file', 'redis+socket'))\ndef test_socket_paths_explicit(prefix):\n    actual = parse_url((prefix + '://user:pass@redis.sock'))\n    assert (dict(password='pass', unix_socket_path='redis.sock') == actual)\n    actual = parse_url((prefix + '://user:pass@../redis.sock'))\n    assert (dict(password='pass', unix_socket_path='../redis.sock') == actual)\n    actual = parse_url((prefix + '://user:pass@./redis.sock'))\n    assert (dict(password='pass', unix_socket_path='./redis.sock') == actual)\n    actual = parse_url((prefix + '://redis.SOCK'))\n    assert (dict(unix_socket_path='redis.SOCK') == actual)\n    actual = parse_url((prefix + '://../redis.sock'))\n    assert (dict(unix_socket_path='../redis.sock') == actual)\n    actual = parse_url((prefix + '://./redis.SOCK'))\n    assert (dict(unix_socket_path='./redis.SOCK') == actual)\n    if (os.name != 'nt'):\n        actual = parse_url((prefix + '://user:pass@/tmp/redis.SOCK'))\n        assert (dict(password='pass', unix_socket_path='/tmp/redis.SOCK') == actual)\n        actual = parse_url((prefix + ':///tmp/redis.sock'))\n        assert (dict(unix_socket_path='/tmp/redis.sock') == actual)\n    else:\n        actual = parse_url((prefix + '://user:pass@C:\\\\Windows\\\\Temp\\\\redis.SOCK'))\n        assert (dict(password='pass', unix_socket_path='C:\\\\Windows\\\\Temp\\\\redis.SOCK') == actual)\n        actual = parse_url((prefix + '://C:\\\\Windows\\\\Temp\\\\redis.sock'))\n        assert (dict(unix_socket_path='C:\\\\Windows\\\\Temp\\\\redis.sock') == actual)\n", "label": 1}
{"function": "\n\ndef numeric_state(hass, entity, below=None, above=None, value_template=None, variables=None):\n    'Test a numeric state condition.'\n    if isinstance(entity, str):\n        entity = hass.states.get(entity)\n    if (entity is None):\n        return False\n    if (value_template is None):\n        value = entity.state\n    else:\n        variables = dict((variables or {\n            \n        }))\n        variables['state'] = entity\n        try:\n            value = render(hass, value_template, variables)\n        except TemplateError as ex:\n            _LOGGER.error(ex)\n            return False\n    try:\n        value = float(value)\n    except ValueError:\n        _LOGGER.warning('Value cannot be processed as a number: %s', value)\n        return False\n    print(below, value, above)\n    if ((below is not None) and (value > below)):\n        return False\n    if ((above is not None) and (value < above)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef struct_type_from_fields(self, field_types, struct_name='tuple_type', field_names=None, field_repeats={\n    \n}):\n    if any(((not isinstance(t, str)) for t in field_types)):\n        field_types = tuple(((self.to_ctype(t) if isinstance(t, Type) else t) for t in field_types))\n    else:\n        field_types = tuple(field_types)\n    if (field_names is None):\n        field_names = tuple((('elt%d' % i) for i in xrange(len(field_types))))\n    else:\n        assert (len(field_names) == len(field_types)), ('Mismatching number of types %d and field names %d' % (len(field_types), len(field_names)))\n        field_names = tuple(field_names)\n    repeat_set = frozenset(sorted(field_repeats.items()))\n    key = (field_types, struct_name, field_names, repeat_set)\n    if (key in _struct_type_names):\n        typename = _struct_type_names[key]\n        decl = _struct_type_decls[typename]\n        if (decl not in self.declarations):\n            self.declarations.append(decl)\n        return typename\n    typename = names.fresh(struct_name).replace('.', '')\n    field_decls = []\n    for (t, field_name) in zip(field_types, field_names):\n        if (field_name in field_repeats):\n            field_decl = ('  %s %s[%d];' % (t, field_name, field_repeats[field_name]))\n        else:\n            field_decl = ('  %s %s;' % (t, field_name))\n        field_decls.append(field_decl)\n    decl = ('typedef struct %s {\\n%s\\n} %s;' % (typename, '\\n'.join(field_decls), typename))\n    _struct_type_names[key] = typename\n    _struct_type_decls[typename] = decl\n    self.add_decl(decl)\n    return typename\n", "label": 1}
{"function": "\n\ndef _unify(f, g):\n    g = sympify(g)\n    if (not g.is_Poly):\n        try:\n            return (f.rep.dom, f.per, f.rep, f.rep.per(f.rep.dom.from_sympy(g)))\n        except CoercionFailed:\n            raise UnificationFailed((\"can't unify %s with %s\" % (f, g)))\n    if (isinstance(f.rep, DMP) and isinstance(g.rep, DMP)):\n        gens = _unify_gens(f.gens, g.gens)\n        (dom, lev) = (f.rep.dom.unify(g.rep.dom, gens), (len(gens) - 1))\n        if (f.gens != gens):\n            (f_monoms, f_coeffs) = _dict_reorder(f.rep.to_dict(), f.gens, gens)\n            if (f.rep.dom != dom):\n                f_coeffs = [dom.convert(c, f.rep.dom) for c in f_coeffs]\n            F = DMP(dict(list(zip(f_monoms, f_coeffs))), dom, lev)\n        else:\n            F = f.rep.convert(dom)\n        if (g.gens != gens):\n            (g_monoms, g_coeffs) = _dict_reorder(g.rep.to_dict(), g.gens, gens)\n            if (g.rep.dom != dom):\n                g_coeffs = [dom.convert(c, g.rep.dom) for c in g_coeffs]\n            G = DMP(dict(list(zip(g_monoms, g_coeffs))), dom, lev)\n        else:\n            G = g.rep.convert(dom)\n    else:\n        raise UnificationFailed((\"can't unify %s with %s\" % (f, g)))\n    cls = f.__class__\n\n    def per(rep, dom=dom, gens=gens, remove=None):\n        if (remove is not None):\n            gens = (gens[:remove] + gens[(remove + 1):])\n            if (not gens):\n                return dom.to_sympy(rep)\n        return cls.new(rep, *gens)\n    return (dom, per, F, G)\n", "label": 1}
{"function": "\n\ndef _parse_network_settings(opts, current):\n    '\\n    Filters given options and outputs valid settings for\\n    the global network settings file.\\n    '\n    opts = dict(((k.lower(), v) for (k, v) in six.iteritems(opts)))\n    current = dict(((k.lower(), v) for (k, v) in six.iteritems(current)))\n    result = {\n        \n    }\n    valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n    if ('enabled' not in opts):\n        try:\n            opts['networking'] = current['networking']\n            _log_default_network('networking', current['networking'])\n        except ValueError:\n            _raise_error_network('networking', valid)\n    else:\n        opts['networking'] = opts['enabled']\n    if (opts['networking'] in valid):\n        if (opts['networking'] in _CONFIG_TRUE):\n            result['networking'] = 'yes'\n        elif (opts['networking'] in _CONFIG_FALSE):\n            result['networking'] = 'no'\n    else:\n        _raise_error_network('networking', valid)\n    if ('hostname' not in opts):\n        try:\n            opts['hostname'] = current['hostname']\n            _log_default_network('hostname', current['hostname'])\n        except ValueError:\n            _raise_error_network('hostname', ['server1.example.com'])\n    if opts['hostname']:\n        result['hostname'] = opts['hostname']\n    else:\n        _raise_error_network('hostname', ['server1.example.com'])\n    if ('search' in opts):\n        result['search'] = opts['search']\n    return result\n", "label": 1}
{"function": "\n\ndef MagneticLoopVectorPotential(srcLoc, obsLoc, component, radius, mu=mu_0):\n    \"\\n        Calculate the vector potential of horizontal circular loop\\n        at given locations\\n\\n        :param numpy.ndarray srcLoc: Location of the source(s) (x, y, z)\\n        :param numpy.ndarray,SimPEG.Mesh obsLoc: Where the potentials will be calculated (x, y, z) or a SimPEG Mesh\\n        :param str,list component: The component to calculate - 'x', 'y', or 'z' if an array, or grid type if mesh, can be a list\\n        :param numpy.ndarray I: Input current of the loop\\n        :param numpy.ndarray radius: radius of the loop\\n        :rtype: numpy.ndarray\\n        :return: The vector potential each dipole at each observation location\\n    \"\n    if (type(component) in [list, tuple]):\n        out = range(len(component))\n        for (i, comp) in enumerate(component):\n            out[i] = MagneticLoopVectorPotential(srcLoc, obsLoc, comp, radius, mu)\n        return np.concatenate(out)\n    if isinstance(obsLoc, Mesh.BaseMesh):\n        mesh = obsLoc\n        assert (component in ['Ex', 'Ey', 'Ez', 'Fx', 'Fy', 'Fz']), \"Components must be in: ['Ex','Ey','Ez','Fx','Fy','Fz']\"\n        return MagneticLoopVectorPotential(srcLoc, getattr(mesh, ('grid' + component)), component[1], radius, mu)\n    srcLoc = np.atleast_2d(srcLoc)\n    obsLoc = np.atleast_2d(obsLoc)\n    n = obsLoc.shape[0]\n    nSrc = srcLoc.shape[0]\n    if (component == 'z'):\n        A = np.zeros((n, nSrc))\n        if (nSrc == 1):\n            return A.flatten()\n        return A\n    else:\n        A = np.zeros((n, nSrc))\n        for i in range(nSrc):\n            x = (obsLoc[:, 0] - srcLoc[(i, 0)])\n            y = (obsLoc[:, 1] - srcLoc[(i, 1)])\n            z = (obsLoc[:, 2] - srcLoc[(i, 2)])\n            r = np.sqrt(((x ** 2) + (y ** 2)))\n            m = (((4 * radius) * r) / (((radius + r) ** 2) + (z ** 2)))\n            m[(m > 1.0)] = 1.0\n            K = ellipk(m)\n            E = ellipe(m)\n            ind = ((r > 0) & (m < 1))\n            Aphi = np.zeros(n)\n            Aphi[ind] = (((4e-07 / np.sqrt(m[ind])) * np.sqrt((radius / r[ind]))) * (((1.0 - (m[ind] / 2.0)) * K[ind]) - E[ind]))\n            if (component == 'x'):\n                A[(ind, i)] = (Aphi[ind] * ((- y[ind]) / r[ind]))\n            elif (component == 'y'):\n                A[(ind, i)] = (Aphi[ind] * (x[ind] / r[ind]))\n            else:\n                raise ValueError('Invalid component')\n        if (nSrc == 1):\n            return A.flatten()\n        return A\n", "label": 1}
{"function": "\n\ndef main():\n    'Tool main.'\n    try:\n        signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n    except AttributeError:\n        pass\n    try:\n        args = parse_args(sys.argv[1:])\n        if args.list_fixes:\n            for (code, description) in sorted(supported_fixes()):\n                print('{code} - {description}'.format(code=code, description=description))\n            return 0\n        if (args.files == ['-']):\n            assert (not args.in_place)\n            sys.stdout.write(fix_code(sys.stdin.read(), args))\n        else:\n            if (args.in_place or args.diff):\n                args.files = list(set(args.files))\n            else:\n                assert (len(args.files) == 1)\n                assert (not args.recursive)\n            fix_multiple_files(args.files, args, sys.stdout)\n    except KeyboardInterrupt:\n        return 1\n", "label": 1}
{"function": "\n\ndef fetch_data(self, connection):\n    'Loop in and read in all the data until we have received it all.\\n\\n        :param  socket connection: The connection\\n        :rtype: dict\\n\\n        '\n    connection.send('*0\\r\\ninfo\\r\\n')\n    buffer_value = connection.recv(self.SOCKET_RECV_MAX)\n    lines = buffer_value.split('\\r\\n')\n    if (lines[0][0] == '$'):\n        byte_size = int(lines[0][1:].strip())\n    else:\n        return None\n    while (len(buffer_value) < byte_size):\n        buffer_value += connection.recv(self.SOCKET_RECV_MAX)\n    lines = buffer_value.split('\\r\\n')\n    values = dict()\n    for line in lines:\n        if (':' in line):\n            (key, value) = line.strip().split(':', 1)\n            if (key[:2] == 'db'):\n                values[key] = dict()\n                subvalues = value.split(',')\n                for temp in subvalues:\n                    subvalue = temp.split('=')\n                    value = subvalue[(- 1)]\n                    try:\n                        values[key][subvalue[0]] = int(value)\n                    except ValueError:\n                        try:\n                            values[key][subvalue[0]] = float(value)\n                        except ValueError:\n                            values[key][subvalue[0]] = value\n                continue\n            try:\n                values[key] = int(value)\n            except ValueError:\n                try:\n                    values[key] = float(value)\n                except ValueError:\n                    values[key] = value\n    return values\n", "label": 1}
{"function": "\n\ndef handle(self, request, data):\n    meta = create_image_metadata(data)\n    if (settings.HORIZON_IMAGES_ALLOW_UPLOAD and policy.check((('image', 'upload_image'),), request) and data.get('image_file', None)):\n        meta['data'] = self.files['image_file']\n    elif data['is_copying']:\n        meta['copy_from'] = data['image_url']\n    else:\n        meta['location'] = data['image_url']\n    try:\n        image = api.glance.image_create(request, **meta)\n        messages.info(request, (_('Your image %s has been queued for creation.') % meta['name']))\n        return image\n    except Exception as e:\n        msg = _('Unable to create new image')\n        if (hasattr(e, 'code') and (e.code == 400)):\n            if ('Invalid disk format' in e.details):\n                msg = (_('Unable to create new image: Invalid disk format %s for image.') % meta['disk_format'])\n            elif ('Image name too long' in e.details):\n                msg = _('Unable to create new image: Image name too long.')\n            elif ('not supported' in e.details):\n                msg = _('Unable to create new image: URL scheme not supported.')\n        exceptions.handle(request, msg)\n        return False\n", "label": 1}
{"function": "\n\ndef test_ndependents():\n    from operator import add\n    (a, b, c) = 'abc'\n    dsk = dict(chain((((a, i), (i * 2)) for i in range(5)), (((b, i), (add, i, (a, i))) for i in range(5)), (((c, i), (add, i, (b, i))) for i in range(5))))\n    result = ndependents(*get_deps(dsk))\n    expected = dict(chain((((a, i), 3) for i in range(5)), (((b, i), 2) for i in range(5)), (((c, i), 1) for i in range(5))))\n    assert (result == expected)\n    dsk = {\n        a: 1,\n        b: 1,\n    }\n    deps = get_deps(dsk)\n    assert (ndependents(*deps) == dsk)\n    dsk = {\n        a: 1,\n        b: (add, a, 1),\n        c: (add, b, a),\n    }\n    assert (ndependents(*get_deps(dsk)) == {\n        a: 4,\n        b: 2,\n        c: 1,\n    })\n    dsk = {\n        a: 1,\n        b: a,\n        c: b,\n    }\n    deps = get_deps(dsk)\n    assert (ndependents(*deps) == {\n        a: 3,\n        b: 2,\n        c: 1,\n    })\n", "label": 1}
{"function": "\n\ndef check_permissions(self):\n    '\\n        Checks that all permissions are set correctly for the users.\\n\\n        :return: A set of users whose permissions was wrong.\\n\\n        '\n    changed_permissions = []\n    changed_users = []\n    warnings = []\n    for (model, perms) in ASSIGNED_PERMISSIONS.items():\n        if (model == 'profile'):\n            model_obj = get_profile_model()\n        else:\n            model_obj = get_user_model()\n        model_content_type = ContentType.objects.get_for_model(model_obj)\n        for perm in perms:\n            try:\n                Permission.objects.get(codename=perm[0], content_type=model_content_type)\n            except Permission.DoesNotExist:\n                changed_permissions.append(perm[1])\n                Permission.objects.create(name=perm[1], codename=perm[0], content_type=model_content_type)\n    for user in get_user_model().objects.exclude(id=settings.ANONYMOUS_USER_ID):\n        try:\n            user_profile = get_user_profile(user=user)\n        except ObjectDoesNotExist:\n            warnings.append((_('No profile found for %(username)s') % {\n                'username': user.username,\n            }))\n        else:\n            all_permissions = (get_perms(user, user_profile) + get_perms(user, user))\n            for (model, perms) in ASSIGNED_PERMISSIONS.items():\n                if (model == 'profile'):\n                    perm_object = get_user_profile(user=user)\n                else:\n                    perm_object = user\n                for perm in perms:\n                    if (perm[0] not in all_permissions):\n                        assign_perm(perm[0], user, perm_object)\n                        changed_users.append(user)\n    return (changed_permissions, changed_users, warnings)\n", "label": 1}
{"function": "\n\ndef loop(self):\n    'Continually check the queue for new items and process them.'\n    last_runs = {\n        \n    }\n    while True:\n        try:\n            try:\n                item = self.q.get(block=True, timeout=self.MIN_DELAY)\n            except Empty:\n                for (view_id, (timestamp, delay)) in last_runs.copy().items():\n                    if (time.monotonic() > (timestamp + delay)):\n                        self.last_runs[view_id] = time.monotonic()\n                        del last_runs[view_id]\n                        self.lint(view_id, timestamp)\n                continue\n            if isinstance(item, tuple):\n                (view_id, timestamp, delay) = item\n                if ((view_id in self.last_runs) and (timestamp < self.last_runs[view_id])):\n                    continue\n                last_runs[view_id] = (timestamp, delay)\n            elif isinstance(item, (int, float)):\n                time.sleep(item)\n            elif isinstance(item, str):\n                if (item == 'reload'):\n                    persist.printf('daemon detected a reload')\n                    self.last_runs.clear()\n                    last_runs.clear()\n            else:\n                persist.printf('unknown message sent to daemon:', item)\n        except:\n            persist.printf('error in SublimeLinter daemon:')\n            persist.printf(('-' * 20))\n            persist.printf(traceback.format_exc())\n            persist.printf(('-' * 20))\n", "label": 1}
{"function": "\n\ndef test_concat_l1_l1(backend_default, allrand_args):\n    dtypeu = np.float32\n    (w_rng, rngmax) = allrand_args\n    nins = [128, 1024]\n    nouts = [64, 2048]\n    batch_size = 16\n    NervanaObject.be.bsz = batch_size\n    be = NervanaObject.be\n    init_unif = Uniform(low=w_rng[0], high=w_rng[1])\n    layers = [Sequential(Affine(nout=nout, init=init_unif)) for nout in nouts]\n    inputs = [be.array(dtypeu(np.random.random((nin, batch_size)))) for nin in nins]\n    merge = MergeMultistream(layers, merge='stack')\n    assert (len(inputs) == len(layers))\n    merge.configure(inputs)\n    merge.allocate()\n    merge.set_deltas(None)\n    out = merge.fprop(inputs).get()\n    sublayers = [s.layers[0] for s in layers]\n    weights = [layer.W.get() for layer in sublayers]\n    out_exp = np.concatenate([np.dot(w, inp.get()) for (w, inp) in zip(weights, inputs)])\n    assert np.allclose(out, out_exp, atol=0.001)\n    err_lst = [dtypeu(np.random.random((nout, batch_size))) for nout in nouts]\n    err_concat = np.concatenate(err_lst)\n    merge.bprop(be.array(err_concat))\n    dW_exp_lst = [np.dot(err, inp.get().T) for (err, inp) in zip(err_lst, inputs)]\n    for (layer, dW_exp) in zip(sublayers, dW_exp_lst):\n        assert np.allclose(layer.dW.get(), dW_exp)\n    return\n", "label": 1}
{"function": "\n\ndef computeStatementsSequence(self, constraint_collection):\n    new_statements = []\n    statements = self.getStatements()\n    assert statements, self\n    for (count, statement) in enumerate(statements):\n        if statement.isStatementsFrame():\n            new_statement = statement.computeStatementsSequence(constraint_collection)\n        else:\n            new_statement = constraint_collection.onStatement(statement=statement)\n        if (new_statement is not None):\n            if (new_statement.isStatementsSequence() and (not new_statement.isStatementsFrame())):\n                new_statements.extend(new_statement.getStatements())\n            else:\n                new_statements.append(new_statement)\n            if ((statement is not statements[(- 1)]) and new_statement.isStatementAborting()):\n                constraint_collection.signalChange('new_statements', statements[(count + 1)].getSourceReference(), 'Removed dead statements.')\n                break\n    if (statements != new_statements):\n        if new_statements:\n            self.setStatements(new_statements)\n            return self\n        else:\n            return None\n    else:\n        return self\n", "label": 1}
{"function": "\n\ndef issueBatchJob(self, command, memory, cores, disk, preemptable):\n    '\\n        Issues parasol with job commands.\\n        '\n    self.checkResourceRequest(memory, cores, disk)\n    MiB = (1 << 20)\n    truncatedMemory = ((memory / MiB) * MiB)\n    if (len(self.resultsFiles) >= self.maxBatches):\n        raise RuntimeError(('Number of batches reached limit of %i' % self.maxBatches))\n    try:\n        results = self.resultsFiles[(truncatedMemory, cores)]\n    except KeyError:\n        results = getTempFile(rootDir=self.parasolResultsDir)\n        self.resultsFiles[(truncatedMemory, cores)] = results\n    command = ' '.join(concat('env', self.__environment(), command))\n    parasolCommand = ['-verbose', ('-ram=%i' % memory), ('-cpu=%i' % cores), ('-results=' + results), 'add', 'job', command]\n    self.usedCpus += cores\n    while True:\n        try:\n            jobID = self.cpuUsageQueue.get_nowait()\n        except Empty:\n            break\n        if (jobID in self.jobIDsToCpu.keys()):\n            self.usedCpus -= self.jobIDsToCpu.pop(jobID)\n        assert (self.usedCpus >= 0)\n    while (self.usedCpus > self.maxCores):\n        jobID = self.cpuUsageQueue.get()\n        if (jobID in self.jobIDsToCpu.keys()):\n            self.usedCpus -= self.jobIDsToCpu.pop(jobID)\n        assert (self.usedCpus >= 0)\n    while True:\n        line = self._runParasol(parasolCommand)[1][0]\n        match = self.parasolOutputPattern.match(line)\n        if (match is None):\n            logger.info('We failed to properly add the job, we will try again after a 5s.')\n            time.sleep(5)\n        else:\n            jobID = int(match.group(1))\n            self.jobIDsToCpu[jobID] = cores\n            self.runningJobs.add(jobID)\n            logger.debug(('Got the parasol job id: %s from line: %s' % (jobID, line)))\n            return jobID\n", "label": 1}
{"function": "\n\ndef _check_mandatory_parameters(self, request):\n    if (not all((request.signature, request.client_key, request.nonce, request.timestamp, request.signature_method))):\n        raise errors.InvalidRequestError(description='Missing mandatory OAuth parameters.')\n    if (not (request.signature_method in self.request_validator.allowed_signature_methods)):\n        raise errors.InvalidSignatureMethodError(description=('Invalid signature, %s not in %r.' % (request.signature_method, self.request_validator.allowed_signature_methods)))\n    if (('oauth_version' in request.oauth_params) and (request.oauth_params['oauth_version'] != '1.0')):\n        raise errors.InvalidRequestError(description='Invalid OAuth version.')\n    if (len(request.timestamp) != 10):\n        raise errors.InvalidRequestError(description='Invalid timestamp size')\n    try:\n        ts = int(request.timestamp)\n    except ValueError:\n        raise errors.InvalidRequestError(description='Timestamp must be an integer.')\n    else:\n        if (abs((time.time() - ts)) > self.request_validator.timestamp_lifetime):\n            raise errors.InvalidRequestError(description=('Timestamp given is invalid, differ from allowed by over %s seconds.' % self.request_validator.timestamp_lifetime))\n    if (not self.request_validator.check_client_key(request.client_key)):\n        raise errors.InvalidRequestError(description='Invalid client key format.')\n    if (not self.request_validator.check_nonce(request.nonce)):\n        raise errors.InvalidRequestError(description='Invalid nonce format.')\n", "label": 1}
{"function": "\n\ndef __interact_copy(self, escape_character=None, input_filter=None, output_filter=None):\n    'This is used by the interact() method.\\n        '\n    while self.isalive():\n        (r, w, e) = self.__select([self.child_fd, self.STDIN_FILENO], [], [])\n        if (self.child_fd in r):\n            try:\n                data = self.__interact_read(self.child_fd)\n            except OSError as err:\n                if (err.args[0] == errno.EIO):\n                    break\n                raise\n            if (data == b''):\n                break\n            if output_filter:\n                data = output_filter(data)\n            if (self.logfile is not None):\n                self.logfile.write(data)\n                self.logfile.flush()\n            os.write(self.STDOUT_FILENO, data)\n        if (self.STDIN_FILENO in r):\n            data = self.__interact_read(self.STDIN_FILENO)\n            if input_filter:\n                data = input_filter(data)\n            i = data.rfind(escape_character)\n            if (i != (- 1)):\n                data = data[:i]\n                self.__interact_writen(self.child_fd, data)\n                break\n            self.__interact_writen(self.child_fd, data)\n", "label": 1}
{"function": "\n\ndef test_draw_component_model_hyperparameters_multiple(self):\n    n_draws = 3\n    draw_list = self.component_class.draw_hyperparameters(self.X, n_draws=n_draws)\n    assert (type(draw_list) is list)\n    assert (len(draw_list) == 3)\n    for draw in draw_list:\n        assert (type(draw) is dict)\n        for (key, value) in six.iteritems(draw):\n            assert (key in ['mu', 'nu', 'r', 's'])\n            assert ((type(value) is float) or (type(value) is numpy.float64))\n            assert (not math.isnan(value))\n            assert (not math.isinf(value))\n            if (key in ['nu', 's', 'r']):\n                assert (value > 0.0)\n", "label": 1}
{"function": "\n\ndef _compile(node, source=None, mode='eval', filename=None, lineno=(- 1), xform=None):\n    if (not filename):\n        filename = '<string>'\n    if IS_PYTHON2:\n        if isinstance(filename, str):\n            filename = filename.encode('utf-8', 'replace')\n    elif (not isinstance(filename, str)):\n        filename = filename.decode('utf-8', 'replace')\n    if (lineno <= 0):\n        lineno = 1\n    if (xform is None):\n        xform = {\n            'eval': ExpressionASTTransformer,\n        }.get(mode, TemplateASTTransformer)\n    tree = xform().visit(node)\n    if (mode == 'eval'):\n        name = ('<Expression %r>' % (source or '?'))\n    else:\n        lines = source.splitlines()\n        if (not lines):\n            extract = ''\n        else:\n            extract = lines[0]\n        if (len(lines) > 1):\n            extract += ' ...'\n        name = ('<Suite %r>' % extract)\n    new_source = ASTCodeGenerator(tree).code\n    code = compile(new_source, filename, mode)\n    try:\n        return build_code_chunk(code, filename, name, lineno)\n    except RuntimeError:\n        return code\n", "label": 1}
{"function": "\n\ndef _getsignature(func, skipfirst):\n    if (inspect is None):\n        raise ImportError('inspect module not available')\n    if inspect.isclass(func):\n        func = func.__init__\n        skipfirst = True\n    elif (not (inspect.ismethod(func) or inspect.isfunction(func))):\n        func = func.__call__\n    (regargs, varargs, varkwargs, defaults) = inspect.getargspec(func)\n    if (getattr(func, self, None) is not None):\n        regargs = regargs[1:]\n    _msg = \"_mock_ is a reserved argument name, can't mock signatures using _mock_\"\n    assert ('_mock_' not in regargs), _msg\n    if (varargs is not None):\n        assert ('_mock_' not in varargs), _msg\n    if (varkwargs is not None):\n        assert ('_mock_' not in varkwargs), _msg\n    if skipfirst:\n        regargs = regargs[1:]\n    signature = inspect.formatargspec(regargs, varargs, varkwargs, defaults, formatvalue=(lambda value: ''))\n    return (signature[1:(- 1)], func)\n", "label": 1}
{"function": "\n\ndef create_alias(self, request):\n    if (not request.user.is_staff):\n        return HttpResponseForbidden('not enough privileges')\n    if ((not ('plugin_id' in request.POST)) and (not ('placeholder_id' in request.POST))):\n        return HttpResponseBadRequest('plugin_id or placeholder_id POST parameter missing.')\n    plugin = None\n    placeholder = None\n    if ('plugin_id' in request.POST):\n        pk = request.POST['plugin_id']\n        try:\n            plugin = CMSPlugin.objects.get(pk=pk)\n        except CMSPlugin.DoesNotExist:\n            return HttpResponseBadRequest(('plugin with id %s not found.' % pk))\n    if ('placeholder_id' in request.POST):\n        pk = request.POST['placeholder_id']\n        try:\n            placeholder = Placeholder.objects.get(pk=pk)\n        except Placeholder.DoesNotExist:\n            return HttpResponseBadRequest(('placeholder with id %s not found.' % pk))\n        if (not placeholder.has_change_permission(request)):\n            return HttpResponseBadRequest('You do not have enough permission to alias this placeholder.')\n    clipboard = request.toolbar.clipboard\n    clipboard.cmsplugin_set.all().delete()\n    language = get_language()\n    if plugin:\n        language = plugin.language\n    alias = AliasPluginModel(language=language, placeholder=clipboard, plugin_type='AliasPlugin')\n    if plugin:\n        alias.plugin = plugin\n    if placeholder:\n        alias.alias_placeholder = placeholder\n    alias.save()\n    return HttpResponse('ok')\n", "label": 1}
{"function": "\n\ndef mds_create(args):\n    cfg = conf.ceph.load(args)\n    LOG.debug('Deploying mds, cluster %s hosts %s', args.cluster, ' '.join((':'.join(((x or '') for x in t)) for t in args.mds)))\n    key = get_bootstrap_mds_key(cluster=args.cluster)\n    bootstrapped = set()\n    errors = 0\n    failed_on_rhel = False\n    for (hostname, name) in args.mds:\n        try:\n            distro = hosts.get(hostname, username=args.username)\n            rlogger = distro.conn.logger\n            LOG.info('Distro info: %s %s %s', distro.name, distro.release, distro.codename)\n            LOG.debug('remote host will use %s', distro.init)\n            if (hostname not in bootstrapped):\n                bootstrapped.add(hostname)\n                LOG.debug('deploying mds bootstrap to %s', hostname)\n                conf_data = StringIO()\n                cfg.write(conf_data)\n                distro.conn.remote_module.write_conf(args.cluster, conf_data.getvalue(), args.overwrite_conf)\n                path = '/var/lib/ceph/bootstrap-mds/{cluster}.keyring'.format(cluster=args.cluster)\n                if (not distro.conn.remote_module.path_exists(path)):\n                    rlogger.warning('mds keyring does not exist yet, creating one')\n                    distro.conn.remote_module.write_keyring(path, key)\n            create_mds(distro, name, args.cluster, distro.init)\n            distro.conn.exit()\n        except RuntimeError as e:\n            if (distro.normalized_name == 'redhat'):\n                LOG.error(('this feature may not yet available for %s %s' % (distro.name, distro.release)))\n                failed_on_rhel = True\n            LOG.error(e)\n            errors += 1\n    if errors:\n        if failed_on_rhel:\n            LOG.error('RHEL RHCS systems do not have the ability to deploy MDS yet')\n        raise exc.GenericError(('Failed to create %d MDSs' % errors))\n", "label": 1}
{"function": "\n\ndef disable_beacon(name, **kwargs):\n    \"\\n    Disable beacon on the minion\\n\\n    :name:                  Name of the beacon to disable.\\n    :return:                Boolean and status message on success or failure of disable.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' beacons.disable_beacon ps\\n    \"\n    ret = {\n        'comment': [],\n        'result': True,\n    }\n    if (not name):\n        ret['comment'] = 'Beacon name is required.'\n        ret['result'] = False\n        return ret\n    if (('test' in kwargs) and kwargs['test']):\n        ret['comment'] = 'Beacons would be enabled.'\n    else:\n        _beacons = list_(return_yaml=False)\n        if (name not in _beacons):\n            ret['comment'] = 'Beacon {0} is not currently configured.'.format(name)\n            ret['result'] = False\n            return ret\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({\n                'func': 'disable_beacon',\n                'name': name,\n            }, 'manage_beacons')\n            if res:\n                event_ret = eventer.get_event(tag='/salt/minion/minion_beacon_disabled_complete', wait=30)\n                if (event_ret and event_ret['complete']):\n                    beacons = event_ret['beacons']\n                    beacon_config_dict = _get_beacon_config_dict(beacons[name])\n                    if (('enabled' in beacon_config_dict) and (not beacon_config_dict['enabled'])):\n                        ret['result'] = True\n                        ret['comment'] = 'Disabled beacon {0} on minion.'.format(name)\n                    else:\n                        ret['result'] = False\n                        ret['comment'] = 'Failed to disable beacon on minion.'\n                    return ret\n        except KeyError:\n            ret['comment'] = 'Event module not available. Beacon disable job failed.'\n    return ret\n", "label": 1}
{"function": "\n\ndef testCollection(self):\n    ad = pkg_resources.Environment([], platform=None, python=None)\n    assert (list(ad) == [])\n    assert (ad['FooPkg'] == [])\n    ad.add(dist_from_fn('FooPkg-1.3_1.egg'))\n    ad.add(dist_from_fn('FooPkg-1.4-py2.4-win32.egg'))\n    ad.add(dist_from_fn('FooPkg-1.2-py2.4.egg'))\n    assert ad['FooPkg']\n    assert (list(ad) == ['foopkg'])\n    assert ([dist.version for dist in ad['FooPkg']] == ['1.4', '1.3-1', '1.2'])\n    ad.remove(ad['FooPkg'][1])\n    assert ([dist.version for dist in ad['FooPkg']] == ['1.4', '1.2'])\n    ad.add(dist_from_fn('FooPkg-1.9.egg'))\n    assert ([dist.version for dist in ad['FooPkg']] == ['1.9', '1.4', '1.2'])\n    ws = WorkingSet([])\n    foo12 = dist_from_fn('FooPkg-1.2-py2.4.egg')\n    foo14 = dist_from_fn('FooPkg-1.4-py2.4-win32.egg')\n    (req,) = parse_requirements('FooPkg>=1.3')\n    assert (ad.best_match(req, ws).version == '1.9')\n    ws.add(foo14)\n    assert (ad.best_match(req, ws).version == '1.4')\n    ws = WorkingSet([])\n    ws.add(foo12)\n    ws.add(foo14)\n    with pytest.raises(VersionConflict):\n        ad.best_match(req, ws)\n    ws = WorkingSet([])\n    ws.add(foo14)\n    ws.add(foo12)\n    ws.add(foo14)\n    assert (ad.best_match(req, ws).version == '1.4')\n", "label": 1}
{"function": "\n\ndef _detach_volume(self, context, volume_id, instance, destroy_bdm=True, attachment_id=None):\n    \"Detach a volume from an instance.\\n\\n        :param context: security context\\n        :param volume_id: the volume id\\n        :param instance: the Instance object to detach the volume from\\n        :param destroy_bdm: if True, the corresponding BDM entry will be marked\\n                            as deleted. Disabling this is useful for operations\\n                            like rebuild, when we don't want to destroy BDM\\n\\n        \"\n    bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(context, volume_id, instance.uuid)\n    if (CONF.volume_usage_poll_interval > 0):\n        vol_stats = []\n        mp = bdm.device_name\n        if ('/dev/' in mp):\n            mp = mp[5:]\n        try:\n            vol_stats = self.driver.block_stats(instance, mp)\n        except NotImplementedError:\n            pass\n        if vol_stats:\n            LOG.debug('Updating volume usage cache with totals', instance=instance)\n            (rd_req, rd_bytes, wr_req, wr_bytes, flush_ops) = vol_stats\n            vol_usage = objects.VolumeUsage(context)\n            vol_usage.volume_id = volume_id\n            vol_usage.instance_uuid = instance.uuid\n            vol_usage.project_id = instance.project_id\n            vol_usage.user_id = instance.user_id\n            vol_usage.availability_zone = instance.availability_zone\n            vol_usage.curr_reads = rd_req\n            vol_usage.curr_read_bytes = rd_bytes\n            vol_usage.curr_writes = wr_req\n            vol_usage.curr_write_bytes = wr_bytes\n            vol_usage.save(update_totals=True)\n            self.notifier.info(context, 'volume.usage', compute_utils.usage_volume_info(vol_usage))\n    connection_info = self._driver_detach_volume(context, instance, bdm)\n    connector = self.driver.get_volume_connector(instance)\n    if (connection_info and (not destroy_bdm) and (connector.get('host') != instance.host)):\n        stashed_connector = connection_info.get('connector')\n        if (not stashed_connector):\n            LOG.warning(_LW('Host mismatch detected, but stashed volume connector not found. Instance host is %(ihost)s, but volume connector host is %(chost)s.'), {\n                'ihost': instance.host,\n                'chost': connector.get('host'),\n            })\n        elif (stashed_connector.get('host') != instance.host):\n            LOG.error(_LE('Host mismatch detected in stashed volume connector. Will use local volume connector. Instance host is %(ihost)s. Local volume connector host is %(chost)s. Stashed volume connector host is %(schost)s.'), {\n                'ihost': instance.host,\n                'chost': connector.get('host'),\n                'schost': stashed_connector.get('host'),\n            })\n        else:\n            LOG.debug('Host mismatch detected. Found usable stashed volume connector. Instance host is %(ihost)s. Local volume connector host was %(chost)s. Stashed volume connector host is %(schost)s.', {\n                'ihost': instance.host,\n                'chost': connector.get('host'),\n                'schost': stashed_connector.get('host'),\n            })\n            connector = stashed_connector\n    self.volume_api.terminate_connection(context, volume_id, connector)\n    if destroy_bdm:\n        bdm.destroy()\n    info = dict(volume_id=volume_id)\n    self._notify_about_instance_usage(context, instance, 'volume.detach', extra_usage_info=info)\n    self.volume_api.detach(context.elevated(), volume_id, instance.uuid, attachment_id)\n", "label": 1}
{"function": "\n\ndef _autodiscover(self):\n    'Discovers panels to register from the current dashboard module.'\n    if getattr(self, '_autodiscover_complete', False):\n        return\n    panels_to_discover = []\n    panel_groups = []\n    if all([isinstance(i, basestring) for i in self.panels]):\n        self.panels = [self.panels]\n    for panel_set in self.panels:\n        if ((not isinstance(panel_set, collections.Iterable)) and issubclass(panel_set, PanelGroup)):\n            panel_group = panel_set(self)\n        elif (not isinstance(panel_set, PanelGroup)):\n            panel_group = PanelGroup(self, panels=panel_set)\n        panels_to_discover.extend(panel_group.panels)\n        panel_groups.append((panel_group.slug, panel_group))\n    self._panel_groups = SortedDict(panel_groups)\n    package = '.'.join(self.__module__.split('.')[:(- 1)])\n    mod = import_module(package)\n    for panel in panels_to_discover:\n        try:\n            before_import_registry = copy.copy(self._registry)\n            import_module(('.%s.panel' % panel), package)\n        except Exception:\n            self._registry = before_import_registry\n            if module_has_submodule(mod, panel):\n                raise\n    self._autodiscover_complete = True\n", "label": 1}
{"function": "\n\ndef get_pid(self, unique_id, configs=None):\n    'Gets the pid of the process with `unique_id`.  If the deployer does not know of a process\\n    with `unique_id` then it should return a value of constants.PROCESS_NOT_RUNNING_PID\\n    '\n    RECV_BLOCK_SIZE = 16\n    if (configs is None):\n        configs = {\n            \n        }\n    tmp = self.default_configs.copy()\n    tmp.update(configs)\n    configs = tmp\n    if (unique_id in self.processes):\n        hostname = self.processes[unique_id].hostname\n    else:\n        return constants.PROCESS_NOT_RUNNING_PID\n    if (self.processes[unique_id].start_command is None):\n        return constants.PROCESS_NOT_RUNNING_PID\n    if (self.processes[unique_id].pid_file is not None):\n        with open_remote_file(hostname, self.processes[unique_id].pid_file, username=runtime.get_username(), password=runtime.get_password()) as pid_file:\n            full_output = pid_file.read()\n    elif ('pid_file' in configs.keys()):\n        with open_remote_file(hostname, configs['pid_file'], username=runtime.get_username(), password=runtime.get_password()) as pid_file:\n            full_output = pid_file.read()\n    else:\n        pid_keyword = self.processes[unique_id].start_command\n        if (self.processes[unique_id].args is not None):\n            pid_keyword = '{0} {1}'.format(pid_keyword, ' '.join(self.processes[unique_id].args))\n        pid_keyword = configs.get('pid_keyword', pid_keyword)\n        pid_command = \"ps aux | grep '{0}' | grep -v grep | tr -s ' ' | cut -d ' ' -f 2 | grep -Eo '[0-9]+'\".format(pid_keyword)\n        pid_command = configs.get('pid_command', pid_command)\n        non_failing_command = '{0}; if [ $? -le 1 ]; then true;  else false; fi;'.format(pid_command)\n        env = configs.get('env', {\n            \n        })\n        with get_ssh_client(hostname, username=runtime.get_username(), password=runtime.get_password()) as ssh:\n            chan = exec_with_env(ssh, non_failing_command, msg='Failed to get PID', env=env)\n        output = chan.recv(RECV_BLOCK_SIZE)\n        full_output = output\n        while (len(output) > 0):\n            output = chan.recv(RECV_BLOCK_SIZE)\n            full_output += output\n    if (len(full_output) > 0):\n        pids = [int(pid_str) for pid_str in full_output.split('\\n') if pid_str.isdigit()]\n        if (len(pids) > 0):\n            return pids\n    return constants.PROCESS_NOT_RUNNING_PID\n", "label": 1}
{"function": "\n\ndef _new_transaction(self, conn, desc, after_callbacks, logging_context, func, *args, **kwargs):\n    start = (time.time() * 1000)\n    txn_id = self._TXN_ID\n    self._TXN_ID = ((self._TXN_ID + 1) % (sys.maxint - 1))\n    name = ('%s-%x' % (desc, txn_id))\n    transaction_logger.debug('[TXN START] {%s}', name)\n    try:\n        i = 0\n        N = 5\n        while True:\n            try:\n                txn = conn.cursor()\n                txn = LoggingTransaction(txn, name, self.database_engine, after_callbacks)\n                r = func(txn, *args, **kwargs)\n                conn.commit()\n                return r\n            except self.database_engine.module.OperationalError as e:\n                logger.warn('[TXN OPERROR] {%s} %s %d/%d', name, e, i, N)\n                if (i < N):\n                    i += 1\n                    try:\n                        conn.rollback()\n                    except self.database_engine.module.Error as e1:\n                        logger.warn('[TXN EROLL] {%s} %s', name, e1)\n                    continue\n                raise\n            except self.database_engine.module.DatabaseError as e:\n                if self.database_engine.is_deadlock(e):\n                    logger.warn('[TXN DEADLOCK] {%s} %d/%d', name, i, N)\n                    if (i < N):\n                        i += 1\n                        try:\n                            conn.rollback()\n                        except self.database_engine.module.Error as e1:\n                            logger.warn('[TXN EROLL] {%s} %s', name, e1)\n                        continue\n                raise\n    except Exception as e:\n        logger.debug('[TXN FAIL] {%s} %s', name, e)\n        raise\n    finally:\n        end = (time.time() * 1000)\n        duration = (end - start)\n        if (logging_context is not None):\n            logging_context.add_database_transaction(duration)\n        transaction_logger.debug('[TXN END] {%s} %f', name, duration)\n        self._current_txn_total_time += duration\n        self._txn_perf_counters.update(desc, start, end)\n        sql_txn_timer.inc_by(duration, desc)\n", "label": 1}
{"function": "\n\ndef test_mergeComps_K5_D3_withELBO_kA3(self, K=5, D=3):\n    SS = self.makeSuffStatBagAndFillWithOnes(K, D)\n    self.addELBOtoSuffStatBag(SS, K)\n    SS.mergeComps(3, 4)\n    (s, N, x, xxT) = self.getExpectedMergedFields(K, D, kA=3)\n    assert (SS.K == (K - 1))\n    assert (SS._ELBOTerms.K == (K - 1))\n    assert (SS._MergeTerms.K == (K - 1))\n    assert np.allclose(SS.s, s)\n    assert np.allclose(SS.N, N)\n    assert np.allclose(SS.x, x)\n    assert np.allclose(SS.xxT, xxT)\n    assert np.allclose(SS.getELBOTerm('Elogz'), [1.0, 1, 1, 2.0])\n    assert np.allclose(SS.getELBOTerm('Econst'), 1.0)\n    assert np.all(np.isnan(SS._MergeTerms.Elogz[3, 4:]))\n    assert np.all(np.isnan(SS._MergeTerms.Elogz[:3, 3]))\n", "label": 1}
{"function": "\n\ndef call(self, command, opts={\n    \n}, on_response=None, on_notify=None, on_error=None, wait=False, timeout=None, id=None):\n    args_cmd = 'hsdev {0}'.format(command)\n    call_cmd = 'hsdev {0} with {1}'.format(command, opts)\n    if (not self.verify_connected()):\n        return (None if wait else False)\n    try:\n        wait_receive = (threading.Event() if wait else None)\n        x = {\n            \n        }\n\n        def on_response_(r):\n            x['result'] = r\n            call_callback(on_response, r)\n            if wait_receive:\n                wait_receive.set()\n\n        def on_error_(e, ds):\n            call_callback(on_error, e, ds)\n            if wait_receive:\n                wait_receive.set()\n        if (wait or on_response or on_notify or on_error):\n            if (id is None):\n                id = str(self.id)\n                self.id = (self.id + 1)\n            self.on_receive(id, args_cmd, on_response_, on_notify, on_error_)\n        opts.update({\n            'no-file': True,\n        })\n        opts.update({\n            'id': id,\n            'command': command,\n        })\n        msg = json.dumps(opts, separators=(',', ':'))\n        self.hsdev_socket.sendall(msg.encode('utf-8'))\n        self.hsdev_socket.sendall('\\n'.encode('utf-8'))\n        log(call_cmd, log_trace)\n        if wait:\n            wait_receive.wait(timeout)\n            return x.get('result')\n        return True\n    except Exception as e:\n        log('{0} fails with exception: {1}'.format(call_cmd, e), log_error)\n        self.connection_lost('call', e)\n        return False\n", "label": 1}
{"function": "\n\ndef values(self, *args, **kwargs):\n    \"\\n        find/construct field-value(s) for the given header\\n\\n        Resolution is done according to the following arguments:\\n\\n        - If only keyword arguments are given, then this is equivalent\\n          to ``compose(**kwargs)``.\\n\\n        - If the first (and only) argument is a dict, it is assumed\\n          to be a WSGI ``environ`` and the result of the corresponding\\n          ``HTTP_`` entry is returned.\\n\\n        - If the first (and only) argument is a list, it is assumed\\n          to be a WSGI ``response_headers`` and the field-value(s)\\n          for this header are collected and returned.\\n\\n        - In all other cases, the arguments are collected, checked that\\n          they are string values, possibly verified by the header's\\n          logic, and returned.\\n\\n        At this time it is an error to provide keyword arguments if args\\n        is present (this might change).  It is an error to provide both\\n        a WSGI object and also string arguments.  If no arguments are\\n        provided, then ``compose()`` is called to provide a default\\n        value for the header; if there is not default it is an error.\\n        \"\n    if (not args):\n        return self.compose(**kwargs)\n    if (list == type(args[0])):\n        assert (1 == len(args))\n        result = []\n        name = self.name.lower()\n        for value in [value for (header, value) in args[0] if (header.lower() == name)]:\n            result.append(value)\n        return result\n    if (dict == type(args[0])):\n        assert ((1 == len(args)) and ('wsgi.version' in args[0]))\n        value = args[0].get(self._environ_name)\n        if (not value):\n            return ()\n        return (value,)\n    for item in args:\n        assert (not (type(item) in (dict, list)))\n    return args\n", "label": 1}
{"function": "\n\n@register_specialize\n@register_canonicalize\n@gof.local_optimizer([T.Join])\ndef local_join_empty(node):\n    'Join(i, x, y, empty) => Join(i, x, y)\\n\\n    Remove empty inputs to joins. The empty inputs can be anywhere.\\n\\n    '\n    if (not isinstance(node.op, T.Join)):\n        return\n    new_inputs = []\n    try:\n        join_idx = get_scalar_constant_value(node.inputs[0])\n    except NotScalarConstantError:\n        return\n    for idx in xrange(1, len(node.inputs)):\n        inp = node.inputs[idx]\n        if (isinstance(inp, theano.Constant) and (inp.data.shape[join_idx] == 0)):\n            continue\n        new_inputs.append(inp)\n    if (len(new_inputs) < (len(node.inputs) - 1)):\n        if (len(new_inputs) == 0):\n            return\n        ret = T.join(node.inputs[0], *new_inputs)\n        o = node.outputs[0]\n        if (ret.dtype != o.dtype):\n            return\n        copy_stack_trace(node.outputs, ret)\n        if (ret.type != o.type):\n            assert (ret.dtype == o.dtype)\n            assert (ret.ndim == o.ndim)\n            ret = T.patternbroadcast(ret, node.outputs[0].broadcastable)\n        copy_stack_trace(node.outputs, ret)\n        return [ret]\n", "label": 1}
{"function": "\n\ndef prepare_filename(self, info_dict):\n    'Generate the output filename.'\n    try:\n        template_dict = dict(info_dict)\n        template_dict['epoch'] = int(time.time())\n        autonumber_size = self.params.get('autonumber_size')\n        if (autonumber_size is None):\n            autonumber_size = 5\n        autonumber_templ = (('%0' + str(autonumber_size)) + 'd')\n        template_dict['autonumber'] = (autonumber_templ % self._num_downloads)\n        if (template_dict.get('playlist_index') is not None):\n            template_dict['playlist_index'] = ('%0*d' % (len(str(template_dict['n_entries'])), template_dict['playlist_index']))\n        if (template_dict.get('resolution') is None):\n            if (template_dict.get('width') and template_dict.get('height')):\n                template_dict['resolution'] = ('%dx%d' % (template_dict['width'], template_dict['height']))\n            elif template_dict.get('height'):\n                template_dict['resolution'] = ('%sp' % template_dict['height'])\n            elif template_dict.get('width'):\n                template_dict['resolution'] = ('?x%d' % template_dict['width'])\n        sanitize = (lambda k, v: sanitize_filename(compat_str(v), restricted=self.params.get('restrictfilenames'), is_id=(k == 'id')))\n        template_dict = dict(((k, sanitize(k, v)) for (k, v) in template_dict.items() if (v is not None)))\n        template_dict = collections.defaultdict((lambda : 'NA'), template_dict)\n        outtmpl = self.params.get('outtmpl', DEFAULT_OUTTMPL)\n        tmpl = compat_expanduser(outtmpl)\n        filename = (tmpl % template_dict)\n        return filename\n    except ValueError as err:\n        self.report_error((((('Error in output template: ' + str(err)) + ' (encoding: ') + repr(preferredencoding())) + ')'))\n        return None\n", "label": 1}
{"function": "\n\ndef get_user_details(self, response):\n    'Return user details from an OpenID request'\n    values = {\n        'username': '',\n        'email': '',\n        'fullname': '',\n        'first_name': '',\n        'last_name': '',\n    }\n    values.update(self.values_from_response(response, self.get_sreg_attributes(), self.get_ax_attributes()))\n    fullname = (values.get('fullname') or '')\n    first_name = (values.get('first_name') or '')\n    last_name = (values.get('last_name') or '')\n    email = (values.get('email') or '')\n    if ((not fullname) and first_name and last_name):\n        fullname = ((first_name + ' ') + last_name)\n    elif fullname:\n        try:\n            (first_name, last_name) = fullname.rsplit(' ', 1)\n        except ValueError:\n            last_name = fullname\n    username_key = (self.setting('USERNAME_KEY') or self.USERNAME_KEY)\n    values.update({\n        'fullname': fullname,\n        'first_name': first_name,\n        'last_name': last_name,\n        'username': (values.get(username_key) or (first_name.title() + last_name.title())),\n        'email': email,\n    })\n    return values\n", "label": 1}
{"function": "\n\ndef __init__(self, verbose_name=None, name=None, max_digits=None, decimal_places=None, default=None, default_currency=DEFAULT_CURRENCY, currency_choices=CURRENCY_CHOICES, **kwargs):\n    nullable = kwargs.get('null', False)\n    if ((default is None) and (not nullable)):\n        default = 0.0\n    if isinstance(default, string_types):\n        try:\n            (amount, currency) = default.split(' ')\n        except ValueError:\n            amount = default\n            currency = default_currency\n        default = Money(Decimal(amount), Currency(code=currency))\n    elif isinstance(default, (float, Decimal, int)):\n        default = Money(default, default_currency)\n    if ((not (nullable and (default is None))) and (not isinstance(default, Money))):\n        raise Exception(('default value must be an instance of Money, is: %s' % str(default)))\n    if (max_digits is None):\n        raise Exception('You have to provide a max_digits attribute to Money fields.')\n    if (decimal_places is None):\n        raise Exception('You have to provide a decimal_places attribute to Money fields.')\n    if (not default_currency):\n        default_currency = default.currency\n    self.default_currency = default_currency\n    self.currency_choices = currency_choices\n    self.frozen_by_south = kwargs.pop('frozen_by_south', False)\n    super(MoneyField, self).__init__(verbose_name, name, max_digits, decimal_places, default=default, **kwargs)\n", "label": 1}
{"function": "\n\ndef _get_rc():\n    \"\\n    Returns a dict where the key is the daemon's name and\\n    the value a boolean indicating its status (True: enabled or False: disabled).\\n    Check the daemons started by the system in /etc/rc and\\n    configured in /etc/rc.conf and /etc/rc.conf.local.\\n    Also add to the dict all the localy enabled daemons via $pkg_scripts.\\n    \"\n    daemons_flags = {\n        \n    }\n    try:\n        with salt.utils.fopen('/etc/rc', 'r') as handle:\n            lines = handle.readlines()\n    except IOError:\n        log.error('Unable to read /etc/rc')\n    else:\n        for line in lines:\n            match = start_daemon_call_regex.match(line)\n            if match:\n                line = line[len(match.group(1)):]\n                for daemon in start_daemon_parameter_regex.findall(line):\n                    daemons_flags[daemon] = True\n    variables = __salt__['cmd.run']('(. /etc/rc.conf && set)', clean_env=True, output_loglevel='quiet', python_shell=True).split('\\n')\n    for var in variables:\n        match = service_flags_regex.match(var)\n        if match:\n            if (match.group(2) == 'NO'):\n                daemons_flags[match.group(1)] = False\n        else:\n            match = pkg_scripts_regex.match(var)\n            if match:\n                for daemon in match.group(1).split():\n                    daemons_flags[daemon] = True\n    return daemons_flags\n", "label": 1}
{"function": "\n\ndef cart_inline(request, template_name='lfs/checkout/checkout_cart_inline.html'):\n    'Displays the cart items of the checkout page.\\n\\n    Factored out to be reusable for the starting request (which renders the\\n    whole checkout page and subsequent ajax requests which refresh the\\n    cart items.\\n    '\n    cart = cart_utils.get_cart(request)\n    selected_shipping_method = lfs.shipping.utils.get_selected_shipping_method(request)\n    shipping_costs = lfs.shipping.utils.get_shipping_costs(request, selected_shipping_method)\n    selected_payment_method = lfs.payment.utils.get_selected_payment_method(request)\n    payment_costs = lfs.payment.utils.get_payment_costs(request, selected_payment_method)\n    cart_price = 0\n    cart_tax = 0\n    if (cart is not None):\n        cart_price = ((cart.get_price_gross(request) + shipping_costs['price_gross']) + payment_costs['price'])\n        cart_tax = ((cart.get_tax(request) + shipping_costs['tax']) + payment_costs['tax'])\n    discounts = lfs.discounts.utils.get_valid_discounts(request)\n    for discount in discounts:\n        cart_price = (cart_price - discount['price_gross'])\n        cart_tax = (cart_tax - discount['tax'])\n    voucher_number = ''\n    display_voucher = False\n    voucher_value = 0\n    voucher_tax = 0\n    voucher_message = MESSAGES[6]\n    if (cart is not None):\n        try:\n            voucher_number = lfs.voucher.utils.get_current_voucher_number(request)\n            voucher = Voucher.objects.get(number=voucher_number)\n        except Voucher.DoesNotExist:\n            pass\n        else:\n            lfs.voucher.utils.set_current_voucher_number(request, voucher_number)\n            (is_voucher_effective, voucher_message) = voucher.is_effective(request, cart)\n            if is_voucher_effective:\n                display_voucher = True\n                voucher_value = voucher.get_price_gross(request, cart)\n                cart_price = (cart_price - voucher_value)\n                voucher_tax = voucher.get_tax(request, cart)\n                cart_tax = (cart_tax - voucher_tax)\n            else:\n                display_voucher = False\n                voucher_value = 0\n                voucher_tax = 0\n    if (cart_price < 0):\n        cart_price = 0\n    if (cart_tax < 0):\n        cart_tax = 0\n    cart_items = []\n    if cart:\n        for cart_item in cart.get_items():\n            product = cart_item.product\n            quantity = product.get_clean_quantity(cart_item.amount)\n            cart_items.append({\n                'obj': cart_item,\n                'quantity': quantity,\n                'product': product,\n                'product_price_net': cart_item.get_price_net(request),\n                'product_price_gross': cart_item.get_price_gross(request),\n                'product_tax': cart_item.get_tax(request),\n            })\n    return render_to_string(template_name, RequestContext(request, {\n        'cart': cart,\n        'cart_items': cart_items,\n        'cart_price': cart_price,\n        'cart_tax': cart_tax,\n        'display_voucher': display_voucher,\n        'discounts': discounts,\n        'voucher_value': voucher_value,\n        'voucher_tax': voucher_tax,\n        'shipping_costs': shipping_costs,\n        'payment_price': payment_costs['price'],\n        'selected_shipping_method': selected_shipping_method,\n        'selected_payment_method': selected_payment_method,\n        'voucher_number': voucher_number,\n        'voucher_message': voucher_message,\n    }))\n", "label": 1}
{"function": "\n\ndef _handle_network(self, payload, client, networks):\n    'Set the network part for the payload. Support the following:\\n\\n        1) --net-id=public\\n\\n        2) --net-id=private\\n\\n        3) --net-id=<private id>\\n\\n        4) --net-id=<private id> --net-id=<public id>\\n        '\n    if (len(networks) > 2):\n        raise Exception('Too many net-id arguments')\n    if (networks[0]['uuid'] == 'public'):\n        if (len(networks) > 1):\n            raise Exception('Too many net-id arguments. Please indicate only \"public\" or \"private\"')\n        payload['private'] = False\n        return\n    elif (networks[0]['uuid'] == 'private'):\n        if (len(networks) > 1):\n            raise Exception('Too many net-id arguments. Please indicate only \"public\" or \"private\"')\n        payload['private'] = True\n        return\n    private_network_only = True\n    try:\n        _filter = {\n            'networkVlans': {\n                'id': {\n                    'operation': int(networks[0]['uuid']),\n                },\n            },\n        }\n    except Exception:\n        raise ValueError('Invalid id format')\n    priv_id_valid = client['Account'].getPrivateNetworkVlans(filter=_filter)\n    if priv_id_valid:\n        payload['private_vlan'] = int(networks[0]['uuid'])\n    else:\n        raise Exception('Private vlan must be specified first or is invalid')\n    if (len(networks) == 2):\n        try:\n            _filter = {\n                'networkVlans': {\n                    'id': {\n                        'operation': int(networks[1]['uuid']),\n                    },\n                },\n            }\n        except Exception:\n            raise ValueError('Invalid id format')\n        pub_id_valid = client['Account'].getPublicNetworkVlans(filter=_filter)\n        if pub_id_valid:\n            payload['public_vlan'] = int(networks[1]['uuid'])\n            private_network_only = False\n        else:\n            raise Exception('Public vlan must be specified second or is invalid')\n    payload['private'] = private_network_only\n", "label": 1}
{"function": "\n\ndef run(self):\n    self.socket.settimeout(1)\n    debug('Socket about to accept')\n    while (self.socket is not None):\n        try:\n            (self.conn, addr) = self.socket.accept()\n            self.conn.settimeout(None)\n        except socket.error as msg:\n            debug(('Accept: ' + str(msg)))\n        else:\n            debug('Socket accepted')\n            break\n    if (self.socket is None):\n        return\n    debug(((('Connected with ' + addr[0]) + ':') + str(addr[1])))\n    self.initPUTComms()\n    self.writeToPUT('STEP\\n')\n    data = self.readFromPUT()\n    debug(('data: ' + str(data)))\n    bpResponse = self.readFromPUT()\n    debug(('bpResponse: ' + bpResponse))\n    bpMatches = re.search('^202 Paused\\\\s+(.+?)\\\\s+(\\\\d+)$', bpResponse.strip())\n    if (bpMatches is not None):\n        filename = bpMatches.group(1)\n        line = bpMatches.group(2)\n        debug('run: filename {0}, line {1}'.format(filename, line))\n        if (not filename.endswith('main.lua')):\n            debugger_status('Error running main.lua')\n            on_main_thread((lambda : sublime.error_message('There was an error running main.lua.\\n\\nCheck Console for error messages.')))\n        else:\n            debugger_status('Paused at line {0} of {1}'.format(line, filename))\n            on_main_thread((lambda : self.showSublimeContext(filename, int(line))))\n    else:\n        errMatches = re.search('^401 Error in Execution (\\\\d+)$', bpResponse.strip())\n        if (errMatches is not None):\n            size = errMatches.group(1)\n            console_output('Error in remote application: ')\n            console_output(self.readFromPUT(size))\n        else:\n            print('Corona Editor Error: ', bpResponse)\n            on_main_thread((lambda : sublime.error_message((('Unexpected response from Simulator:\\n\\n' + str(bpResponse)) + '\\n\\nCheck Console for error messages.'))))\n    on_main_thread((lambda : self.restore_breakpoints()))\n    self.doCommand('backtrace')\n    if (corona_sdk_version and ((int(corona_sdk_version) >= 2489) and (int(corona_sdk_version) < 2517))):\n        variables_output((('Local variable display disabled with this version of Corona SDK (' + corona_sdk_version) + ').  Try a build after 2515'))\n    else:\n        self.doCommand('locals')\n    while self.debugger_running:\n        cmd = debuggerCmdQ.get()\n        self.performCommand(cmd)\n        debuggerCmdQ.task_done()\n    on_main_thread((lambda : self.completionCallback(self.threadID)))\n    debug('CoronaDebuggerThread: ends')\n", "label": 1}
{"function": "\n\ndef invite(so, stdout, stderr, offer_mailbox=False, accept_mailbox=False):\n    args = {\n        'petname': so.get('petname'),\n        'code': so.get('code'),\n    }\n    if offer_mailbox:\n        args['offer_mailbox'] = True\n    if accept_mailbox:\n        args['accept_mailbox'] = True\n    (ok, result) = command(so['basedir'], 'invite', args, stderr)\n    if (not ok):\n        ((print >> stderr), result['err'])\n        return 1\n    cid = result['contact-id']\n    if (so.get('code') is not None):\n        stdout.write(('Invitation code: %s\\n' % so['code']))\n        stdout.flush()\n        return 0\n    try:\n        resp = follow_events(so['basedir'], 'addressbook', catchup=True)\n        if (resp.status != 200):\n            ((print >> stderr), 'Error:', resp.status, resp.reason)\n            return 1\n        while True:\n            (fieldname, value) = get_field(resp)\n            if (fieldname == 'data'):\n                data = json.loads(value)\n                if ((data['type'] == 'addressbook') and (data['new_value']['id'] == cid)):\n                    code = data['new_value']['invitation_code']\n                    stdout.write(('Invitation code: %s\\n' % code))\n                    stdout.flush()\n                    return 0\n    except (KeyboardInterrupt, EOFError):\n        return 0\n", "label": 1}
{"function": "\n\ndef autodiscover():\n    from django.conf import settings\n    cache = getattr(settings, 'CUDDLYBUDDLY_THUMBNAIL_CACHE', None)\n    if (cache is None):\n        return\n    global LOADING\n    if LOADING:\n        return\n    LOADING = True\n    import imp\n    from django.db.models import Model\n    from django.db.models.base import ModelBase\n    from django.db.models.fields.files import FieldFile\n    from django.db.models.signals import post_save, pre_delete\n    from django.utils.importlib import import_module\n    from cuddlybuddly.thumbnail.listeners import update_cache\n    for app in settings.INSTALLED_APPS:\n        try:\n            app_path = import_module(app).__path__\n        except AttributeError:\n            continue\n        try:\n            imp.find_module('models', app_path)\n        except ImportError:\n            continue\n        models = import_module(('%s.models' % app))\n        for model in models.__dict__.values():\n            if (isinstance(model, ModelBase) and (model is not Model)):\n                modelinit = model()\n                for field in modelinit.__dict__.keys():\n                    if isinstance(getattr(modelinit, field, None), FieldFile):\n                        post_save.connect(update_cache, sender=model)\n                        pre_delete.connect(update_cache, sender=model)\n    LOADING = False\n", "label": 1}
{"function": "\n\ndef test_reduction():\n    t = Symbol('t', 'var * {name: string, amount: int32}')\n    r = sum(t['amount'])\n    assert (r.dshape in (dshape('int64'), dshape('{amount: int64}'), dshape('{amount_sum: int64}')))\n    assert ('amount' not in str(t.count().dshape))\n    assert (t.count().dshape[0] in (int32, int64))\n    assert ('int' in str(t.count().dshape))\n    assert ('int' in str(t.nunique().dshape))\n    assert ('string' in str(t['name'].max().dshape))\n    assert ('string' in str(t['name'].min().dshape))\n    assert ('string' not in str(t.count().dshape))\n    t = Symbol('t', 'var * {name: string, amount: real, id: int}')\n    assert ('int' in str(t['id'].sum().dshape))\n    assert ('int' not in str(t['amount'].sum().dshape))\n", "label": 1}
{"function": "\n\ndef proc_dam(filename):\n    \"Load an dam file that has already been processed by the official matlab\\n    file converter.  That matlab data is saved to an m-file, which is then\\n    converted to a numpy '.npz' file.  This numpy file is the file actually\\n    loaded.  This function converts it to a neo block and returns the block.\\n    This block can be compared to the block produced by BrainwareDamIO to\\n    make sure BrainwareDamIO is working properly\\n\\n    block = proc_dam(filename)\\n\\n    filename: The file name of the numpy file to load.  It should end with\\n    '*_dam_py?.npz'. This will be converted to a neo 'file_origin' property\\n    with the value '*.dam', so the filename to compare should fit that pattern.\\n    'py?' should be 'py2' for the python 2 version of the numpy file or 'py3'\\n    for the python 3 version of the numpy file.\\n\\n    example: filename = 'file1_dam_py2.npz'\\n             dam file name = 'file1.dam'\\n    \"\n    with np.load(filename) as damobj:\n        damfile = damobj.items()[0][1].flatten()\n    filename = os.path.basename((filename[:(- 12)] + '.dam'))\n    signals = [res.flatten() for res in damfile['signal']]\n    stimIndexes = [int(res[(0, 0)].tolist()) for res in damfile['stimIndex']]\n    timestamps = [res[(0, 0)] for res in damfile['timestamp']]\n    block = Block(file_origin=filename)\n    rcg = RecordingChannelGroup(file_origin=filename)\n    chan = RecordingChannel(file_origin=filename, index=0, name='Chan1')\n    rcg.channel_indexes = np.array([1])\n    rcg.channel_names = np.array(['Chan1'], dtype='S')\n    block.recordingchannelgroups.append(rcg)\n    rcg.recordingchannels.append(chan)\n    params = [res['params'][(0, 0)].flatten() for res in damfile['stim']]\n    values = [res['values'][(0, 0)].flatten() for res in damfile['stim']]\n    params = [[res1[0] for res1 in res] for res in params]\n    values = [[res1 for res1 in res] for res in values]\n    stims = [dict(zip(param, value)) for (param, value) in zip(params, values)]\n    fulldam = zip(stimIndexes, timestamps, signals, stims)\n    for (stimIndex, timestamp, signal, stim) in fulldam:\n        sig = AnalogSignal(signal=(signal * pq.mV), t_start=(timestamp * pq.d), file_origin=filename, sampling_period=(1.0 * pq.s))\n        segment = Segment(file_origin=filename, index=stimIndex, **stim)\n        segment.analogsignals = [sig]\n        block.segments.append(segment)\n    block.create_many_to_one_relationship()\n    return block\n", "label": 1}
{"function": "\n\ndef loop(self, start, niters, loop_body, return_stmt=False, while_loop=False, step=None, merge=None):\n    if isinstance(start, (int, long)):\n        start = self.int(start)\n    assert isinstance(start, Expr)\n    if isinstance(niters, (int, long)):\n        niters = self.int(niters)\n    assert isinstance(niters, Expr)\n    if (step is None):\n        step = one(start.type)\n    elif isinstance(step, (int, long)):\n        step = self.int(step)\n    assert isinstance(step, Expr)\n    if (merge is None):\n        merge = {\n            \n        }\n    if while_loop:\n        (i, i_after, i_merge) = self.loop_var('i', start)\n        merge.update(i_merge)\n        cond = self.lt(i, niters)\n        self.blocks.push()\n        loop_body(i)\n        self.assign(i_after, self.add(i, step))\n        body = self.blocks.pop()\n        loop_stmt = While(cond, body, merge)\n    else:\n        var = self.fresh_var(start.type, 'i')\n        self.blocks.push()\n        loop_body(var)\n        body = self.blocks.pop()\n        loop_stmt = ForLoop(var, start, niters, step, body, merge)\n    if return_stmt:\n        return loop_stmt\n    else:\n        self.blocks += loop_stmt\n", "label": 1}
{"function": "\n\ndef main():\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter, description='Downloads one or more Flickr photo sets.\\n\\nTo use it you need to get your own Flickr API key here:\\nhttps://www.flickr.com/services/api/misc.api_keys.html\\n\\nFor more information see:\\nhttps://github.com/beaufour/flickr-download', epilog='examples:\\n  list all sets for a user:\\n  > {app} -k <api_key> -s <api_secret> -l beaufour\\n\\n  download a given set:\\n  > {app} -k <api_key> -s <api_secret> -d 72157622764287329\\n\\n  download a given set, keeping duplicate names:\\n  > {app} -k <api_key> -s <api_secret> -d 72157622764287329 -n title_increment\\n'.format(app=sys.argv[0]))\n    parser.add_argument('-k', '--api_key', type=str, help='Flickr API key')\n    parser.add_argument('-s', '--api_secret', type=str, help='Flickr API secret')\n    parser.add_argument('-t', '--user_auth', action='store_true', help='Enable user authentication')\n    parser.add_argument('-l', '--list', type=str, metavar='USER', help='List photosets for a user')\n    parser.add_argument('-d', '--download', type=str, metavar='SET_ID', help='Download the given set')\n    parser.add_argument('-u', '--download_user', type=str, metavar='USERNAME', help='Download all sets for a given user')\n    parser.add_argument('-q', '--quality', type=str, metavar='SIZE_LABEL', default=None, help='Quality of the picture')\n    parser.add_argument('-n', '--naming', type=str, metavar='NAMING_MODE', help='Photo naming mode')\n    parser.add_argument('-m', '--list_naming', action='store_true', help='List naming modes')\n    parser.set_defaults(**_load_defaults())\n    args = parser.parse_args()\n    if args.list_naming:\n        print(get_filename_handler_help())\n        return 1\n    if ((not args.api_key) or (not args.api_secret)):\n        print('You need to pass in both \"api_key\" and \"api_secret\" arguments', file=sys.stderr)\n        return 1\n    ret = _init(args.api_key, args.api_secret, args.user_auth)\n    if (not ret):\n        return 1\n    if (sys.stdout.encoding.lower() != 'utf-8'):\n        sys.stdout = codecs.getwriter(sys.stdout.encoding)(sys.stdout, 'replace')\n    if args.list:\n        print_sets(args.list)\n        return 0\n    if (args.download or args.download_user):\n        try:\n            get_filename = get_filename_handler(args.naming)\n            if args.download:\n                download_set(args.download, get_filename, args.quality)\n            else:\n                download_user(args.download_user, get_filename, args.quality)\n        except KeyboardInterrupt:\n            print('Forcefully aborting. Last photo download might be partial :(', file=sys.stderr)\n        return 0\n    print('ERROR: Must pass either --list or --download\\n', file=sys.stderr)\n    parser.print_help()\n    return 1\n", "label": 1}
{"function": "\n\ndef readline(self):\n    if self.no_more_data_for_this_file:\n        return b''\n    boundary_pos = newline_pos = (- 1)\n    while ((newline_pos < 0) and (boundary_pos < 0)):\n        try:\n            chunk = self.wsgi_input.read(self.read_chunk_size)\n        except (IOError, ValueError) as e:\n            raise swift.common.exceptions.ChunkReadError(str(e))\n        self.input_buffer += chunk\n        newline_pos = self.input_buffer.find(b'\\r\\n')\n        boundary_pos = self.input_buffer.find(self.boundary)\n        if (not chunk):\n            self.no_more_files = True\n            break\n    if ((newline_pos >= 0) and ((boundary_pos < 0) or (newline_pos < boundary_pos))):\n        ret = b''\n        to_read = (newline_pos + 2)\n        while (to_read > 0):\n            chunk = self.read(to_read)\n            if (not chunk):\n                break\n            to_read -= len(chunk)\n            ret += chunk\n        return ret\n    else:\n        return self.read(len(self.input_buffer))\n", "label": 1}
{"function": "\n\ndef process_body_row(linedict, filingnum, header_id, is_amended, cd, filer_id):\n    form = linedict['form_parser']\n    try:\n        if (linedict['memo_code'] == 'X'):\n            linedict['superceded_by_amendment'] = True\n    except KeyError:\n        pass\n    if (form == 'SchA'):\n        skeda_from_skedadict(linedict, filingnum, header_id, is_amended, cd)\n    elif (form == 'SchB'):\n        skedb_from_skedbdict(linedict, filingnum, header_id, is_amended, cd)\n    elif (form == 'SchE'):\n        skede_from_skededict(linedict, filingnum, header_id, is_amended, cd)\n    elif (form == 'F65'):\n        skeda_from_f65(linedict, filingnum, header_id, is_amended, cd)\n    elif (form == 'F56'):\n        skeda_from_f56(linedict, filingnum, header_id, is_amended, cd)\n    elif (form == 'F92'):\n        skeda_from_f92(linedict, filingnum, header_id, is_amended, cd)\n    elif (form == 'F132'):\n        skeda_from_f132(linedict, filingnum, header_id, is_amended, cd)\n    elif (form == 'F133'):\n        skeda_from_f133(linedict, filingnum, header_id, is_amended, cd)\n    elif (form == 'F57'):\n        skede_from_f57(linedict, filingnum, header_id, is_amended, cd)\n    else:\n        otherline_from_line(linedict, filingnum, header_id, is_amended, cd, filer_id)\n", "label": 1}
{"function": "\n\ndef build_proof(self, env, message=''):\n    'Generates a proof within an environment of assigned public and secret variables.'\n    self._check_env(env)\n    if __debug__:\n        for (base, expr) in self.proofs:\n            xGen = base.val(env)\n            xExpr = expr.val(env)\n            try:\n                assert (xGen == xExpr)\n            except:\n                raise Exception((\"Proof about '%s' does not hold.\" % base.name))\n    G = self.G\n    order = G.order()\n    state = ['ZKP', G.nid(), message]\n    for v in sorted(self.Const.keys()):\n        state += [env[v]]\n    for v in sorted(self.Pub.keys()):\n        state += [env[v]]\n    witnesses = dict(env.items())\n    for w in self.Sec.keys():\n        assert (w in witnesses)\n        witnesses[w] = order.random()\n    for (base, expr) in self.proofs:\n        Cw = expr.val(witnesses)\n        state += [Cw]\n    hash_c = challenge(state)\n    c = (Bn.from_binary(hash_c) % order)\n    responses = dict(env.items())\n    for w in self.Sec.keys():\n        responses[w] = ((witnesses[w] - (c * env[w])) % order)\n    for v in self.Const:\n        del responses[v]\n    return (c, responses)\n", "label": 1}
{"function": "\n\ndef create(self, **fields):\n    'Creates a new resource in Redmine database and returns resource object on success'\n    if ((self.resource_class.query_create is None) or (self.resource_class.container_create is None)):\n        raise ResourceBadMethodError\n    if (not fields):\n        raise ResourceNoFieldsProvidedError\n    formatter = MemorizeFormatter()\n    title = fields.get('title')\n    if ((title is not None) and is_unicode(title)):\n        fields['title'] = to_string(title)\n    try:\n        url = '{0}{1}'.format(self.redmine.url, formatter.format(self.resource_class.query_create, **fields))\n    except KeyError as exception:\n        raise ValidationError('{0} field is required'.format(exception))\n    self.container = self.resource_class.container_one\n    data = {\n        self.resource_class.container_create: self.prepare_params(formatter.unused_kwargs),\n    }\n    if ('uploads' in data[self.resource_class.container_create]):\n        data['attachments'] = data[self.resource_class.container_create].pop('uploads')\n        for (index, attachment) in enumerate(data['attachments']):\n            data['attachments'][index]['token'] = self.redmine.upload(attachment.get('path', ''))\n    try:\n        response = self.redmine.request('post', url, data=data)\n    except ResourceNotFoundError:\n        response = self.redmine.request('put', url, data=data)\n    try:\n        resource = self.to_resource(response[self.container])\n    except TypeError:\n        raise ValidationError('Resource already exists')\n    self.params = formatter.used_kwargs\n    self.url = '{0}{1}'.format(self.redmine.url, self.resource_class.query_one.format(resource.internal_id, **fields))\n    return resource\n", "label": 1}
{"function": "\n\ndef post_process(self, paths, dry_run=False, **options):\n    '\\n        Post process the given list of files (called from collectstatic).\\n\\n        Processing finds paths that match the configuration,\\n        gzips them and copies them to the target storage with\\n        the name generated by utils.get_gzipped_name.\\n\\n        '\n    if hasattr(super(SaveGzippedCopyMixin, self), 'post_process'):\n        processor = super(SaveGzippedCopyMixin, self).post_process(paths=paths.copy(), dry_run=dry_run, options=options)\n        for (original_path, processed_path, processed) in processor:\n            if (processed and (original_path != processed_path)):\n                paths[processed_path] = (self, processed_path)\n            (yield (original_path, processed_path, processed))\n    if dry_run:\n        return\n    path_level = (lambda name: len(name.split(os.sep)))\n    adjustable_paths = [path for path in sorted(paths.keys(), key=path_level, reverse=True) if utils.should_save_gzipped_copy(path)]\n    for name in adjustable_paths:\n        (storage, path) = paths[name]\n        gzipped_name = utils.get_gzipped_name(name)\n        if (not self.should_skip_processing(storage, path, gzipped_name)):\n            with storage.open(path) as original_file:\n                if hasattr(original_file, 'seek'):\n                    original_file.seek(0)\n                pregzipped_file = ContentFile(original_file.read())\n                pregzipped_file = self.pre_save_gzipped(name, gzipped_name, pregzipped_file)\n                if self.exists(gzipped_name):\n                    self.delete(gzipped_name)\n                gzipped_file = self.gzipped_file(name, gzipped_name, pregzipped_file)\n                saved_name = self._save(gzipped_name, gzipped_file)\n                gzipped_name = force_text(saved_name.replace('\\\\', '/'))\n                self.post_save_gzipped(name, gzipped_name, gzipped_file)\n                (yield (name, gzipped_name, True))\n", "label": 1}
{"function": "\n\ndef getFeature(self, name):\n    xname = _name_xform(name)\n    try:\n        return getattr(self._options, xname)\n    except AttributeError:\n        if (name == 'infoset'):\n            options = self._options\n            return (options.datatype_normalization and options.whitespace_in_element_content and options.comments and options.charset_overrides_xml_encoding and (not (options.namespace_declarations or options.validate_if_schema or options.create_entity_ref_nodes or options.entities or options.cdata_sections)))\n        raise xml.dom.NotFoundErr(('feature %s not known' % repr(name)))\n", "label": 1}
{"function": "\n\ndef update():\n    '\\n    Execute an hg pull on all of the repos\\n    '\n    data = {\n        'changed': False,\n        'backend': 'hgfs',\n    }\n    (data['changed'], repos) = _clear_old_remotes()\n    for repo in repos:\n        if os.path.exists(repo['lockfile']):\n            log.warning(\"Update lockfile is present for hgfs remote {0}, skipping. If this warning persists, it is possible that the update process was interrupted. Removing {1} or running 'salt-run fileserver.clear_lock hgfs' will allow updates to continue for this remote.\".format(repo['url'], repo['lockfile']))\n            continue\n        (_, errors) = lock(repo)\n        if errors:\n            log.error('Unable to set update lock for hgfs remote {0}, skipping.'.format(repo['url']))\n            continue\n        log.debug('hgfs is fetching from {0}'.format(repo['url']))\n        repo['repo'].open()\n        curtip = repo['repo'].tip()\n        try:\n            repo['repo'].pull()\n        except Exception as exc:\n            log.error('Exception {0} caught while updating hgfs remote {1}'.format(exc, repo['url']), exc_info_on_loglevel=logging.DEBUG)\n        else:\n            newtip = repo['repo'].tip()\n            if (curtip[1] != newtip[1]):\n                data['changed'] = True\n        repo['repo'].close()\n        clear_lock(repo)\n    env_cache = os.path.join(__opts__['cachedir'], 'hgfs/envs.p')\n    if ((data.get('changed', False) is True) or (not os.path.isfile(env_cache))):\n        env_cachedir = os.path.dirname(env_cache)\n        if (not os.path.exists(env_cachedir)):\n            os.makedirs(env_cachedir)\n        new_envs = envs(ignore_cache=True)\n        serial = salt.payload.Serial(__opts__)\n        with salt.utils.fopen(env_cache, 'w+') as fp_:\n            fp_.write(serial.dumps(new_envs))\n            log.trace('Wrote env cache data to {0}'.format(env_cache))\n    if __opts__.get('fileserver_events', False):\n        event = salt.utils.event.get_event('master', __opts__['sock_dir'], __opts__['transport'], opts=__opts__, listen=False)\n        event.fire_event(data, tagify(['hgfs', 'update'], prefix='fileserver'))\n    try:\n        salt.fileserver.reap_fileserver_cache_dir(os.path.join(__opts__['cachedir'], 'hgfs/hash'), find_file)\n    except (IOError, OSError):\n        pass\n", "label": 1}
{"function": "\n\ndef updateClientSideDetails(self, sw, sh, tzo, rtzo, dstSavings, dstInEffect, curDate, touchDevice):\n    'For internal use by AbstractApplicationServlet only. Updates all\\n        properties in the class according to the given information.\\n\\n        @param sw:\\n                   Screen width\\n        @param sh:\\n                   Screen height\\n        @param tzo:\\n                   TimeZone offset in minutes from GMT\\n        @param rtzo:\\n                   raw TimeZone offset in minutes from GMT (w/o DST adjustment)\\n        @param dstSavings:\\n                   the difference between the raw TimeZone and DST in minutes\\n        @param dstInEffect:\\n                   is DST currently active in the region or not?\\n        @param curDate:\\n                   the current date in milliseconds since the epoch\\n        @param touchDevice:\\n        '\n    if (sw is not None):\n        try:\n            self._screenHeight = int(sh)\n            self._screenWidth = int(sw)\n        except ValueError:\n            self._screenHeight = self._screenWidth = 0\n    if (tzo is not None):\n        try:\n            self._timezoneOffset = (((- int(tzo)) * 60) * 1000)\n        except ValueError:\n            self._timezoneOffset = 0\n    if (rtzo is not None):\n        try:\n            self._rawTimezoneOffset = (((- int(rtzo)) * 60) * 1000)\n        except ValueError:\n            self._rawTimezoneOffset = 0\n    if (dstSavings is not None):\n        try:\n            self._dstSavings = ((int(dstSavings) * 60) * 1000)\n        except ValueError:\n            self._dstSavings = 0\n    if (dstInEffect is not None):\n        self._dstInEffect = bool(dstInEffect)\n    if (curDate is not None):\n        try:\n            curTime = int(curDate)\n            self._clientServerTimeDelta = (curTime - time())\n        except ValueError:\n            self._clientServerTimeDelta = 0\n    self._touchDevice = touchDevice\n", "label": 1}
{"function": "\n\ndef updatedJobWorker(self):\n    \"\\n        We use the parasol results to update the status of jobs, adding them\\n        to the list of updated jobs.\\n\\n        Results have the following structure.. (thanks Mark D!)\\n\\n        int status;    /* Job status - wait() return format. 0 is good. */\\n        char *host;    /* Machine job ran on. */\\n        char *jobId;    /* Job queuing system job ID */\\n        char *exe;    /* Job executable file (no path) */\\n        int usrTicks;    /* 'User' CPU time in ticks. */\\n        int sysTicks;    /* 'System' CPU time in ticks. */\\n        unsigned submitTime;    /* Job submission time in seconds since 1/1/1970 */\\n        unsigned startTime;    /* Job start time in seconds since 1/1/1970 */\\n        unsigned endTime;    /* Job end time in seconds since 1/1/1970 */\\n        char *user;    /* User who ran job */\\n        char *errFile;    /* Location of stderr file on host */\\n\\n        Plus you finally have the command name.\\n        \"\n    resultsFiles = set()\n    resultsFileHandles = []\n    try:\n        while self.running:\n            newResultsFiles = set(os.listdir(self.parasolResultsDir)).difference(resultsFiles)\n            for newFile in newResultsFiles:\n                newFilePath = os.path.join(self.parasolResultsDir, newFile)\n                resultsFileHandles.append(open(newFilePath, 'r'))\n                resultsFiles.add(newFile)\n            for fileHandle in resultsFileHandles:\n                while self.running:\n                    line = fileHandle.readline()\n                    if (not line):\n                        break\n                    assert (line[(- 1)] == '\\n')\n                    (status, host, jobId, exe, usrTicks, sysTicks, submitTime, startTime, endTime, user, errFile, command) = line[:(- 1)].split(None, 11)\n                    status = int(status)\n                    jobId = int(jobId)\n                    if os.WIFEXITED(status):\n                        status = os.WEXITSTATUS(status)\n                    else:\n                        status = (- status)\n                    self.cpuUsageQueue.put(jobId)\n                    startTime = int(startTime)\n                    endTime = int(endTime)\n                    if (endTime == startTime):\n                        usrTicks = int(usrTicks)\n                        sysTicks = int(sysTicks)\n                        wallTime = (float(max(1, (usrTicks + sysTicks))) * 0.01)\n                    else:\n                        wallTime = float((endTime - startTime))\n                    self.updatedJobsQueue.put((jobId, status, wallTime))\n            time.sleep(1)\n    except:\n        logger.warn('Error occurred while parsing parasol results files.')\n        raise\n    finally:\n        for fileHandle in resultsFileHandles:\n            fileHandle.close()\n", "label": 1}
{"function": "\n\n@handle_response_format\n@treeio_login_required\ndef messaging_compose(request, response_format='html'):\n    'New message page'\n    user = request.user.profile\n    if request.POST:\n        if ('cancel' not in request.POST):\n            message = Message()\n            message.author = user.get_contact()\n            if (not message.author):\n                return user_denied(request, message=\"You can't send message without a Contact Card assigned to you.\", response_format=response_format)\n            form = MessageForm(request.user.profile, None, None, request.POST, instance=message)\n            if form.is_valid():\n                message = form.save()\n                message.recipients.add(user.get_contact())\n                message.set_user_from_request(request)\n                message.read_by.add(user)\n                try:\n                    if (('multicomplete_recipients' in request.POST) and request.POST['multicomplete_recipients']):\n                        try:\n                            conf = ModuleSetting.get_for_module('treeio.messaging', 'default_contact_type')[0]\n                            default_contact_type = ContactType.objects.get(pk=long(conf.value))\n                        except Exception:\n                            default_contact_type = None\n                        emails = request.POST['multicomplete_recipients'].split(',')\n                        for email in emails:\n                            emailstr = unicode(email).strip()\n                            if re.match('[a-zA-Z0-9+_\\\\-\\\\.]+@[0-9a-zA-Z][.-0-9a-zA-Z]*.[a-zA-Z]+', emailstr):\n                                (contact, created) = Contact.get_or_create_by_email(emailstr, contact_type=default_contact_type)\n                                message.recipients.add(contact)\n                                if created:\n                                    contact.set_user_from_request(request)\n                except:\n                    pass\n                message.send_email()\n                return HttpResponseRedirect(reverse('messaging'))\n        else:\n            return HttpResponseRedirect(reverse('messaging'))\n    else:\n        form = MessageForm(request.user.profile, None)\n    context = _get_default_context(request)\n    context.update({\n        'form': form,\n    })\n    return render_to_response('messaging/message_compose', context, context_instance=RequestContext(request), response_format=response_format)\n", "label": 1}
{"function": "\n\ndef path(graph, source, target, excluded_edges=None, ooc_types=ooc_types):\n    ' Path of functions between two types '\n    if (not isinstance(source, type)):\n        source = type(source)\n    if (not isinstance(target, type)):\n        target = type(target)\n    if (source not in graph):\n        for cls in valid_subclasses:\n            if issubclass(source, cls):\n                source = cls\n                break\n    if ooc_types:\n        oocs = tuple(ooc_types)\n        if (issubclass(source, oocs) and issubclass(target, oocs)):\n            graph = graph.subgraph([n for n in graph.nodes() if issubclass(n, oocs)])\n    with without_edges(graph, excluded_edges) as g:\n        pth = nx.shortest_path(g, source=source, target=target, weight='cost')\n        result = [(src, tgt, graph.edge[src][tgt]['func']) for (src, tgt) in zip(pth, pth[1:])]\n    return result\n", "label": 1}
{"function": "\n\ndef run():\n    'CLI runner\\n    '\n    gogo_logger = gogo.Gogo(__name__, verbose=args.verbose).get_logger('run')\n    if args.version:\n        gogo_logger.info(('gogo v%s' % gogo.__version__))\n        exit(0)\n    counted = {'get', 'tcp'}\n    appended = {'filename', 'subject', 'url', 'host', 'port'}\n    items = args._get_kwargs()\n    counted_args = [i for i in items if (i[0] in counted)]\n    appended_args = [i for i in items if (i[0] in appended)]\n    high_appended_args = [(k, v[0]) for (k, v) in appended_args]\n    high_counted_args = [(k, (v > 0)) for (k, v) in counted_args]\n    high_kwargs = dict(it.chain(high_appended_args, high_counted_args))\n    low_appended_args = [(k, v[(- 1)]) for (k, v) in appended_args]\n    low_counted_args = [(k, (v > 1)) for (k, v) in counted_args]\n    low_kwargs = dict(it.chain(low_appended_args, low_counted_args))\n    high_hdlr = getattr(gogo.handlers, ('%s_hdlr' % args.high_hdlr))\n    low_hdlr = getattr(gogo.handlers, ('%s_hdlr' % args.low_hdlr))\n    high_format = getattr(gogo.formatters, ('%s_formatter' % args.high_format))\n    low_format = getattr(gogo.formatters, ('%s_formatter' % args.low_format))\n    nkwargs = {\n        'verbose': args.verbose,\n        'high_level': args.high_level.upper(),\n        'low_level': args.low_level.upper(),\n        'high_formatter': high_format,\n        'low_formatter': low_format,\n        'monolog': args.monolog,\n        'high_hdlr': high_hdlr(**high_kwargs),\n        'low_hdlr': low_hdlr(**low_kwargs),\n    }\n    logger = gogo.Gogo(args.name, **nkwargs).get_logger('runner')\n    try:\n        message = args.message.read()\n    except AttributeError:\n        message = args.message\n    getattr(logger, args.level)(message)\n    exit(0)\n", "label": 1}
{"function": "\n\n@webapi_check_local_site\n@webapi_login_required\n@webapi_response_errors(DOES_NOT_EXIST, NOT_LOGGED_IN, PERMISSION_DENIED, INVALID_FORM_DATA, SERVER_CONFIG_ERROR, BAD_HOST_KEY, UNVERIFIED_HOST_KEY, UNVERIFIED_HOST_CERT, REPO_AUTHENTICATION_ERROR, REPO_INFO_ERROR)\n@webapi_request_fields(optional={\n    'bug_tracker': {\n        'type': six.text_type,\n        'description': 'The URL to a bug in the bug tracker for this repository, with ``%s`` in place of the bug ID.',\n        'added_in': '1.6',\n    },\n    'encoding': {\n        'type': six.text_type,\n        'description': 'The encoding used for files in the repository. This is an advanced setting and should only be used if you absolutely need it.',\n        'added_in': '1.6',\n    },\n    'mirror_path': {\n        'type': six.text_type,\n        'description': 'An alternate path to the repository.',\n        'added_in': '1.6',\n    },\n    'name': {\n        'type': six.text_type,\n        'description': 'The human-readable name of the repository.',\n        'added_in': '1.6',\n    },\n    'password': {\n        'type': six.text_type,\n        'description': 'The password used to access the repository.',\n        'added_in': '1.6',\n    },\n    'path': {\n        'type': six.text_type,\n        'description': 'The path to the repository.',\n        'added_in': '1.6',\n    },\n    'public': {\n        'type': bool,\n        'description': 'Whether or not review requests on the repository will be publicly accessible by users on the site. The default is true.',\n        'added_in': '1.6',\n    },\n    'raw_file_url': {\n        'type': six.text_type,\n        'description': \"A URL mask used to check out a particular file using HTTP. This is needed for repository types that can't access files natively. Use ``<revision>`` and ``<filename>`` in the URL in place of the revision and filename parts of the path.\",\n        'added_in': '1.6',\n    },\n    'trust_host': {\n        'type': bool,\n        'description': 'Whether or not any unknown host key or certificate should be accepted. The default is false, in which case this will error out if encountering an unknown host key or certificate.',\n        'added_in': '1.6',\n    },\n    'username': {\n        'type': six.text_type,\n        'description': 'The username used to access the repository.',\n        'added_in': '1.6',\n    },\n    'archive_name': {\n        'type': bool,\n        'description': \"Whether or not the (non-user-visible) name of the repository should be changed so that it (probably) won't conflict with any future repository names.\",\n        'added_in': '1.6.2',\n    },\n    'visible': {\n        'type': bool,\n        'description': 'Whether the repository is visible.',\n        'added_in': '2.0',\n    },\n})\ndef update(self, request, trust_host=False, *args, **kwargs):\n    \"Updates a repository.\\n\\n        This will update the information on a repository. If the path,\\n        username, or password has changed, Review Board will try again to\\n        verify access to the repository.\\n\\n        In the event of an access problem (authentication problems,\\n        bad/unknown SSH key, or unknown certificate), an error will be\\n        returned and the repository information won't be updated. Pass\\n        ``trust_host=1`` to approve bad/unknown SSH keys or certificates.\\n        \"\n    try:\n        repository = self.get_object(request, *args, **kwargs)\n    except ObjectDoesNotExist:\n        return DOES_NOT_EXIST\n    if (not self.has_modify_permissions(request, repository)):\n        return self.get_no_access_error(request)\n    for field in ('bug_tracker', 'encoding', 'mirror_path', 'name', 'password', 'path', 'public', 'raw_file_url', 'username', 'visible'):\n        value = kwargs.get(field, None)\n        if (value is not None):\n            setattr(repository, field, value)\n    if (('path' in kwargs) or ('username' in kwargs) or ('password' in kwargs)):\n        cert = {\n            \n        }\n        error_result = self._check_repository(repository.tool.get_scmtool_class(), repository.path, repository.username, repository.password, repository.local_site, trust_host, cert, request)\n        if (error_result is not None):\n            return error_result\n        if cert:\n            repository.extra_data['cert'] = cert\n    if kwargs.get('archive_name', False):\n        repository.archive(save=False)\n    try:\n        repository.full_clean()\n    except ValidationError as e:\n        return (INVALID_FORM_DATA, {\n            'fields': {\n                e.params['field']: e.message,\n            },\n        })\n    repository.save()\n    return (200, {\n        self.item_result_key: repository,\n    })\n", "label": 1}
{"function": "\n\ndef _construct_form(self, i, **kwargs):\n    \"\\n        Limit the queryset of the thumbs for performance reasons (so that it doesn't\\n        pull in every available thumbnail into the selectbox)\\n        \"\n    from cropduster.models import Thumb\n    form = super(CropDusterInlineFormSet, self)._construct_form(i, **kwargs)\n    field_identifier_field = form.fields['field_identifier']\n    field_identifier_field.widget = forms.HiddenInput()\n    field_identifier_field.initial = self.field_identifier\n    thumbs_field = form.fields['thumbs']\n    if (form.instance and form.instance.pk):\n        if (django.VERSION < (1, 7)):\n            thumbs_field.queryset = form.instance.thumbs.get_query_set()\n        else:\n            thumbs_field.queryset = form.instance.thumbs.get_queryset()\n    else:\n        thumbs_field.queryset = Thumb.objects.none()\n    if form.data:\n        try:\n            thumb_pks = [int(v) for v in form['thumbs'].value()]\n        except (TypeError, ValueError):\n            pass\n        else:\n            if (thumb_pks and (thumb_pks != [o.pk for o in thumbs_field.queryset])):\n                thumbs_field.queryset = Thumb.objects.filter(pk__in=thumb_pks)\n    return form\n", "label": 1}
{"function": "\n\ndef __call__(self, event):\n    '\\n        Handle event and print the specified variables.\\n        '\n    first = True\n    frame_symbols = set(event.locals)\n    if self.globals:\n        frame_symbols |= set(event.globals)\n    thread_name = (threading.current_thread().name if event.tracer.threading_support else '')\n    thread_align = (self.thread_alignment if event.tracer.threading_support else '')\n    for (code, symbols) in self.names.items():\n        try:\n            obj = eval(code, (event.globals if self.globals else {\n                \n            }), event.locals)\n        except AttributeError:\n            continue\n        except Exception as exc:\n            printout = '{internal-failure}FAILED EVAL: {internal-detail}{!r}'.format(exc, **self.event_colors)\n        else:\n            printout = self._safe_repr(obj)\n        if (frame_symbols >= symbols):\n            self.stream.write('{thread:{thread_align}}{:>{align}}       {vars}{:9} {vars-name}{} {vars}=> {reset}{}{reset}\\n'.format('', ('vars' if first else '...'), code, printout, thread=thread_name, thread_align=thread_align, align=self.filename_alignment, **self.event_colors))\n            first = False\n", "label": 1}
{"function": "\n\ndef get_config(name, region=None, key=None, keyid=None, profile=None):\n    '\\n    Get the configuration for an autoscale group.\\n\\n    CLI example::\\n\\n        salt myminion boto_asg.get_config myasg region=us-east-1\\n    '\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        asg = conn.get_all_groups(names=[name])\n        if asg:\n            asg = asg[0]\n        else:\n            return {\n                \n            }\n        ret = odict.OrderedDict()\n        attrs = ['name', 'availability_zones', 'default_cooldown', 'desired_capacity', 'health_check_period', 'health_check_type', 'launch_config_name', 'load_balancers', 'max_size', 'min_size', 'placement_group', 'vpc_zone_identifier', 'tags', 'termination_policies', 'suspended_processes']\n        for attr in attrs:\n            if (attr == 'tags'):\n                _tags = []\n                for tag in asg.tags:\n                    _tag = odict.OrderedDict()\n                    _tag['key'] = tag.key\n                    _tag['value'] = tag.value\n                    _tag['propagate_at_launch'] = tag.propagate_at_launch\n                    _tags.append(_tag)\n                ret['tags'] = _tags\n            elif (attr == 'vpc_zone_identifier'):\n                ret[attr] = getattr(asg, attr).split(',')\n            elif (attr == 'suspended_processes'):\n                suspended_processes = getattr(asg, attr)\n                ret[attr] = sorted([x.process_name for x in suspended_processes])\n            else:\n                ret[attr] = getattr(asg, attr)\n        policies = conn.get_all_policies(as_group=name)\n        ret['scaling_policies'] = []\n        for policy in policies:\n            ret['scaling_policies'].append(dict([('name', policy.name), ('adjustment_type', policy.adjustment_type), ('scaling_adjustment', policy.scaling_adjustment), ('min_adjustment_step', policy.min_adjustment_step), ('cooldown', policy.cooldown)]))\n        actions = conn.get_all_scheduled_actions(as_group=name)\n        ret['scheduled_actions'] = {\n            \n        }\n        for action in actions:\n            end_time = None\n            if action.end_time:\n                end_time = action.end_time.isoformat()\n            ret['scheduled_actions'][action.name] = dict([('min_size', action.min_size), ('max_size', action.max_size), ('desired_capacity', int(action.desired_capacity)), ('start_time', action.start_time.isoformat()), ('end_time', end_time), ('recurrence', action.recurrence)])\n        return ret\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        return {\n            \n        }\n", "label": 1}
{"function": "\n\ndef __init__(self, model, *args, **kwargs):\n    fieldnames = kwargs.pop('fieldnames', [])\n    super(Export, self).__init__(*args, **kwargs)\n    self.fieldsets = (('Options', {\n        'fields': ('export_format', 'export_fields', 'export_order_by', 'export_order_direction'),\n    }), ('Filters', {\n        'description': 'Objects will be filtered to match the criteria                 as specified in the fields below. If a value is not specified                 for a field the field is ignored during the filter process.',\n        'fields': [],\n    }))\n    field_choices = []\n    for field in model._meta.fields:\n        field.editable = True\n    all_form_fields = forms.models.fields_for_model(model)\n    if fieldnames:\n        filter_form_fields = forms.models.fields_for_model(model, fields=fieldnames)\n    else:\n        filter_form_fields = all_form_fields\n    for field in model._meta.fields:\n        name = field.name\n        if (name not in all_form_fields):\n            continue\n        form_field = all_form_fields[name]\n        field_choices.append([name, form_field.label.capitalize()])\n        if (name not in filter_form_fields.keys()):\n            continue\n        if (form_field.__class__ in [forms.models.ModelChoiceField, forms.models.ModelMultipleChoiceField]):\n            self.fields[name] = getattr(fields, field.__class__.__name__)(form_field, form_field.queryset)\n        else:\n            try:\n                self.fields[name] = getattr(fields, field.__class__.__name__)(form_field)\n            except AttributeError:\n                for parent_field in inspect.getmro(field.__class__):\n                    if (parent_field.__module__ == 'django.db.models.fields'):\n                        self.fields[name] = getattr(fields, parent_field.__name__)(form_field)\n                        break\n        if (name not in self.fieldsets[1][1]['fields']):\n            self.fieldsets[1][1]['fields'].append(name)\n    self.fields['export_fields'].choices = field_choices\n    self.fields['export_order_by'].choices = field_choices\n", "label": 1}
{"function": "\n\ndef parse_args_and_create_context(self, args):\n    '\\n        Helper method that will parse the args into options and\\n        remaining args as well as create an initial\\n        :py:class:`swiftly.cli.context.CLIContext`.\\n\\n        The new context will be a copy of\\n        :py:attr:`swiftly.cli.cli.CLI.context` with the following\\n        attributes added:\\n\\n        =======================  ===================================\\n        muted_account_headers    The headers to omit when outputting\\n                                 account headers.\\n        muted_container_headers  The headers to omit when outputting\\n                                 container headers.\\n        muted_object_headers     The headers to omit when outputting\\n                                 object headers.\\n        =======================  ===================================\\n\\n        :returns: options, args, context\\n        '\n    original_args = args\n    try:\n        (options, args) = self.option_parser.parse_args(args)\n    except UnboundLocalError:\n        pass\n    if self.option_parser.error_encountered:\n        if (('-?' in original_args) or ('-h' in original_args) or ('--help' in original_args)):\n            self.option_parser.print_help()\n        raise ReturnCode()\n    if options.help:\n        self.option_parser.print_help()\n        raise ReturnCode()\n    if ((self.min_args is not None) and (len(args) < self.min_args)):\n        raise ReturnCode(('requires at least %s args.' % self.min_args))\n    if ((self.max_args is not None) and (len(args) > self.max_args)):\n        raise ReturnCode(('requires no more than %s args.' % self.max_args))\n    context = self.cli.context.copy()\n    context.muted_account_headers = ['accept-ranges', 'content-length', 'content-type', 'date']\n    context.muted_container_headers = ['accept-ranges', 'content-length', 'content-type', 'date']\n    context.muted_object_headers = ['accept-ranges', 'date']\n    return (options, args, context)\n", "label": 1}
{"function": "\n\ndef run(self):\n    if (self.arguments and self.content):\n        raise RuntimeError(\"bokeh-plot:: directive can't have both args and content\")\n    env = self.state.document.settings.env\n    app = env.app\n    if (not hasattr(env, 'bokeh_plot_tmpdir')):\n        env.bokeh_plot_tmpdir = mkdtemp()\n        app.verbose(('creating new temp dir for bokeh-plot cache: %s' % env.bokeh_plot_tmpdir))\n    else:\n        tmpdir = env.bokeh_plot_tmpdir\n        if ((not exists(tmpdir)) or (not isdir(tmpdir))):\n            app.verbose(('creating new temp dir for bokeh-plot cache: %s' % env.bokeh_plot_tmpdir))\n            env.bokeh_plot_tmpdir = mkdtemp()\n        else:\n            app.verbose(('using existing temp dir for bokeh-plot cache: %s' % env.bokeh_plot_tmpdir))\n    rst_source = self.state_machine.document['source']\n    rst_dir = dirname(rst_source)\n    rst_filename = basename(rst_source)\n    target_id = ('%s.bokeh-plot-%d' % (rst_filename, env.new_serialno('bokeh-plot')))\n    target_node = nodes.target('', '', ids=[target_id])\n    result = [target_node]\n    try:\n        source = self._get_source()\n    except Exception:\n        (app.warn(('Unable to generate Bokeh plot at %s:%d:' % (basename(rst_source), self.lineno))),)\n        node = nodes.error(None, nodes.paragraph(text=('Unable to generate Bokeh plot at %s:%d:' % (basename(rst_source), self.lineno))), nodes.paragraph(text=str(sys.exc_info()[1])))\n        return [node]\n    source_position = self.options.get('source-position', 'below')\n    if (source_position == 'above'):\n        result += self._get_source_nodes(source)\n    node = bokeh_plot()\n    node['target_id'] = target_id\n    node['source'] = source\n    node['relpath'] = relpath(rst_dir, env.srcdir)\n    node['rst_source'] = rst_source\n    node['rst_lineno'] = self.lineno\n    if ('alt' in self.options):\n        node['alt'] = self.options['alt']\n    if self.arguments:\n        node['path'] = self.arguments[0]\n        env.note_dependency(node['path'])\n    if (len(self.arguments) == 2):\n        node['symbol'] = self.arguments[1]\n    result += [node]\n    if (source_position == 'below'):\n        result += self._get_source_nodes(source)\n    return result\n", "label": 1}
{"function": "\n\ndef _qfiledialog_wrapper(attr, parent=None, caption='', basedir='', filters='', selectedfilter='', options=None):\n    if (options is None):\n        options = QFileDialog.Options(0)\n    try:\n        from winpython.qt.QtCore import QString\n    except ImportError:\n        QString = None\n    tuple_returned = True\n    try:\n        func = getattr(QFileDialog, (attr + 'AndFilter'))\n    except AttributeError:\n        func = getattr(QFileDialog, attr)\n        if (QString is not None):\n            selectedfilter = QString()\n            tuple_returned = False\n    if (sys.platform == 'win32'):\n        (_temp1, _temp2) = (sys.stdout, sys.stderr)\n        (sys.stdout, sys.stderr) = (None, None)\n    try:\n        result = func(parent, caption, basedir, filters, selectedfilter, options)\n    except TypeError:\n        result = func(parent, caption, basedir, filters, options)\n    finally:\n        if (sys.platform == 'win32'):\n            (sys.stdout, sys.stderr) = (_temp1, _temp2)\n    if tuple_returned:\n        (output, selectedfilter) = result\n    else:\n        output = result\n    if (QString is not None):\n        selectedfilter = to_text_string(selectedfilter)\n        if isinstance(output, QString):\n            output = to_text_string(output)\n        else:\n            output = [to_text_string(fname) for fname in output]\n    return (output, selectedfilter)\n", "label": 1}
{"function": "\n\ndef process_ddp(self, data):\n    'Process a single DDP message.'\n    msg_id = data.get('id', None)\n    try:\n        msg = data.pop('msg')\n    except KeyError:\n        self.reply('error', reason='Bad request', offendingMessage=data)\n        return\n    try:\n        self.dispatch(msg, data)\n    except Exception as err:\n        kwargs = {\n            'msg': {\n                'method': 'result',\n            }.get(msg, 'error'),\n        }\n        if (msg_id is not None):\n            kwargs['id'] = msg_id\n        if isinstance(err, MeteorError):\n            error = err.as_dict()\n        else:\n            error = {\n                'error': 500,\n                'reason': 'Internal server error',\n            }\n        if (kwargs['msg'] == 'error'):\n            kwargs.update(error)\n        else:\n            kwargs['error'] = error\n        if (not isinstance(err, MeteorError)):\n            (stack, _) = safe_call(self.logger.error, '%r %r', msg, data, exc_info=1)\n            if (stack is not None):\n                traceback.print_exc(file=sys.stderr)\n                sys.stderr.write('Additionally, while handling the above error the following error was encountered:\\n')\n                sys.stderr.write(stack)\n        elif settings.DEBUG:\n            print(('ERROR: %s' % err))\n            dprint('msg', msg)\n            dprint('data', data)\n            error.setdefault('details', traceback.format_exc())\n            print(error['details'])\n        self.reply(**kwargs)\n        if (msg_id and (msg == 'method')):\n            self.reply('updated', methods=[msg_id])\n", "label": 1}
{"function": "\n\ndef test_docopt_parser_shorts():\n    help_string = '    Some Tool\\n\\n    Usage: tools [-t] [--input <input>...] <cmd>\\n    '\n    opts = Options.from_docopt(help_string)\n    assert (len(opts) == 3)\n    assert (opts['input'] is not None)\n    assert (opts['input'].nargs == '*')\n    assert (not opts['input'].required)\n    assert (opts['t'] is not None)\n    assert (opts['t'].nargs == 0)\n    assert (not opts['t'].required)\n    assert (opts['cmd'] is not None)\n    assert (opts['cmd'].nargs == 1)\n    assert opts['cmd'].required\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Continuously read data from the source and attempt to parse a valid\\n        message from the buffer of bytes. When a message is parsed, passes it\\n        off to the callback if one is set.\\n        '\n    while self.running:\n        try:\n            payload = self.read()\n        except DataSourceError as e:\n            if self.running:\n                LOG.warn(\"Can't read from data source -- stopping: %s\", e)\n            break\n        try:\n            self.streamer\n        except MissingPayloadFormatError:\n            json_chars = ['\\x00']\n            json_chars.extend(string.printable)\n            if all(((char in json_chars) for char in payload)):\n                self.format = 'json'\n            else:\n                self.format = 'protobuf'\n        self.streamer.receive(payload)\n        while True:\n            message = self.streamer.parse_next_message()\n            if (message is None):\n                break\n            if (not self._message_valid(message)):\n                self.corrupted_messages += 1\n                break\n            if (self.callback is not None):\n                self.callback(message)\n            self._receive_command_response(message)\n", "label": 1}
{"function": "\n\ndef _make_request(self, conn, method, url, timeout=_Default, **httplib_request_kw):\n    '\\n        Perform a request on a given httplib connection object taken from our\\n        pool.\\n\\n        :param conn:\\n            a connection from one of our connection pools\\n\\n        :param timeout:\\n            Socket timeout in seconds for the request. This can be a\\n            float or integer, which will set the same timeout value for\\n            the socket connect and the socket read, or an instance of\\n            :class:`urllib3.util.Timeout`, which gives you more fine-grained\\n            control over your timeouts.\\n        '\n    self.num_requests += 1\n    timeout_obj = self._get_timeout(timeout)\n    try:\n        timeout_obj.start_connect()\n        conn.timeout = timeout_obj.connect_timeout\n        conn.request(method, url, **httplib_request_kw)\n    except SocketTimeout:\n        raise ConnectTimeoutError(self, ('Connection to %s timed out. (connect timeout=%s)' % (self.host, timeout_obj.connect_timeout)))\n    read_timeout = timeout_obj.read_timeout\n    if hasattr(conn, 'sock'):\n        if (read_timeout == 0):\n            raise ReadTimeoutError(self, url, ('Read timed out. (read timeout=%s)' % read_timeout))\n        if (read_timeout is Timeout.DEFAULT_TIMEOUT):\n            conn.sock.settimeout(socket.getdefaulttimeout())\n        else:\n            conn.sock.settimeout(read_timeout)\n    try:\n        try:\n            httplib_response = conn.getresponse(buffering=True)\n        except TypeError:\n            httplib_response = conn.getresponse()\n    except SocketTimeout:\n        raise ReadTimeoutError(self, url, ('Read timed out. (read timeout=%s)' % read_timeout))\n    except BaseSSLError as e:\n        if (('timed out' in str(e)) or ('did not complete (read)' in str(e))):\n            raise ReadTimeoutError(self, url, 'Read timed out.')\n        raise\n    except SocketError as e:\n        if (e.errno in _blocking_errnos):\n            raise ReadTimeoutError(self, url, ('Read timed out. (read timeout=%s)' % read_timeout))\n        raise\n    http_version = getattr(conn, '_http_vsn_str', 'HTTP/?')\n    log.debug(('\"%s %s %s\" %s %s' % (method, url, http_version, httplib_response.status, httplib_response.length)))\n    return httplib_response\n", "label": 1}
{"function": "\n\ndef integerbox(msg='', title=' ', default='', lowerbound=0, upperbound=99, image=None, root=None):\n    '\\n    Show a box in which a user can enter an integer.\\n\\n    In addition to arguments for msg and title, this function accepts\\n    integer arguments for \"default\", \"lowerbound\", and \"upperbound\".\\n\\n    The default argument may be None.\\n\\n    When the user enters some text, the text is checked to verify that it\\n    can be converted to an integer between the lowerbound and upperbound.\\n\\n    If it can be, the integer (not the text) is returned.\\n\\n    If it cannot, then an error msg is displayed, and the integerbox is\\n    redisplayed.\\n\\n    If the user cancels the operation, None is returned.\\n\\n    :param str msg: the msg to be displayed\\n    :param str title: the window title\\n    :param str default: The default value to return\\n    :param int lowerbound: The lower-most value allowed\\n    :param int upperbound: The upper-most value allowed\\n    :param str image: Filename of image to display\\n    :param tk_widget root: Top-level Tk widget\\n    :return: the integer value entered by the user\\n\\n    '\n    if (not msg):\n        msg = 'Enter an integer between {0} and {1}'.format(lowerbound, upperbound)\n    exception_string = 'integerbox \"{0}\" must be an integer.  It is >{1}< of type {2}'\n    if default:\n        try:\n            default = int(default)\n        except ValueError:\n            raise ValueError(exception_string.format('default', default, type(default)))\n    try:\n        lowerbound = int(lowerbound)\n    except ValueError:\n        raise ValueError(exception_string.format('lowerbound', lowerbound, type(lowerbound)))\n    try:\n        upperbound = int(upperbound)\n    except ValueError:\n        raise ValueError(exception_string.format('upperbound', upperbound, type(upperbound)))\n    while True:\n        reply = enterbox(msg, title, str(default), image=image, root=root)\n        if (reply is None):\n            return None\n        try:\n            reply = int(reply)\n        except:\n            msgbox('The value that you entered:\\n\\t\"{}\"\\nis not an integer.'.format(reply), 'Error')\n            continue\n        if (reply < lowerbound):\n            msgbox('The value that you entered is less than the lower bound of {}.'.format(lowerbound), 'Error')\n            continue\n        if (reply > upperbound):\n            msgbox('The value that you entered is greater than the upper bound of {}.'.format(upperbound), 'Error')\n            continue\n        return reply\n", "label": 1}
{"function": "\n\ndef _update_usage_from_migrations(self, context, migrations):\n    filtered = {\n        \n    }\n    instances = {\n        \n    }\n    self.tracked_migrations.clear()\n    for migration in migrations:\n        uuid = migration.instance_uuid\n        try:\n            if (uuid not in instances):\n                instances[uuid] = migration.instance\n        except exception.InstanceNotFound as e:\n            LOG.debug('Migration instance not found: %s', e)\n            continue\n        if (not _instance_in_resize_state(instances[uuid])):\n            LOG.warning(_LW('Instance not resizing, skipping migration.'), instance_uuid=uuid)\n            continue\n        other_migration = filtered.get(uuid, None)\n        if ((not other_migration) or (migration.updated_at and other_migration.updated_at and (migration.updated_at >= other_migration.updated_at))):\n            filtered[uuid] = migration\n    for migration in filtered.values():\n        instance = instances[migration.instance_uuid]\n        try:\n            self._update_usage_from_migration(context, instance, None, migration)\n        except exception.FlavorNotFound:\n            LOG.warning(_LW('Flavor could not be found, skipping migration.'), instance_uuid=uuid)\n            continue\n", "label": 1}
{"function": "\n\ndef is_safe_to_overwrite(source, target):\n    \"Check whether it is safe to overwrite target exe with source exe.\\n\\n    This function checks whether two exe files 'source' and 'target' differ\\n    only in the contents of certain non-critical resource segments.  If so,\\n    then overwriting the target file with the contents of the source file\\n    should be safe even in the face of system crashes or power outages; the\\n    worst outcome would be a corrupted resource such as an icon.\\n    \"\n    if ((not source.endswith('.exe')) or (not target.endswith('.exe'))):\n        return False\n    s_sz = os.stat(source).st_size\n    t_sz = os.stat(target).st_size\n    if (s_sz != t_sz):\n        return False\n    locs = []\n    for (rtype, rid, rlang) in COMMON_SAFE_RESOURCES:\n        try:\n            s_loc = find_resource(source, rtype, rid, rlang)\n        except WindowsError:\n            s_loc = None\n        try:\n            t_loc = find_resource(target, rtype, rid, rlang)\n        except WindowsError:\n            t_loc = None\n        if (s_loc != t_loc):\n            return False\n        if (s_loc is not None):\n            locs.append(s_loc)\n    if locs:\n        locs.extend(((0, 0), (s_sz, s_sz)))\n        locs.sort()\n        for ((_, start), (stop, _)) in pairwise(locs):\n            if files_differ(source, target, start, stop):\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef main(args=None):\n    \"Entry point for `fuel-convert` script.\\n\\n    This function can also be imported and used from Python.\\n\\n    Parameters\\n    ----------\\n    args : iterable, optional (default: None)\\n        A list of arguments that will be passed to Fuel's conversion\\n        utility. If this argument is not specified, `sys.argv[1:]` will\\n        be used.\\n\\n    \"\n    built_in_datasets = dict(converters.all_converters)\n    if fuel.config.extra_converters:\n        for name in fuel.config.extra_converters:\n            extra_datasets = dict(importlib.import_module(name).all_converters)\n            if any(((key in built_in_datasets) for key in extra_datasets.keys())):\n                raise ValueError('extra converters conflict in name with built-in converters')\n            built_in_datasets.update(extra_datasets)\n    parser = argparse.ArgumentParser(description='Conversion script for built-in datasets.')\n    subparsers = parser.add_subparsers()\n    parent_parser = argparse.ArgumentParser(add_help=False)\n    parent_parser.add_argument('-d', '--directory', help='directory in which input files reside', type=str, default=os.getcwd())\n    convert_functions = {\n        \n    }\n    for (name, fill_subparser) in built_in_datasets.items():\n        subparser = subparsers.add_parser(name, parents=[parent_parser], help='Convert the {} dataset'.format(name))\n        subparser.add_argument('-o', '--output-directory', help='where to save the dataset', type=str, default=os.getcwd(), action=CheckDirectoryAction)\n        subparser.add_argument('-r', '--output_filename', help='new name of the created dataset', type=str, default=None)\n        subparser.set_defaults(which_=name)\n        convert_functions[name] = fill_subparser(subparser)\n    args = parser.parse_args(args)\n    args_dict = vars(args)\n    if ((args_dict['output_filename'] is not None) and (os.path.splitext(args_dict['output_filename'])[1] not in ('.hdf5', '.hdf', '.h5'))):\n        args_dict['output_filename'] += '.hdf5'\n    if (args_dict['output_filename'] is None):\n        args_dict.pop('output_filename')\n    convert_function = convert_functions[args_dict.pop('which_')]\n    try:\n        output_paths = convert_function(**args_dict)\n    except MissingInputFiles as e:\n        intro = 'The following required files were not found:\\n'\n        message = '\\n'.join(([intro] + [('   * ' + f) for f in e.filenames]))\n        message += '\\n\\nDid you forget to run fuel-download?'\n        parser.error(message)\n    for output_path in output_paths:\n        h5file = h5py.File(output_path, 'a')\n        interface_version = H5PYDataset.interface_version.encode('utf-8')\n        h5file.attrs['h5py_interface_version'] = interface_version\n        fuel_convert_version = converters.__version__.encode('utf-8')\n        h5file.attrs['fuel_convert_version'] = fuel_convert_version\n        command = ([os.path.basename(sys.argv[0])] + sys.argv[1:])\n        h5file.attrs['fuel_convert_command'] = ' '.join(command).encode('utf-8')\n        h5file.flush()\n        h5file.close()\n", "label": 1}
{"function": "\n\n@register.tag(name='image')\ndef image(parser, token):\n    bits = token.split_contents()[1:]\n    image_expr = parser.compile_filter(bits[0])\n    bits = bits[1:]\n    filter_specs = []\n    attrs = {\n        \n    }\n    output_var_name = None\n    as_context = False\n    is_valid = True\n    for bit in bits:\n        if (bit == 'as'):\n            as_context = True\n        elif as_context:\n            if (output_var_name is None):\n                output_var_name = bit\n            else:\n                is_valid = False\n        else:\n            try:\n                (name, value) = bit.split('=')\n                attrs[name] = parser.compile_filter(value)\n            except ValueError:\n                if allowed_filter_pattern.match(bit):\n                    filter_specs.append(bit)\n                else:\n                    raise template.TemplateSyntaxError(\"filter specs in 'image' tag may only contain A-Z, a-z, 0-9, dots, hyphens and underscores. (given filter: {})\".format(bit))\n    if (as_context and (output_var_name is None)):\n        is_valid = False\n    if (output_var_name and attrs):\n        is_valid = False\n    if is_valid:\n        return ImageNode(image_expr, '|'.join(filter_specs), attrs=attrs, output_var_name=output_var_name)\n    else:\n        raise template.TemplateSyntaxError('\\'image\\' tag should be of the form {% image self.photo max-320x200 [ custom-attr=\"value\" ... ] %} or {% image self.photo max-320x200 as img %}')\n", "label": 1}
{"function": "\n\ndef digest_auth(realm, get_ha1, key, debug=False):\n    'A CherryPy tool which hooks at before_handler to perform\\n    HTTP Digest Access Authentication, as specified in :rfc:`2617`.\\n    \\n    If the request has an \\'authorization\\' header with a \\'Digest\\' scheme, this\\n    tool authenticates the credentials supplied in that header.  If\\n    the request has no \\'authorization\\' header, or if it does but the scheme is\\n    not \"Digest\", or if authentication fails, the tool sends a 401 response with\\n    a \\'WWW-Authenticate\\' Digest header.\\n    \\n    realm\\n        A string containing the authentication realm.\\n    \\n    get_ha1\\n        A callable which looks up a username in a credentials store\\n        and returns the HA1 string, which is defined in the RFC to be\\n        MD5(username : realm : password).  The function\\'s signature is:\\n        ``get_ha1(realm, username)``\\n        where username is obtained from the request\\'s \\'authorization\\' header.\\n        If username is not found in the credentials store, get_ha1() returns\\n        None.\\n    \\n    key\\n        A secret string known only to the server, used in the synthesis of nonces.\\n    \\n    '\n    request = cherrypy.serving.request\n    auth_header = request.headers.get('authorization')\n    nonce_is_stale = False\n    if (auth_header is not None):\n        try:\n            auth = HttpDigestAuthorization(auth_header, request.method, debug=debug)\n        except ValueError:\n            raise cherrypy.HTTPError(400, 'The Authorization header could not be parsed.')\n        if debug:\n            TRACE(str(auth))\n        if auth.validate_nonce(realm, key):\n            ha1 = get_ha1(realm, auth.username)\n            if (ha1 is not None):\n                digest = auth.request_digest(ha1, entity_body=request.body)\n                if (digest == auth.response):\n                    if debug:\n                        TRACE('digest matches auth.response')\n                    nonce_is_stale = auth.is_nonce_stale(max_age_seconds=600)\n                    if (not nonce_is_stale):\n                        request.login = auth.username\n                        if debug:\n                            TRACE(('authentication of %s successful' % auth.username))\n                        return\n    header = www_authenticate(realm, key, stale=nonce_is_stale)\n    if debug:\n        TRACE(header)\n    cherrypy.serving.response.headers['WWW-Authenticate'] = header\n    raise cherrypy.HTTPError(401, 'You are not authorized to access that resource')\n", "label": 1}
{"function": "\n\ndef main(args):\n    for choice in ['netcat', 'nc', 'telnet']:\n        if (os.system(('which %s &> /dev/null' % choice)) == 0):\n            prog = choice\n            break\n    else:\n        raise CommandError('Could not find an installed telnet.')\n    target = args.target\n    if (':' in target):\n        (host, port) = target.split(':', 1)\n    else:\n        (host, port) = ('localhost', target)\n    rlwrap = args.rlwrap\n    cmd = [prog, str(host), str(port)]\n    if (prog == 'netcat'):\n        cmd.append('--close')\n    if (rlwrap is None):\n        rlwrap = (os.system('which rlwrap &> /dev/null') == 0)\n    if rlwrap:\n        cmd.insert(0, 'rlwrap')\n    try:\n        if (call(cmd) != 0):\n            raise CommandError('Backdoor unreachable on {}'.format(target))\n    except (EOFError, KeyboardInterrupt):\n        print()\n        if ((choice == 'telnet') and rlwrap):\n            call(['reset'])\n", "label": 1}
{"function": "\n\ndef __init__(self, var):\n    self.var = var\n    self.literal = None\n    self.lookups = None\n    self.translate = False\n    self.message_context = None\n    if (not isinstance(var, six.string_types)):\n        raise TypeError(('Variable must be a string or number, got %s' % type(var)))\n    try:\n        self.literal = float(var)\n        if (('.' not in var) and ('e' not in var.lower())):\n            self.literal = int(self.literal)\n        if var.endswith('.'):\n            raise ValueError\n    except ValueError:\n        if (var.startswith('_(') and var.endswith(')')):\n            self.translate = True\n            var = var[2:(- 1)]\n        try:\n            self.literal = mark_safe(unescape_string_literal(var))\n        except ValueError:\n            if ((var.find((VARIABLE_ATTRIBUTE_SEPARATOR + '_')) > (- 1)) or (var[0] == '_')):\n                raise TemplateSyntaxError((\"Variables and attributes may not begin with underscores: '%s'\" % var))\n            self.lookups = tuple(var.split(VARIABLE_ATTRIBUTE_SEPARATOR))\n", "label": 1}
{"function": "\n\ndef computeStatement(self, constraint_collection):\n    outer_constraint_collection = constraint_collection\n    constraint_collection = ConstraintCollectionBranch(parent=constraint_collection, name='loop')\n    (loop_body, break_collections) = self.computeLoopBody(constraint_collection)\n    if (loop_body is not None):\n        assert loop_body.isStatementsSequence()\n        statements = loop_body.getStatements()\n        assert statements\n        last_statement = statements[(- 1)]\n        if last_statement.isStatementLoopContinue():\n            if (len(statements) == 1):\n                self.setLoopBody(None)\n                loop_body = None\n            else:\n                last_statement.replaceWith(None)\n            constraint_collection.signalChange('new_statements', last_statement.getSourceReference(), \"Removed useless terminal 'continue' as last statement of loop.\")\n    if break_collections:\n        outer_constraint_collection.mergeMultipleBranches(break_collections)\n    if (loop_body is not None):\n        assert loop_body.isStatementsSequence()\n        statements = loop_body.getStatements()\n        assert statements\n        if ((len(statements) == 1) and statements[(- 1)].isStatementLoopBreak()):\n            return (None, 'new_statements', \"Removed useless loop with immediate 'break' statement.\")\n    outer_constraint_collection.onExceptionRaiseExit(BaseException)\n    return (self, None, None)\n", "label": 1}
{"function": "\n\ndef __init__(self, switchboard, master=None):\n    self.__sb = switchboard\n    optiondb = switchboard.optiondb()\n    root = self.__root = Toplevel(master, class_='Pynche')\n    root.protocol('WM_DELETE_WINDOW', self.withdraw)\n    root.title('Pynche Text Window')\n    root.iconname('Pynche Text Window')\n    root.bind('<Alt-q>', self.__quit)\n    root.bind('<Alt-Q>', self.__quit)\n    root.bind('<Alt-w>', self.withdraw)\n    root.bind('<Alt-W>', self.withdraw)\n    self.__text = Text(root, relief=SUNKEN, background=optiondb.get('TEXTBG', 'black'), foreground=optiondb.get('TEXTFG', 'white'), width=35, height=15)\n    sfg = optiondb.get('TEXT_SFG')\n    if sfg:\n        self.__text.configure(selectforeground=sfg)\n    sbg = optiondb.get('TEXT_SBG')\n    if sbg:\n        self.__text.configure(selectbackground=sbg)\n    ibg = optiondb.get('TEXT_IBG')\n    if ibg:\n        self.__text.configure(insertbackground=ibg)\n    self.__text.pack()\n    self.__text.insert(0.0, optiondb.get('TEXT', 'Insert some stuff here and play\\nwith the buttons below to see\\nhow the colors interact in\\ntextual displays.\\n\\nSee how the selection can also\\nbe affected by tickling the buttons\\nand choosing a color.'))\n    insert = optiondb.get('TEXTINS')\n    if insert:\n        self.__text.mark_set(INSERT, insert)\n    try:\n        (start, end) = optiondb.get('TEXTSEL', (6.0, END))\n        self.__text.tag_add(SEL, start, end)\n    except ValueError:\n        pass\n    self.__text.focus_set()\n    self.__trackp = BooleanVar()\n    self.__trackp.set(optiondb.get('TRACKP', 0))\n    self.__which = IntVar()\n    self.__which.set(optiondb.get('WHICH', 0))\n    self.__t = Checkbutton(root, text='Track color changes', variable=self.__trackp, relief=GROOVE, command=self.__toggletrack)\n    self.__t.pack(fill=X, expand=YES)\n    frame = self.__frame = Frame(root)\n    frame.pack()\n    self.__labels = []\n    row = 2\n    for text in ('Text:', 'Selection:', 'Insertion:'):\n        l = Label(frame, text=text)\n        l.grid(row=row, column=0, sticky=E)\n        self.__labels.append(l)\n        row += 1\n    col = 1\n    for text in ('Foreground', 'Background'):\n        l = Label(frame, text=text)\n        l.grid(row=1, column=col)\n        self.__labels.append(l)\n        col += 1\n    self.__radios = []\n    for col in (1, 2):\n        for row in (2, 3, 4):\n            if ((row == 4) and (col == 1)):\n                continue\n            r = Radiobutton(frame, variable=self.__which, value=((((row - 2) * 2) + col) - 1), command=self.__set_color)\n            r.grid(row=row, column=col)\n            self.__radios.append(r)\n    self.__toggletrack()\n", "label": 1}
{"function": "\n\n@locations_access_required\ndef child_locations_for_select2(request, domain):\n    id = request.GET.get('id')\n    ids = request.GET.get('ids')\n    query = request.GET.get('name', '').lower()\n    user = request.couch_user\n\n    def loc_to_payload(loc):\n        return {\n            'id': loc.location_id,\n            'name': loc.display_name,\n        }\n    if id:\n        try:\n            loc = SQLLocation.objects.get(location_id=id)\n            if (loc.domain != domain):\n                raise SQLLocation.DoesNotExist()\n        except SQLLocation.DoesNotExist:\n            return json_response({\n                'message': ('no location with id %s found' % id),\n            }, status_code=404)\n        else:\n            return json_response(loc_to_payload(loc))\n    elif ids:\n        from corehq.apps.locations.util import get_locations_from_ids\n        ids = json.loads(ids)\n        try:\n            locations = get_locations_from_ids(ids, domain)\n        except SQLLocation.DoesNotExist:\n            return json_response({\n                'message': 'one or more locations not found',\n            }, status_code=404)\n        return json_response([loc_to_payload(loc) for loc in locations])\n    else:\n        locs = []\n        user_loc = user.get_sql_location(domain)\n        if user_can_edit_any_location(user, request.project):\n            locs = SQLLocation.objects.filter(domain=domain, is_archived=False)\n        elif user_loc:\n            locs = user_loc.get_descendants(include_self=True)\n        if ((locs != []) and query):\n            locs = locs.filter(name__icontains=query)\n        return json_response(map(loc_to_payload, locs[:10]))\n", "label": 1}
{"function": "\n\ndef _design_poll(self, name, mode, oldres, timeout=5, use_devmode=False):\n    \"\\n        Poll for an 'async' action to be complete.\\n        :param string name: The name of the design document\\n        :param string mode: One of ``add`` or ``del`` to indicate whether\\n            we should check for addition or deletion of the document\\n        :param oldres: The old result from the document's previous state, if\\n            any\\n        :param float timeout: How long to poll for. If this is 0 then this\\n            function returns immediately\\n        :type oldres: :class:`~couchbase.result.HttpResult`\\n        \"\n    if (not timeout):\n        return True\n    if (timeout < 0):\n        raise ArgumentError.pyexc('Interval must not be negative')\n    t_end = (time.time() + timeout)\n    old_rev = None\n    if oldres:\n        old_rev = self._doc_rev(oldres)\n    while (time.time() < t_end):\n        try:\n            cur_resp = self.design_get(name, use_devmode=use_devmode)\n            if (old_rev and (self._doc_rev(cur_resp) == old_rev)):\n                continue\n            try:\n                if (not self._poll_vq_single(name, use_devmode, cur_resp.value)):\n                    continue\n                return True\n            except CouchbaseError:\n                continue\n        except CouchbaseError:\n            if (mode == 'del'):\n                return True\n    raise exceptions.TimeoutError.pyexc('Wait time for design action completion exceeded')\n", "label": 1}
{"function": "\n\ndef _process_request(self, request):\n    if self.cache_anonymous_only:\n        if (not hasattr(request, 'user')):\n            raise ImproperlyConfigured(\"The Django cache middleware with CACHE_MIDDLEWARE_ANONYMOUS_ONLY=True requires authentication middleware to be installed. Edit your MIDDLEWARE_CLASSES setting to insert 'django.contrib.auth.middleware.AuthenticationMiddleware'before the CacheMiddleware.\")\n    if (request.method not in ('GET', 'HEAD')):\n        request._cache_update_cache = False\n        return None\n    if (self.cache_anonymous_only and request.user.is_authenticated()):\n        request._cache_update_cache = False\n        return None\n    if callable(self.key_prefix):\n        key_prefix = self.key_prefix(request)\n        if (key_prefix is None):\n            request._cache_update_cache = False\n            return None\n    else:\n        key_prefix = self.key_prefix\n    with RequestPath(request, self.only_get_keys, self.forget_get_keys):\n        cache_key = get_cache_key(request, key_prefix)\n    if (cache_key is None):\n        request._cache_update_cache = True\n        return None\n    response = cache.get(cache_key, None)\n    if (response is None):\n        request._cache_update_cache = True\n        return None\n    request._cache_update_cache = False\n    if self.post_process_response_always:\n        response = self.post_process_response_always(response, request=request)\n    return response\n", "label": 1}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(UpdateProjectGroupsAction, self).__init__(request, *args, **kwargs)\n    err_msg = _('Unable to retrieve group list. Please try again later.')\n    domain_id = self.initial.get('domain_id', None)\n    project_id = ''\n    if ('project_id' in self.initial):\n        project_id = self.initial['project_id']\n    try:\n        default_role = api.keystone.get_default_role(self.request)\n        if (default_role is None):\n            default = getattr(settings, 'OPENSTACK_KEYSTONE_DEFAULT_ROLE', None)\n            msg = (_('Could not find default role \"%s\" in Keystone') % default)\n            raise exceptions.NotFound(msg)\n    except Exception:\n        exceptions.handle(self.request, err_msg, redirect=reverse(INDEX_URL))\n    default_role_name = self.get_default_role_field_name()\n    self.fields[default_role_name] = forms.CharField(required=False)\n    self.fields[default_role_name].initial = default_role.id\n    all_groups = []\n    try:\n        all_groups = api.keystone.group_list(request, domain=domain_id)\n    except Exception:\n        exceptions.handle(request, err_msg)\n    groups_list = [(group.id, group.name) for group in all_groups]\n    role_list = []\n    try:\n        role_list = api.keystone.role_list(request)\n    except Exception:\n        exceptions.handle(request, err_msg, redirect=reverse(INDEX_URL))\n    for role in role_list:\n        field_name = self.get_member_field_name(role.id)\n        label = role.name\n        self.fields[field_name] = forms.MultipleChoiceField(required=False, label=label)\n        self.fields[field_name].choices = groups_list\n        self.fields[field_name].initial = []\n    if project_id:\n        try:\n            groups_roles = api.keystone.get_project_groups_roles(request, project_id)\n        except Exception:\n            exceptions.handle(request, err_msg, redirect=reverse(INDEX_URL))\n        for group_id in groups_roles:\n            roles_ids = groups_roles[group_id]\n            for role_id in roles_ids:\n                field_name = self.get_member_field_name(role_id)\n                self.fields[field_name].initial.append(group_id)\n", "label": 1}
{"function": "\n\ndef savetxt(fname, X, names=None, fmt='%.18e', delimiter=' '):\n    \"\\n    Save an array to a text file.\\n\\n    This is just a copy of numpy.savetxt patched to support structured arrays\\n    or a header of names.  Does not include py3 support now in savetxt.\\n\\n    Parameters\\n    ----------\\n    fname : filename or file handle\\n        If the filename ends in ``.gz``, the file is automatically saved in\\n        compressed gzip format.  `loadtxt` understands gzipped files\\n        transparently.\\n    X : array_like\\n        Data to be saved to a text file.\\n    names : list, optional\\n        If given names will be the column header in the text file.  If None and\\n        X is a structured or recarray then the names are taken from\\n        X.dtype.names.\\n    fmt : str or sequence of strs\\n        A single format (%10.5f), a sequence of formats, or a\\n        multi-format string, e.g. 'Iteration %d -- %10.5f', in which\\n        case `delimiter` is ignored.\\n    delimiter : str\\n        Character separating columns.\\n\\n    See Also\\n    --------\\n    save : Save an array to a binary file in NumPy ``.npy`` format\\n    savez : Save several arrays into a ``.npz`` compressed archive\\n\\n    Notes\\n    -----\\n    Further explanation of the `fmt` parameter\\n    (``%[flag]width[.precision]specifier``):\\n\\n    flags:\\n        ``-`` : left justify\\n\\n        ``+`` : Forces to preceed result with + or -.\\n\\n        ``0`` : Left pad the number with zeros instead of space (see width).\\n\\n    width:\\n        Minimum number of characters to be printed. The value is not truncated\\n        if it has more characters.\\n\\n    precision:\\n        - For integer specifiers (eg. ``d,i,o,x``), the minimum number of\\n          digits.\\n        - For ``e, E`` and ``f`` specifiers, the number of digits to print\\n          after the decimal point.\\n        - For ``g`` and ``G``, the maximum number of significant digits.\\n        - For ``s``, the maximum number of characters.\\n\\n    specifiers:\\n        ``c`` : character\\n\\n        ``d`` or ``i`` : signed decimal integer\\n\\n        ``e`` or ``E`` : scientific notation with ``e`` or ``E``.\\n\\n        ``f`` : decimal floating point\\n\\n        ``g,G`` : use the shorter of ``e,E`` or ``f``\\n\\n        ``o`` : signed octal\\n\\n        ``s`` : string of characters\\n\\n        ``u`` : unsigned decimal integer\\n\\n        ``x,X`` : unsigned hexadecimal integer\\n\\n    This explanation of ``fmt`` is not complete, for an exhaustive\\n    specification see [1]_.\\n\\n    References\\n    ----------\\n    .. [1] `Format Specification Mini-Language\\n           <http://docs.python.org/library/string.html#\\n           format-specification-mini-language>`_, Python Documentation.\\n\\n    Examples\\n    --------\\n    >>> savetxt('test.out', x, delimiter=',')   # x is an array\\n    >>> savetxt('test.out', (x,y,z))   # x,y,z equal sized 1D arrays\\n    >>> savetxt('test.out', x, fmt='%1.4e')   # use exponential notation\\n\\n    \"\n    with get_file_obj(fname, 'w') as fh:\n        X = np.asarray(X)\n        if (X.ndim == 1):\n            if (X.dtype.names is None):\n                X = np.atleast_2d(X).T\n                ncol = 1\n            else:\n                ncol = len(X.dtype.descr)\n        else:\n            ncol = X.shape[1]\n        if isinstance(fmt, (list, tuple)):\n            if (len(fmt) != ncol):\n                raise AttributeError(('fmt has wrong shape.  %s' % str(fmt)))\n            format = delimiter.join(fmt)\n        elif isinstance(fmt, string_types):\n            if (fmt.count('%') == 1):\n                fmt = ([fmt] * ncol)\n                format = delimiter.join(fmt)\n            elif (fmt.count('%') != ncol):\n                raise AttributeError(('fmt has wrong number of %% formats.  %s' % fmt))\n            else:\n                format = fmt\n        if ((names is None) and X.dtype.names):\n            names = X.dtype.names\n        if (names is not None):\n            fh.write((delimiter.join(names) + '\\n'))\n        for row in X:\n            fh.write(((format % tuple(row)) + '\\n'))\n", "label": 1}
{"function": "\n\ndef add_global_view_imports(self, path):\n    import_list = list()\n    with open(path, 'r') as import_file:\n        need_import_shortcut = True\n        need_import_urlresolvers = True\n        need_import_users = True\n        need_import_token = True\n        need_import_JsonResponse = True\n        for line in import_file.readlines():\n            if ('from django.shortcuts import render, redirect, get_object_or_404' in line):\n                need_import_shortcut = False\n            if ('from django.core.urlresolvers import reverse' in line):\n                need_import_urlresolvers = False\n            if ('from django.contrib.auth.models import User, Group' in line):\n                need_import_users = False\n            if ('from django.middleware.csrf import get_token' in line):\n                need_import_token = False\n            if ('from django_common.http import JsonResponse' in line):\n                need_import_JsonResponse = False\n        if need_import_shortcut:\n            import_list.append('from django.shortcuts import render, redirect, get_object_or_404')\n        if need_import_urlresolvers:\n            import_list.append('from django.core.urlresolvers import reverse')\n        if need_import_users:\n            import_list.append('from django.contrib.auth.models import User, Group')\n        if need_import_token:\n            import_list.append('from django.middleware.csrf import get_token')\n        if need_import_JsonResponse:\n            import_list.append('from django_common.http import JsonResponse')\n    return import_list\n", "label": 1}
{"function": "\n\ndef _parse_settings_bond_4(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond4.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '4',\n    }\n    for binding in ['miimon', 'downdelay', 'updelay', 'lacp_rate', 'ad_select']:\n        if (binding in opts):\n            if (binding == 'lacp_rate'):\n                if (opts[binding] == 'fast'):\n                    opts.update({\n                        binding: '1',\n                    })\n                if (opts[binding] == 'slow'):\n                    opts.update({\n                        binding: '0',\n                    })\n                valid = ['fast', '1', 'slow', '0']\n            else:\n                valid = ['integer']\n            try:\n                int(opts[binding])\n                bond.update({\n                    binding: opts[binding],\n                })\n            except Exception:\n                _raise_error_iface(iface, binding, valid)\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({\n                binding: bond_def[binding],\n            })\n    if ('use_carrier' in opts):\n        if (opts['use_carrier'] in _CONFIG_TRUE):\n            bond.update({\n                'use_carrier': '1',\n            })\n        elif (opts['use_carrier'] in _CONFIG_FALSE):\n            bond.update({\n                'use_carrier': '0',\n            })\n        else:\n            valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({\n            'use_carrier': bond_def['use_carrier'],\n        })\n    if ('hashing-algorithm' in opts):\n        valid = ['layer2', 'layer2+3', 'layer3+4']\n        if (opts['hashing-algorithm'] in valid):\n            bond.update({\n                'xmit_hash_policy': opts['hashing-algorithm'],\n            })\n        else:\n            _raise_error_iface(iface, 'hashing-algorithm', valid)\n    return bond\n", "label": 1}
{"function": "\n\ndef _check_required(self):\n    errors = {\n        \n    }\n    if self.required:\n        for (field_name, field) in self.schema.fields.items():\n            if (not field.required):\n                continue\n            error_field_name = (field.load_from or field_name)\n            if (isinstance(field, Nested) and (self.nested != _RECURSIVE_NESTED) and (field.nested != _RECURSIVE_NESTED)):\n                errors[error_field_name] = field._check_required()\n            else:\n                try:\n                    field._validate_missing(field.missing)\n                except ValidationError as ve:\n                    errors[error_field_name] = ve.messages\n        if (self.many and errors):\n            errors = {\n                0: errors,\n            }\n        if (not errors):\n            self.fail('required')\n    return errors\n", "label": 1}
{"function": "\n\ndef strStr(self, haystack, needle):\n    '\\n        KMP algorithm\\n        :type haystack: str\\n        :type needle: str\\n        :param haystack:\\n        :param needle:\\n        :return:\\n        '\n    ln = len(needle)\n    lh = len(haystack)\n    if (ln == 0):\n        return haystack\n    if (ln == 1):\n        try:\n            index = haystack.index(needle)\n            return haystack[index:]\n        except ValueError:\n            return None\n    T = [0 for _ in xrange(ln)]\n    T[0] = (- 1)\n    T[1] = 0\n    pos = 2\n    cnd = 0\n    while (pos < ln):\n        if (needle[(pos - 1)] == needle[cnd]):\n            cnd += 1\n            T[pos] = cnd\n            pos += 1\n        elif (T[cnd] != (- 1)):\n            cnd = T[cnd]\n        else:\n            cnd = 0\n            T[pos] = cnd\n            pos += 1\n    i = 0\n    m = 0\n    while ((m + i) < lh):\n        if (needle[i] == haystack[(m + i)]):\n            i += 1\n            if (i == ln):\n                return haystack[m:]\n        elif (T[i] != (- 1)):\n            m = ((m + i) - T[i])\n            i = T[i]\n        else:\n            m += 1\n            i = 0\n    return None\n", "label": 1}
{"function": "\n\ndef test_reasonable_feasibility_results():\n    pf = sqpf.SqFtProForma()\n    df = pd.DataFrame({\n        'residential': [30, 20, 10],\n        'office': [15, 15, 15],\n        'retail': [12, 12, 12],\n        'industrial': [12, 12, 12],\n        'land_cost': [(1000 * 100), (1000 * 100), (1000 * 100)],\n        'parcel_size': [1000, 1000, 1000],\n        'max_far': [2.0, 2.0, 2.0],\n        'max_height': [80, 80, 80],\n    }, index=['a', 'b', 'c'])\n    out = pf.lookup('residential', df)\n    first = out.iloc[0]\n    assert (first.max_profit_far == 1.8)\n    assert (first.building_sqft == 1800)\n    assert (100 < (first.building_cost / first.building_sqft) < 400)\n    assert (first.total_cost == (first.building_cost + df.iloc[0].land_cost))\n    assert (200 < (first.building_revenue / first.building_sqft) < 800)\n    assert (first.residential_sqft == (first.building_sqft * pf.config.building_efficiency))\n    assert (first.max_profit_far < first.stories < (first.max_profit_far * 3.0))\n    assert (first.non_residential_sqft == 0)\n    assert (first.max_profit > 0)\n    assert (len(out) == 1)\n    c = sqpf.SqFtProFormaConfig()\n    c.parking_rates['residential'] = 0\n    pf = sqpf.SqFtProForma(c)\n    out = pf.lookup('residential', df)\n    second = out.iloc[1]\n    assert (second.max_profit_far == 2.0)\n", "label": 1}
{"function": "\n\ndef draw_networkx_edge_labels(G, pos, edge_labels=None, label_pos=0.5, font_size=10, font_color='k', font_family='sans-serif', font_weight='normal', alpha=1.0, bbox=None, ax=None, rotate=True, **kwds):\n    \"Draw edge labels.\\n\\n    Parameters\\n    ----------\\n    G : graph\\n       A networkx graph\\n\\n    pos : dictionary, optional\\n       A dictionary with nodes as keys and positions as values.\\n       If not specified a spring layout positioning will be computed.\\n       See networkx.layout for functions that compute node positions.\\n\\n    ax : Matplotlib Axes object, optional\\n       Draw the graph in the specified Matplotlib axes.\\n\\n    alpha : float\\n       The text transparency (default=1.0)\\n\\n    edge_labels : dictionary\\n       Edge labels in a dictionary keyed by edge two-tuple of text\\n       labels (default=None). Only labels for the keys in the dictionary\\n       are drawn.\\n\\n    label_pos : float\\n       Position of edge label along edge (0=head, 0.5=center, 1=tail)\\n\\n    font_size : int\\n       Font size for text labels (default=12)\\n\\n    font_color : string\\n       Font color string (default='k' black)\\n\\n    font_weight : string\\n       Font weight (default='normal')\\n\\n    font_family : string\\n       Font family (default='sans-serif')\\n\\n    bbox : Matplotlib bbox\\n       Specify text box shape and colors.\\n\\n    clip_on : bool\\n       Turn on clipping at axis boundaries (default=True)\\n\\n    Examples\\n    --------\\n    >>> G=nx.dodecahedral_graph()\\n    >>> edge_labels=nx.draw_networkx_edge_labels(G,pos=nx.spring_layout(G))\\n\\n    Also see the NetworkX drawing examples at\\n    http://networkx.lanl.gov/gallery.html\\n\\n    See Also\\n    --------\\n    draw()\\n    draw_networkx()\\n    draw_networkx_nodes()\\n    draw_networkx_edges()\\n    draw_networkx_labels()\\n    \"\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.cbook as cb\n        import numpy\n    except ImportError:\n        raise ImportError('Matplotlib required for draw()')\n    except RuntimeError:\n        print('Matplotlib unable to open display')\n        raise\n    if (ax is None):\n        ax = plt.gca()\n    if (edge_labels is None):\n        labels = dict((((u, v), d) for (u, v, d) in G.edges(data=True)))\n    else:\n        labels = edge_labels\n    text_items = {\n        \n    }\n    for ((n1, n2), label) in labels.items():\n        (x1, y1) = pos[n1]\n        (x2, y2) = pos[n2]\n        (x, y) = (((x1 * label_pos) + (x2 * (1.0 - label_pos))), ((y1 * label_pos) + (y2 * (1.0 - label_pos))))\n        if rotate:\n            angle = ((numpy.arctan2((y2 - y1), (x2 - x1)) / (2.0 * numpy.pi)) * 360)\n            if (angle > 90):\n                angle -= 180\n            if (angle < (- 90)):\n                angle += 180\n            xy = numpy.array((x, y))\n            trans_angle = ax.transData.transform_angles(numpy.array((angle,)), xy.reshape((1, 2)))[0]\n        else:\n            trans_angle = 0.0\n        if (bbox is None):\n            bbox = dict(boxstyle='round', ec=(1.0, 1.0, 1.0), fc=(1.0, 1.0, 1.0))\n        if (not cb.is_string_like(label)):\n            label = str(label)\n        horizontalalignment = kwds.get('horizontalalignment', 'center')\n        verticalalignment = kwds.get('verticalalignment', 'center')\n        t = ax.text(x, y, label, size=font_size, color=font_color, family=font_family, weight=font_weight, horizontalalignment=horizontalalignment, verticalalignment=verticalalignment, rotation=trans_angle, transform=ax.transData, bbox=bbox, zorder=1, clip_on=True)\n        text_items[(n1, n2)] = t\n    return text_items\n", "label": 1}
{"function": "\n\ndef collect_from(self, filename):\n    if (not os.access(filename, os.R_OK)):\n        self.log.error('HBaseCollector unable to read \"%s\"', filename)\n        return False\n    fd = open(filename, 'r+')\n    for line in fd:\n        match = self.re_log.match(line)\n        if (not match):\n            continue\n        metrics = {\n            \n        }\n        data = match.groupdict()\n        for metric in data['metrics'].split(','):\n            metric = metric.strip()\n            if ('=' in metric):\n                (key, value) = metric.split('=', 1)\n                metrics[key] = value\n        for metric in metrics.keys():\n            try:\n                if (data['name'] == 'jvm.metrics'):\n                    path = self.get_metric_path('.'.join([data['name'], metrics['hostName'].replace('.', '_'), metrics['processName'].replace(' ', '_'), metric]))\n                elif (data['name'] == 'mapred.job'):\n                    path = self.get_metric_path('.'.join([data['name'], metrics['hostName'].replace('.', '_'), metrics['group'].replace(' ', '_'), metrics['counter'].replace(' ', '_'), metric]))\n                elif (data['name'] == 'rpc.metrics'):\n                    if (metric == 'port'):\n                        continue\n                    path = self.get_metric_path('.'.join([data['name'], metrics['hostName'].replace('.', '_'), metrics['port'], metric]))\n                else:\n                    path = self.get_metric_path('.'.join([data['name'], metric]))\n                value = float(metrics[metric])\n                self.publish_metric(Metric(path, value, timestamp=(int(data['timestamp']) / 1000)))\n            except ValueError:\n                pass\n    fd.seek(0)\n    fd.truncate()\n    fd.close()\n", "label": 1}
{"function": "\n\ndef parse_declaration(decl):\n    try:\n        what = decl['decl']['what']\n        docs = crlf2lf(decl.get('docs'))\n        name = decl['name']\n        pos = parse_position(decl.get('pos'))\n        imported = []\n        if (('imported' in decl) and decl['imported']):\n            imported = [parse_import(d) for d in decl['imported']]\n        defined = None\n        if (('defined' in decl) and decl['defined']):\n            defined = parse_module_id(decl['defined'])\n        if (what == 'function'):\n            return symbols.Function(name, decl['decl'].get('type'), docs, imported, defined, pos)\n        elif (what == 'type'):\n            return symbols.Type(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)\n        elif (what == 'newtype'):\n            return symbols.Newtype(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)\n        elif (what == 'data'):\n            return symbols.Data(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)\n        elif (what == 'class'):\n            return symbols.Class(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)\n        else:\n            return None\n    except Exception as e:\n        log('Error pasring declaration: {0}'.format(e), log_error)\n        return None\n", "label": 1}
{"function": "\n\ndef _hijack_tty(self, pumps):\n    with tty.Terminal(self.stdin, raw=self.israw()):\n        self.resize()\n        while True:\n            read_pumps = [p for p in pumps if (not p.eof)]\n            write_streams = [p.to_stream for p in pumps if p.to_stream.needs_write()]\n            (read_ready, write_ready) = io.select(read_pumps, write_streams, timeout=60)\n            try:\n                for write_stream in write_ready:\n                    write_stream.do_write()\n                for pump in read_ready:\n                    pump.flush()\n                if all([p.is_done() for p in pumps]):\n                    break\n            except SSLError as e:\n                if ('The operation did not complete' not in e.strerror):\n                    raise e\n", "label": 1}
{"function": "\n\ndef test_int_contains_lookup(self):\n    onetwo = IntSetModel.objects.create(field={1, 2})\n    ones = IntSetModel.objects.filter(field__contains=1)\n    assert (ones.count() == 1)\n    assert (ones[0] == onetwo)\n    twos = IntSetModel.objects.filter(field__contains=2)\n    assert (twos.count() == 1)\n    assert (twos[0] == onetwo)\n    threes = IntSetModel.objects.filter(field__contains=3)\n    assert (threes.count() == 0)\n    with pytest.raises(ValueError):\n        list(IntSetModel.objects.filter(field__contains={1, 2}))\n    ones_and_twos = IntSetModel.objects.filter((Q(field__contains=1) & Q(field__contains=2)))\n    assert (ones_and_twos.count() == 1)\n    assert (ones_and_twos[0] == onetwo)\n    ones_and_threes = IntSetModel.objects.filter((Q(field__contains=1) & Q(field__contains=3)))\n    assert (ones_and_threes.count() == 0)\n    ones_or_threes = IntSetModel.objects.filter((Q(field__contains=1) | Q(field__contains=3)))\n    assert (ones_or_threes.count() == 1)\n    no_three = IntSetModel.objects.exclude(field__contains=3)\n    assert (no_three.count() == 1)\n    no_one = IntSetModel.objects.exclude(field__contains=1)\n    assert (no_one.count() == 0)\n", "label": 1}
{"function": "\n\ndef _get_socket(self, sslversion=ssl.PROTOCOL_TLSv1):\n    'Sets up an https connection and do an HTTP/raw socket request\\n\\n        :param sslversion: version of ssl session\\n        :raises: IloConnectionError, for connection failures\\n        :returns: ssl wrapped socket object\\n        '\n    err = None\n    sock = None\n    try:\n        for res in socket.getaddrinfo(self.hostname, self.port, 0, socket.SOCK_STREAM):\n            (af, socktype, proto, canonname, sa) = res\n            try:\n                sock = socket.socket(af, socktype, proto)\n                sock.settimeout(self.timeout)\n                sock.connect(sa)\n            except socket.timeout:\n                if (sock is not None):\n                    sock.close()\n                err = exception.IloConnectionError(('Timeout connecting to %(hostname)s:%(port)d' % {\n                    'hostname': self.hostname,\n                    'port': self.port,\n                }))\n            except socket.error:\n                if (sock is not None):\n                    sock.close()\n                e = sys.exc_info()[1]\n                err = exception.IloConnectionError(('Error connecting to %(hostname)s:%(port)d : %(error)s' % {\n                    'hostname': self.hostname,\n                    'port': self.port,\n                    'error': str(e),\n                }))\n    except Exception:\n        raise exception.IloConnectionError(('Unable to resolve %s' % self.hostname))\n    if (err is not None):\n        raise err\n    try:\n        return ssl.wrap_socket(sock, ssl_version=sslversion)\n    except socket.sslerror:\n        e = sys.exc_info()[1]\n        msg = (getattr(e, 'reason', None) or getattr(e, 'message', None))\n        if (('wrong version number' in msg) and (sslversion == ssl.PROTOCOL_TLSv1)):\n            return self._get_socket(ssl.PROTOCOL_SSLv3)\n        raise exception.IloConnectionError(('Cannot establish ssl session with %(hostname)s:%(port)d : %(error)s' % {\n            'hostname': self.hostname,\n            'port': self.port,\n            'error': str(e),\n        }))\n", "label": 1}
{"function": "\n\ndef _update_project_groups(self, request, data, project_id, domain_id):\n    groups_to_modify = 0\n    member_step = self.get_step(PROJECT_GROUP_MEMBER_SLUG)\n    try:\n        available_roles = self._get_available_roles(request)\n        project_groups = api.keystone.group_list(request, domain=domain_id, project=project_id)\n        groups_to_modify = len(project_groups)\n        for group in project_groups:\n            current_roles = api.keystone.roles_for_group(self.request, group=group.id, project=project_id)\n            current_role_ids = [role.id for role in current_roles]\n            for role in available_roles:\n                field_name = member_step.get_member_field_name(role.id)\n                if (group.id in data[field_name]):\n                    if (role.id not in current_role_ids):\n                        api.keystone.add_group_role(request, role=role.id, group=group.id, project=project_id)\n                    else:\n                        index = current_role_ids.index(role.id)\n                        current_role_ids.pop(index)\n            for id_to_delete in current_role_ids:\n                api.keystone.remove_group_role(request, role=id_to_delete, group=group.id, project=project_id)\n            groups_to_modify -= 1\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            groups_to_modify += len(data[field_name])\n        for role in available_roles:\n            groups_added = 0\n            field_name = member_step.get_member_field_name(role.id)\n            for group_id in data[field_name]:\n                if (not filter((lambda x: (group_id == x.id)), project_groups)):\n                    api.keystone.add_group_role(request, role=role.id, group=group_id, project=project_id)\n                groups_added += 1\n            groups_to_modify -= groups_added\n        return True\n    except Exception:\n        exceptions.handle(request, (_('Failed to modify %s project members, update project groups and update project quotas.') % groups_to_modify))\n        return False\n", "label": 1}
{"function": "\n\ndef _rpc(self, method, callback=None, acceptable_replies=None):\n    \"Make a syncronous channel RPC call for a synchronous method frame. If\\n        the channel is already in the blocking state, then enqueue the request,\\n        but don't send it at this time; it will be eventually sent by\\n        `_on_synchronous_complete` after the prior blocking request receives a\\n        resposne. If the channel is not in the blocking state and\\n        `acceptable_replies` is not empty, transition the channel to the\\n        blocking state and register for `_on_synchronous_complete` before\\n        sending the request.\\n\\n        NOTE: A callback must be accompanied by non-empty acceptable_replies.\\n\\n        :param pika.amqp_object.Method method: The AMQP method to invoke\\n        :param callable callback: The callback for the RPC response\\n        :param acceptable_replies: A (possibly empty) sequence of\\n            replies this RPC call expects or None\\n        :type acceptable_replies: list or None\\n\\n        \"\n    assert method.synchronous, ('Only synchronous-capable methods may be used with _rpc: %r' % (method,))\n    if (not isinstance(acceptable_replies, (type(None), list))):\n        raise TypeError('acceptable_replies should be list or None')\n    if (callback is not None):\n        if (not is_callable(callback)):\n            raise TypeError('callback should be None or a callable')\n        if (not acceptable_replies):\n            raise ValueError('Unexpected callback for asynchronous (nowait) operation.')\n    if self.is_closed:\n        raise exceptions.ChannelClosed\n    if self._blocking:\n        LOGGER.debug('Already in blocking state, so enqueueing method %s; acceptable_replies=%r', method, acceptable_replies)\n        return self._blocked.append([method, callback, acceptable_replies])\n    if acceptable_replies:\n        self._blocking = method.NAME\n        LOGGER.debug('Entering blocking state on frame %s; acceptable_replies=%r', method, acceptable_replies)\n        for reply in acceptable_replies:\n            if isinstance(reply, tuple):\n                (reply, arguments) = reply\n            else:\n                arguments = None\n            LOGGER.debug('Adding on_synchronous_complete callback')\n            self.callbacks.add(self.channel_number, reply, self._on_synchronous_complete, arguments=arguments)\n            if (callback is not None):\n                LOGGER.debug('Adding passed-in callback')\n                self.callbacks.add(self.channel_number, reply, callback, arguments=arguments)\n    self._send_method(method)\n", "label": 1}
{"function": "\n\ndef _determine_joins(self):\n    \"Determine the 'primaryjoin' and 'secondaryjoin' attributes,\\n        if not passed to the constructor already.\\n\\n        This is based on analysis of the foreign key relationships\\n        between the parent and target mapped selectables.\\n\\n        \"\n    if ((self.secondaryjoin is not None) and (self.secondary is None)):\n        raise sa_exc.ArgumentError(('Property %s specified with secondary join condition but no secondary argument' % self.prop))\n    try:\n        consider_as_foreign_keys = (self.consider_as_foreign_keys or None)\n        if (self.secondary is not None):\n            if (self.secondaryjoin is None):\n                self.secondaryjoin = join_condition(self.child_selectable, self.secondary, a_subset=self.child_local_selectable, consider_as_foreign_keys=consider_as_foreign_keys)\n            if (self.primaryjoin is None):\n                self.primaryjoin = join_condition(self.parent_selectable, self.secondary, a_subset=self.parent_local_selectable, consider_as_foreign_keys=consider_as_foreign_keys)\n        elif (self.primaryjoin is None):\n            self.primaryjoin = join_condition(self.parent_selectable, self.child_selectable, a_subset=self.parent_local_selectable, consider_as_foreign_keys=consider_as_foreign_keys)\n    except sa_exc.NoForeignKeysError:\n        if (self.secondary is not None):\n            raise sa_exc.NoForeignKeysError((\"Could not determine join condition between parent/child tables on relationship %s - there are no foreign keys linking these tables via secondary table '%s'.  Ensure that referencing columns are associated with a ForeignKey or ForeignKeyConstraint, or specify 'primaryjoin' and 'secondaryjoin' expressions.\" % (self.prop, self.secondary)))\n        else:\n            raise sa_exc.NoForeignKeysError((\"Could not determine join condition between parent/child tables on relationship %s - there are no foreign keys linking these tables.  Ensure that referencing columns are associated with a ForeignKey or ForeignKeyConstraint, or specify a 'primaryjoin' expression.\" % self.prop))\n    except sa_exc.AmbiguousForeignKeysError:\n        if (self.secondary is not None):\n            raise sa_exc.AmbiguousForeignKeysError((\"Could not determine join condition between parent/child tables on relationship %s - there are multiple foreign key paths linking the tables via secondary table '%s'.  Specify the 'foreign_keys' argument, providing a list of those columns which should be counted as containing a foreign key reference from the secondary table to each of the parent and child tables.\" % (self.prop, self.secondary)))\n        else:\n            raise sa_exc.AmbiguousForeignKeysError((\"Could not determine join condition between parent/child tables on relationship %s - there are multiple foreign key paths linking the tables.  Specify the 'foreign_keys' argument, providing a list of those columns which should be counted as containing a foreign key reference to the parent table.\" % self.prop))\n", "label": 1}
{"function": "\n\n@add_auth_token_to_kwargs_from_cli\ndef _print_help(self, args, **kwargs):\n    action_mgr = self.app.client.managers['Action']\n    action_exec_mgr = self.app.client.managers['LiveAction']\n    if args.help:\n        action_ref_or_id = getattr(args, 'ref_or_id', None)\n        action_exec_id = getattr(args, 'id', None)\n        if (action_exec_id and (not action_ref_or_id)):\n            action_exec = action_exec_mgr.get_by_id(action_exec_id, **kwargs)\n            args.ref_or_id = action_exec.action\n        if action_ref_or_id:\n            try:\n                action = action_mgr.get_by_ref_or_id(args.ref_or_id, **kwargs)\n                if (not action):\n                    raise resource.ResourceNotFoundError('Action %s not found', args.ref_or_id)\n                runner_mgr = self.app.client.managers['RunnerType']\n                runner = runner_mgr.get_by_name(action.runner_type, **kwargs)\n                (parameters, required, optional, _) = self._get_params_types(runner, action)\n                print('')\n                print(textwrap.fill(action.description))\n                print('')\n                if required:\n                    required = self._sort_parameters(parameters=parameters, names=required)\n                    print('Required Parameters:')\n                    [self._print_param(name, parameters.get(name)) for name in required]\n                if optional:\n                    optional = self._sort_parameters(parameters=parameters, names=optional)\n                    print('Optional Parameters:')\n                    [self._print_param(name, parameters.get(name)) for name in optional]\n            except resource.ResourceNotFoundError:\n                print((('Action \"%s\" is not found. ' % args.ref_or_id) + 'Do \"st2 action list\" to see list of available actions.'))\n            except Exception as e:\n                print(('ERROR: Unable to print help for action \"%s\". %s' % (args.ref_or_id, e)))\n        else:\n            self.parser.print_help()\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef test_can_load_from_site_meta(self):\n    d = {\n        'title': 'A nice title',\n        'author': 'Lakshmi Vyas',\n    }\n    text = '\\n---\\ntitle: Even nicer title\\n---\\n{%% extends \"base.html\" %%}\\n\\n{%% block main %%}\\n    Hi!\\n\\n    I am a test template to make sure jinja2 generation works well with hyde.\\n    <span class=\"title\">{{resource.meta.title}}</span>\\n    <span class=\"author\">{{resource.meta.author}}</span>\\n    <span class=\"twitter\">{{resource.meta.twitter}}</span>\\n{%% endblock %%}\\n'\n    about2 = File(TEST_SITE.child('content/about2.html'))\n    about2.write((text % d))\n    meta = File(TEST_SITE.child('content/nodemeta.yaml'))\n    meta.write(yaml.dump(d))\n    s = Site(TEST_SITE)\n    s.config.plugins = ['hyde.ext.plugins.meta.MetaPlugin']\n    s.config.meta = {\n        'author': 'Lakshmi',\n        'twitter': 'lakshmivyas',\n    }\n    gen = Generator(s)\n    gen.generate_all()\n    res = s.content.resource_from_path(about2.path)\n    assert hasattr(res, 'meta')\n    assert hasattr(res.meta, 'title')\n    assert hasattr(res.meta, 'author')\n    assert hasattr(res.meta, 'twitter')\n    assert (res.meta.title == 'Even nicer title')\n    assert (res.meta.author == 'Lakshmi Vyas')\n    assert (res.meta.twitter == 'lakshmivyas')\n    target = File(Folder(s.config.deploy_root_path).child('about2.html'))\n    text = target.read_all()\n    q = PyQuery(text)\n    for (k, v) in d.items():\n        if (not (k == 'title')):\n            assert (v in q(('span.' + k)).text())\n    assert (q('span.title').text() == 'Even nicer title')\n", "label": 1}

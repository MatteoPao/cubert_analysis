{"function": "\n\ndef test_s3_domain_with_default_root_object(self):\n    cmdline = ((self.prefix + '--origin-domain-name foo.s3.amazonaws.com ') + '--default-root-object index.html')\n    result = {\n        'DistributionConfig': {\n            'Origins': {\n                'Quantity': 1,\n                'Items': [{\n                    'S3OriginConfig': mock.ANY,\n                    'DomainName': 'foo.s3.amazonaws.com',\n                    'Id': mock.ANY,\n                    'OriginPath': '',\n                }],\n            },\n            'CallerReference': mock.ANY,\n            'Comment': '',\n            'Enabled': True,\n            'DefaultCacheBehavior': mock.ANY,\n            'DefaultRootObject': 'index.html',\n        },\n    }\n    self.run_cmd(cmdline)\n    self.assertEqual(self.last_kwargs, result)\n", "label": 0}
{"function": "\n\ndef pages_dynamic_tree_menu(context, page, url='/'):\n    '\\n    Render a \"dynamic\" tree menu, with all nodes expanded which are either\\n    ancestors or the current page itself.\\n\\n    Override ``pages/dynamic_tree_menu.html`` if you want to change the\\n    design.\\n\\n    :param page: the current page\\n    :param url: not used anymore\\n    '\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    children = None\n    if (page and ('current_page' in context)):\n        current_page = context['current_page']\n        if ((page.tree_id == current_page.tree_id) and (page.lft <= current_page.lft) and (page.rght >= current_page.rght)):\n            children = page.get_children_for_frontend()\n    context.update({\n        'children': children,\n        'page': page,\n    })\n    return context\n", "label": 1}
{"function": "\n\ndef remove(path, recursive=False, use_sudo=False):\n    '\\n    Remove a file or directory\\n    '\n    func = ((use_sudo and run_as_root) or run)\n    options = ('-r ' if recursive else '')\n    func('/bin/rm {0}{1}'.format(options, quote(path)))\n", "label": 0}
{"function": "\n\ndef marshal_dump(code, f):\n    if isinstance(f, file):\n        marshal.dump(code, f)\n    else:\n        f.write(marshal.dumps(code))\n", "label": 0}
{"function": "\n\ndef test_show_body(self):\n    client = Mock()\n    client.indices.get_settings.return_value = testvars.settings_one\n    client.cluster.state.return_value = testvars.clu_state_one\n    client.indices.stats.return_value = testvars.stats_one\n    ilo = curator.IndexList(client)\n    ao = curator.Alias(name='alias')\n    ao.remove(ilo)\n    ao.add(ilo)\n    body = ao.body()\n    self.assertEqual(testvars.alias_one_body['actions'][0], body['actions'][0])\n    self.assertEqual(testvars.alias_one_body['actions'][1], body['actions'][1])\n", "label": 0}
{"function": "\n\n@mock.patch('SoftLayer.API.BaseClient.iter_call')\ndef test_iterate(self, _iter_call):\n    self.client['SERVICE'].METHOD(iter=True)\n    _iter_call.assert_called_with('SERVICE', 'METHOD')\n", "label": 0}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    try:\n        self._call(*args, **kwargs)\n    except Exception as err:\n        stats.incr('callback-failure', 1)\n        logging.exception('Callback Failed: %s', err)\n", "label": 0}
{"function": "\n\n@register.filter\ndef lookup(h, key):\n    try:\n        return h[key]\n    except KeyError:\n        return ''\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    return self.build('add', self, other)\n", "label": 0}
{"function": "\n\n@classmethod\ndef read(cls, handle):\n    self = cls()\n    self._read_luminosity(handle)\n    self.name = handle.attrs['name'].decode('utf-8')\n    self.peeloff = str2bool(handle.attrs['peeloff'])\n    if (handle.attrs['spectrum'] == b'spectrum'):\n        self.spectrum = Table(np.array(handle['spectrum']))\n    elif (handle.attrs['spectrum'] == b'temperature'):\n        self.temperature = handle.attrs['temperature']\n    elif (handle.attrs['spectrum'] == b'lte'):\n        pass\n    else:\n        raise ValueError(('Unexpected value for `spectrum`: %s' % handle.attrs['spectrum']))\n    return self\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    category = resolve(self.category, context)\n    if isinstance(category, CategoryBase):\n        cat = category\n    else:\n        cat = get_category(category, self.model)\n    try:\n        if (cat is not None):\n            context[self.varname] = drilldown_tree_for_node(cat)\n        else:\n            context[self.varname] = []\n    except:\n        context[self.varname] = []\n    return ''\n", "label": 0}
{"function": "\n\ndef __init__(self, manufacturer, data):\n    self.manufacturer = manufacturer\n    self.data = list(data)\n", "label": 0}
{"function": "\n\ndef process_response(self, request, response):\n    from django.conf import settings\n    from rest_framework.exceptions import PermissionDenied\n    from .access_control import upload_prefix_for_request\n    cookie_name = getattr(settings, 'UPLOAD_PREFIX_COOKIE_NAME', 'upload_prefix')\n    try:\n        response.set_cookie(cookie_name, upload_prefix_for_request(request))\n    except PermissionDenied:\n        response.delete_cookie(cookie_name)\n    return response\n", "label": 0}
{"function": "\n\ndef createLineSet(self, indices, inputlist, materialid):\n    'Create a set of lines for use in this geometry instance.\\n\\n        :param numpy.array indices:\\n          unshaped numpy array that contains the indices for\\n          the inputs referenced in inputlist\\n        :param collada.source.InputList inputlist:\\n          The inputs for this primitive\\n        :param str materialid:\\n          A string containing a symbol that will get used to bind this lineset\\n          to a material when instantiating into a scene\\n\\n        :rtype: :class:`collada.lineset.LineSet`\\n        '\n    inputdict = primitive.Primitive._getInputsFromList(self.collada, self.sourceById, inputlist.getList())\n    return lineset.LineSet(inputdict, materialid, indices)\n", "label": 0}
{"function": "\n\ndef stepSlice(self, offset):\n    ' Move the selected structure one slice up or down\\n        :param offset: +1 or -1\\n        :return:\\n        '\n    volumeId = self.volumeSelector.currentNodeId\n    if (volumeId == ''):\n        self.showUnselectedVolumeWarningMessage()\n        return\n    selectedStructure = self.getCurrentSelectedStructure()\n    if (selectedStructure == self.logic.NONE):\n        self.showUnselectedStructureWarningMessage()\n        return\n    if (selectedStructure == self.logic.BOTH):\n        self.logic.stepSlice(volumeId, self.logic.AORTA, offset)\n        newSlice = self.logic.stepSlice(volumeId, self.logic.PA, offset)\n    else:\n        newSlice = self.logic.stepSlice(volumeId, selectedStructure, offset)\n    self.moveRedWindowToSlice(newSlice)\n", "label": 0}
{"function": "\n\ndef normalize_diff_filename(self, filename):\n    \"Normalize filenames in diffs.\\n\\n        The default behavior of stripping off leading slashes doesn't work for\\n        Perforce (because depot paths start with //), so this overrides it to\\n        just return the filename un-molested.\\n        \"\n    return filename\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/item/shared_item_chair_metal_s1.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef test_content_url_encoding_safe(self):\n    s = Site(self.SITE_PATH, config=self.config)\n    s.load()\n    path = '\".jpg/abc'\n    print(s.content_url(path, ''))\n    print(('/' + quote(path, '')))\n    assert (s.content_url(path, '') == ('/' + quote(path, '')))\n", "label": 0}
{"function": "\n\ndef post_undelete(self, *args, **kwargs):\n    self.post_undelete_called = True\n", "label": 0}
{"function": "\n\ndef timecolon(data):\n    match = re.search('(\\\\d+:\\\\d+:\\\\d+):(\\\\d+)', data)\n    return ('%s,%s' % (match.group(1), match.group(2)))\n", "label": 0}
{"function": "\n\ndef getUpdatedBatchJob(self, maxWait):\n    while True:\n        try:\n            (jobID, status, wallTime) = self.updatedJobsQueue.get(timeout=maxWait)\n        except Empty:\n            return None\n        try:\n            self.runningJobs.remove(jobID)\n        except KeyError:\n            pass\n        else:\n            return (jobID, status, wallTime)\n", "label": 0}
{"function": "\n\ndef package_private_devel_path(self, package):\n    'The path to the linked devel space for a given package.'\n    return os.path.join(self.private_devel_path, package.name)\n", "label": 0}
{"function": "\n\ndef test_getset_owner(self):\n    m = meta.Metadata()\n    o = m.get_owner('files/one')\n    m.set_owner('files/one', *o)\n", "label": 0}
{"function": "\n\ndef indent(string, prefix='    '):\n    '\\n    Indent every line of this string.\\n    '\n    return ''.join((('%s%s\\n' % (prefix, s)) for s in string.split('\\n')))\n", "label": 0}
{"function": "\n\ndef stories(self, task, params={\n    \n}, **options):\n    'Returns a compact representation of all of the stories on the task.\\n\\n        Parameters\\n        ----------\\n        task : {Id} The task containing the stories to get.\\n        [params] : {Object} Parameters for the request\\n        '\n    path = ('/tasks/%s/stories' % task)\n    return self.client.get_collection(path, params, **options)\n", "label": 0}
{"function": "\n\ndef get_context_types_value(context_id, source, filter_func, codebase):\n    query = CodeElementLink.objects.filter(code_element__kind__is_type=True).filter(index=0).filter(code_element__codebase=codebase).filter(code_reference__source=source)\n    query = filter_func(query, context_id)\n    context_types = []\n    pk_set = set()\n    for link in query.all():\n        code_element = link.code_element\n        if (code_element.pk not in pk_set):\n            context_types.append(code_element)\n            pk_set.add(code_element.pk)\n    return context_types\n", "label": 0}
{"function": "\n\ndef _validate_simple_authn(self, username, credentials):\n    '\\n        When the login() method is called, this method is used with the \\n        username and credentials (e.g., password, IP address, etc.). This\\n        method will only check the SimpleAuthn instances.\\n        '\n    try:\n        (login, role_name, user_auths) = self._db.retrieve_role_and_user_auths(username)\n    except DbUserNotFoundError:\n        return self._process_invalid()\n    errors = False\n    for user_auth in user_auths:\n        if user_auth.is_simple_authn():\n            try:\n                authenticated = user_auth.authenticate(login, credentials)\n            except:\n                log.log(LoginManager, log.level.Warning, ('Username: %s with user_auth %s: ERROR' % (login, user_auth)))\n                log.log_exc(LoginManager, log.level.Warning)\n                errors = True\n                traceback.print_exc()\n                continue\n            if authenticated:\n                log.log(LoginManager, log.level.Debug, ('Username: %s with user_auth %s: SUCCESS' % (login, user_auth)))\n                return ValidDatabaseSessionId(login, role_name)\n            else:\n                log.log(LoginManager, log.level.Warning, ('Username: %s with user_auth %s: FAIL' % (login, user_auth)))\n    if errors:\n        raise LoginErrors.LoginError('Error checking credentials. Contact administrators!')\n    return self._process_invalid()\n", "label": 1}
{"function": "\n\ndef test_elemwise_thats_also_a_column():\n    t = symbol('t', 'var * {x: int, time: datetime, y: int}')\n    expr = t[(t.x > 0)].time.truncate(months=1)\n    expected = t[['time', 'x']]\n    result = lean_projection(expr)\n    assert result._child._child._child.isidentical(t[['time', 'x']])\n", "label": 0}
{"function": "\n\ndef test_list(self):\n    lists = {('l%s' % i): list(string.ascii_lowercase[:i]) for i in range(1, 10)}\n    self.collections_common_tests(lists, 'l')\n", "label": 0}
{"function": "\n\ndef fit_transform(self, X, y):\n    'Fit and transform.'\n    self.fit(X, y)\n    return self.transform(X)\n", "label": 0}
{"function": "\n\ndef update_app(self, app_id, app, force=False, minimal=True):\n    'Update an app.\\n\\n        Applies writable settings in `app` to `app_id`\\n        Note: this method can not be used to rename apps.\\n\\n        :param str app_id: target application ID\\n        :param app: application settings\\n        :type app: :class:`marathon.models.app.MarathonApp`\\n        :param bool force: apply even if a deployment is in progress\\n        :param bool minimal: ignore nulls and empty collections\\n\\n        :returns: a dict containing the deployment id and version\\n        :rtype: dict\\n        '\n    app.version = None\n    params = {\n        'force': force,\n    }\n    data = app.to_json(minimal=minimal)\n    response = self._do_request('PUT', '/v2/apps/{app_id}'.format(app_id=app_id), params=params, data=data)\n    return response.json()\n", "label": 0}
{"function": "\n\ndef create_toplevel_ws(self, wsname, width, height, group=2, x=None, y=None):\n    root = self.app.make_window()\n    ws = self.make_ws(wsname, wstype='tabs')\n    vbox = Widgets.VBox()\n    vbox.set_border_width(0)\n    self._add_toolbar(vbox, ws)\n    vbox.add_widget(bnch.widget)\n    root.set_widget(vbox)\n    root.resize(width, height)\n    root.show()\n    self.toplevels.append(root)\n    if (x is not None):\n        root.move(x, y)\n    return bnch\n", "label": 0}
{"function": "\n\ndef get_queryset(self):\n    queryset = self.queryset\n    if isinstance(queryset, (QuerySet, Manager)):\n        queryset = queryset.all()\n    return queryset\n", "label": 0}
{"function": "\n\ndef get_current_editor(self):\n    page = self.notebook.get_current_page()\n    if (page is None):\n        return None\n    return page.get_text_widget()\n", "label": 0}
{"function": "\n\ndef test_error_load_single_field_type(single_schema):\n    (data, errors) = single_schema.load({\n        'child': {\n            'id': 'foo',\n        },\n    })\n    assert (not data)\n    assert (errors == {\n        'child': {\n            'id': [fields.Integer().error_messages['invalid']],\n        },\n    })\n", "label": 0}
{"function": "\n\ndef start(self, fileStore):\n    subprocess.check_call((self.cmd + ' 1'), shell=True)\n", "label": 0}
{"function": "\n\ndef test_reraise(self):\n    self.assertRaises(RuntimeError, reraise, RuntimeError, RuntimeError())\n    try:\n        raise RuntimeError('bla')\n    except Exception:\n        exc_info = sys.exc_info()\n    self.assertRaises(RuntimeError, reraise, *exc_info)\n", "label": 0}
{"function": "\n\ndef check_write_package(self, username, package_reference):\n    '\\n        username: User that request to write the package\\n        package_reference: PackageReference\\n        '\n    self.check_write_conan(username, package_reference.conan)\n", "label": 0}
{"function": "\n\ndef test_it_knows_how_many_total_errors_it_contains(self):\n    errors = [mock.MagicMock() for _ in range(8)]\n    tree = exceptions.ErrorTree(errors)\n    self.assertEqual(tree.total_errors, 8)\n", "label": 0}
{"function": "\n\ndef test_get_lock_multiple_coords(self):\n    member_id2 = self._get_random_uuid()\n    client2 = tooz.coordination.get_coordinator(self.url, member_id2)\n    client2.start()\n    lock_name = self._get_random_uuid()\n    lock = self._coord.get_lock(lock_name)\n    self.assertTrue(lock.acquire())\n    lock2 = client2.get_lock(lock_name)\n    self.assertFalse(lock2.acquire(blocking=False))\n    self.assertTrue(lock.release())\n    self.assertTrue(lock2.acquire(blocking=True))\n    self.assertTrue(lock2.release())\n", "label": 0}
{"function": "\n\ndef test_keys(self):\n    getkeys = self.ts.keys\n    self.assertIs(getkeys(), self.ts.index)\n", "label": 0}
{"function": "\n\ndef user_add_stage(request):\n    if (not request.user.has_perm('auth.change_user')):\n        raise PermissionDenied\n    manipulator = UserCreationForm()\n    if (request.method == 'POST'):\n        new_data = request.POST.copy()\n        errors = manipulator.get_validation_errors(new_data)\n        if (not errors):\n            new_user = manipulator.save(new_data)\n            msg = (_('The %(name)s \"%(obj)s\" was added successfully.') % {\n                'name': 'user',\n                'obj': new_user,\n            })\n            if request.POST.has_key('_addanother'):\n                request.user.message_set.create(message=msg)\n                return HttpResponseRedirect(request.path)\n            else:\n                request.user.message_set.create(message=((msg + ' ') + _('You may edit it again below.')))\n                return HttpResponseRedirect(('../%s/' % new_user.id))\n    else:\n        errors = new_data = {\n            \n        }\n    form = oldforms.FormWrapper(manipulator, new_data, errors)\n    return render_to_response('admin/auth/user/add_form.html', {\n        'title': _('Add user'),\n        'form': form,\n        'is_popup': request.REQUEST.has_key('_popup'),\n        'add': True,\n        'change': False,\n        'has_delete_permission': False,\n        'has_change_permission': True,\n        'has_file_field': False,\n        'has_absolute_url': False,\n        'auto_populated_fields': (),\n        'bound_field_sets': (),\n        'first_form_field_id': 'id_username',\n        'opts': User._meta,\n        'username_help_text': User._meta.get_field('username').help_text,\n    }, context_instance=template.RequestContext(request))\n", "label": 0}
{"function": "\n\ndef generateDictOperationInCode(to_name, expression, emit, context):\n    inverted = expression.isExpressionDictOperationNOTIn()\n    (dict_name, key_name) = generateChildExpressionsCode(expression=expression, emit=emit, context=context)\n    res_name = context.getIntResName()\n    emit(('%s = PyDict_Contains( %s, %s );' % (res_name, key_name, dict_name)))\n    getReleaseCodes(release_names=(dict_name, key_name), emit=emit, context=context)\n    getErrorExitBoolCode(condition=('%s == -1' % res_name), needs_check=expression.mayRaiseException(BaseException), emit=emit, context=context)\n    emit(('%s = BOOL_FROM( %s == %s );' % (to_name, res_name, ('1' if (not inverted) else '0'))))\n", "label": 0}
{"function": "\n\ndef Add(self, node):\n    self.binary('Add', node)\n", "label": 0}
{"function": "\n\ndef random_orthogonal(dim, special=True):\n    if (dim == 1):\n        if (np.random.uniform() < 0.5):\n            return np.ones((1, 1))\n        return (- np.ones((1, 1)))\n    P = np.random.randn(dim, dim)\n    while (np.linalg.matrix_rank(P) != dim):\n        P = np.random.randn(dim, dim)\n    (U, S, V) = np.linalg.svd(P)\n    P = np.dot(U, V)\n    if special:\n        if (np.linalg.det(P) < 0):\n            P[:, [0, 1]] = P[:, [1, 0]]\n    return P\n", "label": 1}
{"function": "\n\ndef register(self, observer):\n    ' Called when an observer wants to be notified\\n        about project changes\\n\\n        '\n    self._observers.append(observer)\n", "label": 0}
{"function": "\n\n@feature('cxx')\n@after('apply_lib_vars')\ndef apply_defines_cxx(self):\n    'after uselib is set for CXXDEFINES'\n    self.defines = getattr(self, 'defines', [])\n    lst = (self.to_list(self.defines) + self.to_list(self.env['CXXDEFINES']))\n    milst = []\n    for defi in lst:\n        if (not (defi in milst)):\n            milst.append(defi)\n    libs = self.to_list(self.uselib)\n    for l in libs:\n        val = self.env[('CXXDEFINES_' + l)]\n        if val:\n            milst += self.to_list(val)\n    self.env['DEFLINES'] = [('%s %s' % (x[0], Utils.trimquotes('='.join(x[1:])))) for x in [y.split('=') for y in milst]]\n    y = self.env['CXXDEFINES_ST']\n    self.env['_CXXDEFFLAGS'] = [(y % x) for x in milst]\n", "label": 1}
{"function": "\n\ndef __iadd__(self, other):\n    self.extend(other)\n    return self\n", "label": 0}
{"function": "\n\n@classmethod\ndef __subclasshook__(cls, other_cls):\n    if (cls is Tombola):\n        interface_names = function_names(cls)\n        found_names = set()\n        for a_cls in other_cls.__mro__:\n            found_names |= function_names(a_cls)\n        if (found_names >= interface_names):\n            return True\n    return NotImplemented\n", "label": 0}
{"function": "\n\ndef destroy(self):\n    ' Destroy the dock manager.\\n\\n        This method will free all of the resources held by the dock\\n        manager. The primary dock area and dock items will not be\\n        destroyed. After the method is called, the dock manager is\\n        invalid and should no longer be used.\\n\\n        '\n    for frame in self._dock_frames:\n        if isinstance(frame, QDockContainer):\n            frame.setDockItem(None)\n            frame.setParent(None, Qt.Widget)\n            frame.hide()\n    for frame in self._dock_frames:\n        if isinstance(frame, QDockWindow):\n            frame.setParent(None, Qt.Widget)\n            frame.hide()\n    for item in self._dock_items:\n        item._manager = None\n    self._dock_area.setCentralWidget(None)\n    self._dock_area.setMaximizedWidget(None)\n    del self._dock_area\n    del self._dock_frames\n    del self._dock_items\n    del self._proximity_handler\n    del self._container_monitor\n    del self._overlay\n", "label": 1}
{"function": "\n\ndef test_incr_sample_rate(self):\n    client = statsd.StatsdClient('localhost', 8125, prefix='', sample_rate=0.999)\n    client.incr('buck.counter', 5)\n    self.assertEqual(client._socket.data, b'buck.counter:5|c|@0.999')\n    if (client._socket.data != 'buck.counter:5|c'):\n        self.assertTrue(client._socket.data.endswith(b'|@0.999'))\n", "label": 0}
{"function": "\n\ndef format_author(self, entry):\n    try:\n        persons = entry.persons['author']\n        if (sys.version_info[0] == 2):\n            authors = [unicode(au) for au in persons]\n        elif (sys.version_info[0] == 3):\n            authors = [str(au) for au in persons]\n    except KeyError:\n        authors = ['']\n    authors = self.strip_chars('; '.join(authors))\n    return authors\n", "label": 1}
{"function": "\n\ndef test_delete_group_inuse_process(self):\n    url = ('/v1/groups/' + GID)\n    req = get_request(url, 'DELETE')\n    self.stubs.Set(db, 'keypair_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'securitygroup_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'network_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'process_get_all', fake_not_group_data_exists)\n    res = req.get_response(self.app)\n    self.assertEqual(res.status_code, 409)\n", "label": 0}
{"function": "\n\ndef synchro_connect(self):\n    try:\n        self.synchronize(self.delegate.open)()\n    except AutoReconnect as e:\n        raise ConnectionFailure(str(e))\n", "label": 0}
{"function": "\n\ndef test_validate_configuration_invalid_disk_type(self):\n    raid_config = json.loads(raid_constants.RAID_CONFIG_INVALID_DISK_TYPE)\n    self.assertRaises(exception.InvalidParameterValue, raid.validate_configuration, raid_config, raid_config_schema=self.schema)\n", "label": 0}
{"function": "\n\ndef test_notification_no_pause(self):\n    self.displayer.notification('message', 10)\n    string = self.mock_stdout.write.call_args[0][0]\n    self.assertTrue(('message' in string))\n", "label": 0}
{"function": "\n\ndef _augmented_orthonormal_cols(x, k):\n    (n, m) = x.shape\n    y = np.empty((n, (m + k)), dtype=x.dtype)\n    y[:, :m] = x\n    for i in range(k):\n        v = np.random.randn(n)\n        if np.iscomplexobj(x):\n            v = (v + (1j * np.random.randn(n)))\n        for j in range((m + i)):\n            u = y[:, j]\n            v -= ((np.dot(v, u.conj()) / np.dot(u, u.conj())) * u)\n        v /= np.sqrt(np.dot(v, v.conj()))\n        y[:, (m + i)] = v\n    return y\n", "label": 0}
{"function": "\n\ndef test_call_and_missing_check_with_obj_list(self):\n\n    def yield_hashes(device, partition, policy, suffixes=None, **kwargs):\n        if ((device == 'dev') and (partition == '9') and (suffixes == ['abc']) and (policy == POLICIES.legacy)):\n            (yield ('/srv/node/dev/objects/9/abc/9d41d8cd98f00b204e9800998ecf0abc', '9d41d8cd98f00b204e9800998ecf0abc', {\n                'ts_data': Timestamp(1380144470.0),\n            }))\n        else:\n            raise Exception(('No match for %r %r %r' % (device, partition, suffixes)))\n    job = {\n        'device': 'dev',\n        'partition': '9',\n        'policy': POLICIES.legacy,\n        'frag_index': 0,\n    }\n    self.sender = ssync_sender.Sender(self.daemon, None, job, ['abc'], ['9d41d8cd98f00b204e9800998ecf0abc'])\n    self.sender.connection = FakeConnection()\n    self.sender.response = FakeResponse(chunk_body=':MISSING_CHECK: START\\r\\n:MISSING_CHECK: END\\r\\n')\n    self.sender.daemon._diskfile_mgr.yield_hashes = yield_hashes\n    self.sender.connect = mock.MagicMock()\n    self.sender.updates = mock.MagicMock()\n    self.sender.disconnect = mock.MagicMock()\n    (success, candidates) = self.sender()\n    self.assertTrue(success)\n    self.assertEqual(candidates, dict([('9d41d8cd98f00b204e9800998ecf0abc', {\n        'ts_data': Timestamp(1380144470.0),\n    })]))\n    self.assertEqual(self.sender.failures, 0)\n", "label": 0}
{"function": "\n\ndef test_try_failure_bad_arg(self):\n    rv = self.app.get('/trivial_fn?nothing=1')\n    assert (rv.status_code == 200)\n    data = rv.data.decode('utf8')\n    jsn = json.loads(data)\n    assert (jsn['success'] == False), 'We expect this call failed as it has the wrong argument'\n", "label": 0}
{"function": "\n\ndef test_queryset_deleted_on(self):\n    'qs delete() sets deleted_on to same time as parent on cascade.'\n    p = self.F.ProductFactory.create()\n    s = self.F.SuiteFactory.create(product=p)\n    self.model.Product.objects.all().delete()\n    p = self.refresh(p)\n    s = self.refresh(s)\n    self.assertIsNot(p.deleted_on, None)\n    self.assertEqual(s.deleted_on, p.deleted_on)\n", "label": 0}
{"function": "\n\ndef __init__(self, status):\n    super(RCException, self).__init__(('RAMCloud error ' + str(status)))\n    self.status = status\n", "label": 0}
{"function": "\n\ndef test_delete_all_lines_inversed(self):\n    text = '1\\n22\\n3\\n44\\n5\\n66\\n'\n    self.fillAndClear(text)\n    self.buffer.delete(Range(6, 1))\n    assert (str(self.buffer) == '')\n    assert (self.buffer.lines == [])\n    assert (self.deleted('afterPosition') == Position(1, 1))\n    assert (self.deleted('startPosition') == Position(7, 1))\n", "label": 0}
{"function": "\n\ndef run_osprey(self, config):\n    '\\n        Run osprey-worker.\\n\\n        Parameters\\n        ----------\\n        config : str\\n            Configuration string.\\n        '\n    (fh, filename) = tempfile.mkstemp(dir=self.temp_dir)\n    with open(filename, 'wb') as f:\n        f.write(config)\n    args = Namespace(config=filename, n_iters=1, output='json')\n    execute_worker.execute(args, None)\n    dump = json.loads(execute_dump.execute(args, None))\n    assert (len(dump) == 1)\n    assert (dump[0]['status'] == 'SUCCEEDED'), dump[0]['status']\n", "label": 0}
{"function": "\n\ndef from_text(cls, rdclass, rdtype, tok, origin=None, relativize=True):\n    address = tok.get_string()\n    tok.get_eol()\n    return cls(rdclass, rdtype, address)\n", "label": 0}
{"function": "\n\ndef update_parser_common(self, parser):\n    parser.add_argument('network', metavar='<network>', nargs='+', help='Network(s) to delete (name or ID)')\n    return parser\n", "label": 0}
{"function": "\n\n@process_multiple\ndef to_html(self, values, fields, context):\n    toks = []\n    for value in values:\n        if (value in self.html_map):\n            tok = self.html_map[value]\n        elif (value is None):\n            continue\n        elif (type(value) is float):\n            tok = filters.floatformat(value)\n        else:\n            tok = unicode(value)\n        toks.append(tok)\n    return self.delimiter.join(toks)\n", "label": 0}
{"function": "\n\ndef test_dependency_sorting_4(self):\n    sorted_deps = sort_dependencies([('fixtures_regress', [Store, Person, Book])])\n    self.assertEqual(sorted_deps, [Store, Person, Book])\n", "label": 0}
{"function": "\n\ndef _eval_rewrite_as_gamma(self, x, k):\n    from sympy import gamma\n    return ((((- 1) ** k) * gamma((k - x))) / gamma((- x)))\n", "label": 0}
{"function": "\n\ndef _get_x(self):\n    if (len(self.names) > 1):\n        return ([self.__getattribute__(name) for name in self.names] + list(self.args))\n    return ([self.__getattribute__(self.names[0])] + list(self.args))\n", "label": 0}
{"function": "\n\ndef turn_off(self, **kwargs):\n    'Turn the device off/open the device.'\n    self.action_node.runElse()\n", "label": 0}
{"function": "\n\ndef test_set_rewrite(self):\n    '`LocalMemStorage` set method of existing key'\n    s = LocalMemStorage()\n    s.set('key', 'value')\n    s.set('key', 'value1')\n    self.assertEqual(s.storage['key'], 'value1')\n", "label": 0}
{"function": "\n\ndef do_create(self, max_size=0, dir=None, pre='', suf=''):\n    if (dir is None):\n        dir = tempfile.gettempdir()\n    file = tempfile.SpooledTemporaryFile(max_size=max_size, dir=dir, prefix=pre, suffix=suf)\n    return file\n", "label": 0}
{"function": "\n\ndef mapToJson(self, objects, writer):\n    writer.write(self.header)\n    writer.write('\\n')\n    for (ind, obj) in enumerate(objects):\n        if (ind > 0):\n            writer.write(',\\n')\n        else:\n            writer.write('\\n')\n        writer.write(self.jsonDumpser(self.objConverter(obj)))\n    writer.write(self.footer)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (self is other):\n        return True\n    if (not isinstance(other, collections.Sequence)):\n        raise TypeError('Can only compare repeated scalar fields against sequences.')\n    return (other == self[slice(None, None, None)])\n", "label": 0}
{"function": "\n\ndef test_download_file_proxies_to_transfer_object(self):\n    with mock.patch('boto3.s3.inject.S3Transfer') as transfer:\n        inject.download_file(mock.sentinel.CLIENT, Bucket='bucket', Key='key', Filename='filename')\n        transfer.return_value.download_file.assert_called_with(bucket='bucket', key='key', filename='filename', extra_args=None, callback=None)\n", "label": 0}
{"function": "\n\ndef RemoveMenu(self, menu):\n    ' Remove a wx menu from the Menu.\\n\\n        If the menu does not exist in the menu, this is a no-op.\\n\\n        Parameters\\n        ----------\\n        menu : wxMenu\\n            The wxMenu instance to remove from this menu.\\n\\n        '\n    all_items = self._all_items\n    if (menu in all_items):\n        all_items.remove(menu)\n        menu.Unbind(EVT_MENU_CHANGED, handler=self.OnMenuChanged)\n        menu_item = self._menus_map.pop(menu, None)\n        if (menu_item is not None):\n            self.RemoveItem(menu_item)\n            menu_item.SetSubMenu(None)\n", "label": 0}
{"function": "\n\ndef get_latest_dist():\n    lib = file(os.path.join('petlib', '__init__.py')).read()\n    v = re.findall('VERSION.*=.*[\\'\"](.*)[\\'\"]', lib)[0]\n    return os.path.join('dist', ('petlib-%s.tar.gz' % v))\n", "label": 0}
{"function": "\n\ndef dump(self):\n    out = []\n    for key in self._keys:\n        att_key = self._att_key(key)\n        value = self[att_key]\n        if hasattr(self, ('dump_%s' % att_key)):\n            value = getattr(self, ('dump_%s' % att_key))(value)\n        out.append(('%s: %s' % (key, value)))\n    return '\\n'.join(out)\n", "label": 0}
{"function": "\n\ndef get(self, request):\n    form = bforms.PasswordResetForm()\n    self.payload['form'] = form\n    return render(request, self.payload, 'registration/reset_password.html')\n", "label": 0}
{"function": "\n\ndef test_no_repeats(self):\n    with self.assertNumQueries(2):\n        authors = Author.objects.sql_calc_found_rows().sql_calc_found_rows()[:5]\n        list(authors)\n        assert (authors.found_rows == 10)\n", "label": 0}
{"function": "\n\ndef test_default_theme_is_empty(self):\n    doc = Document()\n    for (class_name, props) in doc.theme._json['attrs'].items():\n        self._compare_dict_to_model_defaults(props, class_name)\n    self.assertEqual(0, len(doc.theme._json['attrs']))\n    self._compare_dict_to_model_class_defaults(doc.theme._fill_defaults, FillProps)\n    self.assertEqual(0, len(doc.theme._fill_defaults))\n    self._compare_dict_to_model_class_defaults(doc.theme._text_defaults, TextProps)\n    self.assertEqual(0, len(doc.theme._text_defaults))\n    self._compare_dict_to_model_class_defaults(doc.theme._line_defaults, LineProps)\n    self.assertEqual(0, len(doc.theme._line_defaults))\n", "label": 0}
{"function": "\n\ndef test_reindex():\n    s = pd.Series([0.5, 1.0, 1.5], index=[2, 1, 3])\n    s2 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n    assert (list(reindex(s, s2).values) == [1.0, 0.5, 1.5])\n", "label": 0}
{"function": "\n\ndef default(self, obj):\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    return super(_JSONEncoder, self).default(obj)\n", "label": 0}
{"function": "\n\ndef _is_numeric(self, value):\n    try:\n        int(value)\n    except (TypeError, ValueError):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef set_service_target(context, policy_target_id, relationship):\n    session = context.session\n    with session.begin(subtransactions=True):\n        owner = ServiceTarget(policy_target_id=policy_target_id, servicechain_instance_id=context.instance['id'], servicechain_node_id=context.current_node['id'], position=context.current_position, relationship=relationship)\n        session.add(owner)\n", "label": 0}
{"function": "\n\ndef __init__(self, r, color4):\n    super(ProbeQuad, self).__init__()\n    self.color4 = color4\n    self.vertexes = [(r, 0, 0), (0, r, 0), ((- r), 0, 0), (0, (- r), 0)]\n", "label": 0}
{"function": "\n\ndef format(self, value):\n    if isinstance(value, types.StringTypes):\n        return value\n    else:\n        return str(value)\n", "label": 0}
{"function": "\n\ndef find_item_before(menu, index=0):\n    _items = menu['menu'][:index][:]\n    _items.reverse()\n    for item in _items:\n        if item['enabled']:\n            return menu['menu'].index(item)\n    return find_item_before(menu, index=len(menu['menu']))\n", "label": 0}
{"function": "\n\ndef onWindowResized(self, width, height):\n    shortcutHeight = ((height - self.shortcuts.getAbsoluteTop()) - 8)\n    if (shortcutHeight < 1):\n        shortcutHeight = 1\n    self.shortcuts.setHeight(('%dpx' % shortcutHeight))\n    self.mailDetail.adjustSize(width, height)\n", "label": 0}
{"function": "\n\ndef test_ex_get_node_security_groups(self):\n    node = Node(id='1c01300f-ef97-4937-8f03-ac676d6234be', name=None, state=None, public_ips=None, private_ips=None, driver=self.driver)\n    security_groups = self.driver.ex_get_node_security_groups(node)\n    self.assertEqual(len(security_groups), 2, 'Wrong security groups count')\n    security_group = security_groups[1]\n    self.assertEqual(security_group.id, 4)\n    self.assertEqual(security_group.tenant_id, '68')\n    self.assertEqual(security_group.name, 'ftp')\n    self.assertEqual(security_group.description, 'FTP Client-Server - Open 20-21 ports')\n    self.assertEqual(security_group.rules[0].id, 1)\n    self.assertEqual(security_group.rules[0].parent_group_id, 4)\n    self.assertEqual(security_group.rules[0].ip_protocol, 'tcp')\n    self.assertEqual(security_group.rules[0].from_port, 20)\n    self.assertEqual(security_group.rules[0].to_port, 21)\n    self.assertEqual(security_group.rules[0].ip_range, '0.0.0.0/0')\n", "label": 0}
{"function": "\n\ndef test_call_chooses_correct_handler(self):\n    (sentinel1, sentinel2, sentinel3) = (object(), object(), object())\n    self.commands.add('foo')((lambda context: sentinel1))\n    self.commands.add('bar')((lambda context: sentinel2))\n    self.commands.add('baz')((lambda context: sentinel3))\n    self.assertEqual(sentinel1, self.commands.call(['foo']))\n    self.assertEqual(sentinel2, self.commands.call(['bar']))\n    self.assertEqual(sentinel3, self.commands.call(['baz']))\n", "label": 0}
{"function": "\n\ndef apply_linear(self, params, unknowns, dparams, dunknowns, dresids, mode):\n    \"\\n        Multiplies incoming vector by the Jacobian (fwd mode) or the\\n        transpose Jacobian (rev mode). If the user doesn't provide this\\n        method, then we just multiply by the cached jacobian.\\n\\n        Args\\n        ----\\n        params : `VecWrapper`\\n            `VecWrapper` containing parameters. (p)\\n\\n        unknowns : `VecWrapper`\\n            `VecWrapper` containing outputs and states. (u)\\n\\n        dparams : `VecWrapper`\\n            `VecWrapper` containing either the incoming vector in forward mode\\n            or the outgoing result in reverse mode. (dp)\\n\\n        dunknowns : `VecWrapper`\\n            In forward mode, this `VecWrapper` contains the incoming vector for\\n            the states. In reverse mode, it contains the outgoing vector for\\n            the states. (du)\\n\\n        dresids : `VecWrapper`\\n            `VecWrapper` containing either the outgoing result in forward mode\\n            or the incoming vector in reverse mode. (dr)\\n\\n        mode : string\\n            Derivative mode, can be 'fwd' or 'rev'.\\n        \"\n    self._apply_linear_jac(params, unknowns, dparams, dunknowns, dresids, mode)\n", "label": 0}
{"function": "\n\ndef test_context(self):\n    order = Order(name='Dummy Order')\n    order.save()\n    for i in range(10):\n        item = Item(name=('Item %i' % i), sku=(str(i) * 13), price=D('9.99'), order=order, status=0)\n        item.save()\n    res = self.client.get('/modelformset/simple/')\n    self.assertTrue(('object_list' in res.context))\n    self.assertEqual(len(res.context['object_list']), 10)\n", "label": 0}
{"function": "\n\ndef read_channel(model, channel_name, monitor_name='monitor'):\n    '\\n    Returns the last value recorded in a channel.\\n\\n    Parameters\\n    ----------\\n    model : Model\\n        The model to read the channel from\\n    channel_name : str\\n        The name of the channel to read from\\n    monitor_name : str, optional\\n        The name of the Monitor to read from\\n        (In case you want to read from an old Monitor moved by\\n        `push_monitor`)\\n\\n    Returns\\n    -------\\n    value : float\\n        The last value recorded in this monitoring channel\\n    '\n    return getattr(model, monitor_name).channels[channel_name].val_record[(- 1)]\n", "label": 0}
{"function": "\n\ndef test_add_listener_exception(self):\n    cap = [':candidate']\n    obj = Session(cap)\n    listener = Session(None)\n    with self.assertRaises(SessionError):\n        obj.add_listener(listener)\n", "label": 0}
{"function": "\n\ndef set_h_ffactor(self, *args, **kwargs):\n    return apply(self._cobj.set_h_ffactor, args, kwargs)\n", "label": 0}
{"function": "\n\ndef init_stroke(self, g, touch):\n    l = [touch.x, touch.y]\n    col = g.color\n    new_line = Line(points=l, width=self.line_width, group=g.id)\n    g._strokes[str(touch.uid)] = new_line\n    if self.line_width:\n        canvas_add = self.canvas.add\n        canvas_add(Color(col[0], col[1], col[2], mode='rgb', group=g.id))\n        canvas_add(new_line)\n    g.update_bbox(touch)\n    if self.draw_bbox:\n        self._update_canvas_bbox(g)\n    g.add_stroke(touch, new_line)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/item/shared_item_music_microphone_s2.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef test_scan_clear_product(self):\n    with HTTMock(wechat_api_mock):\n        res = self.client.scan.clear_product('ean13', '6900873042720')\n    self.assertEqual(0, res['errcode'])\n", "label": 0}
{"function": "\n\ndef __init__(self, dateTime, frequency):\n    super(IntraDayRange, self).__init__()\n    assert isinstance(frequency, int)\n    assert (frequency > 1)\n    assert (frequency < bar.Frequency.DAY)\n    ts = int(dt.datetime_to_timestamp(dateTime))\n    slot = int((ts / frequency))\n    slotTs = (slot * frequency)\n    self.__begin = dt.timestamp_to_datetime(slotTs, (not dt.datetime_is_naive(dateTime)))\n    if (not dt.datetime_is_naive(dateTime)):\n        self.__begin = dt.localize(self.__begin, dateTime.tzinfo)\n    self.__end = (self.__begin + datetime.timedelta(seconds=frequency))\n", "label": 0}
{"function": "\n\ndef autodiscover():\n    'Auto-discover INSTALLED_APPS mails.py modules.'\n    for app in settings.INSTALLED_APPS:\n        module = ('%s.mails' % app)\n        try:\n            import_module(module)\n        except:\n            app_module = import_module(app)\n            if module_has_submodule(app_module, 'mails'):\n                raise\n", "label": 0}
{"function": "\n\ndef setup_basic_delete_test(self, user, with_local_site, local_site_name):\n    review_request = self.create_review_request(with_local_site=with_local_site, publish=True)\n    profile = user.get_profile()\n    profile.starred_review_requests.add(review_request)\n    return (get_watched_review_request_item_url(user.username, review_request.display_id, local_site_name), [profile, review_request])\n", "label": 0}
{"function": "\n\ndef unregister_module(self, module):\n    if (module not in self.modules):\n        raise NotRegistered(('The module %s is not registered' % module.__name__))\n    del self.modules[module]\n", "label": 0}
{"function": "\n\ndef sh(cmdline, stdout=subprocess.PIPE, stderr=subprocess.PIPE):\n    'run cmd in a subprocess and return its output.\\n    raises RuntimeError on error.\\n    '\n    p = subprocess.Popen(cmdline, shell=True, stdout=stdout, stderr=stderr)\n    (stdout, stderr) = p.communicate()\n    if (p.returncode != 0):\n        raise RuntimeError(stderr)\n    if stderr:\n        warn(stderr)\n    if PY3:\n        stdout = str(stdout, sys.stdout.encoding)\n    return stdout.strip()\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.content_type == other.content_type) and (_join_b(self.iter_bytes()) == _join_b(other.iter_bytes())))\n", "label": 0}
{"function": "\n\ndef _libname(self, libpath):\n    \"Converts a full library filepath to the library's name.\\n    Ex: /path/to/libhello.a --> hello\\n    \"\n    return os.path.basename(libpath)[3:(- 2)]\n", "label": 0}
{"function": "\n\ndef test_deprecated_simple(self):\n\n    @deprecated()\n    def f(arg):\n        return arg\n    ARG = object()\n    with warnings.catch_warnings(record=True) as recorded:\n        returned = f(ARG)\n    self.assertIs(returned, ARG)\n    self.assertEqual(len(recorded), 1)\n", "label": 0}
{"function": "\n\ndef description(self, around=False):\n    if around:\n        return 'Expand Selection to Quotes'\n    else:\n        return 'Expand Selection to Quoted'\n", "label": 0}
{"function": "\n\ndef get_form(self, request, obj=None, **kwargs):\n    _thread_locals.request = request\n    _thread_locals.obj = obj\n    return super(XOSAdminMixin, self).get_form(request, obj, **kwargs)\n", "label": 0}
{"function": "\n\ndef input(self, data):\n    if ('type' in data):\n        function_name = ('process_' + data['type'])\n        self._dbg('got {}'.format(function_name))\n        for plugin in self.bot_plugins:\n            plugin.register_jobs()\n            plugin.do(function_name, data)\n", "label": 0}
{"function": "\n\ndef db_exists(database_name, **kwargs):\n    \"\\n    Find if a specific database exists on the MS SQL server.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt minion mssql.db_exists database_name='DBNAME'\\n    \"\n    return (len(tsql_query(\"SELECT database_id FROM sys.databases WHERE NAME='{0}'\".format(database_name), **kwargs)) == 1)\n", "label": 0}
{"function": "\n\ndef load_proposal_rlp(self, blockhash):\n    try:\n        prlp = self.chainservice.db.get(('blockproposal:%s' % blockhash))\n        assert isinstance(prlp, bytes)\n        return prlp\n    except KeyError:\n        return None\n", "label": 0}
{"function": "\n\ndef test_sort_mapping_reverse(self):\n    stream = DataStream(IterableDataset(self.data))\n    transformer = Mapping(stream, SortMapping(operator.itemgetter(0), reverse=True))\n    assert_equal(list(transformer.get_epoch_iterator()), list(zip(([[3, 2, 1]] * 3))))\n", "label": 0}
{"function": "\n\ndef load_train_data(self, input_data_file=''):\n    '\\n        Load train data\\n        Please check dataset/logistic_regression_train.dat to understand the data format\\n        Each feature of data x separated with spaces\\n        And the ground truth y put in the end of line separated by a space\\n        '\n    self.status = 'load_train_data'\n    if (input_data_file == ''):\n        input_data_file = os.path.normpath(os.path.join(os.path.join(os.getcwd(), os.path.dirname(__file__)), 'dataset/logistic_regression_train.dat'))\n    elif (os.path.isfile(input_data_file) is not True):\n        print('Please make sure input_data_file path is correct.')\n        return (self.train_X, self.train_Y)\n    (self.train_X, self.train_Y) = utility.DatasetLoader.load(input_data_file)\n    return (self.train_X, self.train_Y)\n", "label": 0}
{"function": "\n\ndef test_realtime_with_batch_computation(self):\n    with self._get_swap_context():\n        user_id = 'uid'\n        exp_id = 'eid'\n        self.save_new_valid_exploration(exp_id, 'owner')\n        feedback_services.create_thread(exp_id, 'a_state_name', None, 'a subject', 'some text')\n        self.process_and_flush_pending_tasks()\n        ModifiedFeedbackAnalyticsAggregator.start_computation()\n        self.assertEqual(self.count_jobs_in_taskqueue(), 1)\n        self.process_and_flush_pending_tasks()\n        ModifiedFeedbackAnalyticsAggregator.stop_computation(user_id)\n        self.assertEqual(self.count_jobs_in_taskqueue(), 0)\n        self._flush_tasks_and_check_analytics(exp_id, {\n            'num_open_threads': 1,\n            'num_total_threads': 1,\n        })\n        feedback_services.create_thread(exp_id, 'a_state_name', None, 'a subject', 'some text')\n        self._flush_tasks_and_check_analytics(exp_id, {\n            'num_open_threads': 2,\n            'num_total_threads': 2,\n        })\n", "label": 0}
{"function": "\n\ndef register(self, parent, key, create_only=False, **kwargs):\n    '\\n        Add/replace an entry within directory, below a parent node or \"/\".\\n        Note: Replaces (not merges) the attribute values of the entry if existing\\n        @param create_only  If True, does not change an existing entry\\n        @retval  DirEntry if previously existing\\n        '\n    if (not (parent and key)):\n        raise BadRequest('Illegal arguments')\n    if ((not (type(parent) is str)) or (not parent.startswith('/'))):\n        raise BadRequest('Illegal arguments: parent')\n    dn = self._get_path(parent, key)\n    log.debug('Directory.register(%s): %s', dn, kwargs)\n    entry_old = None\n    cur_time = get_ion_ts()\n    direntry = self._read_by_path(dn)\n    if (direntry and create_only):\n        return direntry\n    elif direntry:\n        entry_old = direntry.attributes\n        direntry.attributes = kwargs\n        direntry.ts_updated = cur_time\n        self.dir_store.update(direntry)\n    else:\n        direntry = self._create_dir_entry(parent, key, attributes=kwargs, ts=cur_time)\n        self._ensure_parents_exist([direntry])\n        self.dir_store.create(direntry, create_unique_directory_id())\n    return entry_old\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    s = ('%s' % self._name)\n    if self._location:\n        s = ('%s@%s' % (s, self._location))\n    return s\n", "label": 0}
{"function": "\n\ndef test_dispose1():\n    h = event.HasEvents()\n\n    @h.connect('x1', 'x2')\n    def handler(*events):\n        pass\n    handler_ref = weakref.ref(handler)\n    del handler\n    gc.collect()\n    assert (handler_ref() is not None)\n    handler_ref().dispose()\n    gc.collect()\n    assert (handler_ref() is None)\n", "label": 0}
{"function": "\n\ndef test_add(self):\n    'Test that we can add an image via the s3 backend'\n    expected_image_id = utils.generate_uuid()\n    expected_s3_size = FIVE_KB\n    expected_s3_contents = ('*' * expected_s3_size)\n    expected_checksum = hashlib.md5(expected_s3_contents).hexdigest()\n    expected_location = format_s3_location(S3_CONF['s3_store_access_key'], S3_CONF['s3_store_secret_key'], S3_CONF['s3_store_host'], S3_CONF['s3_store_bucket'], expected_image_id)\n    image_s3 = StringIO.StringIO(expected_s3_contents)\n    (location, size, checksum) = self.store.add(expected_image_id, image_s3, expected_s3_size)\n    self.assertEquals(expected_location, location)\n    self.assertEquals(expected_s3_size, size)\n    self.assertEquals(expected_checksum, checksum)\n    loc = get_location_from_uri(expected_location)\n    (new_image_s3, new_image_size) = self.store.get(loc)\n    new_image_contents = StringIO.StringIO()\n    for chunk in new_image_s3:\n        new_image_contents.write(chunk)\n    new_image_s3_size = new_image_contents.len\n    self.assertEquals(expected_s3_contents, new_image_contents.getvalue())\n    self.assertEquals(expected_s3_size, new_image_s3_size)\n", "label": 0}
{"function": "\n\ndef move_cat(self):\n    speed = random.randint(20, 200)\n    self.cat_body.angle -= random.randint((- 1), 1)\n    direction = Vec2d(1, 0).rotated(self.cat_body.angle)\n    self.cat_body.velocity = (speed * direction)\n", "label": 0}
{"function": "\n\ndef job_status(self, job_id=None):\n    job_id = (job_id or self.lookup_job_id(batch_id))\n    uri = urlparse.urljoin((self.endpoint + '/'), 'job/{0}'.format(job_id))\n    response = requests.get(uri, headers=self.headers())\n    if (response.status_code != 200):\n        self.raise_error(response.content, response.status_code)\n    tree = ET.fromstring(response.content)\n    result = {\n        \n    }\n    for child in tree:\n        result[re.sub('{.*?}', '', child.tag)] = child.text\n    return result\n", "label": 0}
{"function": "\n\ndef test_issue(self):\n    'Show that one can retrieve the associated issue of a PR.'\n    cassette_name = self.cassette_name('issue')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        issue = p.issue()\n        assert isinstance(issue, github3.issues.Issue)\n", "label": 0}
{"function": "\n\ndef __init__(self, view=False):\n    self.view = view\n    if self.view:\n        self.view_map = {\n            0: [0],\n        }\n", "label": 0}
{"function": "\n\ndef replace_body(self, body, payload):\n    if (body is None):\n        return body\n    if (self.fsig in body):\n        return body.replace(self.fsig, urllib.quote_plus(payload))\n    template_sig = self.template_signature(body)\n    if template_sig:\n        tp = TemplateParser()\n        tp.set_payload(payload)\n        new_payload = repr(tp.transform(self.template_signature(body), self.sig))[1:(- 1)]\n        return body.replace(template_sig, new_payload)\n    return body\n", "label": 0}
{"function": "\n\ndef delete(self, image):\n    \"\\n        Delete an image.\\n        \\n        It should go without saying that you can't delete an image \\n        that you didn't create.\\n        \\n        :param image: The :class:`Image` (or its ID) to delete.\\n        \"\n    self._delete(('/images/%s' % base.getid(image)))\n", "label": 0}
{"function": "\n\ndef facettupletrees(table, key, start='start', stop='stop', value=None):\n    '\\n    Construct faceted interval trees for the given table, where each node in\\n    the tree is a row of the table.\\n\\n    '\n    import intervaltree\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    assert (start in flds), 'start field not recognised'\n    assert (stop in flds), 'stop field not recognised'\n    getstart = itemgetter(flds.index(start))\n    getstop = itemgetter(flds.index(stop))\n    if (value is None):\n        getvalue = tuple\n    else:\n        valueindices = asindices(hdr, value)\n        assert (len(valueindices) > 0), 'invalid value field specification'\n        getvalue = itemgetter(*valueindices)\n    keyindices = asindices(hdr, key)\n    assert (len(keyindices) > 0), 'invalid key'\n    getkey = itemgetter(*keyindices)\n    trees = dict()\n    for row in it:\n        k = getkey(row)\n        if (k not in trees):\n            trees[k] = intervaltree.IntervalTree()\n        trees[k].addi(getstart(row), getstop(row), getvalue(row))\n    return trees\n", "label": 1}
{"function": "\n\ndef load_parent(parent_id):\n    parent = Node.load(parent_id)\n    if (parent is None):\n        return None\n    parent_info = {\n        \n    }\n    if ((parent is not None) and parent.is_public):\n        parent_info['title'] = parent.title\n        parent_info['url'] = parent.url\n        parent_info['is_registration'] = parent.is_registration\n        parent_info['id'] = parent._id\n    else:\n        parent_info['title'] = '-- private project --'\n        parent_info['url'] = ''\n        parent_info['is_registration'] = None\n        parent_info['id'] = None\n    return parent_info\n", "label": 0}
{"function": "\n\ndef test_sys_stderr_should_have_no_output_when_no_logger_is_set(memcached):\n    mc = cmemcached.Client([memcached])\n    with patch('sys.stderr') as mock_stderr:\n        mc.get('test_key_with_no_logger')\n        mc.set('test_key_with_no_logger', 'test_value_with_no_logger')\n        assert (not mock_stderr.write.called)\n", "label": 0}
{"function": "\n\ndef write(self, filename, content):\n    create_dirs(self.webd, dirname(filename))\n    buff = BytesIO(content)\n    self.webd.upload_from(buff, b(filename))\n", "label": 0}
{"function": "\n\ndef test_gtkApplicationActivate(self):\n    '\\n        L{Gtk.Application} instances can be registered with a gtk3reactor.\\n        '\n    reactor = gtk3reactor.Gtk3Reactor()\n    self.addCleanup(self.unbuildReactor, reactor)\n    app = Gtk.Application(application_id='com.twistedmatrix.trial.gtk3reactor', flags=Gio.ApplicationFlags.FLAGS_NONE)\n    self.runReactor(app, reactor)\n", "label": 0}
{"function": "\n\ndef test_patch_mocksignature_callable(self):\n    original_something = something\n    something_name = ('%s.something' % __name__)\n\n    @patch(something_name, mocksignature=True)\n    def test(MockSomething):\n        something(3, 4)\n        MockSomething.assert_called_with(3, 4)\n        something(6)\n        MockSomething.assert_called_with(6, 5)\n        self.assertRaises(TypeError, something)\n    test()\n    self.assertIs(something, original_something)\n", "label": 0}
{"function": "\n\ndef __init__(self, allure_helper, title):\n    self.allure_helper = allure_helper\n    self.title = title\n    self.step = None\n", "label": 0}
{"function": "\n\ndef load_xml_config(self, path=None):\n    if (path is not None):\n        self.path = path\n    if (not os.path.isfile(self.path)):\n        raise KaresasnuiServiceConfigParamException(('service.xml not found. path=%s' % str(self.path)))\n    document = XMLParse(self.path)\n    self.services = []\n    service_num = XMLXpathNum(document, '/services/service')\n    for n in xrange(1, (service_num + 1)):\n        system_name = XMLXpath(document, ('/services/service[%i]/system/name/text()' % n))\n        system_command = XMLXpath(document, ('/services/service[%i]/system/command/text()' % n))\n        system_readonly = XMLXpath(document, ('/services/service[%i]/system/readonly/text()' % n))\n        display_name = XMLXpath(document, ('/services/service[%i]/display/name/text()' % n))\n        display_description = XMLXpath(document, ('/services/service[%i]/display/description/text()' % n))\n        self.add_service(str(system_name), str(system_command), str(system_readonly), str(display_name), str(display_description))\n", "label": 0}
{"function": "\n\ndef _should_create_constraint(self, compiler):\n    return (not compiler.dialect.supports_native_boolean)\n", "label": 0}
{"function": "\n\ndef put_object(self, name, fp, metadata):\n    '\\n        Store object into memory\\n\\n        :param name: standard object name\\n        :param fp: `StringIO` in-memory representation object\\n        :param metadata: dictionary of metadata to be written\\n        '\n    self._filesystem[name] = (fp, metadata)\n", "label": 0}
{"function": "\n\ndef _raise_test_exc(self, exc_msg):\n    raise TestException(exc_msg)\n", "label": 0}
{"function": "\n\ndef install_inplace(pkg):\n    'Install scripts of pkg in the current directory.'\n    for (basename, executable) in pkg.executables.items():\n        version_str = '.'.join([str(i) for i in sys.version_info[:2]])\n        scripts_node = root._ctx.srcnode\n        for name in [basename, ('%s-%s' % (basename, version_str))]:\n            nodes = _create_executable(name, executable, scripts_node)\n            installed = ','.join([n.path_from(scripts_node) for n in nodes])\n            pprint('GREEN', ('installing %s in current directory' % installed))\n", "label": 0}
{"function": "\n\ndef testSuccess(self):\n    vor = rapi.testutils.VerifyOpResult\n    vor(opcodes.OpClusterVerify.OP_ID, {\n        constants.JOB_IDS_KEY: [(False, 'error message')],\n    })\n", "label": 0}
{"function": "\n\ndef kilobyte(self, value=None):\n    return self.convertb(value, self.byte)\n", "label": 0}
{"function": "\n\ndef _irfft_out_chunks(a, n, axis):\n    if (n is None):\n        n = (2 * (a.chunks[axis][0] - 1))\n    chunks = list(a.chunks)\n    chunks[axis] = (n,)\n    return chunks\n", "label": 0}
{"function": "\n\n@skipUnless(cache_file_exists('ia', '20100608__ia__primary__adair__precinct.xls'), CACHED_FILE_MISSING_MSG)\ndef test_results(self):\n    filename = '20100608__ia__primary__adair__precinct.xls'\n    mapping = self._get_mapping(filename)\n    self._prep_loader_attrs(mapping)\n    results = self.loader._results(mapping)\n    us_rep_dist_5_rep_results = [r for r in results if ((r.office == 'U.S. REPRESENTATIVE') and (r.district == '5') and (r.primary_party == 'REPUBLICAN'))]\n    self.assertEqual(len(us_rep_dist_5_rep_results), 35)\n    result = us_rep_dist_5_rep_results[0]\n    self.assertEqual(result.source, mapping['generated_filename'])\n    self.assertEqual(result.election_id, mapping['election'])\n    self.assertEqual(result.state, 'IA')\n    self.assertEqual(result.election_type, 'primary')\n    self.assertEqual(result.district, '5')\n    self.assertEqual(result.party, 'REPUBLICAN')\n    self.assertEqual(result.jurisdiction, '1 NW')\n    self.assertEqual(result.reporting_level, 'precinct')\n    self.assertEqual(result.full_name, 'STEVE KING')\n    self.assertEqual(result.votes, 123)\n", "label": 0}
{"function": "\n\ndef get_context_data(self, **kwargs):\n    ctx = super(AddressCreateView, self).get_context_data(**kwargs)\n    ctx['title'] = _('Add a new address')\n    return ctx\n", "label": 0}
{"function": "\n\ndef find_by_selector(self, selector, search_regions=None):\n    search_regions = (search_regions or self.regions)\n    return GrammarParser.filter_by_selector(selector, search_regions)\n", "label": 0}
{"function": "\n\ndef nova(context):\n    global _nova_api_version\n    if (not _nova_api_version):\n        _nova_api_version = _get_nova_api_version(context)\n    clnt = novaclient.Client(_nova_api_version, session=context.session, service_type=CONF.nova_service_type)\n    if (not hasattr(clnt.client, 'last_request_id')):\n        setattr(clnt.client, 'last_request_id', None)\n    return clnt\n", "label": 0}
{"function": "\n\ndef iteritems(self):\n    for tag in self.tags:\n        (yield (tag.name, tag))\n", "label": 0}
{"function": "\n\ndef split_multiline(value):\n    value = [element for element in (line.strip() for line in value.split('\\n')) if element]\n    return value\n", "label": 0}
{"function": "\n\ndef get_tags_count(journal):\n    'Returns a set of tuples (count, tag) for all tags present in the journal.'\n    tags = [tag for entry in journal.entries for tag in set(entry.tags)]\n    tag_counts = set([(tags.count(tag), tag) for tag in tags])\n    return tag_counts\n", "label": 0}
{"function": "\n\ndef vmstats():\n    \"\\n    Return the virtual memory stats for this minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' status.vmstats\\n    \"\n\n    def linux_vmstats():\n        '\\n        linux specific implementation of vmstats\\n        '\n        procf = '/proc/vmstat'\n        if (not os.path.isfile(procf)):\n            return {\n                \n            }\n        stats = salt.utils.fopen(procf, 'r').read().splitlines()\n        ret = {\n            \n        }\n        for line in stats:\n            if (not line):\n                continue\n            comps = line.split()\n            ret[comps[0]] = _number(comps[1])\n        return ret\n\n    def freebsd_vmstats():\n        '\\n        freebsd specific implementation of vmstats\\n        '\n        ret = {\n            \n        }\n        for line in __salt__['cmd.run']('vmstat -s').splitlines():\n            comps = line.split()\n            if comps[0].isdigit():\n                ret[' '.join(comps[1:])] = _number(comps[0])\n        return ret\n    get_version = {\n        'Linux': linux_vmstats,\n        'FreeBSD': freebsd_vmstats,\n    }\n    errmsg = 'This method is unsupported on the current operating system!'\n    return get_version.get(__grains__['kernel'], (lambda : errmsg))()\n", "label": 0}
{"function": "\n\ndef test_get_children_duplicates(self):\n    from psutil._compat import defaultdict\n    table = defaultdict(int)\n    for p in psutil.process_iter():\n        try:\n            table[p.ppid] += 1\n        except psutil.Error:\n            pass\n    pid = max(sorted(table, key=(lambda x: table[x])))\n    p = psutil.Process(pid)\n    try:\n        c = p.get_children(recursive=True)\n    except psutil.AccessDenied:\n        pass\n    else:\n        self.assertEqual(len(c), len(set(c)))\n", "label": 0}
{"function": "\n\ndef __call__(self, driver):\n    try:\n        return _element_if_visible(_find_element(driver, self.locator))\n    except StaleElementReferenceException:\n        return False\n", "label": 0}
{"function": "\n\ndef _test_update_routing_table(self, is_snat_host=True):\n    router = l3_test_common.prepare_router_data()\n    uuid = router['id']\n    s_netns = ('snat-' + uuid)\n    q_netns = ('qrouter-' + uuid)\n    fake_route1 = {\n        'destination': '135.207.0.0/16',\n        'nexthop': '19.4.4.200',\n    }\n    calls = [mock.call('replace', fake_route1, q_netns)]\n    agent = l3_agent.L3NATAgent(HOSTNAME, self.conf)\n    ri = dvr_router.DvrEdgeRouter(agent, HOSTNAME, uuid, router, **self.ri_kwargs)\n    ri._update_routing_table = mock.Mock()\n    with mock.patch.object(ri, '_is_this_snat_host') as snat_host:\n        snat_host.return_value = is_snat_host\n        ri.update_routing_table('replace', fake_route1)\n        if is_snat_host:\n            ri._update_routing_table('replace', fake_route1, s_netns)\n            calls += [mock.call('replace', fake_route1, s_netns)]\n        ri._update_routing_table.assert_has_calls(calls, any_order=True)\n", "label": 0}
{"function": "\n\ndef install_ssl_certs(instances):\n    certs = []\n    if CONF.object_store_access.public_identity_ca_file:\n        certs.append(CONF.object_store_access.public_identity_ca_file)\n    if CONF.object_store_access.public_object_store_ca_file:\n        certs.append(CONF.object_store_access.public_object_store_ca_file)\n    if (not certs):\n        return\n    with context.ThreadGroup() as tg:\n        for inst in instances:\n            tg.spawn(('configure-ssl-cert-%s' % inst.instance_id), _install_ssl_certs, inst, certs)\n", "label": 0}
{"function": "\n\ndef test_node_site():\n    s = Site(TEST_SITE_ROOT)\n    r = RootNode(TEST_SITE_ROOT.child_folder('content'), s)\n    assert (r.site == s)\n    n = Node(r.source_folder.child_folder('blog'), r)\n    assert (n.site == s)\n", "label": 0}
{"function": "\n\ndef render(self, name, value, attrs=None):\n    value = util.serialize_references(value)\n    return super(ReferencesFieldWidget, self).render(name, value, attrs)\n", "label": 0}
{"function": "\n\ndef on_request(self, context, request):\n    if ('PowerView.ps1' == request.path[1:]):\n        request.send_response(200)\n        request.end_headers()\n        with open('data/PowerSploit/Recon/PowerView.ps1', 'r') as ps_script:\n            ps_script = obfs_ps_script(ps_script.read())\n            request.wfile.write(ps_script)\n    else:\n        request.send_response(404)\n        request.end_headers()\n", "label": 0}
{"function": "\n\ndef test_column_expr(self):\n    c = Column('x', Integer)\n    is_(inspect(c), c)\n    assert (not c.is_selectable)\n    assert (not hasattr(c, 'selectable'))\n", "label": 0}
{"function": "\n\ndef inline_assets(self, base_path, content):\n    for type in self.asset_types:\n        for (statement, path) in self.get_matches(type['pattern'], base_path, content):\n            asset_content = self.get_binary_file_contents(path)\n            encoded_content = urllib.quote(base64.encodestring(asset_content))\n            new_statement = ('url(data:%s;base64,%s)' % (type['mime'], encoded_content))\n            content = content.replace(statement, new_statement)\n    return content\n", "label": 0}
{"function": "\n\ndef test_choice_update(self):\n    self.choice.choice_text = 'third text'\n    self.choice.save()\n    p = Choice.objects.get()\n    self.assertEqual(p.choice_text, 'third text')\n", "label": 0}
{"function": "\n\ndef abort_run(self, drain=False):\n    self._aborting_run = drain\n", "label": 0}
{"function": "\n\ndef test_oldPythonPy3(self):\n    '\\n        L{_checkRequirements} raises L{ImportError} when run on a version of\\n        Python that is too old.\\n        '\n    sys.version_info = self.Py3unsupportedPythonVersion\n    with self.assertRaises(ImportError) as raised:\n        _checkRequirements()\n    self.assertEqual(('Twisted on Python 3 requires Python %d.%d or later.' % self.Py3supportedPythonVersion), str(raised.exception))\n", "label": 0}
{"function": "\n\ndef get_oauth_request(request):\n    ' Converts a Django request object into an `oauth2.Request` object. '\n    headers = {\n        \n    }\n    if ('HTTP_AUTHORIZATION' in request.META):\n        headers['Authorization'] = request.META['HTTP_AUTHORIZATION']\n    return oauth.Request.from_request(request.method, request.build_absolute_uri(request.path), headers, dict(request.REQUEST))\n", "label": 0}
{"function": "\n\ndef _apply_filters(self, query, count_query, joins, count_joins, filters):\n    for (idx, flt_name, value) in filters:\n        flt = self._filters[idx]\n        alias = None\n        count_alias = None\n        if isinstance(flt, sqla_filters.BaseSQLAFilter):\n            path = self._filter_joins.get(flt.column, [])\n            (query, joins, alias) = self._apply_path_joins(query, joins, path, inner_join=False)\n            if (count_query is not None):\n                (count_query, count_joins, count_alias) = self._apply_path_joins(count_query, count_joins, path, inner_join=False)\n        clean_value = flt.clean(value)\n        try:\n            query = flt.apply(query, clean_value, alias)\n        except TypeError:\n            spec = inspect.getargspec(flt.apply)\n            if (len(spec.args) == 3):\n                warnings.warn(('Please update your custom filter %s to include additional `alias` parameter.' % repr(flt)))\n            else:\n                raise\n            query = flt.apply(query, clean_value)\n        if (count_query is not None):\n            try:\n                count_query = flt.apply(count_query, clean_value, count_alias)\n            except TypeError:\n                count_query = flt.apply(count_query, clean_value)\n    return (query, count_query, joins, count_joins)\n", "label": 1}
{"function": "\n\n@property\ndef vcf(self):\n    'serialize to VCARD as specified in RFC2426,\\n        if no UID is specified yet, one will be added (as a UID is mandatory\\n        for carddav as specified in RFC6352\\n        TODO make shure this random uid is unique'\n    import string\n    import random\n\n    def generate_random_uid():\n        \"generate a random uid, when random isn't broken, getting a\\n            random UID from a pool of roughly 10^56 should be good enough\"\n        choice = (string.ascii_uppercase + string.digits)\n        return ''.join([random.choice(choice) for _ in range(36)])\n    if ('UID' not in self.keys()):\n        self['UID'] = [(generate_random_uid(), dict())]\n    collector = list()\n    collector.append('BEGIN:VCARD')\n    collector.append('VERSION:3.0')\n    for key in ['FN', 'N']:\n        try:\n            collector.append(((key + ':') + self[key][0][0]))\n        except IndexError:\n            collector.append((key + ':'))\n    for prop in self.alt_keys():\n        for line in self[prop]:\n            types = self._line_helper(line)\n            collector.append((((prop + types) + ':') + line[0]))\n    collector.append('END:VCARD')\n    return '\\n'.join(collector)\n", "label": 1}
{"function": "\n\ndef close_review_request(server_url, username, password, review_request_id, description):\n    'Closes the specified review request as submitted.'\n    (api_client, api_root) = get_api(server_url, username, password)\n    review_request = get_review_request(review_request_id, api_root)\n    if (review_request.status == SUBMITTED):\n        logging.warning('Review request #%s is already %s.', review_request_id, SUBMITTED)\n        return\n    if description:\n        review_request = review_request.update(status=SUBMITTED, description=description)\n    else:\n        review_request = review_request.update(status=SUBMITTED)\n    print(('Review request #%s is set to %s.' % (review_request_id, review_request.status)))\n", "label": 0}
{"function": "\n\ndef GetLastRequestTimedelta(api_query, from_time=None):\n    'Returns how long since the API Query response was last requested.\\n\\n  Args:\\n    api_query: The API Query from which to retrieve the last request timedelta.\\n    from_time: A DateTime object representing the start time to calculate the\\n               timedelta from.\\n\\n  Returns:\\n    A string that describes how long since the API Query response was last\\n    requested in the form of \"HH hours, MM minutes, ss seconds ago\" or None\\n    if the API Query response has never been requested.\\n  '\n    if (not from_time):\n        from_time = datetime.utcnow()\n    if api_query.last_request:\n        time_delta = (from_time - api_query.last_request)\n        return FormatTimedelta(time_delta)\n    return None\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/particle/shared_particle_test_16.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef __init__(self, name):\n    Exception.__init__(self, (\"Method not found: '%s'\" % name))\n", "label": 0}
{"function": "\n\ndef testUnshareSecondLevelRemoved(self):\n    'Re-share photos, remove the reshared viewpoint, then unshare the source viewpoint.'\n    (child_vp_id, child_ep_ids) = self._tester.ShareNew(self._cookie2, [(self._new_ep_id, self._photo_ids)], [self._user3.user_id], **self._CreateViewpointDict(self._cookie2))\n    self._tester.RemoveViewpoint(self._cookie3, child_vp_id)\n    self._tester.Unshare(self._cookie, self._new_vp_id, [(self._new_ep_id, self._photo_ids[:1])])\n", "label": 0}
{"function": "\n\ndef test_search_comment(self):\n    result = self.search(comment=['fantastic'])\n    self.assertEqual(list(result[0].tracks), self.tracks[3:4])\n    result = self.search(comment=['antasti'])\n    self.assertEqual(list(result[0].tracks), self.tracks[3:4])\n", "label": 0}
{"function": "\n\n@converts('ImageField')\ndef conv_Image(self, model, field, kwargs):\n    return f.FileField(**kwargs)\n", "label": 0}
{"function": "\n\ndef parse(self, text):\n    return [ErrorLine(m) for m in self.regex.finditer(text)]\n", "label": 0}
{"function": "\n\ndef clamp_vect(self, v):\n    'Returns a copy of the vector v clamped to the bounding box'\n    return cpffi.cpBBClampVect(self._bb, v)\n", "label": 0}
{"function": "\n\ndef __exit__(self, *args):\n    self.delegate.disconnect()\n", "label": 0}
{"function": "\n\n@view_config(context='velruse.AuthenticationComplete', renderer='{}:templates/result.mako'.format(__name__))\ndef login_complete_view(request):\n    context = request.context\n    result = {\n        'profile': context.profile,\n        'credentials': context.credentials,\n    }\n    return {\n        'result': json.dumps(result, indent=4),\n    }\n", "label": 0}
{"function": "\n\ndef draw_outlines(context, box, enable_hinting):\n    width = box.style.outline_width\n    color = box.style.get_color('outline_color')\n    style = box.style.outline_style\n    if ((box.style.visibility == 'visible') and (width != 0) and (color.alpha != 0)):\n        outline_box = ((box.border_box_x() - width), (box.border_box_y() - width), (box.border_width() + (2 * width)), (box.border_height() + (2 * width)))\n        for side in SIDES:\n            with stacked(context):\n                clip_border_segment(context, enable_hinting, style, width, side, outline_box)\n                draw_rect_border(context, outline_box, (4 * (width,)), style, styled_color(style, color, side))\n    if isinstance(box, boxes.ParentBox):\n        for child in box.children:\n            if isinstance(child, boxes.Box):\n                draw_outlines(context, child, enable_hinting)\n", "label": 1}
{"function": "\n\ndef emit(self, *args, **kwargs):\n    try:\n        self.__emitting = True\n        for handler in self.__handlers:\n            handler(*args, **kwargs)\n    finally:\n        self.__emitting = False\n        self.__applyChanges()\n", "label": 0}
{"function": "\n\ndef test_existing_spawn(self):\n    child = pexpect.spawnu('bash', timeout=5, echo=False)\n    repl = replwrap.REPLWrapper(child, re.compile('[$#]'), \"PS1='{0}' PS2='{1}' PROMPT_COMMAND=''\")\n    res = repl.run_command('echo $HOME')\n    assert res.startswith('/'), res\n", "label": 0}
{"function": "\n\ndef child_removed(self, child):\n    ' Handle the child removed event for a QtWindow.\\n\\n        '\n    if isinstance(child, WxContainer):\n        self.widget.SetCentralWidget(self.central_widget())\n", "label": 0}
{"function": "\n\ndef test_stubs(self):\n    df = pd.DataFrame([[0, 1, 2, 3, 8], [4, 5, 6, 7, 9]])\n    df.columns = ['id', 'inc1', 'inc2', 'edu1', 'edu2']\n    stubs = ['inc', 'edu']\n    df_long = pd.wide_to_long(df, stubs, i='id', j='age')\n    self.assertEqual(stubs, ['inc', 'edu'])\n", "label": 0}
{"function": "\n\ndef collectstreamuuid(self, streamname):\n    if (not streamname):\n        return\n    shouter.shout(('Get UUID of configured stream ' + streamname))\n    showuuidcommand = ('%s --show-alias n --show-uuid y show attributes -r %s -w %s' % (self.scmcommand, self.repo, streamname))\n    output = shell.getoutput(showuuidcommand)\n    splittedfirstline = output[0].split(' ')\n    streamuuid = splittedfirstline[0].strip()[1:(- 1)]\n    return streamuuid\n", "label": 0}
{"function": "\n\n@classmethod\ndef resource_uri(cls, obj=None):\n    object_id = 'id'\n    if (obj is not None):\n        object_id = obj.id\n    return ('api_events', [object_id])\n", "label": 0}
{"function": "\n\ndef __init__(self, text):\n    if (len(text) >= 10000):\n        text = (text[:9995] + '\\n...')\n    self.text = text\n    self.recipient = None\n", "label": 0}
{"function": "\n\ndef info(self, msg_format, *values):\n    'For progress and other informative messages.'\n    if (len(values) > 0):\n        msg_format = (msg_format % values)\n    print(msg_format)\n", "label": 0}
{"function": "\n\ndef freeze(self, skipSet=None):\n    assert (len(self()) in self.allowedSize)\n    return StringStream.freeze(self, skipSet=skipSet)\n", "label": 0}
{"function": "\n\ndef get_result(self, vlan_range_len):\n    self.intersect()\n    if (vlan_range_len > 1):\n        return self.get_final_available_vlan_range(vlan_range_len)\n    else:\n        return self.get_final_available_vlan()\n", "label": 0}
{"function": "\n\ndef __init__(self, collector, callback=None, *args, **kw):\n    '\\n        Create a pager with a Reference to a remote collector and\\n        an optional callable to invoke upon completion.\\n        '\n    if callable(callback):\n        self.callback = callback\n        self.callbackArgs = args\n        self.callbackKeyword = kw\n    else:\n        self.callback = None\n    self._stillPaging = 1\n    self.collector = collector\n    collector.broker.registerPageProducer(self)\n", "label": 0}
{"function": "\n\ndef __init__(self, name, consumer_key, consumer_secret):\n    self.name = name\n    self.type = 'twitter'\n    self.consumer_key = consumer_key\n    self.consumer_secret = consumer_secret\n    self.login_route = ('velruse.%s-login' % name)\n    self.callback_route = ('velruse.%s-callback' % name)\n", "label": 0}
{"function": "\n\n@mock.patch(('%s.flavors.osclients.Clients' % CTX))\ndef test_cleanup(self, mock_clients):\n    real_context = {\n        'flavors': {\n            'flavor_name': {\n                'flavor_name': 'flavor_name',\n                'id': 'flavor_name',\n            },\n        },\n        'admin': {\n            'credential': mock.MagicMock(),\n        },\n        'task': mock.MagicMock(),\n    }\n    flavors_ctx = flavors.FlavorsGenerator(real_context)\n    flavors_ctx.cleanup()\n    mock_clients.assert_called_with(real_context['admin']['credential'])\n    mock_flavors_delete = mock_clients().nova().flavors.delete\n    mock_flavors_delete.assert_called_with('flavor_name')\n", "label": 0}
{"function": "\n\ndef get_all_vms(self):\n    '\\n        Returns a generator over all VMs known to this vCenter host.\\n        '\n    for folder in self.get_first_level_of_vm_folders():\n        for vm in get_all_vms_in_folder(folder):\n            (yield vm)\n", "label": 0}
{"function": "\n\ndef wait_for_responses(self, client):\n    'Waits for all responses to come back and resolves the\\n        eventual results.\\n        '\n    assert_open(self)\n    if self.has_pending_requests:\n        raise RuntimeError('Cannot wait for responses if there are pending requests outstanding.  You need to wait for pending requests to be sent first.')\n    pending = self.pending_responses\n    self.pending_responses = []\n    for (command_name, promise) in pending:\n        value = client.parse_response(self.connection, command_name)\n        promise.resolve(value)\n", "label": 0}
{"function": "\n\ndef __update_copyright(self):\n    'Finds the copyright text and replaces it.'\n    region = self.__find_copyright()\n    self.__replace_copyright(region)\n", "label": 0}
{"function": "\n\ndef __init__(self, idx):\n    self.idx = _uidx()\n    self.isBatch = False\n    self.isSeq = True\n    if isinstance(idx, BaseArray):\n        arr = ct.c_void_p(0)\n        if (idx.type() == Dtype.b8.value):\n            safe_call(backend.get().af_where(ct.pointer(arr), idx.arr))\n        else:\n            safe_call(backend.get().af_retain_array(ct.pointer(arr), idx.arr))\n        self.idx.arr = arr\n        self.isSeq = False\n    elif isinstance(idx, ParallelRange):\n        self.idx.seq = idx\n        self.isBatch = True\n    else:\n        self.idx.seq = Seq(idx)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/armor/component/shared_deflector_shield_generator_energy_ray.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef leq(levels, int_time=1.0):\n    '\\n    Equivalent level :math:`L_{eq}`.\\n    \\n    :param levels: Levels as function of time.\\n    :param int_time: Integration time. Default value is 1.0 second.\\n    :returns: Equivalent level L_{eq}.\\n    \\n    Sum of levels in dB.\\n    '\n    levels = np.asarray(levels)\n    time = (levels.size * int_time)\n    return _leq(levels, time)\n", "label": 0}
{"function": "\n\ndef get_next_instruction(self):\n    dis = self.disassemble(address=self.program_counter()[1], count=1)\n    return dis.partition('\\n')[0].strip()\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/munition/shared_detonator_thermal_imperial_issue.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef get_dates(self, resource):\n    '\\n        Retrieve dates from mercurial\\n        '\n    try:\n        commits = subprocess.check_output(['hg', 'log', '--template={date|isodatesec}\\n', resource.path]).split('\\n')\n        commits = commits[:(- 1)]\n    except subprocess.CalledProcessError:\n        self.logger.warning(('Unable to get mercurial history for [%s]' % resource))\n        commits = None\n    if (not commits):\n        self.logger.warning(('No mercurial history for [%s]' % resource))\n        return (None, None)\n    created = parse(commits[(- 1)].strip())\n    modified = parse(commits[0].strip())\n    return (created, modified)\n", "label": 0}
{"function": "\n\ndef test_tx_out_bitcoin_address(self):\n    coinbase_bytes = h2b('04ed66471b02c301')\n    tx = Tx.coinbase_tx(COINBASE_PUB_KEY_FROM_80971, int((50 * 100000000.0)), COINBASE_BYTES_FROM_80971)\n    self.assertEqual(tx.txs_out[0].bitcoin_address(), '1DmapcnrJNGeJB13fv9ngRFX1iRvR4zamn')\n", "label": 0}
{"function": "\n\ndef alert_smtp(alert, metric):\n    if ('@' in alert[1]):\n        sender = settings.ALERT_SENDER\n        recipient = alert[1]\n    else:\n        sender = settings.SMTP_OPTS['sender']\n        recipients = settings.SMTP_OPTS['recipients'][alert[0]]\n    if (type(recipients) is str):\n        recipients = [recipients]\n    for recipient in recipients:\n        msg = MIMEMultipart('alternative')\n        msg['Subject'] = ('[skyline alert] ' + metric[1])\n        msg['From'] = sender\n        msg['To'] = recipient\n        link = (settings.GRAPH_URL % metric[1])\n        body = ('Anomalous value: %s <br> Next alert in: %s seconds <a href=\"%s\"><img src=\"%s\"/></a>' % (metric[0], alert[2], link, link))\n        msg.attach(MIMEText(body, 'html'))\n        s = SMTP('127.0.0.1')\n        s.sendmail(sender, recipient, msg.as_string())\n        s.quit()\n", "label": 0}
{"function": "\n\ndef delete_key(self, key):\n    self.server.request('delete', ('/keys/%s' % key))\n", "label": 0}
{"function": "\n\ndef _create_placeholders(self, n_features, n_classes):\n    ' Create the TensorFlow placeholders for the model.\\n        :param n_features: number of features of the first layer\\n        :param n_classes: number of classes\\n        :return: self\\n        '\n    self.keep_prob = tf.placeholder('float')\n    self.hrand = [tf.placeholder('float', [None, self.layers[(l + 1)]]) for l in range((self.n_layers - 1))]\n    self.vrand = [tf.placeholder('float', [None, self.layers[l]]) for l in range((self.n_layers - 1))]\n    self.x = tf.placeholder('float', [None, n_features])\n    self.y_ = tf.placeholder('float', [None, n_classes])\n", "label": 0}
{"function": "\n\ndef __init__(self, name_suggestion):\n    self.name_suggestion = name_suggestion\n", "label": 0}
{"function": "\n\ndef render_datalist(self, list_id):\n    return ''.join([('<datalist id=\"%s\">' % list_id), ''.join([('<option>%s</option>' % color) for color in self.colors]), '</datalist>'])\n", "label": 0}
{"function": "\n\ndef _do_remove(self, section, option):\n    if (not self.config.has_option(section, option)):\n        raise AdminCommandError(_(\"Option '%(option)s' doesn't exist in section '%(section)s'\", option=option, section=section))\n    self.config.remove(section, option)\n    self.config.save()\n    if ((section == 'inherit') and (option == 'file')):\n        self.config.parse_if_needed(force=True)\n", "label": 0}
{"function": "\n\ndef _unit_file(self, name):\n    for extension in ['service', 'yaml']:\n        file_path = '{0}.{1}'.format(name, extension)\n        if path.exists(file_path):\n            with open(file_path) as handle:\n                if (extension == 'service'):\n                    return handle.read()\n                data = yaml.load(handle)\n                if (self._global and ('global' in data)):\n                    return data['global']\n                if (self._name in data):\n                    return data[self._name]\n                raise ValueError('No unit found for {0}'.format(self._name))\n    raise ValueError('No unit file: '.format(name))\n", "label": 1}
{"function": "\n\ndef _create_flavor(self, description=None):\n    flavor = {\n        'flavor': {\n            'name': 'GOLD',\n            'service_type': constants.DUMMY,\n            'description': (description or 'the best flavor'),\n            'enabled': True,\n        },\n    }\n    return (self.plugin.create_flavor(self.ctx, flavor), flavor)\n", "label": 0}
{"function": "\n\ndef test_logout(self):\n    'Tests when logging out with and without continue URL.'\n    host = 'foo.com:1234'\n    path_info = '/_ah/login'\n    cookie_dict = {\n        'dev_appserver_login': ('%s:False:%s' % (EMAIL, USER_ID)),\n    }\n    action = 'Logout'\n    set_email = ''\n    set_admin = False\n    continue_url = ''\n    expected_set = login._clear_user_info_cookie().strip()\n    (status, location, set_cookie, _) = self._run_test(host, path_info, cookie_dict, action, set_email, set_admin, continue_url)\n    self.assertEqual(302, status)\n    self.assertEqual(('http://%s%s' % (host, path_info)), location)\n    self.assertEqual(expected_set, set_cookie)\n    self.assertIsInstance(location, str)\n    self.assertIsInstance(set_cookie, str)\n    continue_url = 'http://foo.com/blah'\n    (status, location, set_cookie, _) = self._run_test(host, path_info, cookie_dict, action, set_email, set_admin, continue_url)\n    self.assertEqual(302, status)\n    self.assertEqual(continue_url, location)\n    self.assertEqual(expected_set, set_cookie)\n    self.assertIsInstance(location, str)\n    self.assertIsInstance(set_cookie, str)\n", "label": 0}
{"function": "\n\ndef test04c__getitem__(self):\n    'Checking cols.__getitem__() with subgroups with a range index with\\n        step.'\n    tbl = self.h5file.create_table('/', 'test', self._TestTDescr, title=self._getMethodName())\n    tbl.append(self._testAData)\n    if self.reopen:\n        self._reopen()\n        tbl = self.h5file.root.test\n    nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)\n    tblcols = tbl.cols._f_col('Info')[0::2]\n    nrarrcols = nrarr['Info'][0::2]\n    if common.verbose:\n        print('Read cols:', tblcols)\n        print('Should look like:', nrarrcols)\n    self.assertTrue(common.areArraysEqual(nrarrcols, tblcols), \"Original array are retrieved doesn't match.\")\n", "label": 0}
{"function": "\n\ndef test_key_has_correct_repr(self):\n    '\\n        Calling repr on a Key instance returns the proper string.\\n        '\n    key = pem.Key(b'test')\n    assert ('<Key({0})>'.format(TEST_DIGEST) == repr(key))\n", "label": 0}
{"function": "\n\ndef calcScale(self, testCount):\n    import math\n    scale = int((self.size / (math.sqrt(testCount) + 1)))\n    return scale\n", "label": 0}
{"function": "\n\ndef get_repository_info(self):\n    '\\n        Find out information about the current Bazaar branch (if any) and\\n        return it.\\n        '\n    if (not check_install(['bzr', 'help'])):\n        logging.debug('Unable to execute \"bzr help\": skipping Bazaar')\n        return None\n    bzr_info = execute(['bzr', 'info'], ignore_errors=True)\n    if ('ERROR: Not a branch:' in bzr_info):\n        repository_info = None\n    else:\n        branch_match = re.search(self.BRANCH_REGEX, bzr_info, re.MULTILINE)\n        path = branch_match.group('branch_path')\n        if (path == '.'):\n            path = os.getcwd()\n        repository_info = RepositoryInfo(path=path, base_path='/', supports_parent_diffs=True)\n    return repository_info\n", "label": 0}
{"function": "\n\ndef do_access_token_response(self, access_token, atinfo, state, refresh_token=None):\n    _tinfo = {\n        'access_token': access_token,\n        'expires_in': atinfo['exp'],\n        'token_type': 'bearer',\n        'state': state,\n    }\n    try:\n        _tinfo['scope'] = atinfo['scope']\n    except KeyError:\n        pass\n    if refresh_token:\n        _tinfo['refresh_token'] = refresh_token\n    return AccessTokenResponse(**by_schema(AccessTokenResponse, **_tinfo))\n", "label": 0}
{"function": "\n\ndef test_set_messages_success(self):\n    author = {\n        'name': 'John Doe',\n        'slug': 'success-msg',\n    }\n    add_url = reverse('add_success_msg')\n    req = self.client.post(add_url, author)\n    self.assertIn((ContactFormViewWithMsg.success_message % author), req.cookies['messages'].value)\n", "label": 0}
{"function": "\n\ndef __init__(self, question, docs):\n    super(Extractor, self).__init__(question, docs, tag=TAG)\n", "label": 0}
{"function": "\n\ndef expect(self, method=None, uri=None, params={\n    \n}):\n    if method:\n        self.assertEqual(method, self.executor.request.method)\n    if uri:\n        self.assertEqual(self.executor.request.uri, ('https://api-ssl.bitly.com/v3' + uri))\n    if params:\n        params.update({\n            'access_token': 'my-access-token',\n        })\n        self.assertEqual(self.executor.request.params, params)\n", "label": 0}
{"function": "\n\ndef test_handle_error_401_sends_challege_default_realm(self):\n    api = restplus.Api(self.app, serve_challenge_on_401=True)\n    exception = HTTPException()\n    exception.code = 401\n    exception.data = {\n        'foo': 'bar',\n    }\n    with self.app.test_request_context('/foo'):\n        resp = api.handle_error(exception)\n        self.assertEqual(resp.status_code, 401)\n        self.assertEqual(resp.headers['WWW-Authenticate'], 'Basic realm=\"flask-restplus\"')\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    if isinstance(other, BaseNull):\n        return other\n    return self.map((Q + _unwrap(other)))\n", "label": 0}
{"function": "\n\ndef next_hop(tokeniser):\n    value = tokeniser()\n    if (value.lower() == 'self'):\n        return (IPSelf(tokeniser.afi), NextHopSelf(tokeniser.afi))\n    else:\n        ip = IP.create(value)\n        if (ip.afi == AFI.ipv4):\n            return (ip, NextHop(ip.top()))\n        return (ip, None)\n", "label": 0}
{"function": "\n\ndef test_is_variant(self):\n    expander = GvcfExpander()\n    self.assertTrue(expander.is_variant(json.loads(self.snp_1)))\n    self.assertTrue(expander.is_variant(json.loads(self.snp_2)))\n    self.assertTrue(expander.is_variant(json.loads(self.insertion_1)))\n    self.assertTrue(expander.is_variant(json.loads(self.deletion_1)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_a)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_b)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_c)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_d)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_ambiguous)))\n    self.assertFalse(expander.is_variant(json.loads(self.no_call_1)))\n", "label": 0}
{"function": "\n\ndef test_coerce_on_select(nyc):\n    t = symbol('t', discover(nyc))\n    t = t[(((((((((t.pickup_latitude >= 40.477399) & (t.pickup_latitude <= 40.917577)) & (t.dropoff_latitude >= 40.477399)) & (t.dropoff_latitude <= 40.917577)) & (t.pickup_longitude >= (- 74.25909))) & (t.pickup_longitude <= (- 73.700272))) & (t.dropoff_longitude >= (- 74.25909))) & (t.dropoff_longitude <= (- 73.700272))) & (t.passenger_count < 6))]\n    t = transform(t, pass_count=(t.passenger_count + 1))\n    result = compute(t.pass_count.coerce('float64'), nyc, return_type='native')\n    s = odo(result, pd.Series)\n    expected = (compute(t, nyc, return_type=pd.DataFrame).passenger_count.astype('float64') + 1.0)\n    assert (list(s) == list(expected))\n", "label": 0}
{"function": "\n\ndef modify_updates(self, updates):\n    '\"\\n        Modifies the parameters before a learning update is applied. Behavior\\n        is defined by subclass\\'s implementation of _modify_updates and any\\n        ModelExtension\\'s implementation of post_modify_updates.\\n\\n        Parameters\\n        ----------\\n        updates : dict\\n            A dictionary mapping shared variables to symbolic values they\\n            will be updated to\\n\\n        Notes\\n        -----\\n        For example, if a given parameter is not meant to be learned, a\\n        subclass or extension\\n        should remove it from the dictionary. If a parameter has a restricted\\n        range, e.g.. if it is the precision of a normal distribution,\\n        a subclass or extension should clip its update to that range. If a\\n        parameter\\n        has any other special properties, its updates should be modified\\n        to respect that here, e.g. a matrix that must be orthogonal should\\n        have its update value modified to be orthogonal here.\\n\\n        This is the main mechanism used to make sure that generic training\\n        algorithms such as those found in pylearn2.training_algorithms\\n        respect the specific properties of the models passed to them.\\n        '\n    self._modify_updates(updates)\n    self._ensure_extensions()\n    for extension in self.extensions:\n        extension.post_modify_updates(updates, self)\n", "label": 0}
{"function": "\n\ndef set_flipped(self, x, y):\n    ' Sets the specified piece as flipped.\\n        '\n    self.pieces[(x + (y * self.width))].set_flipped()\n", "label": 0}
{"function": "\n\ndef on_leave(self, details):\n    self.disconnect()\n", "label": 0}
{"function": "\n\n@needs_mail\n@needs_link\ndef proxy(request, mail, link):\n    return link.get_target(mail)(request, mail.person, mail.job.group_object)\n", "label": 0}
{"function": "\n\ndef TestParallelModify(instances):\n    'PERFORMANCE: Parallel instance modify.\\n\\n  @type instances: list of L{qa_config._QaInstance}\\n  @param instances: list of instances to issue modify commands against\\n\\n  '\n    job_driver = _JobQueueDriver()\n    new_min_mem = qa_config.get(constants.BE_MAXMEM)\n    for instance in instances:\n        cmd = ['gnt-instance', 'modify', '--submit', '-B', ('%s=%s' % (constants.BE_MINMEM, new_min_mem))]\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n        cmd = ['gnt-instance', 'modify', '--submit', '-O', 'fake_os_param=fake_value']\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n        cmd = ['gnt-instance', 'modify', '--submit', '-O', 'fake_os_param=fake_value', '-B', ('%s=%s' % (constants.BE_MINMEM, new_min_mem))]\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n    job_driver.WaitForCompletion()\n", "label": 0}
{"function": "\n\ndef get_dir(self, path, dest='', saltenv='base', gzip=None, cachedir=None):\n    '\\n        Get a directory recursively from the salt-master\\n        '\n    ret = []\n    path = self._check_proto(path).rstrip('/')\n    separated = path.rsplit('/', 1)\n    if (len(separated) != 2):\n        prefix = ''\n    else:\n        prefix = separated[0]\n    for fn_ in self.file_list(saltenv, prefix=path):\n        try:\n            if (fn_[len(path)] != '/'):\n                continue\n        except IndexError:\n            continue\n        minion_relpath = fn_[len(prefix):].lstrip('/')\n        ret.append(self.get_file(salt.utils.url.create(fn_), '{0}/{1}'.format(dest, minion_relpath), True, saltenv, gzip))\n    try:\n        for fn_ in self.file_list_emptydirs(saltenv, prefix=path):\n            try:\n                if (fn_[len(path)] != '/'):\n                    continue\n            except IndexError:\n                continue\n            minion_relpath = fn_[len(prefix):].lstrip('/')\n            minion_mkdir = '{0}/{1}'.format(dest, minion_relpath)\n            if (not os.path.isdir(minion_mkdir)):\n                os.makedirs(minion_mkdir)\n            ret.append(minion_mkdir)\n    except TypeError:\n        pass\n    ret.sort()\n    return ret\n", "label": 1}
{"function": "\n\n@require_creds(True)\n@rpcmethod(signature=[SUCCESS_TYPE, URN_TYPE, CREDENTIALS_TYPE], url_name='openflow_gapi')\ndef DeleteSliver(slice_urn, credentials, **kwargs):\n    logger.debug('Called DeleteSliver')\n    try:\n        return gapi.DeleteSliver(slice_urn, kwargs['request'].user)\n    except Slice.DoesNotExist:\n        no_such_slice(slice_urn)\n", "label": 0}
{"function": "\n\ndef _dependencies(self):\n    projects = []\n    for attr in ('install_requires', 'tests_require'):\n        requirements = (getattr(self.distribution, attr, None) or [])\n        for project in requirements:\n            if (not project):\n                continue\n            projects.append(pypi.just_name(project))\n    extras = (getattr(self.distribution, 'extras_require', None) or {\n        \n    })\n    for value in extras.values():\n        projects.extend(map(pypi.just_name, value))\n    return projects\n", "label": 1}
{"function": "\n\ndef identity_provider_create(request, idp_id, description=None, enabled=False, remote_ids=None):\n    manager = keystoneclient(request, admin=True).federation.identity_providers\n    try:\n        return manager.create(id=idp_id, description=description, enabled=enabled, remote_ids=remote_ids)\n    except keystone_exceptions.Conflict:\n        raise exceptions.Conflict()\n", "label": 0}
{"function": "\n\ndef save_graph_db_refs(self, sourceDB=None, targetDB=None, edgeDB=None, simpleKeys=False, unpack_edge=None, edgeDictClass=None, graph=None, **kwargs):\n    'apply kwargs to reference DB objects for this graph'\n    if (sourceDB is not None):\n        self.sourceDB = sourceDB\n    else:\n        simpleKeys = True\n    if (targetDB is not None):\n        self.targetDB = targetDB\n    if (edgeDB is not None):\n        self.edgeDB = edgeDB\n    else:\n        self.pack_edge = self.unpack_edge = (lambda edge: edge)\n    if simpleKeys:\n        self.__class__ = self._IDGraphClass\n    if (unpack_edge is not None):\n        self.unpack_edge = unpack_edge\n    if (graph is not None):\n        self.graph = graph\n    if (edgeDictClass is not None):\n        self.edgeDictClass = edgeDictClass\n", "label": 1}
{"function": "\n\ndef __init__(self, errmsg='You need override this method'):\n    super(NeedOverrideError, self).__init__(self, errmsg)\n", "label": 0}
{"function": "\n\ndef test_past_datetime(self):\n    value = self.sd.past_datetime()\n    self.assertTrue(isinstance(value, datetime.datetime))\n    self.assertTrue((value <= datetime.datetime.utcnow().replace(tzinfo=utc)))\n    self.assertTrue((value >= (datetime.datetime.utcnow().replace(tzinfo=utc) - datetime.timedelta(minutes=1440))))\n    value = self.sd.past_datetime(0, 10)\n    self.assertTrue((value <= datetime.datetime.utcnow().replace(tzinfo=utc)))\n    self.assertTrue((value >= (datetime.datetime.utcnow().replace(tzinfo=utc) - datetime.timedelta(minutes=10))))\n    with self.assertRaises(ParameterError):\n        self.sd.past_datetime(100, 0)\n    with self.assertRaises(ParameterError):\n        self.sd.past_datetime((- 10), 10)\n", "label": 0}
{"function": "\n\ndef play_rtmpdump_stream(player, url, params={\n    \n}):\n    cmdline = (\"rtmpdump -r '%s' \" % url)\n    for key in params.keys():\n        cmdline += (((key + ' ') + params[key]) if (params[key] != None) else ('' + ' '))\n    cmdline += (' -o - | %s -' % player)\n    print(cmdline)\n    os.system(cmdline)\n    return\n", "label": 0}
{"function": "\n\ndef test_disable_logging(self):\n    NAME = 'name'\n    before = {\n        'logging': {\n            'logBucket': 'logs',\n            'logObjectPrefix': 'pfx',\n        },\n    }\n    bucket = self._makeOne(name=NAME, properties=before)\n    self.assertTrue((bucket.get_logging() is not None))\n    bucket.disable_logging()\n    self.assertTrue((bucket.get_logging() is None))\n", "label": 0}
{"function": "\n\ndef test_benchmark_variance_06(self):\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['Monthly'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.three_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['3-Month'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.six_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['6-month'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.year_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['year'])\n", "label": 0}
{"function": "\n\ndef all_job_data(jobs, job_type):\n    ' Return an iterator over all job data. Exclude config template dups. '\n    conf_tmpl_ids = []\n    for job in jobs:\n        for (jt, data) in job.iteritems():\n            if (jt == job_type):\n                if (data['config_template_id'] not in conf_tmpl_ids):\n                    conf_tmpl_ids.append(data['config_template_id'])\n                    (yield data)\n", "label": 0}
{"function": "\n\ndef take_action(self, parsed_args):\n    identity_client = self.app.client_manager.identity\n    consumer = utils.find_resource(identity_client.oauth1.consumers, parsed_args.consumer)\n    identity_client.oauth1.consumers.delete(consumer.id)\n", "label": 0}
{"function": "\n\ndef _download_pdf(self, url, base_path):\n    local_file_path = os.path.join(base_path, 'billing-temp-document.pdf')\n    response = requests.get(url, stream=True)\n    should_wipe_bad_headers = True\n    with open(local_file_path, 'wb') as out_file:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                if should_wipe_bad_headers:\n                    pdf_header_pos = chunk.find('%PDF-')\n                    if (pdf_header_pos > 0):\n                        chunk = chunk[pdf_header_pos:]\n                    should_wipe_bad_headers = False\n                out_file.write(chunk)\n                out_file.flush()\n    return local_file_path\n", "label": 0}
{"function": "\n\ndef test_validate_type_negative(self):\n    sla1 = TestCriterion(0)\n\n    class AnotherTestCriterion(TestCriterion):\n        pass\n    sla2 = AnotherTestCriterion(0)\n    self.assertRaises(TypeError, sla1.validate_type, sla2)\n", "label": 0}
{"function": "\n\ndef test_profile_topics_bookmarks(self):\n    \"\\n        profile user's topics with bookmarks\\n        \"\n    bookmark = CommentBookmark.objects.create(topic=self.topic, user=self.user)\n    utils.login(self)\n    response = self.client.get(reverse('spirit:user:topics', kwargs={\n        'pk': self.user2.pk,\n        'slug': self.user2.st.slug,\n    }))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(list(response.context['topics']), [self.topic])\n    self.assertEqual(response.context['topics'][0].bookmark, bookmark)\n", "label": 0}
{"function": "\n\ndef test_volume_create_properties(self):\n    arglist = ['--property', 'Alpha=a', '--property', 'Beta=b', '--size', str(self.new_volume.size), self.new_volume.name]\n    verifylist = [('property', {\n        'Alpha': 'a',\n        'Beta': 'b',\n    }), ('size', self.new_volume.size), ('name', self.new_volume.name)]\n    parsed_args = self.check_parser(self.cmd, arglist, verifylist)\n    (columns, data) = self.cmd.take_action(parsed_args)\n    self.volumes_mock.create.assert_called_with(size=self.new_volume.size, snapshot_id=None, name=self.new_volume.name, description=None, volume_type=None, user_id=None, project_id=None, availability_zone=None, metadata={\n        'Alpha': 'a',\n        'Beta': 'b',\n    }, imageRef=None, source_volid=None)\n    self.assertEqual(self.columns, columns)\n    self.assertEqual(self.datalist, data)\n", "label": 0}
{"function": "\n\ndef init_relation(self, models, relation):\n    '\\n        Initialize the relation on a set of models.\\n\\n        :type models: list\\n        :type relation: str\\n        '\n    for model in models:\n        model.set_relation(relation, Result(None, self, model))\n    return models\n", "label": 0}
{"function": "\n\ndef test_with_some_synonyms(self):\n    SynonymFactory(from_words='foo', to_words='bar')\n    SynonymFactory(from_words='baz', to_words='qux')\n    (_, body) = es_utils.es_get_synonym_filter('en-US')\n    expected = {\n        'type': 'synonym',\n        'synonyms': ['foo => bar', 'baz => qux'],\n    }\n    eq_(body, expected)\n", "label": 0}
{"function": "\n\ndef run(self):\n    f = self.output().open('w')\n    print('hello, world', file=f)\n    f.close()\n", "label": 0}
{"function": "\n\ndef __init__(self, area, width=4, char=' '):\n    self.width = width\n    self.char = char\n    area.install(('NORMAL', '<Key-greater>', (lambda event: event.widget.shift_sel_right(self.width, self.char))), ('NORMAL', '<Key-less>', (lambda event: event.widget.shift_sel_left(self.width))))\n", "label": 0}
{"function": "\n\n@classmethod\ndef validate(cls, level):\n    level = int(level)\n    if (level in (cls.NONE, cls.READ, cls.WRITE, cls.ADMIN, cls.SITE_ADMIN)):\n        return level\n    else:\n        raise ValueError(('Invalid AccessType: %d.' % level))\n", "label": 0}
{"function": "\n\ndef __init__(self, lib, dtype, N, C, K, H, W, P, Q, pad_h, pad_w, relu, bsum):\n    (R, S) = (3, 3)\n    GC32 = _ceil_div(C, 32)\n    GC16 = _ceil_div((GC32 * 32), 16)\n    GK16 = _ceil_div(K, 16)\n    self.filter_func = _get_bprop_filter_trans_4x4_kernel\n    self.filter_size = (((dtype.itemsize * 1152) * K) * GC32)\n    self.filter_args = [(GK16, GC16, 1), (256, 1, 1), None, None, None, ((R * S) * K), (S * K), ((S * K) * 2), K, C, (K * 1152)]\n    super(BpropWinograd_4x4_3x3, self).__init__(lib, dtype, N, K, C, P, Q, H, W, (2 - pad_h), (2 - pad_w), relu, bsum)\n", "label": 0}
{"function": "\n\n@records.post(validators=record_validator, permission='post_record')\ndef post_record(request):\n    'Saves a single model record.\\n\\n    Posted record attributes will be matched against the related model\\n    definition.\\n\\n    '\n    if (request.headers.get('Validate-Only', 'false') == 'true'):\n        return\n    model_id = request.matchdict['model_id']\n    if request.credentials_id:\n        credentials_id = request.credentials_id\n    else:\n        credentials_id = Everyone\n    record_id = request.db.put_record(model_id, request.data_clean, [credentials_id])\n    request.notify('RecordCreated', model_id, record_id)\n    created = ('%s/models/%s/records/%s' % (request.application_url, model_id, record_id))\n    request.response.status = '201 Created'\n    request.response.headers['location'] = str(created)\n    return {\n        'id': record_id,\n    }\n", "label": 0}
{"function": "\n\ndef on_files_selected(self, paths):\n    \" Handle the 'filesSelected' signal from the dialog.\\n\\n        \"\n    d = self.declaration\n    if (d is not None):\n        d.selected_paths = paths\n", "label": 0}
{"function": "\n\ndef _dispatch(self, inst, kws):\n    assert (self.current_block is not None)\n    fname = ('op_%s' % inst.opname.replace('+', '_'))\n    try:\n        fn = getattr(self, fname)\n    except AttributeError:\n        raise NotImplementedError(inst)\n    else:\n        try:\n            return fn(inst, **kws)\n        except errors.NotDefinedError as e:\n            if (e.loc is None):\n                e.loc = self.loc\n            raise e\n", "label": 1}
{"function": "\n\ndef set_context(self, serializer):\n    '\\n        This hook is called by the serializer instance,\\n        prior to the validation call being made.\\n        '\n    self.instance = getattr(serializer, 'instance', None)\n", "label": 0}
{"function": "\n\ndef db_add_ts_start(self, ts_start):\n    self._db_ts_start = ts_start\n", "label": 0}
{"function": "\n\n@expose('/<string:locale>')\ndef index(self, locale):\n    session['locale'] = locale\n    refresh()\n    self.update_redirect()\n    return redirect(self.get_redirect())\n", "label": 0}
{"function": "\n\ndef reselect(self, pos):\n\n    def select(view, edit):\n        region = pos\n        if hasattr(pos, '__call__'):\n            region = run_callback(pos, view)\n        if isinstance(region, int):\n            region = sublime.Region(region, region)\n        elif isinstance(region, (tuple, list)):\n            region = sublime.Region(*region)\n        view.sel().clear()\n        view.sel().add(region)\n        view.show(region, False)\n    self.callback(select)\n", "label": 0}
{"function": "\n\ndef test_mutual_info_regression():\n    (X, y) = make_regression(n_samples=100, n_features=10, n_informative=2, shuffle=False, random_state=0, noise=10)\n    univariate_filter = SelectKBest(mutual_info_regression, k=2)\n    X_r = univariate_filter.fit(X, y).transform(X)\n    assert_best_scores_kept(univariate_filter)\n    X_r2 = GenericUnivariateSelect(mutual_info_regression, mode='k_best', param=2).fit(X, y).transform(X)\n    assert_array_equal(X_r, X_r2)\n    support = univariate_filter.get_support()\n    gtruth = np.zeros(10)\n    gtruth[:2] = 1\n    assert_array_equal(support, gtruth)\n    univariate_filter = SelectPercentile(mutual_info_regression, percentile=20)\n    X_r = univariate_filter.fit(X, y).transform(X)\n    X_r2 = GenericUnivariateSelect(mutual_info_regression, mode='percentile', param=20).fit(X, y).transform(X)\n    assert_array_equal(X_r, X_r2)\n    support = univariate_filter.get_support()\n    gtruth = np.zeros(10)\n    gtruth[:2] = 1\n    assert_array_equal(support, gtruth)\n", "label": 0}
{"function": "\n\ndef test_derived(self):\n    import time\n\n    class Local(threading.local):\n\n        def __init__(self):\n            time.sleep(0.01)\n    local = Local()\n\n    def f(i):\n        local.x = i\n        self.assertEqual(local.x, i)\n    threads = []\n    for i in range(10):\n        t = threading.Thread(target=f, args=(i,))\n        t.start()\n        threads.append(t)\n    for t in threads:\n        t.join()\n", "label": 0}
{"function": "\n\ndef test_retry_in_graph_flow_requires_and_provides(self):\n    flow = gf.Flow('gf', retry.AlwaysRevert('rt', requires=['x', 'y'], provides=['a', 'b']))\n    self.assertEqual(set(['x', 'y']), flow.requires)\n    self.assertEqual(set(['a', 'b']), flow.provides)\n", "label": 0}
{"function": "\n\ndef test_review_comments(self):\n    \"Show that one can iterate over a PR's review comments.\"\n    cassette_name = self.cassette_name('review_comments')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        for comment in p.review_comments():\n            assert isinstance(comment, github3.pulls.ReviewComment)\n", "label": 0}
{"function": "\n\ndef update(self, t, dt):\n    if (random.random() < 0.02):\n        self.a += ((random.randint((- 1), 1) * pi) / 8)\n    dx = cos(self.a)\n    dz = sin(self.a)\n    self.x += (dx * dt)\n    self.z += (dz * dt)\n", "label": 0}
{"function": "\n\ndef encryption_oracle(rawInput):\n    key = generateAESKey()\n    iv = generateAESKey()\n    prependAmount = (5 + (getOneRandomByte() % 6))\n    appendAmount = (5 + (getOneRandomByte() % 6))\n    plaintext = (((b'x' * prependAmount) + rawInput) + (b'y' * appendAmount))\n    if (getOneRandomByte() & 1):\n        return aes_ecb_enc(addPKCS7Padding(plaintext, 16), key)\n    else:\n        return aes_cbc_enc(addPKCS7Padding(plaintext, 16), key, iv)\n", "label": 0}
{"function": "\n\ndef parse_status(self, lines):\n    activity = []\n    seen_times = set()\n    for line in lines:\n        (time, fields) = line.split('|')\n        if (time not in seen_times):\n            seen_times.add(time)\n            status_obj = status.Status(int(float(time)), fields)\n            activity.append(status_obj)\n    return activity\n", "label": 0}
{"function": "\n\n@property\ndef directions(self):\n    hashes = []\n    t_query = 'select trip_headsign, trip_short_name, bikes_allowed,\\n        trip_id from trips where route_id=:route_id and _feed=:_feed'\n    t_filter = {\n        'route_id': self.id,\n        '_feed': self.provider.feed_id,\n    }\n    cur = self.provider.conn.cursor()\n    for trip in cur.execute(t_query, t_filter):\n        direction = {\n            \n        }\n        innercur = self.provider.conn.cursor()\n        result = innercur.execute('select s.* from stop_times as st join stops as s\\n                on st.stop_id=s.stop_id and st._feed=s._feed\\n                where st.trip_id=:t_id and st._feed=:_feed', {\n            't_id': trip['trip_id'],\n            '_feed': self.provider.feed_id,\n        })\n        direction['stops'] = [GTFSStop(self.provider, **dict(row)) for row in result]\n        if (trip['trip_headsign'] is not None):\n            direction['headsign'] = trip['trip_headsign']\n        if (trip['trip_short_name'] is not None):\n            direction['short_name'] = trip['trip_short_name']\n        if (trip['bikes_allowed'] is not None):\n            direction['bikes_ok'] = trip['bikes_allowed']\n        h = util.freezehash(direction)\n        if (h not in hashes):\n            hashes.append(h)\n            (yield direction)\n", "label": 1}
{"function": "\n\ndef _partitions_to_src(partitions):\n    return ''.join((part.src for part in partitions))\n", "label": 0}
{"function": "\n\ndef test_input_extra_rewrite(self):\n    self.client_job_description.rewrite_paths = True\n    extra_file = os.path.join(self.input1_files_path, 'moo', 'cow.txt')\n    os.makedirs(os.path.dirname(extra_file))\n    open(extra_file, 'w').write('Hello World!')\n    command_line = ('test.exe %s' % extra_file)\n    self.client_job_description.command_line = command_line\n    self.client.expect_command_line('test.exe /pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt')\n    self.client.expect_put_paths(['/pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt'])\n    self._submit()\n    uploaded_file1 = self.client.put_files[0]\n    assert (uploaded_file1[1] == 'input')\n    assert (uploaded_file1[0] == extra_file)\n", "label": 0}
{"function": "\n\ndef get_member_addresses(self):\n    addresses = []\n    for member in self.config_members:\n        addresses.append(member.get_server().get_address())\n    return addresses\n", "label": 0}
{"function": "\n\ndef test_dispatch_on_heartbeat_frame(self):\n    frame = mock()\n    expect(frame.type).returns(HeartbeatFrame.type())\n    expect(self.ch.send_heartbeat)\n    self.ch.dispatch(frame)\n", "label": 0}
{"function": "\n\ndef __init__(self, output_type, inplace=False):\n    Op.__init__(self)\n    self.output_type = output_type\n    self.inplace = inplace\n    if inplace:\n        self.destroy_map = {\n            0: [0],\n        }\n    self.warned_numpy_version = False\n", "label": 0}
{"function": "\n\ndef CheckLoggingWorks(self):\n    logger = StringIO.StringIO()\n    expected_output = (';\\n'.join([sqlite.main._BEGIN, 'CREATE TABLE TEST(FOO INTEGER)', 'INSERT INTO TEST(FOO) VALUES (?)', 'ROLLBACK']) + ';\\n')\n    self.cnx = sqlite.connect(self.getfilename(), command_logfile=logger)\n    cu = self.cnx.cursor()\n    cu.execute('CREATE TABLE TEST(FOO INTEGER)')\n    cu.execute('INSERT INTO TEST(FOO) VALUES (?)', (5,))\n    self.cnx.rollback()\n    logger.seek(0)\n    real_output = logger.read()\n    if (expected_output != real_output):\n        self.fail(\"Logging didn't produce expected output.\")\n", "label": 0}
{"function": "\n\ndef bitand(self, other):\n    return self._combine(other, self.BITAND, False)\n", "label": 0}
{"function": "\n\ndef get_environment(self, app):\n    return app.extensions['gears']['environment']\n", "label": 0}
{"function": "\n\ndef load(self, odffile):\n    ' Loads a document into the parser and parses it.\\n            The argument can either be a filename or a document in memory.\\n        '\n    self.lines = []\n    self._wfunc = self._wlines\n    if isinstance(odffile, str):\n        self.document = load(odffile)\n    else:\n        self.document = odffile\n    self._walknode(self.document.topnode)\n", "label": 0}
{"function": "\n\ndef db_children(self, parent=(None, None), orphan=False):\n    children = []\n    children.append((self, parent[0], parent[1]))\n    return children\n", "label": 0}
{"function": "\n\ndef blacklist_delete_user_agents(self, request, queryset):\n    self.blacklist_user_agents(request, queryset)\n    self.delete_queryset(request, queryset)\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n\n    def format_row(row):\n        return ('(%s)' % ', '.join((format_number(value) for value in row)))\n    return ('Matrix44(%s)' % ', '.join((format_row(row) for row in self.rows())))\n", "label": 0}
{"function": "\n\ndef _createFile(self):\n    self.h5file.create_array('/', 'arr1', [1, 2])\n    group1 = self.h5file.create_group('/', 'group1')\n    arr2 = self.h5file.create_array(group1, 'arr2', [1, 2, 3])\n    lgroup1 = self.h5file.create_hard_link('/', 'lgroup1', '/group1')\n    self.assertTrue((lgroup1 is not None))\n    larr1 = self.h5file.create_hard_link(group1, 'larr1', '/arr1')\n    self.assertTrue((larr1 is not None))\n    larr2 = self.h5file.create_hard_link('/', 'larr2', arr2)\n    self.assertTrue((larr2 is not None))\n", "label": 0}
{"function": "\n\ndef saveXML(self, snode):\n    if (snode is None):\n        snode = self\n    elif (snode.ownerDocument is not self):\n        raise xml.dom.WrongDocumentErr()\n    return snode.toxml()\n", "label": 0}
{"function": "\n\ndef install(self, instance):\n    cfg = self.config\n    if (self.database == self.master_database):\n        template = 'template1'\n    else:\n        template = self.master_database\n    config_opt_map = dict(host='host', user='username', password='password', encoding='encoding', lc_collate='lc-collate', lc_ctype='lc-ctype', tablespace='tablespace')\n    options = [((('--' + opt) + '=') + str(cfg[cfg_name])) for (cfg_name, opt) in config_opt_map.iteritems() if (cfg_name in cfg)]\n    fail = instance.do(((['createdb', '--template', template] + options) + [self.database]))\n    if fail:\n        instance.do(['pg_dump', '-h', str(cfg['host']), '-U', str(cfg['user']), '-E', str(cfg['encoding']), '-f', (('/tmp/' + template) + '.sql'), template])\n        instance.do(((['createdb', '--template', 'template1'] + options) + [self.database]))\n        instance.do(['psql', '-h', str(cfg['host']), '-U', str(cfg['user']), '-f', (('/tmp/' + template) + '.sql'), self.database])\n    info = self.connection_info\n    info['database'] = self.database\n    return info\n", "label": 0}
{"function": "\n\n@contextmanager\ndef trace_ms(statName):\n    if (_instatrace is None):\n        (yield)\n        return\n    now = _instatrace.now_ms()\n    (yield)\n    _instatrace.trace(statName, (_instatrace.now_ms() - now))\n", "label": 0}
{"function": "\n\ndef test_get_bad_column_qualifier(self):\n    families = {\n        cf2: 'oberyn',\n    }\n    res = self.c.get(table, self.row_prefix, families=families)\n    self.assertEqual(result_to_dict(res), {\n        \n    })\n", "label": 0}
{"function": "\n\ndef db_add_portSpec(self, portSpec):\n    self.__db_portSpec = portSpec\n", "label": 0}
{"function": "\n\ndef do(args):\n    ' Main method '\n    if args.name:\n        tc_names = [args.name]\n    else:\n        tc_names = qitoolchain.get_tc_names()\n    for tc_name in tc_names:\n        toolchain = qitoolchain.get_toolchain(tc_name)\n        ui.info(str(toolchain))\n", "label": 0}
{"function": "\n\ndef test_IRParamStmt_dump():\n    stmt = {\n        'param': 'IR',\n        'angle': '45',\n    }\n    ir = IRParamStmt.from_dict(stmt)\n    assert_equal(ir.to_gerber(), '%IR45*%')\n", "label": 0}
{"function": "\n\ndef get_resources(self):\n    resources = []\n    resource = extensions.ResourceExtension('os-agents', AgentController())\n    resources.append(resource)\n    return resources\n", "label": 0}
{"function": "\n\ndef _save(self, key, attributes):\n    s_uuid = self.repo.save(key, attributes)\n    self.logger.debug(('creating object with uuid = %s' % s_uuid))\n    attribute_type = AT.UNIQUE_IDENTIFIER\n    attribute = self.attribute_factory.create_attribute(attribute_type, s_uuid)\n    attributes.append(attribute)\n    self.repo.update(s_uuid, key, attributes)\n    return (s_uuid, attribute)\n", "label": 0}
{"function": "\n\ndef to_array(self):\n    '\\n        Convert the RiakLinkPhase to a format that can be output into\\n        JSON. Used internally.\\n        '\n    stepdef = {\n        'bucket': self._bucket,\n        'tag': self._tag,\n        'keep': self._keep,\n    }\n    return {\n        'link': stepdef,\n    }\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Tangible()\n    result.template = 'object/tangible/ship/crafted/capacitor/shared_quick_recharge_battery_mk5.iff'\n    result.attribute_template_id = 8\n    result.stfName('space_crafting_n', 'quick_recharge_battery_mk5')\n    return result\n", "label": 0}
{"function": "\n\ndef abort_request(self, stream, ident, parent):\n    'abort a specifig msg by id'\n    msg_ids = parent['content'].get('msg_ids', None)\n    if isinstance(msg_ids, str):\n        msg_ids = [msg_ids]\n    if (not msg_ids):\n        self.abort_queues()\n    for mid in msg_ids:\n        self.aborted.add(str(mid))\n    content = dict(status='ok')\n    reply_msg = self.session.send(stream, 'abort_reply', content=content, parent=parent, ident=ident)\n    self.log.debug(str(reply_msg))\n", "label": 0}
{"function": "\n\ndef p_basic_statement(self, p):\n    'basic_statement : if_statement\\n        | case_statement\\n        | casex_statement\\n        | for_statement\\n        | while_statement\\n        | event_statement\\n        | wait_statement\\n        | forever_statement\\n        | block\\n        | namedblock\\n        | parallelblock\\n        | blocking_substitution\\n        | nonblocking_substitution\\n        | single_statement\\n        '\n    p[0] = p[1]\n    p.set_lineno(0, p.lineno(1))\n", "label": 0}
{"function": "\n\ndef write(self, handle, name):\n    self._check_all_set()\n    g = handle.create_group(name)\n    g.attrs['type'] = np.string_('extern_box'.encode('utf-8'))\n    g.attrs['xmin'] = self.bounds[0][0]\n    g.attrs['xmax'] = self.bounds[0][1]\n    g.attrs['ymin'] = self.bounds[1][0]\n    g.attrs['ymax'] = self.bounds[1][1]\n    g.attrs['zmin'] = self.bounds[2][0]\n    g.attrs['zmax'] = self.bounds[2][1]\n    Source.write(self, g)\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    self.one_to_one = True\n    return super(OneToOneFieldDefinition, self).save(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef testComplexLabels(self):\n    logging.debug('Running testComplexLabels method.')\n    expression = 'a_123'\n    self._RunMathQuery(expression, ['a_123'])\n    expression = 'a_1.b2'\n    self._RunMathQuery(expression, ['a_1.b2'])\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    ' Returns the materialized path '\n    return ('/'.join([x.value for x in self.parts]) + ('/' if self.is_dir else ''))\n", "label": 0}
{"function": "\n\ndef retry_subflow(self, retry):\n    'Prepares a retrys + its subgraph for execution.\\n\\n        This sets the retrys intention to ``EXECUTE`` and resets all of its\\n        subgraph (its successors) to the ``PENDING`` state with an ``EXECUTE``\\n        intention.\\n        '\n    tweaked = self.reset_atoms([retry], state=None, intention=st.EXECUTE)\n    tweaked.extend(self.reset_subgraph(retry))\n    return tweaked\n", "label": 0}
{"function": "\n\ndef draw_random(G, **kwargs):\n    'Draw the graph G with a random layout.'\n    draw(G, random_layout(G), **kwargs)\n", "label": 0}
{"function": "\n\ndef post(self):\n    ' pass additionalMetadata and file to global\\n        variables.\\n        '\n    global received_file\n    global received_meta\n    received_file = self.request.files['file'][0].body\n    received_meta = self.get_argument('additionalMetadata')\n", "label": 0}
{"function": "\n\ndef __init__(self, gates, system_desc, wh_codes):\n    self.gates = gates\n    self.system_desc = system_desc\n    self.wh_codes = wh_codes\n", "label": 0}
{"function": "\n\ndef __init__(self, args):\n    super(RemoveVariantSetRunner, self).__init__(args)\n    self.variantSetName = args.variantSetName\n", "label": 0}
{"function": "\n\n@override_djconfig(comments_per_page=1)\ndef test_profile_comments_paginate(self):\n    \"\\n        profile user's comments paginated\\n        \"\n    utils.create_comment(user=self.user2, topic=self.topic)\n    comment = utils.create_comment(user=self.user2, topic=self.topic)\n    utils.login(self)\n    response = self.client.get(reverse('spirit:user:detail', kwargs={\n        'pk': self.user2.pk,\n        'slug': self.user2.st.slug,\n    }))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(list(response.context['comments']), [comment])\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    self.maxDiff = None\n    filename = 'chart_data_labels24.xlsx'\n    test_dir = 'xlsxwriter/test/comparison/'\n    self.got_filename = ((test_dir + '_test_') + filename)\n    self.exp_filename = ((test_dir + 'xlsx_files/') + filename)\n    self.ignore_files = []\n    self.ignore_elements = {\n        \n    }\n", "label": 0}
{"function": "\n\ndef get_available_user_FIELD_transitions(instance, user, field):\n    '\\n    List of transitions available in current model state\\n    with all conditions met and user have rights on it\\n    '\n    for transition in get_available_FIELD_transitions(instance, field):\n        if transition.has_perm(instance, user):\n            (yield transition)\n", "label": 0}
{"function": "\n\ndef pixelCollision(rect1, rect2, hitmask1, hitmask2):\n    'Checks if two objects collide and not just their rects'\n    rect = rect1.clip(rect2)\n    if ((rect.width == 0) or (rect.height == 0)):\n        return False\n    (x1, y1) = ((rect.x - rect1.x), (rect.y - rect1.y))\n    (x2, y2) = ((rect.x - rect2.x), (rect.y - rect2.y))\n    for x in xrange(rect.width):\n        for y in xrange(rect.height):\n            if (hitmask1[(x1 + x)][(y1 + y)] and hitmask2[(x2 + x)][(y2 + y)]):\n                return True\n    return False\n", "label": 1}
{"function": "\n\ndef _item_position(self, item):\n    return self.items.index(item)\n", "label": 0}
{"function": "\n\ndef GetModifiedShellCommand(self, Command, PluginOutputDir):\n    self.RefreshReplacements()\n    NewCommand = ((('cd ' + self.ShellPathEscape(PluginOutputDir)) + '; ') + MultipleReplace(Command, self.DynamicReplacements))\n    self.OldCommands[NewCommand] = Command\n    return NewCommand\n", "label": 0}
{"function": "\n\ndef report_for_conf(self, conf):\n    'Returns the path to the ivy report for the provided conf.\\n\\n     Returns None if there is no path.\\n    '\n    return self._reports_by_conf.get(conf)\n", "label": 0}
{"function": "\n\ndef our_x2_iterates(n_iters=100):\n    history = []\n    searchspace = SearchSpace()\n    searchspace.add_float('x', (- 10), 10)\n    random = np.random.RandomState(0)\n\n    def fn(params):\n        return (- (params['x'] ** 2))\n    for i in range(n_iters):\n        params = HyperoptTPE(seed=random).suggest(history, searchspace)\n        history.append((params, fn(params), 'SUCCEEDED'))\n    return np.array([h[0]['x'] for h in history])\n", "label": 0}
{"function": "\n\ndef insert(self, window, first_line, *lines):\n    (row, column) = cursor = self.cursors[window]\n    (left, right) = (self[row][:column], self[row][column:])\n    added = len(lines)\n    if lines:\n        last_line = lines[(- 1)]\n        column = len(last_line)\n    else:\n        last_line = first_line\n        column += len(first_line)\n    self[row] = (left + first_line)\n    self[(row + 1):(row + 1)] = lines\n    self[(row + added)] += right\n    for other in self.cursors.itervalues():\n        if (other.row > row):\n            other._row += added\n    cursor.coords = ((row + added), column)\n", "label": 0}
{"function": "\n\ndef clean_message(self):\n    message = self.cleaned_data['message']\n    try:\n        message = message.decode('base64')\n    except TypeError as e:\n        raise ValidationError(('Cannot convert to binary: %r' % e.msg))\n    if (len(message) % 16):\n        raise ValidationError('Wrong block size for message !')\n    if (len(message) <= 16):\n        raise ValidationError('Message too short or missing IV !')\n    return message\n", "label": 0}
{"function": "\n\n@keep_alive('server')\ndef address_is_mine(self, address):\n    result = self.server.validateaddress(address)\n    return result['ismine']\n", "label": 0}
{"function": "\n\ndef _bump_version(self, version):\n    try:\n        parts = map(int, version.split('.'))\n    except ValueError:\n        self._fail('Current version is not numeric')\n    parts[(- 1)] += 1\n    return '.'.join(map(str, parts))\n", "label": 0}
{"function": "\n\ndef npm_command(self, args):\n    'Creates a command that can run `npm`, passing the given args to it.\\n\\n    :param list args: A list of arguments to pass to `npm`.\\n    :returns: An `npm` command that can be run later.\\n    :rtype: :class:`NodeDistribution.Command`\\n    '\n    return self._create_command('npm', args)\n", "label": 0}
{"function": "\n\ndef test_disenroll_with_no_enrollment(self):\n    courses = Course.objects.all()\n    for course in courses:\n        course.delete()\n    client = Client()\n    client.login(username=TEST_USER_USERNAME, password=TEST_USER_PASSWORD)\n    kwargs = {\n        'HTTP_X_REQUESTED_WITH': 'XMLHttpRequest',\n    }\n    response = client.post('/disenroll', {\n        'course_id': 1,\n    }, **kwargs)\n    self.assertEqual(response.status_code, 200)\n    json_string = response.content.decode(encoding='UTF-8')\n    array = json.loads(json_string)\n    self.assertEqual(array['message'], 'record does not exist')\n    self.assertEqual(array['status'], 'failed')\n", "label": 0}
{"function": "\n\n@parameterized.expand([('split', 2, 3, 3, 1.5), ('merger', 2, 3, 3, 1.8), ('dividend', 2, 3, 3, 2.88)])\ndef test_spot_price_adjustments(self, adjustment_type, liquid_day_0_price, liquid_day_1_price, illiquid_day_0_price, illiquid_day_1_price_adjusted):\n    'Test the behaviour of spot prices during adjustments.'\n    table_name = (adjustment_type + 's')\n    liquid_asset = getattr(self, (adjustment_type.upper() + '_ASSET'))\n    illiquid_asset = getattr(self, (('ILLIQUID_' + adjustment_type.upper()) + '_ASSET'))\n    adjustments = self.adjustment_reader.get_adjustments_for_sid(table_name, liquid_asset.sid)\n    self.assertEqual(1, len(adjustments))\n    adjustment = adjustments[0]\n    self.assertEqual(adjustment[0], pd.Timestamp('2016-01-06', tz='UTC'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[0]), 'daily')\n    self.assertEqual(liquid_day_0_price, bar_data.current(liquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[1]), 'daily')\n    self.assertEqual(liquid_day_1_price, bar_data.current(liquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[1]), 'daily')\n    self.assertEqual(illiquid_day_0_price, bar_data.current(illiquid_asset, 'price'))\n    bar_data = BarData(self.data_portal, (lambda : self.bcolz_daily_bar_days[2]), 'daily')\n    self.assertAlmostEqual(illiquid_day_1_price_adjusted, bar_data.current(illiquid_asset, 'price'))\n", "label": 0}
{"function": "\n\ndef get_ud(self, cardinal, user, channel, msg):\n    try:\n        word = msg.split(' ', 1)[1]\n    except IndexError:\n        cardinal.sendMsg(channel, 'Syntax: .ud <word>')\n        return\n    try:\n        url = (URBANDICT_API_PREFIX + word)\n        f = urlopen(url).read()\n        data = json.loads(f)\n        word_def = data['list'][0]['definition']\n        link = data['list'][0]['permalink']\n        response = ('UD for %s: %s (%s)' % (word, word_def, link))\n        cardinal.sendMsg(channel, response.encode('utf-8'))\n    except Exception:\n        cardinal.sendMsg(channel, ('Could not retrieve definition for %s' % word))\n", "label": 0}
{"function": "\n\ndef test__merge(self):\n    seg1a = fake_neo(Block, seed=self.seed1, n=self.nchildren).segments[0]\n    assert_same_sub_schema(self.seg1, seg1a)\n    seg1a.spikes.append(self.spikes2[0])\n    seg1a.epocharrays.append(self.epcas2[0])\n    seg1a.annotate(seed=self.seed2)\n    seg1a.merge(self.seg2)\n    self.check_creation(self.seg2)\n    assert_same_sub_schema((self.sigs1a + self.sigs2), seg1a.analogsignals)\n    assert_same_sub_schema((self.sigarrs1a + self.sigarrs2), seg1a.analogsignalarrays)\n    assert_same_sub_schema((self.irsigs1a + self.irsigs2), seg1a.irregularlysampledsignals)\n    assert_same_sub_schema((self.epcs1 + self.epcs2), seg1a.epochs)\n    assert_same_sub_schema((self.epcas1 + self.epcas2), seg1a.epocharrays)\n    assert_same_sub_schema((self.evts1 + self.evts2), seg1a.events)\n    assert_same_sub_schema((self.evtas1 + self.evtas2), seg1a.eventarrays)\n    assert_same_sub_schema((self.spikes1 + self.spikes2), seg1a.spikes)\n    assert_same_sub_schema((self.trains1 + self.trains2), seg1a.spiketrains)\n", "label": 0}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    (ishape, fshape) = shapes\n    (igroups, icolors_per_group, irows, icols, icount) = ishape\n    (fmodulesR, fmodulesC, fcolors, frows, fcols) = fshape[:(- 2)]\n    (fgroups, filters_per_group) = fshape[(- 2):]\n    if ((not any_symbolic(irows, icols)) and (irows != icols)):\n        raise ValueError('non-square image argument', (irows, icols))\n    if ((not any_symbolic(frows, fcols)) and (frows != fcols)):\n        raise ValueError('non-square filter shape', (frows, fcols))\n    if ((not any_symbolic(fmodulesR, fmodulesC)) and (fmodulesR != fmodulesC)):\n        raise ValueError('non-square filter grouping', (fmodulesR, fmodulesC))\n    if ((not any_symbolic(icolors_per_group, fcolors)) and (icolors_per_group != fcolors)):\n        raise ValueError(\"color counts don't match\", (icolors_per_group, fcolors))\n    hshape = (fgroups, filters_per_group, fmodulesR, fmodulesC, icount)\n    return [hshape]\n", "label": 1}
{"function": "\n\ndef store_and_use_artifact(self, cache_key, src, results_dir=None):\n    'Read the content of a tarball from an iterator and return an artifact stored in the cache.'\n    with self._tmpfile(cache_key, 'read') as tmp:\n        for chunk in src:\n            tmp.write(chunk)\n        tmp.close()\n        tarball = self._store_tarball(cache_key, tmp.name)\n        artifact = self._artifact(tarball)\n        if (results_dir is not None):\n            safe_rmtree(results_dir)\n        artifact.extract()\n        return True\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    return ('Tuple(%s)' % ', '.join((str(elt) for elt in self.elts)))\n", "label": 0}
{"function": "\n\ndef test_create_ticket_ticket(self):\n    '\\n        A ticket ought to be created with a provided ticket string,\\n        if present.\\n        '\n    ticket = 'ST-0000000000-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'\n    st = ServiceTicket.objects.create_ticket(ticket=ticket, user=self.user)\n    self.assertEqual(st.ticket, ticket)\n", "label": 0}
{"function": "\n\ndef test_binary_infix_operators(self):\n    (a, b, h) = self.table.get_columns(['a', 'b', 'h'])\n    bool_col = (a > 0)\n    cases = [((a + b), '`a` + `b`'), ((a - b), '`a` - `b`'), ((a * b), '`a` * `b`'), ((a / b), '`a` / `b`'), ((a ** b), 'pow(`a`, `b`)'), ((a < b), '`a` < `b`'), ((a <= b), '`a` <= `b`'), ((a > b), '`a` > `b`'), ((a >= b), '`a` >= `b`'), ((a == b), '`a` = `b`'), ((a != b), '`a` != `b`'), ((h & bool_col), '`h` AND (`a` > 0)'), ((h | bool_col), '`h` OR (`a` > 0)'), ((h ^ bool_col), '(`h` OR (`a` > 0)) AND NOT (`h` AND (`a` > 0))')]\n    self._check_expr_cases(cases)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (other is None):\n        return (self._value is None)\n    other = str(other)\n    if (other not in self.values_range):\n        raise ConanException(bad_value_msg(self._name, other, self.values_range))\n    return (other == self.__str__())\n", "label": 0}
{"function": "\n\ndef _set_play_state(self, state):\n    'Helper method for play/pause/toggle.'\n    players = self._get_players()\n    if (len(players) != 0):\n        self._server.Player.PlayPause(players[0]['playerid'], state)\n    self.update_ha_state()\n", "label": 0}
{"function": "\n\ndef test_multiple_sequences(self):\n    msa = TabularMSA([DNA('ACGT'), DNA('AG-.'), DNA('AC-.')])\n    cons = msa.consensus()\n    self.assertEqual(cons, DNA('AC--'))\n", "label": 0}
{"function": "\n\ndef appletGetDetails(*args, **kwargs):\n    '\\n\\n    .. deprecated:: 0.42.0\\n       Use :func:`applet_get_details()` instead.\\n\\n    '\n    print('dxpy.appletGetDetails is deprecated; please use applet_get_details instead.', file=sys.stderr)\n    return applet_get_details(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _prepare_ivy_xml(self, frozen_resolution, ivyxml, resolve_hash_name_for_report):\n    default_resolution = frozen_resolution.get('default')\n    if (default_resolution is None):\n        raise IvyUtils.IvyError(\"Couldn't find the frozen resolution for the 'default' ivy conf.\")\n    try:\n        jars = default_resolution.jar_dependencies\n        IvyUtils.generate_fetch_ivy(jars, ivyxml, self.confs, resolve_hash_name_for_report)\n    except Exception as e:\n        raise IvyUtils.IvyError('Failed to prepare ivy resolve: {}'.format(e))\n", "label": 0}
{"function": "\n\n@continuation\ndef imp_struct_set_cont(orig_struct, setter, field, app, env, cont, _vals):\n    from pycket.interpreter import check_one_val\n    val = check_one_val(_vals)\n    if (setter is values.w_false):\n        return orig_struct.set_with_extra_info(field, val, app, env, cont)\n    return setter.call_with_extra_info([orig_struct, val], env, cont, app)\n", "label": 0}
{"function": "\n\ndef _do_if_else_condition(self, condition):\n    '\\n        Common logic for evaluating the conditions on #if, #ifdef and\\n        #ifndef lines.\\n        '\n    self.save()\n    d = self.dispatch_table\n    if condition:\n        self.start_handling_includes()\n        d['elif'] = self.stop_handling_includes\n        d['else'] = self.stop_handling_includes\n    else:\n        self.stop_handling_includes()\n        d['elif'] = self.do_elif\n        d['else'] = self.start_handling_includes\n", "label": 0}
{"function": "\n\ndef loadWordFile(self, pre_processor=None):\n    filename = self.getDictionaryPath()\n    with codecs.open(filename, 'r', 'utf-8') as fp:\n        for word in fp.readlines():\n            if pre_processor:\n                self.add(pre_processor(word.strip()))\n            else:\n                self.add(word.strip())\n    return\n", "label": 0}
{"function": "\n\ndef test_level(self):\n    key = EncryptionKey(data='', level='SL3')\n    self.assertEquals('SL3', key.level)\n", "label": 0}
{"function": "\n\ndef __setitem__(self, key, value):\n    'Dictionary style assignment.'\n    (rval, cval) = self.value_encode(value)\n    self.__set(key, rval, cval)\n", "label": 0}
{"function": "\n\n@sig((((H / ((H / 'a') >> bool)) >> ['a']) >> [int]))\ndef findIndicies(f, xs):\n    '\\n    findIndices :: (a -> Bool) -> [a] -> [Int]\\n\\n    The findIndices function extends findIndex, by returning the indices of all\\n    elements satisfying the predicate, in ascending order.\\n    '\n    return L[(i for (i, x) in enumerate(xs) if f(x))]\n", "label": 0}
{"function": "\n\ndef __init__(self, mediator=None):\n    'Initializes the scanner object.\\n\\n    Args:\\n      mediator: a volume scanner mediator (instance of\\n                VolumeScannerMediator) or None.\\n    '\n    super(VolumeScanner, self).__init__()\n    self._mediator = mediator\n    self._source_path = None\n    self._source_scanner = source_scanner.SourceScanner()\n    self._source_type = None\n    self._vss_stores = None\n", "label": 0}
{"function": "\n\ndef get_all_active_nodes(self, is_running=None):\n    if self.active_gen_id:\n        return self.get_all_nodes(self.active_gen_id, is_running=is_running)\n    return []\n", "label": 0}
{"function": "\n\ndef test_basic_start(self):\n    configjson = self.experiment.do_start_experiment()\n    self.assertIsNotNone(configjson)\n", "label": 0}
{"function": "\n\ndef test_options_disallowed(self):\n    request = factory.options('/', HTTP_AUTHORIZATION=self.disallowed_credentials)\n    response = root_view(request, pk='1')\n    self.assertEqual(response.status_code, status.HTTP_200_OK)\n    self.assertNotIn('actions', response.data)\n    request = factory.options('/1', HTTP_AUTHORIZATION=self.disallowed_credentials)\n    response = instance_view(request, pk='1')\n    self.assertEqual(response.status_code, status.HTTP_200_OK)\n    self.assertNotIn('actions', response.data)\n", "label": 0}
{"function": "\n\ndef test__build_key_none(self):\n    result = filecache._build_key(None, None)\n    self.assertEqual('None:None', result)\n", "label": 0}
{"function": "\n\ndef _option_required(self, key):\n    conf = S3_CONF.copy()\n    del conf[key]\n    try:\n        self.store = Store(test_utils.TestConfigOpts(conf))\n        return (self.store.add == self.store.add_disabled)\n    except:\n        return False\n    return False\n", "label": 0}
{"function": "\n\ndef show_analyzer_list_panel(self, callback):\n    list_panel = AnalyzerListPanel(self.window, self.client, self.settings.index)\n    list_panel.show(callback)\n", "label": 0}
{"function": "\n\ndef clone_settings(self, original):\n    self.replace_tabs_by_spaces = original.replace_tabs_by_spaces\n    self.safe_save = original.replace_tabs_by_spaces\n    self.clean_trailing_whitespaces = original.clean_trailing_whitespaces\n    self.restore_cursor = original.restore_cursor\n", "label": 0}
{"function": "\n\ndef _is_verified_address(self, address):\n    if (address in self.addresses):\n        return True\n    (user, host) = address.split('@', 1)\n    return (host in self.domains)\n", "label": 0}
{"function": "\n\ndef deserialize(self, raw_value):\n    if (raw_value.upper() in self.TRUE_RAW_VALUES):\n        return True\n    elif (raw_value.upper() in self.FALSE_RAW_VALUES):\n        return False\n    else:\n        raise DeserializationError('Value \"{}\" must be one of {} for \"{}\"!'.format(raw_value, self.ALLOWED_RAW_VALUES, self.name), raw_value, self.name)\n", "label": 0}
{"function": "\n\ndef _create_service_command(self, name, command):\n    command_table = self.create_command_table(command.get('subcommands', {\n        \n    }), self._create_operation_command)\n    service_command = ServiceCommand(name, None)\n    service_command._service_model = {\n        \n    }\n    service_command._command_table = command_table\n    return service_command\n", "label": 0}
{"function": "\n\n@override_settings(DEFAULT_FROM_EMAIL='foo@bar.com')\ndef test_sender_from_email(self):\n    '\\n        Should use DEFAULT_FROM_EMAIL instead of the default\\n        '\n\n    class SiteMock():\n        name = 'foo'\n        domain = 'bar.com'\n\n    def monkey_get_current_site(*args, **kw):\n        return SiteMock\n\n    def monkey_render_to_string(*args, **kw):\n        return 'email body'\n    req = RequestFactory().get('/')\n    token = 'token'\n    subject = SiteMock.name\n    template_name = 'template.html'\n    context = {\n        'user_id': self.user.pk,\n        'token': token,\n    }\n    (org_site, email.get_current_site) = (email.get_current_site, monkey_get_current_site)\n    (org_render_to_string, email.render_to_string) = (email.render_to_string, monkey_render_to_string)\n    try:\n        sender(req, subject, template_name, context, [self.user.email])\n    finally:\n        email.get_current_site = org_site\n        email.render_to_string = org_render_to_string\n    self.assertEquals(len(mail.outbox), 1)\n    self.assertEquals(mail.outbox[0].from_email, 'foo@bar.com')\n", "label": 0}
{"function": "\n\ndef extract_info(self, body):\n    '\\n        Extract metadata url\\n        '\n    xhr_url_match = re.search(self._XHR_REQUEST_PATH, body)\n    if (xhr_url_match is not None):\n        xhr_url = xhr_url_match.group(1)\n    else:\n        xhr_url = None\n    if ((xhr_url is not None) and xhr_url.endswith('xml')):\n        default_filename = xhr_url.split('/')[1]\n    else:\n        self.error(ExtractionError, \"ERROR: can't get default_filename.\")\n    return {\n        'default_filename': default_filename,\n        'xhr_url': xhr_url,\n    }\n", "label": 0}
{"function": "\n\ndef test_content_length_0(self):\n\n    class ContentLengthChecker(list):\n\n        def __init__(self):\n            list.__init__(self)\n            self.content_length = None\n\n        def append(self, item):\n            kv = item.split(b':', 1)\n            if ((len(kv) > 1) and (kv[0].lower() == b'content-length')):\n                self.content_length = kv[1].strip()\n            list.append(self, item)\n    conn = client.HTTPConnection('example.com')\n    conn.sock = FakeSocket(None)\n    conn._buffer = ContentLengthChecker()\n    conn.request('POST', '/', '')\n    self.assertEqual(conn._buffer.content_length, b'0', 'Header Content-Length not set')\n    conn = client.HTTPConnection('example.com')\n    conn.sock = FakeSocket(None)\n    conn._buffer = ContentLengthChecker()\n    conn.request('PUT', '/', '')\n    self.assertEqual(conn._buffer.content_length, b'0', 'Header Content-Length not set')\n", "label": 0}
{"function": "\n\ndef WriteScanNode(self, scan_node, indentation=''):\n    'Writes the source scanner node to stdout.\\n\\n    Args:\\n      scan_node: the scan node (instance of SourceScanNode).\\n      indentation: optional indentation string.\\n      scan_step: optional integer indicating the scan step.\\n    '\n    if (not scan_node):\n        return\n    values = []\n    part_index = getattr(scan_node.path_spec, 'part_index', None)\n    if (part_index is not None):\n        values.append('{0:d}'.format(part_index))\n    store_index = getattr(scan_node.path_spec, 'store_index', None)\n    if (store_index is not None):\n        values.append('{0:d}'.format(store_index))\n    start_offset = getattr(scan_node.path_spec, 'start_offset', None)\n    if (start_offset is not None):\n        values.append('start offset: {0:d} (0x{0:08x})'.format(start_offset))\n    location = getattr(scan_node.path_spec, 'location', None)\n    if (location is not None):\n        values.append('location: {0:s}'.format(location))\n    print('{0:s}{1:s}: {2:s}'.format(indentation, scan_node.path_spec.type_indicator, ', '.join(values)))\n    indentation = '  {0:s}'.format(indentation)\n    for sub_scan_node in scan_node.sub_nodes:\n        self.WriteScanNode(sub_scan_node, indentation=indentation)\n", "label": 1}
{"function": "\n\ndef deserialize(self, obj):\n    return datetime.datetime.strptime(obj, self.format).time()\n", "label": 0}
{"function": "\n\ndef seek(self, offset, whence=os.SEEK_SET):\n    'Seek to the provided location in the file.\\n\\n        :param offset: location to seek to\\n        :type offset: int\\n        :param whence: determines whether `offset` represents a\\n                       location that is absolute, relative to the\\n                       beginning of the file, or relative to the end\\n                       of the file\\n        :type whence: os.SEEK_SET | os.SEEK_CUR | os.SEEK_END\\n        :returns: None\\n        :rtype: None\\n        '\n    if (whence == os.SEEK_SET):\n        self._cursor = (0 + offset)\n    elif (whence == os.SEEK_CUR):\n        self._cursor += offset\n    elif (whence == os.SEEK_END):\n        self._cursor = (self.size() + offset)\n    else:\n        raise ValueError('Unexpected value for `whence`: {}'.format(whence))\n", "label": 0}
{"function": "\n\ndef _eval_is_infinite(self):\n    if any((a.is_infinite for a in self.args)):\n        if any((a.is_zero for a in self.args)):\n            return S.NaN.is_infinite\n        if any(((a.is_zero is None) for a in self.args)):\n            return None\n        return True\n", "label": 1}
{"function": "\n\ndef create_security_groups(self):\n    for hostdef in self.blueprint.host_definitions.all():\n        sg_name = 'stackdio-managed-{0}-stack-{1}'.format(hostdef.slug, self.pk)\n        sg_description = 'stackd.io managed security group'\n        account = hostdef.cloud_image.account\n        if (not account.create_security_groups):\n            logger.debug('Skipping creation of {0} because security group creation is turned off for the account'.format(sg_name))\n            continue\n        driver = account.get_driver()\n        try:\n            sg_id = driver.create_security_group(sg_name, sg_description, delete_if_exists=True)\n        except Exception as e:\n            err_msg = 'Error creating security group: {0}'.format(str(e))\n            self.set_status('create_security_groups', self.ERROR, err_msg, Level.ERROR)\n        logger.debug('Created security group {0}: {1}'.format(sg_name, sg_id))\n        for access_rule in hostdef.access_rules.all():\n            driver.authorize_security_group(sg_id, {\n                'protocol': access_rule.protocol,\n                'from_port': access_rule.from_port,\n                'to_port': access_rule.to_port,\n                'rule': access_rule.rule,\n            })\n        self.security_groups.create(account=account, blueprint_host_definition=hostdef, name=sg_name, description=sg_description, group_id=sg_id, is_managed=True)\n", "label": 0}
{"function": "\n\ndef test_perturb_inv(self):\n    pmat = perturb_inv(closure(self.cdata1), closure([0.1, 0.1, 0.1]))\n    imat = perturb(closure(self.cdata1), closure([10, 10, 10]))\n    npt.assert_allclose(pmat, imat)\n    pmat = perturb_inv(closure(self.cdata1), closure([1, 1, 1]))\n    npt.assert_allclose(pmat, closure([[0.2, 0.2, 0.6], [0.4, 0.4, 0.2]]))\n    pmat = perturb_inv(closure(self.cdata5), closure([0.1, 0.1, 0.1]))\n    imat = perturb(closure(self.cdata1), closure([10, 10, 10]))\n    npt.assert_allclose(pmat, imat)\n    with self.assertRaises(ValueError):\n        perturb_inv(closure(self.cdata1), self.bad1)\n    perturb_inv(self.cdata2, [1, 2, 3])\n    npt.assert_allclose(self.cdata2, np.array([2, 2, 6]))\n", "label": 0}
{"function": "\n\ndef get_style(self, attribute):\n    \"Get the document's named style at the caret's current position.\\n\\n        If there is a text selection and the style varies over the selection,\\n        `pyglet.text.document.STYLE_INDETERMINATE` is returned.\\n\\n        :Parameters:\\n            `attribute` : str\\n                Name of style attribute to retrieve.  See\\n                `pyglet.text.document` for a list of recognised attribute\\n                names.\\n\\n        :rtype: object\\n        \"\n    if ((self._mark is None) or (self._mark == self._position)):\n        try:\n            return self._next_attributes[attribute]\n        except KeyError:\n            return self._layout.document.get_style(attribute, self._position)\n    start = min(self._position, self._mark)\n    end = max(self._position, self._mark)\n    return self._layout.document.get_style_range(attribute, start, end)\n", "label": 0}
{"function": "\n\ndef __call__(self, feature=None):\n    if (not current_app):\n        log.warn(\"Got a request to check for {feature} but we're outside the request context. Returning False\".format(feature=feature))\n        return False\n    try:\n        return self.model.check(feature)\n    except NoResultFound:\n        raise NoFeatureFlagFound()\n", "label": 0}
{"function": "\n\ndef run_and_expect(self, joined_params, retcode, extra_args=['--local-scheduler', '--no-lock']):\n    with self.assertRaises(SystemExit) as cm:\n        luigi_run((joined_params.split(' ') + extra_args))\n    self.assertEqual(cm.exception.code, retcode)\n", "label": 0}
{"function": "\n\ndef process(self, value):\n    return format_date_time_sqlite(value)\n", "label": 0}
{"function": "\n\ndef populate_link(self, finder, upgrade):\n    'Ensure that if a link can be found for this, that it is found.\\n\\n        Note that self.link may still be None - if Upgrade is False and the\\n        requirement is already installed.\\n        '\n    if (self.link is None):\n        self.link = finder.find_requirement(self, upgrade)\n", "label": 0}
{"function": "\n\ndef get_list_display_links(self, request, list_display):\n    '\\n        Return a sequence containing the fields to be displayed as links\\n        on the changelist. The list_display parameter is the list of fields\\n        returned by get_list_display().\\n        '\n    if (self.list_display_links or (self.list_display_links is None) or (not list_display)):\n        return self.list_display_links\n    else:\n        return list(list_display)[:1]\n", "label": 0}
{"function": "\n\ndef __init__(self, canvas, pf, config):\n    super(Win32CanvasConfig, self).__init__(canvas, config)\n    self._pf = pf\n    self._pfd = PIXELFORMATDESCRIPTOR()\n    _gdi32.DescribePixelFormat(canvas.hdc, self._pf, sizeof(PIXELFORMATDESCRIPTOR), byref(self._pfd))\n    self.double_buffer = bool((self._pfd.dwFlags & PFD_DOUBLEBUFFER))\n    self.sample_buffers = 0\n    self.samples = 0\n    self.stereo = bool((self._pfd.dwFlags & PFD_STEREO))\n    self.buffer_size = self._pfd.cColorBits\n    self.red_size = self._pfd.cRedBits\n    self.green_size = self._pfd.cGreenBits\n    self.blue_size = self._pfd.cBlueBits\n    self.alpha_size = self._pfd.cAlphaBits\n    self.accum_red_size = self._pfd.cAccumRedBits\n    self.accum_green_size = self._pfd.cAccumGreenBits\n    self.accum_blue_size = self._pfd.cAccumBlueBits\n    self.accum_alpha_size = self._pfd.cAccumAlphaBits\n    self.depth_size = self._pfd.cDepthBits\n    self.stencil_size = self._pfd.cStencilBits\n    self.aux_buffers = self._pfd.cAuxBuffers\n", "label": 0}
{"function": "\n\ndef merge_bins(distribution, limit):\n    'Merges the bins of a regression distribution to the given limit number\\n\\n    '\n    length = len(distribution)\n    if ((limit < 1) or (length <= limit) or (length < 2)):\n        return distribution\n    index_to_merge = 2\n    shortest = float('inf')\n    for index in range(1, length):\n        distance = (distribution[index][0] - distribution[(index - 1)][0])\n        if (distance < shortest):\n            shortest = distance\n            index_to_merge = index\n    new_distribution = distribution[:(index_to_merge - 1)]\n    left = distribution[(index_to_merge - 1)]\n    right = distribution[index_to_merge]\n    new_bin = [(((left[0] * left[1]) + (right[0] * right[1])) / (left[1] + right[1])), (left[1] + right[1])]\n    new_distribution.append(new_bin)\n    if (index_to_merge < (length - 1)):\n        new_distribution.extend(distribution[(index_to_merge + 1):])\n    return merge_bins(new_distribution, limit)\n", "label": 1}
{"function": "\n\ndef __set_db_what(self, what):\n    self._db_what = what\n    self.is_dirty = True\n", "label": 0}
{"function": "\n\ndef extendMarkdown(self, md, md_globals):\n    ' Replace subscript with SubscriptPattern '\n    md.inlinePatterns.add('subscript', SimpleTagPattern(SUBSCRIPT_RE, 'sub'), '<not_strong')\n", "label": 0}
{"function": "\n\ndef _services_dns_createRecord_php_WITH_EXTRA_PARAMS(self, method, url, body, headers):\n    body = self.fixtures.load('create_record_WITH_EXTRA_PARAMS.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 0}
{"function": "\n\n@defer.inlineCallbacks\ndef test_GetMoreCount(self):\n    counter = _CallCounter(MongoClientProtocol.send_GETMORE)\n    self.patch(MongoClientProtocol, 'send_GETMORE', counter)\n    (yield self.coll.insert([{\n        'x': 42,\n    } for _ in range(20)]))\n    result = (yield self.coll.find({\n        \n    }, limit=10))\n    self.assertEqual(len(result), 10)\n    self.assertEqual(counter.call_count, 0)\n", "label": 0}
{"function": "\n\ndef print_selection(self, *e):\n    if (self._root is None):\n        return\n    if (self._selection is None):\n        tkinter.messagebox.showerror('Print Error', 'No tree selected')\n    else:\n        c = self._cframe.canvas()\n        for widget in self._treewidgets:\n            if (widget is not self._selection):\n                self._cframe.destroy_widget(widget)\n        c.delete(self._selectbox)\n        (x1, y1, x2, y2) = self._selection.bbox()\n        self._selection.move((10 - x1), (10 - y1))\n        c['scrollregion'] = ('0 0 %s %s' % (((x2 - x1) + 20), ((y2 - y1) + 20)))\n        self._cframe.print_to_file()\n        self._treewidgets = [self._selection]\n        self.clear()\n        self.update()\n", "label": 0}
{"function": "\n\ndef smart_split(text):\n    '\\n    Generator that splits a string by spaces, leaving quoted phrases together.\\n    Supports both single and double quotes, and supports escaping quotes with\\n    backslashes. In the output, strings will keep their initial and trailing\\n    quote marks and escaped quotes will remain escaped (the results can then\\n    be further processed with unescape_string_literal()).\\n\\n    >>> list(smart_split(r\\'This is \"a person\\\\\\'s\" test.\\'))\\n    [\\'This\\', \\'is\\', \\'\"a person\\\\\\\\\\\\\\'s\"\\', \\'test.\\']\\n    >>> list(smart_split(r\"Another \\'person\\\\\\'s\\' test.\"))\\n    [\\'Another\\', \"\\'person\\\\\\\\\\'s\\'\", \\'test.\\']\\n    >>> list(smart_split(r\\'A \"\\\\\"funky\\\\\" style\" test.\\'))\\n    [\\'A\\', \\'\"\\\\\\\\\"funky\\\\\\\\\" style\"\\', \\'test.\\']\\n    '\n    text = force_text(text)\n    for bit in smart_split_re.finditer(text):\n        (yield bit.group(0))\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('model_class', (ModelWithVanillaMoneyField, ModelWithChoicesMoneyField))\ndef test_currency_querying(self, model_class):\n    model_class.objects.create(money=Money('100.0', moneyed.ZWN))\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.USD)).count() == 0)\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.ZWN)).count() == 1)\n", "label": 0}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    return shapes\n", "label": 0}
{"function": "\n\ndef put(self, key, value):\n    ' Updates or inserts data for a specified key '\n    url = ((self.base_url + '/') + str(key))\n    headers = {\n        'content-type': 'application/json',\n    }\n    jvalue = jsonpickle.encode(value)\n    data = self.session.put(url, data=jvalue, headers=headers)\n    logging.debug(('Sending request to ' + url))\n    if (data.status_code == 200):\n        logging.debug(((('The value ' + str(value)) + ' was put in the region for the key ') + str(key)))\n        return True\n    else:\n        self.error_response(data)\n", "label": 0}
{"function": "\n\ndef utcoffset(self, dt):\n    'Returns minutesEast from the constructor, as a datetime.timedelta.'\n    return self.offset\n", "label": 0}
{"function": "\n\ndef tearDown(self):\n    for fname in os.listdir(self.tempdir):\n        os.remove(os.path.join(self.tempdir, fname))\n    os.rmdir(self.tempdir)\n", "label": 0}
{"function": "\n\n@property\ndef responses(self):\n    return [response for (request, response) in self.data]\n", "label": 0}
{"function": "\n\ndef _create_hdfs_workflow_dir(self, where, job):\n    constructed_dir = ('/user/%s/' % self.get_hdfs_user())\n    constructed_dir = self._add_postfix(constructed_dir)\n    constructed_dir += ('%s/%s' % (job.name, six.text_type(uuid.uuid4())))\n    with remote.get_remote(where) as r:\n        self.create_hdfs_dir(r, constructed_dir)\n    return constructed_dir\n", "label": 0}
{"function": "\n\ndef _delegate_getter(self, object, name):\n    return getattr(self.delegate, self.name)\n", "label": 0}
{"function": "\n\n@mock.patch('pushmanager.servlets.testtag.urllib2.urlopen')\ndef test_generate_test_tag_no_url(self, mock_urlopen):\n    m = mock.Mock()\n    m.read.side_effect = ['{\"tag\" : \"tag 0 fails\"}', '{\"url\" : \"\"}']\n    mock_urlopen.return_value = m\n    MockedSettings['tests_tag'] = {\n        \n    }\n    MockedSettings['tests_tag']['tag'] = 'test'\n    MockedSettings['tests_tag']['tag_api_endpoint'] = 'example.com'\n    MockedSettings['tests_tag']['tag_api_body'] = '{ \"sha\" : \"%SHA%\" }'\n    MockedSettings['tests_tag']['url_api_endpoint'] = 'http://example.com/api/v1/test_results_url'\n    MockedSettings['tests_tag']['url_api_body'] = '{ \"sha\" : \"%SHA%\" }'\n    MockedSettings['tests_tag']['url_tmpl'] = 'www.example.com/%ID%'\n    request_info = {\n        'tags': 'test',\n        'branch': 'test',\n        'revision': 'abc123',\n    }\n    with mock.patch.dict(Settings, MockedSettings):\n        gen_tags = TestTagServlet._gen_test_tag_resp(request_info)\n        T.assert_equals({\n            'tag': 'tag 0 fails',\n            'url': '',\n        }, gen_tags)\n", "label": 0}
{"function": "\n\ndef _process_element(self, element):\n    'Process first level element of the stream.\\n\\n        The element may be stream error or features, StartTLS\\n        request/response, SASL request/response or a stanza.\\n\\n        :Parameters:\\n            - `element`: XML element\\n        :Types:\\n            - `element`: :etree:`ElementTree.Element`\\n        '\n    tag = element.tag\n    if (tag in self._element_handlers):\n        handler = self._element_handlers[tag]\n        logger.debug('Passing element {0!r} to method {1!r}'.format(element, handler))\n        handled = handler(self, element)\n        if handled:\n            return\n    if tag.startswith(self._stanza_namespace_p):\n        stanza = stanza_factory(element, self, self.language)\n        self.uplink_receive(stanza)\n    elif (tag == ERROR_TAG):\n        error = StreamErrorElement(element)\n        self.process_stream_error(error)\n    elif (tag == FEATURES_TAG):\n        logger.debug('Got features element: {0}'.format(serialize(element)))\n        self._got_features(element)\n    else:\n        logger.debug('Unhandled element: {0}'.format(serialize(element)))\n        logger.debug(' known handlers: {0!r}'.format(self._element_handlers))\n", "label": 1}
{"function": "\n\ndef relax():\n    selection = pm.ls(sl=1)\n    if (not selection):\n        return\n    verts = pm.ls(pm.polyListComponentConversion(tv=1))\n    if (not verts):\n        return\n    shape = verts[0].node()\n    dup = shape.duplicate()[0]\n    dup_shape = dup.getShape()\n    pm.polyAverageVertex(verts, i=1, ch=0)\n    ta_node = pm.transferAttributes(dup, verts, transferPositions=True, transferNormals=False, transferUVs=False, transferColors=False, sampleSpace=0, searchMethod=0, flipUVs=False, colorBorders=1)\n    pm.delete(shape, ch=1)\n    pm.delete(dup)\n    pm.select(selection)\n", "label": 0}
{"function": "\n\ndef archive(self):\n    'Archives an experiment'\n    pipe = self.redis.pipeline(transaction=True)\n    pipe.srem(ACTIVE_EXPERIMENTS_REDIS_KEY, self.name)\n    pipe.sadd(ARCHIVED_EXPERIMENTS_REDIS_KEY, self.name)\n    pipe.execute()\n", "label": 0}
{"function": "\n\ndef mutable_total_billed_ops(self, i):\n    return self.total_billed_ops_[i]\n", "label": 0}
{"function": "\n\ndef get_diffs(self, commit):\n    return commit.parents[0].diff(commit, create_patch=True)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/vehicle/military/shared_military_c.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef test_tee_del_backward(self):\n    (forward, backward) = tee(repeat(None, 20000000))\n    any(forward)\n    del backward\n", "label": 0}
{"function": "\n\ndef test_stats(self):\n    (key, stats) = self.memcache.get_stats()[0]\n    self.assertEqual('127.0.0.1:21122 (1)', key)\n    keys = ['bytes', 'pid', 'time', 'limit_maxbytes', 'cmd_get', 'version', 'bytes_written', 'cmd_set', 'get_misses', 'total_connections', 'curr_connections', 'curr_items', 'uptime', 'get_hits', 'total_items', 'rusage_system', 'rusage_user', 'bytes_read']\n    for key in keys:\n        self.assert_(stats.has_key(key), (\"key '%s' is not in stats\" % key))\n", "label": 0}
{"function": "\n\ndef onMouseUp(self, sender, x, y):\n    (x, y) = self.adjustMousePos(x, y)\n    if self.mousehandler:\n        self.mousetarget.onBrowserEvent(DOM.eventGetCurrentEvent())\n    else:\n        self.mousetarget.onMouseUp(sender, x, y)\n", "label": 0}
{"function": "\n\ndef query_lookupd(self):\n    self.logger.debug('querying lookupd...')\n    lookupd = next(self.iterlookupds)\n    try:\n        producers = lookupd.lookup(self.topic)['producers']\n        self.logger.debug(('found %d producers' % len(producers)))\n    except Exception as error:\n        msg = 'Failed to lookup %s on %s (%s)'\n        self.logger.warn((msg % (self.topic, lookupd.address, error)))\n        return\n    for producer in producers:\n        conn = Nsqd((producer.get('broadcast_address') or producer['address']), producer['tcp_port'], producer['http_port'], **self.conn_kwargs)\n        self.connect_to_nsqd(conn)\n", "label": 0}
{"function": "\n\ndef get_infra_name(host_id):\n    'Return DATABASE_INFRA_NAME'\n    from physical.models import Host\n    host = Host.objects.filter(id=host_id).select_related('instance').select_related('databaseinfra')\n    try:\n        host = host[0]\n    except IndexError as e:\n        LOG.warn('Host id does not exists: {}. {}'.format(host_id, e))\n        return None\n    return host.instance_set.all()[0].databaseinfra.name\n", "label": 0}
{"function": "\n\ndef buildIndex(self, l):\n    index = self.mIndex()\n    for (start, end, value) in self.l:\n        index.add(start, end)\n    return index\n", "label": 0}
{"function": "\n\ndef __init__(self, uri):\n    self.client = pymongo.MongoClient(uri)\n    self.cache = {\n        \n    }\n    self.uri = uri\n", "label": 0}
{"function": "\n\ndef test_escaping(self):\n    text = '<p>Hello World!'\n    app = flask.Flask(__name__)\n\n    @app.route('/')\n    def index():\n        return flask.render_template('escaping_template.html', text=text, html=flask.Markup(text))\n    lines = app.test_client().get('/').data.splitlines()\n    self.assert_equal(lines, ['&lt;p&gt;Hello World!', '<p>Hello World!', '<p>Hello World!', '<p>Hello World!', '&lt;p&gt;Hello World!', '<p>Hello World!'])\n", "label": 0}
{"function": "\n\ndef paid_totals_for(self, year, month):\n    return self.during(year, month).filter(paid=True).aggregate(total_amount=models.Sum('amount'), total_fee=models.Sum('fee'), total_refunded=models.Sum('amount_refunded'))\n", "label": 0}
{"function": "\n\ndef visit_Assign(self, node):\n    left = self.visit(node.left)\n    right = self.visit(node.right)\n    ldelay = (self.visit(node.ldelay.value) if (node.ldelay is not None) else None)\n    rdelay = (self.visit(node.rdelay.value) if (node.rdelay is not None) else None)\n    subst = vtypes.Subst(left, right, ldelay=ldelay, rdelay=rdelay)\n    assign = vtypes.Assign(subst)\n    self.add_object(assign)\n    return assign\n", "label": 0}
{"function": "\n\ndef copy(self):\n    res = LoopType()\n    for (key, value) in self.__dict__.iteritems():\n        setattr(res, key, value)\n    return res\n", "label": 0}
{"function": "\n\ndef test_object_list_delimiter(self):\n    self.requests_mock.register_uri('GET', (FAKE_URL + '/qaz?delimiter=%7C'), json=LIST_OBJECT_RESP, status_code=200)\n    ret = self.api.object_list(container='qaz', delimiter='|')\n    self.assertEqual(LIST_OBJECT_RESP, ret)\n", "label": 0}
{"function": "\n\ndef test_list_switch_machines(self):\n    url = '/switches/2/machines'\n    return_value = self.get(url)\n    resp = json.loads(return_value.get_data())\n    count = len(resp)\n    self.assertEqual(count, 2)\n    self.assertEqual(return_value.status_code, 200)\n    url = '/switches/99/machines'\n    return_value = self.get(url)\n    self.assertEqual(return_value.status_code, 404)\n", "label": 0}
{"function": "\n\ndef full_clean(self):\n    '\\n        Cleans all of self.data and populates self._errors.\\n        '\n    self._errors = []\n    if (not self.is_bound):\n        return\n    for i in range(0, self.total_form_count()):\n        form = self.forms[i]\n        self._errors.append(form.errors)\n    try:\n        self.clean()\n    except ValidationError as e:\n        self._non_form_errors = self.error_class(e.messages)\n", "label": 0}
{"function": "\n\n@retry()\ndef node_list(self):\n    return [item.name() for item in self.conn.listAllDomains()]\n", "label": 0}
{"function": "\n\n@classmethod\ndef convert_json(cls, d, convert):\n    new_d = {\n        \n    }\n    for (k, v) in d.iteritems():\n        new_d[convert(k)] = (cls.convert_json(v, convert) if isinstance(v, dict) else v)\n    return new_d\n", "label": 0}
{"function": "\n\ndef add_dependency_links(self, links):\n    if self.process_dependency_links:\n        warnings.warn('Dependency Links processing has been deprecated and will be removed in a future release.', RemovedInPip8Warning)\n        self.dependency_links.extend(links)\n", "label": 0}
{"function": "\n\ndef test_deepcopy_shared_container(self):\n    (a, x) = T.scalars('ax')\n    h = function([In(a, value=0.0)], a)\n    f = function([x, In(a, value=h.container[a], implicit=True)], (x + a))\n    try:\n        memo = {\n            \n        }\n        ac = copy.deepcopy(a)\n        memo.update({\n            id(a): ac,\n        })\n        hc = copy.deepcopy(h, memo=memo)\n        memo.update({\n            id(h): hc,\n        })\n        fc = copy.deepcopy(f, memo=memo)\n    except NotImplementedError as e:\n        if e[0].startswith('DebugMode is not picklable'):\n            return\n        else:\n            raise\n    h[a] = 1\n    hc[ac] = 2\n    self.assertTrue((f[a] == 1))\n    self.assertTrue((fc[ac] == 2))\n", "label": 0}
{"function": "\n\ndef test_rerun_after_depletion_calls_once(self):\n    'Ensure MessageIterator works when used manually.'\n    from furious.batcher import MessageIterator\n    payload = '[\"test\"]'\n    task = Mock(payload=payload, tag='tag')\n    iterator = MessageIterator('tag', 'qn', 1)\n    with patch.object(iterator, 'queue') as queue:\n        queue.lease_tasks_by_tag.return_value = [task]\n        results = [payload for payload in iterator]\n        self.assertEqual(results, [payload])\n        results = [payload for payload in iterator]\n    queue.lease_tasks_by_tag.assert_called_once_with(60, 1, tag='tag', deadline=10)\n", "label": 0}
{"function": "\n\ndef run(self, suite):\n    filtered_test = FilterSuite(suite, self.ShouldTestRun)\n    return super(_RunnerImpl, self).run(filtered_test)\n", "label": 0}
{"function": "\n\ndef _untagged_response(self, typ, dat, name):\n    if (typ == 'NO'):\n        return (typ, dat)\n    data = self._get_untagged_response(name)\n    if (not data):\n        return (typ, [None])\n    while True:\n        dat = self._get_untagged_response(name)\n        if (not dat):\n            break\n        data += dat\n    if __debug__:\n        self._log(4, ('_untagged_response(%s, ?, %s) => %.80r' % (typ, name, data)))\n    return (typ, data)\n", "label": 1}
{"function": "\n\n@mock.patch.object(shade.OpenStackCloud, 'search_subnets')\n@mock.patch.object(shade.OpenStackCloud, 'neutron_client')\ndef test_delete_subnet_not_found(self, mock_client, mock_search):\n    mock_search.return_value = []\n    r = self.cloud.delete_subnet('goofy')\n    self.assertFalse(r)\n    self.assertFalse(mock_client.delete_subnet.called)\n", "label": 0}
{"function": "\n\ndef get_conf_from_module(mod):\n    'return configuration from module with defaults no worry about None type\\n\\n    '\n    conf = ModuleConfig(CONF_SPEC)\n    mod = _get_correct_module(mod)\n    conf.set_module(mod)\n    if hasattr(mod, 'default'):\n        default = mod.default\n        conf = extract_conf_from(default, conf)\n    else:\n        conf = extract_conf_from(mod, conf)\n    return conf\n", "label": 0}
{"function": "\n\ndef onBeforeTabSelected(self, sender, tabIndex):\n    if (self.fTabs.getWidgetCount() == 6):\n        self.fTabs.add(HTML('2nd Test.<br />Tab should be on right'), '2nd Test', name='test2')\n        return True\n    self.fTabs.remove('test2')\n    return (tabIndex != 6)\n", "label": 0}
{"function": "\n\ndef process(args):\n    conduit = phlsys_makeconduit.make_conduit(args.uri, args.user, args.cert, args.act_as_user)\n    if args.diff_id:\n        diff_id = args.diff_id\n    else:\n        d = {\n            'diff': args.raw_diff_file.read(),\n        }\n        diff_id = conduit('differential.createrawdiff', d)['id']\n    fields = {\n        \n    }\n    d = {\n        'id': args.revision_id,\n        'diffid': diff_id,\n        'fields': fields,\n        'message': args.message,\n    }\n    MessageFields = phlcon_differential.MessageFields\n    args.ccs = _get_set_or_none(args.ccs)\n    args.reviewers = _get_set_or_none(args.reviewers)\n    if args.reviewers:\n        fields[MessageFields.reviewer_phids] = args.reviewers\n    if args.ccs:\n        fields[MessageFields.cc_phids] = args.ccs\n    user_phids = phlcon_user.UserPhidCache(conduit)\n    for users in fields.itervalues():\n        user_phids.add_hint_list(users)\n    for key in fields.iterkeys():\n        fields[key] = [user_phids.get_phid(u) for u in fields[key]]\n    result = conduit('differential.updaterevision', d)\n    if args.format_id:\n        print(result['revisionid'])\n    elif args.format_url:\n        print(result['uri'])\n    else:\n        print(\"Updated revision '{rev_id}', you can view it at this URL:\\n  {url}\".format(rev_id=result['revisionid'], url=result['uri']))\n", "label": 1}
{"function": "\n\ndef update(self, action, action_id):\n    updated_action = {\n        \n    }\n    updated_action['freezer_action'] = utils.create_dict(**action)\n    try:\n        if (action['mandatory'] != ''):\n            updated_action['mandatory'] = action['mandatory']\n    except KeyError:\n        pass\n    try:\n        if (action['max_retries'] != ''):\n            updated_action['max_retries'] = action['max_retries']\n    except KeyError:\n        pass\n    try:\n        if (action['max_retries_interval'] != ''):\n            updated_action['max_retries_interval'] = action['max_retries_interval']\n    except KeyError:\n        pass\n    return self.client.actions.update(action_id, updated_action)\n", "label": 1}
{"function": "\n\ndef test_create_handler_with_str_method_name(self):\n\n    @endpoint('/api', 'GET')\n    def fake_handler(request, *args, **kwargs):\n        pass\n    (path, handler, methods, name) = fake_handler()\n    self.assertEqual(methods, 'GET')\n", "label": 0}
{"function": "\n\ndef _prep_loader_attrs(self, mapping):\n    self.loader.source = mapping['generated_filename']\n    self.loader.election_id = mapping['election']\n    self.loader.timestamp = datetime.datetime.now()\n", "label": 0}
{"function": "\n\ndef test_remove_unpickables_http_exception(self):\n    try:\n        urllib2.urlopen('http://localhost/this.does.not.exist')\n        self.fail('exception expected')\n    except urllib2.URLError as e:\n        pass\n    except urllib2.HTTPError as e:\n        pass\n    removed = mapper.remove_unpickables(e)\n    pickled = pickle.dumps(removed)\n    pickle.loads(pickled)\n", "label": 0}
{"function": "\n\ndef test_has_error_type(self):\n    error = spotify.LibError(0)\n    self.assertEqual(error.error_type, 0)\n    error = spotify.LibError(1)\n    self.assertEqual(error.error_type, 1)\n", "label": 0}
{"function": "\n\n@classmethod\ndef make(cls, value, cache=None, timeout=None):\n    self = CacheKey(value)\n    self.cache = cache\n    self.timeout = timeout\n    return self\n", "label": 0}
{"function": "\n\ndef test_stdinCache_trailing_backslash_3(self):\n    stdinCache = StdinCache.StdinCache()\n    stdinCache.refreshFromText(dedentAndStrip('\\n                x+                z+                y\\n                f+                g+                h\\n                '))\n    self.assertEqual(len(stdinCache.blocks), 2)\n", "label": 0}
{"function": "\n\n@property\ndef parents(self):\n    if (self._parents is None):\n        self._parents = [self._odb.get_commit(hash) for hash in self._obj.parents]\n    return list(self._parents)\n", "label": 0}
{"function": "\n\ndef process_request(self, request):\n    if (not getattr(settings, 'MOBILE_DOMAIN', False)):\n        return\n    if ((request.COOKIES.get('ismobile', '0') == '1') or (('HTTP_USER_AGENT' in request.META) and (request.COOKIES.get('isbrowser', '0') != '1') and is_mobile(request.META['HTTP_USER_AGENT']))):\n        redirect = settings.MOBILE_DOMAIN\n        if getattr(settings, 'MOBILE_REDIRECT_PRESERVE_URL', False):\n            redirect = (redirect.rstrip('/') + request.path_info)\n        response = HttpResponseRedirect(redirect)\n        max_age = getattr(settings, 'MOBILE_COOKIE_MAX_AGE', DEFAULT_COOKIE_MAX_AGE)\n        expires_time = (time.time() + max_age)\n        expires = cookie_date(expires_time)\n        response.set_cookie('ismobile', '1', domain=settings.SESSION_COOKIE_DOMAIN, max_age=max_age, expires=expires)\n        return response\n", "label": 1}
{"function": "\n\ndef setHighlighted(self, highlighted):\n    GafferUI.PlugValueWidget.setHighlighted(self, highlighted)\n    self.__boolWidget.setHighlighted(highlighted)\n", "label": 0}
{"function": "\n\ndef gen_key(self, prefix=None):\n    if (not prefix):\n        prefix = 'python-couchbase-key_'\n    ret = '{0}{1}'.format(prefix, self._key_counter)\n    self._key_counter += 1\n    return ret\n", "label": 0}
{"function": "\n\ndef test_functions_unchanged(self):\n    s = 'def foo(): pass'\n    self.unchanged(s, from3=True)\n    s = '\\n        def foo():\\n            pass\\n            pass\\n        '\n    self.unchanged(s, from3=True)\n    s = \"\\n        def foo(bar='baz'):\\n            pass\\n            pass\\n        \"\n    self.unchanged(s, from3=True)\n", "label": 0}
{"function": "\n\ndef test_project_add_child(self):\n    project = Project()\n    child = Task()\n    project.add_child(child)\n    self.assertEquals(project, child.parent)\n    self.assertEquals(project, child.project)\n    project = Project()\n    child = Task()\n    grandchild = Task()\n    child.add_child(grandchild)\n    project.add_child(child)\n    self.assertEquals(project, child.parent)\n    self.assertEquals(project, child.project)\n    self.assertEquals(project, grandchild.project)\n", "label": 0}
{"function": "\n\ndef RegisterSubException(self, hunt_urn, plugin_name, exception):\n    self.exceptions_by_hunt.setdefault(hunt_urn, {\n        \n    }).setdefault(plugin_name, []).append(exception)\n", "label": 0}
{"function": "\n\ndef test_issue_1264(self):\n    n = 100\n    x = np.random.uniform(size=(n * 3)).reshape((n, 3))\n    expected = distance_matrix(x)\n    actual = njit(distance_matrix)(x)\n    np.testing.assert_array_almost_equal(expected, actual)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(CreateFlavorInfoAction, self).clean()\n    name = cleaned_data.get('name')\n    flavor_id = cleaned_data.get('flavor_id')\n    try:\n        flavors = api.nova.flavor_list(self.request, None)\n    except Exception:\n        flavors = []\n        msg = _('Unable to get flavor list')\n        exceptions.check_message(['Connection', 'refused'], msg)\n        raise\n    if (flavors is not None):\n        for flavor in flavors:\n            if (flavor.name == name):\n                raise forms.ValidationError((_('The name \"%s\" is already used by another flavor.') % name))\n            if (flavor.id == flavor_id):\n                raise forms.ValidationError((_('The ID \"%s\" is already used by another flavor.') % flavor_id))\n    return cleaned_data\n", "label": 1}
{"function": "\n\ndef test_start_run(self):\n    assert_false(os.path.exists(self.result_file_path))\n    self.run_results.start_run(self.scenario)\n    assert_equal(len(self.scenario.packb()), self._current_size())\n    assert_greater(self._current_size(), 0)\n    with open(self.result_file_path, 'rb') as f:\n        unpacker = msgpack.Unpacker(file_like=f)\n        got_scenario = Scenario.unpackb(unpacker)\n        for attr in ['name', '_scenario_data', 'user_count', 'operation_count', 'run_seconds', 'container_base', 'container_count', 'containers', 'container_concurrency', 'sizes_by_name', 'version', 'bench_size_thresholds']:\n            assert_equal(getattr(got_scenario, attr), getattr(self.scenario, attr))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef info(msg, *args):\n    ((print >> sys.stderr), (msg % args))\n", "label": 0}
{"function": "\n\ndef Layout(self, request, response):\n    self.default = str(self.descriptor.type().Generate())\n    response = super(AES128KeyFormRenderer, self).Layout(request, response)\n    return self.CallJavascript(response, 'AES128KeyFormRenderer.Layout', prefix=self.prefix)\n", "label": 0}
{"function": "\n\ndef new(self, user, repo, title, body=''):\n    'Create a new issue.'\n    return self._posted('/'.join(['issues', 'open', user, repo]), title=title, body=body)\n", "label": 0}
{"function": "\n\ndef __init__(self, n):\n    assert (n == 1)\n", "label": 0}
{"function": "\n\ndef _str_allocation_pools(allocation_pools):\n    if isinstance(allocation_pools, str):\n        return allocation_pools\n    return '\\n'.join([('%s,%s' % (pool['start'], pool['end'])) for pool in allocation_pools])\n", "label": 0}
{"function": "\n\ndef _put_n_deployments(self, id_prefix, number_of_deployments, skip_creation=None, add_modification=None):\n    for i in range(0, number_of_deployments):\n        deployment_id = '{0}{1}_{2}'.format(id_prefix, str(i), 'deployment')\n        blueprint_id = '{0}{1}_{2}'.format(id_prefix, str(i), 'blueprint')\n        if (not skip_creation):\n            self.put_deployment(deployment_id=deployment_id, blueprint_id=blueprint_id)\n        if add_modification:\n            response = self._put_deployment_modification(deployment_id=deployment_id)\n            self._mark_deployment_modification_finished(modification_id=response['id'])\n", "label": 0}
{"function": "\n\ndef get_extractor(coarse, fine):\n    log.debug(\"getting fine extractor for '{}: {}'\".format(coarse, fine))\n    try:\n        extractor = importlib.import_module(((__package__ + '.') + question_types[fine]))\n    except (ImportError, KeyError):\n        log.warn(\"Extractor for fine type '{}: {}' not implemented\".format(coarse, fine))\n        raise NoExtractorError(coarse, fine)\n    return extractor.Extractor\n", "label": 0}
{"function": "\n\ndef _eval_rewrite_as_FallingFactorial(self, n, k):\n    if k.is_integer:\n        return (ff(n, k) / factorial(k))\n", "label": 0}
{"function": "\n\n@classmethod\ndef _parse_repo(cls, repo, name=None):\n    regexp = '(?P<type>deb(-src)?) (?P<uri>[^\\\\s]+) (?P<suite>[^\\\\s]+)( (?P<section>[\\\\w\\\\s]*))?(,(?P<priority>[\\\\d]+))?'\n    match = re.match(regexp, repo)\n    if (not match):\n        raise errors.IncorrectRepository(\"Couldn't parse repository '{0}'\".format(repo))\n    repo_type = match.group('type')\n    repo_suite = match.group('suite')\n    repo_section = match.group('section')\n    repo_uri = match.group('uri')\n    repo_priority = match.group('priority')\n    return {\n        'name': name,\n        'type': repo_type,\n        'uri': repo_uri,\n        'priority': repo_priority,\n        'suite': repo_suite,\n        'section': (repo_section or ''),\n    }\n", "label": 0}
{"function": "\n\ndef register_category(self, category, label, index=None):\n    if index:\n        self._categories.insert(index, category, label)\n    else:\n        self._categories[category] = label\n", "label": 0}
{"function": "\n\ndef get_formsets_with_inlines(self, request, obj=None):\n    if (request.is_add_view and (obj is not None)):\n        raise Exception(\"'obj' passed to get_formsets_with_inlines wasn't None during add_view\")\n    if ((not request.is_add_view) and (obj is None)):\n        raise Exception(\"'obj' passed to get_formsets_with_inlines was None during change_view\")\n    return super(GetFormsetsArgumentCheckingAdmin, self).get_formsets_with_inlines(request, obj)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    super(RequestPageTypeFormSet, self).clean()\n    cnt_rpts = 0\n    cnt_rpts_mp = 0\n    cnt_rpts_dp = 0\n    for form in self.forms:\n        if (not hasattr(form, 'cleaned_data')):\n            continue\n        data = form.cleaned_data\n        if (('DELETE' in data) and data['DELETE']):\n            continue\n        if (not ('page_type' in data)):\n            continue\n        cnt_rpts += 1\n        pt = data['page_type']\n        if (pt == 'MP'):\n            cnt_rpts_mp += 1\n        else:\n            cnt_rpts_dp += 1\n    if (cnt_rpts_mp == 0):\n        raise ValidationError('For every request page type used for scraper elems definition a RequestPageType object with a corresponding page type has to be added!')\n    if (cnt_rpts_mp > 1):\n        raise ValidationError('Only one RequestPageType object for main page requests allowed!')\n", "label": 1}
{"function": "\n\ndef _get_method(self, request, action, content_type, body):\n    'Look up the action-specific method and its extensions.'\n    try:\n        if (not self.controller):\n            meth = getattr(self, action)\n        else:\n            meth = getattr(self.controller, action)\n    except AttributeError:\n        if ((not self.wsgi_actions) or (action not in (_ROUTES_METHODS + ['action']))):\n            raise\n    else:\n        return (meth, self.wsgi_extensions.get(action, []))\n    if (action == 'action'):\n        action_name = action_peek(body)\n    else:\n        action_name = action\n    return (self.wsgi_actions[action_name], self.wsgi_action_extensions.get(action_name, []))\n", "label": 1}
{"function": "\n\ndef test_id(self):\n    'Each test annotation should be created with a unique ID.'\n    annotation_1 = factories.Annotation()\n    annotation_2 = factories.Annotation()\n    assert annotation_1.get('id')\n    assert annotation_2.get('id')\n    assert (annotation_1['id'] != annotation_2['id'])\n", "label": 0}
{"function": "\n\n@property\ndef primary_key_names(self):\n    'Primary keys of the table\\n        '\n    return [c.name for c in self.columns() if c.primary]\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    super(OrderPayment, self).save(*args, **kwargs)\n    self._recalculate_paid()\n    if (self.currency != self.order.currency):\n        self.order.notes += ('\\n' + (_('Currency of payment %s does not match.') % self))\n        self.order.save()\n", "label": 0}
{"function": "\n\ndef __init__(self, activation, dims=None, **kwargs):\n    super(SpeechBottom, self).__init__(**kwargs)\n    self.num_features = self.input_dims['recordings']\n    if (activation is None):\n        activation = Tanh()\n    if dims:\n        child = MLP(([activation] * len(dims)), ([self.num_features] + dims), name='bottom')\n        self.output_dim = child.output_dim\n    else:\n        child = Identity(name='bottom')\n        self.output_dim = self.num_features\n    self.children.append(child)\n    self.mask = tensor.matrix('recordings_mask')\n    self.batch_inputs = {\n        'recordings': tensor.tensor3('recordings'),\n    }\n    self.single_inputs = {\n        'recordings': tensor.matrix('recordings'),\n    }\n", "label": 0}
{"function": "\n\ndef load(self):\n    session_data = self._cache.get(self.session_key)\n    if (session_data is not None):\n        return session_data\n    self.create()\n    return {\n        \n    }\n", "label": 0}
{"function": "\n\ndef test_name_and_description(self):\n    '\\n        Tests that the benefit proxy classes all return a name and\\n        description. Unfortunately, the current implementations means\\n        a valid range is required.\\n        This test became necessary because the complex name/description logic\\n        broke with the python_2_unicode_compatible decorator.\\n        '\n    range = factories.RangeFactory()\n    for (type, __) in Benefit.TYPE_CHOICES:\n        benefit = Benefit(type=type, range=range)\n        self.assertTrue(all([benefit.name, benefit.description, six.text_type(benefit)]))\n", "label": 0}
{"function": "\n\n@defer.deferredGenerator\ndef allapps_action(self, argstr):\n    \"Usage allapps: <method> [args]\\n\\n  dispatch the same command to all application managers.\\n\\n    <method>\\tmethod to invoke on all appmanagers.\\n    [args]\\toptional arguments to pass along.\\n\\n  examples:\\n\\n    ''            #shows help documentation for all applications\\n    'status'      #invoke status assumes there is only one instance\\n    'status all'  #invoke status on all application instances\\n    'status 0'    #invoke status on application instance label '0'\\n\\n  full cli usage:\\n\\n    $ droneblaster allapps\\n    $ droneblaster allapps status\\n    $ droneblaster allapps status all\\n    $ droneblaster allapps status 0\\n\"\n    result = {\n        \n    }\n    descriptions = []\n    code = 0\n    for obj in AppManager.objects:\n        try:\n            action = obj.action\n            if (not action):\n                continue\n            d = action(argstr)\n            wfd = defer.waitForDeferred(d)\n            (yield wfd)\n            foo = wfd.getResult()\n            descriptions.append(foo.get('description', 'None'))\n            code += int(foo.get('code', 0))\n        except:\n            pass\n    result['description'] = '\\n'.join(descriptions)\n    if (not result['description']):\n        result['description'] = 'None'\n    result['code'] = code\n    (yield result)\n", "label": 0}
{"function": "\n\n@test.attr(type='benchmark')\ndef test_002_fill_volume(self):\n    'Fill volume with data'\n    if (self.ctx.ssh is None):\n        raise self.skipException('Booting failed')\n    if (not self.ctx.volume_ready):\n        raise self.skipException('Volume preparation failed')\n    self._start_test()\n    self.ctx.ssh.exec_command('sudo mkdir -m 777 /vol/data')\n    file_lines = (102 * int(self.volume_size))\n    for i in xrange(int(self.volume_fill)):\n        self.ctx.ssh.exec_command((((\"cat /dev/urandom | tr -d -c 'a-zA-Z0-9' | fold -w 1020 | head -n \" + str(file_lines)) + ' > /vol/data/file') + str(i)))\n    self._end_test('Volume filling')\n    self.ctx.volume_filled = True\n    self._check_test()\n", "label": 0}
{"function": "\n\ndef _url_coerce_fn(r):\n    '\\n    :rtype: str\\n    '\n    p = urllib.parse.urlparse(r)\n    if (not p.scheme):\n        raise InvalidInput('Specify an URL scheme (e.g. http://)')\n    if (not p.netloc):\n        raise InvalidInput('Specify a domain (e.g. example.com)')\n    if (p.path and (p.path != '/')):\n        raise InvalidInput('Do not specify a path')\n    if (p.params or p.query or p.fragment):\n        raise InvalidInput('Do not leave trailing elements')\n    if (not p.path):\n        r += '/'\n    r = r.lower()\n    return r\n", "label": 1}
{"function": "\n\ndef _reindent_stats(tokens):\n    \"Return list of (lineno, indentlevel) pairs.\\n\\n    One for each stmt and comment line. indentlevel is -1 for comment lines, as\\n    a signal that tokenize doesn't know what to do about them; indeed, they're\\n    our headache!\\n\\n    \"\n    find_stmt = 1\n    level = 0\n    stats = []\n    for t in tokens:\n        token_type = t[0]\n        sline = t[2][0]\n        line = t[4]\n        if (token_type == tokenize.NEWLINE):\n            find_stmt = 1\n        elif (token_type == tokenize.INDENT):\n            find_stmt = 1\n            level += 1\n        elif (token_type == tokenize.DEDENT):\n            find_stmt = 1\n            level -= 1\n        elif (token_type == tokenize.COMMENT):\n            if find_stmt:\n                stats.append((sline, (- 1)))\n        elif (token_type == tokenize.NL):\n            pass\n        elif find_stmt:\n            find_stmt = 0\n            if line:\n                stats.append((sline, level))\n    return stats\n", "label": 1}
{"function": "\n\n@classmethod\ndef _get_data_source_properties_from_case(cls, case_properties):\n    property_map = {\n        'closed': _('Case Closed'),\n        'user_id': _('User ID Last Updating Case'),\n        'owner_name': _('Case Owner'),\n        'mobile worker': _('Mobile Worker Last Updating Case'),\n    }\n    properties = OrderedDict()\n    for property in case_properties:\n        properties[property] = DataSourceProperty(type='case_property', id=property, column_id=get_column_name(property), text=property_map.get(property, property.replace('_', ' ')), source=property)\n    properties['computed/owner_name'] = cls._get_owner_name_pseudo_property()\n    properties['computed/user_name'] = cls._get_user_name_pseudo_property()\n    return properties\n", "label": 0}
{"function": "\n\n@httprettified\ndef test_likes_with_after(self):\n    HTTPretty.register_uri(HTTPretty.GET, 'https://api.tumblr.com/v2/user/likes', body='{\"meta\": {\"status\": 200, \"msg\": \"OK\"}, \"response\": []}')\n    response = self.client.likes(after=1418684291)\n    assert (response == [])\n", "label": 0}
{"function": "\n\ndef __init__(self, mu, var, **kwargs):\n    (self.mu, self.var) = (None, None)\n    if (not isinstance(mu, Layer)):\n        (self.mu, mu) = (mu, None)\n    if (not isinstance(var, Layer)):\n        (self.var, var) = (var, None)\n    input_lst = [i for i in [mu, var] if (not (i is None))]\n    super(GaussianMarginalLogDensityLayer, self).__init__(input_lst, **kwargs)\n", "label": 0}
{"function": "\n\ndef do_undef(self, t):\n    '\\n        Default handling of a #undef line.\\n        '\n    try:\n        del self.cpp_namespace[t[1]]\n    except KeyError:\n        pass\n", "label": 0}
{"function": "\n\ndef run(self, command_line=''):\n    assert command_line, 'expected non-empty command line'\n    parsed = parse_command_line(command_line)\n    self.window.run_command('tab_control', {\n        'command': 'only',\n        'forced': parsed.command.forced,\n    })\n", "label": 0}
{"function": "\n\ndef confidence(self):\n    \"\\n        Returns a tuple (chi squared, confident) of the experiment. Confident\\n        is simply a boolean specifying whether we're > 95%% sure that the\\n        results are statistically significant.\\n        \"\n    choices = self.choices\n    if (len(choices) >= 2):\n        csq = chi_squared(*choices)\n        confident = (is_confident(csq, len(choices)) if (len(choices) <= 10) else None)\n    else:\n        csq = None\n        confident = False\n    return (csq, confident)\n", "label": 0}
{"function": "\n\ndef compile_function(code, arg_names, local_dict, global_dict, module_dir, compiler='', verbose=1, support_code=None, headers=[], customize=None, type_converters=None, auto_downcast=1, **kw):\n    code = ((ndarray_api_version + '\\n') + code)\n    module_path = function_catalog.unique_module_name(code, module_dir)\n    (storage_dir, module_name) = os.path.split(module_path)\n    mod = inline_ext_module(module_name, compiler)\n    ext_func = inline_ext_function('compiled_func', code, arg_names, local_dict, global_dict, auto_downcast, type_converters=type_converters)\n    mod.add_function(ext_func)\n    if customize:\n        mod.customize = customize\n    if support_code:\n        mod.customize.add_support_code(support_code)\n    for header in headers:\n        mod.customize.add_header(header)\n    if (verbose > 0):\n        print('<weave: compiling>')\n    mod.compile(location=storage_dir, compiler=compiler, verbose=verbose, **kw)\n    try:\n        sys.path.insert(0, storage_dir)\n        exec(('import ' + module_name))\n        func = eval((module_name + '.compiled_func'))\n    finally:\n        del sys.path[0]\n    return func\n", "label": 0}
{"function": "\n\n@testing.requires.threading_with_mock\n@testing.requires.timing_intensive\ndef test_timeout_race(self):\n    dbapi = MockDBAPI()\n    p = pool.QueuePool(creator=(lambda : dbapi.connect(delay=0.05)), pool_size=2, max_overflow=1, use_threadlocal=False, timeout=3)\n    timeouts = []\n\n    def checkout():\n        for x in range(1):\n            now = time.time()\n            try:\n                c1 = p.connect()\n            except tsa.exc.TimeoutError:\n                timeouts.append((time.time() - now))\n                continue\n            time.sleep(4)\n            c1.close()\n    threads = []\n    for i in range(10):\n        th = threading.Thread(target=checkout)\n        th.start()\n        threads.append(th)\n    for th in threads:\n        th.join(join_timeout)\n    assert (len(timeouts) > 0)\n    for t in timeouts:\n        assert (t >= 3), ('Not all timeouts were >= 3 seconds %r' % timeouts)\n        assert (t < 14), ('Not all timeouts were < 14 seconds %r' % timeouts)\n", "label": 1}
{"function": "\n\ndef _load_allowed_remote_addresses(self, app):\n    key = 'PSDASH_ALLOWED_REMOTE_ADDRESSES'\n    addrs = app.config.get(key)\n    if (not addrs):\n        return\n    if isinstance(addrs, (str, unicode)):\n        app.config[key] = [a.strip() for a in addrs.split(',')]\n", "label": 0}
{"function": "\n\ndef validate_config(self):\n    self.config.set('boss', 'data_dir', fs.abspath(self.config.get('boss', 'data_dir')))\n    if (not os.path.exists(self.config.get('boss', 'data_dir'))):\n        os.makedirs(self.config.get('boss', 'data_dir'))\n    pth = os.path.join(self.config.get('boss', 'data_dir'), 'cache')\n    if (not os.path.exists(fs.abspath(pth))):\n        os.makedirs(fs.abspath(pth))\n    self.config.set('boss', 'cache_dir', pth)\n    pth = os.path.join(self.config.get('boss', 'data_dir'), 'boss.db')\n    self.config.set('boss', 'db_path', pth)\n", "label": 0}
{"function": "\n\ndef test_length(session):\n    set_ = session.set(key('test_sortedset_length'), S('abc'), SortedSet)\n    assert (len(set_) == 3)\n    setx = session.set(key('test_sortedsetx_length'), S([1, 2, 3]), IntSet)\n    assert (len(setx) == 3)\n", "label": 0}
{"function": "\n\ndef __init__(self, notifier=None):\n    if (self.__class__.__instance is None):\n        self.__class__.__instance = self\n        if (notifier is None):\n            self._notifier = _AsyncNotifier()\n        else:\n            self._notifier = notifier\n        self._location = None\n        self._name = None\n        self._coros = {\n            \n        }\n        self._scheduled = set()\n        self._suspended = set()\n        self._timeouts = []\n        self._lock = threading.RLock()\n        self._quit = False\n        self._complete = threading.Event()\n        self._daemons = 0\n        self._polling = False\n        self._channels = {\n            \n        }\n        self._atexit = []\n        Coro._asyncoro = Channel._asyncoro = self\n        self._scheduler = threading.Thread(target=self._schedule)\n        self._scheduler.daemon = True\n        self._scheduler.start()\n        atexit.register(self.finish)\n        logger.info('version %s with %s I/O notifier', __version__, self._notifier._poller_name)\n", "label": 0}
{"function": "\n\ndef from_jsobj(jsobj, cls=None):\n    'Create an instance of the given class from a JSON object.\\n\\n    Arguments:\\n      cls: a class that serves as a \"type hint.\"\\n    '\n    if isinstance(jsobj, LIST_TYPES):\n        return [from_jsobj(o, cls=cls) for o in jsobj]\n    if (cls is not None):\n        return cls.from_jsobj(jsobj)\n    if (jsobj is None):\n        return JS_NULL\n    return jsobj\n", "label": 0}
{"function": "\n\ndef shouldDestroyCircuit(self, circuit):\n    'Return **True** iff CircuitManager thinks the calling circuit\\n        should be destroyed.\\n\\n        Circuits call shouldDestroyCircuit() when their number of open\\n        streams drops to zero. Since CircuitManager knows about all open\\n        and pending circuits, it can make an informed judgement about whether\\n        the calling circuit should be destroyed or remain open.\\n\\n        Currently, CircuitManager maintains at least 4 open or pending IPv4\\n        circuits and one open or pending IPv6 circuit. If the number of\\n        streams on any circuit drops to zero and it can be closed while still\\n        satisfying these basic constraints, then CircuitManager tells it\\n        to begin destroying itself (returns True).\\n\\n        :param oppy.circuit.circuit.Circuit circuit: circuit to\\n            consider destroying.\\n        :returns: **bool** **True** if CircuitManager decides this circuit\\n            should be destroyed, **False** otherwise.\\n        '\n    if (circuit.circuit_type == CircuitType.IPv4):\n        return ((self._totalIPv4Count() - 1) > self._min_IPv4_count)\n    else:\n        return ((self._totalIPv6Count() - 1) > self._min_IPv6_count)\n", "label": 0}
{"function": "\n\ndef contains_subsequence(seq, subseq):\n    for i in range((len(seq) - len(subseq))):\n        if (seq[i:(i + len(subseq))] == subseq):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef tables(self, db=None):\n    '\\n        Enumerates all tables fro a given database. If not specified, use the\\n        current database.\\n        '\n    if self.has_cap(TABLES_ENUM):\n        if (db is None):\n            if (self.current_db is None):\n                self.database()\n            db = self.current_db\n        n = self.get_nb_tables(db)\n        for i in range(n):\n            (yield TableWrapper(self, self.get_table_name(i, db), db))\n    else:\n        raise Unavailable()\n", "label": 0}
{"function": "\n\ndef _is_us_state(abbr, result):\n    for sep in ('/', '-'):\n        if (result.source_base == 'us{sep}{abbr}'.format(**locals())):\n            return True\n        if result.source_base.startswith('us{sep}{abbr}.'.format(**locals())):\n            return True\n        if result.source_base.startswith('us{sep}{abbr}{sep}'.format(**locals())):\n            return True\n    return False\n", "label": 0}
{"function": "\n\n@util.positional(2)\ndef error(status_code, status_message=None, content_type='text/plain; charset=utf-8', headers=None, content=None):\n    'Create WSGI application that statically serves an error page.\\n\\n  Creates a static error page specifically for non-200 HTTP responses.\\n\\n  Browsers such as Internet Explorer will display their own error pages for\\n  error content responses smaller than 512 bytes.  For this reason all responses\\n  are right-padded up to 512 bytes.\\n\\n  Error pages that are not provided will content will contain the standard HTTP\\n  status message as their content.\\n\\n  Args:\\n    status_code: Integer status code of error.\\n    status_message: Status message.\\n\\n  Returns:\\n    Static WSGI application that sends static error response.\\n  '\n    if (status_message is None):\n        status_message = httplib.responses.get(status_code, 'Unknown Error')\n    if (content is None):\n        content = status_message\n    content = util.pad_string(content)\n    return static_page(content, status=(status_code, status_message), content_type=content_type, headers=headers)\n", "label": 0}
{"function": "\n\ndef test_params(self):\n    params = np.array([r.params for r in self.results])\n    params_1 = np.array(([self.results[0].params] * len(self.results)))\n    assert_allclose(params, params_1)\n", "label": 0}
{"function": "\n\ndef usesTime(self):\n    fmt = self._fmt\n    return ((fmt.find('$asctime') >= 0) or (fmt.find(self.asctime_format) >= 0))\n", "label": 0}
{"function": "\n\ndef _validate_python(self, field_dict, state):\n    try:\n        ref = field_dict[self.field_names[0]]\n    except TypeError:\n        raise Invalid(self.message('notDict', state), field_dict, state)\n    except KeyError:\n        ref = ''\n    errors = {\n        \n    }\n    for name in self.field_names[1:]:\n        if (field_dict.get(name, '') != ref):\n            if self.show_match:\n                errors[name] = self.message('invalid', state, match=ref)\n            else:\n                errors[name] = self.message('invalidNoMatch', state)\n    if errors:\n        error_list = sorted(six.iteritems(errors))\n        error_message = '<br>\\n'.join((('%s: %s' % (name, value)) for (name, value) in error_list))\n        raise Invalid(error_message, field_dict, state, error_dict=errors)\n", "label": 1}
{"function": "\n\ndef get_list(self, *args, **kwargs):\n    (count, data) = super(TweetView, self).get_list(*args, **kwargs)\n    query = {\n        '_id': {\n            '$in': [x['user_id'] for x in data],\n        },\n    }\n    users = db.user.find(query, fields=('name',))\n    users_map = dict(((x['_id'], x['name']) for x in users))\n    for item in data:\n        item['user_name'] = users_map.get(item['user_id'])\n    return (count, data)\n", "label": 0}
{"function": "\n\n@conf.commands.register\ndef srbt1(peer, pkts, *args, **kargs):\n    'send and receive 1 packet using a bluetooth socket'\n    (a, b) = srbt(peer, pkts, *args, **kargs)\n    if (len(a) > 0):\n        return a[0][1]\n", "label": 0}
{"function": "\n\ndef find_payload_class(payload_type):\n    'Iterate through inherited classes to find a matching class name'\n    subclasses = set()\n    work = [Payload]\n    while work:\n        parent_subclass = work.pop()\n        for child_subclass in parent_subclass.__subclasses__():\n            if (child_subclass not in subclasses):\n                if (hasattr(child_subclass, 'payload_type') and (child_subclass.payload_type == payload_type)):\n                    return child_subclass\n                subclasses.add(child_subclass)\n                work.append(child_subclass)\n    return None\n", "label": 1}
{"function": "\n\ndef parse_inline(text):\n    '\\n    Takes a string of text from a text inline and returns a 3 tuple of\\n    (name, value, **kwargs).\\n    '\n    m = INLINE_SPLITTER.match(text)\n    if (not m):\n        raise InlineUnparsableError\n    args = m.group('args')\n    name = m.group('name')\n    value = ''\n    kwtxt = ''\n    kwargs = {\n        \n    }\n    if args:\n        kwtxt = INLINE_KWARG_PARSER.search(args).group('kwargs')\n        value = re.sub(('%s\\\\Z' % kwtxt), '', args)\n        value = value.strip()\n    if m.group('variant'):\n        kwargs['variant'] = m.group('variant')\n    if kwtxt:\n        for kws in kwtxt.split():\n            (k, v) = kws.split('=')\n            kwargs[str(k)] = v\n    return (name, value, kwargs)\n", "label": 1}
{"function": "\n\ndef write(self, *args, **kwargs):\n    if (not self.file):\n        self.file = tempfile.TemporaryFile()\n    self.file.write(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _Rotate(self, image, transform):\n    'Use PIL to rotate the given image with the given transform.\\n\\n    Args:\\n      image: PIL.Image.Image object to rotate.\\n      transform: images_service_pb.Transform to use when rotating.\\n\\n    Returns:\\n      PIL.Image.Image with transforms performed on it.\\n\\n    Raises:\\n      BadRequestError if the rotate data given is bad.\\n    '\n    degrees = transform.rotate()\n    if ((degrees < 0) or ((degrees % 90) != 0)):\n        raise apiproxy_errors.ApplicationError(images_service_pb.ImagesServiceError.BAD_TRANSFORM_DATA)\n    degrees %= 360\n    degrees = (360 - degrees)\n    return image.rotate(degrees)\n", "label": 0}
{"function": "\n\ndef iter_keys(self, filename):\n    with open(filename, 'rb') as f:\n        header = f.read(8)\n        self._verify_header(header)\n        current_offset = 8\n        file_size_bytes = os.path.getsize(filename)\n        while True:\n            current_contents = f.read(8)\n            current_offset += 8\n            if (len(current_contents) < 8):\n                if (len(current_contents) > 0):\n                    raise DBMLoadError('Error loading db: partial header read')\n                else:\n                    return\n            (key_size, val_size) = struct.unpack('!ii', current_contents)\n            key = f.read(key_size)\n            if (len(key) != key_size):\n                raise DBMLoadError(('Error loading db: key size does not match (expected %s bytes, got %s instead.' % (key_size, len(key))))\n            value_offset = (current_offset + key_size)\n            if ((value_offset + val_size) > file_size_bytes):\n                return\n            (yield (key, value_offset, val_size))\n            if (val_size == _DELETED):\n                val_size = 0\n            skip_ahead = ((key_size + val_size) + 4)\n            current_offset += skip_ahead\n            if (current_offset > file_size_bytes):\n                raise DBMLoadError('Error loading db: reading past the end of the file (file possibly truncated)')\n            f.seek(current_offset)\n", "label": 1}
{"function": "\n\ndef publish_state(self, payload, state):\n    if (not state):\n        raise Exception('Unable to publish unassigned state.')\n    self._state_publisher.publish(payload, self._state_exchange, state)\n", "label": 0}
{"function": "\n\ndef test_iter_smart_pk_range(self):\n    seen = []\n    for (start_pk, end_pk) in Author.objects.iter_smart_pk_ranges():\n        seen.extend(Author.objects.filter(id__gte=start_pk, id__lt=end_pk).values_list('id', flat=True))\n    all_ids = list(Author.objects.order_by('id').values_list('id', flat=True))\n    assert (seen == all_ids)\n", "label": 0}
{"function": "\n\ndef no_translate_debug_logs(logical_line, filename):\n    \"Check for 'LOG.debug(_('\\n\\n    As per our translation policy,\\n    https://wiki.openstack.org/wiki/LoggingStandards#Log_Translation\\n    we shouldn't translate debug level logs.\\n\\n    * This check assumes that 'LOG' is a logger.\\n    * Use filename so we can start enforcing this in specific folders instead\\n      of needing to do so all at once.\\n    S373\\n    \"\n    msg = \"S373 Don't translate debug level logs\"\n    if logical_line.startswith('LOG.debug(_('):\n        (yield (0, msg))\n", "label": 0}
{"function": "\n\ndef get_command(self, ctx, name):\n    \"\\n        Get a specific command by looking up the module.\\n\\n        :param ctx: Click context\\n        :param name: Command name\\n        :return: Module's cli function\\n        \"\n    try:\n        if (sys.version_info[0] == 2):\n            name = name.encode('ascii', 'replace')\n        mod = __import__(('cli.commands.cmd_' + name), None, None, ['cli'])\n    except ImportError as e:\n        logging.error('Error importing module {0}:\\n{0}'.format(name, e))\n        exit(1)\n    return mod.cli\n", "label": 0}
{"function": "\n\ndef all_terms(f):\n    '\\n        Returns all terms from a univariate polynomial ``f``.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy import Poly\\n        >>> from sympy.abc import x\\n\\n        >>> Poly(x**3 + 2*x - 1, x).all_terms()\\n        [((3,), 1), ((2,), 0), ((1,), 2), ((0,), -1)]\\n\\n        '\n    return [(m, f.rep.dom.to_sympy(c)) for (m, c) in f.rep.all_terms()]\n", "label": 0}
{"function": "\n\ndef ensure_role(self, role, dry_run=False):\n    '\\n        Adds the role if it does not already exist, otherwise skips it.\\n        '\n    existing_roles = Role.objects.filter(slug=role.slug)\n    if existing_roles:\n        logger.info('Role already exists: %s', role.name)\n        return existing_roles[0]\n    elif dry_run:\n        logger.info('[DRY RUN] Creating role: %s', role.name)\n    else:\n        if self.verbose:\n            logger.info('Creating role: %s', role.name)\n        role.save()\n", "label": 0}
{"function": "\n\ndef _initPopulation(self, seeds):\n    if (self.parentChildAverage < 1):\n        for s in seeds:\n            s.parent = None\n    self.pop = self._extendPopulation(seeds, self.populationSize)\n", "label": 0}
{"function": "\n\ndef _get_task_with_policy(queue_name, task_id, owner):\n    'Fetches the specified task and enforces ownership policy.\\n\\n    Args:\\n        queue_name: Name of the queue the work item is on.\\n        task_id: ID of the task that is finished.\\n        owner: Who or what has the current lease on the task.\\n\\n    Returns:\\n        The valid WorkQueue task that is currently owned.\\n\\n    Raises:\\n        TaskDoesNotExistError if the task does not exist.\\n        LeaseExpiredError if the lease is no longer active.\\n        NotOwnerError if the specified owner no longer owns the task.\\n    '\n    now = datetime.datetime.utcnow()\n    task = WorkQueue.query.filter_by(queue_name=queue_name, task_id=task_id).with_lockmode('update').first()\n    if (not task):\n        raise TaskDoesNotExistError(('task_id=%r' % task_id))\n    lease_delta = (now - task.eta)\n    if (lease_delta > datetime.timedelta(0)):\n        db.session.rollback()\n        raise LeaseExpiredError(('queue=%r, task_id=%r expired %s' % (task.queue_name, task_id, lease_delta)))\n    if (task.last_owner != owner):\n        db.session.rollback()\n        raise NotOwnerError(('queue=%r, task_id=%r, owner=%r' % (task.queue_name, task_id, task.last_owner)))\n    return task\n", "label": 0}
{"function": "\n\ndef showSublimeContext(self, filename, line):\n    debug(((('showSublimeContext: ' + str(filename)) + ' : ') + str(line)))\n    console_output((((('@@@ Stopped at ' + str(filename.replace((self.projectDir + '/'), ''))) + ':') + str(line)) + ' @@@'))\n    window = sublime.active_window()\n    if window:\n        window.focus_group(0)\n        view = window.active_view()\n        if ((view is not None) and (view.size() >= 0)):\n            filename = os.path.join(self.projectDir, filename)\n            if (view.file_name() != filename):\n                self.activateViewWithFile(filename, line)\n            window.run_command('goto_line', {\n                'line': line,\n            })\n            view = window.active_view()\n            mark = [view.line(view.text_point((line - 1), 0))]\n            view.erase_regions('current_line')\n            view.add_regions('current_line', mark, 'current_line', 'dot', sublime.DRAW_OUTLINED)\n        else:\n            debug('No current view')\n", "label": 0}
{"function": "\n\ndef do_command(self, verb, args):\n    conn = http_client.HTTPConnection(self.host, self.port, timeout=self.http_timeout)\n    try:\n        body = ('cmd=' + urllib_parse.quote_plus(unicode(verb).encode('utf-8')))\n        for i in range(len(args)):\n            body += ((('&' + unicode((i + 1))) + '=') + urllib_parse.quote_plus(unicode(args[i]).encode('utf-8')))\n        if (None != self.sessionId):\n            body += ('&sessionId=' + unicode(self.sessionId))\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8',\n        }\n        conn.request('POST', '/selenium-server/driver/', body, headers)\n        response = conn.getresponse()\n        data = unicode(response.read(), 'UTF-8')\n        if (not data.startswith('OK')):\n            raise Exception(data)\n        return data\n    finally:\n        conn.close()\n", "label": 0}
{"function": "\n\ndef fdiff(self, argindex=1):\n    (z, m) = self.args\n    fm = sqrt((1 - (m * (sin(z) ** 2))))\n    if (argindex == 1):\n        return (1 / fm)\n    elif (argindex == 2):\n        return (((elliptic_e(z, m) / ((2 * m) * (1 - m))) - (elliptic_f(z, m) / (2 * m))) - (sin((2 * z)) / ((4 * (1 - m)) * fm)))\n    raise ArgumentIndexError(self, argindex)\n", "label": 0}
{"function": "\n\ndef bayesdb_generator_column_stattype(bdb, generator_id, colno):\n    'Return the statistical type of the column `colno` in `generator_id`.'\n    sql = '\\n        SELECT stattype FROM bayesdb_generator_column\\n            WHERE generator_id = ? AND colno = ?\\n    '\n    cursor = bdb.sql_execute(sql, (generator_id, colno))\n    try:\n        row = cursor.next()\n    except StopIteration:\n        generator = bayesdb_generator_name(bdb, generator_id)\n        sql = '\\n            SELECT COUNT(*)\\n                FROM bayesdb_generator AS g, bayesdb_column AS c\\n                WHERE g.id = :generator_id\\n                    AND g.tabname = c.tabname\\n                    AND c.colno = :colno\\n        '\n        cursor = bdb.sql_execute(sql, {\n            'generator_id': generator_id,\n            'colno': colno,\n        })\n        if (cursor_value(cursor) == 0):\n            raise ValueError(('No such column in generator %s: %d' % (generator, colno)))\n        else:\n            raise ValueError(('Column not modelled in generator %s: %d' % (generator, colno)))\n    else:\n        assert (len(row) == 1)\n        return row[0]\n", "label": 0}
{"function": "\n\ndef test_serialization_type(self):\n    activity_object = Pin(id=1)\n    activity = Activity(1, LoveVerb, activity_object)\n    assert isinstance(activity.serialization_id, (six.integer_types, float))\n", "label": 0}
{"function": "\n\n@pytest.mark.xfail\ndef test_fails(self):\n    contact = models.Contact(name='Example')\n    contact.put()\n    models.PhoneNumber(contact=self.contact_key, phone_type='home', number='(650) 555 - 2200').put()\n    numbers = contact.phone_numbers.fetch()\n    assert (1 == len(numbers))\n", "label": 0}
{"function": "\n\ndef _mount_shares_to_instance(self, instance):\n    for share in self.shares:\n        share.handler.allow_access_to_instance(instance, share.share_config)\n    with instance.remote() as remote:\n        share_types = set((type(share.handler) for share in self.shares))\n        for share_type in share_types:\n            share_type.setup_instance(remote)\n        for share in self.shares:\n            share.handler.mount_to_instance(remote, share.share_config)\n", "label": 0}
{"function": "\n\ndef get_default_machine(self):\n    ' Reads the default machine from the package configuration\\n        \\n        '\n    server = username = port = password = ''\n    if configuration.check('server'):\n        server = configuration.server\n    if (not server):\n        return None\n    if configuration.check('username'):\n        username = configuration.username\n    if (not username):\n        username = current_user()\n    if (configuration.check('port') is not None):\n        port = configuration.port\n    if configuration.check('password'):\n        password = configuration.password\n    self.annotate({\n        'RemoteQ-server': server,\n        'RemoteQ-username': username,\n        'RemoteQ-port': port,\n    })\n    return (server, port, username, password)\n", "label": 1}
{"function": "\n\ndef get_all_hosts(self):\n    '\\n            Get list of all hosts in cluster\\n            Args:\\n                None\\n            Return:\\n                list of hostnames\\n            Raise:\\n                None\\n       '\n    zook = self.zk_client\n    broker_id_path = self.zk_paths[BROKER_IDS]\n    if zook.exists(broker_id_path):\n        broker_ids = zook.get_children(broker_id_path)\n        brokers = []\n        for broker_id in broker_ids:\n            brokers.append(self.get_host(broker_id))\n        return brokers\n", "label": 0}
{"function": "\n\ndef parse(cls, signed_request, application_secret_key):\n    'Parse a signed request, returning a dictionary describing its payload.'\n\n    def decode(encoded):\n        padding = ('=' * (len(encoded) % 4))\n        return base64.urlsafe_b64decode((encoded + padding))\n    try:\n        (encoded_signature, encoded_payload) = (str(string) for string in signed_request.split('.', 2))\n        signature = decode(encoded_signature)\n        signed_request_data = json.loads(decode(encoded_payload).decode('utf-8'))\n    except (TypeError, ValueError):\n        raise SignedRequestError('Signed request had a corrupt payload')\n    if (signed_request_data.get('algorithm', '').upper() != 'HMAC-SHA256'):\n        raise SignedRequestError('Signed request is using an unknown algorithm')\n    expected_signature = hmac.new(application_secret_key.encode('utf-8'), msg=encoded_payload.encode('utf-8'), digestmod=hashlib.sha256).digest()\n    if (signature != expected_signature):\n        raise SignedRequestError('Signed request signature mismatch')\n    return signed_request_data\n", "label": 0}
{"function": "\n\ndef assemble(self):\n    assembled = {\n        self.type: self.body,\n    }\n    if self.aggregations:\n        assembled['aggs'] = {\n            \n        }\n        for agg in self.aggregations:\n            assembled['aggs'][agg.name] = agg.assemble()\n    return assembled\n", "label": 0}
{"function": "\n\ndef _ml_train_iterative(self, database_matrix, params=[], sliding_window=168, k=1):\n    '\\n            Training method used by Fred 09 paper.\\n        '\n    p = (database_matrix.shape[1] - 1)\n    number_iterations = ((((database_matrix.shape[0] + p) - k) - sliding_window) + 1)\n    print('Number of iterations: ', number_iterations)\n    tr_size = ((sliding_window - p) - 1)\n    z = (database_matrix[0:(- k), 1].reshape((- 1), 1) * np.ones((1, p)))\n    database_matrix[k:, 1:] = (database_matrix[k:, 1:] - z)\n    pr_target = []\n    ex_target = []\n    for i in range(number_iterations):\n        self._ml_train(database_matrix[(k + i):(((k + i) + tr_size) - 1), :], params)\n        pr_t = self._ml_predict(horizon=1)\n        pr_t = (pr_t[0][0] + z[(i, 0)])\n        pr_target.append(pr_t)\n        ex_target.append(database_matrix[(((k + i) + tr_size), 0)])\n    pr_result = Error(expected=ex_target, predicted=pr_target)\n    return pr_result\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    \" Add an issue to the test repository and save it's id.\"\n    super(IssueCommentAuthenticatedMethodsTest, self).setUp()\n    (success, result) = self.bb.issue.create(title='Test Issue Bitbucket API', content='Test Issue Bitbucket API', responsible=self.bb.username, status='new', kind='bug')\n    assert success\n    self.bb.issue.comment.issue_id = result['local_id']\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    assert (not cls._meta.has_auto_field), \"A model can't have more than one AutoField.\"\n    super(AutoField, self).contribute_to_class(cls, name)\n    cls._meta.has_auto_field = True\n    cls._meta.auto_field = self\n", "label": 0}
{"function": "\n\ndef get(self, filepath, version=None, mode='r'):\n    'Returns a bytestring with the file content, but no metadata.'\n    file_stream = self.open(filepath, version=version, mode=mode)\n    if (file_stream is None):\n        raise IOError(('File %s (version %s) not found.' % (filepath, (version if version else 'latest'))))\n    return file_stream.read()\n", "label": 0}
{"function": "\n\ndef read_bytesmap(f):\n    numpairs = read_short(f)\n    bytesmap = {\n        \n    }\n    for _ in range(numpairs):\n        k = read_string(f)\n        bytesmap[k] = read_value(f)\n    return bytesmap\n", "label": 0}
{"function": "\n\ndef clearAlert(self):\n    ' Clear the current alert level, if any.\\n\\n        '\n    if (self._alert_data is not None):\n        self._alert_data.timer.stop()\n        self._alert_data = None\n        app = QApplication.instance()\n        app.focusChanged.disconnect(self._onAppFocusChanged)\n        self.alerted.emit('')\n", "label": 0}
{"function": "\n\ndef upload_form(self):\n    '\\n            Instantiate file upload form and return it.\\n\\n            Override to implement custom behavior.\\n        '\n    upload_form_class = self.get_upload_form()\n    if request.form:\n        formdata = request.form.copy()\n        formdata.update(request.files)\n        return upload_form_class(formdata, admin=self)\n    elif request.files:\n        return upload_form_class(request.files, admin=self)\n    else:\n        return upload_form_class(admin=self)\n", "label": 0}
{"function": "\n\ndef make_node(self, images):\n    '\\n        .. todo::\\n\\n            WRITEME\\n        '\n    images = as_cuda_ndarray_variable(images)\n    assert (images.ndim == 4)\n    channels_broadcastable = images.type.broadcastable[0]\n    batch_broadcastable = images.type.broadcastable[3]\n    rows_broadcastable = False\n    cols_broadcastable = False\n    targets_broadcastable = (channels_broadcastable, rows_broadcastable, cols_broadcastable, batch_broadcastable)\n    targets_type = CudaNdarrayType(broadcastable=targets_broadcastable)\n    targets = targets_type()\n    return Apply(self, [images], [targets])\n", "label": 0}
{"function": "\n\ndef _get_url(self, url):\n    if (self.access == 'public'):\n        url = url.replace('https://', 'http://')\n        req = urllib.request.Request(url)\n        try:\n            return urllib.request.urlopen(req).read()\n        except urllib.error.HTTPError:\n            raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n    else:\n        raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n", "label": 0}
{"function": "\n\ndef downcaseTokens(s, l, t):\n    'Helper parse action to convert tokens to lower case.'\n    return [tt.lower() for tt in map(_ustr, t)]\n", "label": 0}
{"function": "\n\n@skipUnless(cache_file_exists('ia', '20101102__ia__general__poweshiek__precinct.xls'), CACHED_FILE_MISSING_MSG)\ndef test_results(self):\n    filename = '20101102__ia__general__poweshiek__precinct.xls'\n    mapping = self._get_mapping(filename)\n    self._prep_loader_attrs(mapping)\n    results = self.loader._results(mapping)\n    montezuma_abs_results = [r for r in results if ((r.jurisdiction == 'Montezuma') and (r.votes_type == 'absentee'))]\n    self.assertEqual(len(montezuma_abs_results), 34)\n    result = montezuma_abs_results[0]\n    self.assertEqual(result.office, 'United States Senator')\n    self.assertEqual(result.district, None)\n    self.assertEqual(result.full_name, 'Roxanne Conlin')\n    self.assertEqual(result.party, 'DEM')\n    self.assertEqual(result.write_in, None)\n    self.assertEqual(result.votes, 59)\n    result = montezuma_abs_results[(- 1)]\n    self.assertEqual(result.office, 'State Rep')\n    self.assertEqual(result.district, '75')\n    self.assertEqual(result.full_name, 'Write-In')\n    self.assertEqual(result.party, None)\n    self.assertEqual(result.write_in, 'Write-In')\n    self.assertEqual(result.votes, 0)\n", "label": 0}
{"function": "\n\ndef _as_vector(self, keep_channels=False):\n    '\\n        The vectorized form of this image.\\n\\n        Parameters\\n        ----------\\n        keep_channels : `bool`, optional\\n\\n            ========== =============================\\n            Value      Return shape\\n            ========== =============================\\n            `False`    ``(n_channels * n_pixels,)``\\n            `True`     ``(n_channels, n_pixels)``\\n            ========== =============================\\n\\n        Returns\\n        -------\\n        vec : (See ``keep_channels`` above) `ndarray`\\n            Flattened representation of this image, containing all pixel\\n            and channel information.\\n        '\n    if keep_channels:\n        return self.pixels.reshape([self.n_channels, (- 1)])\n    else:\n        return self.pixels.ravel()\n", "label": 0}
{"function": "\n\ndef show(mousetarget, **kwargs):\n    global mousecapturer\n    mc = getMouseCapturer(**kwargs)\n    mc.mousetarget = mousetarget\n    if isinstance(mousetarget, MouseHandler):\n        mc.mousehandler = True\n    mc.show()\n", "label": 0}
{"function": "\n\ndef write(self, bytes):\n    '\\n        Write C{bytes} to the underlying consumer unless\\n        C{_noMoreWritesExpected} has been called or there are/have been too\\n        many bytes.\\n        '\n    if (self._finished is None):\n        self._producer.stopProducing()\n        raise ExcessWrite()\n    if (len(bytes) <= self._length):\n        self._length -= len(bytes)\n        self._consumer.write(bytes)\n    else:\n        _callAppFunction(self._producer.stopProducing)\n        self._finished.errback(WrongBodyLength('too many bytes written'))\n        self._allowNoMoreWrites()\n", "label": 0}
{"function": "\n\ndef LVMPathSpecGetVolumeIndex(path_spec):\n    'Retrieves the volume index from the path specification.\\n\\n  Args:\\n    path_spec: the path specification (instance of PathSpec).\\n  '\n    volume_index = getattr(path_spec, 'volume_index', None)\n    if (volume_index is None):\n        location = getattr(path_spec, 'location', None)\n        if ((location is None) or (not location.startswith('/lvm'))):\n            return\n        volume_index = None\n        try:\n            volume_index = (int(location[4:], 10) - 1)\n        except ValueError:\n            pass\n        if ((volume_index is None) or (volume_index < 0)):\n            return\n    return volume_index\n", "label": 1}
{"function": "\n\ndef handle_socket_write(self):\n    'Write to socket'\n    try:\n        count = self.socket.send(bytes(self.buffer_ser2net))\n        self.buffer_ser2net = self.buffer_ser2net[count:]\n    except socket.error:\n        self.handle_socket_error()\n", "label": 0}
{"function": "\n\ndef update_fpointer(self, nid, mode=ADD):\n    'set _fpointer recursively'\n    if (nid is None):\n        return\n    if (mode is self.ADD):\n        self._fpointer.append(nid)\n    elif (mode is self.DELETE):\n        if (nid in self._fpointer):\n            self._fpointer.remove(nid)\n    elif (mode is self.INSERT):\n        print('WARNNING: INSERT is deprecated to ADD mode')\n        self.update_fpointer(nid)\n", "label": 1}
{"function": "\n\ndef __init__(self):\n    super().__init__()\n    db_host = os.environ.get('MONGO_HOST')\n    db_host = (db_host if db_host else 'localhost')\n    db_port = int(os.environ.get('MONGO_PORT'))\n    db_port = (db_port if db_port else 27017)\n    db_name = os.environ.get('MONGO_DB')\n    db_name = (db_name if db_name else 'default')\n    db_bucket = os.environ.get('MONGO_BUCKET')\n    db_bucket = (db_bucket if db_bucket else 'rxnorm')\n    import pymongo\n    conn = pymongo.MongoClient(host=db_host, port=db_port)\n    db = conn[db_name]\n    db_user = os.environ.get('MONGO_USER')\n    db_pass = os.environ.get('MONGO_PASS')\n    if (db_user and db_pass):\n        db.authenticate(db_user, db_pass)\n    self.mng = db[db_bucket]\n    self.mng.ensure_index('ndc')\n    self.mng.ensure_index('label', text=pymongo.TEXT)\n", "label": 1}
{"function": "\n\n@responses.activate\ndef test_bitly_total_clicks_bad_response():\n    body = '20'\n    params = urlencode(dict(link=shorten, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/link/clicks', params)\n    responses.add(responses.GET, url, body=body, status=400, match_querystring=True)\n    body = shorten\n    params = urlencode(dict(uri=expanded, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/shorten', params)\n    responses.add(responses.GET, url, body=body, match_querystring=True)\n    s.short(expanded)\n    assert (s.total_clicks() == 0)\n    assert (s.total_clicks(shorten) == 0)\n", "label": 0}
{"function": "\n\ndef prune_overridden(ansi_string):\n    'Remove color codes that are rendered ineffective by subsequent codes in one escape sequence then sort codes.\\n\\n    :param str ansi_string: Incoming ansi_string with ANSI color codes.\\n\\n    :return: Color string with pruned color sequences.\\n    :rtype: str\\n    '\n    multi_seqs = set((p for p in RE_ANSI.findall(ansi_string) if (';' in p[1])))\n    for (escape, codes) in multi_seqs:\n        r_codes = list(reversed(codes.split(';')))\n        try:\n            r_codes = r_codes[:(r_codes.index('0') + 1)]\n        except ValueError:\n            pass\n        for group in CODE_GROUPS:\n            for pos in reversed([i for (i, n) in enumerate(r_codes) if (n in group)][1:]):\n                r_codes.pop(pos)\n        reduced_codes = ';'.join(sorted(r_codes, key=int))\n        if (codes != reduced_codes):\n            ansi_string = ansi_string.replace(escape, (('\\x1b[' + reduced_codes) + 'm'))\n    return ansi_string\n", "label": 1}
{"function": "\n\ndef __init__(self, **kwargs):\n    super(discreteBarChart, self).__init__(**kwargs)\n    self.model = 'discreteBarChart'\n    height = kwargs.get('height', 450)\n    width = kwargs.get('width', None)\n    if kwargs.get('x_is_date', False):\n        self.set_date_flag(True)\n        self.create_x_axis('xAxis', format=kwargs.get('x_axis_format', '%d %b %Y %H %S'), date=True)\n    else:\n        self.create_x_axis('xAxis', format=None)\n    self.create_y_axis('yAxis', format=kwargs.get('y_axis_format', '.0f'))\n    self.set_custom_tooltip_flag(True)\n    self.set_graph_height(height)\n    if width:\n        self.set_graph_width(width)\n    tooltips = kwargs.get('tooltips', True)\n    if (not tooltips):\n        self.chart_attr = {\n            'tooltips': 'false',\n        }\n", "label": 0}
{"function": "\n\n@access.public\ndef describeResource(self, resource, params):\n    if (resource not in docs.routes):\n        raise RestException(('Invalid resource: %s' % resource))\n    return {\n        'apiVersion': API_VERSION,\n        'swaggerVersion': SWAGGER_VERSION,\n        'basePath': getApiUrl(),\n        'models': dict(docs.models[resource], **docs.models[None]),\n        'apis': [{\n            'path': route,\n            'operations': sorted(op, key=functools.cmp_to_key(self._compareOperations)),\n        } for (route, op) in sorted(six.viewitems(docs.routes[resource]), key=functools.cmp_to_key(self._compareRoutes))],\n    }\n", "label": 0}
{"function": "\n\ndef _es_down_template(request, *args, **kwargs):\n    'Returns the appropriate \"Elasticsearch is down!\" template'\n    return ('search/mobile/down.html' if request.MOBILE else 'search/down.html')\n", "label": 0}
{"function": "\n\ndef capture(self, money, authorization, options=None):\n    options = (options or {\n        \n    })\n    params = {\n        'checkout_id': authorization,\n    }\n    token = options.pop('access_token', self.we_pay_settings['ACCESS_TOKEN'])\n    try:\n        response = self.we_pay.call('/checkout/capture', params, token=token)\n    except WePayError as error:\n        transaction_was_unsuccessful.send(sender=self, type='capture', response=error)\n        return {\n            'status': 'FAILURE',\n            'response': error,\n        }\n    transaction_was_successful.send(sender=self, type='capture', response=response)\n    return {\n        'status': 'SUCCESS',\n        'response': response,\n    }\n", "label": 0}
{"function": "\n\ndef __isub__(self, val):\n    if (type(val) in (int, float)):\n        self.x -= val\n        self.y -= val\n    else:\n        self.x -= val.x\n        self.y -= val.y\n    return self\n", "label": 0}
{"function": "\n\ndef reserve_provider_segment(self, session, segment):\n    filters = {\n        \n    }\n    physical_network = segment.get(api.PHYSICAL_NETWORK)\n    if (physical_network is not None):\n        filters['physical_network'] = physical_network\n        vlan_id = segment.get(api.SEGMENTATION_ID)\n        if (vlan_id is not None):\n            filters['vlan_id'] = vlan_id\n    if self.is_partial_segment(segment):\n        alloc = self.allocate_partially_specified_segment(session, **filters)\n        if (not alloc):\n            raise exc.NoNetworkAvailable()\n    else:\n        alloc = self.allocate_fully_specified_segment(session, **filters)\n        if (not alloc):\n            raise exc.VlanIdInUse(**filters)\n    return {\n        api.NETWORK_TYPE: p_const.TYPE_VLAN,\n        api.PHYSICAL_NETWORK: alloc.physical_network,\n        api.SEGMENTATION_ID: alloc.vlan_id,\n        api.MTU: self.get_mtu(alloc.physical_network),\n    }\n", "label": 1}
{"function": "\n\ndef _match_rhs(self, rhs, rightmost_stack):\n    \"\\n        :rtype: bool\\n        :return: true if the right hand side of a CFG production\\n            matches the rightmost elements of the stack.  ``rhs``\\n            matches ``rightmost_stack`` if they are the same length,\\n            and each element of ``rhs`` matches the corresponding\\n            element of ``rightmost_stack``.  A nonterminal element of\\n            ``rhs`` matches any Tree whose node value is equal\\n            to the nonterminal's symbol.  A terminal element of ``rhs``\\n            matches any string whose type is equal to the terminal.\\n        :type rhs: list(terminal and Nonterminal)\\n        :param rhs: The right hand side of a CFG production.\\n        :type rightmost_stack: list(string and Tree)\\n        :param rightmost_stack: The rightmost elements of the parser's\\n            stack.\\n        \"\n    if (len(rightmost_stack) != len(rhs)):\n        return False\n    for i in range(len(rightmost_stack)):\n        if isinstance(rightmost_stack[i], Tree):\n            if (not isinstance(rhs[i], Nonterminal)):\n                return False\n            if (rightmost_stack[i].label() != rhs[i].symbol()):\n                return False\n        else:\n            if isinstance(rhs[i], Nonterminal):\n                return False\n            if (rightmost_stack[i] != rhs[i]):\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef put(self):\n    pet = self.json_args\n    if (not isinstance(pet['id'], int)):\n        self.set_status(400)\n    if (not self.db.update_(**pet)):\n        self.set_status(404)\n    else:\n        self.set_status(200)\n    self.finish()\n", "label": 0}
{"function": "\n\ndef test_getitem_slice_big():\n    slt = SortedList(range(4))\n    lst = list(range(4))\n    itr = ((start, stop, step) for start in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for stop in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for step in [(- 3), (- 2), (- 1), 1, 2, 3])\n    for (start, stop, step) in itr:\n        assert (slt[start:stop:step] == lst[start:stop:step])\n", "label": 1}
{"function": "\n\ndef get_response(self, cmd, fid, *args):\n    for source in self.select_best_source(fid.decode()):\n        dealer = None\n        try:\n            dealer = self.context.socket(zmq.DEALER)\n            dealer.connect(get_events_uri(self.session, source, 'router'))\n            dealer.send_multipart(((cmd, fid) + args))\n            response = dealer.recv_multipart()\n            if ((not response) or (response[0] == ERROR)):\n                self.logger.debug('Error with source {}', source)\n                continue\n            return response\n        finally:\n            if dealer:\n                dealer.close()\n    self.logger.debug('No more source available.')\n    return [ERROR]\n", "label": 0}
{"function": "\n\ndef httpapi(self, arg, opts):\n    sc = HttpAPIStatsCollector()\n    headers = ['#Item', 'Value']\n    table = []\n    for (k, v) in sc.get().getStats().iteritems():\n        if isinstance(v, dict):\n            v = json.dumps(v)\n        row = []\n        row.append(('#%s' % k))\n        if (k[(- 3):] == '_at'):\n            row.append(formatDateTime(v))\n        else:\n            row.append(v)\n        table.append(row)\n    self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))\n", "label": 0}
{"function": "\n\ndef test_cmovpe(self):\n    asm = ['cmovpe eax, ebx']\n    ctx_init = self.__init_context()\n    (x86_ctx_out, reil_ctx_out) = self.__run_code(asm, 3735928559, ctx_init)\n    cmp_result = self.__compare_contexts(ctx_init, x86_ctx_out, reil_ctx_out)\n    if (not cmp_result):\n        self.__save_failing_context(ctx_init)\n    self.assertTrue(cmp_result, self.__print_contexts(ctx_init, x86_ctx_out, reil_ctx_out))\n", "label": 0}
{"function": "\n\ndef _find_tab(self, widget):\n    for (key, bnch) in self.tab.items():\n        if (widget == bnch.widget):\n            return bnch\n    return None\n", "label": 0}
{"function": "\n\ndef redraw(self):\n    self.data.sort(self.sortfn)\n    rows = len(self.data)\n    cols = 0\n    if (rows > 0):\n        cols = len(self.data[0])\n    self.grid.resize(rows, cols)\n    self.header.resize(1, cols)\n    cf = self.grid.getCellFormatter()\n    for (nrow, row) in enumerate(self.data):\n        for (ncol, item) in enumerate(row):\n            self.grid.setHTML(nrow, ncol, str(item))\n            cf.setWidth(nrow, ncol, '200px')\n    cf = self.header.getCellFormatter()\n    self.sortbuttons = []\n    for ncol in range(cols):\n        sb = Button(('sort col %d' % ncol))\n        sb.addClickListener(self)\n        self.header.setWidget(0, ncol, sb)\n        cf.setWidth(0, ncol, '200px')\n        self.sortbuttons.append(sb)\n", "label": 0}
{"function": "\n\ndef is_user_notified(user_id, target_id):\n    res = dbsession.query(Notify).filter((Notify.target_id == target_id))\n    res = res.filter((Notify.user_id == user_id))\n    try:\n        r = res.all()[0]\n        return r\n    except:\n        return False\n", "label": 0}
{"function": "\n\n@users_delete_fixtures\ndef test_users_delete_redirect(User):\n    request = DummyRequest(params={\n        'username': 'bob',\n    })\n    User.get_by_username.return_value = None\n    result = views.users_delete(request)\n    assert (result.__class__ == httpexceptions.HTTPFound)\n", "label": 0}
{"function": "\n\ndef get_primary_address(self):\n    'Return the primary address of this partner.\\n\\n        '\n    Address = rt.modules.addresses.Address\n    try:\n        return Address.objects.get(partner=self, primary=True)\n    except Address.DoesNotExist:\n        pass\n", "label": 0}
{"function": "\n\ndef clear(self):\n    'od.clear() -> None.  Remove all items from od.'\n    try:\n        for node in self.__map.itervalues():\n            del node[:]\n        root = self.__root\n        root[:] = [root, root, None]\n        self.__map.clear()\n    except AttributeError:\n        pass\n    dict.clear(self)\n", "label": 0}
{"function": "\n\n@group_only\ndef seen(self, msg, matches):\n    chat_id = msg.dest.id\n    if (matches.group(2) is not None):\n        return self.seen_by_id(chat_id, matches.group(2))\n    elif (matches.group(3) is not None):\n        return self.seen_by_username(chat_id, matches.group(3))\n    else:\n        return self.seen_by_fullname(chat_id, matches.group(4))\n", "label": 0}
{"function": "\n\ndef updateProperties(self):\n    if (self.modelXbrl is not None):\n        modelXbrl = self.modelXbrl\n        if (modelXbrl.modelDocument.type == ModelDocument.Type.INSTANCE):\n            tbl = self.table\n            coordinates = tbl.getCurrentCellCoordinates()\n            if (coordinates is not None):\n                objId = tbl.getObjectId(coordinates)\n                if ((objId is not None) and (len(objId) > 0)):\n                    if (objId and (objId[0] == 'f')):\n                        viewableObject = self.factPrototypes[int(objId[1:])]\n                    elif (objId[0] != 'a'):\n                        viewableObject = self.modelXbrl.modelObject(objId)\n                    else:\n                        return\n                    modelXbrl.viewModelObject(viewableObject)\n", "label": 1}
{"function": "\n\ndef Validate(self):\n    'Attempt to validate the artifact has been well defined.\\n\\n    This is used to enforce Artifact rules. Since it checks all dependencies are\\n    present, this method can only be called once all artifacts have been loaded\\n    into the registry. Use ValidateSyntax to check syntax for each artifact on\\n    import.\\n\\n    Raises:\\n      ArtifactDefinitionError: If artifact is invalid.\\n    '\n    self.ValidateSyntax()\n    try:\n        for dependency in self.GetArtifactDependencies():\n            dependency_obj = REGISTRY.GetArtifact(dependency)\n            if dependency_obj.error_message:\n                raise ArtifactDefinitionError(('Dependency %s has an error!' % dependency))\n    except ArtifactNotRegisteredError as e:\n        raise ArtifactDefinitionError(e)\n", "label": 0}
{"function": "\n\ndef _destroy_kernel_ramdisk(self, instance, vm_ref):\n    'Three situations can occur:\\n\\n            1. We have neither a ramdisk nor a kernel, in which case we are a\\n               RAW image and can omit this step\\n\\n            2. We have one or the other, in which case, we should flag as an\\n               error\\n\\n            3. We have both, in which case we safely remove both the kernel\\n               and the ramdisk.\\n\\n        '\n    instance_uuid = instance['uuid']\n    if ((not instance['kernel_id']) and (not instance['ramdisk_id'])):\n        LOG.debug('Using RAW or VHD, skipping kernel and ramdisk deletion', instance=instance)\n        return\n    if (not (instance['kernel_id'] and instance['ramdisk_id'])):\n        raise exception.InstanceUnacceptable(instance_id=instance_uuid, reason=_('instance has a kernel or ramdisk but not both'))\n    (kernel, ramdisk) = vm_utils.lookup_kernel_ramdisk(self._session, vm_ref)\n    if (kernel or ramdisk):\n        vm_utils.destroy_kernel_ramdisk(self._session, instance, kernel, ramdisk)\n        LOG.debug('kernel/ramdisk files removed', instance=instance)\n", "label": 1}
{"function": "\n\ndef filter_log_files_for_zipping(log_files):\n    \"Identify unzipped log files that are approporate for zipping.\\n\\n    Each unique log type found should have the most recent log file unzipped\\n    as it's probably still in use.\\n    \"\n    out_files = []\n    for lf in filter_log_files_for_active(log_files):\n        if lf.bzip:\n            continue\n        out_files.append(lf)\n    return out_files\n", "label": 0}
{"function": "\n\ndef http_method_not_allowed(self, request, *args, **kwargs):\n    allowed_methods = [m for m in self.http_method_names if hasattr(self, m)]\n    logger.warning(('Method Not Allowed (%s): %s' % (request.method, request.path)), extra={\n        'status_code': 405,\n        'request': self.request,\n    })\n    return http.HttpResponseNotAllowed(allowed_methods)\n", "label": 0}
{"function": "\n\ndef register(self, name, c):\n    if ((name in self.registered) and (c is not self.registered[name])):\n        raise NameError('{} has been registered by {}'.format(name, self.registered[name]))\n    self.registered[name] = c\n", "label": 0}
{"function": "\n\ndef __init__(self, pattern, flags=0):\n    'The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.'\n    super(Regex, self).__init__()\n    if isinstance(pattern, basestring):\n        if (len(pattern) == 0):\n            warnings.warn('null string passed to Regex; use Empty() instead', SyntaxWarning, stacklevel=2)\n        self.pattern = pattern\n        self.flags = flags\n        try:\n            self.re = re.compile(self.pattern, self.flags)\n            self.reString = self.pattern\n        except sre_constants.error:\n            warnings.warn(('invalid pattern (%s) passed to Regex' % pattern), SyntaxWarning, stacklevel=2)\n            raise\n    elif isinstance(pattern, Regex.compiledREtype):\n        self.re = pattern\n        self.pattern = self.reString = str(pattern)\n        self.flags = flags\n    else:\n        raise ValueError('Regex may only be constructed with a string or a compiled RE object')\n    self.name = _ustr(self)\n    self.errmsg = ('Expected ' + self.name)\n    self.mayIndexError = False\n    self.mayReturnEmpty = True\n", "label": 0}
{"function": "\n\ndef get_cpu_state(self):\n    '\\n        Retrieves CPU state from client\\n        '\n    state = c_int(0)\n    self.library.Cli_GetPlcStatus(self.pointer, byref(state))\n    try:\n        status_string = cpu_statuses[state.value]\n    except KeyError:\n        status_string = None\n    if (not status_string):\n        raise Snap7Exception(('The cpu state (%s) is invalid' % state.value))\n    logging.debug(('CPU state is %s' % status_string))\n    return status_string\n", "label": 0}
{"function": "\n\ndef test_local_bower_json_dependencies():\n    bower = bowerstatic.Bower()\n    components = bower.components('components', os.path.join(os.path.dirname(__file__), 'bower_components'))\n    local = bower.local_components('local', components)\n    path = os.path.join(os.path.dirname(__file__), 'local_component_deps')\n    local.component(path, version='2.0')\n\n    def wsgi(environ, start_response):\n        start_response('200 OK', [('Content-Type', 'text/html;charset=UTF-8')])\n        include = local.includer(environ)\n        include('local_component')\n        return [b'<html><head></head><body>Hello!</body></html>']\n    wrapped = bower.wrap(wsgi)\n    c = Client(wrapped)\n    response = c.get('/')\n    assert (response.body == b'<html><head><script type=\"text/javascript\" src=\"/bowerstatic/components/jquery/2.1.1/dist/jquery.js\"></script>\\n<script type=\"text/javascript\" src=\"/bowerstatic/local/local_component/2.0/local.js\"></script></head><body>Hello!</body></html>')\n", "label": 0}
{"function": "\n\ndef stackhut_api_call(endpoint, msg, secure=True, return_json=True):\n    url = urllib.parse.urljoin(utils.SERVER_URL, endpoint)\n    log.debug('Calling Stackhut Server at {} with \\n\\t{}'.format(url, json.dumps(msg)))\n    r = requests.post(url, data=json.dumps(msg), headers=json_header)\n    if (r.status_code == requests.codes.ok):\n        return (r.json() if return_json else r.text)\n    else:\n        log.error('Error {} talking to Stackhut Server'.format(r.status_code))\n        log.error(r.text)\n        r.raise_for_status()\n", "label": 0}
{"function": "\n\ndef test_basic(self):\n    'messages are sent and received properly'\n    question = b'sucess?'\n    answer = b'yeah, success'\n\n    def handler(sock):\n        text = sock.recv(1000)\n        assert (text == question)\n        sock.sendall(answer)\n    with Server(handler) as (host, port):\n        sock = socket.socket()\n        sock.connect((host, port))\n        sock.sendall(question)\n        text = sock.recv(1000)\n        assert (text == answer)\n        sock.close()\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    from pennyblack.models import Link, Newsletter\n    if ('mail' not in context):\n        return '#'\n    mail = context['mail']\n    newsletter = mail.job.newsletter\n    if newsletter.is_workflow():\n        job = newsletter.get_default_job()\n    else:\n        job = mail.job\n    try:\n        link = job.links.get(identifier=self.identifier)\n    except job.links.model.DoesNotExist:\n        link = Newsletter.add_view_link_to_job(self.identifier, job)\n    return (context['base_url'] + reverse('pennyblack.redirect_link', args=(mail.mail_hash, link.link_hash)))\n", "label": 0}
{"function": "\n\ndef IsAtEnd(self, string):\n    'Returns whether this position is at the end of the given string.\\n\\n    Args:\\n      string: The string to test for the end of.\\n\\n    Returns:\\n      Whether this position is at the end of the given string.\\n    '\n    return ((self.start == len(string)) and (self.length == 0))\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    'We want it to print as a Cycle, not as a dict.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.combinatorics import Cycle\\n        >>> Cycle(1, 2)\\n        (1 2)\\n        >>> print(_)\\n        (1 2)\\n        >>> list(Cycle(1, 2).items())\\n        [(1, 2), (2, 1)]\\n        '\n    if (not self):\n        return 'Cycle()'\n    cycles = Permutation(self).cyclic_form\n    s = ''.join((str(tuple(c)) for c in cycles))\n    big = (self.size - 1)\n    if (not any(((i == big) for c in cycles for i in c))):\n        s += ('(%s)' % big)\n    return ('Cycle%s' % s)\n", "label": 1}
{"function": "\n\n@plug.handler()\ndef delete_file(metadata):\n    try:\n        ignore_delete.add(metadata.path)\n        os.unlink(metadata.path)\n    except (IOError, OSError) as e:\n        ignore_delete.discard(metadata.path)\n        raise ServiceError(\"Error deleting file '{}': {}\".format(metadata.path, e))\n", "label": 0}
{"function": "\n\ndef test_write_a_file(self, gitfs_log):\n    content = 'Just a small file'\n    filename = '{}/new_file'.format(self.current_path)\n    with gitfs_log('SyncWorker: Set push_successful'):\n        with open(filename, 'w') as f:\n            f.write(content)\n    with open(filename) as f:\n        assert (f.read() == content)\n    with pull(self.sh):\n        self.assert_new_commit()\n        self.assert_commit_message('Update /new_file')\n", "label": 0}
{"function": "\n\ndef testConnectionLeaks(self):\n    for i in range(3):\n        self.assertEquals(self._countConnections(11211), 0)\n        new_conf = {\n            'init_config': {\n                \n            },\n            'instances': [{\n                'url': 'localhost',\n            }],\n        }\n        self.run_check(new_conf)\n        self.assertEquals(self._countConnections(11211), 0)\n", "label": 0}
{"function": "\n\ndef info2iob(sentence, chunks, informations):\n    info_list = ([], [], [])\n    for information in informations:\n        temp_list = positions(information, sentence)\n        for i in range(3):\n            if (temp_list[i] not in info_list[i]):\n                info_list[i].append(temp_list[i])\n    return tag_sent(chunks, info_list)\n", "label": 0}
{"function": "\n\ndef _setup_units(self, connections, params_dict, unknowns_dict):\n    '\\n        Calculate unit conversion factors for any connected\\n        variables having different units and store them in params_dict.\\n\\n        Args\\n        ----\\n        connections : dict\\n            A dict of target variables (absolute name) mapped\\n            to the absolute name of their source variable and the\\n            relevant indices of that source if applicable.\\n\\n        params_dict : OrderedDict\\n            A dict of parameter metadata for the whole `Problem`.\\n\\n        unknowns_dict : OrderedDict\\n            A dict of unknowns metadata for the whole `Problem`.\\n        '\n    to_prom_name = self.root._sysdata.to_prom_name\n    for (target, (source, idxs)) in iteritems(connections):\n        tmeta = params_dict[target]\n        smeta = unknowns_dict[source]\n        if (('units' not in tmeta) or ('units' not in smeta)):\n            continue\n        src_unit = smeta['units']\n        tgt_unit = tmeta['units']\n        try:\n            (scale, offset) = get_conversion_tuple(src_unit, tgt_unit)\n        except TypeError as err:\n            if (str(err) == 'Incompatible units'):\n                msg = \"Unit '{0}' in source {1} is incompatible with unit '{2}' in target {3}.\".format(src_unit, _both_names(smeta, to_prom_name), tgt_unit, _both_names(tmeta, to_prom_name))\n                self._setup_errors.append(msg)\n                continue\n            else:\n                raise\n        if ((scale != 1.0) or (offset != 0.0)):\n            tmeta['unit_conv'] = (scale, offset)\n", "label": 1}
{"function": "\n\n@classmethod\ndef load_missing(cls, cloud, object_id):\n    identity_client = clients.identity_client(cloud)\n    try:\n        raw_tenant = identity_client.tenants.get(object_id.id)\n        return cls.load_from_cloud(cloud, raw_tenant)\n    except exceptions.NotFound:\n        return None\n", "label": 0}
{"function": "\n\ndef find_duplicates(conn, limit=50, index=None):\n    query = 'SELECT f.id, fingerprint, length FROM fingerprint f LEFT JOIN fingerprint_deduplicate d ON f.id=d.id WHERE d.id IS NULL ORDER BY f.id LIMIT 1000'\n    for fingerprint in conn.execute(query):\n        find_track_duplicates(conn, fingerprint, index=index)\n", "label": 0}
{"function": "\n\ndef _delete(self, filter, multi=False):\n    if (filter is None):\n        filter = {\n            \n        }\n    if (not isinstance(filter, collections.Mapping)):\n        filter = {\n            '_id': filter,\n        }\n    to_delete = list(self.find(filter))\n    deleted_count = 0\n    for doc in to_delete:\n        doc_id = doc['_id']\n        if isinstance(doc_id, dict):\n            doc_id = helpers.hashdict(doc_id)\n        del self._documents[doc_id]\n        deleted_count += 1\n        if (not multi):\n            break\n    return {\n        'connectionId': self._database.client._id,\n        'n': deleted_count,\n        'ok': 1.0,\n        'err': None,\n    }\n", "label": 1}
{"function": "\n\ndef items(self):\n    'Dict-like items() that returns a list of name-value tuples from the jar.\\n        See keys() and values(). Allows client-code to call \"dict(RequestsCookieJar)\\n        and get a vanilla python dict of key value pairs.'\n    items = []\n    for cookie in iter(self):\n        items.append((cookie.name, cookie.value))\n    return items\n", "label": 0}
{"function": "\n\ndef intersects(self, other):\n    return ((other.start < self.end) or (self.start < other.end))\n", "label": 0}
{"function": "\n\ndef select_coins(self, colorvalue, use_fee_estimator=None):\n    self._validate_select_coins_parameters(colorvalue, use_fee_estimator)\n    colordef = colorvalue.get_colordef()\n    if (colordef == UNCOLORED_MARKER):\n        return self.select_uncolored_coins(colorvalue, use_fee_estimator)\n    color_id = colordef.get_color_id()\n    if (color_id in self.inputs):\n        total = SimpleColorValue.sum([cv_u[0] for cv_u in self.inputs[color_id]])\n        if (total < colorvalue):\n            msg = 'Not enough coins: %s requested, %s found!'\n            raise InsufficientFundsError((msg % (colorvalue, total)))\n        return ([cv_u[1] for cv_u in self.inputs[color_id]], total)\n    if (colorvalue > self.our_value_limit):\n        raise InsufficientFundsError(('%s requested, %s found!' % (colorvalue, self.our_value_limit)))\n    return super(OperationalETxSpec, self).select_coins(colorvalue)\n", "label": 1}
{"function": "\n\ndef match(self, left, collected=None):\n    collected = ([] if (collected is None) else collected)\n    outcomes = []\n    for p in self.children:\n        (matched, _, _) = outcome = p.match(left, collected)\n        if matched:\n            outcomes.append(outcome)\n    if outcomes:\n        return min(outcomes, key=(lambda outcome: len(outcome[1])))\n    return (False, left, collected)\n", "label": 0}
{"function": "\n\ndef _get_input(self):\n    (title, msg) = ('Get Input Dialog', 'Enter your name:')\n    (text, resp) = QtGui.QInputDialog.getText(self, title, msg)\n    if resp:\n        self.label.setText(web.safeunicode(text))\n", "label": 0}
{"function": "\n\ndef _replace_cdata_list_attribute_values(self, tag_name, attrs):\n    'Replaces class=\"foo bar\" with class=[\"foo\", \"bar\"]\\n\\n        Modifies its input in place.\\n        '\n    if (not attrs):\n        return attrs\n    if self.cdata_list_attributes:\n        universal = self.cdata_list_attributes.get('*', [])\n        tag_specific = self.cdata_list_attributes.get(tag_name.lower(), None)\n        for attr in list(attrs.keys()):\n            if ((attr in universal) or (tag_specific and (attr in tag_specific))):\n                value = attrs[attr]\n                if isinstance(value, str):\n                    values = whitespace_re.split(value)\n                else:\n                    values = value\n                attrs[attr] = values\n    return attrs\n", "label": 1}
{"function": "\n\ndef _sanitize_mod_params(self, other):\n    \"Sanitize the object being modded with this Message.\\n\\n        - Add support for modding 'None' so translation supports it\\n        - Trim the modded object, which can be a large dictionary, to only\\n        those keys that would actually be used in a translation\\n        - Snapshot the object being modded, in case the message is\\n        translated, it will be used as it was when the Message was created\\n        \"\n    if (other is None):\n        params = (other,)\n    elif isinstance(other, dict):\n        params = self._trim_dictionary_parameters(other)\n    else:\n        params = self._copy_param(other)\n    return params\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (not (type(other) is STP)):\n        return False\n    return ((self.network == other.network) and (self.port == other.port) and (self.label == other.label))\n", "label": 0}
{"function": "\n\ndef validate(self, doc):\n    if (not doc.get('_id', '')):\n        raise Exception('Attempting to store empty password.')\n    return doc\n", "label": 0}
{"function": "\n\ndef __init__(self, **kwargs):\n    dict_type = (kwargs.pop('dict_type', None) or OrderedDict)\n    ConfigParser.__init__(self, dict_type=dict_type, **kwargs)\n", "label": 0}
{"function": "\n\ndef setup():\n    '\\n    Create necessary directories for testing.\\n    '\n    train_dir = join(_my_dir, 'train')\n    if (not exists(train_dir)):\n        os.makedirs(train_dir)\n    output_dir = join(_my_dir, 'output')\n    if (not exists(output_dir)):\n        os.makedirs(output_dir)\n", "label": 0}
{"function": "\n\ndef render_result(result_data, test_bundle):\n    result_data = lcase_keys(result_data)\n    result_string = (sublime.expand_variables(RESULTS_TEMPLATES['results'], filter_stats_dict(result_data)) + '\\n')\n    for bundle in result_data['bundlestats']:\n        if (len(test_bundle) and (bundle['path'] != test_bundle)):\n            continue\n        result_string += (('\\n' + sublime.expand_variables(RESULTS_TEMPLATES['bundle'], filter_stats_dict(bundle))) + '\\n')\n        if isinstance(bundle['globalexception'], dict):\n            result_string += (('\\n' + sublime.expand_variables(RESULTS_TEMPLATES['global_exception'], filter_exception_dict(bundle['globalexception']))) + '\\n')\n        for suite in bundle['suitestats']:\n            result_string += ('\\n' + gen_suite_report(suite))\n    result_string += ('\\n' + RESULTS_TEMPLATES['legend'])\n    return result_string\n", "label": 1}
{"function": "\n\ndef _dictify(data, name='input', key_mod=(lambda x: x), value_mod=(lambda x: x)):\n    if data:\n        if isinstance(data, collections.Sequence):\n            return dict(((key_mod(str(v)), value_mod(str(v))) for v in data))\n        elif isinstance(data, collections.Mapping):\n            return dict(((key_mod(str(k)), value_mod(str((v or k)))) for (k, v) in list(data.items())))\n        else:\n            raise BlockadeConfigError(('invalid %s: need list or map' % (name,)))\n    else:\n        return {\n            \n        }\n", "label": 1}
{"function": "\n\ndef login_action(request):\n    username = request.REQUEST['username']\n    password = request.REQUEST['password']\n    user = authenticate(username=username, password=password)\n    if (user is None):\n        return json_failure(_('Invalid username or password'))\n    if (not user.is_active):\n        return json_failure(_('Account disabled.'))\n    login(request, user)\n    return json_response()\n", "label": 0}
{"function": "\n\ndef message_user(self, request, message, level=messages.INFO, extra_tags='', fail_silently=False):\n    '\\n        Send a message to the user. The default implementation\\n        posts a message using the django.contrib.messages backend.\\n\\n        Exposes almost the same API as messages.add_message(), but accepts the\\n        positional arguments in a different order to maintain backwards\\n        compatibility. For convenience, it accepts the `level` argument as\\n        a string rather than the usual level number.\\n        '\n    if (not isinstance(level, int)):\n        try:\n            level = getattr(messages.constants, level.upper())\n        except AttributeError:\n            levels = messages.constants.DEFAULT_TAGS.values()\n            levels_repr = ', '.join((('`%s`' % l) for l in levels))\n            raise ValueError(('Bad message level string: `%s`. Possible values are: %s' % (level, levels_repr)))\n    messages.add_message(request, level, message, extra_tags=extra_tags, fail_silently=fail_silently)\n", "label": 0}
{"function": "\n\ndef add(self, grid):\n    '\\n        Used to add quantities from another grid\\n\\n        Parameters\\n        ----------\\n        grid : 3D Numpy array or SphericalPolarGridView instance\\n            The grid to copy the quantity from\\n        '\n    if (type(self.quantities[self.viewed_quantity]) is list):\n        raise Exception('need to first specify the item to add to')\n    if isinstance(grid, SphericalPolarGridView):\n        if (type(grid.quantities[grid.viewed_quantity]) is list):\n            raise Exception('need to first specify the item to add')\n        self._check_array_dimensions(grid.quantities[grid.viewed_quantity])\n        self.quantities[self.viewed_quantity] += grid.quantities[grid.viewed_quantity]\n    elif isinstance(grid, np.ndarray):\n        self._check_array_dimensions(grid)\n        self.quantities[self.viewed_quantity] += grid\n    else:\n        raise ValueError('grid should be a Numpy array or a SphericalPolarGridView instance')\n", "label": 0}
{"function": "\n\ndef create_junk():\n    fileh = open_file(filename, mode='w')\n    group = fileh.create_group(fileh.root, 'newgroup')\n    for i in range(NLEAVES):\n        table = fileh.create_table(group, ('table' + str(i)), Particle, 'A table', Filters(1))\n        particle = table.row\n        print('Creating table-->', table._v_name)\n        for i in range(NROWS):\n            particle.append()\n        table.flush()\n    fileh.close()\n", "label": 0}
{"function": "\n\ndef _send_500(req, extra=None):\n    req.start_response('500 Error', [('Content-Type', 'text/html')])\n    req.write('<h1>500 Internal Server Error</h1>\\n')\n    req.write('The server encountered an internal error or misconfiguration and was unable to complete your request.\\n')\n    if (extra is not None):\n        req.write(extra)\n    req.close()\n", "label": 0}
{"function": "\n\ndef __iter__(self):\n    for d in reversed(self.dicts):\n        (yield d)\n", "label": 0}
{"function": "\n\ndef getopt(args, shortopts):\n    'getopt(args, options) -> opts, long_opts, args \\nReturns options as list of tuples, long options as entries in a dictionary, and\\nthe remaining arguments.'\n    opts = []\n    longopts = {\n        \n    }\n    while (args and args[0].startswith('-') and (args[0] != '-')):\n        if (args[0] == '--'):\n            args = args[1:]\n            break\n        if args[0].startswith('--'):\n            arg = args.pop(0)\n            _do_longs(longopts, arg)\n        else:\n            (opts, args) = _do_shorts(opts, args[0][1:], shortopts, args[1:])\n    return (opts, longopts, args)\n", "label": 1}
{"function": "\n\n@app.route('/api')\ndef api():\n    \"WebSocket endpoint; Takes a 'topic' GET param.\"\n    ws = request.environ.get('wsgi.websocket')\n    topic = request.args.get('topic')\n    if (None in (ws, topic)):\n        return\n    topic = topic.encode('ascii')\n    for (message, message_topic) in CircusConsumer(topic, endpoint=ZMQ_ENDPOINT):\n        response = json.dumps(dict(message=message, topic=message_topic))\n        ws.send(response)\n", "label": 0}
{"function": "\n\ndef test_active_contributors(self):\n    'Test the active_contributors util method.'\n    start_date = self.start_date\n    en_us_contributors = active_contributors(from_date=start_date, locale='en-US')\n    es_contributors = active_contributors(from_date=start_date, locale='es')\n    all_contributors = active_contributors(from_date=start_date)\n    eq_(3, len(en_us_contributors))\n    assert (self.user in en_us_contributors)\n    assert (self.en_us_old.creator not in en_us_contributors)\n    eq_(4, len(es_contributors))\n    assert (self.user in es_contributors)\n    assert (self.es_old.creator not in es_contributors)\n    eq_(6, len(all_contributors))\n    assert (self.user in all_contributors)\n    assert (self.en_us_old.creator not in all_contributors)\n    assert (self.es_old.creator not in all_contributors)\n", "label": 1}
{"function": "\n\ndef _cleanup_groups(self):\n    exception_list = list()\n    for group in self.cloud.list_groups():\n        if group['name'].startswith(self.group_prefix):\n            try:\n                self.cloud.delete_group(group['id'])\n            except Exception as e:\n                exception_list.append(str(e))\n                continue\n    if exception_list:\n        raise OpenStackCloudException('\\n'.join(exception_list))\n", "label": 0}
{"function": "\n\ndef _expand_glob_path(file_roots):\n    '\\n    Applies shell globbing to a set of directories and returns\\n    the expanded paths\\n    '\n    unglobbed_path = []\n    for path in file_roots:\n        try:\n            if glob.has_magic(path):\n                unglobbed_path.extend(glob.glob(path))\n            else:\n                unglobbed_path.append(path)\n        except Exception:\n            unglobbed_path.append(path)\n    return unglobbed_path\n", "label": 0}
{"function": "\n\ndef authenticate(self):\n    (client_address, _) = self.client_address\n    NAMESPACE.machine = NAMESPACE.session.query(Builder).filter_by(ip=client_address).first()\n    NAMESPACE.user = NAMESPACE.session.query(Person).filter_by(ip=client_address).first()\n    return (NAMESPACE.machine or NAMESPACE.user)\n", "label": 0}
{"function": "\n\ndef listKeysAndSizes(self, bucketName):\n    'Return a list of (name, size) pairs of keys in the bucket'\n    with self.state_.lock:\n        if (bucketName not in self.state_.buckets_):\n            raise S3Interface.BucketNotFound(bucketName)\n        self.state_.validateAccess(bucketName, self.credentials_)\n        return [(key, len(val.value), val.mtime) for (key, val) in self.state_.buckets_[bucketName].iteritems()]\n", "label": 0}
{"function": "\n\ndef cleanup(self, tc_name=''):\n    if (tc_name != ''):\n        self._log.info(('%s: FAILED' % tc_name))\n    for obj in ['ruleset', 'rule', 'classifier', 'action']:\n        self.gbpcfg.gbp_del_all_anyobj(obj)\n", "label": 0}
{"function": "\n\ndef _geo_field(self, field_name=None):\n    \"\\n        Returns the first Geometry field encountered; or specified via the\\n        `field_name` keyword.  The `field_name` may be a string specifying\\n        the geometry field on this GeoQuery's model, or a lookup string\\n        to a geometry field via a ForeignKey relation.\\n        \"\n    if (field_name is None):\n        for fld in self.model._meta.fields:\n            if isinstance(fld, GeometryField):\n                return fld\n        return False\n    else:\n        return GeoWhereNode._check_geo_field(self.model._meta, field_name)\n", "label": 0}
{"function": "\n\ndef compute(self):\n    if self.has_input('foo'):\n        v1 = self.get_input('foo')\n    else:\n        v1 = 0\n    if (v1 != 12):\n        self.change_parameter('foo', (v1 + 1))\n", "label": 0}
{"function": "\n\ndef __init__(self, parentContainer, localId, randomSeed=1, numCalls=1, variantDensity=1):\n    super(SimulatedVariantSet, self).__init__(parentContainer, localId)\n    self._randomSeed = randomSeed\n    self._numCalls = numCalls\n    for j in range(numCalls):\n        self.addCallSet('simCallSet_{}'.format(j))\n    self._variantDensity = variantDensity\n    now = protocol.convertDatetime(datetime.datetime.now())\n    self._creationTime = now\n    self._updatedTime = now\n", "label": 0}
{"function": "\n\n@staticmethod\ndef overwrite_attribute(entity_id, attrs, vals):\n    'Overwrite any attribute of an entity.\\n\\n        This function should receive a list of attributes and a\\n        list of values. Set attribute to None to remove any overwritten\\n        value in place.\\n        '\n    for (attr, val) in zip(attrs, vals):\n        if (val is None):\n            _OVERWRITE[entity_id.lower()].pop(attr, None)\n        else:\n            _OVERWRITE[entity_id.lower()][attr] = val\n", "label": 0}
{"function": "\n\ndef _check_params(length, size):\n    _check_size(size)\n    if ((length % size) != 0):\n        raise error('not a whole number of frames')\n", "label": 0}
{"function": "\n\ndef ex_update_node_affinity_group(self, node, affinity_group_list):\n    '\\n        Updates the affinity/anti-affinity group associations of a virtual\\n        machine. The VM has to be stopped and restarted for the new properties\\n        to take effect.\\n\\n        :param node: Node to update.\\n        :type node: :class:`CloudStackNode`\\n\\n        :param affinity_group_list: List of CloudStackAffinityGroup to\\n                                    associate\\n        :type affinity_group_list: ``list`` of :class:`CloudStackAffinityGroup`\\n\\n        :rtype :class:`CloudStackNode`\\n        '\n    affinity_groups = ','.join((ag.id for ag in affinity_group_list))\n    result = self._async_request(command='updateVMAffinityGroup', params={\n        'id': node.id,\n        'affinitygroupids': affinity_groups,\n    }, method='GET')\n    return self._to_node(data=result['virtualmachine'])\n", "label": 0}
{"function": "\n\ndef html_body(input_string, source_path=None, destination_path=None, input_encoding='unicode', output_encoding='unicode', doctitle=1, initial_header_level=1):\n    '\\n    Given an input string, returns an HTML fragment as a string.\\n\\n    The return value is the contents of the <body> element.\\n\\n    Parameters (see `html_parts()` for the remainder):\\n\\n    - `output_encoding`: The desired encoding of the output.  If a Unicode\\n      string is desired, use the default value of \"unicode\" .\\n    '\n    parts = html_parts(input_string=input_string, source_path=source_path, destination_path=destination_path, input_encoding=input_encoding, doctitle=doctitle, initial_header_level=initial_header_level)\n    fragment = parts['html_body']\n    if (output_encoding != 'unicode'):\n        fragment = fragment.encode(output_encoding)\n    return fragment\n", "label": 0}
{"function": "\n\ndef init(self, modelDocument):\n    super(ModelRssItem, self).init(modelDocument)\n    try:\n        if (self.modelXbrl.modelManager.rssWatchOptions.latestPubDate and (self.pubDate <= self.modelXbrl.modelManager.rssWatchOptions.latestPubDate)):\n            self.status = _('tested')\n        else:\n            self.status = _('not tested')\n    except AttributeError:\n        self.status = _('not tested')\n    self.results = None\n    self.assertions = None\n", "label": 0}
{"function": "\n\ndef rgb_to_hsv(r, g, b):\n    maxc = max(r, g, b)\n    minc = min(r, g, b)\n    v = maxc\n    if (minc == maxc):\n        return (0.0, 0.0, v)\n    s = ((maxc - minc) / maxc)\n    rc = ((maxc - r) / (maxc - minc))\n    gc = ((maxc - g) / (maxc - minc))\n    bc = ((maxc - b) / (maxc - minc))\n    if (r == maxc):\n        h = (bc - gc)\n    elif (g == maxc):\n        h = ((2.0 + rc) - bc)\n    else:\n        h = ((4.0 + gc) - rc)\n    h = ((h / 6.0) % 1.0)\n    return (h, s, v)\n", "label": 0}
{"function": "\n\ndef build_xform(self):\n    xform = XFormBuilder(self.name)\n    for ig in self.iter_item_groups():\n        data_type = ('repeatGroup' if self.is_repeating else 'group')\n        group = xform.new_group(ig.question_name, ig.question_label, data_type)\n        for item in ig.iter_items():\n            group.new_question(item.question_name, item.question_label, ODK_DATA_TYPES[item.data_type], choices=item.choices)\n    return xform.tostring(pretty_print=True, encoding='utf-8', xml_declaration=True)\n", "label": 0}
{"function": "\n\ndef permutations(xs):\n    if (not xs):\n        (yield [])\n    else:\n        for (y, ys) in selections(xs):\n            for pys in permutations(ys):\n                (yield ([y] + pys))\n", "label": 0}
{"function": "\n\ndef on_response(self, response):\n    self.stop()\n    self.got_response = True\n    if (not (response.status_code == 418)):\n        self.response_valid = False\n", "label": 0}
{"function": "\n\ndef _annotate_local(self):\n    \"Annotate the primaryjoin and secondaryjoin\\n        structures with 'local' annotations.\\n\\n        This annotates all column elements found\\n        simultaneously in the parent table\\n        and the join condition that don't have a\\n        'remote' annotation set up from\\n        _annotate_remote() or user-defined.\\n\\n        \"\n    if self._has_annotation(self.primaryjoin, 'local'):\n        return\n    if self._local_remote_pairs:\n        local_side = util.column_set([l for (l, r) in self._local_remote_pairs])\n    else:\n        local_side = util.column_set(self.parent_selectable.c)\n\n    def locals_(elem):\n        if (('remote' not in elem._annotations) and (elem in local_side)):\n            return elem._annotate({\n                'local': True,\n            })\n    self.primaryjoin = visitors.replacement_traverse(self.primaryjoin, {\n        \n    }, locals_)\n", "label": 0}
{"function": "\n\ndef __cmp__(self, other):\n    'ensure that same seq intervals match in cmp()'\n    if (not isinstance(other, SeqPath)):\n        return (- 1)\n    if (self.path is other.path):\n        return cmp((self.start, self.stop), (other.start, other.stop))\n    else:\n        return NOT_ON_SAME_PATH\n", "label": 0}
{"function": "\n\n@attr('numpy')\ndef test_empty(self):\n    try:\n        import numpy\n    except ImportError:\n        raise SkipTest('numpy not available.')\n    G = networkx.Graph()\n    assert_equal(networkx.hits(G), ({\n        \n    }, {\n        \n    }))\n    assert_equal(networkx.hits_numpy(G), ({\n        \n    }, {\n        \n    }))\n    assert_equal(networkx.authority_matrix(G).shape, (0, 0))\n    assert_equal(networkx.hub_matrix(G).shape, (0, 0))\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    if ((not self.frozen_by_south) and (name not in [f.name for f in cls._meta.fields])):\n        super(CurrencyField, self).contribute_to_class(cls, name)\n", "label": 0}
{"function": "\n\ndef dapInfo(self, id_):\n    cmd = []\n    cmd.append(COMMAND_ID['DAP_INFO'])\n    cmd.append(ID_INFO[id_])\n    self.interface.write(cmd)\n    resp = self.interface.read()\n    if (resp[0] != COMMAND_ID['DAP_INFO']):\n        raise ValueError('DAP_INFO response error')\n    if (resp[1] == 0):\n        return\n    if (id_ in ('CAPABILITIES', 'PACKET_COUNT', 'PACKET_SIZE')):\n        if (resp[1] == 1):\n            return resp[2]\n        if (resp[1] == 2):\n            return ((resp[3] << 8) | resp[2])\n    x = array.array('B', [i for i in resp[2:(2 + resp[1])]])\n    return x.tostring()\n", "label": 1}
{"function": "\n\ndef test_pos_list_append_with_nonexistent_key(self):\n    '\\n        Invoke list_append() with non-existent key\\n        '\n    charSet = 'abcdefghijklmnopqrstuvwxyz1234567890'\n    minLength = 5\n    maxLength = 30\n    length = random.randint(minLength, maxLength)\n    key = ('test', 'demo', (''.join(map((lambda unused: random.choice(charSet)), range(length))) + '.com'))\n    status = self.as_connection.list_append(key, 'abc', 122)\n    assert (status == 0)\n    (key, _, bins) = self.as_connection.get(key)\n    self.as_connection.remove(key)\n    assert (status == 0)\n    assert (bins == {\n        'abc': [122],\n    })\n", "label": 0}
{"function": "\n\ndef fingerprint(self):\n    try:\n        pubkey = sshpubkeys.SSHKey(self.key)\n        return pubkey.hash()\n    except:\n        'There are a small parcel of exceptions that can be throw to indicate invalid keys'\n        return ''\n", "label": 0}
{"function": "\n\ndef start(self):\n    'Start watching the directory for changes.'\n    with self._inotify_fd_lock:\n        if (self._inotify_fd < 0):\n            return\n        self._inotify_poll.register(self._inotify_fd, select.POLLIN)\n        for directory in self._directories:\n            self._add_watch_for_path(directory)\n", "label": 0}
{"function": "\n\n@property\ndef servicenames(self):\n    'Give the list of services available in this folder.'\n    return set([service['name'].rstrip('/').split('/')[(- 1)] for service in self._json_struct.get('services', [])])\n", "label": 0}
{"function": "\n\ndef is_lazy_user(user):\n    ' Return True if the passed user is a lazy user. '\n    if user.is_anonymous():\n        return False\n    backend = getattr(user, 'backend', None)\n    if (backend == 'lazysignup.backends.LazySignupBackend'):\n        return True\n    from lazysignup.models import LazyUser\n    return bool((LazyUser.objects.filter(user=user).count() > 0))\n", "label": 0}
{"function": "\n\ndef check_variable(self, name):\n    ' check_variable(name: str) -> Boolean\\n        Returns True if the vistrail already has the variable name\\n\\n        '\n    variableBox = self.parent().parent().parent()\n    if variableBox.controller:\n        return variableBox.controller.check_vistrail_variable(name)\n    return False\n", "label": 0}
{"function": "\n\n@click.command()\n@click.argument('identifier')\n@click.option('--postinstall', '-i', help='Post-install script to download')\n@click.option('--image', help=\"Image ID. The default is to use the current operating system.\\nSee: 'slcli image list' for reference\")\n@helpers.multi_option('--key', '-k', help='SSH keys to add to the root user')\n@environment.pass_env\ndef cli(env, identifier, postinstall, key, image):\n    'Reload operating system on a virtual server.'\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    keys = []\n    if key:\n        for single_key in key:\n            resolver = SoftLayer.SshKeyManager(env.client).resolve_ids\n            key_id = helpers.resolve_id(resolver, single_key, 'SshKey')\n            keys.append(key_id)\n    if (not (env.skip_confirmations or formatting.no_going_back(vs_id))):\n        raise exceptions.CLIAbort('Aborted')\n    vsi.reload_instance(vs_id, post_uri=postinstall, ssh_keys=keys, image_id=image)\n", "label": 0}
{"function": "\n\ndef findPeak(self, A):\n    '\\n        Binary search\\n        Microsoft Interview, Oct 2014\\n\\n        To reduce the complexity of dealing the edge cases:\\n        * add two anti-peak dummies on the both ends\\n\\n        :param A: An integers list. A[0] and A[-1] are dummies.\\n        :return: return any of peek positions.\\n        '\n    n = len(A)\n    l = 0\n    h = n\n    while (l < h):\n        m = ((l + h) / 2)\n        if (A[(m - 1)] < A[m] > A[(m + 1)]):\n            return m\n        elif (A[(m + 1)] > A[m]):\n            l = (m + 1)\n        else:\n            h = m\n    raise Exception\n", "label": 0}
{"function": "\n\ndef season_by_id(season_id):\n    url = endpoints.season_by_id.format(season_id)\n    q = _query_endpoint(url)\n    if q:\n        return Season(q)\n    else:\n        raise SeasonNotFound(\"Couldn't find Season with ID: {0}\".format(season_id))\n", "label": 0}
{"function": "\n\ndef check_migrations(self):\n    \"\\n        Checks to see if the set of migrations on disk matches the\\n        migrations in the database. Prints a warning if they don't match.\\n        \"\n    executor = MigrationExecutor(connections[DEFAULT_DB_ALIAS])\n    plan = executor.migration_plan(executor.loader.graph.leaf_nodes())\n    if (plan and self.show_startup_messages):\n        self.stdout.write(self.style.NOTICE('\\nYou have unapplied migrations; your app may not work properly until they are applied.'))\n        self.stdout.write(self.style.NOTICE(\"Run 'python manage.py migrate' to apply them.\\n\"))\n", "label": 0}
{"function": "\n\ndef friend_list(self, player_id):\n    return ' '.join([user_manager.id_to_name(friend_id) for friend_id in fetch_set_keys(friend_key(player_id))])\n", "label": 0}
{"function": "\n\ndef onModelChanged(self, model):\n    newTrackPosition = np.array(self.jointController.q[:3])\n    delta = (newTrackPosition - self.lastTrackPosition)\n    for i in xrange(3):\n        if (not self.followAxes[i]):\n            delta[i] = 0.0\n    self.lastTrackPosition = newTrackPosition\n    c = self.view.camera()\n    oldFocalPoint = np.array(c.GetFocalPoint())\n    oldPosition = np.array(c.GetPosition())\n    c.SetFocalPoint((oldFocalPoint + delta))\n    c.SetPosition((oldPosition + delta))\n    self.view.render()\n", "label": 0}
{"function": "\n\ndef test_invalid_base_fields(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.ForeignKey('testapp.Author'), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E005')\n    assert ('Base field for list must be' in errors[0].msg)\n", "label": 0}
{"function": "\n\n@contextlib.contextmanager\ndef _assert_warns_context_manager(warning_class=None, warnings_test=None):\n    '\\n    Builds a context manager for testing code that should throw a warning.\\n    This will look for a given class, call a custom test, or both.\\n\\n    Args:\\n        warning_class - a class or subclass of Warning. If not None, then\\n            the context manager will raise an AssertionError if the block\\n            does not throw at least one warning of that type.\\n        warnings_test - a function which takes a list of warnings caught,\\n            and makes a number of assertions about the result. If the function\\n            returns without an exception, the context manager will consider\\n            this a successful assertion.\\n    '\n    with warnings.catch_warnings(record=True) as caught:\n        warnings.resetwarnings()\n        if warning_class:\n            warnings.simplefilter('ignore')\n            warnings.simplefilter('always', category=warning_class)\n        else:\n            warnings.simplefilter('always')\n        (yield)\n        assert_gt(len(caught), 0, 'expected at least one warning to be thrown')\n        if warnings_test:\n            warnings_test(caught)\n", "label": 0}
{"function": "\n\ndef log_notifications(self, notifications):\n    main_logger = logging.getLogger(config.main_logger_name)\n    notification_logger = logging.getLogger(config.notifications_logger_name)\n    for notification in notifications:\n        try:\n            notification['content'] = notification['content'].encode('utf-8').replace(',', '\\\\,')\n            keys = ['status', 'login_id', 'content', 'message_id', 'campaign_id', 'sending_id', 'game', 'world_id', 'screen', 'time', 'time_to_live_ts_bigint', 'platform', 'receiver_id']\n            notification_logger.info(','.join([str(notification[key]) for key in keys]))\n        except:\n            main_logger.exception('Error while logging notification to csv log!')\n", "label": 0}
{"function": "\n\n@replace_call(BaseDatabaseWrapper.cursor)\ndef cursor(func, self):\n    djdt = DebugToolbarMiddleware.get_current()\n    if djdt:\n        djdt._panels[SQLDebugPanel] = djdt.get_panel(SQLLoggingPanel)\n    return func(self)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    if isinstance(key, int):\n        return list(self.values()).__getitem__(key)\n    elif isinstance(key, slice):\n        items = list(self.items()).__getitem__(key)\n        return Layers(items)\n    else:\n        return super(Layers, self).__getitem__(key)\n", "label": 0}
{"function": "\n\ndef load(self, config):\n    self.items = collections.OrderedDict()\n    values = config.get('axes', self.name).split(',')\n    if config.has_section(('axis:%s' % self.name)):\n        self.defaults = collections.OrderedDict(config.items(('axis:%s' % self.name)))\n    else:\n        self.defaults = {\n            \n        }\n    for value in values:\n        self.items[value.strip('*')] = AxisItem(self, value, config)\n", "label": 0}
{"function": "\n\ndef write_output(args, powerline, segment_info, write):\n    if args.renderer_arg:\n        segment_info.update(args.renderer_arg)\n    if args.side.startswith('above'):\n        for line in powerline.render_above_lines(width=args.width, segment_info=segment_info, mode=segment_info.get('mode', None)):\n            if line:\n                write((line + '\\n'))\n        args.side = args.side[len('above'):]\n    if args.side:\n        rendered = powerline.render(width=args.width, side=args.side, segment_info=segment_info, mode=segment_info.get('mode', None))\n        write(rendered)\n", "label": 1}
{"function": "\n\ndef lines(self, text):\n    for line in text.split('\\n'):\n        (yield ('         \"%s\\\\n\"' % escape_quote(line)))\n", "label": 0}
{"function": "\n\n@pg.production('binop_expr : binop_expr PIPELINE_FIRST_BIND binop_expr')\ndef binop_expr(p):\n    (left, _, right) = p\n    input_sym = get_temp_name()\n    return [Symbol('|>'), p[0], [Symbol('bind'), [Symbol('fn'), [input_sym], ([p[2][0], input_sym] + p[2][(1 if (len(p[2]) > 1) else len(p[2])):])]]]\n", "label": 0}
{"function": "\n\ndef write_file(filename, content):\n    'Write content to file.'\n    (_dir, _) = os.path.split(filename)\n    if (not os.path.exists(_dir)):\n        logging.debug('The directory %s not exists, create it', _dir)\n        mkdir_p(_dir)\n    with io.open(filename, 'wt', encoding='utf-8') as fd:\n        fd.write(content)\n", "label": 0}
{"function": "\n\ndef index(self, keypair_list):\n    return dict(keypairs=[self._base_response(keypair) for keypair in keypair_list])\n", "label": 0}
{"function": "\n\ndef validate(self, value):\n    super(Interval, self).validate(value)\n    if (not ((value is None) or (self.interval_type.is_valid(value) and (value >= self.start) and (value <= self.end)))):\n        raise ValueError(('expected a value of type %s in range [%s, %s], got %r' % (self.interval_type, self.start, self.end, value)))\n", "label": 0}
{"function": "\n\ndef _on_nick(self, c, e):\n    '[Internal]'\n    before = nm_to_n(e.source())\n    after = e.target()\n    for ch in self.channels.values():\n        if ch.has_user(before):\n            ch.change_nick(before, after)\n", "label": 0}
{"function": "\n\ndef test_K4_normalized(self):\n    'Betweenness centrality: K4'\n    G = networkx.complete_graph(4)\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True)\n    b_answer = {\n        0: 0.25,\n        1: 0.25,\n        2: 0.25,\n        3: 0.25,\n    }\n    for n in sorted(G):\n        assert_almost_equal(b[n], b_answer[n])\n    G.add_edge(0, 1, {\n        'weight': 0.5,\n        'other': 0.3,\n    })\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True, weight=None)\n    for n in sorted(G):\n        assert_almost_equal(b[n], b_answer[n])\n    wb_answer = {\n        0: 0.2222222,\n        1: 0.2222222,\n        2: 0.30555555,\n        3: 0.30555555,\n    }\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert_almost_equal(b[n], wb_answer[n])\n    wb_answer = {\n        0: 0.2051282,\n        1: 0.2051282,\n        2: 0.33974358,\n        3: 0.33974358,\n    }\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True, weight='other')\n    for n in sorted(G):\n        assert_almost_equal(b[n], wb_answer[n])\n", "label": 0}
{"function": "\n\ndef get_auth_params(self, request, action):\n    settings = self.get_settings()\n    ret = settings.get('AUTH_PARAMS', {\n        \n    })\n    dynamic_auth_params = request.GET.get('auth_params', None)\n    if dynamic_auth_params:\n        ret.update(dict(parse_qsl(dynamic_auth_params)))\n    return ret\n", "label": 0}
{"function": "\n\ndef _emit(self, record, stream):\n    self.stream = stream\n    try:\n        return logging.StreamHandler.emit(self, record)\n    except:\n        raise\n    else:\n        self.stream = None\n", "label": 0}
{"function": "\n\ndef data_path(path, createdir=False):\n    'If path is relative, return the given path inside the project data dir,\\n    otherwise return the path unmodified\\n    '\n    if (not isabs(path)):\n        path = join(project_data_dir(), path)\n    if (createdir and (not exists(path))):\n        os.makedirs(path)\n    return path\n", "label": 0}
{"function": "\n\ndef __init__(self, groupDateTime, bars, frequency):\n    resamplebase.Grouper.__init__(self, groupDateTime)\n    self.__barGroupers = {\n        \n    }\n    self.__frequency = frequency\n    for (instrument, bar_) in bars.items():\n        barGrouper = resampled.BarGrouper(groupDateTime, bar_, frequency)\n        self.__barGroupers[instrument] = barGrouper\n", "label": 0}
{"function": "\n\ndef ex_attach_nic_to_node(self, node, network, ip_address=None):\n    \"\\n        Add an extra Nic to a VM\\n\\n        :param  network: NetworkOffering object\\n        :type   network: :class:'CloudStackNetwork`\\n\\n        :param  node: Node Object\\n        :type   node: :class:'CloudStackNode`\\n\\n        :param  ip_address: Optional, specific IP for this Nic\\n        :type   ip_address: ``str``\\n\\n\\n        :rtype: ``bool``\\n        \"\n    args = {\n        'virtualmachineid': node.id,\n        'networkid': network.id,\n    }\n    if (ip_address is not None):\n        args['ipaddress'] = ip_address\n    self._async_request(command='addNicToVirtualMachine', params=args)\n    return True\n", "label": 0}
{"function": "\n\ndef longRunHighGrayLevelEmphasis(self, P_glrl, ivector, jvector, sumP_glrl, meanFlag=True):\n    try:\n        lrhgle = (numpy.sum(numpy.sum(((P_glrl * (ivector ** 2)[:, None, None]) * (jvector ** 2)[None, :, None]), 0), 0) / sumP_glrl[None, None, :])\n    except ZeroDivisionError:\n        lrhgle = 0\n    if meanFlag:\n        return lrhgle.mean()\n    else:\n        return lrhgle\n", "label": 0}
{"function": "\n\ndef bind(self, lan):\n    'bind to a LAN.'\n    if _debug:\n        Node._debug('bind %r', lan)\n    lan.add_node(self)\n", "label": 0}
{"function": "\n\ndef columns_used(self):\n    '\\n        Returns all the columns used across all models in the group\\n        for filtering and in the model expression.\\n\\n        '\n    return list(tz.unique(tz.concat((m.columns_used() for m in self.models.values()))))\n", "label": 0}
{"function": "\n\ndef S_e(self, prob):\n    '\\n        Electric source term\\n\\n        :param Problem prob: FDEM Problem\\n        :rtype: numpy.ndarray\\n        :return: electric source term on mesh\\n        '\n    if ((prob._formulation is 'EB') and (self.integrate is True)):\n        return (prob.Me * self._S_e)\n    return self._S_e\n", "label": 0}
{"function": "\n\ndef __call__(self, fn):\n\n    def wrapper(*args, **kwargs):\n        that = args[0]\n        that.logger.debug(self.start)\n        ret = fn(*args, **kwargs)\n        that.logger.debug(self.finish)\n        if self.getter:\n            that.logger.debug(pformat(self.getter(ret)))\n        else:\n            that.logger.debug(pformat(ret))\n        return ret\n    wrapper.func_name = fn.func_name\n    if hasattr(fn, '__name__'):\n        wrapper.__name__ = self.name = fn.__name__\n    if hasattr(fn, '__doc__'):\n        wrapper.__doc__ = fn.__doc__\n    if hasattr(fn, '__module__'):\n        wrapper.__module__ = fn.__module__\n    return wrapper\n", "label": 0}
{"function": "\n\ndef write_packed(self, outfile, rows):\n    '\\n        Write PNG file to `outfile`.  The pixel data comes from `rows`\\n        which should be in boxed row packed format.  Each row should be\\n        a sequence of packed bytes.\\n\\n        Technically, this method does work for interlaced images but it\\n        is best avoided.  For interlaced images, the rows should be\\n        presented in the order that they appear in the file.\\n\\n        This method should not be used when the source image bit depth\\n        is not one naturally supported by PNG; the bit depth should be\\n        1, 2, 4, 8, or 16.\\n        '\n    if self.rescale:\n        raise Error(('write_packed method not suitable for bit depth %d' % self.rescale[0]))\n    return self.write_passes(outfile, rows, packed=True)\n", "label": 0}
{"function": "\n\ndef clean_votes(self, value):\n    assert (value > 0), 'Must be greater than 0.'\n    assert (value < 51), 'Must be less than 51.'\n    return value\n", "label": 0}
{"function": "\n\ndef _print_slots(self):\n    slots = ', '.join((((\"'\" + snake(name)) + \"'\") for (type, name, nullable, plural) in self._fields))\n    print(\"    __slots__ = ('loc', {slots},)\".format(slots=slots))\n", "label": 0}
{"function": "\n\ndef register_scheme(scheme):\n    for method in dir(urlparse):\n        if method.startswith('uses_'):\n            getattr(urlparse, method).append(scheme)\n", "label": 0}
{"function": "\n\ndef configure_uploads(app, upload_sets):\n    \"\\n    Call this after the app has been configured. It will go through all the\\n    upload sets, get their configuration, and store the configuration on the\\n    app. It will also register the uploads module if it hasn't been set. This\\n    can be called multiple times with different upload sets.\\n    \\n    .. versionchanged:: 0.1.3\\n       The uploads module/blueprint will only be registered if it is needed\\n       to serve the upload sets.\\n    \\n    :param app: The `~flask.Flask` instance to get the configuration from.\\n    :param upload_sets: The `UploadSet` instances to configure.\\n    \"\n    if isinstance(upload_sets, UploadSet):\n        upload_sets = (upload_sets,)\n    if (not hasattr(app, 'upload_set_config')):\n        app.upload_set_config = {\n            \n        }\n    set_config = app.upload_set_config\n    defaults = dict(dest=app.config.get('UPLOADS_DEFAULT_DEST'), url=app.config.get('UPLOADS_DEFAULT_URL'))\n    for uset in upload_sets:\n        config = config_for_set(uset, app, defaults)\n        set_config[uset.name] = config\n    should_serve = any(((s.base_url is None) for s in set_config.itervalues()))\n    if using_blueprints:\n        if (('_uploads' not in app.blueprints) and should_serve):\n            app.register_blueprint(uploads_mod)\n    elif (('_uploads' not in app.modules) and should_serve):\n        app.register_module(uploads_mod)\n", "label": 1}
{"function": "\n\ndef change_primary_name(self, name):\n    '\\n        Changes the primary/default name of the policy to a specified name.\\n\\n        :param name: a string name to replace the current primary name.\\n        '\n    if (name == self.name):\n        return\n    elif (name in self.alias_list):\n        self.remove_name(name)\n    else:\n        self._validate_policy_name(name)\n    self.alias_list.insert(0, name)\n", "label": 0}
{"function": "\n\ndef configure_host(self):\n    if self.mail.use_ssl:\n        host = smtplib.SMTP_SSL(self.mail.server, self.mail.port)\n    else:\n        host = smtplib.SMTP(self.mail.server, self.mail.port)\n    host.set_debuglevel(int(self.mail.debug))\n    if self.mail.use_tls:\n        host.starttls()\n    if (self.mail.username and self.mail.password):\n        host.login(self.mail.username, self.mail.password)\n    return host\n", "label": 0}
{"function": "\n\ndef host_to_ip(host):\n    '\\n    Returns the IP address of a given hostname\\n    '\n    try:\n        (family, socktype, proto, canonname, sockaddr) = socket.getaddrinfo(host, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0]\n        if (family == socket.AF_INET):\n            (ip, port) = sockaddr\n        elif (family == socket.AF_INET6):\n            (ip, port, flow_info, scope_id) = sockaddr\n    except Exception:\n        ip = None\n    return ip\n", "label": 0}
{"function": "\n\ndef get(self, key):\n    key = key.lower()\n    if self.has_key(key):\n        return self._config.get(key.lower())\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef add_all_wordstarts_matching(self, lower_hits, query, max_hits_hint):\n    lower_query = query.lower()\n    if (lower_query in self.basenames_by_wordstarts):\n        for basename in self.basenames_by_wordstarts[lower_query]:\n            lower_hits.add(basename)\n            if (len(lower_hits) >= max_hits_hint):\n                return\n", "label": 0}
{"function": "\n\ndef _setup_nodes(self):\n    self.add_node(LocalNode())\n    nodes = self.app.config.get('PSDASH_NODES', [])\n    logger.info('Registering %d nodes', len(nodes))\n    for n in nodes:\n        self.register_node(n['name'], n['host'], int(n['port']))\n", "label": 0}
{"function": "\n\ndef __init__(self, parent_model, admin_site):\n    self.admin_site = admin_site\n    self.parent_model = parent_model\n    self.opts = self.model._meta\n    self.has_registered_model = admin_site.is_registered(self.model)\n    super(InlineModelAdmin, self).__init__()\n    if (self.verbose_name is None):\n        self.verbose_name = self.model._meta.verbose_name\n    if (self.verbose_name_plural is None):\n        self.verbose_name_plural = self.model._meta.verbose_name_plural\n", "label": 0}
{"function": "\n\ndef __init__(self, name, table=None, foreign_key=None, other_key=None, relation=None):\n    if isinstance(foreign_key, (types.FunctionType, types.MethodType)):\n        raise RuntimeError('morphed_by_many relation requires a name')\n    self._name = name\n    self._table = table\n    self._foreign_key = foreign_key\n    self._other_key = other_key\n    super(morphed_by_many, self).__init__(relation=relation)\n", "label": 0}
{"function": "\n\ndef get_object(self, bits):\n    if (len(bits) != 0):\n        raise models.Topic.DoesNotExist\n    return 'LatestFeed'\n", "label": 0}
{"function": "\n\n@ComputedGraph.Function\ndef extractVectorDataAsPythonArray(self):\n    if (self.computedValueVector.vectorImplVal is None):\n        return None\n    if ((len(self.vectorDataIds) > 0) and (not self.isLoaded)):\n        return None\n    result = ComputedValueGateway.getGateway().extractVectorDataAsPythonArray(self.computedValueVector, self.lowIndex, self.highIndex)\n    if ((result is None) and (not self.vdmThinksIsLoaded())):\n        logging.info('CumulusClient: %s was marked loaded but returned None. reloading', self)\n        self.isLoaded = False\n        ComputedValueGateway.getGateway().reloadVector(self)\n    return result\n", "label": 1}
{"function": "\n\ndef parse_policy(policy):\n    ret = {\n        \n    }\n    ret['name'] = policy['name']\n    ret['type'] = policy['type']\n    attrs = policy['Attributes']\n    if (policy['type'] != 'SSLNegotiationPolicyType'):\n        return ret\n    ret['sslv2'] = bool(attrs.get('Protocol-SSLv2'))\n    ret['sslv3'] = bool(attrs.get('Protocol-SSLv3'))\n    ret['tlsv1'] = bool(attrs.get('Protocol-TLSv1'))\n    ret['tlsv1_1'] = bool(attrs.get('Protocol-TLSv1.1'))\n    ret['tlsv1_2'] = bool(attrs.get('Protocol-TLSv1.2'))\n    ret['server_defined_cipher_order'] = bool(attrs.get('Server-Defined-Cipher-Order'))\n    ret['reference_security_policy'] = attrs.get('Reference-Security-Policy', None)\n    non_ciphers = ['Server-Defined-Cipher-Order', 'Protocol-SSLv2', 'Protocol-SSLv3', 'Protocol-TLSv1', 'Protocol-TLSv1.1', 'Protocol-TLSv1.2', 'Reference-Security-Policy']\n    ciphers = []\n    for cipher in attrs:\n        if (attrs[cipher] and (cipher not in non_ciphers)):\n            ciphers.append(cipher)\n    ciphers.sort()\n    ret['supported_ciphers'] = ciphers\n    return ret\n", "label": 0}
{"function": "\n\ndef _update(self, context):\n    'Update partial stats locally and populate them to Scheduler.'\n    if (not self._resource_change()):\n        return\n    self.scheduler_client.update_resource_stats(self.compute_node)\n    if self.pci_tracker:\n        self.pci_tracker.save(context)\n", "label": 0}
{"function": "\n\ndef test_gzip():\n    res = app.get('/', extra_environ=dict(HTTP_ACCEPT_ENCODING='gzip'))\n    assert (int(res.header('content-length')) == len(res.body))\n    assert (res.body != b'this is a test')\n    actual = gzip.GzipFile(fileobj=six.BytesIO(res.body)).read()\n    assert (actual == b'this is a test')\n", "label": 0}
{"function": "\n\ndef get_command_aliases(self):\n    if (not self.config.has_option('commands', 'aliases')):\n        return []\n    value = self.config.get('commands', 'aliases')\n    return list(map((lambda x: x.strip()), value.split(',')))\n", "label": 0}
{"function": "\n\ndef make_url(base, filename, rev):\n    'Helper to construct the URL to fetch.\\n\\n  Args:\\n    base: The base property of the Issue to which the Patch belongs.\\n    filename: The filename property of the Patch instance.\\n    rev: Revision number, or None for head revision.\\n\\n  Returns:\\n    A URL referring to the given revision of the file.\\n  '\n    (scheme, netloc, path, _, _, _) = urlparse.urlparse(base)\n    if netloc.endswith('.googlecode.com'):\n        if (rev is None):\n            raise FetchError(\"Can't access googlecode.com without a revision\")\n        if (not path.startswith('/svn/')):\n            raise FetchError(('Malformed googlecode.com URL (%s)' % base))\n        path = path[5:]\n        url = ('%s://%s/svn-history/r%d/%s/%s' % (scheme, netloc, rev, path, filename))\n        return url\n    elif (netloc.endswith('sourceforge.net') and (rev is not None)):\n        if path.strip().endswith('/'):\n            path = path.strip()[:(- 1)]\n        else:\n            path = path.strip()\n        splitted_path = path.split('/')\n        url = ('%s://%s/%s/!svn/bc/%d/%s/%s' % (scheme, netloc, '/'.join(splitted_path[1:3]), rev, '/'.join(splitted_path[3:]), filename))\n        return url\n    url = base\n    if (not url.endswith('/')):\n        url += '/'\n    url += filename\n    if (rev is not None):\n        url += ('?rev=%s' % rev)\n    return url\n", "label": 1}
{"function": "\n\n@mock.patch('sys.platform', 'linux2')\n@mock.patch('bento.commands.configure.virtualenv_prefix', (lambda : None))\n@mock.patch('bento.core.platforms.sysconfig.bento.utils.path.find_root', (lambda ignored: '/'))\n@mock.patch('distutils.command.install.INSTALL_SCHEMES', {\n    'unix_local': MOCK_DEBIAN_SCHEME,\n}, create=True)\ndef test_scheme_debian(self):\n    bento_info = 'Name: foo\\n'\n    scheme = self._compute_scheme(bento_info, self.options)\n    prefix = scheme.pop('prefix')\n    eprefix = scheme.pop('eprefix')\n    sitedir = scheme.pop('sitedir')\n    includedir = scheme.pop('includedir')\n    self.assertEqual(prefix, '/usr/local')\n    self.assertEqual(eprefix, '/usr/local')\n    self.assertEqual(sitedir, ('/usr/local/lib/python%s/dist-packages' % PY_VERSION_SHORT))\n    self.assertEqual(includedir, ('/usr/local/include/python%s/foo' % PY_VERSION_SHORT))\n    scheme.pop('py_version_short')\n    scheme.pop('pkgname')\n    for (k, v) in scheme.items():\n        self.assertEqual(UNIX_REFERENCE[k], v)\n", "label": 0}
{"function": "\n\ndef visit_binop(self, obj):\n    lhs = obj.lhs.accept(self)\n    op = obj.op\n    rhs = obj.rhs.accept(self)\n    if (op == '+'):\n        return (lhs + rhs)\n    elif (op == '-'):\n        return (lhs - rhs)\n    elif (op == '*'):\n        return (lhs * rhs)\n    elif (op == '/'):\n        return (lhs / rhs)\n    else:\n        raise ValueError('invalid op', op)\n", "label": 0}
{"function": "\n\n@object_base.remotable\ndef update_test(self, context=None):\n    if (context and (context.tenant == 'alternate')):\n        self.bar = 'alternate-context'\n    else:\n        self.bar = 'updated'\n", "label": 0}
{"function": "\n\ndef StartTransform(self):\n    'Starts CSV transformation on Hadoop cluster.'\n    self._LoadMapper()\n    gcs_dir = self.config['hadoopTmpDir']\n    hadoop_input_filename = ('%s/inputs/input.csv' % gcs_dir)\n    logging.info('Starting Hadoop transform from %s to %s', self.config['sources'][0], self.config['sinks'][0])\n    logging.debug('Hadoop input file: %s', hadoop_input_filename)\n    output_file = self.cloud_storage_client.OpenObject(self.config['sinks'][0], mode='w')\n    input_file = self.cloud_storage_client.OpenObject(self.config['sources'][0])\n    hadoop_input = self.cloud_storage_client.OpenObject(hadoop_input_filename, mode='w')\n    line_count = 0\n    for line in input_file:\n        if (line_count < self.config['skipLeadingRows']):\n            output_file.write(line)\n        else:\n            hadoop_input.write(line)\n        line_count += 1\n    hadoop_input.close()\n    input_file.close()\n    mapreduce_id = self._StartHadoopMapReduce(gcs_dir)\n    self._WaitForMapReduce(mapreduce_id)\n    (bucket, hadoop_dir) = gcs.Gcs.UrlToBucketAndName(gcs_dir)\n    tab_strip_pattern = re.compile('\\t\\r?\\n')\n    for hadoop_result in self.cloud_storage_client.ListBucket(('/%s' % bucket), prefix=('%s/outputs/part-' % hadoop_dir)):\n        logging.debug('Hadoop result file: %s', hadoop_result)\n        hadoop_output = self.cloud_storage_client.OpenObject(hadoop_result)\n        for line in hadoop_output:\n            output_file.write(tab_strip_pattern.sub('\\n', line))\n    output_file.close()\n", "label": 0}
{"function": "\n\ndef file_upload_view_verify(request):\n    '\\n    Use the sha digest hash to verify the uploaded contents.\\n    '\n    form_data = request.POST.copy()\n    form_data.update(request.FILES)\n    for (key, value) in form_data.items():\n        if key.endswith('_hash'):\n            continue\n        if ((key + '_hash') not in form_data):\n            continue\n        submitted_hash = form_data[(key + '_hash')]\n        if isinstance(value, UploadedFile):\n            new_hash = hashlib.sha1(value.read()).hexdigest()\n        else:\n            new_hash = hashlib.sha1(force_bytes(value)).hexdigest()\n        if (new_hash != submitted_hash):\n            return HttpResponseServerError()\n    largefile = request.FILES['file_field2']\n    obj = FileModel()\n    obj.testfile.save(largefile.name, largefile)\n    return HttpResponse('')\n", "label": 1}
{"function": "\n\ndef __init__(self, name, loop_chain, tile_size):\n    if self._initialized:\n        return\n    if (not hasattr(self, '_inspected')):\n        self._inspected = 0\n    self._name = name\n    self._tile_size = tile_size\n    self._loop_chain = loop_chain\n", "label": 0}
{"function": "\n\ndef _limit(self, uri, comment):\n    rv = self.db.execute(['SELECT id FROM comments WHERE remote_addr = ? AND ? - created < 60;'], (comment['remote_addr'], time.time())).fetchall()\n    if (len(rv) >= self.conf.getint('ratelimit')):\n        return (False, '{0}: ratelimit exceeded ({1})'.format(comment['remote_addr'], ', '.join(Guard.ids(rv))))\n    if (comment['parent'] is None):\n        rv = self.db.execute(['SELECT id FROM comments WHERE', '    tid = (SELECT id FROM threads WHERE uri = ?)', 'AND remote_addr = ?', 'AND parent IS NULL;'], (uri, comment['remote_addr'])).fetchall()\n        if (len(rv) >= self.conf.getint('direct-reply')):\n            return (False, ('%i direct responses to %s' % (len(rv), uri)))\n    elif (self.conf.getboolean('reply-to-self') == False):\n        rv = self.db.execute(['SELECT id FROM comments WHERE    remote_addr = ?', 'AND id = ?', 'AND ? - created < ?'], (comment['remote_addr'], comment['parent'], time.time(), self.max_age)).fetchall()\n        if (len(rv) > 0):\n            return (False, 'edit time frame is still open')\n    if (self.conf.getboolean('require-email') and (not comment.get('email'))):\n        return (False, 'email address required but not provided')\n    return (True, '')\n", "label": 1}
{"function": "\n\ndef search(self, query, **kwargs):\n    qstring = query['query']['query_string']['query']\n    if (qstring in self._queries):\n        return load_by_bug(self._queries[qstring])\n    return load_empty()\n", "label": 0}
{"function": "\n\ndef execute(self, content):\n    command = self.get_link_command()\n    if (not command):\n        sublime.error_message('Could not get link opener command.\\nPlatform not yet supported.')\n        return None\n    if (sys.version_info[0] < 3):\n        content = content.encode(sys.getfilesystemencoding())\n    cmd = (command + [content])\n    arg_list_wrapper = self.settings.get('orgmode.open_link.resolver.abstract.arg_list_wrapper', [])\n    if arg_list_wrapper:\n        cmd = (arg_list_wrapper + [' '.join(cmd)])\n        source_filename = (('\"' + self.view.file_name()) + '\"')\n        cmd += [source_filename]\n        if (sys.platform != 'win32'):\n            cmd += ['--origin', source_filename, '--quiet']\n    print('*****')\n    print(repr(content), content)\n    print(cmd)\n    sublime.status_message(('Executing: %s' % cmd))\n    if (sys.platform != 'win32'):\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    else:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    (stdout, stderr) = process.communicate()\n    if stdout:\n        stdout = str(stdout, sys.getfilesystemencoding())\n        sublime.status_message(stdout)\n    if stderr:\n        stderr = str(stderr, sys.getfilesystemencoding())\n        sublime.error_message(stderr)\n", "label": 1}
{"function": "\n\ndef test_handle_router_snat_rules_add_back_jump(self):\n    ri = l3router.RouterInfo(_uuid(), {\n        \n    }, **self.ri_kwargs)\n    ri.iptables_manager = mock.MagicMock()\n    port = {\n        'fixed_ips': [{\n            'ip_address': '192.168.1.4',\n        }],\n    }\n    ri._handle_router_snat_rules(port, 'iface')\n    nat = ri.iptables_manager.ipv4['nat']\n    nat.empty_chain.assert_any_call('snat')\n    nat.add_rule.assert_any_call('snat', '-j $float-snat')\n    for call in nat.mock_calls:\n        (name, args, kwargs) = call\n        if (name == 'add_rule'):\n            self.assertEqual(('snat', '-j $float-snat'), args)\n            self.assertEqual({\n                \n            }, kwargs)\n            break\n", "label": 0}
{"function": "\n\ndef test_should_exclude_with__returns_false_with_disabled_tag_and_more(self):\n    traits = self.traits\n    test_patterns = [([traits.category1_enabled_tag, traits.category1_disabled_tag], 'case: first'), ([traits.category1_disabled_tag, traits.category1_enabled_tag], 'case: last'), (['foo', traits.category1_enabled_tag, traits.category1_disabled_tag, 'bar'], 'case: middle')]\n    enabled = True\n    for (tags, case) in test_patterns:\n        self.assertEqual((not enabled), self.tag_matcher.should_exclude_with(tags), ('%s: tags=%s' % (case, tags)))\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TSentryPrivilegeMap')\n    if (self.privilegeMap is not None):\n        oprot.writeFieldBegin('privilegeMap', TType.MAP, 1)\n        oprot.writeMapBegin(TType.STRING, TType.SET, len(self.privilegeMap))\n        for (kiter104, viter105) in self.privilegeMap.items():\n            oprot.writeString(kiter104)\n            oprot.writeSetBegin(TType.STRUCT, len(viter105))\n            for iter106 in viter105:\n                iter106.write(oprot)\n            oprot.writeSetEnd()\n        oprot.writeMapEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\n@classmethod\ndef handle_token(cls, parser, token):\n    bits = token.split_contents()\n    if ((len(bits) == 3) and (bits[1] == 'as')):\n        return cls(bits[2])\n    elif ((len(bits) == 4) and (bits[2] == 'as')):\n        return cls(bits[3], bits[1])\n    else:\n        raise template.TemplateSyntaxError((\"%r takes 'as var' or 'level as var'\" % bits[0]))\n", "label": 0}
{"function": "\n\ndef occurrence_view(request, event_pk, pk, template='swingtime/occurrence_detail.html', form_class=forms.SingleOccurrenceForm):\n    '\\n    View a specific occurrence and optionally handle any updates.\\n    \\n    Context parameters:\\n    \\n    ``occurrence``\\n        the occurrence object keyed by ``pk``\\n\\n    ``form``\\n        a form object for updating the occurrence\\n    '\n    occurrence = get_object_or_404(Occurrence, pk=pk, event__pk=event_pk)\n    if (request.method == 'POST'):\n        form = form_class(request.POST, instance=occurrence)\n        if form.is_valid():\n            form.save()\n            return http.HttpResponseRedirect(request.path)\n    else:\n        form = form_class(instance=occurrence)\n    return render(request, template, {\n        'occurrence': occurrence,\n        'form': form,\n    })\n", "label": 0}
{"function": "\n\ndef testWhitelisted(self):\n    mvp = vcluster.MakeVirtualPath\n    for path in vcluster._VPATH_WHITELIST:\n        self.assertEqual(mvp(path), path)\n        self.assertEqual(mvp(path, _noderoot=None), path)\n        self.assertEqual(mvp(path, _noderoot='/tmp'), path)\n", "label": 0}
{"function": "\n\ndef depack(self, args):\n    self.is_touch = True\n    self.sx = args['x']\n    self.sy = args['y']\n    self.profile = ['pos']\n    if (('size_w' in args) and ('size_h' in args)):\n        self.shape = ShapeRect()\n        self.shape.width = args['size_w']\n        self.shape.height = args['size_h']\n        self.profile.append('shape')\n    if ('pressure' in args):\n        self.pressure = args['pressure']\n        self.profile.append('pressure')\n    super(MTDMotionEvent, self).depack(args)\n", "label": 0}
{"function": "\n\ndef send(self, message, flags=0, copy=False, track=False):\n    zmq_msg = ffi.new('zmq_msg_t*')\n    c_message = ffi.new('char[]', message)\n    C.zmq_msg_init_size(zmq_msg, len(message))\n    C.memcpy(C.zmq_msg_data(zmq_msg), c_message, len(message))\n    if (zmq_version == 2):\n        ret = C.zmq_send(self.zmq_socket, zmq_msg, flags)\n    else:\n        ret = C.zmq_sendmsg(self.zmq_socket, zmq_msg, flags)\n    C.zmq_msg_close(zmq_msg)\n    if (ret < 0):\n        self.last_errno = C.zmq_errno()\n    return ret\n", "label": 0}
{"function": "\n\ndef GetBlendMethod(self):\n    'Get the blend method'\n    currentMethod = self.component.PropertyList.Find('SourceBlendMode').Data\n    for (method, idx) in self.kBlendMethods.iteritems():\n        if (currentMethod == idx):\n            return method\n", "label": 0}
{"function": "\n\ndef anno(self, node):\n    if ((node.type is None) and (not getattr(node, 'escapes', False))):\n        return\n    if isinstance(node.type, (types.function, types.Type)):\n        return\n    self.write(' [')\n    if (node.type is not None):\n        self.visit(node.type)\n    if ((node.type is not None) and getattr(node, 'escapes', False)):\n        self.write(':')\n    if getattr(node, 'escapes', False):\n        self.write('E')\n    self.write(']')\n", "label": 1}
{"function": "\n\ndef _read_track_origin(self, group):\n    self.track_origin = group.attrs['track_origin'].decode('ascii')\n    if ('track_n_scat' in group.attrs):\n        self.track_n_scat = group.attrs['track_n_scat']\n    else:\n        self.track_n_scat = 0\n", "label": 0}
{"function": "\n\ndef visit_NVARCHAR(self, type_, **kw):\n    if type_.length:\n        return self._extend_string(type_, {\n            'national': True,\n        }, ('VARCHAR(%(length)s)' % {\n            'length': type_.length,\n        }))\n    else:\n        raise exc.CompileError(('NVARCHAR requires a length on dialect %s' % self.dialect.name))\n", "label": 0}
{"function": "\n\ndef to_ctype(self, parakeet_type):\n    if isinstance(parakeet_type, (NoneT, ScalarT)):\n        return type_mappings.to_ctype(parakeet_type)\n    elif isinstance(parakeet_type, TupleT):\n        return self.struct_type_from_fields(parakeet_type.elt_types)\n    elif isinstance(parakeet_type, PtrT):\n        return self.ptr_struct_type(parakeet_type.elt_type)\n    elif isinstance(parakeet_type, ArrayT):\n        elt_t = parakeet_type.elt_type\n        rank = parakeet_type.rank\n        return self.array_struct_type(elt_t, rank)\n    elif isinstance(parakeet_type, SliceT):\n        return self.slice_struct_type()\n    elif isinstance(parakeet_type, ClosureT):\n        return self.struct_type_from_fields(parakeet_type.arg_types)\n    elif isinstance(parakeet_type, TypeValueT):\n        return 'int'\n    else:\n        assert False, (\"Don't know how to make C type for %s\" % parakeet_type)\n", "label": 1}
{"function": "\n\n@webob.dec.wsgify\ndef process_request(self, req):\n    if (req.path != self._path):\n        return None\n    results = [ext.obj.healthcheck(req.server_port) for ext in self._backends]\n    healthy = self._are_results_healthy(results)\n    if (req.method == 'HEAD'):\n        functor = self._make_head_response\n        status = self.HEAD_HEALTHY_TO_STATUS_CODES[healthy]\n    else:\n        status = self.HEALTHY_TO_STATUS_CODES[healthy]\n        accept_type = req.accept.best_match(self._accept_order)\n        if (not accept_type):\n            accept_type = self._default_accept\n        functor = self._accept_to_functor[accept_type]\n    (body, content_type) = functor(results, healthy)\n    return webob.response.Response(status=status, body=body, content_type=content_type)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    TestCase.__init__(self, *args, **kwargs)\n    for attr in [x for x in dir(self) if x.startswith('test')]:\n        meth = getattr(self, attr)\n\n        def test_(self):\n            try:\n                meth()\n            except psutil.AccessDenied:\n                pass\n        setattr(self, attr, types.MethodType(test_, self))\n", "label": 0}
{"function": "\n\ndef strip_units(self):\n    '\\n        Strips units from an xypoint structure.\\n\\n        Returns:\\n          A copy of the xypoint with no units\\n          The x-units\\n          the y-units\\n        '\n    xunits = (self.x.unit if isinstance(self.x, u.Quantity) else 1.0)\n    yunits = (self.y.unit if isinstance(self.y, u.Quantity) else 1.0)\n    x = (self.x.value if isinstance(self.x, u.Quantity) else self.x)\n    y = (self.y.value if isinstance(self.y, u.Quantity) else self.y)\n    err = (self.err.value if isinstance(self.err, u.Quantity) else self.err)\n    cont = (self.cont.value if isinstance(self.cont, u.Quantity) else self.cont)\n    return (xypoint(x=x, y=y, cont=cont, err=err), xunits, yunits)\n", "label": 1}
{"function": "\n\ndef get_shipping_method(self, basket, shipping_address=None, **kwargs):\n    '\\n        Return the selected shipping method instance from this checkout session\\n\\n        The shipping address is passed as we need to check that the method\\n        stored in the session is still valid for the shipping address.\\n        '\n    code = self.checkout_session.shipping_method_code(basket)\n    methods = Repository().get_shipping_methods(basket=basket, user=self.request.user, shipping_addr=shipping_address, request=self.request)\n    for method in methods:\n        if (method.code == code):\n            return method\n", "label": 0}
{"function": "\n\n@classmethod\ndef get_template(cls, message, messenger):\n    'Get a template path to compile a message.\\n\\n        1. `tpl` field of message context;\\n        2. `template` field of message class;\\n        3. deduced from message, messenger data and `template_ext` message type field\\n           (e.g. `sitemessage/messages/plain__smtp.txt` for `plain` message type).\\n\\n        :param Message message: Message model\\n        :param MessengerBase messenger: a MessengerBase heir\\n        :return: str\\n        :rtype: str\\n        '\n    template = message.context.get('tpl', None)\n    if template:\n        return template\n    if (cls.template is None):\n        cls.template = ('sitemessage/messages/%s__%s.%s' % (cls.get_alias(), messenger.get_alias(), cls.template_ext))\n    return cls.template\n", "label": 0}
{"function": "\n\ndef teardown(self):\n    for key in self._event_fns:\n        event.remove(*key)\n    super_ = super(RemovesEvents, self)\n    if hasattr(super_, 'teardown'):\n        super_.teardown()\n", "label": 0}
{"function": "\n\ndef write_file(self, filename):\n    '\\n        Pass schema object to template engine to be rendered for use.\\n\\n        :param filename: output filename\\n        :return:\\n        '\n    template = self.template_env.get_template('settings.py.j2')\n    settings = template.render(endpoints=OrderedDict([(endpoint, self.format_endpoint(schema)) for (endpoint, schema) in self.endpoints.iteritems()]))\n    with open(filename, 'w') as ofile:\n        ofile.write((settings + '\\n'))\n", "label": 0}
{"function": "\n\ndef process_data(self):\n    polar_data = build_wedge_source(self._data.df, cat_cols=self.attributes['label'].columns, agg_col=self.values.selection, agg=self.agg, level_width=self.level_width, level_spacing=self.level_spacing)\n    polar_data['color'] = ''\n    for group in self._data.groupby(**self.attributes):\n        polar_data.loc[(group['stack'], 'color')] = group['color']\n    self.chart_data = ColumnDataSource(polar_data)\n    self.text_data = build_wedge_text_source(polar_data)\n", "label": 0}
{"function": "\n\n@db_access\ndef _getContainerField(container, field, default):\n    ' Returns the metadata field for the given container or the default value. '\n    container_id = _getContainerId(container)\n    found = _getContainerFieldRecord(container_id, field)\n    return (found.value if found else default)\n", "label": 0}
{"function": "\n\ndef map(self, path):\n    'Map `path` through the aliases.\\n\\n        `path` is checked against all of the patterns.  The first pattern to\\n        match is used to replace the root of the path with the result root.\\n        Only one pattern is ever used.  If no patterns match, `path` is\\n        returned unchanged.\\n\\n        The separator style in the result is made to match that of the result\\n        in the alias.\\n\\n        '\n    for (regex, result, pattern_sep, result_sep) in self.aliases:\n        m = regex.match(path)\n        if m:\n            new = path.replace(m.group(0), result)\n            if (pattern_sep != result_sep):\n                new = new.replace(pattern_sep, result_sep)\n            if self.locator:\n                new = self.locator.canonical_filename(new)\n            return new\n    return path\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _filter_pools_for_numa_cells(pools, numa_cells):\n    numa_cells = ([None] + [cell.id for cell in numa_cells])\n    return [pool for pool in pools if any((utils.pci_device_prop_match(pool, [{\n        'numa_node': cell,\n    }]) for cell in numa_cells))]\n", "label": 0}
{"function": "\n\ndef _setup(self):\n    'Initiate lists for objects contained within this object'\n    for field in self._contains:\n        setattr(self, field, [])\n", "label": 0}
{"function": "\n\ndef test_install_one_host(self):\n    args = self.parser.parse_args('install host1'.split())\n    assert (args.host == ['host1'])\n", "label": 0}
{"function": "\n\ndef SaveTemporaryFile(fp):\n    'Store incoming database file in a temporary directory.'\n    loc = data_store.DB.Location()\n    if (not os.path.exists(loc)):\n        return False\n    if (not os.path.isdir(loc)):\n        return False\n    filecopy_len_str = fp.read(sutils.SIZE_PACKER.size)\n    filecopy_len = sutils.SIZE_PACKER.unpack(filecopy_len_str)[0]\n    filecopy = rdf_data_server.DataServerFileCopy(fp.read(filecopy_len))\n    rebdir = _CreateDirectory(loc, filecopy.rebalance_id)\n    filedir = utils.JoinPath(rebdir, filecopy.directory)\n    try:\n        os.makedirs(filedir)\n    except OSError:\n        pass\n    filepath = utils.JoinPath(filedir, filecopy.filename)\n    logging.info('Writing to file %s', filepath)\n    with open(filepath, 'wb') as wp:\n        decompressor = zlib.decompressobj()\n        while True:\n            block_len_str = fp.read(sutils.SIZE_PACKER.size)\n            block_len = sutils.SIZE_PACKER.unpack(block_len_str)[0]\n            if (not block_len):\n                break\n            block = fp.read(block_len)\n            to_decompress = (decompressor.unconsumed_tail + block)\n            while to_decompress:\n                decompressed = decompressor.decompress(to_decompress)\n                if decompressed:\n                    wp.write(decompressed)\n                    to_decompress = decompressor.unconsumed_tail\n                else:\n                    to_decompress = ''\n        remaining = decompressor.flush()\n        if remaining:\n            wp.write(remaining)\n    if (os.path.getsize(filepath) != filecopy.size):\n        logging.error('Size of file %s is not %d', filepath, filecopy.size)\n        return False\n    return True\n", "label": 1}
{"function": "\n\n@classmethod\ndef validate(cls, value):\n    if (not value):\n        return value\n    value = os.path.abspath(value)\n    dirname = os.path.dirname(value)\n    if os.path.isfile(value):\n        if (not os.access(value, os.W_OK)):\n            raise config_option.BadValue('You do not have write permissions')\n        if (not os.access(dirname, os.W_OK)):\n            raise config_option.BadValue(('You do not have write permissions for \"%s\"' % dirname))\n        return value\n    elif os.path.isdir(value):\n        raise config_option.BadValue(('\"%s\" is a directory' % value))\n    else:\n        if os.path.isdir(dirname):\n            if (not os.access(dirname, os.W_OK)):\n                raise config_option.BadValue(('You do not have write permissions for \"%s\"' % dirname))\n            return value\n        previous_dir = os.path.dirname(dirname)\n        if (not os.path.isdir(previous_dir)):\n            raise config_option.BadValue(('\"%s\" not found' % value))\n        if (not os.access(previous_dir, os.W_OK)):\n            raise config_option.BadValue(('You do not have write permissions for \"%s\"' % previous_dir))\n        return value\n", "label": 1}
{"function": "\n\ndef update(self, x1, x2, y):\n    self.phase = 'train'\n    for layer in self.layers:\n        x1 = layer.fprop(x1)\n    for layer in self.layers2:\n        x2 = layer.fprop(x2)\n    (grad1, grad2) = self.loss.grad(y, x1, x2)\n    layers = self.layers[self.bprop_until:]\n    for layer in reversed(layers[1:]):\n        grad1 = layer.bprop(grad1)\n    layers[0].bprop(grad1)\n    layers2 = self.layers2[self.bprop_until:]\n    for layer in reversed(layers2[1:]):\n        grad2 = layer.bprop(grad2)\n    layers2[0].bprop(grad2)\n    return self.loss.loss(y, x1, x2)\n", "label": 0}
{"function": "\n\ndef type_continue(self, node):\n    return self.__addSemicolon(('continue' if (not hasattr(node, 'label')) else ('continue %s' % node.label)))\n", "label": 0}
{"function": "\n\ndef __int(value):\n    'validate an integer'\n    (valid, _value) = (False, value)\n    try:\n        _value = int(value)\n        valid = True\n    except ValueError:\n        pass\n    return (valid, _value, 'integer')\n", "label": 0}
{"function": "\n\ndef test_can_update_status_via_trigger_on_participant_balance(self):\n    self.db.run(\"UPDATE participants SET balance=10, status_of_1_0_payout='pending-application' WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 10)\n    assert (alice.status_of_1_0_payout == 'pending-application')\n    self.db.run(\"UPDATE participants SET balance=0 WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 0)\n    assert (alice.status_of_1_0_payout == 'completed')\n", "label": 0}
{"function": "\n\n@app.route('/v1/repositories/<path:repository>/tags', methods=['GET'])\n@toolkit.parse_repository_name\n@toolkit.requires_auth\n@mirroring.source_lookup_tag\ndef _get_tags(namespace, repository):\n    logger.debug('[get_tags] namespace={0}; repository={1}'.format(namespace, repository))\n    try:\n        data = get_tags(namespace=namespace, repository=repository)\n    except exceptions.FileNotFoundError:\n        return toolkit.api_error('Repository not found', 404)\n    return toolkit.response(data)\n", "label": 0}
{"function": "\n\ndef __onNewValues(self, dateTime, value):\n    if (self.__range is None):\n        self.__range = resamplebase.build_range(dateTime, self.getFrequency())\n        self.__grouper = BarsGrouper(self.__range.getBeginning(), value, self.getFrequency())\n    elif self.__range.belongs(dateTime):\n        self.__grouper.addValue(value)\n    else:\n        self.__values.append(self.__grouper.getGrouped())\n        self.__range = resamplebase.build_range(dateTime, self.getFrequency())\n        self.__grouper = BarsGrouper(self.__range.getBeginning(), value, self.getFrequency())\n", "label": 0}
{"function": "\n\ndef __init__(self, client, data=None):\n    super(Zone, self).__init__()\n    self.client = client\n    self.zone_type = 'temperatureZone'\n    if (data is not None):\n        self.__dict__.update(data)\n", "label": 0}
{"function": "\n\ndef register(self, field_type, impl=None):\n    '\\n        Register form field data function.\\n        \\n        Could be used as decorator\\n        '\n\n    def _wrapper(func):\n        self.registry[field_type] = func\n        return func\n    if impl:\n        return _wrapper(impl)\n    return _wrapper\n", "label": 0}
{"function": "\n\ndef test_unknown_apps_are_ignored(self):\n    'Unknown engines get ignored.'\n    self.create_appversion('firefox', '33.0a1')\n    self.create_appversion('thunderbird', '33.0a1')\n    data = {\n        'engines': {\n            'firefox': '>=33.0a1',\n            'thunderbird': '>=33.0a1',\n            'node': '>=0.10',\n        },\n    }\n    apps = self.parse(data)['apps']\n    engines = [app.appdata.short for app in apps]\n    assert (sorted(engines) == ['firefox', 'thunderbird'])\n", "label": 0}
{"function": "\n\ndef scaffold_auto_joins(self):\n    '\\n            Return a list of joined tables by going through the\\n            displayed columns.\\n        '\n    if (not self.column_auto_select_related):\n        return []\n    relations = set()\n    for p in self._get_model_iterator():\n        if hasattr(p, 'direction'):\n            if (p.mapper.class_ == self.model):\n                continue\n            if (p.direction.name in ['MANYTOONE', 'MANYTOMANY']):\n                relations.add(p.key)\n    joined = []\n    for (prop, name) in self._list_columns:\n        if (prop in relations):\n            joined.append(getattr(self.model, prop))\n    return joined\n", "label": 1}
{"function": "\n\ndef _trending_for_month(metric=None):\n    this_month_date = month_for_date(datetime.date.today())\n    previous_month_date = get_previous_month(this_month_date)\n    previous_month_year_date = get_previous_year(this_month_date)\n    data = {\n        'month': 0,\n        'previous_month': 0,\n        'previous_month_year': 0,\n    }\n    try:\n        month = MetricMonth.objects.get(metric=metric, created=this_month_date)\n        data['month'] = month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)\n        data['previous_month'] = previous_month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)\n        data['previous_month_year'] = previous_month_year.num\n    except ObjectDoesNotExist:\n        pass\n    return data\n", "label": 0}
{"function": "\n\n@log_debug\ndef generate_room(self, section):\n    '\\n        Generate room\\n\\n        :param section: section for generator to draw to\\n        :type section: Section\\n        '\n    self.square_generator.generate_room(section)\n    offset = [(1, 1), ((- 1), 1), ((- 1), (- 1)), (1, (- 1))]\n    for (index, corner) in enumerate(self.square_generator.room_corners):\n        self.add_pillar(section, corner, offset[index])\n", "label": 0}
{"function": "\n\ndef report_to_ci_server(self, project):\n    for report in self.reports:\n        test_name = report['test']\n        test_failed = (report['success'] is not True)\n        with test_proxy_for(project).and_test_name(('Integrationtest.%s' % test_name)) as test:\n            if test_failed:\n                test.fails(report['exception'])\n", "label": 0}
{"function": "\n\n@patch('paasta_tools.cli.cmds.check.read_service_configuration')\n@patch('paasta_tools.cli.cmds.check.is_file_in_dir')\n@patch('sys.stdout', new_callable=StringIO)\ndef test_check_smartstack_check_missing_instance(mock_stdout, mock_is_file_in_dir, mock_read_service_info):\n    mock_is_file_in_dir.return_value = True\n    smartstack_dict = {\n        \n    }\n    mock_read_service_info.return_value = smartstack_dict\n    expected_output = ('%s\\n%s\\n' % (PaastaCheckMessages.SMARTSTACK_YAML_FOUND, PaastaCheckMessages.SMARTSTACK_PORT_MISSING))\n    smartstack_check(service='fake_service', service_path='path', soa_dir='path')\n    output = mock_stdout.getvalue()\n    assert (output == expected_output)\n", "label": 0}
{"function": "\n\ndef do_register_opts(opts, group=None, ignore_errors=False):\n    try:\n        cfg.CONF.register_opts(opts, group=group)\n    except:\n        if (not ignore_errors):\n            raise\n", "label": 0}
{"function": "\n\ndef get_select_precolumns(self, select):\n    'Called when building a ``SELECT`` statement, position is just\\n        before column list.\\n\\n        '\n    return ((select._distinct and 'DISTINCT ') or '')\n", "label": 0}
{"function": "\n\ndef visit_WaitStatement(self, node):\n    filename = getfilename(node)\n    template = self.get_template(filename)\n    template_dict = {\n        'cond': del_paren(self.visit(node.cond)),\n        'statement': (self.visit(node.statement) if node.statement else ''),\n    }\n    rslt = template.render(template_dict)\n    return rslt\n", "label": 0}
{"function": "\n\ndef test_None_on_sys_path(self):\n    new_path = sys.path[:]\n    new_path.insert(0, None)\n    new_path_importer_cache = sys.path_importer_cache.copy()\n    new_path_importer_cache.pop(None, None)\n    new_path_hooks = [zipimport.zipimporter, _bootstrap.FileFinder.path_hook(*_bootstrap._get_supported_file_loaders())]\n    missing = object()\n    email = sys.modules.pop('email', missing)\n    try:\n        with util.import_state(meta_path=sys.meta_path[:], path=new_path, path_importer_cache=new_path_importer_cache, path_hooks=new_path_hooks):\n            module = import_module('email')\n            self.assertIsInstance(module, ModuleType)\n    finally:\n        if (email is not missing):\n            sys.modules['email'] = email\n", "label": 0}
{"function": "\n\ndef on_text_changed(self):\n    \" Handle the 'textChanged' signal on the widget.\\n\\n        \"\n    d = self.declaration\n    if (d is not None):\n        d.text_changed()\n", "label": 0}
{"function": "\n\ndef onBrowserEvent(self, event):\n    etype = DOM.eventGetType(event)\n    if (etype == 'mousewheel'):\n        if self._mouseWheelPreventDefault:\n            DOM.eventPreventDefault(event)\n        velocity = DOM.eventGetMouseWheelVelocityY(event)\n        for listener in self._mouseWheelListeners:\n            listener.onMouseWheel(self, velocity)\n        return True\n", "label": 0}
{"function": "\n\ndef raw_field_definition_proxy_post_save(sender, instance, raw, **kwargs):\n    \"\\n    When proxy field definitions are loaded from a fixture they're not\\n    passing through the `field_definition_post_save` signal. Make sure they\\n    are.\\n    \"\n    if raw:\n        model_class = instance.content_type.model_class()\n        opts = model_class._meta\n        if (opts.proxy and (opts.concrete_model is sender)):\n            field_definition_post_save(sender=model_class, instance=instance.type_cast(), raw=raw, **kwargs)\n", "label": 0}
{"function": "\n\ndef test_moves_a_block_up_within_a_container(self):\n    for (idx, pos) in [(0, 0), (1, 1), (2, 2)]:\n        block = TextBlock.objects.get(id=self.left_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n    self.app.put(reverse('fp-api:block-move', kwargs={\n        'uuid': self.main_blocks[1].uuid,\n    }), params={\n        'container': self.left_container.uuid,\n        'index': 1,\n    }, user=self.user)\n    moved_block = TextBlock.objects.get(id=self.main_blocks[1].id)\n    self.assertEquals(moved_block.container, self.page.get_container_from_name('left-container'))\n    self.assertEquals(moved_block.display_order, 1)\n    for (idx, pos) in [(0, 0), (1, 2), (2, 3)]:\n        block = TextBlock.objects.get(id=self.left_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n    for (idx, pos) in [(0, 0), (2, 1)]:\n        block = TextBlock.objects.get(id=self.main_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n", "label": 0}
{"function": "\n\ndef run_cmd(self, util, toggle_active_mark_mode=False):\n    if (util.state.argument_supplied or toggle_active_mark_mode):\n        util.toggle_active_mark_mode()\n    else:\n        util.swap_point_and_mark()\n", "label": 0}
{"function": "\n\ndef test_asizer_limit(self):\n    'Test limit setting for Asizer.\\n        '\n    objs = [Foo(42), ThinFoo('spam'), OldFoo(67)]\n    sizer = [asizeof.Asizer() for _ in range(4)]\n    for (limit, asizer) in enumerate(sizer):\n        asizer.asizeof(objs, limit=limit)\n    limit_sizes = [asizer.total for asizer in sizer]\n    self.assertTrue((limit_sizes[0] < limit_sizes[1]), limit_sizes)\n    self.assertTrue((limit_sizes[1] < limit_sizes[2]), limit_sizes)\n    self.assertTrue((limit_sizes[2] < limit_sizes[3]), limit_sizes)\n", "label": 0}
{"function": "\n\ndef decode(self, file):\n    fStart = file.tell()\n    identifier = None\n    try:\n        identifier = self.iEIEncoder.decode(file)\n    except UDHInformationElementIdentifierUnknownError:\n        pass\n    length = self.int8Encoder.decode(file)\n    data = None\n    if (identifier in self.dataEncoders):\n        data = self.dataEncoders[identifier].decode(file)\n    elif (length > 0):\n        data = self.read(file, length)\n    parsed = (file.tell() - fStart)\n    if (parsed != (length + 2)):\n        raise UDHParseError(('Invalid length: expected %d, parsed %d' % ((length + 2), parsed)))\n    if (identifier is None):\n        return None\n    return gsm_types.InformationElement(identifier, data)\n", "label": 1}
{"function": "\n\ndef _op(self, method, other):\n    if isinstance(other, Counter):\n        other = other.value()\n    if (not isinstance(other, int)):\n        raise TypeError(('Cannot add %s, not an integer.' % other))\n    method(other)\n    return self\n", "label": 0}
{"function": "\n\ndef test_neighbors(self):\n    graph = nx.complete_graph(100)\n    pop = random.sample(list(graph), 1)\n    nbors = list(nx.neighbors(graph, pop[0]))\n    assert_equal(len(nbors), (len(graph) - 1))\n    graph = nx.path_graph(100)\n    node = random.sample(list(graph), 1)[0]\n    nbors = list(nx.neighbors(graph, node))\n    if ((node != 0) and (node != 99)):\n        assert_equal(len(nbors), 2)\n    else:\n        assert_equal(len(nbors), 1)\n    graph = nx.star_graph(99)\n    nbors = list(nx.neighbors(graph, 0))\n    assert_equal(len(nbors), 99)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super().clean()\n    actions = Action.objects.in_bulk(cleaned_data['actions'])\n    attachment_counter = 0\n    one_action = False\n    one_action_name = ''\n    actions_items = actions.items()\n    for (k, v) in actions_items:\n        action = getattr(self.model, v.name)\n        if getattr(action, 'return_attachment', False):\n            attachment_counter += 1\n        if getattr(action, 'only_one_action', False):\n            one_action = True\n            one_action_name = getattr(action, 'verbose_name', '')\n    if (attachment_counter > 1):\n        msg = _('Please select at most one action which return attachment.')\n        self.add_error('actions', msg)\n    if (one_action and (len(actions_items) > 1)):\n        msg = (_('You have chosen action: %(name)s can only be selected for transition') % {\n            'name': one_action_name,\n        })\n        self.add_error('actions', msg)\n    return cleaned_data\n", "label": 1}
{"function": "\n\ndef list_hosts(self, filters):\n    host_data_list = (self._hosts_collection().find(filters) or [])\n    return [host.Host.from_dict(h_data, conf=self.config) for h_data in host_data_list]\n", "label": 0}
{"function": "\n\ndef _collapse(intervals):\n    '\\n    Collapse an iterable of intervals sorted by start coord.\\n    \\n    '\n    span = None\n    for (start, stop) in intervals:\n        if (span is None):\n            span = _Interval(start, stop)\n        elif (start <= span.stop < stop):\n            span = _Interval(span.start, stop)\n        elif (start > span.stop):\n            (yield span)\n            span = _Interval(start, stop)\n    if (span is not None):\n        (yield span)\n", "label": 1}
{"function": "\n\ndef claim_invitations(user):\n    \"Claims any pending invitations for the given user's email address.\"\n    invitation_user_id = ('%s:%s' % (models.User.EMAIL_INVITATION, user.email_address))\n    invitation_user = models.User.query.get(invitation_user_id)\n    if invitation_user:\n        invited_build_list = list(invitation_user.builds)\n        if (not invited_build_list):\n            return\n        db.session.add(user)\n        logging.debug('Found %d build admin invitations for id=%r, user=%r', len(invited_build_list), invitation_user_id, user)\n        for build in invited_build_list:\n            build.owners.remove(invitation_user)\n            if (not build.is_owned_by(user.id)):\n                build.owners.append(user)\n                logging.debug('Claiming invitation for build_id=%r', build.id)\n                save_admin_log(build, invite_accepted=True)\n            else:\n                logging.debug('User already owner of build. id=%r, build_id=%r', user.id, build.id)\n            db.session.add(build)\n        db.session.delete(invitation_user)\n        db.session.commit()\n        db.session.add(current_user)\n", "label": 0}
{"function": "\n\ndef process_exception(self, request, exception):\n    if (settings.DEBUG or isinstance(exception, Http404)):\n        return None\n    if isinstance(exception, apiproxy_errors.CapabilityDisabledError):\n        msg = 'Rietveld: App Engine is undergoing maintenance. Please try again in a while.'\n        status = 503\n    elif isinstance(exception, (DeadlineExceededError, MemoryError)):\n        msg = 'Rietveld is too hungry at the moment.Please try again in a while.'\n        status = 503\n    else:\n        msg = 'Unhandled exception.'\n        status = 500\n    logging.exception(('%s: ' % exception.__class__.__name__))\n    technical = ('%s [%s]' % (exception, exception.__class__.__name__))\n    if self._text_requested(request):\n        content = ('%s\\n\\n%s\\n' % (msg, technical))\n        content_type = 'text/plain'\n    else:\n        tpl = loader.get_template('exception.html')\n        ctx = Context({\n            'msg': msg,\n            'technical': technical,\n        })\n        content = tpl.render(ctx)\n        content_type = 'text/html'\n    return HttpResponse(content, status=status, content_type=content_type)\n", "label": 1}
{"function": "\n\n@property\ndef autofit(self):\n    \"\\n        Return |False| if there is a ``<w:tblLayout>`` child with ``w:type``\\n        attribute set to ``'fixed'``. Otherwise return |True|.\\n        \"\n    tblLayout = self.tblLayout\n    if (tblLayout is None):\n        return True\n    return (False if (tblLayout.type == 'fixed') else True)\n", "label": 0}
{"function": "\n\ndef get_episode(self, url, imdb, tvdb, title, date, season, episode):\n    try:\n        if (url == None):\n            return\n        url = urlparse.urljoin(self.base_link, url)\n        (season, episode) = (('%01d' % int(season)), ('%01d' % int(episode)))\n        result = client.source(url)\n        if (not (season == '1')):\n            url = client.parseDOM(result, 'a', ret='href', attrs={\n                'class': 'season-.+?',\n            })\n            url = [i for i in url if (('/%s-sezon-' % season) in i)][0]\n            result = client.source(url)\n        result = client.parseDOM(result, 'a', ret='href')\n        result = [i for i in result if (('%s-sezon-%s-bolum-' % (season, episode)) in i)][0]\n        try:\n            url = re.compile('//.+?(/.+)').findall(result)[0]\n        except:\n            url = result\n        url = client.replaceHTMLCodes(url)\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef update_dimension_fields(self, instance, force=False, *args, **kwargs):\n    if ((getattr(instance, 'mimetype', None) is not None) and ('image' in instance.mimetype)):\n        super(FileField, self).update_dimension_fields(instance, force, *args, **kwargs)\n    else:\n        pass\n", "label": 0}
{"function": "\n\ndef HKDF_extract(salt, IKM, hashmod=hashlib.sha256):\n    'HKDF-Extract; see RFC-5869 for the details.'\n    if (salt is None):\n        salt = (b'\\x00' * hashmod().digest_size)\n    if isinstance(salt, text_type):\n        salt = salt.encode('utf-8')\n    return python_hmac.new(salt, IKM, hashmod).digest()\n", "label": 0}
{"function": "\n\ndef _handle_object_info_reply(self, rep):\n    ' Handle replies for call tips.\\n        '\n    cursor = self._get_cursor()\n    info = self._request_info.get('call_tip')\n    if (info and (info.id == rep['parent_header']['msg_id']) and (info.pos == cursor.position())):\n        content = rep['content']\n        if content.get('ismagic', False):\n            (call_info, doc) = (None, None)\n        else:\n            (call_info, doc) = call_tip(content, format_call=True)\n        if (call_info or doc):\n            self._call_tip_widget.show_call_info(call_info, doc)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _options(fieldname):\n    \"\\n            Lookup the full set of options for a Filter Widget\\n            - for Subscriptions we don't want to see just the options available in current data\\n        \"\n    db = current.db\n    if (fieldname == 'location_id'):\n        table = current.s3db.gis_location\n        query = ((table.deleted == False) & (table.level == 'L1'))\n        rows = db(query).select(table.id)\n        options = [row.id for row in rows]\n    return options\n", "label": 0}
{"function": "\n\ndef test_radius_neighbors_classifier_when_no_neighbors():\n    X = np.array([[1.0, 1.0], [2.0, 2.0]])\n    y = np.array([1, 2])\n    radius = 0.1\n    z1 = np.array([[1.01, 1.01], [2.01, 2.01]])\n    z2 = np.array([[1.01, 1.01], [1.4, 1.4]])\n    weight_func = _weight_func\n    for outlier_label in [0, (- 1), None]:\n        for algorithm in ALGORITHMS:\n            for weights in ['uniform', 'distance', weight_func]:\n                rnc = neighbors.RadiusNeighborsClassifier\n                clf = rnc(radius=radius, weights=weights, algorithm=algorithm, outlier_label=outlier_label)\n                clf.fit(X, y)\n                assert_array_equal(np.array([1, 2]), clf.predict(z1))\n                if (outlier_label is None):\n                    assert_raises(ValueError, clf.predict, z2)\n                elif False:\n                    assert_array_equal(np.array([1, outlier_label]), clf.predict(z2))\n", "label": 1}
{"function": "\n\ndef OutputPartial(self, out):\n    if self.has_package_:\n        out.putVarInt32(10)\n        out.putPrefixedString(self.package_)\n    for i in xrange(len(self.capability_)):\n        out.putVarInt32(18)\n        out.putPrefixedString(self.capability_[i])\n    for i in xrange(len(self.call_)):\n        out.putVarInt32(26)\n        out.putPrefixedString(self.call_[i])\n", "label": 0}
{"function": "\n\ndef test_func_adds_roots(self):\n\n    def add_roots(doc):\n        doc.add_root(AnotherModelInTestFunction())\n        doc.add_root(SomeModelInTestFunction())\n    handler = FunctionHandler(add_roots)\n    doc = Document()\n    handler.modify_document(doc)\n    if handler.failed:\n        raise RuntimeError(handler.error)\n    assert (len(doc.roots) == 2)\n", "label": 0}
{"function": "\n\ndef has_valid_checksum(self, number):\n    (given_number, given_checksum) = (number[:(- 1)], number[(- 1)])\n    calculated_checksum = 0\n    fragment = ''\n    parameter = 7\n    for i in range(len(given_number)):\n        fragment = str((int(given_number[i]) * parameter))\n        if fragment.isalnum():\n            calculated_checksum += int(fragment[(- 1)])\n        if (parameter == 1):\n            parameter = 7\n        elif (parameter == 3):\n            parameter = 1\n        elif (parameter == 7):\n            parameter = 3\n    return (str(calculated_checksum)[(- 1)] == given_checksum)\n", "label": 1}
{"function": "\n\ndef listen(self):\n    'Listen to incoming clients until\\n        self._running is set to False\\n        '\n    l = self.listener\n    self._running = True\n    try:\n        while self._running:\n            log.debug('Accept connection')\n            c = l.accept()\n            try:\n                action = c.recv()\n            except EOFError:\n                c.close()\n                continue\n            if isinstance(action, basestring):\n                args = ()\n                kwargs = {\n                    \n                }\n            else:\n                args = action.get('args', ())\n                kwargs = (action.get('kwargs') or {\n                    \n                })\n                action = action.get('action')\n            log.info(('Dispatch action \"%s\"' % action))\n            method = getattr(self, ('dispatch_%s' % action), None)\n            if method:\n                try:\n                    result = method(*args, **kwargs)\n                except Exception as err:\n                    log.exception(err)\n                    c.send({\n                        'error': True,\n                        'message': ('Exception in action %s - %s' % (action, err)),\n                    })\n                else:\n                    c.send({\n                        'error': False,\n                        'message': 'ok',\n                        'result': result,\n                    })\n            else:\n                log.warn(('No action %s' % action))\n                c.send({\n                    'error': True,\n                    'message': ('No action %s' % action),\n                })\n            c.close()\n    finally:\n        self._listener = None\n        l.close()\n    log.info('Exiting event loop')\n", "label": 1}
{"function": "\n\ndef convert_fragment(self, fragment, fd):\n    mdat = None\n    try:\n        f4v = F4V(fd, raw_payload=True)\n        for box in f4v:\n            if (box.type == 'mdat'):\n                mdat = box.payload.data\n                break\n    except F4VError as err:\n        self.logger.error('Failed to parse fragment {0}-{1}: {2}', fragment.segment, fragment.fragment, err)\n        return\n    if (not mdat):\n        self.logger.error('No MDAT box found in fragment {0}-{1}', fragment.segment, fragment.fragment)\n        return\n    try:\n        for chunk in self.concater.iter_chunks(buf=mdat, skip_header=True):\n            self.reader.buffer.write(chunk)\n            if self.closed:\n                break\n        else:\n            self.logger.debug('Download of fragment {0}-{1} complete', fragment.segment, fragment.fragment)\n    except IOError as err:\n        if ('Unknown tag type' in str(err)):\n            self.logger.error('Unknown tag type found, this stream is probably encrypted')\n            self.close()\n            return\n        self.logger.error('Error reading fragment {0}-{1}: {2}', fragment.segment, fragment.fragment, err)\n", "label": 1}
{"function": "\n\ndef _wait_async_done(self, reservation_id, reqids):\n    \"\\n        _wait_async_done(session_id, reqids)\\n        Helper methods that waits for the specified asynchronous requests to be finished,\\n        and which asserts that they were successful. Note that it doesn't actually return\\n        their responses.\\n        @param reqids Tuple containing the request ids for the commands to check.\\n        @return Nothing\\n        \"\n    reqsl = list(reqids)\n    max_count = 15\n    while (len(reqsl) > 0):\n        time.sleep(0.1)\n        max_count -= 1\n        if (max_count == 0):\n            raise Exception('Maximum time spent waiting async done')\n        requests = self.client.check_async_command_status(reservation_id, tuple(reqsl))\n        self.assertEquals(len(reqsl), len(requests))\n        for (rid, req) in six.iteritems(requests):\n            status = req[0]\n            self.assertTrue((status in ('running', 'ok', 'error')))\n            if (status != 'running'):\n                self.assertEquals('ok', status, ('Contents: ' + req[1]))\n                reqsl.remove(rid)\n", "label": 0}
{"function": "\n\ndef test_os_stat(self):\n    'Test sizing os.stat and os.statvfs objects.\\n        '\n    try:\n        stat = os.stat(__file__)\n    except Exception:\n        pass\n    else:\n        stat_size = asizeof.asizeof(stat)\n        self.assertTrue((stat_size > 0), stat_size)\n        refs = asizeof.named_refs(stat)\n        ref_names = set([name for (name, _) in refs])\n        self.assertTrue((set(['st_mode', 'st_size', 'st_mtime']) <= ref_names), ref_names)\n    try:\n        stat = os.statvfs(__file__)\n    except Exception:\n        pass\n    else:\n        stat_size = asizeof.asizeof(stat)\n        self.assertTrue((stat_size > 0), stat_size)\n        refs = asizeof.named_refs(stat)\n        ref_names = set([name for (name, _) in refs])\n        self.assertTrue((set(['f_bsize', 'f_blocks']) <= ref_names), ref_names)\n", "label": 1}
{"function": "\n\ndef _unshorten_lnxlu(self, uri):\n    try:\n        r = requests.get(uri, headers=self._headers, timeout=self._timeout)\n        html = r.text\n        code = re.findall('/\\\\?click\\\\=(.*)\\\\.\"', html)\n        if (len(code) > 0):\n            payload = {\n                'click': code[0],\n            }\n            r = requests.get('http://lnx.lu/', params=payload, headers=self._headers, timeout=self._timeout)\n            return (r.url, r.status_code)\n        else:\n            return (uri, 'No click variable found')\n    except Exception as e:\n        return (uri, str(e))\n", "label": 0}
{"function": "\n\ndef list_names(self, **kwargs):\n    'Get a list of metric names.'\n    url_str = (self.base_url + '/names')\n    newheaders = self.get_headers()\n    if ('dimensions' in kwargs):\n        dimstr = self.get_dimensions_url_string(kwargs['dimensions'])\n        kwargs['dimensions'] = dimstr\n    if kwargs:\n        url_str = (url_str + ('?%s' % urlutils.urlencode(kwargs, True)))\n    (resp, body) = self.client.json_request('GET', url_str, headers=newheaders)\n    return (body['elements'] if (type(body) is dict) else body)\n", "label": 0}
{"function": "\n\ndef mayRaiseException(self, exception_type):\n    if self.tolerant:\n        return False\n    else:\n        if (self.variable_trace is not None):\n            variable = self.getTargetVariableRef().getVariable()\n            if variable.isTempVariable():\n                return False\n            if ((self.previous_trace is not None) and self.previous_trace.mustHaveValue()):\n                return False\n        return True\n", "label": 1}
{"function": "\n\ndef __exit__(self, exc_type, exc_value, traceback):\n    ':func:`burpui.misc.auth.ldap.LdapLoader.__exit__` closes the\\n        connection to the LDAP server.\\n        '\n    if (self.ldap and self.ldap.bound):\n        self.ldap.unbind()\n", "label": 0}
{"function": "\n\n@ConnectorExist(cid_key='smppc')\ndef smppc(self, arg, opts):\n    sc = SMPPClientStatsCollector()\n    headers = ['#Item', 'Value']\n    table = []\n    for (k, v) in sc.get(opts.smppc).getStats().iteritems():\n        if isinstance(v, dict):\n            v = json.dumps(v)\n        row = []\n        row.append(('#%s' % k))\n        if (k[(- 3):] == '_at'):\n            row.append(formatDateTime(v))\n        else:\n            row.append(v)\n        table.append(row)\n    self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))\n", "label": 0}
{"function": "\n\n@property\ndef event_description(self):\n    'complete description of this event in text form\\n\\n        :rtype: str\\n        :returns: event description\\n        '\n    location = (('\\nLocation: ' + self.location) if (self.location != '') else '')\n    description = (('\\nDescription: ' + self.description) if (self.description != '') else '')\n    repitition = (('\\nRepeat: ' + self.recurpattern) if (self.recurpattern != '') else '')\n    return '{}: {}{}{}{}'.format(self._rangestr, self.summary, location, repitition, description)\n", "label": 0}
{"function": "\n\ndef create(self):\n    ' Insert Action '\n    token = request.POST.pop('__token', '')\n    if (not self._is_token_match(token)):\n        success = False\n        error_message = 'Invalid Token'\n        data = None\n    else:\n        data = self.__model__()\n        data.set_state_insert()\n        data.assign_from_dict(request.POST)\n        data.save()\n        success = data.success\n        error_message = data.error_message\n    self._setup_view_parameter()\n    self._set_view_parameter(self.__model_name__, data)\n    self._set_view_parameter('success', success)\n    self._set_view_parameter('error_message', error_message)\n    if (request.is_xhr or request.POST.pop('__as_json', False)):\n        if success:\n            token = self._set_token()\n        self._set_view_parameter('__token', token)\n        return self._get_view_parameter_as_json()\n    return self._load_view('create')\n", "label": 0}
{"function": "\n\ndef is_editable(proposal, user):\n    return ((not proposal.scheduled) and (((proposal.proposer == user) and (proposal.status != 'A')) or topiclead(user, proposal.topic)))\n", "label": 0}
{"function": "\n\ndef visit_BIT(self, type_):\n    if type_.varying:\n        compiled = 'BIT VARYING'\n        if (type_.length is not None):\n            compiled += ('(%d)' % type_.length)\n    else:\n        compiled = ('BIT(%d)' % type_.length)\n    return compiled\n", "label": 0}
{"function": "\n\ndef process_get_results_metadata(self, seqid, iprot, oprot):\n    args = get_results_metadata_args()\n    args.read(iprot)\n    iprot.readMessageEnd()\n    result = get_results_metadata_result()\n    try:\n        result.success = self._handler.get_results_metadata(args.handle)\n        msg_type = TMessageType.REPLY\n    except (TTransport.TTransportException, KeyboardInterrupt, SystemExit):\n        raise\n    except QueryNotFoundException as error:\n        msg_type = TMessageType.REPLY\n        result.error = error\n    except Exception as ex:\n        msg_type = TMessageType.EXCEPTION\n        logging.exception(ex)\n        result = TApplicationException(TApplicationException.INTERNAL_ERROR, 'Internal error')\n    oprot.writeMessageBegin('get_results_metadata', msg_type, seqid)\n    result.write(oprot)\n    oprot.writeMessageEnd()\n    oprot.trans.flush()\n", "label": 0}
{"function": "\n\ndef text(self, value=no_default):\n    \"Get or set the text representation of sub nodes.\\n\\n        Get the text value::\\n\\n            >>> doc = PyQuery('<div><span>toto</span><span>tata</span></div>')\\n            >>> print(doc.text())\\n            toto tata\\n\\n        Set the text value::\\n\\n            >>> doc.text('Youhou !')\\n            [<div>]\\n            >>> print(doc)\\n            <div>Youhou !</div>\\n\\n        \"\n    if (value is no_default):\n        if (not self):\n            return None\n        text = []\n\n        def add_text(tag, no_tail=False):\n            if tag.text:\n                text.append(tag.text)\n            for child in tag.getchildren():\n                add_text(child)\n            if ((not no_tail) and tag.tail):\n                text.append(tag.tail)\n        for tag in self:\n            add_text(tag, no_tail=True)\n        return ' '.join([t.strip() for t in text if t.strip()])\n    for tag in self:\n        for child in tag.getchildren():\n            tag.remove(child)\n        tag.text = value\n    return self\n", "label": 1}
{"function": "\n\n@add_method(Return)\ndef handle(self, ch):\n    msg = ch.message\n    msg.rx_channel = ch\n    if ch.on_return:\n        try:\n            ch.on_return(msg)\n        except Exception:\n            logger.error('ERROR in on_return() callback', exc_info=True)\n", "label": 0}
{"function": "\n\ndef shutdown(self):\n    if self.stopped:\n        return\n    self.stopped = True\n    try:\n        for handle in self.map_handles.values():\n            if (handle is not None):\n                handle.close()\n        for handle in self.file_handles.values():\n            if (handle is not None):\n                handle.close()\n    finally:\n        with self.lock:\n            if (self.create_lock_file is True):\n                os.remove(self.lock_file)\n    self.inited = False\n", "label": 1}
{"function": "\n\ndef col_references_table(col, table):\n    for fk in col.foreign_keys:\n        if fk.references(table):\n            return True\n    return False\n", "label": 0}
{"function": "\n\n@auth\ndef _GET(self, *param, **params):\n    (host_id, guest_id) = self.chk_guestby1(param)\n    if (guest_id is None):\n        return web.notfound()\n    model = findbyguest1(self.orm, guest_id)\n    kvc = KaresansuiVirtConnection()\n    try:\n        domname = kvc.uuid_to_domname(model.uniq_key)\n        if (not domname):\n            return web.notfound()\n        virt = kvc.search_kvg_guests(domname)[0]\n        vcpus_info = virt.get_vcpus_info()\n        self.view.max_vcpus_limit = kvc.get_max_vcpus()\n        self.view.max_vcpus = vcpus_info['bootup_vcpus']\n        self.view.vcpus_limit = vcpus_info['max_vcpus']\n        self.view.vcpus = vcpus_info['vcpus']\n        self.view.cpuTime = virt.get_info()['cpuTime']\n        self.view.hypervisor = virt.get_info()['hypervisor']\n        self.view.guest = model\n    finally:\n        kvc.close()\n    return True\n", "label": 0}
{"function": "\n\ndef Execute(self, action, *args, **kw):\n    'Directly execute an action through an Environment\\n        '\n    action = self.Action(action, *args, **kw)\n    result = action([], [], self)\n    if isinstance(result, SCons.Errors.BuildError):\n        errstr = result.errstr\n        if result.filename:\n            errstr = ((result.filename + ': ') + errstr)\n        sys.stderr.write(('scons: *** %s\\n' % errstr))\n        return result.status\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef modify_cluster(self, **cluster_kwargs):\n    cluster_identifier = cluster_kwargs.pop('cluster_identifier')\n    new_cluster_identifier = cluster_kwargs.pop('new_cluster_identifier', None)\n    cluster = self.describe_clusters(cluster_identifier)[0]\n    for (key, value) in cluster_kwargs.items():\n        setattr(cluster, key, value)\n    if new_cluster_identifier:\n        self.delete_cluster(cluster_identifier)\n        cluster.cluster_identifier = new_cluster_identifier\n        self.clusters[new_cluster_identifier] = cluster\n    return cluster\n", "label": 0}
{"function": "\n\ndef categories(self, fileids=None, patterns=None):\n    meta = self._get_meta()\n    fileids = make_iterable(fileids, meta.keys())\n    result = sorted(set((cat for cat in itertools.chain(*(meta[str(doc_id)].categories for doc_id in fileids)))))\n    if patterns:\n        patterns = make_iterable(patterns)\n        result = [cat for cat in result if some_items_match([cat], patterns)]\n    return result\n", "label": 1}
{"function": "\n\ndef __dump_xml(self, filename):\n    self.log.info('Dumping final status as XML: %s', filename)\n    root = etree.Element('FinalStatus')\n    if self.last_sec:\n        for (label, kpiset) in iteritems(self.last_sec[DataPoint.CUMULATIVE]):\n            root.append(self.__get_xml_summary(label, kpiset))\n    with open(get_full_path(filename), 'wb') as fhd:\n        tree = etree.ElementTree(root)\n        tree.write(fhd, pretty_print=True, encoding='UTF-8', xml_declaration=True)\n", "label": 0}
{"function": "\n\ndef update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES):\n    'Update a wrapper function to look like the wrapped function\\n\\n       wrapper is the function to be updated\\n       wrapped is the original function\\n       assigned is a tuple naming the attributes assigned directly\\n       from the wrapped function to the wrapper function (defaults to\\n       functools.WRAPPER_ASSIGNMENTS)\\n       updated is a tuple naming the attributes of the wrapper that\\n       are updated with the corresponding attribute from the wrapped\\n       function (defaults to functools.WRAPPER_UPDATES)\\n    '\n    for attr in assigned:\n        setattr(wrapper, attr, getattr(wrapped, attr))\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {\n            \n        }))\n    return wrapper\n", "label": 0}
{"function": "\n\ndef backward_cpu(self, xs, gys):\n    assert (len(xs) == self.n_in)\n    assert (len(gys) == self.n_out)\n    return tuple((np.zeros_like(xs).astype(np.float32) for _ in six.moves.range(self.n_in)))\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    from ..classtypes.base import FieldsClassType\n    from ..classtypes.inputobjecttype import InputObjectType\n    if issubclass(cls, InputObjectType):\n        inputfield = self.as_inputfield()\n        return inputfield.contribute_to_class(cls, name)\n    elif issubclass(cls, FieldsClassType):\n        field = self.as_field()\n        return field.contribute_to_class(cls, name)\n", "label": 0}
{"function": "\n\ndef go_back(self):\n    isdir = is_dir(self.input)\n    input_stripped = self.input.rstrip(os.sep)\n    if (not input_stripped):\n        return\n    input_splitted = input_stripped.split(os.sep)\n    entry_name = input_splitted[(- 1)]\n    if isdir:\n        entry_name += os.sep\n    new_input = os.sep.join(input_splitted[0:(- 1)])\n    if new_input:\n        new_input += os.sep\n    self.set_input(new_input)\n    self.set_selected_entry(entry_name)\n", "label": 0}
{"function": "\n\ndef _iter_cursor_results(self):\n    col_names = [c[0] for c in self._cursor.description]\n    while 1:\n        row = self._cursor.fetchone()\n        if (row is None):\n            break\n        (yield self._make_row(row, col_names))\n", "label": 0}
{"function": "\n\ndef input_field(self, name, value, sample_values, back_uri):\n    string_value = (self.format(value) if value else '')\n    html = ('<input class=\"%s\" name=\"%s\" type=\"text\" size=\"%d\" value=\"%s\"/>' % (cgi.escape(self.name()), cgi.escape(name), self.input_field_size(), cgi.escape(string_value, True)))\n    if value:\n        html += ('<br><a href=\"/datastore/edit/%s?next=%s\">%s</a>' % (cgi.escape(string_value, True), urllib.quote_plus(back_uri), cgi.escape(_format_datastore_key(value), True)))\n    return html\n", "label": 0}
{"function": "\n\ndef ssq_error(correct, estimate, mask):\n    'Compute the sum-squared-error for an image, where the estimate is\\n    multiplied by a scalar which minimizes the error. Sums over all pixels\\n    where mask is True. If the inputs are color, each color channel can be\\n    rescaled independently.'\n    assert (correct.ndim == 2)\n    if (np.sum(((estimate ** 2) * mask)) > 1e-05):\n        alpha = (np.sum(((correct * estimate) * mask)) / np.sum(((estimate ** 2) * mask)))\n    else:\n        alpha = 0.0\n    return np.sum((mask * ((correct - (alpha * estimate)) ** 2)))\n", "label": 0}
{"function": "\n\ndef assertRedirects(self, response, expected_url, **kwargs):\n    '\\n        Wrapper for assertRedirects to handle Django pre-1.9.\\n        '\n    if ((VERSION >= (1, 9)) and expected_url.startswith('http://testserver')):\n        expected_url = expected_url[len('http://testserver'):]\n    return super(CommentTestCase, self).assertRedirects(response, expected_url, **kwargs)\n", "label": 0}
{"function": "\n\ndef html_tag(self, template, logical_path, debug=False):\n    environment = self.get_environment(current_app)\n    if (debug or self.debug(current_app)):\n        asset = build_asset(environment, logical_path)\n        urls = []\n        for requirement in asset.requirements:\n            logical_path = requirement.attributes.logical_path\n            url = url_for('static', filename=logical_path, body=1)\n            urls.append(url)\n    else:\n        if (logical_path in environment.manifest.files):\n            logical_path = environment.manifest.files[logical_path]\n        urls = (url_for('static', filename=logical_path),)\n    return Markup('\\n'.join((template.format(url=url) for url in urls)))\n", "label": 1}
{"function": "\n\ndef _send_mail(self, handler, trap, is_duplicate):\n    if (is_duplicate and (not handler['mail_on_duplicate'])):\n        return\n    mail = handler['mail']\n    if (not mail):\n        return\n    recipients = handler['mail'].get('recipients')\n    if (not recipients):\n        return\n    subject = (handler['mail']['subject'] % {\n        'trap_oid': trap.oid,\n        'trap_name': ObjectId(trap.oid).name,\n        'ipaddress': trap.host,\n        'hostname': self.resolver.hostname_or_ip(trap.host),\n    })\n    ctxt = dict(trap=trap, dest_host=self.hostname)\n    try:\n        stats.incr('mail_sent_attempted', 1)\n        send_trap_email(recipients, 'trapperkeeper', subject, self.template_env, ctxt)\n        stats.incr('mail_sent_successful', 1)\n    except socket.error as err:\n        stats.incr('mail_sent_failed', 1)\n        logging.warning('Failed to send e-mail for trap: %s', err)\n", "label": 1}
{"function": "\n\ndef testConversion(self):\n    for (expected_value, binary) in self.tests:\n        binary_sid = ''.join([chr(x) for x in binary])\n        if (expected_value is None):\n            self.assertRaises(ValueError, wmi_parser.BinarySIDtoStringSID, binary_sid)\n        else:\n            self.assertEqual(wmi_parser.BinarySIDtoStringSID(binary_sid), expected_value)\n", "label": 0}
{"function": "\n\ndef astar_path_length(G, source, target, heuristic=None, weight='weight'):\n    'Return the length of the shortest path between source and target using\\n    the A* (\"A-star\") algorithm.\\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n\\n    source : node\\n       Starting node for path\\n\\n    target : node\\n       Ending node for path\\n\\n    heuristic : function\\n       A function to evaluate the estimate of the distance\\n       from the a node to the target.  The function takes\\n       two nodes arguments and must return a number.\\n\\n    Raises\\n    ------\\n    NetworkXNoPath\\n        If no path exists between source and target.\\n\\n    See Also\\n    --------\\n    astar_path\\n\\n    '\n    if ((source not in G) or (target not in G)):\n        msg = 'Either source {} or target {} is not in G'\n        raise nx.NodeNotFound(msg.format(source, target))\n    path = astar_path(G, source, target, heuristic, weight)\n    return sum((G[u][v].get(weight, 1) for (u, v) in zip(path[:(- 1)], path[1:])))\n", "label": 0}
{"function": "\n\ndef test_build_graph(self, huang_darwiche_nodes):\n    bbn = build_bbn(huang_darwiche_nodes)\n    nodes = dict([(node.name, node) for node in bbn.nodes])\n    assert (nodes['f_a'].parents == [])\n    assert (nodes['f_b'].parents == [nodes['f_a']])\n    assert (nodes['f_c'].parents == [nodes['f_a']])\n    assert (nodes['f_d'].parents == [nodes['f_b']])\n    assert (nodes['f_e'].parents == [nodes['f_c']])\n    assert (nodes['f_f'].parents == [nodes['f_d'], nodes['f_e']])\n    assert (nodes['f_g'].parents == [nodes['f_c']])\n    assert (nodes['f_h'].parents == [nodes['f_e'], nodes['f_g']])\n", "label": 1}
{"function": "\n\ndef serialize(self):\n    buf = bytearray(struct.pack(self._PACK_STR, self.type_, self.aux_len, self.num, addrconv.ipv6.text_to_bin(self.address)))\n    for src in self.srcs:\n        buf.extend(struct.pack('16s', addrconv.ipv6.text_to_bin(src)))\n    if (0 == self.num):\n        self.num = len(self.srcs)\n        struct.pack_into('!H', buf, 2, self.num)\n    if (self.aux is not None):\n        mod = (len(self.aux) % 4)\n        if mod:\n            self.aux += bytearray((4 - mod))\n            self.aux = six.binary_type(self.aux)\n        buf.extend(self.aux)\n        if (0 == self.aux_len):\n            self.aux_len = (len(self.aux) // 4)\n            struct.pack_into('!B', buf, 1, self.aux_len)\n    return six.binary_type(buf)\n", "label": 1}
{"function": "\n\ndef execute(self, cluster, commands):\n    pool = self.get_pool(commands)\n    for (db_num, command_list) in commands.iteritems():\n        for command in command_list:\n            pool.add(command, command.clone().resolve, [cluster[db_num]])\n    return dict(pool.join())\n", "label": 0}
{"function": "\n\ndef backup_reports(items):\n    if (not items):\n        return\n    KEEP_MAX_REPORTS = 100\n    tm = app.get_state_item('telemetry', {\n        \n    })\n    if ('backup' not in tm):\n        tm['backup'] = []\n    for params in items:\n        for key in params.keys():\n            if (key in ('v', 'tid', 'cid', 'cd1', 'cd2', 'sr', 'an')):\n                del params[key]\n        if ('qt' not in params):\n            params['qt'] = time()\n        elif (not isinstance(params['qt'], float)):\n            params['qt'] = (time() - (params['qt'] / 1000))\n        tm['backup'].append(params)\n    tm['backup'] = tm['backup'][(KEEP_MAX_REPORTS * (- 1)):]\n    app.set_state_item('telemetry', tm)\n", "label": 1}
{"function": "\n\ndef test_setslice2(self):\n    pyfunc = list_setslice2\n    cfunc = jit(nopython=True)(pyfunc)\n    sizes = [5, 40]\n    for (n, n_src) in itertools.product(sizes, sizes):\n        indices = [0, 1, (n - 2), (- 1), (- 2), ((- n) + 3), ((- n) - 1), (- n)]\n        for (start, stop) in itertools.product(indices, indices):\n            expected = pyfunc(n, n_src, start, stop)\n            self.assertPreciseEqual(cfunc(n, n_src, start, stop), expected)\n", "label": 0}
{"function": "\n\ndef __enter__(self):\n    if self._entered:\n        raise RuntimeError(('Cannot enter %r twice' % self))\n    self._entered = True\n    self._filters = self._module.filters\n    self._module.filters = self._filters[:]\n    self._showwarning = self._module.showwarning\n    if self._record:\n        log = []\n\n        def showwarning(*args, **kwargs):\n            log.append(WarningMessage(*args, **kwargs))\n        self._module.showwarning = showwarning\n        return log\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef _bytecode_filenames(self, py_filenames):\n    bytecode_files = []\n    for py_file in py_filenames:\n        if (not py_file.endswith('.py')):\n            continue\n        if self.compile:\n            bytecode_files.append((py_file + 'c'))\n        if (self.optimize > 0):\n            bytecode_files.append((py_file + 'o'))\n    return bytecode_files\n", "label": 0}
{"function": "\n\ndef _faster_to_representation(self, instance):\n    'Modified to_representation with optimizations.\\n\\n        1) Returns a plain old dict as opposed to OrderedDict.\\n            (Constructing ordered dict is ~100x slower than `{}`.)\\n        2) Ensure we use a cached list of fields\\n            (this optimization exists in DRF 3.2 but not 3.1)\\n\\n        Arguments:\\n            instance: a model instance or data object\\n        Returns:\\n            Dict of primitive datatypes.\\n        '\n    ret = {\n        \n    }\n    fields = self._readable_fields\n    for field in fields:\n        try:\n            attribute = field.get_attribute(instance)\n        except SkipField:\n            continue\n        if (attribute is None):\n            ret[field.field_name] = None\n        else:\n            ret[field.field_name] = field.to_representation(attribute)\n    return ret\n", "label": 0}
{"function": "\n\ndef store_catch_412_HTTPError(entity):\n    'Returns the stored Entity if the function succeeds or None if the 412 is caught.'\n    try:\n        return syn.store(entity)\n    except SynapseHTTPError as err:\n        if (err.response.status_code == 412):\n            return None\n        raise\n", "label": 0}
{"function": "\n\ndef check_func(conf, func, libs=None):\n    if (libs is None):\n        libs = []\n    code = ('\\nchar %(func)s (void);\\n\\n#ifdef _MSC_VER\\n#pragma function(%(func)s)\\n#endif\\n\\nint main (void)\\n{\\n    return %(func)s();\\n}\\n' % {\n        'func': func,\n    })\n    if libs:\n        msg = ('Checking for function %s in %s' % (func, ' '.join([(conf.env['LIB_FMT'] % lib) for lib in libs])))\n    else:\n        msg = ('Checking for function %s' % func)\n    conf.start_message(msg)\n    old_lib = copy.deepcopy(conf.env['LIBS'])\n    try:\n        for lib in libs[::(- 1)]:\n            conf.env['LIBS'].insert(0, lib)\n        ret = conf.builders['ctasks'].try_program('check_func', code, None)\n        if ret:\n            conf.end_message('yes')\n        else:\n            conf.end_message('no !')\n    finally:\n        conf.env['LIBS'] = old_lib\n    conf.conf_results.append({\n        'type': 'func',\n        'value': func,\n        'result': ret,\n    })\n    return ret\n", "label": 1}
{"function": "\n\ndef unlink(self, name):\n    symlinks = self.read_bootstrap().get('symlinks', {\n        \n    })\n    removed_targets = set()\n    found = False\n    for (source_glob, target) in symlinks.items():\n        if self._islinkkey(source_glob, name):\n            found = True\n            for (source, target) in self.expandtargets(source_glob, target):\n                self._remove_link_target(source, target)\n                removed_targets.add(target)\n                shutil.move(source, target)\n                print(tty.progress('Moved {0} -> {1}'.format(collapseuser(source), collapseuser(target))))\n    if (not found):\n        raise StowError('No symlink found with name: {0}'.format(name))\n    try:\n        os.rmdir(os.path.join(self.symlink_dir, name))\n    except OSError as e:\n        if (e.errno != errno.ENOTEMPTY):\n            raise e\n    self.remove_symlink(name)\n    self._update_target_cache((set(self._cached_targets()) - removed_targets))\n", "label": 1}
{"function": "\n\ndef get_memory_amount(builder, installProfile, is_mandatory):\n    if (('hardwareSettings' in builder) and ('memory' in builder['hardwareSettings'])):\n        installProfile.memorySize = builder['hardwareSettings']['memory']\n        return installProfile\n    elif is_mandatory:\n        printer.out((('Error: no hardwareSettings part for builder [' + builder['type']) + ']'), printer.ERROR)\n        return 2\n    else:\n        return installProfile\n", "label": 0}
{"function": "\n\ndef _make_table(self, table, fields):\n    'Set up the schema of the database. `fields` is a mapping\\n        from field names to `Type`s. Columns are added if necessary.\\n        '\n    with self.transaction() as tx:\n        rows = tx.query(('PRAGMA table_info(%s)' % table))\n    current_fields = set([row[1] for row in rows])\n    field_names = set(fields.keys())\n    if current_fields.issuperset(field_names):\n        return\n    if (not current_fields):\n        columns = []\n        for (name, typ) in fields.items():\n            columns.append('{0} {1}'.format(name, typ.sql))\n        setup_sql = 'CREATE TABLE {0} ({1});\\n'.format(table, ', '.join(columns))\n    else:\n        setup_sql = ''\n        for (name, typ) in fields.items():\n            if (name in current_fields):\n                continue\n            setup_sql += 'ALTER TABLE {0} ADD COLUMN {1} {2};\\n'.format(table, name, typ.sql)\n    with self.transaction() as tx:\n        tx.script(setup_sql)\n", "label": 1}
{"function": "\n\ndef __init__(self, raw, buffer_size=DEFAULT_BUFFER_SIZE, max_buffer_size=None):\n    if (not raw.writable()):\n        raise IOError('\"raw\" argument must be writable.')\n    _BufferedIOMixin.__init__(self, raw)\n    if (buffer_size <= 0):\n        raise ValueError('invalid buffer size')\n    if (max_buffer_size is not None):\n        warnings.warn('max_buffer_size is deprecated', DeprecationWarning, self._warning_stack_offset)\n    self.buffer_size = buffer_size\n    self._write_buf = bytearray()\n    self._write_lock = Lock()\n    self._ok = True\n", "label": 0}
{"function": "\n\ndef AnnotateMethod(self, unused_the_api, method, unused_resource):\n    'Annotate a Method with Java Proto specific elements.\\n\\n    Args:\\n      unused_the_api: (Api) The API tree which owns this method.\\n      method: (Method) The method to annotate.\\n      unused_resource: (Resource) The resource which owns this method.\\n\\n    Raises:\\n      ValueError: if missing externalTypeName\\n    '\n    for attr in ('requestType', 'responseType'):\n        schema = method.get(attr)\n        if (schema and (not isinstance(schema, data_types.Void))):\n            name = schema.get('externalTypeName')\n            if (not name):\n                raise ValueError(('missing externalTypeName for %s (%s of method %s)' % (schema['id'], attr, method['rpcMethod'])))\n            java_name = schema.get('javaTypeName')\n            proto_name = (java_name or ('TO_BE_COMPUTED.' + name[(name.rfind('.') + 1):]))\n            schema.SetTemplateValue('protoFullClassName', proto_name)\n", "label": 1}
{"function": "\n\ndef _notification(self, context, method, routers, operation, shuffle_agents):\n    'Notify all or individual Cisco cfg agents.'\n    if utils.is_extension_supported(self._l3plugin, L3AGENT_SCHED):\n        adm_context = ((context.is_admin and context) or context.elevated())\n        self._l3plugin.schedule_routers(adm_context, routers)\n        self._agent_notification(context, method, routers, operation, shuffle_agents)\n    else:\n        cctxt = self.client.prepare(topics=topics.L3_AGENT, fanout=True)\n        cctxt.cast(context, method, routers=[r['id'] for r in routers])\n", "label": 0}
{"function": "\n\ndef fireDNDEvent(self, name, target, widget):\n    if (name == 'dragstart'):\n        self.dragDataStore.setMode(READ_WRITE)\n    elif (name == 'drop'):\n        self.dragDataStore.setMode(READ_ONLY)\n    event = self.makeDragEvent(self.mouseEvent, name, target)\n    widget.onBrowserEvent(event)\n    self.finalize(event)\n    return event\n", "label": 0}
{"function": "\n\ndef directory_files(fpath):\n    for (dir, _, files) in walk(fpath):\n        for file_name in files:\n            (yield path.join(dir, file_name))\n", "label": 0}
{"function": "\n\ndef _get_longname(self, obj):\n    names = [obj.name]\n    parent = obj.parent\n    while (parent is not None):\n        if (isinstance(parent, TestCaseFile) or isinstance(parent, TestDataDirectory) or isinstance(parent, TestCase) or isinstance(parent, UserKeyword)):\n            names.insert(0, parent.name)\n        parent = parent.parent\n    return '.'.join(names)\n", "label": 1}
{"function": "\n\ndef bootstrap_unicel_gateway(apps):\n    currency = (apps.get_model('accounting.Currency') if apps else Currency).objects.get(code='INR')\n    sms_gateway_fee_class = (apps.get_model('smsbillables.SmsGatewayFee') if apps else SmsGatewayFee)\n    sms_gateway_fee_criteria_class = (apps.get_model('smsbillables.SmsGatewayFeeCriteria') if apps else SmsGatewayFeeCriteria)\n    SmsGatewayFee.create_new(SQLUnicelBackend.get_api_id(), INCOMING, 0.5, currency=currency, fee_class=sms_gateway_fee_class, criteria_class=sms_gateway_fee_criteria_class)\n    SmsGatewayFee.create_new(SQLUnicelBackend.get_api_id(), OUTGOING, 0.5, currency=currency, fee_class=sms_gateway_fee_class, criteria_class=sms_gateway_fee_criteria_class)\n    log_smsbillables_info('Updated Unicel gateway fees.')\n", "label": 0}
{"function": "\n\ndef format_delta(delta):\n    days = (delta / ((60 * 60) * 24))\n    hours = (delta / (60 * 60))\n    mins = (delta / 60)\n    if (days == 1):\n        return ('%d day' % days)\n    elif (days > 1):\n        return ('%d days' % days)\n    elif (hours == 1):\n        return ('%d hour' % hours)\n    elif (hours > 1):\n        return ('%d hours' % hours)\n    elif (mins == 1):\n        return ('%d min' % mins)\n    elif (mins > 1):\n        return ('%d mins' % mins)\n    else:\n        return 'just now'\n", "label": 1}
{"function": "\n\ndef lt(x, y):\n    if ((x is None) and (y is not None)):\n        return True\n    elif ((x is not None) and (y is None)):\n        return False\n    else:\n        return (x < y)\n", "label": 0}
{"function": "\n\ndef print_row(data):\n    'print a single db row in chr and str\\n    '\n    index_line = ''\n    pri_line1 = ''\n    chr_line2 = ''\n    asci = re.compile('[a-zA-Z0-9 ]')\n    for (i, xi) in enumerate(data):\n        if (not (i % 5)):\n            diff = (len(pri_line1) - len(index_line))\n            i = str(i)\n            index_line += (diff * ' ')\n            index_line += i\n        str_v = str(xi)\n        pri_line1 += (str(xi) + ',')\n        c = chr(xi)\n        c = (c if asci.match(c) else ' ')\n        w = len(str_v)\n        c = ((c + ((w - 1) * ' ')) + ',')\n        chr_line2 += c\n    print(index_line)\n    print(pri_line1)\n    print(chr_line2)\n", "label": 0}
{"function": "\n\ndef tick(self):\n    elapsed = (time.time() - self.startTime)\n    t = ((elapsed / float(self.flyTime)) if (self.flyTime > 0) else 1.0)\n    self.interp.InterpolateCamera(t, self.view.camera())\n    self.view.render()\n    if (t >= 1.0):\n        return False\n", "label": 0}
{"function": "\n\ndef battery_status(self):\n    'Attempts to get the battery charge percent.'\n    value = None\n    methods = [self.battery_status_read, self.battery_status_acpi, self.battery_status_upower]\n    for m in methods:\n        value = m()\n        if (value is not None):\n            break\n    return value\n", "label": 0}
{"function": "\n\ndef start(self, environment=None, user=None):\n    'Mark this result started.'\n    envs = [environment]\n    try:\n        latest = self.results.get(is_latest=True, tester=user, environment=environment)\n        if (latest.status == Result.STATUS.skipped):\n            envs = self.environments.all()\n    except ObjectDoesNotExist:\n        pass\n    for env in envs:\n        Result.objects.create(runcaseversion=self, tester=user, environment=env, status=Result.STATUS.started, user=user)\n", "label": 0}
{"function": "\n\ndef _update_section_contents(self, contents, section_name, new_values):\n    new_values = new_values.copy()\n    section_start_line_num = self._find_section_start(contents, section_name)\n    last_matching_line = section_start_line_num\n    j = (last_matching_line + 1)\n    while (j < len(contents)):\n        line = contents[j]\n        if (self.SECTION_REGEX.search(line) is not None):\n            self._insert_new_values(line_number=last_matching_line, contents=contents, new_values=new_values)\n            return\n        match = self.OPTION_REGEX.search(line)\n        if (match is not None):\n            last_matching_line = j\n            key_name = match.group(1).strip()\n            if (key_name in new_values):\n                if (not isinstance(new_values[key_name], dict)):\n                    option_value = new_values[key_name]\n                    new_line = ('%s = %s\\n' % (key_name, option_value))\n                    contents[j] = new_line\n                    del new_values[key_name]\n                else:\n                    j = self._update_subattributes(j, contents, new_values[key_name], (len(match.group(1)) - len(match.group(1).lstrip())))\n                    return\n        j += 1\n    if new_values:\n        if (not contents[(- 1)].endswith('\\n')):\n            contents.append('\\n')\n        self._insert_new_values(line_number=(last_matching_line + 1), contents=contents, new_values=new_values)\n", "label": 1}
{"function": "\n\ndef _connect_to_upstream(self):\n    if self.proxy_server:\n        upstream_sock = socks.socksocket()\n        upstream_sock.set_proxy(**self.proxy_server)\n    else:\n        upstream_sock = socket.socket()\n    try:\n        upstream_sock.settimeout(self.proxy_timeout)\n        upstream_sock.connect(self.upstream)\n        if self.use_ssl:\n            upstream_sock = wrap_ssl(upstream_sock)\n    except:\n        drop_socket(upstream_sock)\n        raise\n    self.logger.info(('Connected to upstream %s:%d' % self.upstream))\n    return upstream_sock\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(CreateForm, self).clean()\n    source_type = self.cleaned_data.get('volume_source_type')\n    if ((source_type == 'image_source') and (not cleaned_data.get('image_source'))):\n        msg = _('Image source must be specified')\n        self._errors['image_source'] = self.error_class([msg])\n    elif ((source_type == 'snapshot_source') and (not cleaned_data.get('snapshot_source'))):\n        msg = _('Snapshot source must be specified')\n        self._errors['snapshot_source'] = self.error_class([msg])\n    elif ((source_type == 'volume_source') and (not cleaned_data.get('volume_source'))):\n        msg = _('Volume source must be specified')\n        self._errors['volume_source'] = self.error_class([msg])\n    return cleaned_data\n", "label": 1}
{"function": "\n\ndef result_fail(self, environment=None, comment='', stepnumber=None, bug='', user=None):\n    'Create a failed result for this case.'\n    result = Result.objects.create(runcaseversion=self, tester=user, environment=environment, status=Result.STATUS.failed, comment=comment, user=user)\n    if (stepnumber is not None):\n        try:\n            step = self.caseversion.steps.get(number=stepnumber)\n        except CaseStep.DoesNotExist:\n            pass\n        else:\n            stepresult = StepResult(result=result, step=step)\n            stepresult.status = StepResult.STATUS.failed\n            stepresult.bug_url = bug\n            stepresult.save(user=user)\n    self.save(force_update=True, user=user)\n", "label": 0}
{"function": "\n\ndef __init__(self, content, url, headers=None, trusted=None):\n    encoding = None\n    if (headers and ('Content-Type' in headers)):\n        (content_type, params) = cgi.parse_header(headers['Content-Type'])\n        if ('charset' in params):\n            encoding = params['charset']\n    self.content = content\n    self.parsed = html5lib.parse(self.content, encoding=encoding, namespaceHTMLElements=False)\n    self.url = url\n    self.headers = headers\n    self.trusted = trusted\n", "label": 0}
{"function": "\n\ndef __init__(self, alias_format=None, param_stream=None):\n    self._format = (alias_format or '')\n    self._param_stream = (param_stream or '')\n", "label": 0}
{"function": "\n\ndef GetEditMediaLink(self):\n    'The Picasa API mistakenly returns media-edit rather than edit-media, but\\n    this may change soon.\\n    '\n    for a_link in self.link:\n        if (a_link.rel == 'edit-media'):\n            return a_link\n        if (a_link.rel == 'media-edit'):\n            return a_link\n    return None\n", "label": 0}
{"function": "\n\ndef ValidateTree(self, queries, additional_visitors=(), max_alter_rows=100000, allowed_engines=('InnoDB',), fail_fast=False):\n    'Validate a parse tree.\\n\\n    Args:\\n      tokens: pyparsing parse tree\\n\\n    Returns:\\n      Whether the tree validated\\n    '\n    visitors = ([ShardSetChecker(self._schema), AlterChecker(self._schema, max_alter_rows=max_alter_rows), CreateDatabaseChecker(self._schema), DropDatabaseChecker(self._schema), CreateTableChecker(self._schema, allowed_engines=allowed_engines), DropTableChecker(self._schema), ReplaceChecker(self._schema), ColumnChecker(self._schema)] + list(additional_visitors))\n    for query in queries:\n        self._CheckCancelled()\n        assert (query.getName() == 'query'), ('Invalid second-level token: %s' % query.getName())\n        logging.debug('Visiting: %s', query)\n        for visitor in visitors:\n            visitor.visit([query])\n            if (fail_fast and (visitor.Errors() or visitor.Warnings())):\n                self._Finalize(visitors)\n                return False\n        if self._callback:\n            self._callback(self._loc)\n    self._Finalize(visitors)\n    return ((not self._errors) and (not self._warnings))\n", "label": 1}
{"function": "\n\ndef _pd_assert_dibbler_calls(self, expected, actual):\n    \"Check the external process calls for dibbler are expected\\n\\n        in the case of multiple pd-enabled router ports, the exact sequence\\n        of these calls are not deterministic. It's known, though, that each\\n        external_process call is followed with either an enable() or disable()\\n        \"\n    num_ext_calls = (len(expected) // 2)\n    expected_ext_calls = []\n    actual_ext_calls = []\n    expected_action_calls = []\n    actual_action_calls = []\n    for c in range(num_ext_calls):\n        expected_ext_calls.append(expected[(c * 2)])\n        actual_ext_calls.append(actual[(c * 2)])\n        expected_action_calls.append(expected[((c * 2) + 1)])\n        actual_action_calls.append(actual[((c * 2) + 1)])\n    self.assertEqual(expected_action_calls, actual_action_calls)\n    for exp in expected_ext_calls:\n        for act in actual_ext_calls:\n            if (exp == act):\n                break\n        else:\n            msg = 'Unexpected dibbler external process call.'\n            self.fail(msg)\n", "label": 1}
{"function": "\n\ndef do_complete(self, code, cursor_pos):\n    no_complete = {\n        'status': 'ok',\n        'matches': [],\n        'cursor_start': 0,\n        'cursor_end': cursor_pos,\n        'metadata': dict(),\n    }\n    if ((not code) or (code[(- 1)] == ' ')):\n        return no_complete\n    tokens = code.split()\n    if (not tokens):\n        return no_complete\n    token = tokens[(- 1)]\n    start = (cursor_pos - len(token))\n    matches = self.qsh._complete(code, token)\n    return {\n        'status': 'ok',\n        'matches': sorted(matches),\n        'cursor_start': start,\n        'cursor_end': cursor_pos,\n        'metadata': dict(),\n    }\n", "label": 0}
{"function": "\n\ndef _check_expression(self, symbol, myself):\n    for (cre, state, action, next_state) in self._expressions:\n        mo = cre.match(symbol)\n        if ((state is self.current_state) and mo):\n            if (action is not None):\n                action(mo, self)\n            self.current_state = next_state\n", "label": 0}
{"function": "\n\ndef Tool(self, tool, toolpath=None, **kw):\n    if SCons.Util.is_String(tool):\n        tool = self.subst(tool)\n        if (toolpath is None):\n            toolpath = self.get('toolpath', [])\n        toolpath = list(map(self._find_toolpath_dir, toolpath))\n        tool = SCons.Tool.Tool(tool, toolpath, **kw)\n    tool(self)\n", "label": 0}
{"function": "\n\ndef get(self, *args):\n    self.preflight()\n    self.set_header('Content-Type', 'application/json')\n    m = self.settings.get('manager')\n    pname = ('%s.%s' % (args[0], args[1]))\n    if (not self.api_key.can_read(pname)):\n        raise HTTPError(403)\n    try:\n        info = m.info(pname)\n    except ProcessError:\n        self.set_status(404)\n        return self.write({\n            'error': 'not_found',\n        })\n    self.write(info)\n", "label": 0}
{"function": "\n\ndef send_test(self, test=True):\n    if (not test):\n        try:\n            del self._data['test']\n        except KeyError:\n            pass\n    else:\n        self._data['test'] = 1\n    return self\n", "label": 0}
{"function": "\n\ndef get_connection_mgr():\n    connection_mgr = context.get_context().connection_mgr\n    (server_models, sockpools) = ({\n        \n    }, {\n        \n    })\n    for k in connection_mgr.server_models:\n        server_model = connection_mgr.server_models[k]\n        server_models[repr(k)] = {\n            'info': repr(server_model),\n            'fds': [s.fileno() for s in server_model.active_connections.keys()],\n        }\n    for prot in connection_mgr.sockpools:\n        for sock_type in connection_mgr.sockpools[prot]:\n            sockpool = connection_mgr.sockpools[prot][sock_type]\n            sockpools[repr((prot, sock_type))] = {\n                'info': repr(sockpool),\n                'addresses': map(repr, sockpool.free_socks_by_addr.keys()),\n            }\n    return {\n        'server_models': server_models,\n        'sockpools': sockpools,\n    }\n", "label": 0}
{"function": "\n\ndef handler(self, zh, rc, data, stat):\n    'Handle zookeeper.aget() responses.\\n\\n    This code handles the zookeeper.aget callback. It does not handle watches.\\n\\n    Numeric arguments map to constants. See ``DATA`` in ``help(zookeeper)``\\n    for more information.\\n\\n    Args:\\n      zh Zookeeper handle that made this request.\\n      rc Return code.\\n      data Data stored in the znode.\\n\\n    Does not provide a return value.\\n    '\n    if (zookeeper.OK == rc):\n        logger.debug('This is where your application does work.')\n    else:\n        if (zookeeper.NONODE == rc):\n            logger.info('Node not found. Trying again to set the watch.')\n            time.sleep(1)\n        if (zookeeper.OK != self.aget()):\n            logger.critical('Unable to get znode! Exiting.')\n            sys.exit(1)\n", "label": 0}
{"function": "\n\ndef get_user_by_uri(self, uri):\n    for group in self.group_list:\n        user = group.get_user_by_uri(uri)\n        if user:\n            return user\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    super(BaseMigrationTestCase, self).setUp()\n    for model_class in MODELS:\n        model_class._meta.database = self.database\n    self.database.drop_tables(MODELS, True)\n    self.database.create_tables(MODELS)\n    self.migrator = self.migrator_class(self.database)\n    if ('newpages' in User._meta.reverse_rel):\n        del User._meta.reverse_rel['newpages']\n        delattr(User, 'newpages')\n", "label": 0}
{"function": "\n\ndef plot_connectivity_topos(self, fig=None):\n    ' Plot scalp projections of the sources.\\n\\n        This function only plots the topos. Use in combination with connectivity plotting.\\n\\n        Parameters\\n        ----------\\n        fig : {None, Figure object}, optional\\n            Where to plot the topos. f set to **None**, a new figure is created. Otherwise plot into the provided\\n            figure object.\\n\\n        Returns\\n        -------\\n        fig : Figure object\\n            Instance of the figure in which was plotted.\\n        '\n    self._prepare_plots(True, False)\n    if self.plot_outside_topo:\n        fig = self.plotting.plot_connectivity_topos('outside', self.topo_, self.mixmaps_, fig)\n    elif (self.plot_diagonal == 'topo'):\n        fig = self.plotting.plot_connectivity_topos('diagonal', self.topo_, self.mixmaps_, fig)\n    return fig\n", "label": 0}
{"function": "\n\ndef __getattr__(self, name):\n    if (name == 'x'):\n        return self._x\n    elif (name == 'y'):\n        return self._y\n    else:\n        return name\n", "label": 0}
{"function": "\n\ndef matchesPredicates(self, elem):\n    if ((self.elementName != None) and (self.elementName != elem.name)):\n        return 0\n    for p in self.predicates:\n        if (not p.value(elem)):\n            return 0\n    return 1\n", "label": 0}
{"function": "\n\ndef start_selection(self, event):\n    if (event.inaxes != self._axes):\n        return False\n    if (event.key == SCRUBBING_KEY):\n        if (not self._roi.defined()):\n            return False\n        elif (not self._roi.contains(event.xdata, event.ydata)):\n            return False\n    self._roi_store()\n    if (event.key == SCRUBBING_KEY):\n        self._scrubbing = True\n        self._dx = (event.xdata - self._roi.center())\n    else:\n        self.reset()\n        self._roi.set_range(event.xdata, event.xdata)\n        self._xi = event.xdata\n    self._mid_selection = True\n    self._sync_patch()\n", "label": 1}
{"function": "\n\ndef print_stats(stats):\n    console_logger.info((((' %-' + str(STATS_NAME_WIDTH)) + 's %7s %12s %7s %7s %7s  | %7s %7s') % ('Name', '# reqs', '# fails', 'Avg', 'Min', 'Max', 'Median', 'req/s')))\n    console_logger.info(('-' * (80 + STATS_NAME_WIDTH)))\n    total_rps = 0\n    total_reqs = 0\n    total_failures = 0\n    for key in sorted(stats.iterkeys()):\n        r = stats[key]\n        total_rps += r.current_rps\n        total_reqs += r.num_requests\n        total_failures += r.num_failures\n        console_logger.info(r)\n    console_logger.info(('-' * (80 + STATS_NAME_WIDTH)))\n    try:\n        fail_percent = ((total_failures / float(total_reqs)) * 100)\n    except ZeroDivisionError:\n        fail_percent = 0\n    console_logger.info((((' %-' + str(STATS_NAME_WIDTH)) + 's %7d %12s %42.2f') % ('Total', total_reqs, ('%d(%.2f%%)' % (total_failures, fail_percent)), total_rps)))\n    console_logger.info('')\n", "label": 0}
{"function": "\n\ndef __getattr__(self, name):\n    t = type(self)\n    try:\n        i = t.name_to_idx[name]\n    except KeyError:\n        raise AttributeError(name)\n    f = t.fields[i]\n    if (i < len(self.data)):\n        v = self.data[i]\n    else:\n        v = ''\n    if (len(f) >= 3):\n        if (v == ''):\n            return None\n        return f[2](v)\n    else:\n        return v\n", "label": 0}
{"function": "\n\ndef getUncontacted(self):\n    return [n for n in self if (n.id not in self.contacted)]\n", "label": 0}
{"function": "\n\ndef _test_sync_state_helper(self, known_net_ids, active_net_ids):\n    active_networks = set((mock.Mock(id=netid) for netid in active_net_ids))\n    with mock.patch(DHCP_PLUGIN) as plug:\n        mock_plugin = mock.Mock()\n        mock_plugin.get_active_networks_info.return_value = active_networks\n        plug.return_value = mock_plugin\n        dhcp = dhcp_agent.DhcpAgent(HOSTNAME)\n        attrs_to_mock = dict([(a, mock.DEFAULT) for a in ['disable_dhcp_helper', 'cache', 'safe_configure_dhcp_for_network']])\n        with mock.patch.multiple(dhcp, **attrs_to_mock) as mocks:\n            mocks['cache'].get_network_ids.return_value = known_net_ids\n            dhcp.sync_state()\n            diff = (set(known_net_ids) - set(active_net_ids))\n            exp_disable = [mock.call(net_id) for net_id in diff]\n            mocks['cache'].assert_has_calls([mock.call.get_network_ids()])\n            mocks['disable_dhcp_helper'].assert_has_calls(exp_disable)\n", "label": 0}
{"function": "\n\ndef write_lofl(lofl, filename):\n    a_file = open(filename, 'w')\n    for seq in lofl:\n        for it in seq:\n            a_file.write(('%s\\n' % str(it)))\n        a_file.write('\\n')\n    a_file.close()\n", "label": 0}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (node in self.nodes_seen):\n        self.nodes_seen.discard(node)\n        self.process_node(fgraph, node)\n    if (not isinstance(node, string_types)):\n        assert node.inputs\n    if isinstance(new_r, graph.Constant):\n        self.process_constant(fgraph, new_r)\n", "label": 0}
{"function": "\n\n@account_group.command(context_settings=CONTEXT_SETTINGS)\n@click.option('--token', '-t', type=str, help='digital ocean authentication token', metavar='<token>')\n@click.option('--tablefmt', '-f', type=click.Choice(['fancy_grid', 'simple', 'plain', 'grid', 'pipe', 'orgtbl', 'psql', 'rst', 'mediawiki', 'html', 'latex', 'latex_booktabs', 'tsv']), help='output table format', default='fancy_grid', metavar='<format>')\n@click.option('--proxy', '-p', help='proxy url to be used for this call', metavar='<http://ip:port>')\ndef account(token, tablefmt, proxy):\n    '\\n\\tget digital ocean account info\\n\\t'\n    method = 'GET'\n    url = ACCOUNT_INFO\n    result = DigitalOcean.do_request(method, url, token=token, proxy=proxy)\n    if result['has_error']:\n        click.echo()\n        click.echo(('Error: %s' % result['error_message']))\n    else:\n        record = 'account'\n        headers = ['Fields', 'Values']\n        table = []\n        for key in result['account'].keys():\n            table.append([key, result['account'][key]])\n        data = {\n            'headers': headers,\n            'table_data': table,\n        }\n        print_table(tablefmt, data, record)\n", "label": 0}
{"function": "\n\ndef _initialize_trunk_interfaces_to_none(self, switch_ip):\n    'Initialize all nexus interfaces to trunk allowed none.'\n    try:\n        switch_ifs = self._mdriver._get_switch_interfaces(switch_ip)\n        if (not switch_ifs):\n            LOG.debug('Skipping switch %s which has no configured interfaces', switch_ip)\n            return\n        self._driver.initialize_all_switch_interfaces(switch_ifs)\n    except Exception:\n        with excutils.save_and_reraise_exception():\n            LOG.warning(_LW('Unable to initialize interfaces to switch %(switch_ip)s'), {\n                'switch_ip': switch_ip,\n            })\n            self._mdriver.register_switch_as_inactive(switch_ip, 'replay init_interface')\n    for (switch_ip, intf_type, port, is_native, ch_grp) in switch_ifs:\n        try:\n            reserved = nxos_db.get_reserved_port_binding(switch_ip, self._mdriver.format_interface_name(intf_type, port))\n        except excep.NexusPortBindingNotFound:\n            continue\n        if (reserved[0].channel_group != ch_grp):\n            self._mdriver._change_baremetal_interfaces(switch_ip, intf_type, port, reserved[0].channel_group, ch_grp)\n    if self._mdriver.is_replay_enabled():\n        return\n    try:\n        mgr = self._driver.nxos_connect(switch_ip)\n        self._driver._close_session(mgr, switch_ip)\n    except Exception:\n        LOG.warning(_LW('Failed to release connection after initialize interfaces for switch %(switch_ip)s'), {\n            'switch_ip': switch_ip,\n        })\n", "label": 1}
{"function": "\n\ndef test_current_time(self):\n    self.skipTest('time.xmlrpc.com is unreliable')\n    server = xmlrpclib.ServerProxy('http://time.xmlrpc.com/RPC2')\n    try:\n        t0 = server.currentTime.getCurrentTime()\n    except socket.error as e:\n        self.skipTest(('network error: %s' % e))\n        return\n    t1 = xmlrpclib.DateTime()\n    dt0 = xmlrpclib._datetime_type(t0.value)\n    dt1 = xmlrpclib._datetime_type(t1.value)\n    if (dt0 > dt1):\n        delta = (dt0 - dt1)\n    else:\n        delta = (dt1 - dt0)\n    self.assertTrue((delta.days <= 1))\n", "label": 0}
{"function": "\n\ndef write_content(self, content, content_type=None):\n    'Helper method to write content bytes to output stream.'\n    if (content_type is not None):\n        self.send_header(HTTP_HEADER_CONTENT_TYPE, content_type)\n    if ('gzip' in self.headers.get(HTTP_HEADER_ACCEPT_ENCODING, '')):\n        content = gzip.compress(content)\n        self.send_header(HTTP_HEADER_CONTENT_ENCODING, 'gzip')\n        self.send_header(HTTP_HEADER_VARY, HTTP_HEADER_ACCEPT_ENCODING)\n    self.send_header(HTTP_HEADER_CONTENT_LENGTH, str(len(content)))\n    self.end_headers()\n    if (self.command == 'HEAD'):\n        return\n    self.wfile.write(content)\n", "label": 0}
{"function": "\n\ndef test_get_vifs_by_ids(self):\n    for i in range(2):\n        self.create_ovs_port()\n    vif_ports = [self.create_ovs_vif_port() for i in range(3)]\n    by_id = self.br.get_vifs_by_ids([v.vif_id for v in vif_ports])\n    by_id = {vid: str(vport) for (vid, vport) in by_id.items()}\n    self.assertEqual({v.vif_id: str(v) for v in vif_ports}, by_id)\n", "label": 1}
{"function": "\n\ndef all_consumed_offsets(self):\n    'Returns consumed offsets as {TopicPartition: OffsetAndMetadata}'\n    all_consumed = {\n        \n    }\n    for (partition, state) in six.iteritems(self.assignment):\n        if state.has_valid_position:\n            all_consumed[partition] = OffsetAndMetadata(state.position, '')\n    return all_consumed\n", "label": 0}
{"function": "\n\ndef _format_child_instances(self, children, parent_id):\n    '\\n        The goal of this method is to add an indent at every level. This way the\\n        WF is represented as a tree structure while in a list. For the right visuals\\n        representation the list must be a DF traversal else the idents will end up\\n        looking strange.\\n        '\n    children = format_wf_instances(children)\n    depth = {\n        parent_id: 0,\n    }\n    result = []\n    for child in children:\n        if (child.parent not in depth):\n            parent = None\n            for instance in children:\n                if (WF_PREFIX in instance.id):\n                    instance_id = instance.id[(instance.id.index(WF_PREFIX) + len(WF_PREFIX)):]\n                else:\n                    instance_id = instance.id\n                if (instance_id == child.parent):\n                    parent = instance\n            if (parent and parent.parent and (parent.parent in depth)):\n                depth[child.parent] = (depth[parent.parent] + 1)\n            else:\n                depth[child.parent] = 0\n        child.id = ((INDENT_CHAR * depth[child.parent]) + child.id)\n        result.append(self._format_for_common_representation(child))\n    return result\n", "label": 1}
{"function": "\n\ndef send(self):\n    '\\n        Sends the batch request to the server and returns a list of RpcResponse\\n        objects.  The list will be in the order that the requests were made to\\n        the batch.  Note that the RpcResponse objects may contain an error or a\\n        successful result.  When you iterate through the list, you must test for\\n        response.error.\\n\\n        send() may not be called more than once.\\n        '\n    if self.sent:\n        raise Exception('Batch already sent. Cannot send() again.')\n    else:\n        self.sent = True\n        results = self.client.transport.request(self.req_list)\n        id_to_method = {\n            \n        }\n        by_id = {\n            \n        }\n        for res in results:\n            reqid = res['id']\n            by_id[reqid] = res\n        in_req_order = []\n        for req in self.req_list:\n            reqid = req['id']\n            result = None\n            error = None\n            resp = safe_get(by_id, reqid)\n            if (resp is None):\n                msg = ('Batch response missing result for request id: %s' % reqid)\n                error = RpcException(ERR_INVALID_RESP, msg)\n            else:\n                r_err = safe_get(resp, 'error')\n                if (r_err is None):\n                    result = resp['result']\n                else:\n                    error = RpcException(r_err['code'], r_err['message'], safe_get(r_err, 'data'))\n            in_req_order.append(RpcResponse(req, result, error))\n        return in_req_order\n", "label": 1}
{"function": "\n\ndef handle(self, request, data):\n    volume_type_id = self.initial['id']\n    try:\n        cinder.volume_type_update(request, volume_type_id, data['name'], data['description'])\n        message = _('Successfully updated volume type.')\n        messages.success(request, message)\n        return True\n    except Exception as ex:\n        redirect = reverse('horizon:admin:volumes:index')\n        if (ex.code == 409):\n            error_message = _('New name conflicts with another volume type.')\n        else:\n            error_message = _('Unable to update volume type.')\n        exceptions.handle(request, error_message, redirect=redirect)\n", "label": 0}
{"function": "\n\ndef visit_Input(self, node):\n    name = node.name\n    width = (self.visit(node.width) if (node.width is not None) else None)\n    signed = node.signed\n    obj = vtypes.Input(width, signed=signed, name=name)\n    if (node.width is not None):\n        obj._set_raw_width(self.visit(node.width.msb), self.visit(node.width.lsb))\n    self.add_object(obj)\n    return obj\n", "label": 0}
{"function": "\n\ndef find(l, predicate):\n    results = [x for x in l if predicate(x)]\n    return (results[0] if (len(results) > 0) else None)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, position):\n    (i, j) = self._get_indexes(position)\n    if isinstance(i, slice):\n        if (j is None):\n            return Table(self.data[position])\n        else:\n            return Table((cells[j] for cells in self.data[i]))\n    else:\n        try:\n            row = self.data[i]\n        except IndexError:\n            msg = 'no row at index %r of %d-row table'\n            raise IndexError((msg % (position, len(self))))\n        if (j is None):\n            return row\n        else:\n            return row[j]\n", "label": 1}
{"function": "\n\ndef test_exists_many_with_none_keys(self):\n    try:\n        TestExistsMany.client.exists_many(None, {\n            \n        })\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'Keys should be specified as a list or tuple.')\n", "label": 0}
{"function": "\n\ndef _handle_message(self, opcode, data):\n    if self.client_terminated:\n        return\n    if (opcode == 1):\n        try:\n            decoded = data.decode('utf-8')\n        except UnicodeDecodeError:\n            self._abort()\n            return\n        self._run_callback(self.handler.on_message, decoded)\n    elif (opcode == 2):\n        self._run_callback(self.handler.on_message, data)\n    elif (opcode == 8):\n        self.client_terminated = True\n        if (len(data) >= 2):\n            self.handler.close_code = struct.unpack('>H', data[:2])[0]\n        if (len(data) > 2):\n            self.handler.close_reason = to_unicode(data[2:])\n        self.close()\n    elif (opcode == 9):\n        self._write_frame(True, 10, data)\n    elif (opcode == 10):\n        self._run_callback(self.handler.on_pong, data)\n    else:\n        self._abort()\n", "label": 1}
{"function": "\n\ndef ack(self, delivery_tag=None, multiple=False):\n    'Acknowledge Message.\\n\\n        :param int/long delivery_tag: Server-assigned delivery tag\\n        :param bool multiple: Acknowledge multiple messages\\n\\n        :raises AMQPInvalidArgument: Invalid Parameters\\n        :raises AMQPChannelError: Raises if the channel encountered an error.\\n        :raises AMQPConnectionError: Raises if the connection\\n                                     encountered an error.\\n\\n        :return:\\n        '\n    if ((delivery_tag is not None) and (not compatibility.is_integer(delivery_tag))):\n        raise AMQPInvalidArgument('delivery_tag should be an integer or None')\n    elif (not isinstance(multiple, bool)):\n        raise AMQPInvalidArgument('multiple should be a boolean')\n    ack_frame = pamqp_spec.Basic.Ack(delivery_tag=delivery_tag, multiple=multiple)\n    self._channel.write_frame(ack_frame)\n", "label": 0}
{"function": "\n\ndef tostring(self, sep='', endcard=True, padding=True):\n    \"\\n        Returns a string representation of the header.\\n\\n        By default this uses no separator between cards, adds the END card, and\\n        pads the string with spaces to the next multiple of 2880 bytes.  That\\n        is, it returns the header exactly as it would appear in a FITS file.\\n\\n        Parameters\\n        ----------\\n        sep : str, optional\\n            The character or string with which to separate cards.  By default\\n            there is no separator, but one could use ``'\\\\\\\\n'``, for example, to\\n            separate each card with a new line\\n\\n        endcard : bool, optional\\n            If True (default) adds the END card to the end of the header\\n            string\\n\\n        padding : bool, optional\\n            If True (default) pads the string with spaces out to the next\\n            multiple of 2880 characters\\n\\n        Returns\\n        -------\\n        s : string\\n            A string representing a FITS header.\\n        \"\n    lines = []\n    for card in self._cards:\n        s = str(card)\n        while s:\n            lines.append(s[:Card.length])\n            s = s[Card.length:]\n    s = sep.join(lines)\n    if endcard:\n        s += (sep + _pad('END'))\n    if padding:\n        s += (' ' * _pad_length(len(s)))\n    return s\n", "label": 0}
{"function": "\n\ndef mergeStatementsSequence(self, statement_sequence):\n    assert (statement_sequence.parent is self)\n    old_statements = list(self.getStatements())\n    assert (statement_sequence in old_statements), (statement_sequence, self)\n    merge_index = old_statements.index(statement_sequence)\n    new_statements = ((tuple(old_statements[:merge_index]) + statement_sequence.getStatements()) + tuple(old_statements[(merge_index + 1):]))\n    self.setChild('statements', new_statements)\n", "label": 0}
{"function": "\n\ndef _handle_message(self, message, buffer, record_fn):\n    'Build a response calling record_fn on all the TlsRecords in message\\n\\n        message: bytes to parse as TlsRecords\\n        record_fn: one of on_tls_request, on_tls_response to handle the record\\n        Returns tuple containing the bytes to send for all the records handled and any remaining unparsed data\\n        '\n    out = ''\n    message = (buffer.buffer + message)\n    buffer.buffer = ''\n    remaining = message\n    while remaining:\n        record = None\n        try:\n            (record, remaining) = tls.parse_tls(remaining, throw_on_incomplete=True)\n        except tls.types.TlsNotEnoughDataError:\n            if buffer.should_buffer:\n                buffer.buffer = remaining\n                if (len(buffer.buffer) >= buffer.MAX_BUFFER):\n                    buffer.buffer = ''\n            return out\n        if (not record):\n            return out\n        record_bytes = record_fn(record)\n        if record_bytes:\n            out += record_bytes\n        if (record.content_type == record.CONTENT_TYPE.CHANGE_CIPHER_SPEC):\n            buffer.should_buffer = False\n    return out\n", "label": 1}
{"function": "\n\n@classmethod\n@quickcache(['domain', 'app_id'])\ndef _app_data(cls, domain, app_id):\n    defaults = MaltAppData(AMPLIFIES_NOT_SET, AMPLIFIES_NOT_SET, 15, 3, False)\n    if (not app_id):\n        return defaults\n    try:\n        app = get_app(domain, app_id)\n    except Http404:\n        logger.debug(('App not found %s' % app_id))\n        return defaults\n    return MaltAppData(getattr(app, 'amplifies_workers', AMPLIFIES_NOT_SET), getattr(app, 'amplifies_project', AMPLIFIES_NOT_SET), getattr(app, 'minimum_use_threshold', 15), getattr(app, 'experienced_threshold', 3), app.is_deleted())\n", "label": 0}
{"function": "\n\ndef run_conv_nnet2_classif(use_gpu, seed, isize, ksize, bsize, n_train=10, check_isfinite=True, pickle=False, verbose=0, version=(- 1)):\n    'Run the train function returned by build_conv_nnet2_classif on one device.\\n    '\n    utt.seed_rng(seed)\n    (train, params, x_shape, y_shape, mode) = build_conv_nnet2_classif(use_gpu=use_gpu, isize=isize, ksize=ksize, n_batch=bsize, verbose=verbose, version=version, check_isfinite=check_isfinite)\n    if use_gpu:\n        device = 'GPU'\n    else:\n        device = 'CPU'\n    xval = my_rand(*x_shape)\n    yval = my_rand(*y_shape)\n    lr = theano._asarray(0.01, dtype='float32')\n    rvals = my_zeros(n_train)\n    t0 = time.time()\n    for i in xrange(n_train):\n        rvals[i] = train(xval, yval, lr)[0]\n    t1 = time.time()\n    print_mode(mode)\n    if (pickle and isinstance(mode, theano.compile.ProfileMode)):\n        import pickle\n        print(('BEGIN %s profile mode dump' % device))\n        print(pickle.dumps(mode))\n        print(('END %s profile mode dump' % device))\n", "label": 0}
{"function": "\n\ndef iqilu_download(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    ''\n    if re.match('http://v.iqilu.com/\\\\w+', url):\n        html = get_content(url)\n        url = match1(html, \"<input type='hidden' id='playerId' url='(.+)'\")\n        title = match1(html, '<meta name=\"description\" content=\"(.*?)\\\\\"\\\\W')\n        (type_, ext, size) = url_info(url)\n        print_info(site_info, title, type_, size)\n        if (not info_only):\n            download_urls([url], title, ext, total_size=None, output_dir=output_dir, merge=merge)\n", "label": 0}
{"function": "\n\ndef bPrimary(self, prob):\n    '\\n        The primary magnetic flux density from a magnetic vector potential\\n\\n        :param Problem prob: FDEM problem\\n        :rtype: numpy.ndarray\\n        :return: primary magnetic field\\n        '\n    formulation = prob._formulation\n    if (formulation is 'EB'):\n        gridX = prob.mesh.gridEx\n        gridY = prob.mesh.gridEy\n        gridZ = prob.mesh.gridEz\n        C = prob.mesh.edgeCurl\n    elif (formulation is 'HJ'):\n        gridX = prob.mesh.gridFx\n        gridY = prob.mesh.gridFy\n        gridZ = prob.mesh.gridFz\n        C = prob.mesh.edgeCurl.T\n    if (prob.mesh._meshType is 'CYL'):\n        if (not prob.mesh.isSymmetric):\n            raise NotImplementedError('Non-symmetric cyl mesh not implemented yet!')\n        a = MagneticDipoleVectorPotential(self.loc, gridY, 'y', mu=self.mu, moment=self.moment)\n    else:\n        srcfct = MagneticDipoleVectorPotential\n        ax = srcfct(self.loc, gridX, 'x', mu=self.mu, moment=self.moment)\n        ay = srcfct(self.loc, gridY, 'y', mu=self.mu, moment=self.moment)\n        az = srcfct(self.loc, gridZ, 'z', mu=self.mu, moment=self.moment)\n        a = np.concatenate((ax, ay, az))\n    return (C * a)\n", "label": 0}
{"function": "\n\ndef __init__(self, getter, attribute, new, spec, create, spec_set, autospec, new_callable, kwargs):\n    if (new_callable is not None):\n        if (new is not DEFAULT):\n            raise ValueError(\"Cannot use 'new' and 'new_callable' together\")\n        if (autospec is not None):\n            raise ValueError(\"Cannot use 'autospec' and 'new_callable' together\")\n    self.getter = getter\n    self.attribute = attribute\n    self.new = new\n    self.new_callable = new_callable\n    self.spec = spec\n    self.create = create\n    self.has_local = False\n    self.spec_set = spec_set\n    self.autospec = autospec\n    self.kwargs = kwargs\n    self.additional_patchers = []\n", "label": 0}
{"function": "\n\ndef validate_configurator_version():\n    '\\n    Arch is a rolling release distro, therefore it is important to ensure\\n    the configurator version is current.\\n    '\n    if (settings.CONFIGURATOR_MODULE == 'bootmachine.contrib.configurators.salt'):\n        pkgver = settings.SALT_AUR_PKGVER\n        pkgrel = settings.SALT_AUR_PKGREL\n        response = urllib2.urlopen('https://aur.archlinux.org/packages/sa/salt/PKGBUILD')\n        for line in response:\n            if (line.startswith('pkgver=') and (not (pkgver in line))):\n                abort(\"The requested Salt 'pkgrel={0}' in the AUR was updated to '{1}'.\".format(pkgver, line.strip()))\n            if (line.startswith('pkgrel=') and (not (pkgrel in line))):\n                abort(\"The requested Salt 'pkgrel={0}' in the AUR was updated to '{1}'.\".format(pkgrel, line.strip()))\n", "label": 1}
{"function": "\n\ndef slide(*args, **kwargs):\n\n    def wrap(f):\n\n        @wraps(f)\n        def wrapped_f():\n            print_func(f)\n            sys.stdin.readline()\n            if kwargs.get('executable', False):\n                print_func_result(f)\n                sys.stdin.readline()\n        return wrapped_f\n    if ((len(args) == 1) and callable(args[0])):\n        return wrap(args[0])\n    else:\n        return wrap\n", "label": 0}
{"function": "\n\ndef decistmt(s):\n    'Substitute Decimals for floats in a string of statements.\\n\\n    >>> from decimal import Decimal\\n    >>> s = \\'print +21.3e-5*-.1234/81.7\\'\\n    >>> decistmt(s)\\n    \"print +Decimal (\\'21.3e-5\\')*-Decimal (\\'.1234\\')/Decimal (\\'81.7\\')\"\\n\\n    The format of the exponent is inherited from the platform C library.\\n    Known cases are \"e-007\" (Windows) and \"e-07\" (not Windows).  Since\\n    we\\'re only showing 12 digits, and the 13th isn\\'t close to 5, the\\n    rest of the output should be platform-independent.\\n\\n    >>> exec(s) #doctest: +ELLIPSIS\\n    -3.21716034272e-0...7\\n\\n    Output from calculations with Decimal should be identical across all\\n    platforms.\\n\\n    >>> exec(decistmt(s))\\n    -3.217160342717258261933904529E-7\\n    '\n    result = []\n    g = generate_tokens(StringIO(s).readline)\n    for (toknum, tokval, _, _, _) in g:\n        if ((toknum == NUMBER) and ('.' in tokval)):\n            result.extend([(NAME, 'Decimal'), (OP, '('), (STRING, repr(tokval)), (OP, ')')])\n        else:\n            result.append((toknum, tokval))\n    return untokenize(result)\n", "label": 0}
{"function": "\n\ndef onRadioSelect(self, sender, keyCode=None, modifiers=None):\n    if (sender == self.radLevel5):\n        self.level = 5\n    elif (sender == self.radLevel10):\n        self.level = 10\n    elif (sender == self.radLevel15):\n        self.level = 15\n    elif (sender == self.radLevel20):\n        self.level = 20\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _aggregator_counter_kind(combine_fn):\n    \"Returns the counter aggregation kind for the combine_fn passed in.\\n\\n    Args:\\n      combine_fn: The combining function used in an Aggregator.\\n\\n    Returns:\\n      The aggregation_kind (to use in a Counter) that matches combine_fn.\\n\\n    Raises:\\n      ValueError if the combine_fn doesn't map to any supported\\n      aggregation kind.\\n    \"\n    combine_kind_map = {\n        sum: Counter.SUM,\n        max: Counter.MAX,\n        min: Counter.MIN,\n        combiners.Mean: Counter.MEAN,\n    }\n    try:\n        return combine_kind_map[combine_fn]\n    except KeyError:\n        try:\n            return combine_kind_map[combine_fn.__class__]\n        except KeyError:\n            raise ValueError(('combine_fn %r (class %r) does not map to a supported aggregation kind' % (combine_fn, combine_fn.__class__)))\n", "label": 0}
{"function": "\n\ndef run(self):\n    '\\n        Kill any open Redshift sessions for the given database.\\n        '\n    connection = self.output().connect()\n    query = \"select pg_terminate_backend(process) from STV_SESSIONS where db_name=%s and user_name != 'rdsdb' and process != pg_backend_pid()\"\n    cursor = connection.cursor()\n    logger.info('Killing all open Redshift sessions for database: %s', self.database)\n    try:\n        cursor.execute(query, (self.database,))\n        cursor.close()\n        connection.commit()\n    except psycopg2.DatabaseError as e:\n        if (e.message and ('EOF' in e.message)):\n            connection.close()\n            logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n            time.sleep(self.connection_reset_wait_seconds)\n            logger.info('Reconnecting to Redshift')\n            connection = self.output().connect()\n        else:\n            raise\n    try:\n        self.output().touch(connection)\n        connection.commit()\n    finally:\n        connection.close()\n    logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n", "label": 0}
{"function": "\n\n@AccountRequired\n@PostOnly\ndef unfollow(request, user_id):\n    followed = get_object_or_404(User, pk=user_id)\n    try:\n        follow_instance = UserToUserFollow.objects.get(follower=request.user, followed=followed)\n        follow_instance.is_following = False\n        follow_instance.stopped_following = datetime.datetime.now()\n        follow_instance.save()\n    except UserToUserFollow.DoesNotExist:\n        pass\n    cache.bust(followed)\n    if request.is_ajax():\n        button = render_inclusiontag(request, 'follow_button followed', 'users_tags', {\n            'followed': followed,\n        })\n        return json_response({\n            'button': button,\n        })\n    else:\n        return redirect(followed)\n", "label": 0}
{"function": "\n\ndef alphanumeric_sort_key(key):\n    '\\n    Sort the given iterable in the way that humans expect.\\n    Thanks to http://stackoverflow.com/a/2669120/240553\\n    '\n    import re\n    convert = (lambda text: (int(text) if text.isdigit() else text))\n    return [convert(c) for c in re.split('([0-9]+)', key)]\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _fill_scope_refs(name, scope):\n    \"Put referenced name in 'ref' dictionary of a scope.\\n\\n        Walks up the scope tree and adds the name to 'ref' of every scope\\n        up in the tree until a scope that defines referenced name is reached.\\n        \"\n    symbol = scope.resolve(name)\n    if (symbol is None):\n        return\n    orig_scope = symbol.scope\n    scope.refs[name] = orig_scope\n    while (scope is not orig_scope):\n        scope = scope.get_enclosing_scope()\n        scope.refs[name] = orig_scope\n", "label": 0}
{"function": "\n\ndef _read_word_block(self, stream):\n    words = []\n    for i in range(20):\n        line = stream.readline()\n        if (not line):\n            continue\n        words.append(line.strip())\n    return words\n", "label": 0}
{"function": "\n\ndef deallocate_for_instance(self, context, instance, **kwargs):\n    'Deallocate all network resources related to the instance.'\n    LOG.debug('deallocate_for_instance()', instance=instance)\n    search_opts = {\n        'device_id': instance.uuid,\n    }\n    neutron = get_client(context)\n    data = neutron.list_ports(**search_opts)\n    ports = [port['id'] for port in data.get('ports', [])]\n    requested_networks = (kwargs.get('requested_networks') or [])\n    if isinstance(requested_networks, objects.NetworkRequestList):\n        requested_networks = requested_networks.as_tuples()\n    ports_to_skip = set([port_id for (nets, fips, port_id, pci_request_id) in requested_networks])\n    ports_to_skip |= set(self._get_preexisting_port_ids(instance))\n    ports = (set(ports) - ports_to_skip)\n    self._unbind_ports(context, ports_to_skip, neutron)\n    self._delete_ports(neutron, instance, ports, raise_if_fail=True)\n    base_api.update_instance_cache_with_nw_info(self, context, instance, network_model.NetworkInfo([]))\n", "label": 0}
{"function": "\n\ndef process_failure(self, partial_eval, error, feature, dpoint, d_index):\n    logger.warning(('Fail evaluating %s: %s %s' % (feature, type(error), error)))\n    self._fit_failure_stats['discarded_samples'].append(dpoint.get('pk', 'PK-NOT-FOUND'))\n    feature_errors = self._fit_failure_stats['features'][feature]\n    feature_errors.append(dpoint)\n    if (d_index < self.FEATURE_STRICT_UNTIL):\n        self.exclude_feature(feature, partial_eval)\n    elif (len(feature_errors) > self.FEATURE_MAX_ERRORS_ALLOWED):\n        self.exclude_feature(feature, partial_eval)\n", "label": 0}
{"function": "\n\ndef configure_formatter(self, config):\n    'Configure a formatter from a dictionary.'\n    if ('()' in config):\n        factory = config['()']\n        try:\n            result = self.configure_custom(config)\n        except TypeError as te:\n            if (\"'format'\" not in str(te)):\n                raise\n            config['fmt'] = config.pop('format')\n            config['()'] = factory\n            result = self.configure_custom(config)\n    else:\n        fmt = config.get('format', None)\n        dfmt = config.get('datefmt', None)\n        result = logging.Formatter(fmt, dfmt)\n    return result\n", "label": 0}
{"function": "\n\ndef compile_dir(env, src_path, dst_path, pattern='^.*\\\\.html$', encoding='utf-8', base_dir=None):\n    'Compiles a directory of Jinja2 templates to python code.\\n  \\n  :param env: a Jinja2 Environment instance.\\n  :param src_path: path to the source directory.\\n  :param dst_path: path to the destination directory.\\n  :param encoding: template encoding.\\n  :param base_dir: the base path to be removed from the compiled template filename.\\n  '\n    from os import path, listdir, mkdir\n    file_re = re.compile(pattern)\n    if (base_dir is None):\n        base_dir = src_path\n    for filename in listdir(src_path):\n        src_name = path.join(src_path, filename)\n        dst_name = path.join(dst_path, filename)\n        if path.isdir(src_name):\n            mkdir(dst_name)\n            compile_dir(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n        elif (path.isfile(src_name) and file_re.match(filename)):\n            compile_file(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n", "label": 1}
{"function": "\n\ndef get_project_users_roles(request, project):\n    users_roles = collections.defaultdict(list)\n    if (VERSIONS.active < 3):\n        project_users = user_list(request, project=project)\n        for user in project_users:\n            roles = roles_for_user(request, user.id, project)\n            roles_ids = [role.id for role in roles]\n            users_roles[user.id].extend(roles_ids)\n    else:\n        project_role_assignments = role_assignments_list(request, project=project)\n        for role_assignment in project_role_assignments:\n            if (not hasattr(role_assignment, 'user')):\n                continue\n            user_id = role_assignment.user['id']\n            role_id = role_assignment.role['id']\n            if (('project' in role_assignment.scope) and (role_assignment.scope['project']['id'] == project)):\n                users_roles[user_id].append(role_id)\n    return users_roles\n", "label": 1}
{"function": "\n\n@attr('slow')\ndef test_basic(self):\n    '\\n        Tests that basic correlations work for odd and even\\n        dimensions of image and filter shapes, as well as rectangular\\n        images and filters.\\n        '\n    border_modes = ['valid', 'full', 'half', (1, 1), (2, 1), (1, 2), (3, 3), 1]\n    img_shapes = [(2, 2, 3, 3), (3, 2, 8, 8), (3, 2, 7, 5), (3, 2, 7, 5), (3, 2, 8, 8), (3, 2, 7, 5)]\n    fil_shapes = [(2, 2, 2, 2), (4, 2, 5, 5), (5, 2, 2, 3), (5, 2, 3, 2), (4, 2, 5, 5), (5, 2, 2, 3)]\n    for border_mode in border_modes:\n        for (img, fil) in zip(img_shapes, fil_shapes):\n            self.validate(img, fil, border_mode, verify_grad=False)\n    self.validate((1, 10, 213, 129), (46, 10, 212, 1), 'valid', verify_grad=False)\n", "label": 0}
{"function": "\n\ndef _on_auth(self, user):\n    if (not user):\n        raise tornado.web.HTTPError(403, 'Google auth failed')\n    access_token = user['access_token']\n    try:\n        response = httpclient.HTTPClient().fetch('https://www.googleapis.com/plus/v1/people/me', headers={\n            'Authorization': ('Bearer %s' % access_token),\n        })\n    except Exception as e:\n        raise tornado.web.HTTPError(403, ('Google auth failed: %s' % e))\n    email = json.loads(response.body.decode('utf-8'))['emails'][0]['value']\n    if (not re.match(self.application.options.auth, email)):\n        message = \"Access denied to '{email}'. Please use another account or ask your admin to add your email to flower --auth.\".format(email=email)\n        raise tornado.web.HTTPError(403, message)\n    self.set_secure_cookie('user', str(email))\n    next = self.get_argument('next', '/')\n    self.redirect(next)\n", "label": 0}
{"function": "\n\ndef clean_content(form):\n    status = form.cleaned_data.get('status')\n    content = form.cleaned_data.get('content')\n    if ((status == CONTENT_STATUS_PUBLISHED) and (not content)):\n        raise ValidationError(_('This field is required if status is set to published.'))\n    return content\n", "label": 0}
{"function": "\n\ndef create_list_setting(self, name):\n    hlayout = QtGui.QHBoxLayout()\n    setting = self.get_setting(name)\n    button = None\n    if setting.button:\n        button = QtGui.QPushButton(setting.button)\n        button.clicked.connect((lambda : setting.button_callback(button)))\n    combo = QtGui.QComboBox()\n    combo.setObjectName(setting.name)\n    combo.currentIndexChanged.connect(self.call_with_object('setting_changed', combo, setting))\n    combo.editTextChanged.connect(self.call_with_object('setting_changed', combo, setting))\n    combo.setStatusTip(setting.description)\n    combo.setToolTip(setting.description)\n    for val in setting.values:\n        combo.addItem(val)\n    default_index = combo.findText(setting.default_value)\n    if (default_index != (- 1)):\n        combo.setCurrentIndex(default_index)\n    hlayout.addWidget(QtGui.QLabel())\n    hlayout.addWidget(combo)\n    if button:\n        hlayout.addWidget(button)\n    return hlayout\n", "label": 0}
{"function": "\n\ndef _fill_dict(self, xmldoc, element_name):\n    xmlelements = self._get_child_nodes(xmldoc, element_name)\n    if xmlelements:\n        return_obj = {\n            \n        }\n        for child in xmlelements[0].childNodes:\n            if child.firstChild:\n                return_obj[child.nodeName] = child.firstChild.nodeValue\n        return return_obj\n", "label": 0}
{"function": "\n\ndef onMouseUp(self, sender, x, y):\n    self.dragging = NOT_DRAGGING\n    if self.draggingImage:\n        GlassWidget.hide()\n        if ((self.currentDragOperation == 'none') or (not self.currentTargetElement)):\n            if self.currentTargetElement:\n                self.fireDNDEvent('dragleave', self.currentTargetElement, self.currentDropWidget)\n            else:\n                self.currentDragOperation = 'none'\n            self.returnDrag()\n        else:\n            drop_event = self.fireDNDEvent('drop', self.currentTargetElement, self.currentDropWidget)\n            if isCanceled(drop_event):\n                self.currentDragOperation = drop_event.dataTransfer.dropEffect\n            else:\n                self.currentDragOperation = 'none'\n            self.zapDragImage()\n        self.fireDNDEvent('dragend', None, self.dragWidget)\n", "label": 1}
{"function": "\n\n@blueprint.route('/remove', methods=['POST'])\n@api_wrapper\n@require_login\n@require_team\ndef team_remove_hook():\n    if (api.auth.is_logged_in() and api.user.in_team()):\n        uid = request.form.get('uid')\n        user = api.user.get_user()\n        if (uid == user['uid']):\n            confirm = request.form.get('confirm')\n            team = api.team.get_team()\n            if (confirm != team['teamname']):\n                raise WebException('Please confirm your name.')\n        message = api.team.remove(uid)\n        return {\n            'success': 1,\n            'message': message,\n        }\n    else:\n        raise WebException('Stop. Just stop.')\n", "label": 0}
{"function": "\n\ndef handle_entityref(self, ref):\n    if (not self.elementstack):\n        return\n    if (ref in ('lt', 'gt', 'quot', 'amp', 'apos')):\n        text = ('&%s;' % ref)\n    elif (ref in self.entities):\n        text = self.entities[ref]\n        if (text.startswith('&#') and text.endswith(';')):\n            return self.handle_entityref(text)\n    else:\n        try:\n            name2codepoint[ref]\n        except KeyError:\n            text = ('&%s;' % ref)\n        else:\n            text = chr(name2codepoint[ref]).encode('utf-8')\n    self.elementstack[(- 1)][2].append(text)\n", "label": 1}
{"function": "\n\ndef init_layout(self):\n    ' Initialize the layout for the menu bar.\\n\\n        '\n    super(QtMenuBar, self).init_layout()\n    widget = self.widget\n    for child in self.children():\n        if isinstance(child, QtMenu):\n            widget.addMenu(child.widget)\n", "label": 0}
{"function": "\n\ndef _plot_mean(self, canvas, helper_data, helper_prediction, levels=20, projection='2d', label=None, **kwargs):\n    (_, free_dims, Xgrid, x, y, _, _, resolution) = helper_data\n    if (len(free_dims) <= 2):\n        (mu, _, _) = helper_prediction\n        if (len(free_dims) == 1):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_1d)\n            plots = dict(gpmean=[pl().plot(canvas, Xgrid[:, free_dims], mu, label=label, **kwargs)])\n        elif (projection == '2d'):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_2d)\n            plots = dict(gpmean=[pl().contour(canvas, x[:, 0], y[0, :], mu.reshape(resolution, resolution).T, levels=levels, label=label, **kwargs)])\n        elif (projection == '3d'):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_3d)\n            plots = dict(gpmean=[pl().surface(canvas, x, y, mu.reshape(resolution, resolution), label=label, **kwargs)])\n    elif (len(free_dims) == 0):\n        pass\n    else:\n        raise RuntimeError('Cannot plot mean in more then 2 input dimensions')\n    return plots\n", "label": 1}
{"function": "\n\ndef clean_email(self):\n    value = self.cleaned_data['email']\n    if (UNIQUE_EMAIL or EMAIL_AUTHENTICATION):\n        try:\n            User.objects.get(email__iexact=value)\n        except User.DoesNotExist:\n            return value\n        raise forms.ValidationError(_('A user is registered with this e-mail address.'))\n    return value\n", "label": 0}
{"function": "\n\n@classmethod\ndef update_pd(cls, pd):\n    if (len(pd.data_units) > 0):\n        du_urls = [i.url for i in pd.data_units.values()]\n", "label": 0}
{"function": "\n\ndef missing_fetch_positions(self):\n    missing = set()\n    for (partition, state) in six.iteritems(self.assignment):\n        if (not state.has_valid_position):\n            missing.add(partition)\n    return missing\n", "label": 0}
{"function": "\n\ndef append(self, value):\n    'adds value to ring buffer'\n    idx = self.index = ((self.index + 1) % self.data.size)\n    self.data[idx] = max(0, (value - self.lastval))\n    self.lastval = value\n    dt = (time.time() - self.toffset)\n    if ((idx == 0) and (max(self.times) < 0)):\n        dt = 0\n    self.times[idx] = dt\n", "label": 0}
{"function": "\n\ndef get_contents_if_file(contents_or_file_name):\n    'Get the contents of a file.\\n\\n    If the value passed in is a file name or file URI, return the\\n    contents. If not, or there is an error reading the file contents,\\n    return the value passed in as the contents.\\n\\n    For example, a workflow definition will be returned if either the\\n    workflow definition file name, or file URI are passed in, or the\\n    actual workflow definition itself is passed in.\\n    '\n    try:\n        if parse.urlparse(contents_or_file_name).scheme:\n            definition_url = contents_or_file_name\n        else:\n            path = os.path.abspath(contents_or_file_name)\n            definition_url = parse.urljoin('file:', request.pathname2url(path))\n        return request.urlopen(definition_url).read().decode('utf8')\n    except Exception:\n        return contents_or_file_name\n", "label": 0}
{"function": "\n\ndef user_shipping_address_view(request, checkout):\n    data = (request.POST or None)\n    additional_addresses = request.user.addresses.all()\n    checkout.email = request.user.email\n    shipping_address = checkout.shipping_address\n    if ((shipping_address is not None) and shipping_address.id):\n        address_form = AddressForm(data, autocomplete_type='shipping', initial={\n            'country': request.country,\n        })\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses, initial={\n            'address': shipping_address.id,\n        })\n    elif shipping_address:\n        address_form = AddressForm(data, instance=shipping_address)\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses)\n    else:\n        address_form = AddressForm(data, initial={\n            'country': request.country,\n        })\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses)\n    if addresses_form.is_valid():\n        if (addresses_form.cleaned_data['address'] != ShippingAddressesForm.NEW_ADDRESS):\n            address_id = addresses_form.cleaned_data['address']\n            checkout.shipping_address = Address.objects.get(id=address_id)\n            return redirect('checkout:shipping-method')\n        elif address_form.is_valid():\n            checkout.shipping_address = address_form.instance\n            return redirect('checkout:shipping-method')\n    return TemplateResponse(request, 'checkout/shipping_address.html', context={\n        'address_form': address_form,\n        'user_form': addresses_form,\n        'checkout': checkout,\n        'additional_addresses': additional_addresses,\n    })\n", "label": 1}
{"function": "\n\ndef __init__(self, socket_name=None, socket_path=None, config_file=None, colors=None, **kwargs):\n    EnvironmentMixin.__init__(self, '-g')\n    self._windows = []\n    self._panes = []\n    if socket_name:\n        self.socket_name = socket_name\n    if socket_path:\n        self.socket_path = socket_path\n    if config_file:\n        self.config_file = config_file\n    if colors:\n        self.colors = colors\n", "label": 0}
{"function": "\n\ndef length_of_longest_string(list_of_strings):\n    if (len(list_of_strings) == 0):\n        return 0\n    result = 0\n    for string in list_of_strings:\n        length_of_string = len(string)\n        if (length_of_string > result):\n            result = length_of_string\n    return result\n", "label": 0}
{"function": "\n\ndef check_inline(self, cls, parent_model):\n    \" Validate inline class's fk field is not excluded. \"\n    fk = _get_foreign_key(parent_model, cls.model, fk_name=cls.fk_name, can_fail=True)\n    if (hasattr(cls, 'exclude') and cls.exclude):\n        if (fk and (fk.name in cls.exclude)):\n            raise ImproperlyConfigured((\"%s cannot exclude the field '%s' - this is the foreign key to the parent model %s.%s.\" % (cls.__name__, fk.name, parent_model._meta.app_label, parent_model.__name__)))\n", "label": 0}
{"function": "\n\ndef _get_kind(self, path):\n    obj = self.repository._repo[self._get_id_for_path(path)]\n    if isinstance(obj, objects.Blob):\n        return NodeKind.FILE\n    elif isinstance(obj, objects.Tree):\n        return NodeKind.DIR\n", "label": 0}
{"function": "\n\ndef reexecutable_tasks(self, task_filter):\n    'Keep only reexecutable tasks which match the filter.\\n\\n        Filter is the list of values. If task has reexecute_on key and its\\n        value matches the value from filter then task is not skipped.\\n        :param task_filter: filter (list)\\n        '\n    if (not task_filter):\n        return\n    task_filter = set(task_filter)\n    for task in six.itervalues(self.node):\n        reexecute_on = task.get('reexecute_on')\n        if ((reexecute_on is not None) and task_filter.issubset(reexecute_on)):\n            task['skipped'] = False\n        else:\n            self.make_skipped_task(task)\n", "label": 0}
{"function": "\n\ndef post(self, request):\n    email = request.POST.get('email')\n    api_key = request.POST.get('api_key')\n    if ((not email) or (not api_key)):\n        message = {\n            'success': False,\n            'errors': [],\n        }\n        if (not email):\n            message['errors'].append('Email is mandatory.')\n        if (not api_key):\n            message['errors'].append('API key is mandatory.')\n        return HttpResponseBadRequest(json.dumps(message))\n    member = self.get_member(email, api_key)\n    if (member is None):\n        return HttpResponseForbidden(json.dumps({\n            'success': False,\n            'errors': ['Bad credentials.'],\n        }))\n    return HttpResponse(json.dumps({\n        'success': True,\n        'output': member.get_storage_limit(),\n    }))\n", "label": 1}
{"function": "\n\n@property\ndef active_gen_id(self):\n    if self._active_gen_id:\n        return self._active_gen_id\n    active_nodes = InfrastructureNode.objects.filter(deployment_name=self.deployment_name, is_active_generation=1)\n    if (len(active_nodes) == 0):\n        return None\n    first_active_node = active_nodes[0]\n    gen_id = first_active_node.generation_id\n    for active_node in active_nodes:\n        if (active_node.generation_id != gen_id):\n            err_str = 'Inconsistent generation ids in simpledb. %s:%s and %s:%s both marked active'\n            context = (first_active_node.aws_id, first_active_node.generation_id, active_node.aws_id, active_node.generation_id)\n            raise Exception((err_str % context))\n    self._active_gen_id = gen_id\n    return self._active_gen_id\n", "label": 0}
{"function": "\n\ndef get_module_path(module_name):\n    try:\n        module = import_module(module_name)\n    except ImportError as e:\n        raise ImproperlyConfigured(('Error importing HIDE_IN_STACKTRACES: %s' % (e,)))\n    else:\n        source_path = inspect.getsourcefile(module)\n        if source_path.endswith('__init__.py'):\n            source_path = os.path.dirname(source_path)\n        return os.path.realpath(source_path)\n", "label": 0}
{"function": "\n\ndef _extend_network_dict_address_scope(self, network_res, network_db):\n    network_res[ext_address_scope.IPV4_ADDRESS_SCOPE] = None\n    network_res[ext_address_scope.IPV6_ADDRESS_SCOPE] = None\n    subnetpools = {subnet.subnetpool for subnet in network_db.subnets if subnet.subnetpool}\n    for subnetpool in subnetpools:\n        as_id = subnetpool[ext_address_scope.ADDRESS_SCOPE_ID]\n        if (subnetpool['ip_version'] == constants.IP_VERSION_4):\n            network_res[ext_address_scope.IPV4_ADDRESS_SCOPE] = as_id\n        if (subnetpool['ip_version'] == constants.IP_VERSION_6):\n            network_res[ext_address_scope.IPV6_ADDRESS_SCOPE] = as_id\n    return network_res\n", "label": 1}
{"function": "\n\ndef _read_bytes_from_socket(self, msglen):\n    ' Read bytes from the socket. '\n    chunks = []\n    bytes_recd = 0\n    while (bytes_recd < msglen):\n        if self.stop.is_set():\n            raise InterruptLoop('Stopped while reading from socket')\n        try:\n            chunk = self.socket.recv(min((msglen - bytes_recd), 2048))\n            if (chunk == b''):\n                raise socket.error('socket connection broken')\n            chunks.append(chunk)\n            bytes_recd += len(chunk)\n        except socket.timeout:\n            continue\n        except ssl.SSLError as exc:\n            if _is_ssl_timeout(exc):\n                continue\n            raise\n    return b''.join(chunks)\n", "label": 1}
{"function": "\n\ndef fixup_indent(suite):\n    ' If an INDENT is followed by a thing with a prefix then nuke the prefix\\n        Otherwise we get in trouble when removing __metaclass__ at suite start\\n    '\n    kids = suite.children[::(- 1)]\n    while kids:\n        node = kids.pop()\n        if (node.type == token.INDENT):\n            break\n    while kids:\n        node = kids.pop()\n        if (isinstance(node, Leaf) and (node.type != token.DEDENT)):\n            if node.prefix:\n                node.prefix = ''\n            return\n        else:\n            kids.extend(node.children[::(- 1)])\n", "label": 1}
{"function": "\n\n@property\ndef message(self):\n    esc = self.enhanced_status_code\n    msg = self._message\n    if (esc and msg):\n        return ' '.join((esc, msg))\n    else:\n        return msg\n", "label": 0}
{"function": "\n\n@dispatch(Expr, MongoQuery)\ndef post_compute(e, q, scope=None):\n    \"\\n    Execute a query using MongoDB's aggregation pipeline\\n\\n    The compute_up functions operate on Mongo Collection / list-of-dict\\n    queries.  Once they're done we need to actually execute the query on\\n    MongoDB.  We do this using the aggregation pipeline framework.\\n\\n    http://docs.mongodb.org/manual/core/aggregation-pipeline/\\n    \"\n    scope = {\n        '$project': toolz.merge({\n            '_id': 0,\n        }, dict(((col, 1) for col in e.fields))),\n    }\n    q = q.append(scope)\n    if (not e.dshape.shape):\n        result = get_result(q.coll.aggregate(list(q.query)))[0]\n        if isscalar(e.dshape.measure):\n            return result[e._name]\n        else:\n            return get(e.fields, result)\n    dicts = get_result(q.coll.aggregate(list(q.query)))\n    if isscalar(e.dshape.measure):\n        return list(pluck(e.fields[0], dicts, default=None))\n    else:\n        return list(pluck(e.fields, dicts, default=None))\n", "label": 0}
{"function": "\n\ndef test_deploy(self):\n    fake_tenants_list = [self.fake_tenant_0, self.fake_tenant_1]\n    fake_users_list = [self.fake_user_0, self.fake_user_1]\n    fake_roles_list = [self.fake_role_0, self.fake_role_1]\n    fake_info = self._get_fake_info(fake_tenants_list, fake_users_list, fake_roles_list)\n    self.mock_client().tenants.list.return_value = [fake_tenants_list[0]]\n    self.mock_client().users.list.return_value = [fake_users_list[0]]\n    self.mock_client().roles.list.return_value = [fake_roles_list[0]]\n    self.mock_client().roles.roles_for_user.return_value = [self.fake_role_1]\n\n    def tenant_create(**kwargs):\n        self.mock_client().tenants.list.return_value.append(fake_tenants_list[1])\n        return fake_tenants_list[1]\n\n    def user_create(**kwars):\n        self.mock_client().users.list.return_value.append(fake_users_list[1])\n        return fake_users_list[1]\n\n    def roles_create(role_name):\n        self.mock_client().roles.list.return_value.append(fake_roles_list[1])\n        return fake_roles_list[1]\n    self.mock_client().tenants.create = tenant_create\n    self.mock_client().users.create = user_create\n    self.mock_client().roles.create = roles_create\n    self.keystone_client.deploy(fake_info)\n    mock_calls = []\n    for user in fake_users_list:\n        for tenant in fake_tenants_list:\n            mock_calls.append(mock.call(user.id, fake_roles_list[0].id, tenant.id))\n    self.assertEquals(mock_calls, self.mock_client().roles.add_user_role.mock_calls)\n", "label": 0}
{"function": "\n\ndef update(self, other=None, **kwargs):\n    if (other is None):\n        pass\n    elif hasattr(other, 'iteritems'):\n        for (k, v) in other.iteritems():\n            self[k] = v\n    elif hasattr(other, 'keys'):\n        for k in other.keys():\n            self[k] = other[k]\n    else:\n        for (k, v) in other:\n            self[k] = v\n    if kwargs:\n        self.update(kwargs)\n", "label": 1}
{"function": "\n\ndef showStack(self, index):\n    if ((index >= self.getWidgetCount()) or (index == self.visibleStack)):\n        return\n    if (self.visibleStack >= 0):\n        self.setStackVisible(self.visibleStack, False)\n    self.visibleStack = index\n    self.setStackVisible(self.visibleStack, True)\n    for listener in self.stackListeners:\n        listener.onStackChanged(self, index)\n", "label": 0}
{"function": "\n\ndef list_users(order_by='id'):\n    '\\n    Show all users for this company.\\n\\n    CLI Example:\\n\\n        salt myminion bamboohr.list_users\\n\\n    By default, the return data will be keyed by ID. However, it can be ordered\\n    by any other field. Keep in mind that if the field that is chosen contains\\n    duplicate values (i.e., location is used, for a company which only has one\\n    location), then each duplicate value will be overwritten by the previous.\\n    Therefore, it is advisable to only sort by fields that are guaranteed to be\\n    unique.\\n\\n    CLI Examples:\\n\\n        salt myminion bamboohr.list_users order_by=id\\n        salt myminion bamboohr.list_users order_by=email\\n    '\n    ret = {\n        \n    }\n    (status, result) = _query(action='meta', command='users')\n    root = ET.fromstring(result)\n    users = root.getchildren()\n    for user in users:\n        user_id = None\n        user_ret = {\n            \n        }\n        for item in user.items():\n            user_ret[item[0]] = item[1]\n            if (item[0] == 'id'):\n                user_id = item[1]\n        for item in user.getchildren():\n            user_ret[item.tag] = item.text\n        ret[user_ret[order_by]] = user_ret\n    return ret\n", "label": 0}
{"function": "\n\ndef set_display(self, locale_id=None, media_image=None, media_audio=None):\n    text = (Text(locale_id=locale_id) if locale_id else None)\n    if (media_image or media_audio):\n        self.display = Display(text=text, media_image=media_image, media_audio=media_audio)\n    elif text:\n        self.text = text\n", "label": 0}
{"function": "\n\ndef test_many_to_one(self, session, objects, calls):\n    users = session.query(models.User).all()\n    users[0].addresses\n    assert (len(calls) == 1)\n    call = calls[0]\n    assert (call.objects == (models.User, 'User:1', 'addresses'))\n    assert ('users[0].addresses' in ''.join(call.frame[4]))\n", "label": 0}
{"function": "\n\ndef downloadAnimea(self, manga, chapter_start, chapter_end, download_path, download_format):\n    for current_chapter in range(chapter_start, (chapter_end + 1)):\n        manga_chapter_prefix = ((manga.lower().replace('-', '_') + '_') + str(current_chapter).zfill(3))\n        if ((os.path.exists(((download_path + manga_chapter_prefix) + '.cbz')) or os.path.exists(((download_path + manga_chapter_prefix) + '.zip'))) and (overwrite_FLAG == False)):\n            print((('Chapter ' + str(current_chapter)) + ' already downloaded, skipping to next chapter...'))\n            continue\n        url = (((('http://manga.animea.net/' + manga) + '-chapter-') + str(current_chapter)) + '-page-1.html')\n        source = getSourceCode(url)\n        max_pages = int(re.compile('of (.*?)</title>').search(source).group(1))\n        for page in range(1, (max_pages + 1)):\n            url = (((((('http://manga.animea.net/' + manga) + '-chapter-') + str(current_chapter)) + '-page-') + str(page)) + '.html')\n            source = getSourceCode(url)\n            img_url = re.compile('img src=\"(http.*?.[jp][pn]g)\"').search(source).group(1)\n            print((((('Chapter ' + str(current_chapter)) + ' / ') + 'Page ') + str(page)))\n            print(img_url)\n            downloadImage(img_url, os.path.join('mangadl_tmp', ((manga_chapter_prefix + '_') + str(page).zfill(3))))\n        compress(manga_chapter_prefix, download_path, max_pages, download_format)\n", "label": 1}
{"function": "\n\ndef __init__(self, noop=False):\n    if noop:\n        self.func_name = 'posix_fallocate'\n        self.fallocate = noop_libc_function\n        return\n    for func in ('fallocate', 'posix_fallocate'):\n        self.func_name = func\n        self.fallocate = load_libc_function(func, log_error=False)\n        if (self.fallocate is not noop_libc_function):\n            break\n    if (self.fallocate is noop_libc_function):\n        logging.warning(_('Unable to locate fallocate, posix_fallocate in libc.  Leaving as a no-op.'))\n", "label": 0}
{"function": "\n\ndef setup_logger(logger, stream, filename=None, fmt=None):\n    \"Sets up a logger (if no handlers exist) for console output,\\n    and file 'tee' output if desired.\"\n    if (len(logger.handlers) < 1):\n        console = logging.StreamHandler(stream)\n        console.setLevel(logging.DEBUG)\n        console.setFormatter(logging.Formatter(fmt))\n        logger.addHandler(console)\n        logger.setLevel(logging.DEBUG)\n        if filename:\n            outfile = logging.FileHandler(filename)\n            outfile.setLevel(logging.INFO)\n            outfile.setFormatter(logging.Formatter(('%(asctime)s ' + (fmt if fmt else '%(message)s'))))\n            logger.addHandler(outfile)\n", "label": 0}
{"function": "\n\ndef get_value(self, select, table_name, where=None, extra=None):\n    '\\n        Get a value from the table.\\n\\n        :param str select: Attribute for SELECT query\\n        :param str table_name: Table name of executing the query.\\n        :return: Result of execution of the query.\\n        :raises simplesqlite.NullDatabaseConnectionError:\\n            |raises_check_connection|\\n        :raises sqlite3.OperationalError: |raises_operational_error|\\n\\n        .. seealso::\\n\\n            :py:meth:`.sqlquery.SqlQuery.make_select`\\n        '\n    self.verify_table_existence(table_name)\n    query = SqlQuery.make_select(select, table_name, where, extra)\n    result = self.execute_query(query, logging.getLogger().findCaller())\n    if (result is None):\n        return None\n    fetch = result.fetchone()\n    if (fetch is None):\n        return None\n    return fetch[0]\n", "label": 0}
{"function": "\n\n@register.tag\ndef shardtype(parser, token):\n    try:\n        (tag_name, shard_type) = token.split_contents()\n    except ValueError:\n        raise template.TemplateSyntaxError(('%r tag requires a single argument' % token.contents.split()[0]))\n    if (not ((shard_type[0] == shard_type[(- 1)]) and (shard_type[0] in ('\"', \"'\")))):\n        raise template.TemplateSyntaxError((\"%r tag's argument should be in quotes\" % tag_name))\n    shard_type = shard_type[1:(- 1)]\n    nodelist = parser.parse(('end{0}'.format(tag_name),))\n    parser.delete_first_token()\n    return EmailShardTypeNode(shard_type, nodelist)\n", "label": 0}
{"function": "\n\n@property\ndef photos(self):\n    if (not hasattr(self, '_photos')):\n        available_photos = glob.glob(os.path.join(self.directory, '*.jpg'))\n        self._photos = []\n        for photo_path in available_photos:\n            (_pdir, photo) = os.path.split(photo_path)\n            if (self.xml.find(photo) > 0):\n                self._photos.append(photo_path)\n    return self._photos\n", "label": 0}
{"function": "\n\ndef glob_staticfiles(self, item):\n    for finder in finders.get_finders():\n        if hasattr(finder, 'storages'):\n            storages = finder.storages.values()\n        elif hasattr(finder, 'storage'):\n            storages = [finder.storage]\n        else:\n            continue\n        for storage in storages:\n            globber = StorageGlobber(storage)\n            for file in globber.glob(item):\n                (yield storage.path(file))\n", "label": 1}
{"function": "\n\n@test.create_stubs({\n    api.nova: ('aggregate_get', 'add_host_to_aggregate', 'remove_host_from_aggregate', 'host_list'),\n})\ndef _test_manage_hosts_update(self, host, aggregate, form_data, addAggregate=False, cleanAggregates=False):\n    if cleanAggregates:\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host3').InAnyOrder()\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host2').InAnyOrder()\n        api.nova.remove_host_from_aggregate(IsA(http.HttpRequest), str(aggregate.id), 'host1').InAnyOrder()\n    api.nova.aggregate_get(IsA(http.HttpRequest), str(aggregate.id)).AndReturn(aggregate)\n    api.nova.host_list(IsA(http.HttpRequest)).AndReturn(self.hosts.list())\n    api.nova.aggregate_get(IsA(http.HttpRequest), str(aggregate.id)).AndReturn(aggregate)\n    if addAggregate:\n        api.nova.add_host_to_aggregate(IsA(http.HttpRequest), str(aggregate.id), host.host_name)\n    self.mox.ReplayAll()\n    res = self.client.post(reverse(constants.AGGREGATES_MANAGE_HOSTS_URL, args=[aggregate.id]), form_data)\n    self.assertNoFormErrors(res)\n    self.assertRedirectsNoFollow(res, reverse(constants.AGGREGATES_INDEX_URL))\n", "label": 0}
{"function": "\n\ndef _parse_changelog(self, pkg_name):\n    with open('ChangeLog.md') as f:\n        lineiter = iter(f)\n        for line in lineiter:\n            match = re.search(('^%s\\\\s+(.*)' % pkg_name), line.strip())\n            if (match is None):\n                continue\n            length = len(match.group(1))\n            version = match.group(1).strip()\n            if (lineiter.next().count('-') != len(match.group(0))):\n                continue\n            while 1:\n                change_info = lineiter.next().strip()\n                if change_info:\n                    break\n            match = re.search('released on (\\\\w+\\\\s+\\\\d+\\\\w+\\\\s+\\\\d+)', change_info)\n            if (match is None):\n                continue\n            datestr = match.group(1)\n            return (version, self._parse_date(datestr))\n", "label": 1}
{"function": "\n\ndef _run(self):\n    result = StatusCheckResult(check=self)\n    auth = None\n    if (self.username or self.password):\n        auth = (self.username, self.password)\n    try:\n        resp = requests.get(self.endpoint, timeout=self.timeout, verify=self.verify_ssl_certificate, auth=auth, headers={\n            'User-Agent': settings.HTTP_USER_AGENT,\n        })\n    except requests.RequestException as e:\n        result.error = ('Request error occurred: %s' % (e.message,))\n        result.succeeded = False\n    else:\n        if (self.status_code and (resp.status_code != int(self.status_code))):\n            result.error = ('Wrong code: got %s (expected %s)' % (resp.status_code, int(self.status_code)))\n            result.succeeded = False\n            result.raw_data = resp.content\n        elif self.text_match:\n            if (not re.search(self.text_match, resp.content)):\n                result.error = ('Failed to find match regex /%s/ in response body' % self.text_match)\n                result.raw_data = resp.content\n                result.succeeded = False\n            else:\n                result.succeeded = True\n        else:\n            result.succeeded = True\n    return result\n", "label": 1}
{"function": "\n\ndef _uncached_match(self, text, pos, cache, error):\n    new_pos = pos\n    children = []\n    while True:\n        node = self.members[0].match_core(text, new_pos, cache, error)\n        if ((node is None) or (not (node.end - node.start))):\n            return Node(self.name, text, pos, new_pos, children)\n        children.append(node)\n        new_pos += (node.end - node.start)\n", "label": 0}
{"function": "\n\n@authorization_required(is_admin=True)\n@threaded\ndef put(self, uid):\n    try:\n        user = Users.get(id=uid)\n    except DoesNotExist:\n        raise HTTPError(404)\n    try:\n        user.login = self.json.get('login', user.login)\n        user.email = self.json.get('email', user.email)\n        user.is_admin = bool(self.json.get('is_admin', user.is_admin))\n        user.password = self.json.get('password', user.password)\n        if (not all((isinstance(user.login, text_type), isinstance(user.email, text_type), (LOGIN_EXP.match(str(user.login)) is not None), (user.password and (len(user.password) > 3)), (EMAIL_EXP.match(str(user.email)) is not None)))):\n            raise HTTPError(400)\n    except:\n        raise HTTPError(400)\n    user.save()\n    self.response({\n        'id': user.id,\n        'login': user.login,\n        'email': user.email,\n        'is_admin': user.is_admin,\n    })\n", "label": 0}
{"function": "\n\n@pytest.fixture(scope='module')\ndef smtp_servers(request):\n    try:\n        from .local_smtp_severs import SERVERS\n    except ImportError:\n        from .smtp_servers import SERVERS\n    return dict([(k, SMTPTestParams(**v)) for (k, v) in SERVERS.items()])\n", "label": 0}
{"function": "\n\ndef autoassign(self, locals):\n    '\\n    Automatically assigns local variables to `self`.\\n    Generally used in `__init__` methods, as in::\\n\\n        def __init__(self, foo, bar, baz=1): \\n            autoassign(self, locals())\\n\\n    '\n    for (key, value) in locals.iteritems():\n        if (key == 'self'):\n            continue\n        setattr(self, key, value)\n", "label": 0}
{"function": "\n\ndef _version_str_to_list(version):\n    'convert a version string to a list of ints\\n\\n    non-int segments are excluded\\n    '\n    v = []\n    for part in version.split('.'):\n        try:\n            v.append(int(part))\n        except ValueError:\n            pass\n    return v\n", "label": 0}
{"function": "\n\ndef parse_args(self, sys_argv):\n    'Parsing, public for unit testing.  sys_argv should be sys.argv.\\nThe first entry is skipped, to account for sys.argv putting the\\nprogram name as the first element.\\n        '\n    if (len(sys_argv) == 0):\n        raise Driver.ParsingException('sys_argv must contain at least one element (name of the executing program, per sys.argv[0])')\n    parser = argparse.ArgumentParser(description='Migrate one or more databases (or default database, if one is assigned).')\n    parser.add_argument('-n', '--new', help='Create new (empty) database(s)', action='store_true')\n    parser.add_argument('-s', '--schema', help='Run baseline schema(e)', action='store_true')\n    parser.add_argument('-m', '--migrations', help='Run migrations', action='store_true')\n    parser.add_argument('-c', '--code', help='Run code (for views, stored procs, etc)', action='store_true')\n    parser.add_argument('-d', '--data', help='Load reference (bootstrap) data', action='store_true')\n    parser.add_argument('-u', '--update', help='Updates database (runs migrations, code, and data)', action='store_true')\n    parser.add_argument('databases', metavar='db', nargs='*', help='nickname of database to manipulate')\n    args = parser.parse_args(sys_argv[1:])\n    if ((len(args.databases) == 0) and (self.default_database != '')):\n        args.databases.append(self.default_database)\n    return args\n", "label": 0}
{"function": "\n\n@classmethod\ndef award_points(cls, user, addon, status, **kwargs):\n    'Awards points to user based on an event and the queue.\\n\\n        `event` is one of the `REVIEWED_` keys in constants.\\n        `status` is one of the `STATUS_` keys in constants.\\n\\n        '\n    event = cls.get_event(addon, status, **kwargs)\n    score = amo.REVIEWED_SCORES.get(event)\n    try:\n        vq = ViewQueue.objects.get(addon_slug=addon.slug)\n        if (vq.waiting_time_days > amo.REVIEWED_OVERDUE_LIMIT):\n            days_over = (vq.waiting_time_days - amo.REVIEWED_OVERDUE_LIMIT)\n            bonus = (days_over * amo.REVIEWED_OVERDUE_BONUS)\n            score = (score + bonus)\n    except ViewQueue.DoesNotExist:\n        pass\n    if score:\n        cls.objects.create(user=user, addon=addon, score=score, note_key=event)\n        cls.get_key(invalidate=True)\n        user_log.info(('Awarding %s points to user %s for \"%s\" for addon %s' % (score, user, amo.REVIEWED_CHOICES[event], addon.id)).encode('utf-8'))\n    return score\n", "label": 0}
{"function": "\n\ndef computeExpression(self, constraint_collection):\n    function = self.getFunction()\n    values = self.getArgumentValues()\n    cost = function.getCallCost(values)\n    if function.getFunctionRef().getFunctionBody().mayRaiseException(BaseException):\n        constraint_collection.onExceptionRaiseExit(BaseException)\n    if ((cost is not None) and (cost < 50)):\n        result = function.createOutlineFromCall(provider=self.getParentVariableProvider(), values=values)\n        return (result, 'new_statements', 'Function call in-lined.')\n    return (self, None, None)\n", "label": 0}
{"function": "\n\ndef __l2cap_advertise_service(self, name, service_id, service_classes, profiles, provider, description, protocols):\n    if (self._sdpservice is not None):\n        raise BluetoothError('Service already advertised')\n    if (not self.listening):\n        raise BluetoothError('Socket must be listening before advertised')\n    if protocols:\n        raise NotImplementedError('extra protocols not yet supported in Widcomm stack')\n    self._sdpservice = _widcomm._WCSdpService()\n    if service_classes:\n        service_classes = [to_full_uuid(s) for s in service_classes]\n        _sdp_checkraise(self._sdpservice.add_service_class_id_list(service_classes))\n    _sdp_checkraise(self._sdpservice.add_l2cap_protocol_descriptor(self.port))\n    if profiles:\n        for (uuid, version) in profiles:\n            uuid = to_full_uuid(uuid)\n            _sdp_checkraise(self._sdpservice.add_profile_descriptor_list(uuid, version))\n    _sdp_checkraise(self._sdpservice.add_service_name(name))\n    _sdp_checkraise(self._sdpservice.make_public_browseable())\n", "label": 1}
{"function": "\n\ndef test_DELETE(self):\n    res = self.app.delete('/crud.json?ref.id=1', expect_errors=False)\n    print('Received:', res.body)\n    result = json.loads(res.text)\n    print(result)\n    assert (result['data']['id'] == 1)\n    assert (result['data']['name'] == u('test'))\n    assert (result['message'] == 'delete')\n", "label": 0}
{"function": "\n\ndef listPropsAndMethods(self):\n    res = []\n    if (sublime.platform() == 'windows'):\n        app = 'node'\n        pathToJS = (sublime.packages_path() + '\\\\NPMInfo\\\\npm-info.js')\n        cmd = [app, pathToJS, self.pkgPath]\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, startupinfo=startupinfo)\n    else:\n        pathToJS = (sublime.packages_path() + '/NPMInfo/npm-info.js')\n        cmd = ['/usr/local/bin/node', pathToJS, self.pkgPath]\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    for line in iter(p.stdout.readline, b''):\n        lineStrip = line.rstrip()\n        if ((len(lineStrip) > 0) and (' ' not in lineStrip)):\n            res.append(lineStrip)\n    self.view.window().show_quick_panel(res, self.onSelectPropAndMethod)\n", "label": 0}
{"function": "\n\ndef skip_past(self, endtag):\n    while self.tokens:\n        token = self.next_token()\n        if ((token.token_type == TOKEN_BLOCK) and (token.contents == endtag)):\n            return\n    self.unclosed_block_tag([endtag])\n", "label": 0}
{"function": "\n\ndef connect_to_nsqd(self, conn):\n    if (not self.is_running):\n        return\n    if (conn in self.conns):\n        self.logger.debug(('[%s] already connected' % conn))\n        return\n    if (conn in self.pending):\n        self.logger.debug(('[%s] already pending' % conn))\n        return\n    self.logger.debug(('[%s] connecting...' % conn))\n    conn.on_response.connect(self.handle_response)\n    conn.on_error.connect(self.handle_error)\n    conn.on_finish.connect(self.handle_finish)\n    conn.on_requeue.connect(self.handle_requeue)\n    conn.on_auth.connect(self.handle_auth)\n    if self.max_concurrency:\n        conn.on_message.connect(self.queue_message)\n    else:\n        conn.on_message.connect(self.handle_message)\n    self.pending.add(conn)\n    try:\n        conn.connect()\n        conn.identify()\n        if (conn.max_ready_count < self.max_in_flight):\n            msg = ' '.join(['[%s] max RDY count %d < reader max in flight %d,', 'truncation possible'])\n            self.logger.warning((msg % (conn, conn.max_ready_count, self.max_in_flight)))\n        conn.subscribe(self.topic, self.channel)\n        self.send_ready(conn, 1)\n    except NSQException as error:\n        self.logger.warn(('[%s] connection failed (%r)' % (conn, error)))\n        self.handle_connection_failure(conn)\n        return\n    finally:\n        self.pending.remove(conn)\n    if (not self.is_running):\n        conn.close_stream()\n        return\n    self.logger.info(('[%s] connection successful' % conn))\n    self.handle_connection_success(conn)\n", "label": 1}
{"function": "\n\ndef find_by(self, finder, selector, original_find=None, original_query=None):\n    elements = None\n    end_time = (time.time() + self.wait_time)\n    func_name = getattr(getattr(finder, _meth_func), _func_name)\n    find_by = (original_find or func_name[(func_name.rfind('_by_') + 4):])\n    query = (original_query or selector)\n    while (time.time() < end_time):\n        try:\n            elements = finder(selector)\n            if (not isinstance(elements, list)):\n                elements = [elements]\n        except NoSuchElementException:\n            pass\n        if elements:\n            return ElementList([self.element_class(element, self) for element in elements], find_by=find_by, query=query)\n    return ElementList([], find_by=find_by, query=query)\n", "label": 1}
{"function": "\n\n@AccountRequired\n@PostOnly\ndef follow(request, user_id):\n    followed = get_object_or_404(User, pk=user_id)\n    (follow_instance, created) = UserToUserFollow.objects.get_or_create(follower=request.user, followed=followed)\n    if (not follow_instance.is_following):\n        follow_instance.is_following = True\n        follow_instance.save()\n    if created:\n        send_notification(type=EmailTypes.FOLLOW, user=followed, entity=request.user)\n    cache.bust(followed)\n    if request.is_ajax():\n        button = render_inclusiontag(request, 'follow_button followed', 'users_tags', {\n            'followed': followed,\n        })\n        return json_response({\n            'button': button,\n        })\n    else:\n        return redirect(followed)\n", "label": 0}
{"function": "\n\ndef get_version():\n    INIT = os.path.abspath(os.path.join(HERE, '../psutil/__init__.py'))\n    with open(INIT, 'r') as f:\n        for line in f:\n            if line.startswith('__version__'):\n                ret = eval(line.strip().split(' = ')[1])\n                assert (ret.count('.') == 2), ret\n                for num in ret.split('.'):\n                    assert num.isdigit(), ret\n                return ret\n        else:\n            raise ValueError(\"couldn't find version string\")\n", "label": 1}
{"function": "\n\ndef __unicode__(self):\n    qs = []\n    ctx_id = self.context_id\n    if self.is_assignment:\n        qs += ['\"{0}\" = %({1})s'.format(self.field, ctx_id)]\n    else:\n        for _ in (self._updates or []):\n            qs += ['\"{0}\"[%({1})s] = %({2})s'.format(self.field, ctx_id, (ctx_id + 1))]\n            ctx_id += 2\n    return ', '.join(qs)\n", "label": 0}
{"function": "\n\ndef visit_Assign(self, node):\n    is_name = (lambda n: isinstance(n, ast.Name))\n    for name in filter(is_name, node.targets):\n        if ((name.id == '__all__') and isinstance(node.value, ast.List)):\n            for subnode in node.value.elts:\n                if isinstance(subnode, ast.Str):\n                    self._tree.add_explicit_export(subnode.s, 1.2)\n        elif (not name.id.startswith('_')):\n            self._tree.add(name.id, 1.1)\n", "label": 1}
{"function": "\n\ndef valid(self, order=None):\n    '\\n        Can do complex validation about whether or not this option is valid.\\n        For example, may check to see if the recipient is in an allowed country\\n        or location.\\n        '\n    if order:\n        try:\n            for item in order.orderitem_set.all():\n                p = item.product\n                price = self.carrier.price(p)\n        except ProductShippingPriceException:\n            return False\n    elif self.cart:\n        try:\n            price = self.cost()\n        except ProductShippingPriceException:\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_compute_centroid_quantile(self, empty_tdigest, example_centroids):\n    empty_tdigest.C = example_centroids\n    empty_tdigest.n = 4\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 1.1)]) == (((1 / 2.0) + 0) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 0.5)]) == (((1 / 2.0) + 1) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[0.1]) == (((1 / 2.0) + 2) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[1.5]) == (((1 / 2.0) + 3) / 4))\n", "label": 0}
{"function": "\n\ndef cycle(self):\n    '\\n        Perform full thermostat cycle and return state.\\n        '\n    units = cherrypy.config['units']\n    retry_count = cherrypy.config['retry_count']\n    retry_delay = cherrypy.config['retry_delay']\n    envcontroller = cherrypy.config['envcontroller']\n    thermometer = cherrypy.config['thermometer']\n    thermostat = cherrypy.config['thermostat']\n    (current_heat, current_cool) = envcontroller.get_power_levels()\n    for i in range(retry_count):\n        try:\n            current_temp = thermometer.get_temperature(units=units)\n            break\n        except braubuddy.thermometer.ReadError as err:\n            cherrypy.log.error(err.message)\n            time.sleep(retry_delay)\n    else:\n        cherrypy.request.app.log.error('Unable to collect temperature after {0} tries'.format(retry_count))\n        return False\n    (required_heat, required_cool) = thermostat.get_required_state(current_temp, current_heat, current_cool, units=units)\n    envcontroller.set_heater_level(required_heat)\n    envcontroller.set_cooler_level(required_cool)\n    target = thermostat.target\n    for (name, output) in cherrypy.request.app.config['outputs'].iteritems():\n        try:\n            output.publish_status(target, current_temp, current_heat, current_cool)\n        except braubuddy.output.OutputError as err:\n            cherrypy.log.error(err.message)\n    return True\n", "label": 1}
{"function": "\n\ndef www_authenticate(realm, key, algorithm='MD5', nonce=None, qop=qop_auth, stale=False):\n    'Constructs a WWW-Authenticate header for Digest authentication.'\n    if (qop not in valid_qops):\n        raise ValueError((\"Unsupported value for qop: '%s'\" % qop))\n    if (algorithm not in valid_algorithms):\n        raise ValueError((\"Unsupported value for algorithm: '%s'\" % algorithm))\n    if (nonce is None):\n        nonce = synthesize_nonce(realm, key)\n    s = ('Digest realm=\"%s\", nonce=\"%s\", algorithm=\"%s\", qop=\"%s\"' % (realm, nonce, algorithm, qop))\n    if stale:\n        s += ', stale=\"true\"'\n    return s\n", "label": 0}
{"function": "\n\ndef delete_color(self, color, palette_type, palette_name):\n    'Delete color.'\n    if (palette_type == '__special__'):\n        if (palette_name == 'Favorites'):\n            favs = util.get_favs()['colors']\n            if (color in favs):\n                favs.remove(color)\n                util.save_palettes(favs, favs=True)\n                self.show_colors(palette_type, palette_name, delete=True, update=False)\n    elif (palette_type in ('__global__', '__project__')):\n        if (palette_type == '__global__'):\n            color_palettes = util.get_palettes()\n        else:\n            color_palettes = util.get_project_palettes(self.view.window())\n        for palette in color_palettes:\n            if (palette_name == palette['name']):\n                if (color in palette['colors']):\n                    palette['colors'].remove(color)\n                    if (palette_type == '__global__'):\n                        util.save_palettes(color_palettes)\n                    else:\n                        util.save_project_palettes(self.view.window(), color_palettes)\n                    self.show_colors(palette_type, palette_name, delete=True, update=False)\n                    break\n", "label": 1}
{"function": "\n\ndef downloadMovie(self, url, path, title, extension):\n    if (not os.path.exists(path)):\n        common.log('Path does not exist')\n        return None\n    if (title == ''):\n        common.log('No title given')\n        return None\n    file_path = xbmc.makeLegalFilename(os.path.join(path, (title + extension)))\n    file_path = urllib.unquote_plus(file_path)\n    if os.path.isfile(file_path):\n        self.pDialog = xbmcgui.Dialog()\n        if (not common.ask(('File already exists. Overwrite?\\n' + os.path.basename(file_path)))):\n            title = common.showOSK(urllib.unquote_plus(title), common.translate(30102))\n            if (not title):\n                return None\n            file_path = xbmc.makeLegalFilename(os.path.join(path, (title + extension)))\n            file_path = urllib.unquote_plus(file_path)\n    success = self.__download(url, file_path)\n    if success:\n        return file_path\n    else:\n        return None\n", "label": 1}
{"function": "\n\n@classmethod\ndef _is_socket(cls, stream):\n    'Check if the given stream is a socket.'\n    try:\n        fd = stream.fileno()\n    except ValueError:\n        return False\n    sock = socket.fromfd(fd, socket.AF_INET, socket.SOCK_RAW)\n    try:\n        sock.getsockopt(socket.SOL_SOCKET, socket.SO_TYPE)\n    except socket.error as ex:\n        if (ex.args[0] != errno.ENOTSOCK):\n            return True\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef get_user(self, identifier):\n    if self._is_numeric(identifier):\n        return self.user_model.query.get(identifier)\n    for attr in get_identity_attributes():\n        query = getattr(self.user_model, attr).ilike(identifier)\n        rv = self.user_model.query.filter(query).first()\n        if (rv is not None):\n            return rv\n", "label": 0}
{"function": "\n\ndef _get_external_player(self):\n    ' Determines external sound player to available.\\n\\n            Returns string or ``None``.\\n            '\n    if common.IS_MACOSX:\n        return 'afplay'\n    else:\n        for name in ('mpg123', 'play', 'aplay'):\n            if common.which(name):\n                return name\n    return None\n", "label": 0}
{"function": "\n\ndef _get_parent(self, path, ref, parent_type):\n    ' Return the path of the parent of type \"parent_type\" for the object\\n        in \"path\" with id \"ref\". Returns an empty string if no parent extists.\\n        '\n    parts = path.split('/')\n    if ((parent_type.lower() == 'block') or (parts[(- 4)] == (parent_type.lower() + 's'))):\n        return '/'.join(parts[:(- 2)])\n    object_folder = parts[(- 2)]\n    parent_folder = parts[(- 4)]\n    if (parent_folder in ('recordingchannels', 'units')):\n        block_path = '/'.join(parts[:(- 6)])\n    else:\n        block_path = '/'.join(parts[:(- 4)])\n    if (parent_type.lower() in ('recordingchannel', 'unit')):\n        path = (block_path + '/recordingchannelgroups')\n        for n in self._data.iterNodes(path):\n            if (not ('_type' in n._v_attrs)):\n                continue\n            p = self._search_parent(('%s/%ss' % (n._v_pathname, parent_type.lower())), object_folder, ref)\n            if (p != ''):\n                return p\n        return ''\n    if (parent_type.lower() == 'segment'):\n        path = (block_path + '/segments')\n    elif (parent_type.lower() in ('recordingchannelgroup', 'recordingchannelgroups')):\n        path = (block_path + '/recordingchannelgroups')\n    else:\n        return ''\n    return self._search_parent(path, object_folder, ref)\n", "label": 1}
{"function": "\n\n@app.route('/signin', methods=['GET', 'POST'])\ndef signin():\n    'Sign in'\n    form = SignInForm()\n    if form.validate_on_submit():\n        username = form.username.data.strip()\n        password = form.password.data.strip()\n        if is_valid_login(username, password):\n            session['theme'] = _get_users_theme(username)\n            session['logged_in'] = True\n            session['username'] = username\n            if form.remember_me.data:\n                session.permanent = True\n            else:\n                session.permanent = False\n            if (request.args.get('next') == 'signin'):\n                return redirect('/')\n            else:\n                return redirect((request.args.get('next') or request.referrer or '/'))\n        else:\n            return render_template('signin.html', form=form, error='Failed')\n    return render_template('signin.html', form=form, error=None)\n", "label": 1}
{"function": "\n\ndef checkRemainingArguments(self, arguments):\n    allowedAttributes = {\n        \n    }\n    allowedAttributes['allow multiple values'] = (bool, False, True, 'allowMultipleValues')\n    allowedAttributes['command line argument'] = (str, True, True, 'commandLineArgument')\n    allowedAttributes['construct filename'] = (dict, False, True, 'constructionInstructions')\n    allowedAttributes['data type'] = (str, True, True, 'dataType')\n    allowedAttributes['description'] = (str, True, True, 'description')\n    allowedAttributes['extensions'] = (list, False, True, 'extensions')\n    allowedAttributes['hide argument in help'] = (bool, False, True, 'hideInHelp')\n    allowedAttributes['include in reduced plot'] = (bool, False, True, 'includeInReducedPlot')\n    allowedAttributes['include value in quotations'] = (bool, False, True, 'includeInQuotations')\n    allowedAttributes['long form argument'] = (str, True, True, 'longFormArgument')\n    allowedAttributes['modify argument'] = (str, False, True, 'modifyArgument')\n    allowedAttributes['modify value'] = (str, False, True, 'modifyValue')\n    allowedAttributes['replace substring'] = (list, False, False, None)\n    allowedAttributes['required'] = (bool, False, True, 'isRequired')\n    allowedAttributes['short form argument'] = (str, False, True, 'shortFormArgument')\n    allowedAttributes['value command'] = (dict, False, True, 'valueCommand')\n    for category in arguments:\n        if ((category != 'Inputs') and (category != 'Outputs')):\n            self.checkArguments(category, arguments[category], allowedAttributes, isInput=False, isOutput=False)\n", "label": 0}
{"function": "\n\ndef format_text_table(table):\n    col_width = [max((len(str(x)) for x in col)) for col in zip(*table)]\n    output = []\n    for row in table:\n        inner = ' | '.join(('{0:{1}}'.format(x, col_width[i]) for (i, x) in enumerate(row)))\n        output.append('| {0} |'.format(inner))\n    return output\n", "label": 0}
{"function": "\n\ndef test_neg_list_trim_policy_is_string(self):\n    '\\n        Invoke list_trim() with policy is string\\n        '\n    key = ('test', 'demo', 1)\n    try:\n        self.as_connection.list_trim(key, 'contact_no', 0, 1, {\n            \n        }, '')\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'policy must be a dict')\n", "label": 0}
{"function": "\n\n@csrf_protect_m\n@filter_hook\ndef get(self, request, *args, **kwargs):\n    \"\\n        The 'change list' admin view for this model.\\n        \"\n    response = self.get_result_list()\n    if response:\n        return response\n    context = self.get_context()\n    context.update((kwargs or {\n        \n    }))\n    response = self.get_response(context, *args, **kwargs)\n    return (response or TemplateResponse(request, (self.object_list_template or self.get_template_list('views/model_list.html')), context))\n", "label": 0}
{"function": "\n\ndef __init__(self, database, name, is_admin=None, read_from=None, write_to=None):\n    self.__database = database\n    self.__client = database.client\n    self.__name = name\n    self.__is_admin = (is_admin or self.IS_ADMIN)\n    self.__read_from = (read_from or self.READ_FROM)\n    self.__write_to = (write_to or self.WRITE_TO)\n", "label": 0}
{"function": "\n\ndef _on_gstplayer_message(mtype, message):\n    if (mtype == 'error'):\n        Logger.error('VideoGstplayer: {}'.format(message))\n    elif (mtype == 'warning'):\n        Logger.warning('VideoGstplayer: {}'.format(message))\n    elif (mtype == 'info'):\n        Logger.info('VideoGstplayer: {}'.format(message))\n", "label": 0}
{"function": "\n\ndef get_mapping(self, cls, no_mapping_ok=False):\n    db = None\n    for candidate_cls in getmro(cls):\n        db = self.mapping.get(candidate_cls, None)\n        if (db is not None):\n            break\n    if (db is None):\n        db = self.default_database\n    if (db is None):\n        if no_mapping_ok:\n            return None\n        raise ValueError(('There is no database mapping for %s' % repr(cls)))\n    return db\n", "label": 1}
{"function": "\n\ndef _do_login(backend, user, social_user):\n    user.backend = '{0}.{1}'.format(backend.__module__, backend.__class__.__name__)\n    login(backend.strategy.request, user)\n    if backend.setting('SESSION_EXPIRATION', False):\n        expiration = social_user.expiration_datetime()\n        if expiration:\n            try:\n                backend.strategy.request.session.set_expiry((expiration.seconds + (expiration.days * 86400)))\n            except OverflowError:\n                backend.strategy.request.session.set_expiry(None)\n", "label": 0}
{"function": "\n\ndef __init__(self, game_list, game_path=None, slug=None, games_root=None, deploy_enable=False, manifest_name=None):\n    self.game_list = game_list\n    self.slug = None\n    self.title = None\n    self.path = None\n    self.cover_art = ImageDetail(self, 'cover_art.jpg')\n    self.title_logo = ImageDetail(self, 'title_logo.jpg')\n    self.modified = None\n    self.deployed = None\n    self.is_temporary = True\n    self.plugin_main = None\n    self.canvas_main = None\n    self.flash_main = None\n    self.mapping_table = None\n    self.deploy_files = None\n    self.has_mapping_table = None\n    self.engine_version = EngineDetail('')\n    self.is_multiplayer = False\n    self.aspect_ratio = AspectRatioDetail('')\n    if (manifest_name is None):\n        self.manifest_name = 'manifest.yaml'\n    else:\n        self.manifest_name = manifest_name\n    if (game_path is not None):\n        self.load(game_path, self.manifest_name)\n    elif (slug is not None):\n        self.update({\n            'slug': slug,\n        })\n    self.games_root = games_root\n    self.deploy_enable = deploy_enable\n", "label": 0}
{"function": "\n\ndef get_localhost_ip():\n    cmd = ['/sbin/ifconfig']\n    eth_ip = {\n        \n    }\n    try:\n        r = exec_command(cmd)\n    except:\n        current_app.logger.error('[Dial Helpers]: exec_command error: %s:%s', cmd, sys.exc_info()[1])\n        return False\n    if (r['return_code'] == 0):\n        r_data = r['stdout'].split('\\n')\n        for (index, line) in enumerate(r_data):\n            if line.startswith('inet addr:'):\n                eth_ip[r_data[(index - 1)].split()[0]] = line.split().split(':')[1]\n    else:\n        current_app.logger.error('[Dial Helpers]: exec_command return: %s:%s:%s', cmd, r['return_code'], r['stderr'])\n        return False\n    return eth_ip\n", "label": 0}
{"function": "\n\ndef windows_shell(chan):\n    import threading\n    stdout.write('*** Emulating terminal on Windows; press F6 or Ctrl+Z then enter to send EOF,\\r\\nor at the end of the execution.\\r\\n')\n    stdout.flush()\n    out_lock = threading.RLock()\n\n    def write(recv, std):\n        while True:\n            data = recv(256)\n            if (not data):\n                if std:\n                    with out_lock:\n                        stdout.write('\\r\\n*** EOF reached; (press F6 or ^Z then enter to end)\\r\\n')\n                        stdout.flush()\n                break\n            stream = [stderr_bytes, stdout_bytes][std]\n            with out_lock:\n                stream.write(data)\n                stream.flush()\n    threading.Thread(target=write, args=(chan.recv, True)).start()\n    threading.Thread(target=write, args=(chan.recv_stderr, False)).start()\n    try:\n        while True:\n            d = stdin_bytes.read(1)\n            if (not d):\n                chan.shutdown_write()\n                break\n            try:\n                chan.send(d)\n            except socket.error:\n                break\n    except EOFError:\n        pass\n", "label": 0}
{"function": "\n\ndef test_push_selects_groupby(self):\n    'Test pushing selections through groupby.'\n    lp = StoreTemp('OUTPUT', Select(expression.LTEQ(AttRef('c'), AttRef('a')), Select(expression.LTEQ(AttRef('b'), AttRef('c')), GroupBy([AttIndex(1), AttIndex(2), AttIndex(0)], [expression.COUNTALL()], Scan(self.x_key, self.x_scheme)))))\n    expected = collections.Counter([(b, c, a) for (a, b, c) in self.x_data if ((c <= a) and (b <= c))])\n    expected = collections.Counter(((k + (v,)) for (k, v) in expected.items()))\n    self.assertEquals(self.get_count(lp, Select), 2)\n    self.assertEquals(self.get_count(lp, Scan), 1)\n    self.assertIsInstance(lp.input, Select)\n    pp = self.logical_to_physical(lp)\n    self.assertIsInstance(pp.input, MyriaSplitConsumer)\n    self.assertIsInstance(pp.input.input.input, GroupBy)\n    self.assertEquals(self.get_count(pp, Select), 1)\n    self.db.evaluate(pp)\n    result = self.db.get_temp_table('OUTPUT')\n    self.assertEquals(result, expected)\n", "label": 0}
{"function": "\n\ndef _get_project_attribute_data(row, project_attribute_key_data):\n    name = project_attribute_key_data['name']\n    data_type = project_attribute_key_data['data_type']\n    project_attribute_data = {\n        'key': project_attribute_key_data['id'],\n    }\n    if (data_type == 'BOOLEAN'):\n        project_attribute_data['boolean_value'] = (row[name] == 'True')\n    elif (data_type == 'CHAR'):\n        project_attribute_data['char_value'] = row[name]\n    elif (data_type == 'DATE'):\n        project_attribute_data['date_value'] = row[name]\n    elif (data_type == 'DATETIME'):\n        dt = datetime.strptime(row[name], '%Y-%m-%dT%H:%M:%S%z')\n        project_attribute_data['datetime_value'] = row[name]\n    elif (data_type == 'FLOAT'):\n        project_attribute_data['float_value'] = float(row[name])\n    elif (data_type == 'INTEGER'):\n        project_attribute_data['integer_value'] = int(row[name])\n    else:\n        raise NotImplementedError\n    return project_attribute_data\n", "label": 1}
{"function": "\n\ndef get_tokens_unprocessed(self, text):\n    bashlexer = BashLexer(**self.options)\n    pos = 0\n    curcode = ''\n    insertions = []\n    for match in line_re.finditer(text):\n        line = match.group()\n        m = re.match('^((?:\\\\(\\\\S+\\\\))?(?:|sh\\\\S*?|\\\\w+\\\\S+[@:]\\\\S+(?:\\\\s+\\\\S+)?|\\\\[\\\\S+[@:][^\\\\n]+\\\\].+)[$#%])(.*\\\\n?)', line)\n        if m:\n            if (not insertions):\n                pos = match.start()\n            insertions.append((len(curcode), [(0, Generic.Prompt, m.group(1))]))\n            curcode += m.group(2)\n        elif line.startswith('>'):\n            insertions.append((len(curcode), [(0, Generic.Prompt, line[:1])]))\n            curcode += line[1:]\n        else:\n            if insertions:\n                toks = bashlexer.get_tokens_unprocessed(curcode)\n                for (i, t, v) in do_insertions(insertions, toks):\n                    (yield ((pos + i), t, v))\n            (yield (match.start(), Generic.Output, line))\n            insertions = []\n            curcode = ''\n    if insertions:\n        for (i, t, v) in do_insertions(insertions, bashlexer.get_tokens_unprocessed(curcode)):\n            (yield ((pos + i), t, v))\n", "label": 1}
{"function": "\n\ndef makeRadials(self, numRadials):\n    ' make geodesic radials from number of radials '\n    segmentAngle = (360.0 / float(numRadials))\n    segmentAngleList = []\n    a = 0.0\n    while (a < 360.0):\n        segmentAngleList.append(a)\n        a += segmentAngle\n    fields = {\n        'x': 'DOUBLE',\n        'y': 'DOUBLE',\n        'Bearing': 'DOUBLE',\n        'Range': 'DOUBLE',\n    }\n    tab = self._makeTempTable('radTable', fields)\n    cursor = arcpy.da.InsertCursor(tab, ['x', 'y', 'Bearing', 'Range'])\n    for i in self.center:\n        pt = i.firstPoint\n        for r in segmentAngleList:\n            cursor.insertRow([pt.X, pt.Y, r, self.ringMax])\n    del cursor\n    self.deleteme.append(tab)\n    outRadialFeatures = os.path.join('in_memory', 'outRadials')\n    arcpy.BearingDistanceToLine_management(tab, outRadialFeatures, 'x', 'y', 'Range', self.distanceUnits, 'Bearing', 'DEGREES', 'GEODESIC', '#', self.sr)\n    self.deleteme.append(outRadialFeatures)\n    self.radialFeatures = outRadialFeatures\n    return outRadialFeatures\n", "label": 0}
{"function": "\n\ndef indication(self, server, pdu):\n    if _debug:\n        UDPMultiplexer._debug('indication %r %r', server, pdu)\n    if (pdu.pduDestination.addrType == Address.localBroadcastAddr):\n        dest = self.addrBroadcastTuple\n        if _debug:\n            UDPMultiplexer._debug('    - requesting local broadcast: %r', dest)\n    elif (pdu.pduDestination.addrType == Address.localStationAddr):\n        dest = unpack_ip_addr(pdu.pduDestination.addrAddr)\n        if _debug:\n            UDPMultiplexer._debug('    - requesting local station: %r', dest)\n    else:\n        raise RuntimeError('invalid destination address type')\n    self.directPort.indication(PDU(pdu, destination=dest))\n", "label": 1}
{"function": "\n\ndef visit_Assign(self, node):\n    ' Visits an assignment node. '\n    assign = self._assign_factory.from_ast(self.klass, node)\n    if assign.source:\n        assign.is_trait = self.klass.is_trait(assign.source)\n    else:\n        assign.is_trait = False\n    for target in assign.targets:\n        self.klass.locals[target] = assign\n        self.klass._is_trait[target] = assign.is_trait\n        if assign.is_trait:\n            self.klass.traits[target] = assign\n        else:\n            self.klass.attributes[target] = assign\n    return\n", "label": 0}
{"function": "\n\ndef _choose(old_style, new_style):\n    family = distrib_family()\n    if (family == 'debian'):\n        distrib = distrib_id()\n        at_least_trusty = ((distrib == 'Ubuntu') and (V(distrib_release()) >= V('14.04')))\n        at_least_jessie = ((distrib == 'Debian') and (V(distrib_release()) >= V('8.0')))\n        if (at_least_trusty or at_least_jessie):\n            return new_style\n        else:\n            return old_style\n    else:\n        raise UnsupportedFamily(supported=['debian'])\n", "label": 1}
{"function": "\n\ndef startTag(self, namespace, name, attrs):\n    assert ((namespace is None) or isinstance(namespace, string_types)), type(namespace)\n    assert isinstance(name, string_types), type(name)\n    assert all(((((namespace is None) or isinstance(namespace, string_types)) and isinstance(name, string_types) and isinstance(value, string_types)) for ((namespace, name), value) in attrs.items()))\n    return {\n        'type': 'StartTag',\n        'name': text_type(name),\n        'namespace': to_text(namespace),\n        'data': dict((((to_text(namespace, False), to_text(name)), to_text(value, False)) for ((namespace, name), value) in attrs.items())),\n    }\n", "label": 0}
{"function": "\n\n@lower_cast(types.BaseTuple, types.BaseTuple)\ndef tuple_to_tuple(context, builder, fromty, toty, val):\n    if (isinstance(fromty, types.BaseNamedTuple) or isinstance(toty, types.BaseNamedTuple)):\n        raise NotImplementedError\n    if (len(fromty) != len(toty)):\n        raise NotImplementedError\n    olditems = cgutils.unpack_tuple(builder, val, len(fromty))\n    items = [context.cast(builder, v, f, t) for (v, f, t) in zip(olditems, fromty, toty)]\n    return context.make_tuple(builder, toty, items)\n", "label": 0}
{"function": "\n\ndef stderr_to_parser_error(parse_args, *args, **kwargs):\n    if (isinstance(sys.stderr, StdIOBuffer) or isinstance(sys.stdout, StdIOBuffer)):\n        return parse_args(*args, **kwargs)\n    old_stdout = sys.stdout\n    old_stderr = sys.stderr\n    sys.stdout = StdIOBuffer()\n    sys.stderr = StdIOBuffer()\n    try:\n        try:\n            result = parse_args(*args, **kwargs)\n            for key in list(vars(result)):\n                if (getattr(result, key) is sys.stdout):\n                    setattr(result, key, old_stdout)\n                if (getattr(result, key) is sys.stderr):\n                    setattr(result, key, old_stderr)\n            return result\n        except SystemExit:\n            code = sys.exc_info()[1].code\n            stdout = sys.stdout.getvalue()\n            stderr = sys.stderr.getvalue()\n            raise ArgumentParserError('SystemExit', stdout, stderr, code)\n    finally:\n        sys.stdout = old_stdout\n        sys.stderr = old_stderr\n", "label": 1}
{"function": "\n\ndef get(self, name, default=None):\n    '\\n            Get a Python-attribute of this item instance, falls back\\n            to the same attribute in the parent item if not set in\\n            this instance, used to inherit attributes to components\\n\\n            @param name: the attribute name\\n        '\n    if (name in self.__dict__):\n        value = self.__dict__[name]\n    else:\n        value = None\n    if (value is not None):\n        return value\n    if (name[:2] == '__'):\n        raise AttributeError\n    parent = self.parent\n    if (parent is not None):\n        return parent.get(name)\n    return default\n", "label": 0}
{"function": "\n\ndef request(self, action, params=None, data='', headers=None, method='GET'):\n    ' Add the X-NFSN-Authentication header to an HTTP request. '\n    if (not headers):\n        headers = {\n            \n        }\n    if (not params):\n        params = {\n            \n        }\n    header = self._header(action, data)\n    headers['X-NFSN-Authentication'] = header\n    if (method == 'POST'):\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    return ConnectionUserAndKey.request(self, action, params, data, headers, method)\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    try:\n        message = record.getMessage()\n        assert isinstance(message, basestring_type)\n        record.message = _safe_unicode(message)\n    except Exception as e:\n        record.message = ('Bad message (%r): %r' % (e, record.__dict__))\n    record.asctime = self.formatTime(record, self.datefmt)\n    if (record.levelno in self._colors):\n        record.color = self._colors[record.levelno]\n        record.end_color = self._normal\n    else:\n        record.color = record.end_color = ''\n    formatted = (self._fmt % record.__dict__)\n    if record.exc_info:\n        if (not record.exc_text):\n            record.exc_text = self.formatException(record.exc_info)\n    if record.exc_text:\n        lines = [formatted.rstrip()]\n        lines.extend((_safe_unicode(ln) for ln in record.exc_text.split('\\n')))\n        formatted = '\\n'.join(lines)\n    return formatted.replace('\\n', '\\n    ')\n", "label": 1}
{"function": "\n\ndef it_adds_func_names_to_all(self):\n    base = 'uber'\n    namespace = {\n        '__all__': [],\n    }\n    generate_generic_calls(base, namespace)\n    base_funcs = (m.split('.', 1)[1] for m in METHODS if m.startswith(base))\n    assert (sorted(namespace['__all__']) == list(sorted(base_funcs)))\n", "label": 0}
{"function": "\n\ndef next3Fixtures(self, type_return='string'):\n    now = datetime.datetime.now()\n    url = (('http://www.premierleague.com/en-gb/matchday/league-table.html?season=2015-2016&month=' + months[now.month]) + '&timelineView=date&toDate=1451433599999&tableView=NEXT_3_FIXTURES')\n    team_names = soup(template='.next3FixturesTable')\n    for i in range(len(team_names)):\n        team_names[i] = str(team_names[i].text)\n    next_3_fixtures = soup.select('.club-row .col-fixture')\n    for i in range(len(next_3_fixtures)):\n        next_3_fixtures[i] = str(next_3_fixtures[i].text)\n    return_dict = {\n        \n    }\n    for i in range(len(team_names)):\n        return_dict[team_names[i]] = next_3_fixtures[i]\n    if (type_return == 'dict'):\n        return return_dict\n    return str(return_dict)\n", "label": 0}
{"function": "\n\ndef decrypt(self, id, data):\n    if (data[0] != self.TYPE):\n        raise IntegrityError('Invalid encryption envelope')\n    data = zlib.decompress(memoryview(data)[1:])\n    if (id and (sha256(data).digest() != id)):\n        raise IntegrityError('Chunk id verification failed')\n    return data\n", "label": 0}
{"function": "\n\n@defer.inlineCallbacks\ndef test_SpecifiedFields(self):\n    (yield self.coll.insert([dict(((k, v) for k in 'abcdefg')) for v in range(5)], safe=True))\n    res = (yield self.coll.find(fields={\n        'a': 1,\n        'c': 1,\n    }))\n    (yield self.coll.count(fields={\n        'a': 1,\n        'c': 1,\n    }))\n    self.assertTrue(all(((x in ['a', 'c', '_id']) for x in res[0].keys())))\n    res = (yield self.coll.find(fields=['a', 'c']))\n    (yield self.coll.count(fields=['a', 'c']))\n    self.assertTrue(all(((x in ['a', 'c', '_id']) for x in res[0].keys())))\n    res = (yield self.coll.find(fields=[]))\n    (yield self.coll.count(fields=[]))\n    self.assertTrue(all(((x in ['_id']) for x in res[0].keys())))\n    (yield self.assertFailure(self.coll.find({\n        \n    }, fields=[1]), TypeError))\n", "label": 1}
{"function": "\n\ndef handle_noargs(self, *args, **options):\n    r = get_r()\n    try:\n        keys = r.r.smembers(r._metric_slugs_key)\n        slugs = set((s.split(':')[1] for s in keys))\n        r.r.srem(r._metric_slugs_key, *keys)\n        r.r.sadd(r._metric_slugs_key, *slugs)\n        p = '\\nMetrics: Converted {0} Keys to {1} Slugs\\n'\n        self.stdout.write(p.format(len(keys), len(slugs)))\n    except (IndexError, ResponseError):\n        pass\n    try:\n        keys = r.r.smembers(r._gauge_slugs_key)\n        slugs = set((s.split(':')[1] for s in keys))\n        r.r.srem(r._gauge_slugs_key, *keys)\n        r.r.sadd(r._gauge_slugs_key, *slugs)\n        p = 'Gauges: Converted {0} Keys to {1} Slugs\\n'\n        self.stdout.write(p.format(len(keys), len(slugs)))\n    except (IndexError, ResponseError):\n        pass\n    i = 0\n    categories = r.categories()\n    for category in categories:\n        try:\n            k = r._category_key(category)\n            data = r.r.get(k)\n            if data:\n                data = json.loads(data)\n                r.r.delete(k)\n                r.r.sadd(k, *set(data))\n                i += 1\n        except ResponseError:\n            pass\n    if (i > 0):\n        p = 'Converted {0} Categories from JSON -> Redis Sets\\n'\n        self.stdout.write(p.format(i))\n", "label": 1}
{"function": "\n\ndef __init__(self, env, name=None, version=None):\n    'Create a new page object or retrieves an existing page.\\n\\n        :param env: an `Environment` object.\\n        :param name: the page name or a `Resource` object.\\n        :param version: the page version. The value takes precedence over the\\n                        `Resource` version when both are specified.\\n        '\n    self.env = env\n    if version:\n        try:\n            version = int(version)\n        except ValueError:\n            version = None\n    if isinstance(name, Resource):\n        resource = name\n        name = resource.id\n        if ((version is None) and (resource.version is not None)):\n            try:\n                version = int(resource.version)\n            except ValueError:\n                version = None\n    self.name = name\n    self._resource_version = version\n    if name:\n        self._fetch(name, version)\n    else:\n        self.version = 0\n        self.text = self.comment = self.author = ''\n        self.time = None\n        self.readonly = 0\n    self.old_text = self.text\n    self.old_readonly = self.readonly\n", "label": 1}
{"function": "\n\ndef __call__(self, value):\n    if (value is None):\n        return None\n    try:\n        p = value.split(':', 2)\n        _60 = Duration._60\n        _unsigned = Duration._unsigned\n        if (len(p) == 1):\n            result = _unsigned(p[0])\n        if (len(p) == 2):\n            result = ((60 * _unsigned(p[0])) + _60(p[1]))\n        if (len(p) == 3):\n            result = (((3600 * _unsigned(p[0])) + (60 * _60(p[1]))) + _60(p[2]))\n    except ValueError:\n        raise ValueError('Invalid duration value: %s', value)\n    return result\n", "label": 1}
{"function": "\n\ndef returner(ret):\n    '\\n    Return data to a mongodb server\\n    '\n    (conn, mdb) = _get_conn(ret)\n    col = mdb[ret['id']]\n    if isinstance(ret['return'], dict):\n        back = _remove_dots(ret['return'])\n    else:\n        back = ret['return']\n    if isinstance(ret, dict):\n        full_ret = _remove_dots(ret)\n    else:\n        full_ret = ret\n    log.debug(back)\n    sdata = {\n        'minion': ret['id'],\n        'jid': ret['jid'],\n        'return': back,\n        'fun': ret['fun'],\n        'full_ret': full_ret,\n    }\n    if ('out' in ret):\n        sdata['out'] = ret['out']\n    if (float(version) > 2.3):\n        mdb.saltReturns.insert_one(sdata.copy())\n    else:\n        mdb.saltReturns.insert(sdata.copy())\n", "label": 0}
{"function": "\n\n@classmethod\ndef _update(cls, args):\n    api._timeout = args.timeout\n    format = args.format\n    options = None\n    if (args.options is not None):\n        try:\n            options = json.loads(args.options)\n        except:\n            raise Exception('bad json parameter')\n    res = api.Monitor.update(args.monitor_id, type=args.type, query=args.query, name=args.name, message=args.message, options=options)\n    report_warnings(res)\n    report_errors(res)\n    if (format == 'pretty'):\n        print(pretty_json(res))\n    else:\n        print(json.dumps(res))\n", "label": 0}
{"function": "\n\ndef _get_position_ref_node(self, instance):\n    if self.is_sorted:\n        position = 'sorted-child'\n        node_parent = instance.get_parent()\n        if node_parent:\n            ref_node_id = node_parent.pk\n        else:\n            ref_node_id = ''\n    else:\n        prev_sibling = instance.get_prev_sibling()\n        if prev_sibling:\n            position = 'right'\n            ref_node_id = prev_sibling.pk\n        else:\n            position = 'first-child'\n            if instance.is_root():\n                ref_node_id = ''\n            else:\n                ref_node_id = instance.get_parent().pk\n    return {\n        '_ref_node_id': ref_node_id,\n        '_position': position,\n    }\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _parse_rule_sections(parser, config):\n    sections = [section for section in parser.sections() if (section != 'general')]\n    for rule_name in sections:\n        for (option_name, option_value) in parser.items(rule_name):\n            config.set_rule_option(rule_name, option_name, option_value)\n", "label": 0}
{"function": "\n\ndef on_input_timeout(self, cli):\n    if (not self.show_help):\n        return\n    document = cli.current_buffer.document\n    text = document.text\n    LOG.debug('document.text = %s', text)\n    LOG.debug('current_command = %s', self.completer.current_command)\n    if text.strip():\n        command = self.completer.current_command\n        key_name = '.'.join(command.split()).encode('utf-8')\n        last_option = self.completer.last_option\n        if last_option:\n            self.current_docs = self._docs.extract_param(key_name, last_option)\n        else:\n            self.current_docs = self._docs.extract_description(key_name)\n    else:\n        self.current_docs = ''\n    cli.buffers['clidocs'].reset(initial_document=Document(self.current_docs, cursor_position=0))\n    cli.request_redraw()\n", "label": 0}
{"function": "\n\ndef find_by_key(self, key=None, parent='/', **kwargs):\n    \"\\n        Returns a list of DirEntry for each directory entry that matches the given key name.\\n        If a parent is provided, only checks in this parent and all subtree.\\n        These entries are in the same org's directory but have different parents.\\n        \"\n    if (key is None):\n        raise BadRequest('Illegal arguments')\n    if (parent is None):\n        raise BadRequest('Illegal arguments')\n    start_key = [self.orgname, key, parent]\n    end_key = [self.orgname, key, (parent + 'ZZZZZZ')]\n    res = self.dir_store.find_by_view('directory', 'by_key', start_key=start_key, end_key=end_key, id_only=True, convert_doc=True, **kwargs)\n    match = [value for (docid, indexkey, value) in res]\n    return match\n", "label": 0}
{"function": "\n\ndef terminal(self, ret):\n    if (ret is not None):\n        assert os.path.isabs(ret), ret\n        assert os.path.exists(ret), ret\n", "label": 0}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_content():\n        self.set_content(x.content())\n    if x.has_statuscode():\n        self.set_statuscode(x.statuscode())\n    for i in xrange(x.header_size()):\n        self.add_header().CopyFrom(x.header(i))\n    if x.has_contentwastruncated():\n        self.set_contentwastruncated(x.contentwastruncated())\n    if x.has_externalbytessent():\n        self.set_externalbytessent(x.externalbytessent())\n    if x.has_externalbytesreceived():\n        self.set_externalbytesreceived(x.externalbytesreceived())\n    if x.has_finalurl():\n        self.set_finalurl(x.finalurl())\n", "label": 1}
{"function": "\n\ndef get(self, key_path, default=None):\n    try:\n        value = self._data\n        for k in key_path.split('.'):\n            value = value[k]\n        return value\n    except KeyError:\n        if (default is not None):\n            return default\n        else:\n            raise ConfigKeyError(key_path)\n", "label": 0}
{"function": "\n\ndef _parse_request_line(self, line):\n    request_line = _read_request_line_dict(line)\n    if (not request_line):\n        return\n    uri = urlparse(request_line['uri'])\n    if uri.scheme:\n        self.request.protocol = uri.scheme\n    if uri.netloc:\n        if (':' in uri.netloc):\n            (self.request.host, self.request.port) = uri.netloc.split(':')\n        else:\n            self.request.host = uri.netloc\n    if uri.port:\n        self.request.port = uri.port\n    if uri.path:\n        self.request.path = uri.path\n    if uri.query:\n        query = parse_qs(uri.query)\n        for key in query:\n            self.request.query[key] = query[key]\n    if ('method' in request_line):\n        self.request.method = request_line['method']\n", "label": 1}
{"function": "\n\ndef snipt_page(request, user, snipt_id):\n    try:\n        snipt = Snippet.objects.get(slug=snipt_id)\n        if ('c' in request.GET):\n            return HttpResponseRedirect(((snipt.get_absolute_url() + '#comment-') + request.GET['c']))\n    except:\n        return HttpResponseRedirect(((('/' + user) + '/tag/') + snipt_id))\n    context_user = User.objects.get(id=snipt.user.id)\n    if (request.user.id == context_user.id):\n        mine = True\n    else:\n        mine = False\n    if ((not snipt.public) and (not mine)):\n        try:\n            if (request.GET['key'] != snipt.key):\n                raise Http404()\n            else:\n                key = True\n        except:\n            raise Http404()\n    disable_wrap = request.session.get('disable_wrap')\n    disable_message = request.session.get('disable_message')\n    return render_to_response('snipt.html', locals(), context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef test_field_checks(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.CharField(), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E004')\n    assert ('Base field for list has errors' in errors[0].msg)\n    assert ('max_length' in errors[0].msg)\n", "label": 0}
{"function": "\n\ndef load_certificate_request(type, buffer):\n    '\\n    Load a certificate request from a buffer\\n\\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\\n    :param buffer: The buffer the certificate request is stored in\\n    :return: The X509Req object\\n    '\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if (type == FILETYPE_PEM):\n        req = _lib.PEM_read_bio_X509_REQ(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif (type == FILETYPE_ASN1):\n        req = _lib.d2i_X509_REQ_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if (req == _ffi.NULL):\n        _raise_current_error()\n    x509req = X509Req.__new__(X509Req)\n    x509req._req = _ffi.gc(req, _lib.X509_REQ_free)\n    return x509req\n", "label": 0}
{"function": "\n\ndef test_export_xls():\n    headers = ['x', 'y']\n    data = [['1', '2'], ['3', '4'], ['5,6', '7'], [None, None]]\n    sheet = ([headers] + data)\n    generator = create_generator(content_generator(headers, data), 'xls')\n    response = make_response(generator, 'xls', 'foo')\n    assert_equal('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', response['content-type'])\n    expected_data = [[(((cell is not None) and cell) or 'NULL') for cell in row] for row in sheet]\n    sheet_data = _read_xls_sheet_data(response)\n    assert_equal(expected_data, sheet_data)\n    assert_equal('attachment; filename=foo.xlsx', response['content-disposition'])\n", "label": 0}
{"function": "\n\ndef createVM(request_id, hostname, recipe, updateProgress):\n    configParser = configparser.RawConfigParser()\n    configFilePath = '/opt/chef-tools/createvm/createvm.config'\n    configParser.read(configFilePath)\n    subdomain = configParser.get('cocreate_config', 'subdomain')\n    progress = 0\n    validHostname = checkHostname(hostname)\n    if (validHostname == False):\n        return (None, None, 'Invalid hostname', progress)\n    fqdn = ((hostname + '.') + subdomain)\n    validRecipe = checkRecipe(configParser, recipe)\n    if (validRecipe == False):\n        return (None, None, 'Unsupported template', progress)\n    updateProgress(request_id, progress, 'Beginning VM template cloning')\n    try:\n        cloneVM(configParser, hostname)\n    except subprocess.CalledProcessError:\n        print('A cloning error occurred')\n        return (None, None, 'VM cloning failed', progress)\n    progress = 33\n    updateProgress(request_id, progress, 'Waiting for new VM IP address')\n    ipAddress = None\n    try:\n        ipAddress = getIP(configParser, hostname)\n    except:\n        print('Could not get IP Address')\n        return (None, None, 'Could not obtain VM IP address', progress)\n    progress = 67\n    updateProgress(request_id, progress, 'Beginning VM bootstrap')\n    try:\n        bootstrapVM(configParser, ipAddress, hostname, recipe)\n    except subprocess.CalledProcessError:\n        print('An error occurred during bootstrap')\n        return (None, None, 'VM bootstrap failed', progress)\n    url = ((('http://' + fqdn) + '/') + recipe)\n    progress = 100\n    updateProgress(request_id, progress, 'VM creation complete', url)\n    return (ipAddress, fqdn, None, progress)\n", "label": 1}
{"function": "\n\ndef parse_novarc(filename):\n    opts = {\n        \n    }\n    f = open(filename, 'r')\n    for line in f:\n        try:\n            line = line.replace('export', '').strip()\n            parts = line.split('=')\n            if (len(parts) > 1):\n                value = parts[1].replace(\"'\", '')\n                value = value.replace('\"', '')\n                opts[parts[0]] = value\n        except:\n            pass\n    f.close()\n    return opts\n", "label": 0}
{"function": "\n\ndef __init__(self, hits=None, truncated=False):\n    if hits:\n        self._filenames = [x[0] for x in hits]\n        self._ranks = [x[1] for x in hits]\n    else:\n        self._filenames = []\n        self._ranks = []\n    self.truncated = truncated\n    self.debug_info = []\n", "label": 0}
{"function": "\n\ndef opt_rulers_parser(value):\n    try:\n        converted = json.loads(value)\n        if isinstance(converted, list):\n            return converted\n        else:\n            raise ValueError\n    except ValueError:\n        raise\n    except TypeError:\n        raise ValueError\n", "label": 0}
{"function": "\n\ndef terminate(self):\n    ' Method should be called to terminate the client before the reactor\\n            is stopped.\\n\\n            @return:            Deferred which fires as soon as the client is\\n                                ready to stop the reactor.\\n            @rtype:             twisted.internet.defer.Deferred\\n        '\n    for call in self._deathCandidates.itervalues():\n        call.cancel()\n    self._deathCandidates = {\n        \n    }\n    for connection in self._connections.copy():\n        connection.destroy()\n    assert (len(self._connections) == 0)\n    Endpoint.terminate(self)\n", "label": 0}
{"function": "\n\ndef main():\n    import sys\n    import json\n    import os\n    import boto3.session\n    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')\n    if (not os.path.isdir(data_dir)):\n        os.makedirs(data_dir)\n    session = boto3.session.Session()\n    loader = session._loader\n    builder = ResourceIndexBuilder()\n    for resource_name in session.get_available_resources():\n        api_version = loader.determine_latest_version(resource_name, 'resources-1')\n        model = loader.load_service_model(resource_name, 'resources-1', api_version)\n        index = builder.build_index(model)\n        output_file = os.path.join(data_dir, resource_name, api_version, 'completions-1.json')\n        if (not os.path.isdir(os.path.dirname(output_file))):\n            os.makedirs(os.path.dirname(output_file))\n        with open(output_file, 'w') as f:\n            f.write(json.dumps(index, indent=2))\n", "label": 0}
{"function": "\n\ndef run(self, image, command=None, create_kwargs=None, start_kwargs=None):\n    '\\n        create container from provided image and start it\\n\\n        for more info, see documentation of REST API calls:\\n         * containers/{}/start\\n         * container/create\\n\\n        :param image: ImageName or string, name or id of the image\\n        :param command: str\\n        :param create_kwargs: dict, kwargs for docker.create_container\\n        :param start_kwargs: dict, kwargs for docker.start\\n        :return: str, container id\\n        '\n    logger.info(\"creating container from image '%s' and running it\", image)\n    create_kwargs = (create_kwargs or {\n        \n    })\n    start_kwargs = (start_kwargs or {\n        \n    })\n    logger.debug(\"image = '%s', command = '%s', create_kwargs = '%s', start_kwargs = '%s'\", image, command, create_kwargs, start_kwargs)\n    if isinstance(image, ImageName):\n        image = image.to_str()\n    container_dict = self.d.create_container(image, command=command, **create_kwargs)\n    container_id = container_dict['Id']\n    logger.debug(\"container_id = '%s'\", container_id)\n    self.d.start(container_id, **start_kwargs)\n    return container_id\n", "label": 0}
{"function": "\n\ndef test_getset_metadata(self):\n    m = meta.Metadata()\n    md = m.get_metadata('files/one')\n    assert (md == 'r--------')\n    d = self.tmpdir()\n    p = os.path.join(d, 'test')\n    utils.touch(p)\n    assert (m.get_metadata(p) != md)\n    m.set_metadata(p, md)\n    assert (m.get_metadata(p) == md)\n", "label": 0}
{"function": "\n\ndef _walknode(self, node):\n    if (node.nodeType == Node.ELEMENT_NODE):\n        self.startElementNS(node.qname, node.tagName, node.attributes)\n        for c in node.childNodes:\n            self._walknode(c)\n        self.endElementNS(node.qname, node.tagName)\n    if ((node.nodeType == Node.TEXT_NODE) or (node.nodeType == Node.CDATA_SECTION_NODE)):\n        self.characters(str(node))\n", "label": 0}
{"function": "\n\ndef _extract_error_code(self, exception):\n    match = re.compile('^(\\\\d+)L?:|^\\\\((\\\\d+)L?,').match(str(exception))\n    code = ((match.group(1) or match.group(2)) if match else None)\n    if code:\n        return int(code)\n", "label": 0}
{"function": "\n\ndef wait_till_stopped(self, conf, container_id, timeout=10, message=None, waiting=True):\n    'Wait till a container is stopped'\n    stopped = False\n    inspection = None\n    for _ in until(timeout=timeout, action=message):\n        try:\n            inspection = conf.harpoon.docker_context.inspect_container(container_id)\n            if (not isinstance(inspection, dict)):\n                log.error('Weird response from inspecting the container\\tresponse=%s', inspection)\n            elif (not inspection['State']['Running']):\n                stopped = True\n                conf.container_id = None\n                break\n            else:\n                break\n        except (socket.timeout, ValueError):\n            log.warning('Failed to inspect the container\\tcontainer_id=%s', container_id)\n        except DockerAPIError as error:\n            if (error.response.status_code != 404):\n                raise\n            else:\n                break\n    if (not inspection):\n        log.warning('Failed to inspect the container!')\n        stopped = True\n        exit_code = 1\n    else:\n        exit_code = inspection['State']['ExitCode']\n    return (stopped, exit_code)\n", "label": 1}
{"function": "\n\ndef post(self, *args, **kwargs):\n    if (self.request.POST.get('action') == 'invite'):\n        if (not self.domain_object.sms_mobile_worker_registration_enabled):\n            return self.get(*args, **kwargs)\n        if self.invitations_form.is_valid():\n            phone_numbers = self.invitations_form.cleaned_data.get('phone_numbers')\n            app_id = self.invitations_form.cleaned_data.get('app_id')\n            result = SelfRegistrationInvitation.initiate_workflow(self.domain, phone_numbers, app_id=app_id)\n            (success_numbers, invalid_format_numbers, numbers_in_use) = result\n            if success_numbers:\n                messages.success(self.request, (_('Invitations sent to: %(phone_numbers)s') % {\n                    'phone_numbers': ','.join(success_numbers),\n                }))\n            if invalid_format_numbers:\n                messages.error(self.request, (_('Invitations could not be sent to: %(phone_numbers)s. These number(s) are in an invalid format.') % {\n                    'phone_numbers': ','.join(invalid_format_numbers),\n                }))\n            if numbers_in_use:\n                messages.error(self.request, (_('Invitations could not be sent to: %(phone_numbers)s. These number(s) are already in use.') % {\n                    'phone_numbers': ','.join(numbers_in_use),\n                }))\n        return self.get(*args, **kwargs)\n    else:\n        if (not self.domain_object.sms_mobile_worker_registration_enabled):\n            raise Http404()\n        return self.paginate_crud_response\n", "label": 1}
{"function": "\n\ndef discover_affected_files(include_test_sources, include_scripts, project):\n    source_dir = project.get_property('dir_source_main_python')\n    files = discover_python_files(source_dir)\n    if include_test_sources:\n        if project.get_property('dir_source_unittest_python'):\n            unittest_dir = project.get_property('dir_source_unittest_python')\n            files = itertools.chain(files, discover_python_files(unittest_dir))\n        if project.get_property('dir_source_integrationtest_python'):\n            integrationtest_dir = project.get_property('dir_source_integrationtest_python')\n            files = itertools.chain(files, discover_python_files(integrationtest_dir))\n    if (include_scripts and project.get_property('dir_source_main_scripts')):\n        scripts_dir = project.get_property('dir_source_main_scripts')\n        files = itertools.chain(files, discover_files_matching(scripts_dir, '*'))\n    return files\n", "label": 1}
{"function": "\n\ndef _setMod(self, modname, mod):\n    haschanged = self._modules[modname].changed()\n    if haschanged:\n        if (type(mod) == File):\n            mod.update()\n        self._modules[modname].setValue(mod.getValue())\n        self._modules[modname].updateFingerprint()\n        self.untrust(modname)\n        dependents = self._getOutNodesRecursive(modname)\n        if (dependents != []):\n            dbgstr(('These nodes are dependent: ' + str(dependents)), 0)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, index):\n    record = self.resultset.__getitem__(index)\n    array = []\n    for field in self.field_names:\n        value = record\n        for key in field.split('.'):\n            if (key in value):\n                value = value[key]\n            else:\n                break\n        array.append(value)\n    return tuple(array)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args):\n    if (len(args) == 1):\n        (start, stop, step) = (0, args[0], 1)\n    elif (len(args) == 2):\n        (start, stop, step) = (args[0], args[1], 1)\n    elif (len(args) == 3):\n        (start, stop, step) = args\n    else:\n        raise TypeError('range() requires 1-3 int arguments')\n    try:\n        (start, stop, step) = (int(start), int(stop), int(step))\n    except ValueError:\n        raise TypeError('an integer is required')\n    if (step == 0):\n        raise ValueError('range() arg 3 must not be zero')\n    elif (step < 0):\n        stop = min(stop, start)\n    else:\n        stop = max(stop, start)\n    self._start = start\n    self._stop = stop\n    self._step = step\n    self._len = (((stop - start) // step) + bool(((stop - start) % step)))\n", "label": 1}
{"function": "\n\ndef set_editor_memento(self, memento):\n    (structure, editor_references) = memento\n    if (len(structure.contents) > 0):\n        handler = EditorSetStructureHandler(self, editor_references)\n        self._wx_editor_dock_window.set_structure(structure, handler)\n        for editor in self.window.editors:\n            control = self._wx_editor_dock_window.get_control(editor.id)\n            if (control is not None):\n                self._wx_initialize_editor_dock_control(editor, control)\n    return\n", "label": 0}
{"function": "\n\ndef doCapNew(self, msg):\n    if (len(msg.args) != 3):\n        log.warning('Bad CAP NEW from server: %r', msg)\n        return\n    caps = msg.args[2].split()\n    assert caps, 'Empty list of capabilities'\n    self._addCapabilities(msg.args[2])\n    if ((not self.sasl_authenticated) and ('sasl' in self.state.capabilities_ls)):\n        self.resetSasl()\n        s = self.state.capabilities_ls['sasl']\n        if (s is not None):\n            self.filterSaslMechanisms(set(s.split(',')))\n    common_supported_unrequested_capabilities = (set(self.state.capabilities_ls) & (self.REQUEST_CAPABILITIES - self.state.capabilities_ack))\n    if common_supported_unrequested_capabilities:\n        caps = ' '.join(sorted(common_supported_unrequested_capabilities))\n        self.sendMsg(ircmsgs.IrcMsg(command='CAP', args=('REQ', caps)))\n", "label": 1}
{"function": "\n\ndef _remove_client_present(self, client):\n    id_ = client.id_\n    if ((id_ is None) or (id_ not in self.present)):\n        return\n    clients = self.present[id_]\n    if (client not in clients):\n        return\n    clients.remove(client)\n    if (len(clients) == 0):\n        del self.present[id_]\n        if self.presence_events:\n            data = {\n                'new': [],\n                'lost': [id_],\n            }\n            self.event.fire_event(data, salt.utils.event.tagify('change', 'presence'))\n            data = {\n                'present': list(self.present.keys()),\n            }\n            self.event.fire_event(data, salt.utils.event.tagify('present', 'presence'))\n", "label": 1}
{"function": "\n\ndef mean_quadratic_weighted_kappa(kappas, weights=None):\n    \"\\n    Calculates the mean of the quadratic\\n    weighted kappas after applying Fisher's r-to-z transform, which is\\n    approximately a variance-stabilizing transformation.  This\\n    transformation is undefined if one of the kappas is 1.0, so all kappa\\n    values are capped in the range (-0.999, 0.999).  The reverse\\n    transformation is then applied before returning the result.\\n\\n    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\\n    kappa values\\n\\n    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\\n    of weights that is the same size as kappas.  Weights are applied in the\\n    z-space\\n    \"\n    kappas = np.array(kappas, dtype=float)\n    if (weights is None):\n        weights = np.ones(np.shape(kappas))\n    else:\n        weights = (weights / np.mean(weights))\n    kappas = np.array([min(x, 0.999) for x in kappas])\n    kappas = np.array([max(x, (- 0.999)) for x in kappas])\n    z = ((0.5 * np.log(((1 + kappas) / (1 - kappas)))) * weights)\n    z = np.mean(z)\n    return ((np.exp((2 * z)) - 1) / (np.exp((2 * z)) + 1))\n", "label": 0}
{"function": "\n\ndef init(allocator=drv.mem_alloc):\n    '\\n    Initialize libraries used by scikit-cuda.\\n\\n    Initialize the CUBLAS, CUSOLVER, and CULA libraries used by \\n    high-level functions provided by scikit-cuda.\\n\\n    Parameters\\n    ----------\\n    allocator : an allocator used internally by some of the high-level\\n        functions.\\n\\n    Notes\\n    -----\\n    This function does not initialize PyCUDA; it uses whatever device\\n    and context were initialized in the current host thread.\\n    '\n    global _global_cublas_handle, _global_cublas_allocator\n    if (not _global_cublas_handle):\n        from . import cublas\n        _global_cublas_handle = cublas.cublasCreate()\n    if (_global_cublas_allocator is None):\n        _global_cublas_allocator = allocator\n    global _global_cusolver_handle\n    if (not _global_cusolver_handle):\n        from . import cusolver\n        _global_cusolver_handle = cusolver.cusolverDnCreate()\n    if _has_cula:\n        cula.culaInitialize()\n    if _has_magma:\n        magma.magma_init()\n", "label": 1}
{"function": "\n\ndef init_ipython_session(argv=[], auto_symbols=False, auto_int_to_Integer=False):\n    'Construct new IPython session. '\n    import IPython\n    if (V(IPython.__version__) >= '0.11'):\n        if (V(IPython.__version__) >= '1.0'):\n            from IPython.terminal import ipapp\n        else:\n            from IPython.frontend.terminal import ipapp\n        app = ipapp.TerminalIPythonApp()\n        app.display_banner = False\n        app.initialize(argv)\n        if auto_symbols:\n            readline = import_module('readline')\n            if readline:\n                enable_automatic_symbols(app)\n        if auto_int_to_Integer:\n            enable_automatic_int_sympification(app)\n        return app.shell\n    else:\n        from IPython.Shell import make_IPython\n        return make_IPython(argv)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef dict_to_str(dictionary):\n    resstr = ''\n    new_dict = {\n        \n    }\n    for (key, val) in dictionary.items():\n        if (key == 0):\n            key = 'annotation'\n        new_dict[key] = val\n    for key in sorted(new_dict.keys()):\n        resstr += ('%s=%s; ' % (key, str(new_dict[key])))\n    return resstr[:(- 2)]\n", "label": 0}
{"function": "\n\ndef value(self, key, value=1):\n    'Set value of a counter by counter key'\n    if isinstance(key, six.string_types):\n        key = (key,)\n    assert isinstance(key, tuple), 'event key type error'\n    if (key not in self.counters):\n        self.counters[key] = self.cls()\n    self.counters[key].value(value)\n    return self\n", "label": 0}
{"function": "\n\ndef _media(self):\n    if ('collapse' in self.classes):\n        extra = ('' if settings.DEBUG else '.min')\n        js = [('jquery%s.js' % extra), 'jquery.init.js', ('collapse%s.js' % extra)]\n        return forms.Media(js=[static(('admin/js/%s' % url)) for url in js])\n    return forms.Media()\n", "label": 0}
{"function": "\n\ndef _add_xtra_pore_data(self):\n    xpdata = self._xtra_pore_data\n    if (xpdata is not None):\n        if isinstance(xpdata, type([])):\n            for pdata in xpdata:\n                try:\n                    self[('pore.' + pdata)] = self._dictionary[('p' + pdata)][self._pore_map]\n                except:\n                    logger.warning((('Could not add pore data: ' + pdata) + ' to network'))\n                    pass\n        else:\n            try:\n                self[('pore.' + xpdata)] = self._dictionary[('p' + xpdata)][self._pore_map]\n            except:\n                logger.warning((('Could not add pore data: ' + xpdata) + ' to network'))\n                pass\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.version == other.version) and (self.vrid == other.vrid) and (self.priority == other.priority) and (self.ip_addresses == other.ip_addresses) and (self.advertisement_interval == other.advertisement_interval) and (self.preempt_mode == other.preempt_mode) and (self.preempt_delay == other.preempt_delay) and (self.accept_mode == other.accept_mode) and (self.is_ipv6 == other.is_ipv6))\n", "label": 1}
{"function": "\n\ndef mark_invalid_input_sequences(self):\n    \"Fill in domain knowledge about valid input\\n    sequences (e.g. don't prune failure without pruning recovery.)\\n    Only do so if this isn't a view of a previously computed DAG\"\n    fingerprint2previousfailure = {\n        \n    }\n    for event in self._events_list:\n        if hasattr(event, 'fingerprint'):\n            fingerprint = event.fingerprint[1:]\n            if (type(event) in self._failure_types):\n                fingerprint2previousfailure[fingerprint] = event\n            elif (type(event) in self._recovery_types):\n                if (fingerprint in fingerprint2previousfailure):\n                    failure = fingerprint2previousfailure[fingerprint]\n                    failure.dependent_labels.append(event.label)\n", "label": 1}
{"function": "\n\ndef unquote(string):\n    if (not string):\n        return b''\n    res = string.split(b'%')\n    if (len(res) != 1):\n        string = res[0]\n        for item in res[1:]:\n            try:\n                string += (bytes([int(item[:2], 16)]) + item[2:])\n            except ValueError:\n                string += (b'%' + item)\n    return string\n", "label": 0}
{"function": "\n\ndef uncommented_lines(filename, use_sudo=False):\n    '\\n    Get the lines of a remote file, ignoring empty or commented ones\\n    '\n    func = (run_as_root if use_sudo else run)\n    res = func(('cat %s' % quote(filename)), quiet=True)\n    if res.succeeded:\n        return [line for line in res.splitlines() if (line and (not line.startswith('#')))]\n    else:\n        return []\n", "label": 1}
{"function": "\n\ndef create(self, req, body):\n    'Create or import keypair.\\n\\n        Sending name will generate a key and return private_key\\n        and fingerprint.\\n\\n        You can send a public_key to add an existing ssh key\\n\\n        params: keypair object with:\\n            name (required) - string\\n            public_key (optional) - string\\n        '\n    context = req.environ['nova.context']\n    authorize(context, action='create')\n    try:\n        params = body['keypair']\n        name = params['name']\n    except KeyError:\n        msg = _('Invalid request body')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        if ('public_key' in params):\n            keypair = self.api.import_key_pair(context, context.user_id, name, params['public_key'])\n            keypair = self._filter_keypair(keypair, user_id=True)\n        else:\n            (keypair, private_key) = self.api.create_key_pair(context, context.user_id, name)\n            keypair = self._filter_keypair(keypair, user_id=True)\n            keypair['private_key'] = private_key\n        return {\n            'keypair': keypair,\n        }\n    except exception.KeypairLimitExceeded:\n        msg = _('Quota exceeded, too many key pairs.')\n        raise webob.exc.HTTPForbidden(explanation=msg)\n    except exception.InvalidKeypair as exc:\n        raise webob.exc.HTTPBadRequest(explanation=exc.format_message())\n    except exception.KeyPairExists as exc:\n        raise webob.exc.HTTPConflict(explanation=exc.format_message())\n", "label": 1}
{"function": "\n\ndef get_users(self):\n    node = utils.get_repo_node(self.env, self.gitolite_admin_reponame, 'keydir')\n    assert node.isdir, ('Node %s at /keydir/ is not a directory' % node)\n    for child in node.get_entries():\n        name = child.get_name()\n        assert name.endswith('.pub'), ('Node %s' % name)\n        name = name[:(- 4)]\n        (yield name)\n", "label": 0}
{"function": "\n\ndef _walk(self, expr):\n    node = expr.op()\n    if isinstance(node, ops.TableColumn):\n        is_valid = self._validate_column(expr)\n        self.valid = (self.valid and is_valid)\n    for arg in node.flat_args():\n        if isinstance(arg, ir.ValueExpr):\n            self._walk(arg)\n", "label": 0}
{"function": "\n\n@property\ndef line_print(self):\n    inv_types = {v: k for (k, v) in defines.Types.iteritems()}\n    if (self._code is None):\n        self._code = defines.Codes.EMPTY.number\n    msg = 'From {source}, To {destination}, {type}-{mid}, {code}-{token}, ['.format(source=self._source, destination=self._destination, type=inv_types[self._type], mid=self._mid, code=defines.Codes.LIST[self._code].name, token=self._token)\n    for opt in self._options:\n        msg += '{name}: {value}, '.format(name=opt.name, value=opt.value)\n    msg += ']'\n    if (self.payload is not None):\n        msg += ' {payload}...{length} bytes'.format(payload=self.payload[0:20], length=len(self.payload))\n    else:\n        msg += ' No payload'\n    return msg\n", "label": 0}
{"function": "\n\ndef get_profile(self):\n    '\\n        Returns site-specific profile for this user. Raises\\n        SiteProfileNotAvailable if this site does not allow profiles.\\n        '\n    warnings.warn('The use of AUTH_PROFILE_MODULE to define user profiles has been deprecated.', PendingDeprecationWarning)\n    if (not hasattr(self, '_profile_cache')):\n        from django.conf import settings\n        if (not getattr(settings, 'AUTH_PROFILE_MODULE', False)):\n            raise SiteProfileNotAvailable('You need to set AUTH_PROFILE_MODULE in your project settings')\n        try:\n            (app_label, model_name) = settings.AUTH_PROFILE_MODULE.split('.')\n        except ValueError:\n            raise SiteProfileNotAvailable('app_label and model_name should be separated by a dot in the AUTH_PROFILE_MODULE setting')\n        try:\n            model = models.get_model(app_label, model_name)\n            if (model is None):\n                raise SiteProfileNotAvailable('Unable to load the profile model, check AUTH_PROFILE_MODULE in your project settings')\n            self._profile_cache = model._default_manager.using(self._state.db).get(user__id__exact=self.id)\n            self._profile_cache.user = self\n        except (ImportError, ImproperlyConfigured):\n            raise SiteProfileNotAvailable\n    return self._profile_cache\n", "label": 1}
{"function": "\n\ndef _fire_listeners(self, node, lint_context):\n    node_type = NodeType(node['type'])\n    if (node_type not in self._listeners_map):\n        return\n    listening_policies = self._listeners_map[node_type]\n    for listening_policy in listening_policies:\n        violation = listening_policy.get_violation_if_found(node, lint_context)\n        if (violation is not None):\n            self._violations.append(violation)\n", "label": 0}
{"function": "\n\ndef register(self, fid, event):\n    if event:\n        if (event & _AsyncPoller._Read):\n            self.rset.add(fid)\n        if (event & _AsyncPoller._Write):\n            self.wset.add(fid)\n        if (event & _AsyncPoller._Error):\n            self.xset.add(fid)\n", "label": 0}
{"function": "\n\ndef _insert_index(self, data):\n    data = data.copy()\n    idx_nlevels = data.index.nlevels\n    if (idx_nlevels == 1):\n        data.insert(0, 'Index', data.index)\n    else:\n        for i in range(idx_nlevels):\n            data.insert(i, 'Index{0}'.format(i), data.index.get_level_values(i))\n    col_nlevels = data.columns.nlevels\n    if (col_nlevels > 1):\n        col = data.columns.get_level_values(0)\n        values = [data.columns.get_level_values(i).values for i in range(1, col_nlevels)]\n        col_df = pd.DataFrame(values)\n        data.columns = col_df.columns\n        data = pd.concat([col_df, data])\n        data.columns = col\n    return data\n", "label": 0}
{"function": "\n\ndef test_equality():\n    (a, b, c) = (Identity(3), eye(3), ImmutableMatrix(eye(3)))\n    for x in [a, b, c]:\n        for y in [a, b, c]:\n            assert x.equals(y)\n", "label": 0}
{"function": "\n\ndef handle(self, request, data):\n    project = self._update_project(request, data)\n    if (not project):\n        return False\n    project_id = data['project_id']\n    domain_id = getattr(project, 'domain_id', '')\n    ret = self._update_project_members(request, data, project_id)\n    if (not ret):\n        return False\n    if PROJECT_GROUP_ENABLED:\n        ret = self._update_project_groups(request, data, project_id, domain_id)\n        if (not ret):\n            return False\n    if api.keystone.is_cloud_admin(request):\n        ret = self._update_project_quota(request, data, project_id)\n        if (not ret):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef Dictionary(self, *args):\n    if (not args):\n        return self._dict\n    dlist = [self._dict[x] for x in args]\n    if (len(dlist) == 1):\n        dlist = dlist[0]\n    return dlist\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **kwargs):\n    valid_domains = ['@stud.ntnu.no', '@ntnu.no']\n    for campain_id in args:\n        try:\n            campain = Campaign.objects.filter(pk=int(campain_id))\n        except Campaign.DoesNotExist:\n            raise CommandError(('Campaign ID %s does not exits' % campain_id))\n        signatures_qs = Signature.objects.filter(campaign__pk=int(campain_id))\n        for domain in valid_domains:\n            signatures_qs = signatures_qs.exclude(email__endswith=domain)\n        for signature in signatures_qs:\n            signature.delete()\n", "label": 0}
{"function": "\n\ndef getFieldExtractorForReadMessage(self, jsonMessage, objectToRead):\n    if ('field' not in jsonMessage):\n        raise MalformedMessageException(\"missing 'field'\")\n    field = jsonMessage['field']\n    if (not isinstance(field, str)):\n        raise MalformedMessageException('fieldname not a string')\n    field = intern(field)\n    try:\n        fieldDef = getattr(getObjectClass(objectToRead), field)\n    except:\n        raise InvalidFieldException()\n    if (not Decorators.isPropertyToExpose(fieldDef)):\n        raise InvalidFieldException()\n\n    def getter(x):\n        return getattr(x, field)\n    return getter\n", "label": 0}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.add_application_key(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            self.set_tag(d.getPrefixedString())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 0}
{"function": "\n\ndef __call__(self, item, context=None):\n    array_value = self._array_expression(item, context)\n    if (not isinstance(array_value, list)):\n        return None\n    index_value = self._index_expression(item, context)\n    if (not isinstance(index_value, int)):\n        return None\n    try:\n        return array_value[index_value]\n    except IndexError:\n        return None\n", "label": 0}
{"function": "\n\ndef _parse_settings_bond_6(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond6.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '6',\n    }\n    for binding in ['miimon', 'downdelay', 'updelay']:\n        if (binding in opts):\n            try:\n                int(opts[binding])\n                bond.update({\n                    binding: opts[binding],\n                })\n            except ValueError:\n                _raise_error_iface(iface, binding, ['integer'])\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({\n                binding: bond_def[binding],\n            })\n    if ('use_carrier' in opts):\n        if (opts['use_carrier'] in _CONFIG_TRUE):\n            bond.update({\n                'use_carrier': '1',\n            })\n        elif (opts['use_carrier'] in _CONFIG_FALSE):\n            bond.update({\n                'use_carrier': '0',\n            })\n        else:\n            valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({\n            'use_carrier': bond_def['use_carrier'],\n        })\n    return bond\n", "label": 1}
{"function": "\n\ndef _verify_tombstones(self, tx_objs, policy):\n    for (o_name, diskfiles) in tx_objs.items():\n        try:\n            self._open_tx_diskfile(o_name, policy)\n            self.fail('DiskFileDeleted expected')\n        except DiskFileDeleted as exc:\n            tx_delete_time = exc.timestamp\n        try:\n            self._open_rx_diskfile(o_name, policy)\n            self.fail('DiskFileDeleted expected')\n        except DiskFileDeleted as exc:\n            rx_delete_time = exc.timestamp\n        self.assertEqual(tx_delete_time, rx_delete_time)\n", "label": 0}
{"function": "\n\ndef field_count(self):\n    num_of_fields = 0\n    if self.has_auto_field():\n        num_of_fields += 1\n    num_of_fields += len(self.fieldsets[0][1]['fields'])\n    if self.formset.can_order:\n        num_of_fields += 1\n    if self.formset.can_delete:\n        num_of_fields += 1\n    return num_of_fields\n", "label": 0}
{"function": "\n\ndef _layout_state(self, state):\n    ' Layout the dock panes in the specified TaskState using its\\n            TaskLayout.\\n        '\n    for (name, corner) in CORNER_MAP.iteritems():\n        area = getattr(state.layout, (name + '_corner'))\n        self.control.setCorner(corner, AREA_MAP[area])\n    self._main_window_layout.state = state\n    self._main_window_layout.set_layout(state.layout)\n    for dock_pane in state.dock_panes:\n        if (dock_pane.control not in self._main_window_layout.consumed):\n            self.control.addDockWidget(AREA_MAP[dock_pane.dock_area], dock_pane.control)\n            if dock_pane.visible:\n                dock_pane.control.show()\n", "label": 0}
{"function": "\n\ndef _get_axis_class(axis_type, range_input):\n    if (axis_type is None):\n        return None\n    elif (axis_type == 'linear'):\n        return LinearAxis\n    elif (axis_type == 'log'):\n        return LogAxis\n    elif (axis_type == 'datetime'):\n        return DatetimeAxis\n    elif (axis_type == 'auto'):\n        if isinstance(range_input, FactorRange):\n            return CategoricalAxis\n        elif isinstance(range_input, Range1d):\n            try:\n                Datetime.validate(Datetime(), range_input.start)\n                return DatetimeAxis\n            except ValueError:\n                pass\n        return LinearAxis\n    else:\n        raise ValueError((\"Unrecognized axis_type: '%r'\" % axis_type))\n", "label": 1}
{"function": "\n\ndef _parse_metadata(context, repos, record):\n    'parse metadata formats'\n    if isinstance(record, str):\n        exml = etree.fromstring(record, context.parser)\n    elif hasattr(record, 'getroot'):\n        exml = record.getroot()\n    else:\n        exml = record\n    root = exml.tag\n    LOGGER.debug('Serialized metadata, parsing content model')\n    if (root == ('{%s}MD_Metadata' % context.namespaces['gmd'])):\n        return [_parse_iso(context, repos, exml)]\n    elif (root == '{http://www.isotc211.org/2005/gmi}MI_Metadata'):\n        return [_parse_iso(context, repos, exml)]\n    elif (root == 'metadata'):\n        return [_parse_fgdc(context, repos, exml)]\n    elif (root == ('{%s}TRANSFER' % context.namespaces['gm03'])):\n        return [_parse_gm03(context, repos, exml)]\n    elif (root == ('{%s}Record' % context.namespaces['csw'])):\n        return [_parse_dc(context, repos, exml)]\n    elif (root == ('{%s}RDF' % context.namespaces['rdf'])):\n        return [_parse_dc(context, repos, exml)]\n    elif (root == ('{%s}DIF' % context.namespaces['dif'])):\n        pass\n    else:\n        raise RuntimeError('Unsupported metadata format')\n", "label": 1}
{"function": "\n\ndef watcher(self, zh, event, state, path):\n    'Handle zookeeper.aget() watches.\\n\\n    This code is called when a znode changes and triggers a data watch.\\n    It is not called to handle the zookeeper.aget call itself.\\n\\n    Numeric arguments map to constants. See ``DATA`` in ``help(zookeeper)``\\n    for more information.\\n\\n    Args:\\n      zh Zookeeper handle that set this watch.\\n      event Event that caused the watch (often called ``type`` elsewhere).\\n      state Connection state.\\n      path Znode that triggered this watch.\\n\\n    Does not provide a return value.\\n    '\n    out = ['Running watcher:', ('zh=%d' % zh), ('event=%d' % event), ('state=%d' % state), ('path=%s' % path)]\n    logger.debug(' '.join(out))\n    if ((event == zookeeper.CHANGED_EVENT) and (state == zookeeper.CONNECTED_STATE) and (self.znode == path)):\n        if (zookeeper.OK != self.aget()):\n            logger.critical('Unable to get znode! Exiting.')\n            sys.exit(1)\n", "label": 0}
{"function": "\n\ndef request_is_managed_by_flask_classy(self):\n    if (request.endpoint is None):\n        return False\n    if (':' not in request.endpoint):\n        return False\n    (class_name, action) = request.endpoint.split(':')\n    return (any(((class_name == classy_class.__name__) for classy_class in self.flask_classy_classes)) and (action in self.special_methods))\n", "label": 0}
{"function": "\n\ndef get_rule_match(self, gram_handler):\n    for grammar in gram_handler.get_matching_grammars():\n        for rule in grammar._rules:\n            rule_match = matching.get_rule_match(rule, self.remaining_words, grammar.settings['filtered words'])\n            if (rule_match is not None):\n                return rule_match\n", "label": 0}
{"function": "\n\ndef __str__(self, prefix='', printElemNumber=0):\n    res = ''\n    if self.has_is_available_:\n        res += (prefix + ('is_available: %s\\n' % self.DebugFormatBool(self.is_available_)))\n    if self.has_presence_:\n        res += (prefix + ('presence: %s\\n' % self.DebugFormatInt32(self.presence_)))\n    if self.has_valid_:\n        res += (prefix + ('valid: %s\\n' % self.DebugFormatBool(self.valid_)))\n    return res\n", "label": 0}
{"function": "\n\ndef test_create_with_foreign_key(self):\n    'Create a table with a foreign key constraint'\n    inmap = self.std_map()\n    inmap['schema public'].update({\n        'table t1': {\n            'columns': [{\n                'c11': {\n                    'type': 'integer',\n                },\n            }, {\n                'c12': {\n                    'type': 'text',\n                },\n            }],\n        },\n        'table t2': {\n            'columns': [{\n                'c21': {\n                    'type': 'integer',\n                },\n            }, {\n                'c22': {\n                    'type': 'text',\n                },\n            }, {\n                'c23': {\n                    'type': 'integer',\n                },\n            }],\n            'foreign_keys': {\n                't2_c23_fkey': {\n                    'columns': ['c23'],\n                    'references': {\n                        'columns': ['c11'],\n                        'table': 't1',\n                    },\n                },\n            },\n        },\n    })\n    sql = self.to_sql(inmap)\n    crt1 = 0\n    crt2 = 1\n    if ('t1' in sql[1]):\n        crt1 = 1\n        crt2 = 0\n    assert (fix_indent(sql[crt1]) == 'CREATE TABLE t1 (c11 integer, c12 text)')\n    assert (fix_indent(sql[crt2]) == 'CREATE TABLE t2 (c21 integer, c22 text, c23 integer)')\n    assert (fix_indent(sql[2]) == 'ALTER TABLE t2 ADD CONSTRAINT t2_c23_fkey FOREIGN KEY (c23) REFERENCES t1 (c11)')\n", "label": 0}
{"function": "\n\ndef remove(self, entity):\n    '\\n        Remove a subecriber from the channel.\\n\\n        Args:\\n            entity (Player, Object or list): The entity or\\n                entities to un-subscribe from the channel.\\n\\n        '\n    for subscriber in make_iter(entity):\n        if subscriber:\n            clsname = subscriber.__dbclass__.__name__\n            if (clsname == 'PlayerDB'):\n                self.obj.db_subscriptions.remove(entity)\n            elif (clsname == 'ObjectDB'):\n                self.obj.db_object_subscriptions.remove(entity)\n", "label": 0}
{"function": "\n\ndef test_disk_usage(self):\n    usage = psutil.disk_usage(os.getcwd())\n    assert (usage.total > 0), usage\n    assert (usage.used > 0), usage\n    assert (usage.free > 0), usage\n    assert (usage.total > usage.used), usage\n    assert (usage.total > usage.free), usage\n    assert (0 <= usage.percent <= 100), usage.percent\n    fname = tempfile.mktemp()\n    try:\n        psutil.disk_usage(fname)\n    except OSError:\n        err = sys.exc_info()[1]\n        if (err.args[0] != errno.ENOENT):\n            raise\n    else:\n        self.fail('OSError not raised')\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if (not other):\n        return False\n    return ((self.x_coord == other.x_coord) and (self.y_coord == other.y_coord) and (self.z_coord == other.z_coord))\n", "label": 0}
{"function": "\n\ndef beacon(config):\n    '\\n    Emit the status of a connected display to the minion\\n\\n    Mainly this is used to detect when the display fails to connect for whatever reason.\\n\\n    .. code-block:: yaml\\n\\n        beacons:\\n          glxinfo:\\n            user: frank\\n            screen_event: True\\n\\n    '\n    log.trace('glxinfo beacon starting')\n    ret = []\n    _validate = validate(config)\n    if (not _validate[0]):\n        return ret\n    retcode = __salt__['cmd.retcode']('DISPLAY=:0 glxinfo', runas=config['user'], python_shell=True)\n    if (('screen_event' in config) and config['screen_event']):\n        last_value = last_state.get('screen_available', False)\n        screen_available = (retcode == 0)\n        if ((last_value != screen_available) or ('screen_available' not in last_state)):\n            ret.append({\n                'tag': 'screen_event',\n                'screen_available': screen_available,\n            })\n        last_state['screen_available'] = screen_available\n    return ret\n", "label": 1}
{"function": "\n\ndef rescale_parent_proportion(self, *args):\n    if (not self.parent):\n        return\n    if self.rescale_with_parent:\n        parent_proportion = self._parent_proportion\n        if (self.sizable_from in ('top', 'bottom')):\n            new_height = (parent_proportion * self.parent.height)\n            self.height = max(self.min_size, min(new_height, self.max_size))\n        else:\n            new_width = (parent_proportion * self.parent.width)\n            self.width = max(self.min_size, min(new_width, self.max_size))\n", "label": 0}
{"function": "\n\ndef make_command(command, env=None, sudo=False, sudo_user=None):\n    '\\n    Builds a shell command with various kwargs.\\n    '\n    if env:\n        env_string = ' '.join(['{0}={1}'.format(key, value) for (key, value) in six.iteritems(env)])\n        command = '{0} {1}'.format(env_string, command)\n    command = command.replace(\"'\", \"\\\\'\")\n    if (not sudo):\n        command = \"sh -c '{0}'\".format(command)\n    elif sudo_user:\n        command = \"sudo -H -u {0} -S sh -c '{1}'\".format(sudo_user, command)\n    else:\n        command = \"sudo -H -S sh -c '{0}'\".format(command)\n    return command\n", "label": 0}
{"function": "\n\ndef _make_window(window_dict):\n    '\\n    Creates a new class for that window and registers it at this module.\\n    '\n    cls_name = ('%sWindow' % camel_case(str(window_dict['name'])))\n    bases = (Window,)\n    attrs = {\n        '__module__': sys.modules[__name__],\n        'name': str(window_dict['name']),\n        'inv_type': str(window_dict['id']),\n        'inv_data': window_dict,\n    }\n\n    def make_slot_method(index, size=1):\n        if (size == 1):\n            return (lambda self: self.slots[index])\n        else:\n            return (lambda self: self.slots[index:(index + size)])\n    for slots in window_dict.get('slots', []):\n        index = slots['index']\n        size = slots.get('size', 1)\n        attr_name = snake_case(str(slots['name']))\n        attr_name += ('_slot' if (size == 1) else '_slots')\n        slots_method = make_slot_method(index, size)\n        slots_method.__name__ = attr_name\n        attrs[attr_name] = property(slots_method)\n    for (i, prop_name) in enumerate(window_dict.get('properties', [])):\n\n        def make_prop_method(i):\n            return (lambda self: self.properties[i])\n        prop_method = make_prop_method(i)\n        prop_name = snake_case(str(prop_name))\n        prop_method.__name__ = prop_name\n        attrs[prop_name] = property(prop_method)\n    cls = type(cls_name, bases, attrs)\n    assert (not hasattr(sys.modules[__name__], cls_name)), ('Window \"%s\" already registered at %s' % (cls_name, __name__))\n    setattr(sys.modules[__name__], cls_name, cls)\n    return cls\n", "label": 0}
{"function": "\n\ndef __init__(self, qtile):\n    self.groups = []\n    self.screens = {\n        \n    }\n    self.current_screen = 0\n    for group in qtile.groups:\n        self.groups.append((group.name, group.layout.name))\n    for (index, screen) in enumerate(qtile.screens):\n        self.screens[index] = screen.group.name\n        if (screen == qtile.currentScreen):\n            self.current_screen = index\n", "label": 0}
{"function": "\n\ndef __init__(self, parent=None, win=None, xrefs=None, headers=None):\n    super(XrefValueWindow, self).__init__(parent)\n    self.parent = parent\n    self.mainwin = win\n    self.xrefs = xrefs\n    self.headers = headers\n    self.reverse_strings = {\n        \n    }\n    self.proxyModel = QtGui.QSortFilterProxyModel()\n    self.proxyModel.setDynamicSortFilter(True)\n    self.model = QtGui.QStandardItemModel(len(self.xrefs), len(self.headers), self)\n    column = 0\n    for header in headers:\n        self.model.setHeaderData(column, QtCore.Qt.Horizontal, header)\n        column += 1\n    row = 0\n    for ref in xrefs:\n        for column in range(len(self.headers)):\n            self.model.setData(self.model.index(row, column, QtCore.QModelIndex()), ('%s' % ref[column]))\n        row += 1\n    self.proxyModel.setSourceModel(self.model)\n    self.setRootIsDecorated(False)\n    self.setAlternatingRowColors(True)\n    self.setModel(self.proxyModel)\n    self.setSortingEnabled(True)\n    self.setEditTriggers(QtGui.QAbstractItemView.NoEditTriggers)\n    self.doubleClicked.connect(self.slotDoubleClicked)\n", "label": 0}
{"function": "\n\ndef _convert_to_python(self, value_dict, state):\n    is_empty = self.field_is_empty\n    if ((self.field in value_dict) and (value_dict.get(self.field) == self.expected_value)):\n        for required_field in self.required_fields:\n            if ((required_field not in value_dict) or is_empty(value_dict.get(required_field))):\n                raise Invalid((_('You must give a value for %s') % required_field), value_dict, state, error_dict={\n                    required_field: Invalid(self.message('empty', state), value_dict.get(required_field), state),\n                })\n    return value_dict\n", "label": 1}
{"function": "\n\ndef wpc_channel(url):\n    query = urlparse(url)\n    path_elements = query.path.strip('/').split('/')\n    if ((query.hostname in ('www.watchpeoplecode.com', 'watchpeoplecode.com')) and (len(path_elements) == 2) and (path_elements[0] == 'streamer')):\n        return path_elements[1]\n", "label": 0}
{"function": "\n\ndef _get_functions(self):\n    'Create a mapping from commands to command functions.'\n    functions = {\n        \n    }\n    for each in dir(self):\n        if (not each.startswith('cmd_')):\n            continue\n        func = getattr(self, each)\n        for cmd in getattr(self, ('commands_' + each[4:]), '').split():\n            functions[cmd] = func\n    return functions\n", "label": 0}
{"function": "\n\ndef value_ds_to_numpy(ds, count):\n    ret = None\n    try:\n        values = ds[(count * (- 1)):]\n        ret = numpy.array([float(value) for value in values])\n    except IndexError:\n        pass\n    except TypeError:\n        pass\n    return ret\n", "label": 0}
{"function": "\n\ndef test_bad_querysets(self):\n    with pytest.raises(ValueError) as excinfo:\n        Author.objects.all().order_by('name').iter_smart_chunks()\n    assert ('ordering' in str(excinfo.value))\n    with pytest.raises(ValueError) as excinfo:\n        Author.objects.all()[:5].iter_smart_chunks()\n    assert ('sliced QuerySet' in str(excinfo.value))\n    with pytest.raises(ValueError) as excinfo:\n        NameAuthor.objects.all().iter_smart_chunks()\n    assert ('non-integer primary key' in str(excinfo.value))\n", "label": 0}
{"function": "\n\ndef test_server(args, prepare_func=None, family=socket.AF_INET):\n    try:\n        try:\n            sock = socket.socket(family=family)\n            sock.settimeout(args.timeout)\n            sock.connect((args.host, args.port))\n        except socket.error as e:\n            print('Unable to connect to {0}:{1}: {2}'.format(args.host, args.port, e))\n            return False\n        (remote_addr, remote_port) = sock.getpeername()[:2]\n        print('Connected to: {0}:{1}'.format(remote_addr, remote_port))\n        if (prepare_func is not None):\n            prepare_func(sock)\n            print('Pre-TLS stage completed, continuing with handshake')\n        return handle_ssl(sock, args)\n    except (Failure, socket.error) as e:\n        print(('Unable to check for vulnerability: ' + str(e)))\n        return False\n    finally:\n        if sock:\n            sock.close()\n", "label": 0}
{"function": "\n\ndef __lt__(self, other):\n    if (self._version != other._version):\n        raise TypeError(('%s and %s are not of the same version' % (self, other)))\n    if (not isinstance(other, _BaseAddress)):\n        raise TypeError(('%s and %s are not of the same type' % (self, other)))\n    if (self._ip != other._ip):\n        return (self._ip < other._ip)\n    return False\n", "label": 0}
{"function": "\n\ndef exchange(self, pdu, timeout):\n    if ((not isinstance(pdu, Symmetry)) or (self.cfg.get('symm-log') is True)):\n        log.debug('SEND {0}'.format(pdu))\n    data = (pdu.to_string() if pdu else None)\n    try:\n        data = self.mac.exchange(data, timeout)\n        if (data is None):\n            return None\n    except nfc.clf.DigitalProtocolError as error:\n        log.debug('{0!r}'.format(error))\n        return None\n    pdu = ProtocolDataUnit.from_string(data)\n    if ((not isinstance(pdu, Symmetry)) or (self.cfg.get('symm-log') is True)):\n        log.debug('RECV {0}'.format(pdu))\n    return pdu\n", "label": 1}
{"function": "\n\ndef build_5(self):\n    'level 5'\n    pig = Pig(900, 70, self.space)\n    self.pigs.append(pig)\n    pig = Pig(1000, 152, self.space)\n    self.pigs.append(pig)\n    for i in range(9):\n        p = (800, (70 + (i * 21)))\n        self.beams.append(Polygon(p, 85, 20, self.space))\n    for i in range(4):\n        p = (1000, (70 + (i * 21)))\n        self.beams.append(Polygon(p, 85, 20, self.space))\n    p = (970, 176)\n    self.columns.append(Polygon(p, 20, 85, self.space))\n    p = (1026, 176)\n    self.columns.append(Polygon(p, 20, 85, self.space))\n    p = (1000, 230)\n    self.beams.append(Polygon(p, 85, 20, self.space))\n    self.number_of_birds = 4\n    if self.bool_space:\n        self.number_of_birds = 8\n", "label": 0}
{"function": "\n\ndef __init__(self, thread):\n    self.ident = 0\n    try:\n        self.ident = thread.ident\n    except AttributeError:\n        pass\n    if (not self.ident):\n        for (tid, athread) in threading._active.items():\n            if (athread is thread):\n                self.ident = tid\n                break\n    try:\n        self.name = thread.name\n    except AttributeError:\n        self.name = thread.getName()\n    try:\n        self.daemon = thread.daemon\n    except AttributeError:\n        self.daemon = thread.isDaemon()\n", "label": 1}
{"function": "\n\ndef blockCombine(l):\n    ' Produce a matrix from a list of lists of its components. '\n    l = [list(map(mat, row)) for row in l]\n    hdims = [m.shape[1] for m in l[0]]\n    hs = sum(hdims)\n    vdims = [row[0].shape[0] for row in l]\n    vs = sum(vdims)\n    res = zeros((hs, vs))\n    vindex = 0\n    for (i, row) in enumerate(l):\n        hindex = 0\n        for (j, m) in enumerate(row):\n            res[vindex:(vindex + vdims[i]), hindex:(hindex + hdims[j])] = m\n            hindex += hdims[j]\n        vindex += vdims[i]\n    return res\n", "label": 1}
{"function": "\n\n@classmethod\ndef handle_token(cls, parser, token):\n    tokens = token.split_contents()\n    if (len(tokens) == 1):\n        return cls(obj='user-notification', target='page')\n    if (len(tokens) > 3):\n        raise template.TemplateSyntaxError(_('Max arguments are two'))\n    elif (tokens[1] != 'for'):\n        raise template.TemplateSyntaxError(_(\"First argument must be 'for'\"))\n    elif (not tokens[2]):\n        raise template.TemplateSyntaxError(_(\"Second argument must either 'box' or 'page'\"))\n    else:\n        return cls(obj='user-notification', target=tokens[2])\n", "label": 0}
{"function": "\n\ndef filter_song_md(song, md_list=['id'], no_singletons=True):\n    'Returns a list of desired metadata from a song.\\n    Does not modify the given song.\\n\\n    :param song: Dictionary representing a GM song.\\n    :param md_list: (optional) the ordered list of metadata to select.\\n    :param no_singletons: (optional) if md_list is of length 1, return the data,\\n      not a singleton list.\\n    '\n    filtered = [song[md_type] for md_type in md_list]\n    if ((len(md_list) == 1) and no_singletons):\n        return filtered[0]\n    else:\n        return filtered\n", "label": 0}
{"function": "\n\ndef test_werkzeug_upload():\n    try:\n        import werkzeug\n    except ImportError:\n        return\n    storage = app_storage()\n    object_name = 'my-txt-hello.txt'\n    filepath = (CWD + '/data/hello.txt')\n    file = None\n    with open(filepath, 'rb') as fp:\n        file = werkzeug.datastructures.FileStorage(fp)\n        file.filename = object_name\n        o = storage.upload(file, overwrite=True)\n        assert isinstance(o, Object)\n        assert (o.name == object_name)\n", "label": 0}
{"function": "\n\n@classmethod\ndef from_obj(cls, obj, return_obj=None, contained_type=None, binding_var=None):\n    if (not obj):\n        return None\n    if (return_obj is None):\n        return_obj = cls()\n    if (not contained_type):\n        contained_type = cls._contained_type\n    if (not binding_var):\n        binding_var = cls._binding_var\n    for item in getattr(obj, binding_var):\n        return_obj.append(contained_type.from_obj(item))\n    return return_obj\n", "label": 1}
{"function": "\n\ndef test_match_unrolled(self):\n    ' tests that inference with scan matches result using unrolled loops '\n    unrolled_e_step = E_Step(h_new_coeff_schedule=self.h_new_coeff_schedule)\n    unrolled_e_step.register_model(self.model)\n    V = T.matrix()\n    scan_result = self.e_step.infer(V)\n    unrolled_result = unrolled_e_step.infer(V)\n    outputs = []\n    for key in scan_result:\n        outputs.append(scan_result[key])\n        outputs.append(unrolled_result[key])\n    f = function([V], outputs)\n    outputs = f(self.X)\n    assert ((len(outputs) % 2) == 0)\n    for i in xrange(0, len(outputs), 2):\n        assert np.allclose(outputs[i], outputs[(i + 1)])\n", "label": 0}
{"function": "\n\n@view_config(context=APIPackagingResource, request_method='GET', subpath=(), renderer='json')\n@addslash\n@argify\ndef all_packages(request, verbose=False):\n    ' List all packages '\n    if verbose:\n        packages = request.db.summary()\n    else:\n        packages = request.db.distinct()\n    i = 0\n    while (i < len(packages)):\n        package = packages[i]\n        name = (package if isinstance(package, basestring) else package['name'])\n        if (not request.access.has_permission(name, 'read')):\n            del packages[i]\n            continue\n        i += 1\n    return {\n        'packages': packages,\n    }\n", "label": 0}
{"function": "\n\ndef get_artist(self, artists):\n    'Returns an artist string (all artists) and an artist_id (the main\\n        artist) for a list of discogs album or track artists.\\n        '\n    artist_id = None\n    bits = []\n    for (i, artist) in enumerate(artists):\n        if (not artist_id):\n            artist_id = artist['id']\n        name = artist['name']\n        name = re.sub(' \\\\(\\\\d+\\\\)$', '', name)\n        name = re.sub('(?i)^(.*?), (a|an|the)$', '\\\\2 \\\\1', name)\n        bits.append(name)\n        if (artist['join'] and (i < (len(artists) - 1))):\n            bits.append(artist['join'])\n    artist = (' '.join(bits).replace(' ,', ',') or None)\n    return (artist, artist_id)\n", "label": 1}
{"function": "\n\ndef fromhg(src, dst):\n    if (src.startswith('-') or dst.startswith('-')):\n        raise ValueError('Bad src or dst')\n    proc = subprocess.Popen(['hg', 'clone', src, dst], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    output = proc.communicate()[0]\n    ret = proc.wait()\n    if ret:\n        raise CopyError('Could not copy. Mercurial returned {0}'.format(output))\n", "label": 0}
{"function": "\n\ndef spj(path, max_cpu_time, max_memory, in_path, user_out_path):\n    if (file_exists(in_path) and file_exists(user_out_path)):\n        result = judger.run(path=path, in_file=in_path, out_file='/tmp/spj.out', max_cpu_time=max_cpu_time, max_memory=max_memory, args=[in_path, user_out_path], env=[('PATH=' + os.environ.get('PATH', ''))], use_sandbox=True, use_nobody=True)\n        if ((result['signal'] == 0) and (result['exit_status'] in [AC, WA, SPJ_ERROR])):\n            result['spj_result'] = result['exit_status']\n        else:\n            result['spj_result'] = SPJ_ERROR\n        return result\n    else:\n        raise ValueError('in_path or user_out_path does not exist')\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _translate_msgid(msgid, domain, desired_locale=None):\n    if (not desired_locale):\n        system_locale = locale.getdefaultlocale()\n        if (not system_locale[0]):\n            desired_locale = 'en_US'\n        else:\n            desired_locale = system_locale[0]\n    locale_dir = os.environ.get((domain.upper() + '_LOCALEDIR'))\n    lang = gettext.translation(domain, localedir=locale_dir, languages=[desired_locale], fallback=True)\n    if six.PY3:\n        translator = lang.gettext\n    else:\n        translator = lang.ugettext\n    translated_message = translator(msgid)\n    return translated_message\n", "label": 0}
{"function": "\n\ndef calculate_debounced_passing(recent_results, debounce=0):\n    '\\n    `debounce` is the number of previous failures we need (not including this)\\n    to mark a search as passing or failing\\n    Returns:\\n      True if passing given debounce factor\\n      False if failing\\n    '\n    if (not recent_results):\n        return True\n    debounce_window = recent_results[:(debounce + 1)]\n    for r in debounce_window:\n        if r.succeeded:\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef _setup_extensions(self):\n    self.extensions = []\n    if try_murmur3:\n        self.extensions.append(murmur3_ext)\n    if try_libev:\n        self.extensions.append(libev_ext)\n    if try_cython:\n        try:\n            from Cython.Build import cythonize\n            cython_candidates = ['cluster', 'concurrent', 'connection', 'cqltypes', 'metadata', 'pool', 'protocol', 'query', 'util']\n            compile_args = ([] if is_windows else ['-Wno-unused-function'])\n            self.extensions.extend(cythonize([Extension(('cassandra.%s' % m), [('cassandra/%s.py' % m)], extra_compile_args=compile_args) for m in cython_candidates], nthreads=build_concurrency, exclude_failures=True))\n            self.extensions.extend(cythonize(NoPatchExtension('*', ['cassandra/*.pyx'], extra_compile_args=compile_args), nthreads=build_concurrency))\n        except Exception:\n            sys.stderr.write('Failed to cythonize one or more modules. These will not be compiled as extensions (optional).\\n')\n", "label": 1}
{"function": "\n\ndef make_revision_with_plugins(obj, user=None, message=None):\n    '\\n    Only add to revision if it is a draft.\\n    '\n    from cms.models.pluginmodel import CMSPlugin\n    from cms.utils.reversion_hacks import revision_context, revision_manager\n    cls = obj.__class__\n    if hasattr(revision_manager, '_registration_key_for_model'):\n        model_key = revision_manager._registration_key_for_model(cls)\n    else:\n        model_key = cls\n    if (model_key in revision_manager._registered_models):\n        placeholder_relation = find_placeholder_relation(obj)\n        if revision_context.is_active():\n            if user:\n                revision_context.set_user(user)\n            if message:\n                revision_context.set_comment(message)\n            adapter = revision_manager.get_adapter(obj.__class__)\n            revision_context.add_to_context(revision_manager, obj, adapter.get_version_data(obj))\n            for ph in obj.get_placeholders():\n                phadapter = revision_manager.get_adapter(ph.__class__)\n                revision_context.add_to_context(revision_manager, ph, phadapter.get_version_data(ph))\n            filters = {\n                ('placeholder__%s' % placeholder_relation): obj,\n            }\n            for plugin in CMSPlugin.objects.filter(**filters):\n                (plugin_instance, admin) = plugin.get_plugin_instance()\n                if plugin_instance:\n                    padapter = revision_manager.get_adapter(plugin_instance.__class__)\n                    revision_context.add_to_context(revision_manager, plugin_instance, padapter.get_version_data(plugin_instance))\n                bpadapter = revision_manager.get_adapter(plugin.__class__)\n                revision_context.add_to_context(revision_manager, plugin, bpadapter.get_version_data(plugin))\n", "label": 1}
{"function": "\n\ndef _emit(self, cond_br, target=None, reg=None, absolute=False, link=REGISTERS['null']):\n    if (target is None):\n        imm = 0\n    elif isinstance(target, Label):\n        target.pinned = False\n        self.asm._add_backpatch_item(target.name)\n        imm = 0\n    elif isinstance(target, int):\n        imm = target\n    else:\n        raise AssembleError('Invalid branch target: {}'.format(target))\n    if reg:\n        if ((not (reg.spec & _REG_AR)) or (reg.name not in GENERAL_PURPOSE_REGISTERS)):\n            raise AssembleError('Must be general purpose regfile A register {}'.format(reg))\n        assert (reg.addr < 32)\n        raddr_a = reg.addr\n        use_reg = True\n    else:\n        raddr_a = 0\n        use_reg = False\n    (waddr_add, waddr_mul, write_swap, pack) = self._encode_write_operands(link)\n    if pack:\n        raise AssembleError('Packing is not available for link register')\n    insn = BranchInsn(sig=15, cond_br=cond_br, rel=(not absolute), reg=use_reg, raddr_a=raddr_a, ws=write_swap, waddr_add=waddr_add, waddr_mul=waddr_mul, immediate=imm)\n    self.asm._emit(insn)\n", "label": 1}
{"function": "\n\ndef _assert_get_rules_by_name(self, name, expected, immediate=None):\n    if (immediate is None):\n        rules = get_rules_by_name(name, self.rules_dir)\n    else:\n        rules = get_rules_by_name(name, self.rules_dir, immediate)\n    actual = [[rule.name, rule.params.keysets.keys()] for rule in rules]\n    for rule in actual:\n        rule[1].sort()\n    eq_(actual, expected)\n", "label": 0}
{"function": "\n\ndef getPluginNameAndModuleFromStream(self, infoFileObject, candidate_infofile=None):\n    for analyzer in self._analyzers:\n        if (analyzer.name == 'info_ext'):\n            return analyzer.getPluginNameAndModuleFromStream(infoFileObject)\n    else:\n        raise RuntimeError('No current file analyzer is able to provide plugin information from stream')\n", "label": 0}
{"function": "\n\ndef __init__(self, sliceDB, seqDB, annotationType=None, itemClass=AnnotationSeq, itemSliceClass=AnnotationSlice, itemAttrDict=None, sliceAttrDict=None, maxCache=None, autoGC=True, checkFirstID=True, **kwargs):\n    'sliceDB must map identifier to a sliceInfo object;\\n        sliceInfo must have attributes: id, start, stop, orientation;\\n        seqDB must map sequence ID to a sliceable sequence object;\\n        sliceAttrDict gives optional dict of item attributes that\\n        should be mapped to sliceDB item attributes.\\n        maxCache specfies the maximum number of annotation objects\\n        to keep in the cache.'\n    if autoGC:\n        self._weakValueDict = classutil.RecentValueDictionary(autoGC)\n    else:\n        self._weakValueDict = {\n            \n        }\n    self.autoGC = autoGC\n    if (sliceAttrDict is None):\n        sliceAttrDict = {\n            \n        }\n    if (sliceDB is not None):\n        self.sliceDB = sliceDB\n    else:\n        self.sliceDB = classutil.get_shelve_or_dict(**kwargs)\n    self.seqDB = seqDB\n    self.annotationType = annotationType\n    self.itemClass = itemClass\n    self.itemSliceClass = itemSliceClass\n    self.sliceAttrDict = sliceAttrDict\n    if (maxCache is not None):\n        self.maxCache = maxCache\n    if checkFirstID:\n        try:\n            k = iter(self).next()\n            self.get_annot_obj(k, self.sliceDB[k])\n        except KeyError:\n            raise KeyError(('cannot create annotation object %s; sequence database %s may not be correct' % (k, repr(seqDB))))\n        except StopIteration:\n            pass\n", "label": 1}
{"function": "\n\ndef get_field_history_user(self, instance):\n    try:\n        return instance._field_history_user\n    except AttributeError:\n        try:\n            if self.thread.request.user.is_authenticated():\n                return self.thread.request.user\n            return None\n        except AttributeError:\n            return None\n", "label": 0}
{"function": "\n\ndef bot_process_action(self, action):\n    if (action.action_type == 'message'):\n        body = action.meta.get('body')\n        if body:\n            for dest in action.destination_rooms:\n                self.sendLine('>>> {0}: {1}'.format(dest, body))\n", "label": 0}
{"function": "\n\n@classmethod\ndef create(cls, folder, revision=None, path=None):\n    if (revision and path):\n        raise TypeError('You may specify a git revision or a local path; not both.')\n    if revision:\n        return GitRevisionJiraLinkManager(folder, revision)\n    else:\n        return WorkingCopyJiraLinkManager(folder, path)\n", "label": 0}
{"function": "\n\ndef decode_if_str(arg):\n    if isinstance(arg, list):\n        return [decode_if_str(elem) for elem in arg]\n    else:\n        return (arg.decode('utf-8') if isinstance(arg, str) else arg)\n", "label": 0}
{"function": "\n\ndef convert_to_color(object, name, value):\n    ' Converts a tuple or an integer to an RGB color value, or raises a\\n    TraitError if that is not possible.\\n    '\n    if ((type(value) in SequenceTypes) and (len(value) == 3)):\n        return (range_check(value[0]), range_check(value[1]), range_check(value[2]))\n    if (type(value) is int):\n        num = int(value)\n        return ((((num / 65536) / 255.0(((num / 256) & 255))) / 255.0), ((num & 255) / 255.0))\n    raise TraitError\n", "label": 0}
{"function": "\n\n@overrides.setter\ndef overrides(self, newch):\n    self._overrides._active = False\n    self._overrides.clear()\n    for (k, v) in newch.iteritems():\n        if v:\n            self._overrides[str(k)] = v\n        else:\n            try:\n                del self._overrides[str(k)]\n            except:\n                pass\n    self._overrides._active = True\n    self._overrides._send()\n", "label": 0}
{"function": "\n\ndef parse(text, pos=0, endpos=None):\n    pos = 0\n    if (endpos is None):\n        endpos = len(text)\n    d = {\n        \n    }\n    while 1:\n        m = entityRE.search(text, pos, endpos)\n        if (not m):\n            break\n        (name, charcode, comment) = m.groups()\n        d[name] = (charcode, comment)\n        pos = m.end()\n    return d\n", "label": 0}
{"function": "\n\ndef create_hc(G):\n    'Creates hierarchical cluster of graph G from distance matrix'\n    path_length = nx.all_pairs_shortest_path_length(G)\n    distances = numpy.zeros((len(G), len(G)))\n    for (u, p) in path_length.items():\n        for (v, d) in p.items():\n            distances[u][v] = d\n    Y = distance.squareform(distances)\n    Z = hierarchy.complete(Y)\n    membership = list(hierarchy.fcluster(Z, t=1.15))\n    partition = defaultdict(list)\n    for (n, p) in zip(list(range(len(G))), membership):\n        partition[p].append(n)\n    return list(partition.values())\n", "label": 0}
{"function": "\n\ndef test_algorithm_returns_06(self):\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['Monthly'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.three_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['3-Month'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.six_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['6-month'])\n    np.testing.assert_almost_equal([x.algorithm_period_returns for x in self.metrics_06.year_periods], ANSWER_KEY.ALGORITHM_PERIOD_RETURNS['year'])\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    from sympy.combinatorics.permutations import Permutation, Cycle\n    if Permutation.print_cyclic:\n        if (not self.size):\n            return 'Permutation()'\n        s = Cycle(self)((self.size - 1)).__repr__()[len('Cycle'):]\n        last = s.rfind('(')\n        if ((not (last == 0)) and (',' not in s[last:])):\n            s = (s[last:] + s[:last])\n        return ('Permutation%s' % s)\n    else:\n        s = self.support()\n        if (not s):\n            if (self.size < 5):\n                return ('Permutation(%s)' % str(self.array_form))\n            return ('Permutation([], size=%s)' % self.size)\n        trim = (str(self.array_form[:(s[(- 1)] + 1)]) + (', size=%s' % self.size))\n        use = full = str(self.array_form)\n        if (len(trim) < len(full)):\n            use = trim\n        return ('Permutation%s' % use)\n", "label": 1}
{"function": "\n\ndef ui_dialog(ui, parent, is_modal):\n    ' Creates a wxPython dialog box for a specified UI object.\\n\\n    Changes are not immediately applied to the underlying object. The user must\\n    click **Apply** or **OK** to apply changes. The user can revert changes by\\n    clicking **Revert** or **Cancel**.\\n    '\n    if (ui.owner is None):\n        ui.owner = ModalDialog()\n    ui.owner.init(ui, parent, is_modal)\n    ui.control = ui.owner.control\n    ui.control._parent = parent\n    try:\n        ui.prepare_ui()\n    except:\n        ui.control.Destroy()\n        ui.control.ui = None\n        ui.control = None\n        ui.owner = None\n        ui.result = False\n        raise\n    ui.handler.position(ui.info)\n    restore_window(ui)\n    if is_modal:\n        ui.control.ShowModal()\n    else:\n        ui.control.Show()\n", "label": 0}
{"function": "\n\ndef _fit_cg(f, score, start_params, fargs, kwargs, disp=True, maxiter=100, callback=None, retall=False, full_output=True, hess=None):\n    gtol = kwargs.setdefault('gtol', 1e-05)\n    norm = kwargs.setdefault('norm', np.Inf)\n    epsilon = kwargs.setdefault('epsilon', 1.4901161193847656e-08)\n    retvals = optimize.fmin_cg(f, start_params, score, gtol=gtol, norm=norm, epsilon=epsilon, maxiter=maxiter, full_output=full_output, disp=disp, retall=retall, callback=callback)\n    if full_output:\n        if (not retall):\n            (xopt, fopt, fcalls, gcalls, warnflag) = retvals\n        else:\n            (xopt, fopt, fcalls, gcalls, warnflag, allvecs) = retvals\n        converged = (not warnflag)\n        retvals = {\n            'fopt': fopt,\n            'fcalls': fcalls,\n            'gcalls': gcalls,\n            'warnflag': warnflag,\n            'converged': converged,\n        }\n        if retall:\n            retvals.update({\n                'allvecs': allvecs,\n            })\n    else:\n        xopt = retvals\n        retvals = None\n    return (xopt, retvals)\n", "label": 0}
{"function": "\n\ndef get_oauth_signature(self, request):\n    'Get an OAuth signature to be used in signing a request\\n        '\n    if (self.signature_method == SIGNATURE_PLAINTEXT):\n        return signature.sign_plaintext(self.client_secret, self.resource_owner_secret)\n    (uri, headers, body) = self._render(request)\n    collected_params = signature.collect_parameters(uri_query=urlparse.urlparse(uri).query, body=body, headers=headers)\n    logger.debug('Collected params: {0}'.format(collected_params))\n    normalized_params = signature.normalize_parameters(collected_params)\n    normalized_uri = signature.normalize_base_string_uri(request.uri)\n    logger.debug('Normalized params: {0}'.format(normalized_params))\n    logger.debug('Normalized URI: {0}'.format(normalized_uri))\n    base_string = signature.construct_base_string(request.http_method, normalized_uri, normalized_params)\n    logger.debug('Base signing string: {0}'.format(base_string))\n    if (self.signature_method == SIGNATURE_HMAC):\n        sig = signature.sign_hmac_sha1(base_string, self.client_secret, self.resource_owner_secret)\n    elif (self.signature_method == SIGNATURE_RSA):\n        sig = signature.sign_rsa_sha1(base_string, self.rsa_key)\n    else:\n        sig = signature.sign_plaintext(self.client_secret, self.resource_owner_secret)\n    logger.debug('Signature: {0}'.format(sig))\n    return sig\n", "label": 0}
{"function": "\n\ndef test_min_length(self):\n    h = Hashids(min_length=25)\n    assert (h.encode(7452, 2967, 21401) == 'pO3K69b86jzc6krI416enr2B5')\n    assert (h.encode(1, 2, 3) == 'gyOwl4B97bo2fXhVaDR0Znjrq')\n    assert (h.encode(6097) == 'Nz7x3VXyMYerRmWeOBQn6LlRG')\n    assert (h.encode(99, 25) == 'k91nqP3RBe3lKfDaLJrvy8XjV')\n", "label": 0}
{"function": "\n\ndef set_serial_number(self, serial):\n    '\\n        Set the serial number of the certificate.\\n\\n        :param serial: The new serial number.\\n        :type serial: :py:class:`int`\\n\\n        :return: :py:data`None`\\n        '\n    if (not isinstance(serial, _integer_types)):\n        raise TypeError('serial must be an integer')\n    hex_serial = hex(serial)[2:]\n    if (not isinstance(hex_serial, bytes)):\n        hex_serial = hex_serial.encode('ascii')\n    bignum_serial = _ffi.new('BIGNUM**')\n    small_serial = _lib.BN_hex2bn(bignum_serial, hex_serial)\n    if (bignum_serial[0] == _ffi.NULL):\n        set_result = _lib.ASN1_INTEGER_set(_lib.X509_get_serialNumber(self._x509), small_serial)\n        if set_result:\n            _raise_current_error()\n    else:\n        asn1_serial = _lib.BN_to_ASN1_INTEGER(bignum_serial[0], _ffi.NULL)\n        _lib.BN_free(bignum_serial[0])\n        if (asn1_serial == _ffi.NULL):\n            _raise_current_error()\n        asn1_serial = _ffi.gc(asn1_serial, _lib.ASN1_INTEGER_free)\n        set_result = _lib.X509_set_serialNumber(self._x509, asn1_serial)\n        if (not set_result):\n            _raise_current_error()\n", "label": 1}
{"function": "\n\ndef fileUpdated(self, file):\n    '\\n        On file update, if the name or the MIME type changed, we must update\\n        them accordingly on the S3 key so that the file downloads with the\\n        correct name and content type.\\n        '\n    if file.get('imported'):\n        return\n    bucket = self._getBucket()\n    key = bucket.get_key(file['s3Key'], validate=True)\n    if (not key):\n        return\n    disp = ('attachment; filename=\"%s\"' % file['name'])\n    mime = (file.get('mimeType') or '')\n    if ((key.content_type != mime) or (key.content_disposition != disp)):\n        key.set_remote_metadata(metadata_plus={\n            'Content-Type': mime,\n            'Content-Disposition': disp.encode('utf8'),\n        }, metadata_minus=[], preserve_acl=True)\n", "label": 1}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    argvals = list(args)\n    ops = []\n    tapes = set()\n    for (i, arg) in enumerate(args):\n        if isinstance(arg, Node):\n            argvals[i] = arg.value\n            if (i in self.zero_grads):\n                continue\n            for (tape, parent_rnode) in iteritems(arg.tapes):\n                if (not tape.complete):\n                    ops.append((tape, i, parent_rnode))\n                    tapes.add(tape)\n    (result, aux) = self.fun(*argvals, **kwargs)\n    if (result is NotImplemented):\n        return result\n    if ops:\n        result = new_node(result, tapes)\n        for (tape, argnum, parent) in ops:\n            gradfun = self.gradmaker(argnum, aux, result, args, kwargs)\n            rnode = result.tapes[tape]\n            rnode.parent_grad_ops.append((gradfun, parent))\n    return result\n", "label": 1}
{"function": "\n\ndef register_model(self, app_label, model):\n    model_name = model._meta.model_name\n    app_models = self.all_models[app_label]\n    if (model_name in app_models):\n        if ((model.__name__ == app_models[model_name].__name__) and (model.__module__ == app_models[model_name].__module__)):\n            warnings.warn((\"Model '%s.%s' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\" % (app_label, model_name)), RuntimeWarning, stacklevel=2)\n        else:\n            raise RuntimeError((\"Conflicting '%s' models in application '%s': %s and %s.\" % (model_name, app_label, app_models[model_name], model)))\n    app_models[model_name] = model\n    self.do_pending_operations(model)\n    self.clear_cache()\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    self.admin = credentials.UsernamePassword('admin', 'asdf')\n    self.alice = credentials.UsernamePassword('alice', 'foo')\n    self.badPass = credentials.UsernamePassword('alice', 'foobar')\n    self.badUser = credentials.UsernamePassword('x', 'yz')\n    self.checker = strcred.makeChecker('unix')\n    if pwd:\n        database = UserDatabase()\n        for (username, password) in self.users.items():\n            database.addUser(username, crypt.crypt(password, 'F/'), 1000, 1000, username, ('/home/' + username), '/bin/sh')\n        self.patch(pwd, 'getpwnam', database.getpwnam)\n    if spwd:\n        self._spwd_getspnam = spwd.getspnam\n        spwd.getspnam = self._spwd\n", "label": 0}
{"function": "\n\ndef generate(env):\n    'Add Builders and construction variables for gcc to an Environment.'\n    if ('CC' not in env):\n        env['CC'] = (env.Detect(compilers) or compilers[0])\n    cc.generate(env)\n    if (env['PLATFORM'] in ['cygwin', 'win32']):\n        env['SHCCFLAGS'] = SCons.Util.CLVar('$CCFLAGS')\n    else:\n        env['SHCCFLAGS'] = SCons.Util.CLVar('$CCFLAGS -fPIC')\n    version = detect_version(env, env['CC'])\n    if version:\n        env['CCVERSION'] = version\n", "label": 0}
{"function": "\n\ndef _deserialize(self, value, attr, data):\n    if (not self.truthy):\n        return bool(value)\n    else:\n        try:\n            if (value in self.truthy):\n                return True\n            elif (value in self.falsy):\n                return False\n        except TypeError:\n            pass\n    self.fail('invalid')\n", "label": 0}
{"function": "\n\ndef filterOnStatusExt(statusExt, commits):\n    return [commit for commit in commits if (('_' in commit.status) and (commit.status[(commit.status.rfind('_') + 1):] == statusExt))]\n", "label": 0}
{"function": "\n\ndef crossdomain(origin=None, methods=None, headers=None, max_age=21600, attach_to_all=True, automatic_options=True):\n    if (methods is not None):\n        methods = ', '.join(sorted((x.upper() for x in methods)))\n    if ((headers is not None) and (not isinstance(headers, str))):\n        headers = ', '.join((x.upper() for x in headers))\n    if (not isinstance(origin, str)):\n        origin = ', '.join(origin)\n    if isinstance(max_age, timedelta):\n        max_age = max_age.total_seconds()\n\n    def get_methods():\n        if (methods is not None):\n            return methods\n        options_resp = current_app.make_default_options_response()\n        return options_resp.headers['allow']\n\n    def decorator(f):\n\n        def wrapped_function(*args, **kwargs):\n            if (automatic_options and (request.method == 'OPTIONS')):\n                resp = current_app.make_default_options_response()\n            else:\n                resp = make_response(f(*args, **kwargs))\n            if ((not attach_to_all) and (request.method != 'OPTIONS')):\n                return resp\n            h = resp.headers\n            h['Access-Control-Allow-Origin'] = origin\n            h['Access-Control-Allow-Methods'] = get_methods()\n            h['Access-Control-Max-Age'] = str(max_age)\n            if (headers is not None):\n                h['Access-Control-Allow-Headers'] = headers\n            return resp\n        f.provide_automatic_options = False\n        return update_wrapper(wrapped_function, f)\n    return decorator\n", "label": 1}
{"function": "\n\ndef test_registry_uris_param_v2(self):\n    spec = CommonSpec()\n    spec.set_params(registry_uris=['http://registry.example.com:5000/v2'], user=TEST_USER)\n    registry = spec.registry_uris.value[0]\n    assert (registry.uri == 'http://registry.example.com:5000')\n    assert (registry.docker_uri == 'registry.example.com:5000')\n    assert (registry.version == 'v2')\n", "label": 0}
{"function": "\n\ndef test_number_to_string(self):\n    ' Numbers are turned into strings.\\n        '\n    cleaner = Cleaners()\n    in_int = 85\n    in_float = 82.12\n    in_string = 'big frame, small spirit!'\n    in_list = ['hands', 'by', 'the', 'halyards']\n    in_none = None\n    assert (cleaner.number_to_string(in_int) == str(in_int))\n    assert (cleaner.number_to_string(in_float) == str(in_float))\n    assert (cleaner.number_to_string(in_string) == in_string)\n    assert (cleaner.number_to_string(in_list) == in_list)\n    assert (cleaner.number_to_string(in_none) is None)\n", "label": 1}
{"function": "\n\ndef __call__(self, action, *args, **kwargs):\n    module_name = ('%s.%s' % (self.package, action.replace('-', '_')))\n    cwd = kwargs.get('cwd')\n    if cwd:\n        self.chdir(cwd)\n    if kwargs.get('raises'):\n        with pytest.raises(qisys.error.Error) as error:\n            qisys.script.run_action(module_name, args)\n        return str(error.value)\n    if kwargs.get('retcode'):\n        try:\n            qisys.script.run_action(module_name, args)\n        except SystemExit as e:\n            return e.code\n        return 0\n    else:\n        return qisys.script.run_action(module_name, args)\n", "label": 0}
{"function": "\n\ndef GatheringUserDatasGet(self, socketId):\n    command = 'GatheringUserDatasGet(double *,double *,double *,double *,double *,double *,double *,double *)'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(8):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef invert(self):\n    for polygon in self.polygons:\n        polygon.flip()\n    self.plane.flip()\n    if self.front:\n        self.front.invert()\n    if self.back:\n        self.back.invert()\n    (self.front, self.back) = (self.back, self.front)\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.stdout.write('Collecting duplicates...')\n    duplicates = sum([self.get_quota_duplicate_versions(quota) for quota in Quota.objects.all()], [])\n    self.stdout.write('...Done')\n    if (not duplicates):\n        self.stdout.write('No duplicates were found. Congratulations!')\n    else:\n        self.stdout.write(('There are %s duplicates for quotas versions.' % len(duplicates)))\n        while True:\n            delete = (raw_input('  Do you want to delete them? [Y/n]:') or 'y')\n            if (delete.lower() not in ('y', 'n')):\n                self.stdout.write('  Please enter letter \"y\" or \"n\"')\n            else:\n                delete = (delete.lower() == 'y')\n                break\n        if delete:\n            for duplicate in duplicates:\n                duplicate.delete()\n            self.stdout.write('All duplicates were deleted.')\n        else:\n            self.stdout.write('Duplicates were not deleted.')\n", "label": 1}
{"function": "\n\ndef start_upload_workers(self, files, uploaded_queue, boundary, local_deploy):\n    num_files = len(files)\n    num_workers = 4\n    if (num_workers > num_files):\n        num_workers = num_files\n    start = 0\n    step = int(((num_files + (num_workers - 1)) / num_workers))\n    for _ in range(num_workers):\n        end = (start + step)\n        if (end > num_files):\n            end = num_files\n        Thread(target=self.post_files, args=[files, start, end, uploaded_queue.put, boundary, local_deploy]).start()\n        start = end\n", "label": 0}
{"function": "\n\ndef next(self):\n    if ((self.next_cursor == 0) or (self.limit and (self.count == self.limit))):\n        raise StopIteration\n    (data, cursors) = self.method(*self.args, cursor=self.next_cursor, **self.kargs)\n    (self.prev_cursor, self.next_cursor) = cursors\n    if (len(data) == 0):\n        raise StopIteration\n    self.count += 1\n    return data\n", "label": 0}
{"function": "\n\ndef parse_output(output):\n    'Parse fabric output and return the output per host'\n    line_pattern = re.compile('\\\\[(?P<host>.+)\\\\] out: (?P<line>.+)')\n    results = {\n        \n    }\n    for line in output:\n        m = line_pattern.match(line)\n        if m:\n            host = m.group('host')\n            if (host not in results):\n                results[host] = []\n            line = m.group('line').strip()\n            if line:\n                results[host].append(line)\n    return results\n", "label": 0}
{"function": "\n\ndef save_template_dict(self, templ_dict):\n    if templ_dict:\n        try:\n            self.es.index(index=(self.project_name + '-crawler-template_dict'), doc_type='template_dict', id=self.plugin_name, body=json.dumps({repr(k): v for (k, v) in templ_dict.items()}))\n        except:\n            self.es.update(index=(self.project_name + '-crawler-template_dict'), doc_type='template_dict', id=self.plugin_name, body=json.dumps({\n                'doc': {repr(k): v for (k, v) in templ_dict.items()},\n            }))\n", "label": 0}
{"function": "\n\ndef vim_choice(prompt, default, choices):\n    default = (choices.index(default) + 1)\n    choices_str = '\\n'.join([('&%s' % choice) for choice in choices])\n    try:\n        choice = int(vim.eval(('confirm(\"%s\", \"%s\", %s)' % (prompt, choices_str, default))))\n    except KeyboardInterrupt:\n        return None\n    if (choice == 0):\n        return None\n    return choices[(choice - 1)]\n", "label": 0}
{"function": "\n\ndef get_submitted_exams(course, student):\n    try:\n        exams = Exam.objects.filter(course=course).order_by('exam_num')\n    except Exam.DoesNotExist:\n        exams = None\n    try:\n        submitted_exams = ExamSubmission.objects.filter(exam__course=course, student=student)\n    except ExamSubmission.DoesNotExist:\n        submitted_exams = None\n    if (len(exams) != len(submitted_exams)):\n        for exam in exams:\n            found_exam = False\n            for submitted_exam in submitted_exams:\n                if (exam.id == submitted_exam.exam_id):\n                    found_exam = True\n            if (not found_exam):\n                submission = ExamSubmission.objects.create(student=student, exam=exam)\n                submission.save()\n        submitted_exams = ExamSubmission.objects.filter(exam__course=course, student=student)\n    return submitted_exams\n", "label": 1}
{"function": "\n\ndef testExceptions(self):\n    request = roots.Request()\n    try:\n        request.write(b'blah')\n    except NotImplementedError:\n        pass\n    else:\n        self.fail()\n    try:\n        request.finish()\n    except NotImplementedError:\n        pass\n    else:\n        self.fail()\n", "label": 0}
{"function": "\n\ndef execute(self, cmd):\n    if (len(cmd) > 237):\n        print_error('Your command must be at most 237 characters long. Longer strings might crash the server.')\n        return\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    sock.bind(('0.0.0.0', 9999))\n    sock.settimeout(2)\n    packet = ((((b'\\x0c\\x153\\x00' + os.urandom(4)) + (b'\\x00' * 38)) + struct.pack('<H', len(cmd))) + cmd).ljust(512, b'\\x00')\n    try:\n        sock.sendto(packet, (self.target, 9999))\n    except socket.error:\n        return ''\n    while True:\n        try:\n            (data, addr) = sock.recvfrom(512)\n        except socket.timeout:\n            sock.close()\n            raise\n        if ((len(data) == 512) and (data[1] == '\\x16')):\n            break\n    length = struct.unpack('<H', data[14:16])[0]\n    output = data[16:(16 + length)]\n    sock.close()\n    return output\n", "label": 1}
{"function": "\n\ndef group_connections(connections):\n    \"\\n    Return a list of (language code, respective connections) pairs, while\\n    using Django's translation.override() to set each language.\\n    \"\n    grouped_conns = defaultdict(list)\n    if isinstance(connections, QuerySet):\n        languages = connections.values_list('contact__language', flat=True)\n        for language in languages.distinct():\n            lang_conns = connections.filter(contact__language=language)\n            grouped_conns[language].extend(lang_conns)\n    else:\n        for connection in connections:\n            language = connection.contact.language\n            grouped_conns[language].append(connection)\n    for (lang, conns) in grouped_conns.items():\n        (yield (lang, conns))\n", "label": 0}
{"function": "\n\ndef get_user(self, check_permissions=True):\n    key = self.kwargs[self.user_lookup_url_kwarg]\n    current_user = self.request.user\n    if (key == 'me'):\n        if isinstance(current_user, AnonymousUser):\n            raise NotAuthenticated\n        else:\n            return self.request.user\n    obj = get_object_or_error(User, key, 'user')\n    if check_permissions:\n        self.check_object_permissions(self.request, obj)\n    return obj\n", "label": 0}
{"function": "\n\n@log\ndef getattr(self, path, fh=None):\n    (uid, gid, pid) = fuse_get_context()\n    working_path = path\n    if is_special_file(path):\n        (working_path, special) = explode_special_file(working_path)\n    record = self._get_record(working_path)\n    if record.is_directory():\n        mode = (stat.S_IFDIR | PERMISSION_ALL_READ)\n        nlink = 2\n    else:\n        mode = (stat.S_IFREG | PERMISSION_ALL_READ)\n        nlink = 1\n    if is_special_file(path):\n        size = 0\n        (working_path, special) = explode_special_file(path)\n        if (special == 'meta'):\n            node = self._get_node(working_path)\n            record_buf = self._enumerator.get_record_buf(node.get_record_number())\n            size = len(get_meta_for_file(record, working_path))\n    else:\n        data_attribute = record.data_attribute()\n        if (data_attribute is not None):\n            if (data_attribute.non_resident() == 0):\n                size = len(data_attribute.value())\n            else:\n                size = data_attribute.data_size()\n        else:\n            size = record.filename_information().logical_size()\n    return {\n        'st_atime': unixtimestamp(record.standard_information().accessed_time()),\n        'st_ctime': unixtimestamp(record.standard_information().changed_time()),\n        'st_mtime': unixtimestamp(record.standard_information().modified_time()),\n        'st_size': size,\n        'st_uid': uid,\n        'st_gid': gid,\n        'st_mode': mode,\n        'st_nlink': nlink,\n    }\n", "label": 1}
{"function": "\n\ndef deploy_file(self, file_name, calc_md5=True, calc_sha1=True, parameters={\n    \n}):\n    '\\n        Upload the given file to this path\\n        '\n    if calc_md5:\n        md5 = md5sum(file_name)\n    if calc_sha1:\n        sha1 = sha1sum(file_name)\n    target = self\n    if self.is_dir():\n        target = (self / pathlib.Path(file_name).name)\n    with open(file_name, 'rb') as fobj:\n        target.deploy(fobj, md5, sha1, parameters)\n", "label": 0}
{"function": "\n\ndef gradient(self, x, Y):\n    '\\n        Computes the gradient of the Polynomial kernel wrt. to the left argument, i.e.\\n        \\nabla_x k(x,y)=\\nabla_x (1+x^Ty)^d=d(1+x^Ty)^(d-1) y\\n        \\n        x - single sample on right hand side (1D vector)\\n        Y - samples on left hand side (2D matrix)\\n        '\n    assert (len(x.shape) == 1)\n    assert (len(Y.shape) == 2)\n    assert (len(x) == Y.shape[1])\n    return ((self.degree * pow((1 + x.dot(Y.T)), self.degree)) * Y)\n", "label": 0}
{"function": "\n\ndef _update(self, url, data, kwargs, coerce):\n    params = {\n        \n    }\n    for k in data.keys():\n        if (data[k] is None):\n            del data[k]\n    if (('who' in kwargs) and (kwargs['who'] is not None)):\n        params['_who'] = kwargs['who']\n    if (('why' in kwargs) and (kwargs['why'] is not None)):\n        params['_why'] = kwargs['why']\n    headers = kwargs.get('headers', {\n        \n    })\n    resp = self.connection.request(url, method='PUT', params=params, data=data, headers=headers)\n    if (resp.status == httplib.NO_CONTENT):\n        location = resp.headers.get('location')\n        if (not location):\n            raise LibcloudError('Missing location header')\n        obj_ids = self._url_to_obj_ids(location)\n        return coerce(**obj_ids)\n    else:\n        raise LibcloudError(('Unexpected status code: %s' % resp.status))\n", "label": 1}
{"function": "\n\ndef to_json(s):\n    'Return a valid json string, given a jsarray string.\\n\\n    :param s: string of jsarray data\\n    '\n    out = []\n    for t in generate_tokens(StringIO(s).readline):\n        if (out and any(((',' == t[1] == out[(- 1)]), ((out[(- 1)] == '[') and (t[1] == ','))))):\n            out.append('null')\n        out.append(t[1])\n    return ''.join(out)\n", "label": 0}
{"function": "\n\ndef viewRssFeed(self, modelDocument, parentNode):\n    self.id = 1\n    for rssItem in modelDocument.rssItems:\n        node = self.treeView.insert(parentNode, 'end', rssItem.objectId(), text=(rssItem.companyName or ''), tags=(('odd' if (self.id & 1) else 'even'),))\n        self.treeView.set(node, 'form', rssItem.formType)\n        self.treeView.set(node, 'filingDate', rssItem.filingDate)\n        self.treeView.set(node, 'cik', rssItem.cikNumber)\n        self.treeView.set(node, 'status', rssItem.status)\n        self.treeView.set(node, 'period', rssItem.period)\n        self.treeView.set(node, 'fiscalYrEnd', rssItem.fiscalYearEnd)\n        self.treeView.set(node, 'results', (' '.join((str(result) for result in (rssItem.results or []))) + ((' ' + str(rssItem.assertions)) if rssItem.assertions else '')))\n        self.id += 1\n    else:\n        pass\n", "label": 1}
{"function": "\n\ndef convert_from_timestamp(timestamp, to_format, to_type=str):\n    if (to_type == str):\n        return converter.timestamp_to_string(timestamp, to_format)\n    elif (to_type == datetime):\n        return converter.timestamp_to_datetime(timestamp, to_format)\n    elif (to_type == date):\n        return converter.timestamp_to_date(timestamp)\n", "label": 0}
{"function": "\n\ndef __init__(self, path=None):\n    self.path = os.path.abspath((path or os.path.curdir))\n    try:\n        self.master = 'origin/master'\n        master_sha = self.shell('git', 'log', '-1', '--pretty=format:%H', self.master, exceptions=True).split\n    except:\n        self.master = 'master'\n        master_sha = self.shell('git', 'log', '-1', '--pretty=format:%H', self.master).split\n    self.master_sha = ((master_sha and master_sha[0].strip()) or '')\n", "label": 0}
{"function": "\n\ndef _describe_table(self, connection, table, charset=None, full_name=None):\n    'Run DESCRIBE for a ``Table`` and return processed rows.'\n    if (full_name is None):\n        full_name = self.identifier_preparer.format_table(table)\n    st = ('DESCRIBE %s' % full_name)\n    (rp, rows) = (None, None)\n    try:\n        try:\n            rp = connection.execution_options(skip_user_error_events=True).execute(st)\n        except exc.DBAPIError as e:\n            if (self._extract_error_code(e.orig) == 1146):\n                raise exc.NoSuchTableError(full_name)\n            else:\n                raise\n        rows = self._compat_fetchall(rp, charset=charset)\n    finally:\n        if rp:\n            rp.close()\n    return rows\n", "label": 0}
{"function": "\n\ndef get_model(app_label, model_name):\n    '\\n    Given an app label and a model name, returns the corresponding model class.\\n    '\n    try:\n        return apps.get_model(app_label, model_name)\n    except AppRegistryNotReady:\n        if (apps.apps_ready and (not apps.models_ready)):\n            app_config = apps.get_app_config(app_label)\n            import_module(('%s.%s' % (app_config.name, MODELS_MODULE_NAME)))\n            return apps.get_registered_model(app_label, model_name)\n        else:\n            raise\n", "label": 0}
{"function": "\n\n@contextfunction\ndef htform(context, form):\n    'Set time zone'\n    request = context['request']\n    user = None\n    if request.user.username:\n        try:\n            user = request.user.profile\n        except Exception:\n            pass\n    default_timezone = settings.HARDTREE_SERVER_DEFAULT_TIMEZONE\n    try:\n        conf = ModuleSetting.get('default_timezone')[0]\n        default_timezone = conf.value\n    except:\n        pass\n    try:\n        conf = ModuleSetting.get('default_timezone', user=user)[0]\n        default_timezone = conf.value\n    except Exception:\n        default_timezone = getattr(settings, 'HARDTREE_SERVER_TIMEZONE')[default_timezone][0]\n    all_timezones = getattr(settings, 'HARDTREE_SERVER_TIMEZONE', [(1, '(GMT-11:00) International Date Line West')])\n    title = all_timezones[int(default_timezone)][1]\n    GMT = title[4:10]\n    sign = GMT[0:1]\n    hours = int(GMT[1:3])\n    mins = int(GMT[4:6])\n    if (not form.errors):\n        for field in form:\n            try:\n                date = datetime.strptime(str(field.form.initial[field.name]), '%Y-%m-%d %H:%M:%S')\n                if date:\n                    if (sign == '-'):\n                        field.form.initial[field.name] = (date - timedelta(hours=hours, minutes=mins))\n                    else:\n                        field.form.initial[field.name] = (date + timedelta(hours=hours, minutes=mins))\n            except:\n                pass\n    return form\n", "label": 1}
{"function": "\n\ndef _retry_reboot(self, context, instance):\n    current_power_state = self._get_power_state(context, instance)\n    current_task_state = instance.task_state\n    retry_reboot = False\n    reboot_type = compute_utils.get_reboot_type(current_task_state, current_power_state)\n    pending_soft = ((current_task_state == task_states.REBOOT_PENDING) and (instance.vm_state in vm_states.ALLOW_SOFT_REBOOT))\n    pending_hard = ((current_task_state == task_states.REBOOT_PENDING_HARD) and (instance.vm_state in vm_states.ALLOW_HARD_REBOOT))\n    started_not_running = ((current_task_state in [task_states.REBOOT_STARTED, task_states.REBOOT_STARTED_HARD]) and (current_power_state != power_state.RUNNING))\n    if (pending_soft or pending_hard or started_not_running):\n        retry_reboot = True\n    return (retry_reboot, reboot_type)\n", "label": 1}
{"function": "\n\ndef mouseMoveEvent(self, event):\n    channel = None\n    point = None\n    if self.cur_drag:\n        channel = self.cur_drag[0]\n        point = self.cur_drag[1]\n        if (not point.fixed):\n            point.set_pos(channel.get_index_pos(event.x()))\n            point.activate_channels(self.active_channels_string)\n            self.table.sort_control_points()\n        channel.set_value_index(point.color, event.y())\n        self.table_config_changed(final_update=False)\n    screenX = event.x()\n    screenY = event.y()\n    (width, height) = (self.size().width(), self.size().height())\n    master = self.master\n    (s1, s2) = master.get_table_range()\n    if (channel is not None):\n        name = self.text_map[channel.name]\n        pos = (s1 + ((s2 - s1) * point.pos))\n        val = channel.get_value(point.color)\n        txt = ('%s: (%.3f, %.3f)' % (name, pos, val))\n    else:\n        x = (s1 + (((s2 - s1) * float(screenX)) / (width - 1)))\n        y = (1.0 - (float(screenY) / (height - 1)))\n        txt = ('position: (%.3f, %.3f)' % (x, y))\n    self.master.set_status_text(txt)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(UserSettingsForm, self).__init__(*args, **kwargs)\n\n    def get_language_display_name(code, desc):\n        try:\n            desc = translation.get_language_info(code)['name_local']\n            desc = string.capwords(desc)\n        except KeyError:\n            pass\n        return ('%s (%s)' % (desc, code))\n    languages = [(k, get_language_display_name(k, v)) for (k, v) in settings.LANGUAGES]\n    self.fields['language'].choices = languages\n    timezones = []\n    language = translation.get_language()\n    current_locale = translation.to_locale(language)\n    babel_locale = babel.Locale.parse(current_locale)\n    for (tz, offset) in self._sorted_zones():\n        try:\n            utc_offset = (_('UTC %(hour)s:%(min)s') % {\n                'hour': offset[:3],\n                'min': offset[3:],\n            })\n        except Exception:\n            utc_offset = ''\n        if (tz == 'UTC'):\n            tz_name = _('UTC')\n        elif (tz == 'GMT'):\n            tz_name = _('GMT')\n        else:\n            tz_label = babel.dates.get_timezone_location(tz, locale=babel_locale)\n            tz_name = (_('%(offset)s: %(label)s') % {\n                'offset': utc_offset,\n                'label': tz_label,\n            })\n        timezones.append((tz, tz_name))\n    self.fields['timezone'].choices = timezones\n", "label": 1}
{"function": "\n\ndef solve(self, rhs_mat, system, mode):\n    \" Solves the linear system for the problem in self.system. The\\n        full solution vector is returned.\\n\\n        Args\\n        ----\\n        rhs_mat : dict of ndarray\\n            Dictionary containing one ndarry per top level quantity of\\n            interest. Each array contains the right-hand side for the linear\\n            solve.\\n\\n        system : `System`\\n            Parent `System` object.\\n\\n        mode : string\\n            Derivative mode, can be 'fwd' or 'rev'.\\n\\n        Returns\\n        -------\\n        dict of ndarray : Solution vectors\\n        \"\n    self.system = system\n    if (self.mode is None):\n        self.mode = mode\n    sol_buf = OrderedDict()\n    for (voi, rhs) in rhs_mat.items():\n        self.voi = None\n        if system._jacobian_changed:\n            self.jacobian = self._assemble_jacobian(rhs, mode)\n            system._jacobian_changed = False\n            if (self.options['solve_method'] == 'LU'):\n                self.lup = lu_factor(self.jacobian)\n        if (self.options['solve_method'] == 'LU'):\n            deriv = lu_solve(self.lup, rhs)\n        else:\n            deriv = np.linalg.solve(self.jacobian, rhs)\n        self.system = None\n        sol_buf[voi] = deriv\n    return sol_buf\n", "label": 1}
{"function": "\n\ndef test_vector_norm():\n    x = np.arange(30).reshape((5, 6))\n    s = symbol('x', discover(x))\n    assert eq(compute(s.vnorm(), x), np.linalg.norm(x))\n    assert eq(compute(s.vnorm(ord=1), x), np.linalg.norm(x.flatten(), ord=1))\n    assert eq(compute(s.vnorm(ord=4, axis=0), x), np.linalg.norm(x, ord=4, axis=0))\n    expr = s.vnorm(ord=4, axis=0, keepdims=True)\n    assert (expr.shape == compute(expr, x).shape)\n", "label": 0}
{"function": "\n\ndef assert_no_warnings(func, *args, **kw):\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        result = func(*args, **kw)\n        if hasattr(np, 'VisibleDeprecationWarning'):\n            w = [e for e in w if (e.category is not np.VisibleDeprecationWarning)]\n        if (len(w) > 0):\n            raise AssertionError(('Got warnings when calling %s: %s' % (func.__name__, w)))\n    return result\n", "label": 0}
{"function": "\n\ndef get_dots_case_json(casedoc, anchor_date=None):\n    '\\n    Return JSON-ready array of the DOTS block for given patient.\\n    Pulling properties from PATIENT document.\\n    Patient document trumps casedoc in this use case.\\n    '\n    if (anchor_date is None):\n        anchor_date = datetime.now(tz=timezone(settings.TIME_ZONE))\n    enddate = anchor_date\n    ret = {\n        'regimens': [int((getattr(casedoc, CASE_NONART_REGIMEN_PROP, None) or 0)), int((getattr(casedoc, CASE_ART_REGIMEN_PROP, None) or 0))],\n        'regimen_labels': [list(casedoc.nonart_labels), list(casedoc.art_labels)],\n        'days': [],\n        'anchor': anchor_date.strftime('%d %b %Y'),\n    }\n    observations = query_observations(casedoc._id, (enddate - timedelta(days=DOT_DAYS_INTERVAL)), enddate)\n    for delta in range(DOT_DAYS_INTERVAL):\n        obs_date = (enddate - timedelta(days=delta))\n        day_arr = filter_obs_for_day(obs_date.date(), observations)\n        day_data = DOTDay.merge_from_observations(day_arr)\n        ret['days'].append(day_data.to_case_json(casedoc, ret['regimen_labels']))\n    ret['days'].reverse()\n    return ret\n", "label": 0}
{"function": "\n\ndef value__get(self):\n    if (self.selectedIndex is not None):\n        return self.options[self.selectedIndex][0]\n    else:\n        for (option, checked) in self.options:\n            if checked:\n                return option\n        else:\n            if self.options:\n                return self.options[0][0]\n            else:\n                return None\n", "label": 1}
{"function": "\n\ndef compute_files(hive_holder, output, settings):\n    ' given the current hive_holder, compute the files that have to be saved in disk\\n    return: {BlockCellName: StrOrBytes to be saved in disk}\\n    param output: something that supports info, warn, error\\n    '\n    new_files = {\n        \n    }\n    for (block_cell_name, (cell, content)) in hive_holder.resources.iteritems():\n        if isinstance(cell, VirtualCell):\n            try:\n                target = cell.evaluate(settings)\n            except ConfigurationFileError as e:\n                output.error(('Error evaluating virtual %s: %s' % (block_cell_name, e.message)))\n                continue\n            content = hive_holder[target.block_name][target.cell_name].content\n            new_files[block_cell_name] = content.load.load\n        elif content.blob_updated:\n            new_files[block_cell_name] = content.load.load\n    return new_files\n", "label": 0}
{"function": "\n\ndef get_available_screens():\n    '\\n    Gets the available screens in this package for dynamic instantiation.\\n    '\n    ignore_list = ['__init__.py']\n    screens = []\n    for module in os.listdir(os.path.join(os.path.dirname(__file__))):\n        if ((module in ignore_list) or (module[(- 3):] != '.py')):\n            continue\n        module_name = module[:(- 3)]\n        m = __import__(module_name, globals(), locals())\n        for (name, obj) in inspect.getmembers(m):\n            if (inspect.isclass(obj) and issubclass(obj, base.ScreenBase) and (not name.endswith('Base'))):\n                screens.append(obj)\n    screens.extend(get_available_plugin_screens())\n    return screens\n", "label": 1}
{"function": "\n\ndef finalize(self):\n    'finalizing this Report sends off the email.'\n    self.write(self._formatter.finalize())\n    report = ezmail.MIMEText.MIMEText(self._fo.getvalue(), self._formatter.MIMETYPE.split('/')[1])\n    report['Content-Disposition'] = 'inline'\n    self._message.attach(report)\n    if (self._attach_logfile and self._logfile):\n        try:\n            lfd = open(self._logfile, 'rb').read()\n        except:\n            pass\n        else:\n            logmsg = ezmail.MIMEText.MIMEText(lfd, charset=chardet.detect(lfd))\n            logmsg['Content-Disposition'] = ('attachment; filename=%s' % (os.path.basename(self._logfile),))\n            self._message.attach(logmsg)\n    ezmail.mail(self._message)\n", "label": 0}
{"function": "\n\ndef _update_project_members(self, request, data, project_id):\n    users_to_add = 0\n    try:\n        available_roles = api.keystone.role_list(request)\n        member_step = self.get_step(PROJECT_USER_MEMBER_SLUG)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_to_add += len(role_list)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_added = 0\n            for user in role_list:\n                api.keystone.add_tenant_user_role(request, project=project_id, user=user, role=role.id)\n                users_added += 1\n            users_to_add -= users_added\n    except Exception:\n        if PROJECT_GROUP_ENABLED:\n            group_msg = _(', add project groups')\n        else:\n            group_msg = ''\n        exceptions.handle(request, (_('Failed to add %(users_to_add)s project members%(group_msg)s and set project quotas.') % {\n            'users_to_add': users_to_add,\n            'group_msg': group_msg,\n        }))\n", "label": 1}
{"function": "\n\ndef _execute_multi_task(self, gen, executor, task):\n    if task.unordered:\n        results_gen = self._execute_multi_gen_task(gen, executor, task)\n        return gen.send(results_gen)\n    future_tasks = [executor.submit(t) for t in task.tasks]\n    while True:\n        if (not task.wait(executor, future_tasks, self.engine.pool_timeout)):\n            self.engine.update_gui()\n        else:\n            break\n    if task.skip_errors:\n        results = []\n        for f in future_tasks:\n            try:\n                results.append(f.result())\n            except Exception:\n                pass\n    else:\n        try:\n            results = [f.result() for f in future_tasks]\n        except Exception:\n            return gen.throw(*sys.exc_info())\n    return gen.send(results)\n", "label": 1}
{"function": "\n\ndef test_alphabet(self):\n    h = Hashids(alphabet='!\"#%&\\',-/0123456789:;<=>ABCDEFGHIJKLMNOPQRSTUVWXYZ_`abcdefghijklmnopqrstuvwxyz~')\n    assert (h.encode(2839, 12, 32, 5) == '_nJUNTVU3')\n    assert (h.encode(1, 2, 3) == '7xfYh2')\n    assert (h.encode(23832) == 'Z6R>')\n    assert (h.encode(99, 25) == 'AYyIB')\n", "label": 0}
{"function": "\n\ndef store_current():\n    '\\n    Stores the current logs in the database\\n    '\n    for (key, getwork_c) in getworks.items():\n        accepted_c = accepted.get(key, 0)\n        rejected_c = rejected.get(key, 0)\n        server = key[0]\n        username = key[1]\n        password = key[2]\n        difficulty = key[3]\n        timestamp = time.asctime(time.gmtime())\n        try:\n            if (get_diff(server) != difficulty):\n                continue\n        except:\n            logging.error(traceback.format_exc())\n            continue\n        sql = (\"UPDATE Statistics SET Getworks = %s, Accepted = %s, Rejected = %s, Timestamp = '%s' WHERE Server = '%s' AND Username = '%s' AND Password = '%s' AND Difficulty = %s\" % (getwork_c, accepted_c, rejected_c, timestamp, server, username, password, difficulty))\n        bitHopper.Database.execute(sql)\n        result = bitHopper.Database.execute(('SELECT Getworks from Statistics WHERE Server = \"%s\" AND Username = \"%s\" AND Password = \"%s\" AND Difficulty = %s' % (server, username, password, difficulty)))\n        result = list(result)\n        if (len(result) == 0):\n            sql = (\"INSERT INTO Statistics (Server, Username, Password, Difficulty, Timestamp, Getworks, Accepted, Rejected) VALUES ('%s', '%s', '%s', %s, '%s', %s, %s, %s)\" % (server, username, password, difficulty, timestamp, getwork_c, accepted_c, rejected_c))\n            bitHopper.Database.execute(sql)\n", "label": 0}
{"function": "\n\ndef nested_update(d, u):\n    for (k, v) in u.iteritems():\n        if isinstance(v, collections.Mapping):\n            r = nested_update(d.get(k, {\n                \n            }), v)\n            d[k] = r\n        elif isinstance(v, collections.Iterable):\n            try:\n                d[k].extend(u[k])\n            except KeyError:\n                d[k] = u[k]\n        else:\n            d[k] = u[k]\n    return d\n", "label": 0}
{"function": "\n\ndef fix_file(filename, options=None, output=None):\n    if (not options):\n        options = parse_args([filename])\n    original_source = readlines_from_file(filename)\n    fixed_source = original_source\n    if (options.in_place or output):\n        encoding = detect_encoding(filename)\n    if output:\n        output = codecs.getwriter(encoding)((output.buffer if hasattr(output, 'buffer') else output))\n        output = LineEndingWrapper(output)\n    fixed_source = fix_lines(fixed_source, options, filename=filename)\n    if options.diff:\n        new = io.StringIO(fixed_source)\n        new = new.readlines()\n        diff = get_diff_text(original_source, new, filename)\n        if output:\n            output.write(diff)\n            output.flush()\n        else:\n            return diff\n    elif options.in_place:\n        fp = open_with_encoding(filename, encoding=encoding, mode='w')\n        fp.write(fixed_source)\n        fp.close()\n    elif output:\n        output.write(fixed_source)\n        output.flush()\n    else:\n        return fixed_source\n", "label": 1}
{"function": "\n\ndef _check_var(self, doc):\n    '\\n        Run checks on the variable whose documentation is C{var} and\\n        whose name is C{name}.\\n        \\n        @param doc: The documentation for the variable to check.\\n        @type doc: L{APIDoc}\\n        @rtype: C{None}\\n        '\n    if (self._checks & DocChecker.VAR):\n        if ((self._checks & (DocChecker.DESCR | DocChecker.TYPE)) and (doc.descr in (None, UNKNOWN)) and (doc.type_descr in (None, UNKNOWN)) and (doc.docstring in (None, UNKNOWN))):\n            self.warning('Undocumented', doc)\n        else:\n            if ((self._checks & DocChecker.DESCR) and (doc.descr in (None, UNKNOWN))):\n                self.warning('No description', doc)\n            if ((self._checks & DocChecker.TYPE) and (doc.type_descr in (None, UNKNOWN))):\n                self.warning('No type information', doc)\n", "label": 1}
{"function": "\n\ndef validate_prepopulated_fields(self, cls, model):\n    ' Validate that prepopulated_fields if a dictionary  containing allowed field types. '\n    if hasattr(cls, 'prepopulated_fields'):\n        check_isdict(cls, 'prepopulated_fields', cls.prepopulated_fields)\n        for (field, val) in cls.prepopulated_fields.items():\n            f = get_field(cls, model, 'prepopulated_fields', field)\n            if isinstance(f, (models.DateTimeField, models.ForeignKey, models.ManyToManyField)):\n                raise ImproperlyConfigured((\"'%s.prepopulated_fields['%s']' is either a DateTimeField, ForeignKey or ManyToManyField. This isn't allowed.\" % (cls.__name__, field)))\n            check_isseq(cls, (\"prepopulated_fields['%s']\" % field), val)\n            for (idx, f) in enumerate(val):\n                get_field(cls, model, (\"prepopulated_fields['%s'][%d]\" % (field, idx)), f)\n", "label": 0}
{"function": "\n\ndef main(argv=None):\n    argv = sys.argv\n    path = os.path.abspath(os.path.dirname(__file__))\n    if ((len(argv) == 1) or (argv[1] == '--help') or (argv[1] == '-h')):\n        print(globals()['__doc__'])\n        map_keyword2script = mapKeyword2Script(path)\n        if (len(argv) <= 2):\n            print('CGAT tools are grouped by keywords. The following keywords')\n            print('are defined:\\n')\n            print(('%s\\n' % printListInColumns(map_keyword2script.keys(), 3)))\n        if ('all' in argv[2:]):\n            print('The list of all available commands is:\\n')\n            print(('%s\\n' % printListInColumns(sorted([os.path.basename(x)[:(- 3)] for x in glob.glob(os.path.join(path, '*.py'))]), 3)))\n        else:\n            for arg in argv[2:]:\n                if (arg in map_keyword2script):\n                    print((\"Tools matching the keyword '%s':\\n\" % arg))\n                    print(('%s\\n' % printListInColumns(sorted(map_keyword2script[arg]), 3)))\n        return\n    command = argv[1]\n    (file, pathname, description) = imp.find_module(command, [path])\n    module = imp.load_module(command, file, pathname, description)\n    del sys.argv[0]\n    module.main(sys.argv)\n", "label": 1}
{"function": "\n\ndef _LineContainsI18n(uwline):\n    'Return true if there are i18n comments or function calls in the line.\\n\\n  I18n comments and pseudo-function calls are closely related. They cannot\\n  be moved apart without breaking i18n.\\n\\n  Arguments:\\n    uwline: (unwrapped_line.UnwrappedLine) The line currently being formatted.\\n\\n  Returns:\\n    True if the line contains i18n comments or function calls. False otherwise.\\n  '\n    if style.Get('I18N_COMMENT'):\n        for tok in uwline.tokens:\n            if (tok.is_comment and re.match(style.Get('I18N_COMMENT'), tok.value)):\n                return True\n    if style.Get('I18N_FUNCTION_CALL'):\n        length = len(uwline.tokens)\n        index = 0\n        while (index < (length - 1)):\n            if ((uwline.tokens[(index + 1)].value == '(') and (uwline.tokens[index].value in style.Get('I18N_FUNCTION_CALL'))):\n                return True\n            index += 1\n    return False\n", "label": 1}
{"function": "\n\n@classmethod\ndef invalidate(cls, region):\n    'Invalidate an entire region\\n\\n        .. note::\\n\\n            This does not actually *clear* the region of data, but\\n            just sets the value to expire on next access.\\n\\n        :param region: Region name\\n        :type region: string\\n\\n        '\n    redis = global_connection.redis\n    namespaces = redis.smembers(('retools:%s:namespaces' % region))\n    if (not namespaces):\n        return None\n    longest_expire = max([x['expires'] for x in CacheRegion.regions.values()])\n    new_created = ((time.time() - longest_expire) - 3600)\n    for ns in namespaces:\n        cache_keyset_key = ('retools:%s:%s:keys' % (region, ns))\n        keys = (set(['']) | redis.smembers(cache_keyset_key))\n        for key in keys:\n            cache_key = ('retools:%s:%s:%s' % (region, ns, key))\n            if (not redis.exists(cache_key)):\n                redis.srem(cache_keyset_key, key)\n            else:\n                redis.hset(cache_key, 'created', new_created)\n", "label": 1}
{"function": "\n\n@register.simple_tag(takes_context=True)\ndef render_paginator(context, page, page_var='page', hashtag=''):\n    query_dict = context['request'].GET.copy()\n    try:\n        del query_dict[page_var]\n    except KeyError:\n        pass\n    extra_query = ''\n    if query_dict:\n        extra_query = ('&%s' % query_dict.urlencode())\n    if hashtag:\n        hashtag = ('#%s' % hashtag)\n    new_context = {\n        'page': page,\n        'page_var': page_var,\n        'hashtag': hashtag,\n        'extra_query': extra_query,\n    }\n    if isinstance(page, Page):\n        template = 'spirit/utils/paginator/_paginator.html'\n    else:\n        template = 'spirit/utils/paginator/_yt_paginator.html'\n    return render_to_string(template, new_context)\n", "label": 0}
{"function": "\n\ndef encodeFiles(array):\n    IDfile = readIDfile()\n    dictionary = readDictionaryFile()\n    for thisfile in array:\n        try:\n            input = pickle.load(open(thisfile))\n        except ValueError:\n            logging.warn(((('unable to unpicked ' + thisfile) + '... dangerously just ') + 'skipping, some texts may be lost'))\n            continue\n        except:\n            logging.warn((('Some problem: fix if ' + thisfile) + ' should be unpicklable'))\n            continue\n        for level in input.levels:\n            input.encode(level, IDfile, dictionary)\n", "label": 0}
{"function": "\n\ndef LoadServerCertificate(self, server_certificate=None, ca_certificate=None):\n    'Loads and verifies the server certificate.'\n    try:\n        server_cert = X509.load_cert_string(str(server_certificate))\n        ca_cert = X509.load_cert_string(str(ca_certificate))\n        if (server_cert.verify(ca_cert.get_pubkey()) != 1):\n            self.server_name = None\n            raise IOError('Server cert is invalid.')\n        server_cert_serial = server_cert.get_serial_number()\n        if (server_cert_serial < config_lib.CONFIG['Client.server_serial_number']):\n            raise IOError('Server cert is too old.')\n        elif (server_cert_serial > config_lib.CONFIG['Client.server_serial_number']):\n            logging.info('Server serial number updated to %s', server_cert_serial)\n            config_lib.CONFIG.Set('Client.server_serial_number', server_cert_serial)\n            config_lib.CONFIG.Write()\n    except X509.X509Error:\n        raise IOError('Server cert is invalid.')\n    self.server_name = self.pub_key_cache.GetCNFromCert(server_cert)\n    self.server_certificate = server_certificate\n    self.ca_certificate = ca_certificate\n    self.pub_key_cache.Put(self.server_name, self.pub_key_cache.PubKeyFromCert(server_cert))\n", "label": 0}
{"function": "\n\ndef print_ctypes_struct(struct, name='', ident=0, hexa=False):\n    if isinstance(struct, _ctypes._Pointer):\n        if (ctypes.cast(struct, ctypes.c_void_p).value is None):\n            print('{0} -> NULL'.format(name))\n            return\n        return print_ctypes_struct(struct[0], (name + '<deref>'), hexa=hexa)\n    if (not hasattr(struct, '_fields_')):\n        value = struct\n        if hasattr(struct, 'value'):\n            value = struct.value\n        if isinstance(value, basestring):\n            value = repr(value)\n        if hexa:\n            try:\n                print('{0} -> {1}'.format(name, hex(value)))\n                return\n            except TypeError:\n                pass\n        print('{0} -> {1}'.format(name, value))\n        return\n    for (fname, ftype) in struct._fields_:\n        value = getattr(struct, fname)\n        print_ctypes_struct(value, '{0}.{1}'.format(name, fname), hexa=hexa)\n", "label": 1}
{"function": "\n\ndef test_column_optimizations_with_bcolz_and_rewrite():\n    try:\n        import bcolz\n    except ImportError:\n        return\n    bc = bcolz.ctable([[1, 2, 3], [10, 20, 30]], names=['a', 'b'])\n    func = (lambda x: x)\n    for cols in [None, 'abc', ['abc']]:\n        dsk2 = merge(dict(((('x', i), (dataframe_from_ctable, bc, slice(0, 2), cols, {\n            \n        })) for i in [1, 2, 3])), dict(((('y', i), (getitem, ('x', i), (list, ['a', 'b']))) for i in [1, 2, 3])))\n        expected = dict(((('y', i), (dataframe_from_ctable, bc, slice(0, 2), (list, ['a', 'b']), {\n            \n        })) for i in [1, 2, 3]))\n        result = dd.optimize(dsk2, [('y', i) for i in [1, 2, 3]])\n        assert (result == expected)\n", "label": 1}
{"function": "\n\ndef request(url, post=None, headers=None, mobile=False, safe=False, timeout='30'):\n    try:\n        control.log(('[cloudflare] request %s' % url))\n        try:\n            headers.update(headers)\n        except:\n            headers = {\n                \n            }\n        agent = cache.get(cloudflareAgent, 168)\n        if (not ('User-Agent' in headers)):\n            headers['User-Agent'] = agent\n        u = ('%s://%s' % (urlparse.urlparse(url).scheme, urlparse.urlparse(url).netloc))\n        cookie = cache.get(cloudflareCookie, 168, u, post, headers, mobile, safe, timeout)\n        result = client.request(url, cookie=cookie, post=post, headers=headers, mobile=mobile, safe=safe, timeout=timeout, output='response', error=True)\n        if (result[0] == '503'):\n            agent = cache.get(cloudflareAgent, 0)\n            headers['User-Agent'] = agent\n            cookie = cache.get(cloudflareCookie, 0, u, post, headers, mobile, safe, timeout)\n            result = client.request(url, cookie=cookie, post=post, headers=headers, mobile=mobile, safe=safe, timeout=timeout)\n        else:\n            result = result[1]\n        return result\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef __protect__(self, key, value=sentinel):\n    'Protected keys add its parents, not sure if useful'\n    if (not isinstance(key, list)):\n        key = (key.split('.') if isinstance(key, basestring) else [key])\n    (key, path) = (key.pop(0), key)\n    if (len(path) > 0):\n        self.get(key).protect(path, value)\n    elif (value is not sentinel):\n        self[key] = value\n    if (key not in self):\n        raise KeyError(('key %s has no value to protect' % key))\n    self.__PROTECTED__.add(key)\n", "label": 1}
{"function": "\n\ndef decode(self, request, value):\n    value = int(value)\n    if (((self.min is not None) and (value < self.min)) or ((self.max is not None) and (value > self.max))):\n        raise ValueError(('Value %d is out of range.' % value))\n    return value\n", "label": 0}
{"function": "\n\ndef compute(self):\n    im = self.get_input('Input Image')\n    if self.has_input('Input PixelType'):\n        inPixelType = self.get_input('Input PixelType')\n    else:\n        inPixelType = im.getPixelType()\n    if self.has_input('Dimension'):\n        dim = self.get_input('Dimension')\n    else:\n        dim = im.getDim()\n    inImgType = itk.Image[(inPixelType._type, dim)]\n    try:\n        self.filter_ = itk.CurvatureAnisotropicDiffusionImageFilter[(inImgType, inImgType)].New(im.getImg())\n    except:\n        raise ModuleError(self, 'Filter requires a decimal PixelType')\n    if self.has_input('Iterations'):\n        iterations = self.get_input('Iterations')\n    else:\n        iterations = 5\n    if self.has_input('TimeStep'):\n        timestep = self.get_input('TimeStep')\n    elif (dim == 2):\n        timestep = 0.125\n    else:\n        timestep = 0.0625\n    if self.has_input('Conductance'):\n        conductance = self.get_input('Conductance')\n    else:\n        conductance = 3.0\n    self.filter_.SetNumberOfIterations(iterations)\n    self.filter_.SetTimeStep(timestep)\n    self.filter_.SetConductanceParameter(conductance)\n    self.filter_.Update()\n    outIm = Image()\n    outIm.setImg(self.filter_.GetOutput())\n    outIm.setPixelType(inPixelType)\n    outIm.setDim(dim)\n    self.set_output('Output Image', outIm)\n    self.set_output('Filter', self)\n", "label": 1}
{"function": "\n\ndef __init__(self, dictionary):\n    self.championId = dictionary.get('championId', 0)\n    self.createDate = dictionary.get('createDate', 0)\n    self.fellowPlayers = [(Player(player) if (not isinstance(player, Player)) else player) for player in dictionary.get('fellowPlayers', []) if player]\n    self.gameId = dictionary.get('gameId', 0)\n    self.gameMode = dictionary.get('gameMode', '')\n    self.gameType = dictionary.get('gameType', '')\n    self.invalid = dictionary.get('invalid', False)\n    self.ipEarned = dictionary.get('ipEarned', 0)\n    self.level = dictionary.get('level', 0)\n    self.mapId = dictionary.get('mapId', 0)\n    self.spell1 = dictionary.get('spell1', 0)\n    self.spell2 = dictionary.get('spell2', 0)\n    val = dictionary.get('stats', None)\n    self.stats = (RawStats(val) if (val and (not isinstance(val, RawStats))) else val)\n    self.subType = dictionary.get('subType', '')\n    self.teamId = dictionary.get('teamId', 0)\n", "label": 1}
{"function": "\n\ndef _query_metadata_proxy(self, machine):\n    url = ('http://%(host)s:%(port)s' % {\n        'host': dhcp.METADATA_DEFAULT_IP,\n        'port': dhcp.METADATA_PORT,\n    })\n    cmd = ('curl', '--max-time', METADATA_REQUEST_TIMEOUT, '-D-', url)\n    i = 0\n    CONNECTION_REFUSED_TIMEOUT = (METADATA_REQUEST_TIMEOUT // 2)\n    while (i <= CONNECTION_REFUSED_TIMEOUT):\n        try:\n            raw_headers = machine.execute(cmd)\n            break\n        except RuntimeError as e:\n            if ('Connection refused' in str(e)):\n                time.sleep(METADATA_REQUEST_SLEEP)\n                i += METADATA_REQUEST_SLEEP\n            else:\n                self.fail(('metadata proxy unreachable on %s before timeout' % url))\n    if (i > CONNECTION_REFUSED_TIMEOUT):\n        self.fail('Timed out waiting metadata proxy to become available')\n    return raw_headers.splitlines()[0]\n", "label": 0}
{"function": "\n\ndef updateDragOperation(self, event):\n    '\\n        http://dev.w3.org/html5/spec/dnd.html\\n        '\n    dataTransfer = event.dataTransfer\n    ea = dataTransfer.effectAllowed\n    de = dataTransfer.dropEffect\n    if ((de == 'copy') and (ea in ['uninitialized', 'copy', 'copyLink', 'copyMove', 'all'])):\n        self.currentDragOperation = 'copy'\n    elif ((de == 'link') and (ea in ['uninitialized', 'link', 'copyLink', 'linkMove', 'all'])):\n        self.currentDragOperation = 'link'\n    elif ((de == 'move') and (ea in ['uninitialized', 'move', 'copyMove', 'linkMove', 'all'])):\n        self.currentDragOperation = 'move'\n    else:\n        self.currentDragOperation = 'none'\n", "label": 1}
{"function": "\n\ndef get(self, resource):\n    '\\n        Get a resource into the cache,\\n\\n        :param resource: A :class:`Resource` instance.\\n        :return: The pathname of the resource in the cache.\\n        '\n    (prefix, path) = resource.finder.get_cache_info(resource)\n    if (prefix is None):\n        result = path\n    else:\n        result = os.path.join(self.base, self.prefix_to_dir(prefix), path)\n        dirname = os.path.dirname(result)\n        if (not os.path.isdir(dirname)):\n            os.makedirs(dirname)\n        if (not os.path.exists(result)):\n            stale = True\n        else:\n            stale = self.is_stale(resource, path)\n        if stale:\n            with open(result, 'wb') as f:\n                f.write(resource.bytes)\n    return result\n", "label": 0}
{"function": "\n\n@classmethod\ndef all(cls, session, page_size=1000, order_by=None):\n    offset = 0\n    order_by = (order_by or cls.id)\n    while True:\n        page = cls.find(session, order_by=order_by, limit=page_size, offset=offset)\n        for m in page:\n            (yield m)\n        session.flush()\n        if (len(page) != page_size):\n            raise StopIteration()\n        offset += page_size\n", "label": 0}
{"function": "\n\ndef _digest(self, alg, password, salt=None):\n    \"\\n        Helper method to perform the password digest.\\n\\n        :param alg: The hash algorithm to use.\\n        :type alg: str - 'sha512' | 'bcrypt'\\n        :param password: The password to digest.\\n        :type password: str\\n        :param salt: The salt to use. In the case of bcrypt,\\n                     when storing the password, pass None;\\n                     when testing the password, pass the hashed value.\\n        :type salt: None or str\\n        :returns: The hashed value as a string.\\n        \"\n    cur_config = config.getConfig()\n    if (alg == 'sha512'):\n        return hashlib.sha512((password + salt).encode('utf8')).hexdigest()\n    elif (alg == 'bcrypt'):\n        try:\n            import bcrypt\n        except ImportError:\n            raise Exception('Bcrypt module is not installed. See girder.local.cfg.')\n        password = password.encode('utf8')\n        if (salt is None):\n            rounds = int(cur_config['auth']['bcrypt_rounds'])\n            return bcrypt.hashpw(password, bcrypt.gensalt(rounds))\n        else:\n            if isinstance(salt, six.text_type):\n                salt = salt.encode('utf8')\n            return bcrypt.hashpw(password, salt)\n    else:\n        raise Exception(('Unsupported hash algorithm: %s' % alg))\n", "label": 1}
{"function": "\n\ndef read_data(self, offset, length):\n    end = (offset + length)\n    eof = self['size']\n    if (end > eof):\n        end = eof\n        length = (end - offset)\n        if (length <= 0):\n            return ''\n    first_block = (offset / self.blocksize)\n    last_block = (end / self.blocksize)\n    output = StringIO()\n    for n_block in range(first_block, (last_block + 1)):\n        block_offset = (n_block * self.blocksize)\n        fragment_offset = 0\n        if (n_block == first_block):\n            fragment_offset = (offset - block_offset)\n        fragment_end = self.blocksize\n        if (n_block == last_block):\n            fragment_end = (end - block_offset)\n        block_data = self.read_block(n_block)\n        fragment = block_data[fragment_offset:fragment_end]\n        assert (len(fragment) == (fragment_end - fragment_offset))\n        output.write(fragment)\n    output = output.getvalue()\n    assert (len(output) == length)\n    return output\n", "label": 1}
{"function": "\n\ndef getOrCreateUser(self, name, token=''):\n    self.lock.acquire()\n    userIndex = (- 1)\n    for i in self.getActiveUserIndexes():\n        if (self.arrayOfUsers[i]._name == name):\n            userIndex = i\n    if (userIndex < 0):\n        userIndex = self.arrayOfUsers.size()\n        self.arrayOfUsers.add(UserEntry(userIndex, (userIndex - self.deletedUserCount), name, token))\n        for roleIndex in self.getActiveRoleIndexes():\n            self.arrayOfUsers[userIndex].addRoleByIndex(roleIndex)\n    self.lock.release()\n    return userIndex\n", "label": 0}
{"function": "\n\ndef apply(self, obj, defaults={\n    \n}, **kwargs):\n    ': Apply this style to the given object using the supplied defaults.\\n\\n      = NOTE\\n      - This can apply to any matplotlib Text.\\n\\n      = INPUT VARIABLES\\n      - obj       The object to apply the style to.\\n      - defaults  Keyword-value dictionary with defaults values to use if a\\n                  property value is not specified.\\n      - kwargs    Keyword-value dictionary whose values will supercede\\n                  any values set by the properties of this sub-style.\\n      '\n    if (not isinstance(obj, mpltext.Text)):\n        msg = (\"Unable to apply this sub-style to the given element.Expected a matplotlib 'Text' and instead received the following:\\n%s\" % (obj,))\n        raise Exception(msg)\n    properties = {\n        'bgColor': 'backgroundcolor',\n        'fgColor': 'color',\n        'vertAlign': 'verticalalignment',\n        'horizAlign': 'horizontalalignment',\n        'multiAlign': 'multialignment',\n        'lineSpacing': 'linespacing',\n        'rotation': 'rotation',\n    }\n    subKwargs = kwargs.get('font', {\n        \n    })\n    subDefaults = S.lib.resolveDefaults(defaults, ['font'])\n    self.font.apply(obj.get_font_properties(), subDefaults, **subKwargs)\n    MplArtistStyle.apply(self, obj, defaults, **kwargs)\n    kw = {\n        \n    }\n    for p in properties:\n        mplProp = properties[p]\n        value = self.getValue(p, defaults, **kwargs)\n        if (value is not None):\n            kw[mplProp] = value\n    if kw:\n        obj.update(kw)\n", "label": 0}
{"function": "\n\ndef onBrowserEvent(self, event):\n    Widget.onBrowserEvent(self, event)\n    type = DOM.eventGetType(event)\n    if (type == 'load'):\n        for listener in self.loadListeners:\n            listener.onLoad(self)\n    elif (type == 'error'):\n        for listener in self.loadListeners:\n            listener.onError(self)\n", "label": 0}
{"function": "\n\n@classmethod\ndef create_user(cls, name, email, password, email_verified=True):\n    'Create (and save) a new user with the given password and\\n        email address.\\n        '\n    now = datetime.datetime.utcnow()\n    try:\n        (email_name, domain_part) = email.strip().split('@', 1)\n    except ValueError:\n        pass\n    else:\n        email = '@'.join([email_name.lower(), domain_part.lower()])\n    user = User(name=name, email=email, date_joined=now)\n    if (not password):\n        password = generate_password()\n    user.set_password(password)\n    if (not email_verified):\n        user.mark_email_for_activation()\n    else:\n        user.is_email_activated = True\n    user.save()\n    return user\n", "label": 0}
{"function": "\n\ndef test_set_reuse_addr(self):\n    if (HAS_UNIX_SOCKETS and (self.family == socket.AF_UNIX)):\n        self.skipTest('Not applicable to AF_UNIX sockets.')\n    sock = socket.socket(self.family)\n    try:\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    except socket.error:\n        unittest.skip('SO_REUSEADDR not supported on this platform')\n    else:\n        s = asyncore.dispatcher(socket.socket(self.family))\n        self.assertFalse(s.socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR))\n        s.socket.close()\n        s.create_socket(self.family)\n        s.set_reuse_addr()\n        self.assertTrue(s.socket.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR))\n    finally:\n        sock.close()\n", "label": 0}
{"function": "\n\ndef test_login_logout_save():\n    (auth, app, user) = _get_flask_app()\n    auth.session = Session()\n    client = app.test_client()\n    auth.session.saved = 0\n    resp = client.get('/protected/')\n    print(resp.data)\n    assert (auth.session_key not in auth.session)\n    assert (auth.redirect_key in auth.session)\n    assert (auth.session.saved == 1)\n    data = {\n        'login': user.login,\n        'password': 'foobar',\n        '_csrf_token': auth.get_csrf_token(),\n    }\n    client.post(auth.url_sign_in, data=data)\n    assert (auth.session_key in auth.session)\n    assert (auth.session.saved == 4)\n    data = {\n        '_csrf_token': auth.get_csrf_token(),\n    }\n    client.post(auth.url_sign_out, data=data)\n    assert (auth.session_key not in auth.session)\n    assert (auth.session.saved == 5)\n", "label": 1}
{"function": "\n\ndef _index_document(index_list):\n    'Helper to generate an index specifying document.\\n\\n    Takes a list of (key, direction) pairs.\\n    '\n    if isinstance(index_list, collections.Mapping):\n        raise TypeError(('passing a dict to sort/create_index/hint is not allowed - use a list of tuples instead. did you mean %r?' % list(iteritems(index_list))))\n    elif (not isinstance(index_list, (list, tuple))):\n        raise TypeError(('must use a list of (key, direction) pairs, not: ' + repr(index_list)))\n    if (not len(index_list)):\n        raise ValueError('key_or_list must not be the empty list')\n    index = SON()\n    for (key, value) in index_list:\n        if (not isinstance(key, string_type)):\n            raise TypeError('first item in each key pair must be a string')\n        if (not isinstance(value, (string_type, int, collections.Mapping))):\n            raise TypeError(\"second item in each key pair must be 1, -1, '2d', 'geoHaystack', or another valid MongoDB index specifier.\")\n        index[key] = value\n    return index\n", "label": 1}
{"function": "\n\ndef _build_spanning_datetimes(self, d1, d2):\n    businessdays = list(self.iterbusinessdays(d1, d2))\n    if (len(businessdays) == 0):\n        return businessdays\n    businessdays = [datetime.datetime.combine(d, self.business_hours[0]) for d in businessdays]\n    if (d1 > businessdays[0]):\n        businessdays[0] = d1\n    if (self.isbusinessday(d2) and (d2 >= datetime.datetime.combine(d2, self.business_hours[0]))):\n        businessdays.append(datetime.datetime.combine(d2, self.business_hours[1]))\n        if (d2 < businessdays[(- 1)]):\n            businessdays[(- 1)] = datetime.datetime.combine(businessdays[(- 1)], d2.time())\n    elif (len(businessdays) == 1):\n        businessdays.append(datetime.datetime.combine(businessdays[0], self.business_hours[1]))\n    else:\n        businessdays[(- 1)] = datetime.datetime.combine(businessdays[(- 1)], self.business_hours[1])\n    return businessdays\n", "label": 1}
{"function": "\n\ndef validate_email(self, email):\n    if (not email):\n        return (email, 'The e-mail is empty.')\n    parts = email.split('@')\n    if (len(parts) != 2):\n        return (email, 'An email address must contain a single @')\n    (local, domain) = parts\n    (domain, err) = self.validate_domain(domain)\n    if err:\n        return (email, ('The e-mail has a problem to the right of the @: %s' % err))\n    (local, err) = self.validate_local_part(local)\n    if err:\n        return (email, ('The email has a problem to the left of the @: %s' % err))\n    return (((local + '@') + domain), '')\n", "label": 0}
{"function": "\n\ndef write(self, data, escape=True):\n    if (not escape):\n        self.body.write(str(data))\n    elif (hasattr(data, 'xml') and callable(data.xml)):\n        self.body.write(data.xml())\n    else:\n        if (not isinstance(data, (str, unicode))):\n            data = str(data)\n        elif isinstance(data, unicode):\n            data = data.encode('utf8', 'xmlcharrefreplace')\n        data = cgi.escape(data, True).replace(\"'\", '&#x27;')\n        self.body.write(data)\n", "label": 1}
{"function": "\n\ndef __call__(self, event, sep=os.path.sep, join=os.path.join):\n    '\\n        Handle event and print filename, line number and source code. If event.kind is a `return` or `exception` also\\n        prints values.\\n        '\n    lines = self._safe_source(event)\n    thread_name = (threading.current_thread().name if event.tracer.threading_support else '')\n    thread_align = (self.thread_alignment if event.tracer.threading_support else '')\n    self.stream.write('{thread:{thread_align}}{filename}{:>{align}}{colon}:{lineno}{:<5} {kind}{:9} {code}{}{reset}\\n'.format(self._format_filename(event), event.lineno, event.kind, lines[0], thread=thread_name, thread_align=thread_align, align=self.filename_alignment, code=self.code_colors[event.kind], **self.event_colors))\n    for line in lines[1:]:\n        self.stream.write('{thread:{thread_align}}{:>{align}}       {kind}{:9} {code}{}{reset}\\n'.format('', '   |', line, thread=thread_name, thread_align=thread_align, align=self.filename_alignment, code=self.code_colors[event.kind], **self.event_colors))\n    if (event.kind in ('return', 'exception')):\n        self.stream.write('{thread:{thread_align}}{:>{align}}       {continuation}{:9} {color}{} value: {detail}{}{reset}\\n'.format('', '...', event.kind, self._safe_repr(event.arg), thread=thread_name, thread_align=thread_align, align=self.filename_alignment, color=self.event_colors[event.kind], **self.event_colors))\n", "label": 0}
{"function": "\n\ndef _ensure_path(self):\n    result = self.client.ensure_path(self.path)\n    self.assured_path = True\n    if (result is True):\n        (data, _) = self.client.get(self.path)\n        try:\n            leases = int(data.decode('utf-8'))\n        except (ValueError, TypeError):\n            pass\n        else:\n            if (leases != self.max_leases):\n                raise ValueError(('Inconsistent max leases: %s, expected: %s' % (leases, self.max_leases)))\n    else:\n        self.client.set(self.path, str(self.max_leases).encode('utf-8'))\n", "label": 0}
{"function": "\n\ndef test_eval_size_zero(self, TrainSplit, nn):\n    (X, y) = (np.random.random((100, 10)), np.repeat([0, 1, 2, 3], 25))\n    (X_train, X_valid, y_train, y_valid) = TrainSplit(0.0)(X, y, nn)\n    assert (len(X_train) == len(X))\n    assert (len(y_train) == len(y))\n    assert (len(X_valid) == 0)\n    assert (len(y_valid) == 0)\n", "label": 0}
{"function": "\n\ndef compile_template(func):\n    spec = inspect.getargspec(func)\n    assert (len(spec.args) == len((spec.defaults or []))), 'All template args should have AST classes'\n    compiler = TemplateCompiler(zipdict(spec.args, (spec.defaults or [])))\n    template = map(compiler.visit, get_body_ast(func))\n    if ((len(template) == 1) and isinstance(template[0], ast.Expr)):\n        return template[0].value\n    return template\n", "label": 0}
{"function": "\n\ndef match_or_trust(self, host, der_encoded_certificate):\n    base64_encoded_certificate = b64encode(der_encoded_certificate)\n    if isfile(self.path):\n        with open(self.path) as f_in:\n            for line in f_in:\n                (known_host, _, known_cert) = line.strip().partition(':')\n                known_cert = known_cert.encode('utf-8')\n                if (host == known_host):\n                    return (base64_encoded_certificate == known_cert)\n    try:\n        makedirs(dirname(self.path))\n    except OSError:\n        pass\n    f_out = os_open(self.path, ((O_CREAT | O_APPEND) | O_WRONLY), 384)\n    if isinstance(host, bytes):\n        os_write(f_out, host)\n    else:\n        os_write(f_out, host.encode('utf-8'))\n    os_write(f_out, b':')\n    os_write(f_out, base64_encoded_certificate)\n    os_write(f_out, b'\\n')\n    os_close(f_out)\n    return True\n", "label": 1}
{"function": "\n\ndef merge(self, follower):\n    if ((follower not in self.followers) and (follower != self.creator)):\n        logger.error('user {0} is not a follower of {1}@{2}'.format(follower, self.creator, self.name))\n        return False\n    self.mergeQueue.add(follower)\n    if (len(self.mergeQueue) >= min(len(self.followers), self.pPartialBarrier)):\n        for follower in self.mergeQueue:\n            [panda.merge(follower) for panda in self.pandas]\n        self.mergeQueue.clear()\n        [panda.update_fields({\n            panda.FCONSENSUS: panda.z.generic(),\n        }) for panda in self.pandas]\n        self.pMergeClock += 1\n        self.update_fields({\n            self.FMERGECLOCK: self.pMergeClock,\n        })\n        logger.debug('merge clock {0}'.format(self.pMergeClock))\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef __init__(self, idl_parsed):\n    '\\n        Creates a new Contract from the parsed IDL JSON\\n\\n        :Parameters:\\n          idl_parsed\\n            Barrister parsed IDL as a list of dicts\\n        '\n    self.idl_parsed = idl_parsed\n    self.interfaces = {\n        \n    }\n    self.structs = {\n        \n    }\n    self.enums = {\n        \n    }\n    self.meta = {\n        \n    }\n    for e in idl_parsed:\n        if (e['type'] == 'struct'):\n            self.structs[e['name']] = Struct(e, self)\n        elif (e['type'] == 'enum'):\n            self.enums[e['name']] = Enum(e)\n        elif (e['type'] == 'interface'):\n            self.interfaces[e['name']] = Interface(e, self)\n        elif (e['type'] == 'meta'):\n            for (k, v) in list(e.items()):\n                if (k != 'type'):\n                    self.meta[k] = v\n", "label": 1}
{"function": "\n\ndef get_fqhostname():\n    '\\n    Returns the fully qualified hostname\\n    '\n    l = []\n    l.append(socket.getfqdn())\n    try:\n        addrinfo = socket.getaddrinfo(socket.gethostname(), 0, socket.AF_UNSPEC, socket.SOCK_STREAM, socket.SOL_TCP, socket.AI_CANONNAME)\n        for info in addrinfo:\n            if (len(info) >= 4):\n                l.append(info[3])\n    except socket.gaierror:\n        pass\n    l = _sort_hostnames(l)\n    if (len(l) > 0):\n        return l[0]\n    return None\n", "label": 0}
{"function": "\n\ndef _live_receivers(self, senderkey):\n    '\\n        Filter sequence of receivers to get resolved, live receivers.\\n\\n        This checks for weak references and resolves them, then returning only\\n        live receivers.\\n        '\n    none_senderkey = _make_id(None)\n    receivers = []\n    for ((receiverkey, r_senderkey), receiver) in self.receivers:\n        if ((r_senderkey == none_senderkey) or (r_senderkey == senderkey)):\n            if isinstance(receiver, WEAKREF_TYPES):\n                receiver = receiver()\n                if (receiver is not None):\n                    receivers.append(receiver)\n            else:\n                receivers.append(receiver)\n    return receivers\n", "label": 1}
{"function": "\n\ndef parse_date(string, locale=LC_TIME):\n    \"Parse a date from a string.\\n\\n    This function uses the date format for the locale as a hint to determine\\n    the order in which the date fields appear in the string.\\n\\n    >>> parse_date('4/1/04', locale='en_US')\\n    datetime.date(2004, 4, 1)\\n    >>> parse_date('01.04.2004', locale='de_DE')\\n    datetime.date(2004, 4, 1)\\n\\n    :param string: the string containing the date\\n    :param locale: a `Locale` object or a locale identifier\\n    \"\n    format = get_date_format(locale=locale).pattern.lower()\n    year_idx = format.index('y')\n    month_idx = format.index('m')\n    if (month_idx < 0):\n        month_idx = format.index('l')\n    day_idx = format.index('d')\n    indexes = [(year_idx, 'Y'), (month_idx, 'M'), (day_idx, 'D')]\n    indexes.sort()\n    indexes = dict([(item[1], idx) for (idx, item) in enumerate(indexes)])\n    numbers = re.findall('(\\\\d+)', string)\n    year = numbers[indexes['Y']]\n    if (len(year) == 2):\n        year = (2000 + int(year))\n    else:\n        year = int(year)\n    month = int(numbers[indexes['M']])\n    day = int(numbers[indexes['D']])\n    if (month > 12):\n        (month, day) = (day, month)\n    return date(year, month, day)\n", "label": 0}
{"function": "\n\ndef _build_doc(self):\n    '\\n        Raises\\n        ------\\n        ValueError\\n            * If a URL that lxml cannot parse is passed.\\n\\n        Exception\\n            * Any other ``Exception`` thrown. For example, trying to parse a\\n              URL that is syntactically correct on a machine with no internet\\n              connection will fail.\\n\\n        See Also\\n        --------\\n        pandas.io.html._HtmlFrameParser._build_doc\\n        '\n    from lxml.html import parse, fromstring, HTMLParser\n    from lxml.etree import XMLSyntaxError\n    parser = HTMLParser(recover=False, encoding=self.encoding)\n    try:\n        r = parse(self.io, parser=parser)\n        try:\n            r = r.getroot()\n        except AttributeError:\n            pass\n    except (UnicodeDecodeError, IOError):\n        if (not _is_url(self.io)):\n            r = fromstring(self.io, parser=parser)\n            try:\n                r = r.getroot()\n            except AttributeError:\n                pass\n        else:\n            scheme = parse_url(self.io).scheme\n            if (scheme not in _valid_schemes):\n                msg = ('%r is not a valid url scheme, valid schemes are %s' % (scheme, _valid_schemes))\n                raise ValueError(msg)\n            else:\n                raise\n    else:\n        if (not hasattr(r, 'text_content')):\n            raise XMLSyntaxError('no text parsed from document', 0, 0, 0)\n    return r\n", "label": 1}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if (not self.pk):\n        try:\n            self.position = (self.siblings(include_self=True).only('position').order_by('-position')[0].position + 1)\n        except IndexError:\n            if self.parent:\n                self.position = (self.parent.position + 1)\n            else:\n                try:\n                    self.position = (Forum.objects.only('position').order_by('-position')[0].position + 1)\n                except IndexError:\n                    self.position = 1\n        if (self.position != 1):\n            Forum.objects.update_position(self.position)\n        if self.parent:\n            self.level = (self.parent.level + 1)\n    super(Forum, self).save(*args, **kwargs)\n", "label": 1}
{"function": "\n\ndef msvc_exists():\n    ' Determine whether MSVC is available on the machine.\\n    '\n    result = 0\n    try:\n        p = subprocess.Popen(['cl'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        str_result = p.stdout.read()\n        if ('Microsoft' in str_result):\n            result = 1\n    except:\n        import distutils.msvccompiler\n        try:\n            version = distutils.msvccompiler.get_devstudio_versions()\n        except:\n            version = distutils.msvccompiler.get_build_version()\n        if version:\n            result = 1\n    return result\n", "label": 0}
{"function": "\n\ndef level_matches(self, level, consumer_level):\n    '\\n        >>> l = Logger([])\\n        >>> l.level_matches(3, 4)\\n        False\\n        >>> l.level_matches(3, 2)\\n        True\\n        >>> l.level_matches(slice(None, 3), 3)\\n        False\\n        >>> l.level_matches(slice(None, 3), 2)\\n        True\\n        >>> l.level_matches(slice(1, 3), 1)\\n        True\\n        >>> l.level_matches(slice(2, 3), 1)\\n        False\\n        '\n    if isinstance(level, slice):\n        (start, stop) = (level.start, level.stop)\n        if ((start is not None) and (start > consumer_level)):\n            return False\n        if ((stop is not None) and (stop <= consumer_level)):\n            return False\n        return True\n    else:\n        return (level >= consumer_level)\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('PacketCaptureException')\n    if (self.message is not None):\n        oprot.writeFieldBegin('message', TType.STRING, 1)\n        oprot.writeString(self.message)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\ndef download_set(set_id, get_filename, size_label=None):\n    \"\\n    Download the set with 'set_id' to the current directory.\\n\\n    @param set_id: str, id of the photo set\\n    @param get_filename: Function, function that creates a filename for the photo\\n    @param size_label: str|None, size to download (or None for largest available)\\n    \"\n    suffix = (' ({})'.format(size_label) if size_label else '')\n    pset = Flickr.Photoset(id=set_id)\n    photos = pset.getPhotos()\n    pagenum = 2\n    while True:\n        try:\n            page = pset.getPhotos(page=pagenum)\n            photos.extend(page)\n            pagenum += 1\n        except FlickrAPIError as ex:\n            if (ex.code == 1):\n                break\n            raise\n    dirname = pset.title.replace(os.sep, '_')\n    if (not os.path.exists(dirname)):\n        os.mkdir(dirname)\n    for photo in photos:\n        fname = get_full_path(dirname, get_filename(pset, photo, suffix))\n        if os.path.exists(fname):\n            print('Skipping {0}, as it exists already'.format(fname))\n            continue\n        print('Saving: {0}'.format(fname))\n        photo.save(fname, size_label)\n        info = photo.getInfo()\n        taken = parser.parse(info['taken'])\n        taken_unix = time.mktime(taken.timetuple())\n        os.utime(fname, (taken_unix, taken_unix))\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.history == other.history) and (self.channels == other.channels) and (self.supported == other.supported) and (self.nicksToHostmasks == other.nicksToHostmasks) and (self.batches == other.batches))\n", "label": 0}
{"function": "\n\n@content\ndef PUT(self):\n    ':returns: node id.\\n\\n        :http: * 200 (node are successfully updated)\\n               * 304 (node data not changed since last request)\\n               * 400 (data validation failed)\\n               * 404 (node not found)\\n        '\n    nd = self.checked_data(self.validator.validate_update, data=web.data())\n    node = self.collection.single.get_by_meta(nd)\n    if (not node):\n        raise self.http(404, \"Can't find node: {0}\".format(nd))\n    node.timestamp = datetime.now()\n    if (not node.online):\n        node.online = True\n        msg = \"Node '{0}' is back online\".format(node.human_readable_name)\n        logger.info(msg)\n        notifier.notify('discover', msg, node_id=node.id)\n    db().flush()\n    if (('agent_checksum' in nd) and (node.agent_checksum == nd['agent_checksum'])):\n        return {\n            'id': node.id,\n            'cached': True,\n        }\n    self.collection.single.update_by_agent(node, nd)\n    return {\n        'id': node.id,\n    }\n", "label": 0}
{"function": "\n\ndef boundary_edges(polys):\n    'Returns the edges that are on the boundary of a mesh, as defined by belonging to only 1 face'\n    edges = dict()\n    for (i, poly) in enumerate(np.sort(polys)):\n        for (a, b) in [(0, 1), (1, 2), (0, 2)]:\n            key = (poly[a], poly[b])\n            if (key not in edges):\n                edges[key] = []\n            edges[key].append(i)\n    epts = []\n    for (edge, faces) in edges.items():\n        if (len(faces) == 1):\n            epts.append(edge)\n    return np.array(epts)\n", "label": 1}
{"function": "\n\ndef make_trace_rows(trace):\n    if (not trace.events):\n        return []\n    rows = [[trace.request_type, str(datetime_from_utc_to_local(trace.started_at)), trace.coordinator, 0]]\n    for event in trace.events:\n        rows.append([('%s [%s]' % (event.description, event.thread_name)), str(datetime_from_utc_to_local(event.datetime)), event.source, (event.source_elapsed.microseconds if event.source_elapsed else '--')])\n    if trace.duration:\n        finished_at = (datetime_from_utc_to_local(trace.started_at) + trace.duration)\n    else:\n        finished_at = trace.duration = '--'\n    rows.append(['Request complete', str(finished_at), trace.coordinator, trace.duration.microseconds])\n    return rows\n", "label": 0}
{"function": "\n\ndef has_changes(self):\n    if self.is_dirty:\n        return True\n    for child in self._db_actions:\n        if child.has_changes():\n            return True\n    for child in self._db_tags:\n        if child.has_changes():\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef test_replacements_in_network_assignments(self):\n    node_roles_vs_net_names = [(['controller'], ['public', 'management', 'fuelweb_admin']), (['compute', 'cinder'], ['storage', 'management', 'fuelweb_admin'])]\n    template_meta = self.net_template['adv_net_template']['default']\n    iface_var = template_meta['nic_mapping']['default'].keys()[0]\n    ep_with_var = '<% {0} %>.123'.format(iface_var)\n    template_meta['network_assignments']['storage']['ep'] = ep_with_var\n    template_meta['network_scheme']['storage']['endpoints'] = [ep_with_var]\n    objects.Cluster.set_network_template(self.cluster, self.net_template)\n    cluster_db = objects.Cluster.get_by_uid(self.cluster['id'])\n    objects.Cluster.prepare_for_deployment(cluster_db)\n    serializer = get_serializer_for_cluster(self.cluster)\n    serialized_for_astute = serializer(AstuteGraph(cluster_db)).serialize(self.cluster, cluster_db.nodes)\n    for node_data in serialized_for_astute:\n        node = objects.Node.get_by_uid(node_data['uid'])\n        for (node_roles, net_names) in node_roles_vs_net_names:\n            if (node.all_roles == set(node_roles)):\n                self.check_node_ips_on_certain_networks(node, net_names)\n                break\n        else:\n            self.fail('Unexpected combination of node roles: {0}'.format(node.all_roles))\n", "label": 0}
{"function": "\n\ndef unpack_args(self, arg, kwarg_name, kwargs):\n    try:\n        new_args = kwargs[kwarg_name]\n        if (not isinstance(new_args, list)):\n            new_args = [new_args]\n        for i in new_args:\n            if (not isinstance(i, str)):\n                raise MesonException('html_args values must be strings.')\n    except KeyError:\n        return []\n    if (len(new_args) > 0):\n        return [(arg + '@@'.join(new_args))]\n    return []\n", "label": 1}
{"function": "\n\ndef ntime(*args, **kwargs):\n    if args:\n        if (args[0] is None):\n            return None\n    elif kwargs:\n        if (None in [v for (k, v) in kwargs.iteritems() if (k != 'tz')]):\n            return None\n    return SaneTime(*args, **kwargs)\n", "label": 1}
{"function": "\n\ndef str_replace(arr, pat, repl, n=(- 1), case=True, flags=0):\n    '\\n    Replace occurrences of pattern/regex in the Series/Index with\\n    some other string. Equivalent to :meth:`str.replace` or\\n    :func:`re.sub`.\\n\\n    Parameters\\n    ----------\\n    pat : string\\n        Character sequence or regular expression\\n    repl : string\\n        Replacement sequence\\n    n : int, default -1 (all)\\n        Number of replacements to make from start\\n    case : boolean, default True\\n        If True, case sensitive\\n    flags : int, default 0 (no flags)\\n        re module flags, e.g. re.IGNORECASE\\n\\n    Returns\\n    -------\\n    replaced : Series/Index of objects\\n    '\n    use_re = ((not case) or (len(pat) > 1) or flags)\n    if use_re:\n        if (not case):\n            flags |= re.IGNORECASE\n        regex = re.compile(pat, flags=flags)\n        n = (n if (n >= 0) else 0)\n\n        def f(x):\n            return regex.sub(repl, x, count=n)\n    else:\n        f = (lambda x: x.replace(pat, repl, n))\n    return _na_map(f, arr)\n", "label": 1}
{"function": "\n\ndef _expand_node(self, nid):\n    ' Expands the contents of a specified node (if required).\\n        '\n    (expanded, node, object) = self._get_node_data(nid)\n    if (not expanded):\n        dummy = getattr(nid, '_dummy', None)\n        if (dummy is not None):\n            nid.removeChild(dummy)\n            del nid._dummy\n        for child in node.get_children(object):\n            (child, child_node) = self._node_for(child)\n            if (child_node is not None):\n                self._append_node(nid, child_node, child)\n        self._set_node_data(nid, (True, node, object))\n", "label": 0}
{"function": "\n\ndef _AddToHostData(self, host_data, artifact, data, parser):\n    'Parse raw data collected for an artifact into the host_data table.'\n    if (type(data) != dict):\n        raise test_lib.Error(('Data for %s is not of type dictionary.' % artifact))\n    rdfs = []\n    stats = []\n    files = []\n    for (path, lines) in data.items():\n        stat = self.CreateStat(path)\n        stats.append(stat)\n        file_obj = StringIO.StringIO(lines)\n        files.append(file_obj)\n        if (not parser.process_together):\n            rdfs.extend(list(parser.Parse(stat, file_obj, None)))\n    if parser.process_together:\n        rdfs = list(parser.ParseMultiple(stats, files, None))\n    host_data[artifact] = self.SetArtifactData(anomaly=[a for a in rdfs if isinstance(a, rdf_anomaly.Anomaly)], parsed=[r for r in rdfs if (not isinstance(r, rdf_anomaly.Anomaly))], raw=stats, results=host_data.get(artifact))\n    return host_data\n", "label": 1}
{"function": "\n\ndef login(username, password):\n    verify_to_schema(UserLoginSchema, {\n        'username': username,\n        'password': password,\n    })\n    user = api.user.get_user(username_lower=username.lower())\n    if (user is None):\n        raise WebException('No user with that username exists!')\n    if user.get('disabled', False):\n        raise WebException('This account is disabled.')\n    if confirm_password(password, user['password']):\n        if (user['uid'] is not None):\n            session['uid'] = user['uid']\n            if (user['type'] == 0):\n                session['admin'] = True\n            session.permanent = True\n        else:\n            raise WebException('Login error. Error code: 1.')\n    else:\n        raise WebException('Wrong password.')\n", "label": 1}
{"function": "\n\ndef test_variable_access_before_setup(self):\n    prob = Problem(root=ExampleGroup())\n    try:\n        prob['G2.C1.x'] = 5.0\n    except AttributeError as err:\n        msg = \"'unknowns' has not been initialized, setup() must be called before 'G2.C1.x' can be accessed\"\n        self.assertEqual(text_type(err), msg)\n    else:\n        self.fail('Exception expected')\n    try:\n        prob.run()\n    except RuntimeError as err:\n        msg = 'setup() must be called before running the model.'\n        self.assertEqual(text_type(err), msg)\n    else:\n        self.fail('Exception expected')\n", "label": 0}
{"function": "\n\ndef apply_theme(self, property_values):\n    ' Apply a set of theme values which will be used rather than\\n        defaults, but will not override application-set values.\\n\\n        The passed-in dictionary may be kept around as-is and shared with\\n        other instances to save memory (so neither the caller nor the\\n        |HasProps| instance should modify it).\\n\\n        .. |HasProps| replace:: :class:`~bokeh.properties.HasProps`\\n\\n        '\n    old_dict = None\n    if hasattr(self, '__themed_values__'):\n        old_dict = getattr(self, '__themed_values__')\n    if (old_dict is property_values):\n        return\n    removed = set()\n    if (old_dict is not None):\n        removed.update(set(old_dict.keys()))\n    added = set(property_values.keys())\n    old_values = dict()\n    for k in added.union(removed):\n        old_values[k] = getattr(self, k)\n    if (len(property_values) > 0):\n        setattr(self, '__themed_values__', property_values)\n    elif hasattr(self, '__themed_values__'):\n        delattr(self, '__themed_values__')\n    for (k, v) in old_values.items():\n        prop = self.lookup(k)\n        prop.trigger_if_changed(self, v)\n", "label": 1}
{"function": "\n\ndef get_columns(self, with_aliases=False):\n    '\\n        Remove table names and strip quotes from column names.\\n        '\n    soql_trans = self.query_topology()\n    (cols, col_params) = compiler.SQLCompiler.get_columns(self, with_aliases)\n    out = []\n    for col in cols:\n        if (soql_trans and re.match('^\\\\w+\\\\.\\\\w+$', col)):\n            (tab_name, col_name) = col.split('.')\n            out.append(('%s.%s' % (soql_trans[tab_name], col_name)))\n        else:\n            out.append(col)\n    cols = out\n    result = [x.replace(' AS ', ' ') for x in cols]\n    return (result, col_params)\n", "label": 0}
{"function": "\n\ndef __contains__(self, taskid):\n    if ((taskid in self.priority_queue) or (taskid in self.time_queue)):\n        return True\n    if ((taskid in self.processing) and self.processing[taskid].taskid):\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef _register_callbacks(self):\n    zk = self._zk_util._zk\n    path_for_brokers = self._zk_util.path_for_brokers()\n    path_for_topic = self._zk_util.path_for_topic(self.topic)\n    if ((self._brokers_watch is None) and zk.exists(path_for_brokers)):\n        self._brokers_watch = zk.children(path_for_brokers)(self._unbalance)\n    if ((self._topic_watch is None) and zk.exists(path_for_topic)):\n        self._topic_watch = zk.children(path_for_topic)(self._unbalance)\n    log.debug('Producer {0} has watches: {1}'.format(self._id, sorted(zk.watches.data.keys())))\n", "label": 0}
{"function": "\n\ndef random_interface(dut, exclude=None):\n    exclude = ([] if (exclude is None) else exclude)\n    interfaces = dut.api('interfaces')\n    names = [name for name in list(interfaces.keys()) if name.startswith('Et')]\n    exclude_interfaces = dut.settings.get('exclude_interfaces', [])\n    if exclude_interfaces:\n        exclude_interfaces = exclude_interfaces.split(',')\n    exclude_interfaces.extend(exclude)\n    if (sorted(exclude_interfaces) == sorted(names)):\n        raise TypeError('unable to allocate interface from dut')\n    choices = set(names).difference(exclude)\n    return random.choice(list(choices))\n", "label": 1}
{"function": "\n\ndef save(self, *args, **kwargs):\n    '\\n        A custom save that publishes or unpublishes the object where\\n        appropriate.\\n\\n        Save with keyword argument obj.save(publish=False) to skip the process.\\n        '\n    from bakery import tasks\n    from django.contrib.contenttypes.models import ContentType\n    if (not kwargs.pop('publish', True)):\n        super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n    else:\n        try:\n            preexisting = self.__class__.objects.get(pk=self.pk)\n        except self.__class__.DoesNotExist:\n            preexisting = None\n        if (not preexisting):\n            if self.get_publication_status():\n                action = 'publish'\n            else:\n                action = None\n        elif ((not self.get_publication_status()) and preexisting.get_publication_status()):\n            action = 'unpublish'\n        elif self.get_publication_status():\n            action = 'publish'\n        else:\n            action = None\n        with transaction.atomic():\n            super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n        ct = ContentType.objects.get_for_model(self.__class__)\n        if (action == 'publish'):\n            tasks.publish_object.delay(ct.pk, self.pk)\n        elif (action == 'unpublish'):\n            tasks.unpublish_object.delay(ct.pk, self.pk)\n", "label": 1}
{"function": "\n\ndef test_independent_generators(self):\n    N = 10\n    random_seed(1)\n    py_numbers = [random_random() for i in range(N)]\n    numpy_seed(2)\n    np_numbers = [numpy_random() for i in range(N)]\n    random_seed(1)\n    numpy_seed(2)\n    pairs = [(random_random(), numpy_random()) for i in range(N)]\n    self.assertPreciseEqual([p[0] for p in pairs], py_numbers)\n    self.assertPreciseEqual([p[1] for p in pairs], np_numbers)\n", "label": 1}
{"function": "\n\ndef write_image(results, output_filename=None):\n    print(('Gathered %s results that represents a mandelbrot image (using %s chunks that are computed jointly by %s workers).' % (len(results), CHUNK_COUNT, WORKERS)))\n    if (not output_filename):\n        return\n    try:\n        from PIL import Image\n    except ImportError as e:\n        raise RuntimeError(('Pillow is required to write image files: %s' % e))\n    color_max = 0\n    for (_point, color) in results:\n        color_max = max(color, color_max)\n    img = Image.new('L', IMAGE_SIZE, 'black')\n    pixels = img.load()\n    for ((x, y), color) in results:\n        if (color_max == 0):\n            color = 0\n        else:\n            color = int(((float(color) / color_max) * 255.0))\n        pixels[(x, y)] = color\n    img.save(output_filename)\n", "label": 1}
{"function": "\n\ndef Validate(self, value, unused_key=None):\n    'Validates a subnet.'\n    if (value is None):\n        raise validation.MissingAttribute('subnet must be specified')\n    if (not isinstance(value, basestring)):\n        raise validation.ValidationError((\"subnet must be a string, not '%r'\" % type(value)))\n    try:\n        ipaddr.IPNetwork(value)\n    except ValueError:\n        raise validation.ValidationError(('%s is not a valid IPv4 or IPv6 subnet' % value))\n    parts = value.split('/')\n    if ((len(parts) == 2) and (not re.match('^[0-9]+$', parts[1]))):\n        raise validation.ValidationError(('Prefix length of subnet %s must be an integer (quad-dotted masks are not supported)' % value))\n    return value\n", "label": 1}
{"function": "\n\ndef test_cache(self):\n    mocked_repo = MagicMock()\n    mocked_commit = MagicMock()\n    mocked_repo.lookup_reference().resolve().target = 'head'\n    mocked_repo.walk.return_value = [mocked_commit]\n    mocked_commit.commit_time = 1411135000\n    mocked_commit.hex = '1111111111'\n    cache = CommitCache(mocked_repo)\n    cache.update()\n    cache['2014-09-20'] = Commit(1, 1, '1111111111')\n    assert (sorted(cache.keys()) == ['2014-09-19', '2014-09-20'])\n    asserted_time = datetime.fromtimestamp(mocked_commit.commit_time)\n    asserted_time = '{}-{}-{}'.format(asserted_time.hour, asserted_time.minute, asserted_time.second)\n    assert (repr(cache['2014-09-19']) == ('[%s-1111111111]' % asserted_time))\n    del cache['2014-09-20']\n    for commit_date in cache:\n        assert (commit_date == '2014-09-19')\n    mocked_repo.lookup_reference.has_calls([call('HEAD')])\n    mocked_repo.walk.assert_called_once_with('head', GIT_SORT_TIME)\n    assert (mocked_repo.lookup_reference().resolve.call_count == 2)\n", "label": 1}
{"function": "\n\ndef __deleteData__(self, uri, query, auth=True):\n    if self.ssl:\n        url = ('%s://%s:%s%s' % ('https', self.hostname, self.port, uri))\n    else:\n        url = ('%s://%s:%s%s' % ('http', self.hostname, self.port, uri))\n    full_uri = ('%s' % uri)\n    if (query != None):\n        full_uri = ('%s?%s' % (full_uri, urllib.urlencode(query)))\n    if self.token:\n        if auth:\n            headers = {\n                'Content-Type': 'application/json',\n                'X-Auth-Token': self.token,\n            }\n        else:\n            headers = {\n                'Content-Type': 'application/json',\n            }\n    else:\n        date = utils.formatdate()\n        if auth:\n            sig = self.__signRequest__('DELETE', full_uri, date, 'application/json')\n            headers = {\n                'Content-Type': 'application/json',\n                'Date': date,\n                'Authorization': sig,\n            }\n        else:\n            headers = {\n                'Content-Type': 'application/json',\n                'Date': date,\n            }\n    r = requests.delete(url, params=query, headers=headers)\n    return self.__processResponse__({\n        'status': r.status_code,\n        'data': r.text,\n    })\n", "label": 1}
{"function": "\n\ndef candidates(self, items, artist, album, va_likely):\n    'Returns a list of AlbumInfo objects for discogs search results\\n        matching an album and artist (if not various).\\n        '\n    if (not self.discogs_client):\n        return\n    if va_likely:\n        query = album\n    else:\n        query = ('%s %s' % (artist, album))\n    try:\n        return self.get_albums(query)\n    except DiscogsAPIError as e:\n        self._log.debug('API Error: {0} (query: {1})', e, query)\n        if (e.status_code == 401):\n            self.reset_auth()\n            return self.candidates(items, artist, album, va_likely)\n        else:\n            return []\n    except CONNECTION_ERRORS:\n        self._log.debug('Connection error in album search', exc_info=True)\n        return []\n", "label": 1}
{"function": "\n\ndef check_sizes(size, width, height):\n    'Check that these arguments, in supplied, are consistent.\\n    Return a (width, height) pair.\\n    '\n    if (not size):\n        return (width, height)\n    if (len(size) != 2):\n        raise ValueError('size argument should be a pair (width, height)')\n    if ((width is not None) and (width != size[0])):\n        raise ValueError(('size[0] (%r) and width (%r) should match when both are used.' % (size[0], width)))\n    if ((height is not None) and (height != size[1])):\n        raise ValueError(('size[1] (%r) and height (%r) should match when both are used.' % (size[1], height)))\n    return size\n", "label": 1}
{"function": "\n\ndef check_access_key_rotation(self, iamuser_item):\n    '\\n        alert when an IAM User has an active access key created more than 90 days go.\\n        '\n    akeys = iamuser_item.config.get('accesskeys', {\n        \n    })\n    for akey in akeys.keys():\n        if ('status' in akeys[akey]):\n            if (akeys[akey]['status'] == 'Active'):\n                create_date = akeys[akey]['create_date']\n                create_date = parser.parse(create_date)\n                if (create_date < self.ninety_days_ago):\n                    notes = '> 90 days ago'\n                    self.add_issue(1, 'Active accesskey has not been rotated.', iamuser_item, notes=notes)\n", "label": 0}
{"function": "\n\ndef iter_source_code(paths, config, skipped):\n    'Iterate over all Python source files defined in paths.'\n    for path in paths:\n        if os.path.isdir(path):\n            if should_skip(path, config, os.getcwd()):\n                skipped.append(path)\n                continue\n            for (dirpath, dirnames, filenames) in os.walk(path, topdown=True):\n                for dirname in list(dirnames):\n                    if should_skip(dirname, config, dirpath):\n                        skipped.append(dirname)\n                        dirnames.remove(dirname)\n                for filename in filenames:\n                    if filename.endswith('.py'):\n                        if should_skip(filename, config, dirpath):\n                            skipped.append(filename)\n                        else:\n                            (yield os.path.join(dirpath, filename))\n        else:\n            (yield path)\n", "label": 1}
{"function": "\n\ndef _parse(self, opt, fg, bg, attr):\n    if (not opt):\n        return _Color(fg, bg, attr)\n    v = self._config.GetString(('%s.%s' % (self._section, opt)))\n    if (v is None):\n        return _Color(fg, bg, attr)\n    v = v.strip().lower()\n    if (v == 'reset'):\n        return RESET\n    elif (v == ''):\n        return _Color(fg, bg, attr)\n    have_fg = False\n    for a in v.split(' '):\n        if is_color(a):\n            if have_fg:\n                bg = a\n            else:\n                fg = a\n        elif is_attr(a):\n            attr = a\n    return _Color(fg, bg, attr)\n", "label": 1}
{"function": "\n\ndef emit(events, stream=None, Dumper=Dumper, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None):\n    '\\n    Emit YAML parsing events into a stream.\\n    If stream is None, return the produced string instead.\\n    '\n    getvalue = None\n    if (stream is None):\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        stream = StringIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break)\n    for event in events:\n        dumper.emit(event)\n    if getvalue:\n        return getvalue()\n", "label": 0}
{"function": "\n\ndef isnpint(ctx, x):\n    '\\n        Determine if *x* is a nonpositive integer.\\n        '\n    if (not x):\n        return True\n    if hasattr(x, '_mpf_'):\n        (sign, man, exp, bc) = x._mpf_\n        return (sign and (exp >= 0))\n    if hasattr(x, '_mpc_'):\n        return ((not x.imag) and ctx.isnpint(x.real))\n    if (type(x) in int_types):\n        return (x <= 0)\n    if isinstance(x, ctx.mpq):\n        (p, q) = x._mpq_\n        if (not p):\n            return True\n        return ((q == 1) and (p <= 0))\n    return ctx.isnpint(ctx.convert(x))\n", "label": 1}
{"function": "\n\ndef __init__(self, default, *items):\n    DiagramItem.__init__(self, 'g')\n    assert (default < len(items))\n    self.default = default\n    self.items = [wrapString(item) for item in items]\n    self.width = ((ARC_RADIUS * 4) + max((item.width for item in self.items)))\n    self.up = 0\n    self.down = 0\n    self.yAdvance = self.items[self.default].yAdvance\n    for (i, item) in enumerate(self.items):\n        if (i < default):\n            self.up += max(ARC_RADIUS, ((item.up + item.down) + VERTICAL_SEPARATION))\n        elif (i == default):\n            self.up += max(ARC_RADIUS, item.up)\n            self.down += max(ARC_RADIUS, item.down)\n        else:\n            assert (i > default)\n            self.down += max(ARC_RADIUS, ((VERTICAL_SEPARATION + item.up) + item.down))\n    if DEBUG:\n        self.attrs['data-updown'] = '{0} {1}'.format(self.up, self.down)\n        self.attrs['data-type'] = 'choice'\n", "label": 1}
{"function": "\n\n@cached_property\ndef api_version(self):\n    metas = [x for x in self.parsed.findall('.//meta') if (x.get('name', '').lower() == 'api-version')]\n    if metas:\n        try:\n            return int(metas[0].get('value', None))\n        except (TypeError, ValueError):\n            pass\n    return None\n", "label": 0}
{"function": "\n\n@memoize\ndef non_proxy(model):\n    while model._meta.proxy:\n        model = next((b for b in model.__bases__ if (issubclass(b, models.Model) and (not b._meta.abstract))))\n    return model\n", "label": 0}
{"function": "\n\ndef PositionerDACOffsetDualGet(self, socketId, PositionerName):\n    command = (('PositionerDACOffsetDualGet(' + PositionerName) + ',short *,short *,short *,short *)')\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(4):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef _dump_json(data):\n    options = getattr(settings, 'JSON_OPTIONS', {\n        \n    })\n    if ('cls' in options):\n        if isinstance(options['cls'], six.string_types):\n            options['cls'] = import_string(options['cls'])\n    else:\n        try:\n            use_django = getattr(settings, 'JSON_USE_DJANGO_SERIALIZER')\n        except AttributeError:\n            use_django = True\n        else:\n            warnings.warn(\"JSON_USE_DJANGO_SERIALIZER is deprecated and will be removed. Please use JSON_OPTIONS['cls'] instead.\", DeprecationWarning)\n        if use_django:\n            options['cls'] = DjangoJSONEncoder\n    return json.dumps(data, **options)\n", "label": 1}
{"function": "\n\n@gen.engine\ndef Transform(self, client, device_photo, callback):\n    from device import Device\n    from user_photo import UserPhoto\n    device_id = device_photo.device_id\n    if (device_id not in MoveDevicePhoto._device_to_user_cache):\n        query_expr = ('device.device_id={t}', {\n            't': device_id,\n        })\n        devices = (yield gen.Task(Device.IndexQuery, client, query_expr, None))\n        assert (len(devices) == 1)\n        MoveDevicePhoto._device_to_user_cache[device_id] = devices[0].user_id\n    user_id = MoveDevicePhoto._device_to_user_cache[device_id]\n    existing = (yield gen.Task(UserPhoto.Query, client, user_id, device_photo.photo_id, None, must_exist=False))\n    if (existing is None):\n        logging.info('Creating user photo for photo %s, device %s, user %s', device_photo.photo_id, device_id, user_id)\n        user_photo = UserPhoto.CreateFromKeywords(photo_id=device_photo.photo_id, user_id=user_id, asset_keys=device_photo.asset_keys)\n    else:\n        logging.info('Photo %s, device %s, user %s already has user photo', device_photo.photo_id, device_id, user_id)\n        user_photo = None\n    if (user_photo is not None):\n        self._LogUpdate(user_photo)\n    if (Version._mutate_items and (user_photo is not None)):\n        (yield gen.Task(user_photo.Update, client))\n    callback(device_photo)\n", "label": 1}
{"function": "\n\ndef assertErrorPage(self, status, message=None, pattern=''):\n    'Compare the response body with a built in error page.\\n        \\n        The function will optionally look for the regexp pattern,\\n        within the exception embedded in the error page.'\n    page = cherrypy._cperror.get_error_page(status, message=message)\n    esc = re.escape\n    epage = esc(page)\n    epage = epage.replace(esc('<pre id=\"traceback\"></pre>'), ((esc('<pre id=\"traceback\">') + '(.*)') + esc('</pre>')))\n    m = re.match(ntob(epage, self.encoding), self.body, re.DOTALL)\n    if (not m):\n        self._handlewebError(('Error page does not match; expected:\\n' + page))\n        return\n    if (pattern is None):\n        if (m and m.group(1)):\n            self._handlewebError('Error page contains traceback')\n    elif ((m is None) or (not re.search(ntob(re.escape(pattern), self.encoding), m.group(1)))):\n        msg = 'Error page does not contain %s in traceback'\n        self._handlewebError((msg % repr(pattern)))\n", "label": 1}
{"function": "\n\ndef pre_validate(self, form):\n    if self._invalid_formdata:\n        raise ValidationError(self.gettext('Not a valid choice'))\n    elif self.data:\n        obj_list = list((x[1] for x in self._get_object_list()))\n        for v in self.data:\n            if (v not in obj_list):\n                raise ValidationError(self.gettext('Not a valid choice'))\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('SlicePredicate')\n    if (self.column_names != None):\n        oprot.writeFieldBegin('column_names', TType.LIST, 1)\n        oprot.writeListBegin(TType.STRING, len(self.column_names))\n        for iter13 in self.column_names:\n            oprot.writeString(iter13)\n        oprot.writeListEnd()\n        oprot.writeFieldEnd()\n    if (self.slice_range != None):\n        oprot.writeFieldBegin('slice_range', TType.STRUCT, 2)\n        self.slice_range.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n\n    def validate(self):\n        return\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('IndexExpression')\n    if (self.column_name != None):\n        oprot.writeFieldBegin('column_name', TType.STRING, 1)\n        oprot.writeString(self.column_name)\n        oprot.writeFieldEnd()\n    if (self.op != None):\n        oprot.writeFieldBegin('op', TType.I32, 2)\n        oprot.writeI32(self.op)\n        oprot.writeFieldEnd()\n    if (self.value != None):\n        oprot.writeFieldBegin('value', TType.STRING, 3)\n        oprot.writeString(self.value)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n\n    def validate(self):\n        if (self.column_name is None):\n            raise TProtocol.TProtocolException(message='Required field column_name is unset!')\n        if (self.op is None):\n            raise TProtocol.TProtocolException(message='Required field op is unset!')\n        if (self.value is None):\n            raise TProtocol.TProtocolException(message='Required field value is unset!')\n        return\n", "label": 1}
{"function": "\n\ndef delete_mistyped_role():\n    '\\n    Delete \" system_admin\" role which was fat fingered.\\n    '\n    role_name = ' system_admin'\n    assert role_name.startswith(' ')\n    try:\n        role_db = Role.get_by_name(role_name)\n    except:\n        return\n    if (not role_db):\n        return\n    try:\n        Role.delete(role_db)\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef get_search_results(self, req, terms, filters):\n    if (not ('milestone' in filters)):\n        return\n    term_regexps = search_to_regexps(terms)\n    milestone_realm = Resource(self.realm)\n    for (name, due, completed, description) in MilestoneCache(self.env).milestones.itervalues():\n        if all(((r.search(description) or r.search(name)) for r in term_regexps)):\n            milestone = milestone_realm(id=name)\n            if ('MILESTONE_VIEW' in req.perm(milestone)):\n                dt = (completed if completed else (due if due else datetime_now(utc)))\n                (yield (get_resource_url(self.env, milestone, req.href), get_resource_name(self.env, milestone), dt, '', shorten_result(description, terms)))\n    for result in AttachmentModule(self.env).get_search_results(req, milestone_realm, terms):\n        (yield result)\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('query_result')\n    if (self.success is not None):\n        oprot.writeFieldBegin('success', TType.STRUCT, 0)\n        self.success.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.error is not None):\n        oprot.writeFieldBegin('error', TType.STRUCT, 1)\n        self.error.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef __exit__(self, ex_type, ex_value, ex_traceback):\n    if (not ex_value):\n        return True\n    if isinstance(ex_value, exception.Forbidden):\n        raise Fault(webob.exc.HTTPForbidden(explanation=ex_value.format_message()))\n    elif isinstance(ex_value, exception.VersionNotFoundForAPIMethod):\n        raise\n    elif isinstance(ex_value, exception.Invalid):\n        raise Fault(exception.ConvertedException(code=ex_value.code, explanation=ex_value.format_message()))\n    elif isinstance(ex_value, TypeError):\n        exc_info = (ex_type, ex_value, ex_traceback)\n        LOG.error(_LE('Exception handling resource: %s'), ex_value, exc_info=exc_info)\n        raise Fault(webob.exc.HTTPBadRequest())\n    elif isinstance(ex_value, Fault):\n        LOG.info(_LI('Fault thrown: %s'), ex_value)\n        raise ex_value\n    elif isinstance(ex_value, webob.exc.HTTPException):\n        LOG.info(_LI('HTTP exception thrown: %s'), ex_value)\n        raise Fault(ex_value)\n    return False\n", "label": 1}
{"function": "\n\ndef count_types(gen):\n    counts = {\n        'TOTAL': 0,\n        'DEBUG': 0,\n        'INFO': 0,\n        'WARNING': 0,\n        'ERROR': 0,\n        'TRACE': 0,\n        'AUDIT': 0,\n    }\n    laststatus = None\n    for line in gen:\n        counts['TOTAL'] = (counts['TOTAL'] + 1)\n        for key in counts:\n            if ((' %s ' % key) in line):\n                laststatus = key\n                continue\n        if laststatus:\n            counts[laststatus] = (counts[laststatus] + 1)\n    return counts\n", "label": 0}
{"function": "\n\ndef check_args(self, f, methods_args):\n    to_check = []\n    args = inspect.getargspec(f)\n    for arg in self.check:\n        if (arg in args[0]):\n            to_check.append(args[0].index(arg))\n    for index in to_check:\n        arg = methods_args[(index - 1)]\n        if self.look_at.cache.get(arg, False):\n            raise FuseOSError(errno.EACCES)\n        if self.look_at.check_key(arg):\n            self.look_at.cache[arg] = True\n            raise FuseOSError(errno.ENOENT)\n        self.look_at.cache[arg] = False\n", "label": 1}
{"function": "\n\ndef _eval_is_irrational(self):\n    for t in self.args:\n        a = t.is_irrational\n        if a:\n            others = list(self.args)\n            others.remove(t)\n            if all((((x.is_rational and fuzzy_not(x.is_zero)) is True) for x in others)):\n                return True\n            return\n        if (a is None):\n            return\n    return False\n", "label": 1}
{"function": "\n\n@contextfunction\ndef core_logo_content(context, gif=False):\n    'Return current logo encoded as base64'\n    staticpath = getattr(settings, 'STATIC_DOC_ROOT', './static')\n    logopath = (staticpath + '/logo')\n    if gif:\n        logopath += '.gif'\n        mimetype = 'image/gif'\n    else:\n        logopath += '.png'\n        mimetype = 'image/png'\n    customlogo = ''\n    try:\n        conf = ModuleSetting.get_for_module('treeio.core', 'logopath')[0]\n        customlogo = (getattr(settings, 'MEDIA_ROOT', './static/media') + conf.value)\n    except:\n        pass\n    logofile = ''\n    if customlogo:\n        try:\n            logofile = open(customlogo, 'r')\n        except:\n            pass\n    if (not logofile):\n        try:\n            logofile = open(logopath, 'r')\n        except:\n            pass\n    result = ((('data:' + mimetype) + ';base64,') + base64.b64encode(logofile.read()))\n    return Markup(result)\n", "label": 1}
{"function": "\n\n@public\ndef gcd(f, g=None, *gens, **args):\n    '\\n    Compute GCD of ``f`` and ``g``.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy import gcd\\n    >>> from sympy.abc import x\\n\\n    >>> gcd(x**2 - 1, x**2 - 3*x + 2)\\n    x - 1\\n\\n    '\n    if hasattr(f, '__iter__'):\n        if (g is not None):\n            gens = ((g,) + gens)\n        return gcd_list(f, *gens, **args)\n    elif (g is None):\n        raise TypeError('gcd() takes 2 arguments or a sequence of arguments')\n    options.allowed_flags(args, ['polys'])\n    try:\n        ((F, G), opt) = parallel_poly_from_expr((f, g), *gens, **args)\n    except PolificationFailed as exc:\n        (domain, (a, b)) = construct_domain(exc.exprs)\n        try:\n            return domain.to_sympy(domain.gcd(a, b))\n        except NotImplementedError:\n            raise ComputationFailed('gcd', 2, exc)\n    result = F.gcd(G)\n    if (not opt.polys):\n        return result.as_expr()\n    else:\n        return result\n", "label": 1}
{"function": "\n\ndef add_endpoint(self, listener):\n    \"\\n        Adds a listening endpoint to be managed by this ION process.\\n\\n        Spawns the listen loop and sets the routing call to synchronize incoming messages\\n        here. If this process hasn't been started yet, adds it to the list of listeners\\n        to start on startup.\\n        \"\n    if self.proc:\n        listener.routing_call = self._routing_call\n        if self.name:\n            svc_name = 'unnamed-service'\n            if ((self.service is not None) and hasattr(self.service, 'name')):\n                svc_name = self.service.name\n            listen_thread_name = ('%s-%s-listen-%s' % (svc_name, self.name, (len(self.listeners) + 1)))\n        else:\n            listen_thread_name = ('unknown-listener-%s' % (len(self.listeners) + 1))\n        gl = self.thread_manager.spawn(listener.listen, thread_name=listen_thread_name)\n        gl.proc._glname = ('ION Proc listener %s' % listen_thread_name)\n        self._listener_map[listener] = gl\n        self.listeners.append(listener)\n    else:\n        self._startup_listeners.append(listener)\n", "label": 0}
{"function": "\n\ndef _showmodule(module_name):\n    module = sys.modules[module_name]\n    if (not hasattr(module, '__file__')):\n        raise ValueError('cannot display module {0} (no __file__)'.format(module_name))\n    if module.__file__.endswith('.py'):\n        fname = module.__file__\n    else:\n        if (not module.__file__.endswith('.pyc')):\n            raise ValueError('cannot display module file {0} for {1}'.format(module.__file__, module_name))\n        fname = module.__file__[:(- 1)]\n    if (not os.path.exists(fname)):\n        raise ValueError('could not find file {0} for {1}'.format(fname, module_name))\n    (leaf_count, branch_count) = _get_samples_by_line(fname)\n    lines = []\n    with open(fname) as f:\n        for (i, line) in enumerate(f):\n            lines.append(((i + 1), cgi.escape(line), leaf_count[(i + 1)], branch_count[(i + 1)]))\n    return lines\n", "label": 1}
{"function": "\n\ndef _tune_workers(self):\n    for (i, w) in enumerate(self._workers):\n        if (not w.is_alive()):\n            del self._workers[i]\n    need_nums = min(self._queue.qsize(), self.MAX_WORKERS)\n    active_nums = len(self._workers)\n    if (need_nums <= active_nums):\n        return\n    for i in range((need_nums - active_nums)):\n        t = threading.Thread(target=self._worker)\n        t.daemon = True\n        t.start()\n        self._workers.append(t)\n", "label": 0}
{"function": "\n\ndef plugins(self, *plugins):\n    import json\n    ret = []\n    available_plugins = self.get('system/plugins')\n    self.message['debug']['available_plugins'] = available_plugins\n    plugins = set(plugins)\n    enabled_plugins = set(available_plugins['enabled'])\n    if ('*' in plugins):\n        plugins = set(available_plugins['all'].keys())\n    if (not (plugins <= set(available_plugins['all'].keys()))):\n        self.fail('{}, not available!'.format(','.join(list((plugins - set(available_plugins['all'].keys()))))))\n    if (self.module.params['state'] == 'present'):\n        if (not (plugins <= enabled_plugins)):\n            ret = self.put('system/plugins', {\n                'plugins': json.dumps(list((plugins | enabled_plugins))),\n            })\n            self.changed = True\n    elif (self.module.params['state'] == 'absent'):\n        if len((enabled_plugins & plugins)):\n            ret = self.put('system/plugins', {\n                'plugins': json.dumps(list((enabled_plugins - plugins))),\n            })\n            self.changed = True\n    return ret\n", "label": 1}
{"function": "\n\ndef _compress_hextets(self, hextets):\n    'Compresses a list of hextets.\\n\\n        Compresses a list of strings, replacing the longest continuous\\n        sequence of \"0\" in the list with \"\" and adding empty strings at\\n        the beginning or at the end of the string such that subsequently\\n        calling \":\".join(hextets) will produce the compressed version of\\n        the IPv6 address.\\n\\n        Args:\\n            hextets: A list of strings, the hextets to compress.\\n\\n        Returns:\\n            A list of strings.\\n\\n        '\n    best_doublecolon_start = (- 1)\n    best_doublecolon_len = 0\n    doublecolon_start = (- 1)\n    doublecolon_len = 0\n    for (index, hextet) in enumerate(hextets):\n        if (hextet == '0'):\n            doublecolon_len += 1\n            if (doublecolon_start == (- 1)):\n                doublecolon_start = index\n            if (doublecolon_len > best_doublecolon_len):\n                best_doublecolon_len = doublecolon_len\n                best_doublecolon_start = doublecolon_start\n        else:\n            doublecolon_len = 0\n            doublecolon_start = (- 1)\n    if (best_doublecolon_len > 1):\n        best_doublecolon_end = (best_doublecolon_start + best_doublecolon_len)\n        if (best_doublecolon_end == len(hextets)):\n            hextets += ['']\n        hextets[best_doublecolon_start:best_doublecolon_end] = ['']\n        if (best_doublecolon_start == 0):\n            hextets = ([''] + hextets)\n    return hextets\n", "label": 1}
{"function": "\n\ndef post(self, request, *args, **kwargs):\n    if (self.request.POST['form_type'] == 'location-settings'):\n        return self.settings_form_post(request, *args, **kwargs)\n    elif (self.request.POST['form_type'] == 'location-users'):\n        return self.users_form_post(request, *args, **kwargs)\n    elif ((self.request.POST['form_type'] == 'location-products') and toggles.PRODUCTS_PER_LOCATION.enabled(request.domain)):\n        return self.products_form_post(request, *args, **kwargs)\n    else:\n        raise Http404()\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.sync_tenant = options.get('tenant')\n    self.sync_public = options.get('shared')\n    self.schema_name = options.get('schema_name')\n    self.executor = options.get('executor')\n    self.installed_apps = settings.INSTALLED_APPS\n    self.args = args\n    self.options = options\n    if self.schema_name:\n        if self.sync_public:\n            raise CommandError('schema should only be used with the --tenant switch.')\n        elif (self.schema_name == get_public_schema_name()):\n            self.sync_public = True\n        else:\n            self.sync_tenant = True\n    elif ((not self.sync_public) and (not self.sync_tenant)):\n        self.sync_tenant = True\n        self.sync_public = True\n    if hasattr(settings, 'TENANT_APPS'):\n        self.tenant_apps = settings.TENANT_APPS\n    if hasattr(settings, 'SHARED_APPS'):\n        self.shared_apps = settings.SHARED_APPS\n", "label": 1}
{"function": "\n\ndef delete_access_list_items(self, loadbalancer, item_ids):\n    \"\\n        Removes the item(s) from the load balancer's access list\\n        that match the provided IDs. 'item_ids' should be one or\\n        more access list item IDs.\\n        \"\n    if (not isinstance(item_ids, (list, tuple))):\n        item_ids = [item_ids]\n    valid_ids = [itm['id'] for itm in self.get_access_list(loadbalancer)]\n    bad_ids = [str(itm) for itm in item_ids if (itm not in valid_ids)]\n    if bad_ids:\n        raise exc.AccessListIDNotFound(('The following ID(s) are not valid Access List items: %s' % ', '.join(bad_ids)))\n    items = '&'.join([('id=%s' % item_id) for item_id in item_ids])\n    uri = ('/loadbalancers/%s/accesslist?%s' % (utils.get_id(loadbalancer), items))\n    (resp, body) = self.api.method_delete(uri)\n    return body\n", "label": 1}
{"function": "\n\n@expose(help='Change directory to site webroot')\ndef cd(self):\n    if (not self.app.pargs.site_name):\n        try:\n            while (not self.app.pargs.site_name):\n                self.app.pargs.site_name = input('Enter site name : ').strip()\n        except IOError as e:\n            Log.error(self, 'Unable to read input, please try again')\n    self.app.pargs.site_name = self.app.pargs.site_name.strip()\n    (ee_domain, ee_www_domain) = ValidateDomain(self.app.pargs.site_name)\n    if (not check_domain_exists(self, ee_domain)):\n        Log.error(self, 'site {0} does not exist'.format(ee_domain))\n    ee_site_webroot = getSiteInfo(self, ee_domain).site_path\n    EEFileUtils.chdir(self, ee_site_webroot)\n    try:\n        subprocess.call(['bash'])\n    except OSError as e:\n        Log.debug(self, '{0}{1}'.format(e.errno, e.strerror))\n        Log.error(self, 'unable to change directory')\n", "label": 1}
{"function": "\n\ndef gcd(self, other):\n    'Return Factors of ``gcd(self, other)``. The keys are\\n        the intersection of factors with the minimum exponent for\\n        each factor.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.core.exprtools import Factors\\n        >>> from sympy.abc import x, y, z\\n        >>> a = Factors((x*y**2).as_powers_dict())\\n        >>> b = Factors((x*y/z).as_powers_dict())\\n        >>> a.gcd(b)\\n        Factors({x: 1, y: 1})\\n        '\n    if (not isinstance(other, Factors)):\n        other = Factors(other)\n        if other.is_zero:\n            return Factors(self.factors)\n    factors = {\n        \n    }\n    for (factor, exp) in self.factors.items():\n        (factor, exp) = (sympify(factor), sympify(exp))\n        if (factor in other.factors):\n            lt = (exp - other.factors[factor]).is_negative\n            if (lt == True):\n                factors[factor] = exp\n            elif (lt == False):\n                factors[factor] = other.factors[factor]\n    return Factors(factors)\n", "label": 1}
{"function": "\n\ndef GetDisplayHTML(self):\n    if (self.xPolynomialOrder == None):\n        self._HTML = 'z = user-selectable polynomial'\n    else:\n        self._HTML = 'z = '\n        cd = self.GetCoefficientDesignators()\n        indexmax = ((self.xPolynomialOrder + 1) * (self.yPolynomialOrder + 1))\n        for i in range((self.xPolynomialOrder + 1)):\n            for j in range((self.yPolynomialOrder + 1)):\n                index = ((i * (self.yPolynomialOrder + 1)) + j)\n                if (index == 0):\n                    self._HTML += cd[index]\n                else:\n                    self._HTML += (((((cd[index] + 'x<SUP>') + str(i)) + '</SUP>y<SUP>') + str(j)) + '</SUP>')\n                if (((i + 1) * (j + 1)) != indexmax):\n                    self._HTML += ' + '\n    return self.extendedVersionHandler.AssembleDisplayHTML(self)\n", "label": 1}
{"function": "\n\ndef env(target_key, entity, *_):\n    if (not target_key):\n        (yield entity.env)\n    for extra in entity.env.extras:\n        if (target_key in getattr(extra, 'target_keys', ())):\n            (yield extra)\n        for target in recursive_targets(getattr(extra, 'target_providers', ()), target_key):\n            (yield target)\n", "label": 0}
{"function": "\n\ndef ControllerMotionKernelPeriodMinMaxGet(self, socketId):\n    command = 'ControllerMotionKernelPeriodMinMaxGet(double *,double *,double *,double *,double *,double *)'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(6):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef estimate_scale(self):\n    '\\n        Returns an estimate of the scale parameter at the current\\n        parameter value.\\n        '\n    if isinstance(self.family, (families.Binomial, families.Poisson, _Multinomial)):\n        return 1.0\n    endog = self.endog_li\n    exog = self.exog_li\n    cached_means = self.cached_means\n    nobs = self.nobs\n    varfunc = self.family.variance\n    scale = 0.0\n    fsum = 0.0\n    for i in range(self.num_group):\n        if (len(endog[i]) == 0):\n            continue\n        (expval, _) = cached_means[i]\n        f = (self.weights_li[i] if (self.weights is not None) else 1.0)\n        sdev = np.sqrt(varfunc(expval))\n        resid = ((endog[i] - expval) / sdev)\n        scale += (f * np.sum((resid ** 2)))\n        fsum += (f * len(endog[i]))\n    scale /= ((fsum * (nobs - self.ddof_scale)) / float(nobs))\n    return scale\n", "label": 0}
{"function": "\n\ndef get_args_without_osxims():\n    new_argv = []\n    i = 1\n    while (i < len(sys.argv)):\n        arg = sys.argv[i]\n        next_arg = (sys.argv[(i + 1)] if ((i + 1) < len(sys.argv)) else None)\n        if ((arg == '-isysroot') and ('SDKs/MacOSX' in next_arg)):\n            i = (i + 2)\n        elif ('-mmacosx-version-min' in arg):\n            i = (i + 1)\n        elif (arg == '-F/Applications/Xcode.app/Contents/Developer/Library/Frameworks'):\n            i = (i + 1)\n        else:\n            i = (i + 1)\n            new_argv.append(arg)\n    return new_argv\n", "label": 1}
{"function": "\n\ndef GroupPositionSetpointGet(self, socketId, GroupName, nbElement):\n    command = (('GroupPositionSetpointGet(' + GroupName) + ',')\n    for i in range(nbElement):\n        if (i > 0):\n            command += ','\n        command += 'double *'\n    command += ')'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(nbElement):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 1}
{"function": "\n\ndef connectionLost(self, reason):\n    if (self.bodyDecoder is not None):\n        try:\n            try:\n                self.bodyDecoder.noMoreData()\n            except PotentialDataLoss:\n                self.response._bodyDataFinished(Failure())\n            except _DataLoss:\n                self.response._bodyDataFinished(Failure(ResponseFailed([reason, Failure()], self.response)))\n            else:\n                self.response._bodyDataFinished()\n        except:\n            log.err()\n    elif (self.state != DONE):\n        if self._everReceivedData:\n            exceptionClass = ResponseFailed\n        else:\n            exceptionClass = ResponseNeverReceived\n        self._responseDeferred.errback(Failure(exceptionClass([reason])))\n        del self._responseDeferred\n", "label": 1}
{"function": "\n\ndef __init__(self, generator, name=None, storage=None, cachefile_backend=None, cachefile_strategy=None):\n    '\\n        :param generator: The object responsible for generating a new image.\\n        :param name: The filename\\n        :param storage: A Django storage object that will be used to save the\\n            file.\\n        :param cachefile_backend: The object responsible for managing the\\n            state of the file.\\n        :param cachefile_strategy: The object responsible for handling events\\n            for this file.\\n\\n        '\n    self.generator = generator\n    if (not name):\n        try:\n            name = generator.cachefile_name\n        except AttributeError:\n            fn = get_by_qname(settings.IMAGEKIT_CACHEFILE_NAMER, 'namer')\n            name = fn(generator)\n    self.name = name\n    storage = (storage or getattr(generator, 'cachefile_storage', None) or get_singleton(settings.IMAGEKIT_DEFAULT_FILE_STORAGE, 'file storage backend'))\n    self.cachefile_backend = (cachefile_backend or getattr(generator, 'cachefile_backend', None) or get_singleton(settings.IMAGEKIT_DEFAULT_CACHEFILE_BACKEND, 'cache file backend'))\n    self.cachefile_strategy = (cachefile_strategy or getattr(generator, 'cachefile_strategy', None) or get_singleton(settings.IMAGEKIT_DEFAULT_CACHEFILE_STRATEGY, 'cache file strategy'))\n    super(ImageCacheFile, self).__init__(storage=storage)\n", "label": 1}
{"function": "\n\ndef matches(self, elem):\n    if (not self.matchesPredicates(elem)):\n        return 0\n    if (self.childLocation != None):\n        for c in elem.elements():\n            if self.childLocation.matches(c):\n                return 1\n    else:\n        return 1\n    return 0\n", "label": 0}
{"function": "\n\ndef __new__(cls, j, m):\n    j = sympify(j)\n    m = sympify(m)\n    if j.is_number:\n        if ((2 * j) != int((2 * j))):\n            raise ValueError(('j must be integer or half-integer, got: %s' % j))\n        if (j < 0):\n            raise ValueError(('j must be >= 0, got: %s' % j))\n    if m.is_number:\n        if ((2 * m) != int((2 * m))):\n            raise ValueError(('m must be integer or half-integer, got: %s' % m))\n    if (j.is_number and m.is_number):\n        if (abs(m) > j):\n            raise ValueError(('Allowed values for m are -j <= m <= j, got j, m: %s, %s' % (j, m)))\n        if (int((j - m)) != (j - m)):\n            raise ValueError(('Both j and m must be integer or half-integer, got j, m: %s, %s' % (j, m)))\n    return State.__new__(cls, j, m)\n", "label": 1}
{"function": "\n\n@policy.setter\ndef policy(self, args):\n    '\\n        setter for the policy descriptor\\n        '\n    word = args[0]\n    if (word == 'reject'):\n        self.accepted_ports = None\n        self.rejected_ports = []\n        target = self.rejected_ports\n    elif (word == 'accept'):\n        self.accepted_ports = []\n        self.rejected_ports = None\n        target = self.accepted_ports\n    else:\n        raise RuntimeError(('Don\\'t understand policy word \"%s\"' % word))\n    for port in args[1].split(','):\n        if ('-' in port):\n            (a, b) = port.split('-')\n            target.append(PortRange(int(a), int(b)))\n        else:\n            target.append(int(port))\n", "label": 0}
{"function": "\n\ndef test_profiler():\n    with prof:\n        out = get(dsk, 'e')\n    assert (out == 6)\n    prof_data = sorted(prof.results, key=(lambda d: d.key))\n    keys = [i.key for i in prof_data]\n    assert (keys == ['c', 'd', 'e'])\n    tasks = [i.task for i in prof_data]\n    assert (tasks == [(add, 'a', 'b'), (mul, 'a', 'b'), (mul, 'c', 'd')])\n    prof.clear()\n    assert (prof.results == [])\n", "label": 1}
{"function": "\n\ndef assert_raises_and_contains(expected_exception_class, strings, callable_obj, *args, **kwargs):\n    'Assert an exception is raised by passing in a callable and its\\n    arguments and that the string representation of the exception\\n    contains the case-insensitive list of passed in strings.\\n\\n    Args\\n        strings -- can be a string or an iterable of strings\\n    '\n    try:\n        callable_obj(*args, **kwargs)\n    except expected_exception_class as e:\n        message = str(e).lower()\n        try:\n            is_string = isinstance(strings, basestring)\n        except NameError:\n            is_string = isinstance(strings, str)\n        if is_string:\n            strings = [strings]\n        for string in strings:\n            assert_in(string.lower(), message)\n    else:\n        assert_not_reached(('No exception was raised (expected %s)' % expected_exception_class))\n", "label": 1}
{"function": "\n\ndef unique_everseen(iterable, key=None):\n    'List unique elements, preserving order. Remember all elements ever seen.'\n    seen = set()\n    seen_add = seen.add\n    if (key is None):\n        for element in ifilterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            (yield element)\n    else:\n        for element in iterable:\n            k = key(element)\n            if (k not in seen):\n                seen_add(k)\n                (yield element)\n", "label": 0}
{"function": "\n\ndef __updateWidth(self):\n    charWidth = None\n    if (self.getPlug() is not None):\n        charWidth = Gaffer.Metadata.plugValue(self.getPlug(), 'numericPlugValueWidget:fixedCharacterWidth')\n    if ((charWidth is None) and isinstance(self.getPlug(), Gaffer.IntPlug) and self.getPlug().hasMaxValue()):\n        charWidth = len(str(self.getPlug().maxValue()))\n    self.__numericWidget.setFixedCharacterWidth(charWidth)\n", "label": 0}
{"function": "\n\n@classmethod\n@unguarded\ndef validate_url(cls, url):\n    '\\n        Return a boolean indicating whether the URL has a protocol and hostname.\\n        If a port is specified, ensure it is an integer.\\n\\n        Arguments:\\n            url (str): The URL to check.\\n\\n        Returns:\\n            Boolean indicating whether the URL has a protocol and hostname.\\n        '\n    result = urlparse.urlsplit(url)\n    if ((not result.scheme) or (not result.netloc)):\n        return False\n    try:\n        if (result.port is not None):\n            int(result.port)\n        elif result.netloc.endswith(':'):\n            return False\n    except ValueError:\n        return False\n    else:\n        return True\n", "label": 1}
{"function": "\n\ndef test_reflect_entity_overrides():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    b = Symbol('b')\n    m = Symbol('m')\n    l = Line((0, b), slope=m)\n    p = Point(x, y)\n    r = p.reflect(l)\n    c = Circle((x, y), 3)\n    cr = c.reflect(l)\n    assert (cr == Circle(r, (- 3)))\n    assert (c.area == (- cr.area))\n    pent = RegularPolygon((1, 2), 1, 5)\n    l = Line((0, pi), slope=sqrt(2))\n    rpent = pent.reflect(l)\n    assert (rpent.center == pent.center.reflect(l))\n    assert (str([w.n(3) for w in rpent.vertices]) == '[Point2D(-0.586, 4.27), Point2D(-1.69, 4.66), Point2D(-2.41, 3.73), Point2D(-1.74, 2.76), Point2D(-0.616, 3.10)]')\n    assert pent.area.equals((- rpent.area))\n", "label": 1}
{"function": "\n\ndef get_previous_link(self):\n    if (not self.has_previous):\n        return None\n    if (self.cursor and (not self.cursor.reverse) and (self.cursor.offset != 0)):\n        compare = self._get_position_from_instance(self.page[0], self.ordering)\n    else:\n        compare = self.previous_position\n    offset = 0\n    for item in self.page:\n        position = self._get_position_from_instance(item, self.ordering)\n        if (position != compare):\n            break\n        compare = position\n        offset += 1\n    else:\n        if (not self.has_next):\n            offset = self.page_size\n            position = None\n        elif self.cursor.reverse:\n            offset = (self.cursor.offset + self.page_size)\n            position = self.next_position\n        else:\n            offset = 0\n            position = self.next_position\n    cursor = Cursor(offset=offset, reverse=True, position=position)\n    return self.encode_cursor(cursor)\n", "label": 1}
{"function": "\n\ndef summary_cdf(self, idx, frac, crit, varnames=None, title=None):\n    'summary table for cumulative density function\\n\\n\\n        Parameters\\n        ----------\\n        idx : None or list of integers\\n            List of indices into the Monte Carlo results (columns) that should\\n            be used in the calculation\\n        frac : array_like, float\\n            probabilities for which\\n        crit : array_like\\n            values for which cdf is calculated\\n        varnames : None, or list of strings\\n            optional list of variable names, same length as idx\\n\\n        Returns\\n        -------\\n        table : instance of SimpleTable\\n            use `print(table` to see results\\n\\n\\n        '\n    idx = np.atleast_1d(idx)\n    mml = []\n    for i in range(len(idx)):\n        mml.append(self.cdf(crit[:, i], [idx[i]])[1].ravel())\n    mmlar = np.column_stack(([frac] + mml))\n    if title:\n        title = (title + ' Probabilites')\n    else:\n        title = 'Probabilities'\n    if (varnames is None):\n        varnames = [('var%d' % i) for i in range((mmlar.shape[1] - 1))]\n    headers = (['prob'] + varnames)\n    return SimpleTable(mmlar, txt_fmt={\n        'data_fmts': (['%#6.3f'] + (['%#10.4f'] * (np.array(mml).shape[1] - 1))),\n    }, title=title, headers=headers)\n", "label": 0}
{"function": "\n\ndef switch_to_correct_strategy(self, w_dict, w_key):\n    if (type(w_key) is values.W_Fixnum):\n        strategy = FixnumHashmapStrategy.singleton\n    elif (type(w_key) is values.W_Symbol):\n        strategy = SymbolHashmapStrategy.singleton\n    elif isinstance(w_key, values_string.W_String):\n        strategy = StringHashmapStrategy.singleton\n    elif isinstance(w_key, values.W_Bytes):\n        strategy = ByteHashmapStrategy.singleton\n    else:\n        strategy = ObjectHashmapStrategy.singleton\n    storage = strategy.create_storage([], [])\n    w_dict.strategy = strategy\n    w_dict.hstorage = storage\n", "label": 0}
{"function": "\n\ndef visit_table(self, table, asfrom=False, iscrud=False, ashint=False, fromhints=None, **kwargs):\n    if (asfrom or ashint):\n        if getattr(table, 'schema', None):\n            ret = ((self.preparer.quote_schema(table.schema) + '.') + self.preparer.quote(table.name))\n        else:\n            ret = self.preparer.quote(table.name)\n        if (fromhints and (table in fromhints)):\n            ret = self.format_from_hint_text(ret, table, fromhints[table], iscrud)\n        return ret\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef __init__(self, typename=None, critical=None, value=None, subject=None, issuer=None, _ext=None):\n    if (_ext is not None):\n        ext = _ext\n    elif ((subject is None) and (issuer is None)):\n        ext = crypto.X509Extension(typename, critical, value)\n    elif ((subject is not None) and (issuer is None)):\n        subject = subject._cert\n        ext = crypto.X509Extension(typename, critical, value, subject=subject)\n    elif ((subject is None) and (issuer is not None)):\n        issuer = issuer._cert\n        ext = crypto.X509Extension(typename, critical, value, issuer=issuer)\n    elif ((subject is not None) and (issuer is not None)):\n        issuer = issuer._cert\n        ext = crypto.X509Extension(typename, critical, value, subject=subject, issuer=issuer)\n    self._ext = ext\n", "label": 1}
{"function": "\n\ndef __init__(self, vm_spec):\n    'Initialize a CloudStack virtual machine.\\n\\n    Args:\\n      vm_spec: virtual_machine.BaseVirtualMachineSpec object of the vm.\\n    '\n    super(CloudStackVirtualMachine, self).__init__(vm_spec)\n    self.network = cloudstack_network.CloudStackNetwork.GetNetwork(self)\n    self.cs = util.CsClient(FLAGS.CS_API_URL, FLAGS.CS_API_KEY, FLAGS.CS_API_SECRET)\n    self.project_id = None\n    if FLAGS.project:\n        project = self.cs.get_project(FLAGS.project)\n        assert project, 'Project not found'\n        self.project_id = project['id']\n    zone = self.cs.get_zone(self.zone)\n    assert zone, 'Zone not found'\n    self.zone_id = zone['id']\n    self.user_name = self.DEFAULT_USER_NAME\n    self.image = (self.image or self.DEFAULT_IMAGE)\n    self.disk_counter = 0\n", "label": 0}
{"function": "\n\n@count_calls\ndef update_versions(self, reference_resolution):\n    'A mock update_versions implementation, does the update indeed but\\n        partially.\\n\\n        :param reference_resolution: The reference_resolution dictionary\\n        :return: a list of new versions\\n        '\n    new_versions = []\n    from stalker import Version\n    current_version = self.get_current_version()\n    for version in reference_resolution['create']:\n        local_reference_resolution = self.open(version, force=True)\n        new_version = Version(task=version.task, take_name=version.take_name, parent=version, description='Automatically created with Deep Reference Update')\n        new_version.is_published = True\n        for v in self._version.inputs:\n            new_version.inputs.append(v.latest_published_version)\n        new_versions.append(new_version)\n    current_version_after_create = self.get_current_version()\n    if current_version:\n        if (current_version != current_version_after_create):\n            self.open(current_version)\n        reference_resolution['update'].extend(reference_resolution['create'])\n        self.update_first_level_versions(reference_resolution)\n    return new_versions\n", "label": 0}
{"function": "\n\ndef scan(self, result):\n    if result.valid:\n        if result.description.startswith('ASCII cpio archive'):\n            self.consecutive_hits += 1\n            if ((not self.found_archive) or (self.found_archive_in_file != result.file.name)):\n                self.found_archive_in_file = result.file.name\n                self.found_archive = True\n                result.extract = True\n            elif ('TRAILER!!!' in result.description):\n                self.found_archive = False\n                result.extract = False\n                self.consecutive_hits = 0\n            else:\n                result.extract = False\n        elif (self.consecutive_hits < 4):\n            self.found_archive = False\n            self.found_archive_in_file = None\n            self.consecutive_hits = 0\n        elif (self.consecutive_hits >= 4):\n            result.valid = False\n", "label": 1}
{"function": "\n\ndef _visit_call_helper_list(self, node):\n    name = self.visit(node.func)\n    if node.args:\n        args = [self.visit(e) for e in node.args]\n        args = ', '.join([e for e in args if e])\n    else:\n        args = '[]'\n    return ('%s(%s)' % (name, args))\n", "label": 0}
{"function": "\n\ndef pseudo_raw_input(self, prompt):\n    \"copied from cmd's cmdloop; like raw_input, but accounts for changed stdin, stdout\"\n    if self.use_rawinput:\n        try:\n            line = raw_input(prompt)\n        except EOFError:\n            line = 'EOF'\n    else:\n        self.stdout.write(prompt)\n        self.stdout.flush()\n        line = self.stdin.readline()\n        if (not len(line)):\n            line = 'EOF'\n        elif (line[(- 1)] == '\\n'):\n            line = line[:(- 1)]\n    return line\n", "label": 0}
{"function": "\n\ndef _get_lock_path(name, lock_file_prefix, lock_path=None):\n    name = name.replace(os.sep, '_')\n    if lock_file_prefix:\n        sep = ('' if lock_file_prefix.endswith('-') else '-')\n        name = ('%s%s%s' % (lock_file_prefix, sep, name))\n    local_lock_path = (lock_path or CONF.oslo_concurrency.lock_path)\n    if (not local_lock_path):\n        raise cfg.RequiredOptError('lock_path')\n    return os.path.join(local_lock_path, name)\n", "label": 0}
{"function": "\n\ndef perform_search(self, dir, s=None, start=None, update_search_start=False):\n    self.cancel_highlight()\n    if (s is None):\n        s = self.last_search_string\n        if (s is None):\n            self.ui.message('No previous search term.')\n            return False\n    else:\n        self.last_search_string = s\n    if (start is None):\n        start = self.search_start\n    case_insensitive = (s.lower() == s)\n    if (start > len(self.ui.source)):\n        start = 0\n    i = ((start + dir) % len(self.ui.source))\n    if (i >= len(self.ui.source)):\n        i = 0\n    while (i != start):\n        sline = self.ui.source[i].text\n        if case_insensitive:\n            sline = sline.lower()\n        if (s in sline):\n            sl = self.ui.source[i]\n            sl.set_highlight(True)\n            self.highlight_line = sl\n            self.ui.source.set_focus(i)\n            if update_search_start:\n                self.search_start = i\n            return True\n        i = ((i + dir) % len(self.ui.source))\n    return False\n", "label": 1}
{"function": "\n\ndef __init__(self, request, *args, **kwargs):\n    super(SelectPluginAction, self).__init__(request, *args, **kwargs)\n    try:\n        plugins = saharaclient.plugin_list(request)\n    except Exception:\n        plugins = []\n        exceptions.handle(request, _('Unable to fetch plugin list.'))\n    plugin_choices = [(plugin.name, plugin.title) for plugin in plugins]\n    self.fields['plugin_name'] = forms.ChoiceField(label=_('Plugin name'), choices=plugin_choices, widget=forms.Select(attrs={\n        'class': 'plugin_name_choice',\n    }))\n    for plugin in plugins:\n        field_name = (plugin.name + '_version')\n        choice_field = forms.ChoiceField(label=_('Version'), choices=[(version, version) for version in plugin.versions], widget=forms.Select(attrs={\n            'class': (('plugin_version_choice ' + field_name) + '_choice'),\n        }))\n        self.fields[field_name] = choice_field\n", "label": 0}
{"function": "\n\n@float_format.setter\ndef float_format(self, val):\n    if ((val is None) or (isinstance(val, dict) and (len(val) is 0))):\n        self._float_format = {\n            \n        }\n    else:\n        self._validate_option('float_format', val)\n        for field in self._field_names:\n            self._float_format[field] = val\n", "label": 0}
{"function": "\n\ndef __resolve_options(ctx, value):\n    'Resolve the given value to JipUndefined or, if its not\\n    an option, try to find one if a tool is associated with the context\\n\\n    :param ctx: the context\\n    :param value: the source value\\n    :returns: resolved or as is\\n    '\n    if isinstance(value, JipUndefined):\n        value = value._undefined_name\n    if (not isinstance(value, Option)):\n        script = ctx.get('tool', None)\n        if script:\n            v = script.options[value]\n            if (v is not None):\n                value = v\n    return value\n", "label": 0}
{"function": "\n\ndef answers(self, other):\n    if (other.id == self.id):\n        if (self.code == 1):\n            return 1\n        if ((other.code in [2, 4, 6, 8, 10]) and (self.code == (other.code + 1))):\n            if (other.code == 8):\n                return 1\n            return self.payload.answers(other.payload)\n    return 0\n", "label": 1}
{"function": "\n\ndef handle_children_state(children, kids):\n    'Given a list of children (as `children`) of a particular object\\n    and their states in the `kids` argument, this function sets up the\\n    children by removing unnecessary ones, fixing existing ones and\\n    adding new children if necessary (depending on the state).\\n    '\n    m_children = list(children)\n    (n_child, n_kid) = (len(m_children), len(kids))\n    for i in range((n_child - n_kid)):\n        m_children.pop()\n    for i in range(n_child):\n        (child, kid) = (m_children[i], kids[i])\n        md = kid.__metadata__\n        if ((child.__module__ != md['module']) or (child.__class__.__name__ != md['class_name'])):\n            m_children[i] = create_instance(kid)\n    for i in range((n_kid - n_child)):\n        child = create_instance(kids[(n_child + i)])\n        m_children.append(child)\n    children[:] = m_children\n", "label": 1}
{"function": "\n\ndef test_random_udir_noweights(self):\n    if (not use_networkx):\n        sys.stderr.write('Skipping TestBetweenessCentrality.test_random_udir_noweights due to missing networkx library\\n')\n        return\n    Gnx = networkx.Graph()\n    G = Graph()\n    nodes = range(500)\n    random.shuffle(nodes)\n    while (len(nodes) > 0):\n        n1 = nodes.pop()\n        n2 = nodes.pop()\n        w = random.randint(1, 5)\n        eidx = G.add_edge(n1, n2)\n        G.set_weight_(eidx, w)\n        Gnx.add_edge(n1, n2, weight=w)\n    nodes = range(500)\n    random.shuffle(nodes)\n    while (len(nodes) > 0):\n        n1 = nodes.pop()\n        n2 = nodes.pop()\n        if (not G.has_edge(n1, n2)):\n            w = random.randint(1, 5)\n            eidx = G.add_edge(n1, n2)\n            G.set_weight_(eidx, w)\n            Gnx.add_edge(n1, n2, weight=w)\n    R = betweenness_centrality_(G, True, False)\n    Rnx = networkx.betweenness_centrality(Gnx, normalized=True, weight=None)\n    for n in G.nodes_iter():\n        self.assertAlmostEqual(Rnx[n], R[G.node_idx(n)])\n", "label": 1}
{"function": "\n\ndef next(self):\n    if ((self.length is not None) and (self.length <= 0)):\n        raise StopIteration\n    chunk = self.fileobj.read(self.chunk_size)\n    if (not chunk):\n        raise StopIteration\n    if (self.length is not None):\n        self.length -= len(chunk)\n        if (self.length < 0):\n            chunk = chunk[:self.length]\n    return chunk\n", "label": 1}
{"function": "\n\ndef run(self):\n    super(FakeDeletionThread, self).run()\n    receiver = NailgunReceiver\n    kwargs = {\n        'task_uuid': self.task_uuid,\n        'nodes': self.data['args']['nodes'],\n        'status': 'ready',\n    }\n    nodes_to_restore = copy.deepcopy(self.data['args'].get('nodes_to_restore', []))\n    resp_method = getattr(receiver, self.respond_to)\n    try:\n        resp_method(**kwargs)\n        db().commit()\n    except Exception:\n        db().rollback()\n        raise\n    recover_nodes = self.params.get('recover_nodes', True)\n    recover_offline_nodes = self.params.get('recover_offline_nodes', True)\n    if (not recover_nodes):\n        db().commit()\n        return\n    for node_data in nodes_to_restore:\n        is_offline = (('online' in node_data) and (not node_data['online']))\n        if (is_offline and (not recover_offline_nodes)):\n            continue\n        node_data['status'] = 'discover'\n        objects.Node.create(node_data)\n    db().commit()\n", "label": 1}
{"function": "\n\ndef find_node(self, node, path):\n    'Finds a node by the given path from the given node.'\n    for hash_value in path:\n        if isinstance(node, LeafStatisticsNode):\n            break\n        for stats in node.get_child_keys():\n            if (hash(stats) == hash_value):\n                node = node.get_child_node(stats)\n                break\n        else:\n            break\n    return node\n", "label": 1}
{"function": "\n\ndef add_jumpbox(host):\n    if jb.cluster.locate(host):\n        return\n    print('Connecting to jumpbox:', host)\n    try:\n        t = ssh.connection_worker(host, None, init_data['auth'])\n        retries = 3\n        while ((not t.is_authenticated()) and (retries > 0)):\n            print(('Failed to authenticate to Jumpbox (%s)' % host))\n            jb_user = raw_input(('Enter username for [%s]: ' % host))\n            jb_passwd = getpass.getpass(('Enter password for %s@%s: ' % (jb_user, host)))\n            reauth = AuthManager(jb_user, auth_file=None, include_agent=False, include_userkeys=False, default_password=jb_passwd)\n            t = ssh.connection_worker(host, None, reauth)\n            retries -= 1\n    except Exception as e:\n        print(host, repr(e))\n    finally:\n        jb.cluster.connections[host] = t\n", "label": 0}
{"function": "\n\ndef _finish_query_lookupd(self, response, lookupd_url):\n    if response.error:\n        logger.warning('[%s] lookupd %s query error: %s', self.name, lookupd_url, response.error)\n        return\n    try:\n        lookup_data = json.loads(response.body)\n    except ValueError:\n        logger.warning('[%s] lookupd %s failed to parse JSON: %r', self.name, lookupd_url, response.body)\n        return\n    if (lookup_data['status_code'] != 200):\n        logger.warning('[%s] lookupd %s responded with %d', self.name, lookupd_url, lookup_data['status_code'])\n        return\n    for producer in lookup_data['data']['producers']:\n        address = producer.get('broadcast_address', producer.get('address'))\n        assert address\n        self.connect_to_nsqd(address, producer['tcp_port'])\n", "label": 1}
{"function": "\n\ndef create_with_kwargs(self, context, **kwargs):\n    volume_id = kwargs.get('volume_id', None)\n    v = fake_volume(kwargs['size'], kwargs['name'], kwargs['description'], str(volume_id), None, None, None, None)\n    if (kwargs.get('status', None) is not None):\n        v.vol['status'] = kwargs['status']\n    if (kwargs['host'] is not None):\n        v.vol['host'] = kwargs['host']\n    if (kwargs['attach_status'] is not None):\n        v.vol['attach_status'] = kwargs['attach_status']\n    if (kwargs.get('snapshot_id', None) is not None):\n        v.vol['snapshot_id'] = kwargs['snapshot_id']\n    self.volume_list.append(v.vol)\n    return v.vol\n", "label": 0}
{"function": "\n\ndef createAES(key, IV, implList=None):\n    'Create a new AES object.\\n\\n    @type key: str\\n    @param key: A 16, 24, or 32 byte string.\\n\\n    @type IV: str\\n    @param IV: A 16 byte string\\n\\n    @rtype: L{tlslite.utils.AES}\\n    @return: An AES object.\\n    '\n    if (implList == None):\n        implList = ['openssl', 'pycrypto', 'python']\n    for impl in implList:\n        if ((impl == 'openssl') and cryptomath.m2cryptoLoaded):\n            return openssl_aes.new(key, 2, IV)\n        elif ((impl == 'pycrypto') and cryptomath.pycryptoLoaded):\n            return pycrypto_aes.new(key, 2, IV)\n        elif (impl == 'python'):\n            return python_aes.new(key, 2, IV)\n    raise NotImplementedError()\n", "label": 1}
{"function": "\n\n@xmlrpc_func(returns='string[]', args=['string'])\ndef pingback_extensions_get_pingbacks(target):\n    \"\\n    pingback.extensions.getPingbacks(url) => '[url, url, ...]'\\n\\n    Returns an array of URLs that link to the specified url.\\n\\n    See: http://www.aquarionics.com/misc/archives/blogite/0198.html\\n    \"\n    site = Site.objects.get_current()\n    (scheme, netloc, path, query, fragment) = urlsplit(target)\n    if (netloc != site.domain):\n        return TARGET_DOES_NOT_EXIST\n    try:\n        (view, args, kwargs) = resolve(path)\n    except Resolver404:\n        return TARGET_DOES_NOT_EXIST\n    try:\n        entry = Entry.published.get(slug=kwargs['slug'], publication_date__year=kwargs['year'], publication_date__month=kwargs['month'], publication_date__day=kwargs['day'])\n    except (KeyError, Entry.DoesNotExist):\n        return TARGET_IS_NOT_PINGABLE\n    return [pingback.user_url for pingback in entry.pingbacks]\n", "label": 0}
{"function": "\n\ndef on_modified(self, view):\n    if view.settings().get('is_widget'):\n        return\n    if (not view.settings().get('auto_wrap', False)):\n        return\n    if (not self.check_selection(view)):\n        return\n    insertpt = self.get_insert_pt(view)\n    if (not insertpt):\n        return\n    self.set_status()\n    join = (self.status >= 2)\n    left_delete = (' ' not in view.substr(sublime.Region((insertpt - 1), (insertpt + 1))))\n    view.settings().set('auto_wrap', False)\n    view.run_command('auto_wrap_insert', {\n        'insertpt': insertpt,\n        'join': join,\n        'left_delete': self.left_delete,\n    })\n    self.left_delete = left_delete\n    view.settings().set('auto_wrap', True)\n", "label": 0}
{"function": "\n\ndef profile_head_single(benchmark):\n    import gc\n    results = []\n    gc.collect()\n    try:\n        from ctypes import cdll, CDLL\n        cdll.LoadLibrary('libc.so.6')\n        libc = CDLL('libc.so.6')\n        libc.malloc_trim(0)\n    except:\n        pass\n    N = (args.hrepeats + args.burnin)\n    results = []\n    try:\n        for i in range(N):\n            gc.disable()\n            d = dict()\n            try:\n                d = benchmark.run()\n            except KeyboardInterrupt:\n                raise\n            except Exception as e:\n                err = ''\n                try:\n                    err = d.get('traceback')\n                    if (err is None):\n                        err = str(e)\n                except:\n                    pass\n                print(('%s died with:\\n%s\\nSkipping...\\n' % (benchmark.name, err)))\n            results.append(d.get('timing', np.nan))\n            gc.enable()\n            gc.collect()\n    finally:\n        gc.enable()\n    if results:\n        results = results[args.burnin:]\n    sys.stdout.write('.')\n    sys.stdout.flush()\n    return Series(results, name=benchmark.name)\n", "label": 1}
{"function": "\n\ndef replace(self, new):\n    'Replaces this node with a new one in the parent.'\n    assert (self.parent is not None), str(self)\n    assert (new is not None)\n    if (not isinstance(new, list)):\n        new = [new]\n    l_children = []\n    found = False\n    for ch in self.parent.children:\n        if (ch is self):\n            assert (not found), (self.parent.children, self, new)\n            if (new is not None):\n                l_children.extend(new)\n            found = True\n        else:\n            l_children.append(ch)\n    assert found, (self.children, self, new)\n    self.parent.changed()\n    self.parent.children = l_children\n    for x in new:\n        x.parent = self.parent\n    self.parent = None\n", "label": 1}
{"function": "\n\ndef block_average_above(x, y, new_x):\n    '\\n    Linearly interpolates values in new_x based on the values in x and y.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        Independent values.\\n    y : array_like\\n        Dependent values.\\n    new_x : array_like\\n        The x values to interpolate y values.\\n\\n    '\n    bad_index = None\n    x = atleast_1d_and_contiguous(x, np.float64)\n    y = atleast_1d_and_contiguous(y, np.float64)\n    new_x = atleast_1d_and_contiguous(new_x, np.float64)\n    if (y.ndim > 2):\n        raise ValueError('`linear` only works with 1-D or 2-D arrays.')\n    if (len(y.shape) == 2):\n        new_y = np.zeros((y.shape[0], len(new_x)), np.float64)\n        for i in range(len(new_y)):\n            bad_index = _interpolate.block_averave_above_dddd(x, y[i], new_x, new_y[i])\n            if (bad_index is not None):\n                break\n    else:\n        new_y = np.zeros(len(new_x), np.float64)\n        bad_index = _interpolate.block_average_above_dddd(x, y, new_x, new_y)\n    if (bad_index is not None):\n        msg = ('block_average_above cannot extrapolate and new_x[%d]=%f is out of the x range (%f, %f)' % (bad_index, new_x[bad_index], x[0], x[(- 1)]))\n        raise ValueError(msg)\n    return new_y\n", "label": 1}
{"function": "\n\ndef get_flinalg_funcs(names, arrays=(), debug=0):\n    'Return optimal available _flinalg function objects with\\n    names. arrays are used to determine optimal prefix.'\n    ordering = []\n    for i in range(len(arrays)):\n        t = arrays[i].dtype.char\n        if (t not in _type_conv):\n            t = 'd'\n        ordering.append((t, i))\n    if ordering:\n        ordering.sort()\n        required_prefix = _type_conv[ordering[0][0]]\n    else:\n        required_prefix = 'd'\n    if (ordering and has_column_major_storage(arrays[ordering[0][1]])):\n        (suffix1, suffix2) = ('_c', '_r')\n    else:\n        (suffix1, suffix2) = ('_r', '_c')\n    funcs = []\n    for name in names:\n        func_name = (required_prefix + name)\n        func = getattr(_flinalg, (func_name + suffix1), getattr(_flinalg, (func_name + suffix2), None))\n        funcs.append(func)\n    return tuple(funcs)\n", "label": 1}
{"function": "\n\ndef maximalSquare(self, matrix):\n    if (not matrix):\n        return 0\n    (m, n) = (len(matrix), len(matrix[0]))\n    size = [[0 for j in xrange(n)] for i in xrange(2)]\n    max_size = 0\n    for j in xrange(n):\n        if (matrix[0][j] == '1'):\n            size[0][j] = 1\n        max_size = max(max_size, size[0][j])\n    for i in xrange(1, m):\n        if (matrix[i][0] == '1'):\n            size[(i % 2)][0] = 1\n        else:\n            size[(i % 2)][0] = 0\n        for j in xrange(1, n):\n            if (matrix[i][j] == '1'):\n                size[(i % 2)][j] = (min(size[(i % 2)][(j - 1)], size[((i - 1) % 2)][j], size[((i - 1) % 2)][(j - 1)]) + 1)\n                max_size = max(max_size, size[(i % 2)][j])\n            else:\n                size[(i % 2)][j] = 0\n    return (max_size * max_size)\n", "label": 1}
{"function": "\n\n@utils.enforce_id_param\ndef edit_playlist(self, playlist_id, new_name=None, new_description=None, public=None):\n    'Changes the name of a playlist and returns its id.\\n\\n        :param playlist_id: the id of the playlist\\n        :param new_name: (optional) desired title\\n        :param new_description: (optional) desired description\\n        :param public: (optional) if True and the user has a subscription, share playlist.\\n        '\n    if all(((value is None) for value in (new_name, new_description, public))):\n        raise ValueError('new_name, new_description, or public must be provided')\n    if (public is None):\n        share_state = public\n    else:\n        share_state = ('PUBLIC' if public else 'PRIVATE')\n    mutate_call = mobileclient.BatchMutatePlaylists\n    update_mutations = mutate_call.build_playlist_updates([{\n        'id': playlist_id,\n        'name': new_name,\n        'description': new_description,\n        'public': share_state,\n    }])\n    res = self._make_call(mutate_call, update_mutations)\n    return res['mutate_response'][0]['id']\n", "label": 0}
{"function": "\n\ndef main():\n    global imap_account\n    global routes\n    if (conf is None):\n        return 1\n    logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n    for routing in routings:\n        (methods, regex, app) = routing[:3]\n        if isinstance(methods, six.string_types):\n            methods = (methods,)\n        vars = (routing[3] if (len(routing) >= 4) else {\n            \n        })\n        routes.append((methods, re.compile(regex), app, vars))\n        log.info('Route {} openned'.format(regex[1:(- 1)]))\n    try:\n        imap_account = imap_cli.connect(**conf)\n        httpd = simple_server.make_server('127.0.0.1', 8000, router)\n        log.info('Serving on http://127.0.0.1:8000')\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        log.info('Interupt by user, exiting')\n    return 0\n", "label": 1}
{"function": "\n\ndef sql_flush(self, style, tables, sequences):\n    if tables:\n        if ((self.postgres_version[0] >= 8) and (self.postgres_version[1] >= 1)):\n            sql = [('%s %s;' % (style.SQL_KEYWORD('TRUNCATE'), style.SQL_FIELD(', '.join([self.quote_name(table) for table in tables]))))]\n        else:\n            sql = [('%s %s %s;' % (style.SQL_KEYWORD('DELETE'), style.SQL_KEYWORD('FROM'), style.SQL_FIELD(self.quote_name(table)))) for table in tables]\n        for sequence_info in sequences:\n            table_name = sequence_info['table']\n            column_name = sequence_info['column']\n            if (column_name and (len(column_name) > 0)):\n                sequence_name = ('%s_%s_seq' % (table_name, column_name))\n            else:\n                sequence_name = ('%s_id_seq' % table_name)\n            sql.append((\"%s setval('%s', 1, false);\" % (style.SQL_KEYWORD('SELECT'), style.SQL_FIELD(self.quote_name(sequence_name)))))\n        return sql\n    else:\n        return []\n", "label": 1}
{"function": "\n\ndef expandtargets(self, source_glob, target):\n    if ((not isdirname(target)) and (('*' in source_glob) or ('?' in source_glob))):\n        raise SymlinkError(\"Invalid symlink: {0} => {1} (did you mean to add a trailing '/'?)\".format(source_glob, target))\n    mkdir_p(os.path.dirname(os.path.expanduser(target)))\n    sources = iglob(os.path.join(self.symlink_dir, source_glob))\n    expanded = []\n    for source in sources:\n        source = os.path.join(self.cider_dir, source)\n        source_target = self.expandtarget(source, target)\n        expanded.append((source, source_target))\n    return expanded\n", "label": 0}
{"function": "\n\ndef _topological_sort(self, goal_info_by_goal):\n    dependees_by_goal = OrderedDict()\n\n    def add_dependee(goal, dependee=None):\n        dependees = dependees_by_goal.get(goal)\n        if (dependees is None):\n            dependees = set()\n            dependees_by_goal[goal] = dependees\n        if dependee:\n            dependees.add(dependee)\n    for (goal, goal_info) in goal_info_by_goal.items():\n        add_dependee(goal)\n        for dependency in goal_info.goal_dependencies:\n            add_dependee(dependency, goal)\n    satisfied = set()\n    while dependees_by_goal:\n        count = len(dependees_by_goal)\n        for (goal, dependees) in dependees_by_goal.items():\n            unsatisfied = len((dependees - satisfied))\n            if (unsatisfied == 0):\n                satisfied.add(goal)\n                dependees_by_goal.pop(goal)\n                (yield goal_info_by_goal[goal])\n                break\n        if (len(dependees_by_goal) == count):\n            for dependees in dependees_by_goal.values():\n                dependees.difference_update(satisfied)\n            raise self.GoalCycleError('Cycle detected in goal dependencies:\\n\\t{0}'.format('\\n\\t'.join(('{0} <- {1}'.format(goal, list(dependees)) for (goal, dependees) in dependees_by_goal.items()))))\n", "label": 1}
{"function": "\n\ndef overrules(self, other):\n    '\\n        Detects if the other index is a non-unique, non primary index\\n        that can be overwritten by this one.\\n\\n        :param other: The other index\\n        :type other: Index\\n\\n        :rtype: bool\\n        '\n    if other.is_primary():\n        return False\n    elif (self.is_simple_index() and other.is_unique()):\n        return False\n    same_columns = self.spans_columns(other.get_columns())\n    if (same_columns and (self.is_primary() or self.is_unique()) and self.same_partial_index(other)):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef resolve_columns(self, row, fields=()):\n    '\\n        This routine is necessary so that distances and geometries returned\\n        from extra selection SQL get resolved appropriately into Python\\n        objects.\\n        '\n    values = []\n    aliases = self.extra_select.keys()\n    if self.aggregates:\n        aliases.extend([None for i in xrange(len(self.aggregates))])\n    rn_offset = 0\n    if SpatialBackend.oracle:\n        if ((self.high_mark is not None) or self.low_mark):\n            rn_offset = 1\n    index_start = (rn_offset + len(aliases))\n    values = [self.convert_values(v, self.extra_select_fields.get(a, None)) for (v, a) in izip(row[rn_offset:index_start], aliases)]\n    if (SpatialBackend.oracle or getattr(self, 'geo_values', False)):\n        for (value, field) in izip(row[index_start:], fields):\n            values.append(self.convert_values(value, field))\n    else:\n        values.extend(row[index_start:])\n    return tuple(values)\n", "label": 1}
{"function": "\n\n@utils.synchronized('dhcp-agent')\ndef port_update_end(self, context, payload):\n    'Handle the port.update.end notification event.'\n    updated_port = dhcp.DictModel(payload['port'])\n    network = self.cache.get_network_by_id(updated_port.network_id)\n    if network:\n        LOG.info(_LI('Trigger reload_allocations for port %s'), updated_port)\n        driver_action = 'reload_allocations'\n        if self._is_port_on_this_agent(updated_port):\n            orig = self.cache.get_port_by_id(updated_port['id'])\n            old_ips = {i['ip_address'] for i in (orig['fixed_ips'] or [])}\n            new_ips = {i['ip_address'] for i in updated_port['fixed_ips']}\n            if (old_ips != new_ips):\n                driver_action = 'restart'\n        self.cache.put_port(updated_port)\n        self.call_driver(driver_action, network)\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    get_hbase_service_config(args)\n    for job_name in (args.job or reversed(ALL_JOBS)):\n        hosts = args.hbase_config.jobs[job_name].hosts\n        task_list = deploy_utils.schedule_task_for_threads(args, hosts, job_name, 'stop')\n        parallel_deploy.start_deploy_threads(stop_job, task_list)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.hbase_config.jobs[job_name].hosts\n        task_list = deploy_utils.schedule_task_for_threads(args, hosts, job_name, 'start', is_wait=True)\n        parallel_deploy.start_deploy_threads(start_job, task_list)\n", "label": 1}
{"function": "\n\ndef onModuleSourceCode(self, module_name, source_code):\n    annotations = {\n        \n    }\n    for (count, line) in enumerate(source_code.split('\\n')):\n        match = re.search('#.*pylint:\\\\s*disable=\\\\s*([\\\\w,]+)', line)\n        if match:\n            comment_only = (line[:(line.find('#') - 1)].strip() == '')\n            if comment_only:\n                pass\n            else:\n                annotations[(count + 1)] = set((match.strip() for match in match.group(1).split(',')))\n    if annotations:\n        self.line_annotations[module_name] = annotations\n    return source_code\n", "label": 1}
{"function": "\n\ndef get_mtv(self):\n    pos = (self.pos + self.vec)\n    pos = collision.uncenter_position(pos, self.col.bbox)\n    q = collections.deque((Vector3(),))\n    while q:\n        current_vector = q.popleft()\n        transform_vectors = self.col.check_collision(pos, current_vector)\n        if (not all(transform_vectors)):\n            break\n        for vector in transform_vectors:\n            test_vec = ((self.vec + current_vector) + vector)\n            if (test_vec.dist_sq() <= (self.vec.dist_sq() + FP_MAGIC)):\n                q.append((current_vector + vector))\n    else:\n        logger.debug('Physics failed to generate an MTV, bailing out')\n        self.vec.zero()\n        return Vector3()\n    possible_mtv = [current_vector]\n    while q:\n        current_vector = q.popleft()\n        transform_vectors = self.col.check_collision(pos, current_vector)\n        if (not all(transform_vectors)):\n            possible_mtv.append(current_vector)\n    return min(possible_mtv)\n", "label": 1}
{"function": "\n\ndef __call__(self, request, *args, **kwargs):\n    '\\n        In case of the wrapped view being determined as a class (that it has\\n        a dispatch attribute) we return a new instance of the class with all\\n        view arguments passed to the dispatch method. The case of a view\\n        function things are much simpler, we just call the view function with\\n        the view arguments.\\n\\n        For debugging purposes we insert some additional information that is\\n        useful for view classes in the raised exception.\\n        '\n    try:\n        if isclass(self.view):\n            view = self.view()\n            if hasattr(view, 'dispatch'):\n                return view.dispatch(request, *args, **kwargs)\n        else:\n            view = self.view\n        if callable(view):\n            return view(request, *args, **kwargs)\n    except (Http404, PermissionDenied, SystemExit):\n        raise\n    except Exception as ex:\n        try:\n            (cls, e, trace) = sys.exc_info()\n            msg = ('%s in %s.%s: %s' % (cls.__name__, self.view.__module__, self.view.__name__, e))\n        except Exception:\n            raise ex\n        else:\n            raise UtkikException(msg).with_traceback(trace)\n    raise ImproperlyConfigured(('%s.%s does not define a view function or class view.' % (self.view.__module__, self.view.__name__)))\n", "label": 1}
{"function": "\n\ndef _safeio(self, callback):\n    if self.closed:\n        raise ValueError('I/O operation on closed file')\n    for i in range(self.retries):\n        try:\n            if (not self.stream):\n                self.stream = self._reconnect()\n            return callback(self.stream)\n        except (EOFError, IOError, OSError, socket.error):\n            if (i >= (self.retries - 1)):\n                raise\n            if self.stream:\n                self.stream.close()\n            self.stream = None\n            time.sleep(0.5)\n", "label": 1}
{"function": "\n\ndef wannabe_omnispec_to_rspec(omnispec, filter_allocated):\n    root = ET.Element('rspec')\n    for (id, r) in omnispec.get_resources().items():\n        if (filter_allocated and (not r.get_allocate())):\n            continue\n        res_type = r.get_type()\n        if (res_type == 'node'):\n            add_node(root, r)\n        elif (res_type == 'link'):\n            add_link(root, r)\n        else:\n            raise Exception(('Unknown resource type ' + res_type))\n    return ET.tostring(root)\n", "label": 1}
{"function": "\n\ndef run(self):\n    arguments = self.arguments\n    wrong_sorted_files = False\n    arguments['check'] = True\n    for path in self.distribution_files():\n        for python_file in glob.iglob(os.path.join(path, '*.py')):\n            try:\n                incorrectly_sorted = SortImports(python_file, **arguments).incorrectly_sorted\n                if incorrectly_sorted:\n                    wrong_sorted_files = True\n            except IOError as e:\n                print('WARNING: Unable to parse file {0} due to {1}'.format(file_name, e))\n    if wrong_sorted_files:\n        exit(1)\n", "label": 1}
{"function": "\n\ndef get_turns_since(state, maximum=8):\n    \"A feature encoding the age of the stone at each location up to 'maximum'\\n\\n\\tNote:\\n\\t- the [maximum-1] plane is used for any stone with age greater than or equal to maximum\\n\\t- EMPTY locations are all-zero features\\n\\t\"\n    planes = np.zeros((maximum, state.size, state.size))\n    depth = 0\n    for move in state.history[::(- 1)]:\n        if (move is not go.PASS_MOVE):\n            if (state.board[move] != go.EMPTY):\n                (x, y) = move\n                if (np.sum(planes[:, x, y]) == 0):\n                    planes[(depth, x, y)] = 1\n        if (depth < (maximum - 1)):\n            depth += 1\n    return planes\n", "label": 1}
{"function": "\n\ndef download_file(url, chunk_size=(100 * 1024)):\n    ' Helper method to download a file displaying a progress bar '\n    print('Fetching:', url)\n    file_content = None\n    progressbar = None\n    if (sys.version_info.major <= 2):\n        from rplibs.progressbar import FileTransferSpeed, ETA, ProgressBar, Percentage\n        from rplibs.progressbar import Bar\n        widgets = ['\\tDownloading: ', FileTransferSpeed(), ' ', Bar(), Percentage(), '   ', ETA()]\n        file_content = []\n        bytes_read = 0\n        try:\n            usock = urllib.request.urlopen(url)\n            file_size = int(usock.headers.get('Content-Length', 10000000000.0))\n            print('File size is', round((file_size / (1024 ** 2)), 2), 'MB')\n            progressbar = ProgressBar(widgets=widgets, maxval=file_size).start()\n            while True:\n                data = usock.read(chunk_size)\n                file_content.append(data)\n                bytes_read += len(data)\n                progressbar.update(bytes_read)\n                if (not data):\n                    break\n            usock.close()\n        except Exception:\n            print('ERROR: Could not fetch', url, '!', file=sys.stderr)\n            raise\n    else:\n        print('Downloading .. (progressbar disabled due to python 3 build)')\n        try:\n            usock = urllib.request.urlopen(url)\n            file_content = []\n            while True:\n                data = usock.read(chunk_size)\n                file_content.append(data)\n                if (not data):\n                    break\n            usock.close()\n        except Exception:\n            print('ERROR: Could not fetch', url, '!', file=sys.stderr)\n            raise\n    if progressbar:\n        progressbar.finish()\n    return binary_type().join(file_content)\n", "label": 1}
{"function": "\n\ndef bitcount(self, key, start=None, end=None):\n    '\\n        Returns the count of set bits in the value of ``key``.  Optional\\n        ``start`` and ``end`` paramaters indicate which bytes to consider\\n        '\n    params = [key]\n    if ((start is not None) and (end is not None)):\n        params.append(start)\n        params.append(end)\n    elif (((start is not None) and (end is None)) or ((end is not None) and (start is None))):\n        raise RedisError('Both start and end must be specified')\n    return self.execute_command('BITCOUNT', *params)\n", "label": 1}
{"function": "\n\ndef _format_action(self, action):\n    help_position = min((self._action_max_length + 2), self._max_help_position)\n    help_width = (self._width - help_position)\n    action_width = ((help_position - self._current_indent) - 2)\n    action_header = self._format_action_invocation(action)\n    if (not action.help):\n        tup = (self._current_indent, '', action_header)\n        action_header = ('%*s%s\\n' % tup)\n    elif (len(action_header) <= action_width):\n        tup = (self._current_indent, '', action_width, action_header)\n        action_header = ('%*s%-*s  ' % tup)\n        indent_first = 0\n    else:\n        tup = (self._current_indent, '', action_header)\n        action_header = ('%*s%s\\n' % tup)\n        indent_first = help_position\n    parts = [action_header]\n    if action.help:\n        help_text = self._expand_help(action)\n        help_lines = self._split_lines(help_text, help_width)\n        parts.append(('%*s%s\\n' % (indent_first, '', help_lines[0])))\n        for line in help_lines[1:]:\n            parts.append(('%*s%s\\n' % (help_position, '', line)))\n    elif (not action_header.endswith('\\n')):\n        parts.append('\\n')\n    for subaction in self._iter_indented_subactions(action):\n        parts.append(self._format_action(subaction))\n    return self._join_parts(parts)\n", "label": 1}
{"function": "\n\ndef __wrap(self, value):\n    if isinstance(value, (tuple, set, frozenset)):\n        return type(value)([self.__wrap(v) for v in value])\n    elif (isinstance(value, list) and (not isinstance(value, Collection))):\n        return Collection(value, self.__class__)\n    elif isinstance(value, Object):\n        return value\n    elif isinstance(value, Raw):\n        return value.value\n    elif isinstance(value, dict):\n        if isinstance(self, CaseInsensitiveObject):\n            return CaseInsensitiveObject(value)\n        else:\n            return Object(value)\n    else:\n        return value\n", "label": 1}
{"function": "\n\ndef save_new_objects(self, commit=True):\n    self.new_objects = []\n    for form in self.extra_forms:\n        if (not form.has_changed()):\n            continue\n        if (self.can_delete and self._should_delete_form(form)):\n            continue\n        self.new_objects.append(self.save_new(form, commit=commit))\n        if (not commit):\n            self.saved_forms.append(form)\n    return self.new_objects\n", "label": 1}
{"function": "\n\ndef raise_tab(self, tabname):\n    l = []\n    name = tabname.lower()\n    while self.tab.has_key(name):\n        bnch = self.tab[name]\n        l.insert(0, name)\n        name = bnch.wsname.lower()\n        if (name in l):\n            break\n    for name in l:\n        (nb, index) = self._find_nb(name)\n        if ((nb is not None) and (index >= 0)):\n            nb.set_index(index)\n", "label": 1}
{"function": "\n\ndef compute(self):\n    port_object = None\n    if self.has_input('SetInputConnection0'):\n        ic = self.get_input('SetInputConnection0')\n        if hasattr(ic, 'vtkInstance'):\n            ic = ic.vtkInstance\n        producer = ic.GetProducer()\n        try:\n            port_object = producer.GetOutput()\n        except AttributeError:\n            raise ModuleError(self, 'expected a module that supports GetOutput')\n    elif self.has_input('SetInput'):\n        port_object = self.get_input('SetInput')\n        if hasattr(port_object, 'vtkInstance'):\n            port_object = port_object.vtkInstance\n    if port_object:\n        self.auto_set_results(port_object)\n", "label": 1}
{"function": "\n\n@common.check_cells_enabled\ndef sync_instances(self, req, body):\n    'Tell all cells to sync instance info.'\n    context = req.environ['nova.context']\n    authorize(context)\n    authorize(context, action='sync_instances')\n    project_id = body.pop('project_id', None)\n    deleted = body.pop('deleted', False)\n    updated_since = body.pop('updated_since', None)\n    if body:\n        msg = _(\"Only 'updated_since', 'project_id' and 'deleted' are understood.\")\n        raise exc.HTTPBadRequest(explanation=msg)\n    if isinstance(deleted, six.string_types):\n        try:\n            deleted = strutils.bool_from_string(deleted, strict=True)\n        except ValueError as err:\n            raise exc.HTTPBadRequest(explanation=six.text_type(err))\n    if updated_since:\n        try:\n            timeutils.parse_isotime(updated_since)\n        except ValueError:\n            msg = _('Invalid changes-since value')\n            raise exc.HTTPBadRequest(explanation=msg)\n    self.cells_rpcapi.sync_instances(context, project_id=project_id, updated_since=updated_since, deleted=deleted)\n", "label": 1}
{"function": "\n\ndef generate_ipv4list(num_items=100, include_hosts=False):\n    '\\n    Generate a list of unique IPv4 addresses. This is a total hack.\\n\\n    :param num_items:\\n        Number of items to generate\\n\\n    :param include_hosts:\\n        Whether to include /32 addresses\\n    '\n    ipset = set()\n    while (len(ipset) < num_items):\n        ip = generate_ipv4()\n        if ip.startswith('0'):\n            continue\n        if ip.endswith('.0.0.0'):\n            prefix = '/8'\n        elif ip.endswith('.0.0'):\n            prefix = '/16'\n        elif ip.endswith('.0'):\n            prefix = '/24'\n        elif include_hosts:\n            prefix = '/32'\n        else:\n            continue\n        ip += prefix\n        ipset.add(ip)\n    return sorted(ipset)\n", "label": 1}
{"function": "\n\ndef __init__(self, request, **kwargs):\n    \"\\n            Matching the following example jsonpath:\\n            \\n                /path/[?(@.field1='searchterm*'&@.field2='*search*')][/@['field1'],/@['field2']][0:24]\\n                \\n            The last part of the URL will contain a JSONPath-query:\\n            \\n                [filter][sort][start:end:step]\\n        \"\n    path = request.path\n    if (not path.endswith('/')):\n        path = (path + '/')\n    match = re.match('^/.*/(\\\\[.*\\\\])/$', path)\n    if match:\n        self.jsonpath = match.groups()[0]\n    if self.jsonpath:\n        parts = self.jsonpath[1:(- 1)].split('][')\n        for part in parts:\n            if part.startswith('?'):\n                self.jsonpath_filters = part\n            elif re.match('^[/\\\\\\\\].*$', part):\n                self.jsonpath_sorting = part\n            elif re.match('^\\\\d*:\\\\d*:{0,1}\\\\d*$', part):\n                self.jsonpath_paging = part\n    super(JsonQueryRestStoreInfo, self).__init__(request, **kwargs)\n", "label": 1}
{"function": "\n\ndef cwd_for_window(window):\n    \"\\n    Return the working directory in which the window's commands should run.\\n\\n    In the common case when the user has one folder open, return that.\\n    Otherwise, return one of the following (in order of preference):\\n        1) One of the open folders, preferring a folder containing the active\\n           file.\\n        2) The directory containing the active file.\\n        3) The user's home directory.\\n    \"\n    folders = window.folders()\n    if (len(folders) == 1):\n        return folders[0]\n    else:\n        active_view = window.active_view()\n        active_file_name = (active_view.file_name() if active_view else None)\n        if (not active_file_name):\n            return (folders[0] if len(folders) else os.path.expanduser('~'))\n        for folder in folders:\n            if active_file_name.startswith(folder):\n                return folder\n        return os.path.dirname(active_file_name)\n", "label": 1}
{"function": "\n\ndef _complete_batch(self, batch, error, base_offset):\n    'Complete or retry the given batch of records.\\n\\n        Arguments:\\n            batch (RecordBatch): The record batch\\n            error (Exception): The error (or None if none)\\n            base_offset (int): The base offset assigned to the records if successful\\n        '\n    if (error is Errors.NoError):\n        error = None\n    if ((error is not None) and self._can_retry(batch, error)):\n        log.warning('Got error produce response on topic-partition %s, retrying (%d attempts left). Error: %s', batch.topic_partition, ((self.config['retries'] - batch.attempts) - 1), error)\n        self._accumulator.reenqueue(batch)\n    else:\n        if (error is Errors.TopicAuthorizationFailedError):\n            error = error(batch.topic_partition.topic)\n        batch.done(base_offset, error)\n        self._accumulator.deallocate(batch)\n    if getattr(error, 'invalid_metadata', False):\n        self._metadata.request_update()\n", "label": 1}
{"function": "\n\ndef GetRealPath(file=None, encodeURL=False):\n    if (file is None):\n        file = ''\n    if ((file.find('/') != 0) and (file.find('\\\\') != 0) and (not re.search('^[a-zA-Z]+:[/\\\\\\\\]?', file))):\n        if hasattr(sys, 'frozen'):\n            path = os.path.dirname(sys.executable)\n        elif ('__file__' in globals()):\n            path = os.path.dirname(os.path.realpath(__file__))\n        else:\n            path = os.getcwd()\n        path = ((path + os.sep) + file)\n        path = re.sub('[/\\\\\\\\]+', re.escape(os.sep), path)\n        path = re.sub('[/\\\\\\\\]+$', '', path)\n        if encodeURL:\n            return urllib_pathname2url(path)\n        else:\n            return path\n    return file\n", "label": 1}
{"function": "\n\ndef closeUnusedFiles(transport):\n    import os, sys\n    notouch = transport.protectedFileNumList()\n    for each in [sys.stdin, sys.stderr, sys.stdout]:\n        try:\n            notouch.append(each.fileno())\n        except AttributeError:\n            pass\n    for fdnum in range(3, 255):\n        if (fdnum not in notouch):\n            try:\n                os.close(fdnum)\n            except OSError:\n                pass\n", "label": 1}
{"function": "\n\ndef get_context(self, bot, update, **kwargs):\n    queryset = self.get_queryset()\n    if (not self.slug_field):\n        raise AttributeError(('Generic detail view %s must be called with a slug.' % self.__class__.__name__))\n    slug_field = self.get_slug_field(**kwargs)\n    slug = self.get_slug(**kwargs)\n    if slug:\n        try:\n            object = queryset.get(**{\n                slug_field: slug,\n            })\n        except FieldError:\n            raise FieldError(('Field %s not in valid. Review slug_field' % slug_field))\n        except ObjectDoesNotExist:\n            object = None\n    else:\n        object = None\n    context = {\n        'context_object_name': object,\n    }\n    if self.context_object_name:\n        context[self.context_object_name] = object\n    return context\n", "label": 1}
{"function": "\n\ndef _check_boolean(self, row, col):\n    from .base import isspmatrix\n    if (isspmatrix(row) or isspmatrix(col)):\n        raise IndexError('Indexing with sparse matrices is not supported except boolean indexing where matrix and index are equal shapes.')\n    if (isinstance(row, np.ndarray) and (row.dtype.kind == 'b')):\n        row = self._boolean_index_to_array(row)\n    if (isinstance(col, np.ndarray) and (col.dtype.kind == 'b')):\n        col = self._boolean_index_to_array(col)\n    return (row, col)\n", "label": 1}
{"function": "\n\ndef can_fetch(self, useragent, url):\n    'using the parsed robots.txt decide if useragent can fetch url'\n    if self.disallow_all:\n        return False\n    if self.allow_all:\n        return True\n    parsed_url = urlparse.urlparse(urllib.unquote(url))\n    url = urlparse.urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment))\n    url = urllib.quote(url)\n    if (not url):\n        url = '/'\n    for entry in self.entries:\n        if entry.applies_to(useragent):\n            return entry.allowance(url)\n    if self.default_entry:\n        return self.default_entry.allowance(url)\n    return True\n", "label": 1}
{"function": "\n\ndef generate_query_string(self, otp, nonce, timestamp=False, sl=None, timeout=None):\n    '\\n        Returns a query string which is sent to the validation servers.\\n        '\n    data = [('id', self.client_id), ('otp', otp), ('nonce', nonce)]\n    if timestamp:\n        data.append(('timestamp', '1'))\n    if (sl is not None):\n        if ((sl not in range(0, 101)) and (sl not in ['fast', 'secure'])):\n            raise Exception('sl parameter value must be between 0 and 100 or string \"fast\" or \"secure\"')\n        data.append(('sl', sl))\n    if timeout:\n        data.append(('timeout', timeout))\n    query_string = urlencode(data)\n    if self.key:\n        hmac_signature = self.generate_message_signature(query_string)\n        hmac_signature = hmac_signature\n        query_string += ('&h=%s' % hmac_signature.replace('+', '%2B'))\n    return query_string\n", "label": 1}
{"function": "\n\ndef _eval_is_prime(self):\n    '\\n        If product is a positive integer, multiplication\\n        will never result in a prime number.\\n        '\n    if self.is_number:\n        '\\n            If input is a number that is not completely simplified.\\n            e.g. Mul(sqrt(3), sqrt(3), evaluate=False)\\n            So we manually evaluate it and return whether that is prime or not.\\n            '\n        r = S.One\n        for arg in self.args:\n            r *= arg\n        return r.is_prime\n    if (self.is_integer and self.is_positive):\n        '\\n            Here we count the number of arguments that have a minimum value\\n            greater than two.\\n            If there are more than one of such a symbol then the result is not prime.\\n            Else, the result cannot be determined.\\n            '\n        number_of_args = 0\n        for arg in self.args:\n            if (arg - 1).is_positive:\n                number_of_args += 1\n        if (number_of_args > 1):\n            return False\n", "label": 1}
{"function": "\n\n@handle_response_format\n@treeio_login_required\ndef mlist_delete(request, mlist_id, response_format='html'):\n    'Delete mlist page'\n    mlist = get_object_or_404(MailingList, pk=mlist_id)\n    if (not request.user.profile.has_permission(mlist, mode='w')):\n        return user_denied(request, message=\"You don't have access to this Mailing List\", response_format=response_format)\n    if request.POST:\n        if ('delete' in request.POST):\n            if ('trash' in request.POST):\n                mlist.trash = True\n                mlist.save()\n            else:\n                mlist.delete()\n            return HttpResponseRedirect('/messaging/')\n        elif ('cancel' in request.POST):\n            return HttpResponseRedirect(reverse('messaging_mlist_view', args=[mlist.id]))\n    context = _get_default_context(request)\n    context.update({\n        'mlist': mlist,\n    })\n    return render_to_response('messaging/mlist_delete', context, context_instance=RequestContext(request), response_format=response_format)\n", "label": 1}
{"function": "\n\ndef pptable(table):\n    head = table['head']\n    body = table['body']\n    pretty_head = [(('+' + '+'.join(['{:->24}'.format('') for n in head])) + '+'), (('|' + '|'.join(['{:^24}'.format(str(n)[(- 24):]) for n in head])) + '|'), (('+' + '+'.join(['{:=>24}'.format('') for n in head])) + '+')]\n    pretty_body = []\n    for row in body:\n        pretty_body.append((('|' + '|'.join(['{:>24}'.format(str(n)[(- 24):]) for n in row])) + '|'))\n        pretty_body.append((('+' + '+'.join(['{:->24}'.format('') for n in head])) + '+'))\n    prettytable = '\\n'.join((pretty_head + pretty_body))\n    print(prettytable)\n", "label": 1}
{"function": "\n\ndef check_command(command, startswith, endswith):\n    output = subprocess.check_output(command)\n    lines = output.split('\\n')\n    matches = False\n    for line in lines:\n        line = line.strip()\n        if line.startswith(startswith):\n            matches = True\n            if (not line.endswith(endswith)):\n                status = 0\n                break\n    else:\n        if matches:\n            status = 1\n        else:\n            raise BadOutputError(('The output was not in the expected format:\\n%s' % output))\n    return status\n", "label": 1}
{"function": "\n\ndef unique_list(seq, hashfunc=None):\n    seen = {\n        \n    }\n    if (not hashfunc):\n        return [x for x in seq if ((x not in seen) and (not seen.__setitem__(x, True)))]\n    else:\n        return [x for x in seq if ((hashfunc(x) not in seen) and (not seen.__setitem__(hashfunc(x), True)))]\n", "label": 1}
{"function": "\n\ndef paintEvent(self, e):\n    if (not self.addr):\n        return\n    black = QColor(0, 0, 0, 255)\n    white = QColor(255, 255, 255, 255)\n    if (not self.qr):\n        qp = QtGui.QPainter()\n        qp.begin(self)\n        qp.setBrush(white)\n        qp.setPen(white)\n        qp.drawRect(0, 0, 198, 198)\n        qp.end()\n        return\n    k = self.qr.getModuleCount()\n    qp = QtGui.QPainter()\n    qp.begin(self)\n    r = qp.viewport()\n    boxsize = ((min(r.width(), r.height()) * 0.8) / k)\n    size = (k * boxsize)\n    left = ((r.width() - size) / 2)\n    top = ((r.height() - size) / 2)\n    margin = 10\n    qp.setBrush(white)\n    qp.drawRect((left - margin), (top - margin), (size + (margin * 2)), (size + (margin * 2)))\n    for r in range(k):\n        for c in range(k):\n            if self.qr.isDark(r, c):\n                qp.setBrush(black)\n                qp.setPen(black)\n            else:\n                qp.setBrush(white)\n                qp.setPen(white)\n            qp.drawRect((left + (c * boxsize)), (top + (r * boxsize)), boxsize, boxsize)\n    qp.end()\n", "label": 1}
{"function": "\n\ndef GetFont(self):\n    '\\n        Return Font that should be used to draw text in this block\\n        '\n    font = None\n    if (self.engine.reportFormat.UseListCtrlTextFormat and self.GetListCtrl()):\n        listCtrl = self.GetListCtrl()\n        if listCtrl.IsVirtual():\n            attr = listCtrl.OnGetItemAttr(self.rowIndex)\n            if (attr and attr.HasFont()):\n                font = attr.GetFont()\n        else:\n            font = listCtrl.GetItemFont(self.rowIndex)\n    if (font and font.IsOk()):\n        return font\n    else:\n        return self.GetFormat().GetFont()\n", "label": 1}
{"function": "\n\ndef _unpack_index(self, index):\n    ' Parse index. Always return a tuple of the form (row, col).\\n        Where row/col is a integer, slice, or array of integers.\\n        '\n    from .base import spmatrix\n    if (isinstance(index, (spmatrix, np.ndarray)) and (index.ndim == 2) and (index.dtype.kind == 'b')):\n        return index.nonzero()\n    index = self._check_ellipsis(index)\n    if isinstance(index, tuple):\n        if (len(index) == 2):\n            (row, col) = index\n        elif (len(index) == 1):\n            (row, col) = (index[0], slice(None))\n        else:\n            raise IndexError('invalid number of indices')\n    else:\n        (row, col) = (index, slice(None))\n    (row, col) = self._check_boolean(row, col)\n    return (row, col)\n", "label": 1}
{"function": "\n\ndef populate(self):\n    self.page = get_page_draft(self.request.current_page)\n    if (not self.page):\n        return\n    if get_cms_setting('PERMISSION'):\n        has_global_current_page_change_permission = has_page_change_permission(self.request)\n    else:\n        has_global_current_page_change_permission = False\n    can_change = (self.request.current_page and self.request.current_page.has_change_permission(self.request))\n    if (has_global_current_page_change_permission or can_change):\n        try:\n            mypageextension = MyPageExtension.objects.get(extended_object_id=self.page.id)\n        except MyPageExtension.DoesNotExist:\n            mypageextension = None\n        try:\n            if mypageextension:\n                url = admin_reverse('extensionapp_mypageextension_change', args=(mypageextension.pk,))\n            else:\n                url = (admin_reverse('extensionapp_mypageextension_add') + ('?extended_object=%s' % self.page.pk))\n        except NoReverseMatch:\n            pass\n        else:\n            not_edit_mode = (not self.toolbar.edit_mode)\n            current_page_menu = self.toolbar.get_or_create_menu('page')\n            current_page_menu.add_modal_item(_('Page Extension'), url=url, disabled=not_edit_mode)\n", "label": 1}
{"function": "\n\ndef _updateFromPlug(self):\n    plug = self.getPlug()\n    if (plug is not None):\n        with self.getContext():\n            try:\n                value = plug.getValue()\n            except:\n                value = None\n        if (value is not None):\n            with Gaffer.BlockedConnection(self.__valueChangedConnection):\n                self.__numericWidget.setValue(value)\n        self.__numericWidget.setErrored((value is None))\n        animated = Gaffer.Animation.isAnimated(plug)\n        widgetAnimated = (GafferUI._Variant.fromVariant(self.__numericWidget._qtWidget().property('gafferAnimated')) or False)\n        if (widgetAnimated != animated):\n            self.__numericWidget._qtWidget().setProperty('gafferAnimated', GafferUI._Variant.toVariant(bool(animated)))\n            self.__numericWidget._repolish()\n    self.__numericWidget.setEditable(self._editable(canEditAnimation=True))\n", "label": 1}
{"function": "\n\ndef __init__(self, value=None, feature=None, last_supported_version=None, useinstead=None, issue=None, deprecated_since_version=None):\n    self.fullMessage = ''\n    if feature:\n        if deprecated_since_version:\n            self.fullMessage = ('%s has been deprecated since SymPy %s. ' % (feature, deprecated_since_version))\n        else:\n            self.fullMessage = ('%s has been deprecated. ' % feature)\n    if last_supported_version:\n        self.fullMessage += ('It will be last supported in SymPy version %s. ' % last_supported_version)\n    if useinstead:\n        self.fullMessage += ('Use %s instead. ' % useinstead)\n    if issue:\n        self.fullMessage += ('See https://github.com/sympy/sympy/issues/%d for more info. ' % issue)\n    if value:\n        if (not isinstance(value, str)):\n            value = ('(%s)' % repr(value))\n        value = (' ' + value)\n    else:\n        value = ''\n    self.fullMessage += value\n", "label": 1}
{"function": "\n\ndef _exc_info_to_string(self, err, test):\n    '\\n        Converts a sys.exc_info()-style tuple of values into a string.\\n        '\n    (exctype, value, tb) = err\n    while (tb and self._is_relevant_tb_level(tb)):\n        tb = tb.tb_next\n    if (exctype is test.failureException):\n        length = self._count_relevant_tb_levels(tb)\n        msgLines = traceback.format_exception(exctype, value, tb, length)\n    else:\n        msgLines = traceback.format_exception(exctype, value, tb)\n    if self.buffer:\n        output = self._stdout_buffer.getvalue()\n        error = self._stderr_buffer.getvalue()\n        if output:\n            if (not output.endswith('\\n')):\n                output += '\\n'\n            msgLines.append((STDOUT_LINE % output))\n        if error:\n            if (not error.endswith('\\n')):\n                error += '\\n'\n            msgLines.append((STDERR_LINE % error))\n    return ''.join(msgLines)\n", "label": 1}
{"function": "\n\ndef visit_Subscript(self, node):\n    if ((not isinstance(node.ctx, ast.Load)) or (not isinstance(node.value, ast.Name))):\n        return\n    if (node.value.id != 'context'):\n        return\n    if ((not isinstance(node.slice, ast.Index)) or (not isinstance(node.slice.value, ast.Str))):\n        return\n    self.contextReads.add(node.slice.value.s)\n", "label": 1}
{"function": "\n\ndef __init__(self, api, name=None, public_key=None, private_key=None, private_key_passphrase=None, consumers=[], container_ref=None, created=None, updated=None, status=None, public_key_ref=None, private_key_ref=None, private_key_passphrase_ref=None):\n    secret_refs = {\n        \n    }\n    if public_key_ref:\n        secret_refs['public_key'] = public_key_ref\n    if private_key_ref:\n        secret_refs['private_key'] = private_key_ref\n    if private_key_passphrase_ref:\n        secret_refs['private_key_passphrase'] = private_key_passphrase_ref\n    super(RSAContainer, self).__init__(api=api, name=name, consumers=consumers, container_ref=container_ref, created=created, updated=updated, status=status, secret_refs=secret_refs)\n    if public_key:\n        self.public_key = public_key\n    if private_key:\n        self.private_key = private_key\n    if private_key_passphrase:\n        self.private_key_passphrase = private_key_passphrase\n", "label": 1}
{"function": "\n\ndef teardown_method(self, method):\n    '\\n        Default teardown method for all scripts. If run through Sauce Labs OnDemand, the job name, status and tags\\n        are updated. Also the video and server log are downloaded if so configured.\\n        '\n    if hasattr(self, 'config'):\n        if (('sauce labs' in self.cf['browsers'][self.cf['saunter']['default_browser']]) and (not self.cf['browsers'][self.cf['saunter']['default_browser']]['sauce labs']['ondemand']) and self.cf['saunter']['screenshots']['on_finish']):\n            self.take_named_screenshot('final')\n    if hasattr(self, 'driver'):\n        self.driver.quit()\n    if hasattr(self.browser, 'proxy'):\n        self.config['saunter']['proxies'].append(self.proxy)\n", "label": 1}
{"function": "\n\ndef update(self, req, id, body):\n    context = req.environ['nova.context']\n    authorize(context)\n    try:\n        utils.check_string_length(id, 'quota_class_name', min_length=1, max_length=255)\n    except exception.InvalidInput as e:\n        raise webob.exc.HTTPBadRequest(explanation=e.format_message())\n    quota_class = id\n    bad_keys = []\n    if (not self.is_valid_body(body, 'quota_class_set')):\n        msg = _('quota_class_set not specified')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    quota_class_set = body['quota_class_set']\n    for key in quota_class_set.keys():\n        if (key not in self.supported_quotas):\n            bad_keys.append(key)\n            continue\n        try:\n            body['quota_class_set'][key] = utils.validate_integer(body['quota_class_set'][key], key, max_value=db.MAX_INT)\n        except exception.InvalidInput as e:\n            raise webob.exc.HTTPBadRequest(explanation=e.format_message())\n    if bad_keys:\n        msg = (_('Bad key(s) %s in quota_set') % ','.join(bad_keys))\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        nova.context.require_admin_context(context)\n    except exception.AdminRequired:\n        raise webob.exc.HTTPForbidden()\n    for (key, value) in quota_class_set.items():\n        try:\n            db.quota_class_update(context, quota_class, key, value)\n        except exception.QuotaClassNotFound:\n            db.quota_class_create(context, quota_class, key, value)\n    values = QUOTAS.get_class_quotas(context, quota_class)\n    return self._format_quota_set(None, values)\n", "label": 1}
{"function": "\n\n@render_to('race/awards.html')\ndef awards(request):\n    if request.user.is_authenticated():\n        user_badges = set(((slug, level) for (slug, level) in BadgeAward.objects.filter(user=request.user).values_list('slug', 'level')))\n    else:\n        user_badges = []\n    badges_awarded = BadgeAward.objects.values('slug', 'level').annotate(num=Count('pk'))\n    badges_counts = {'{0}_{1}'.format(k['slug'], k['level']): k['num'] for k in badges_awarded}\n    user_count = UserProfile.objects.count()\n    badges_list = list()\n    for badge_cls in badges._registry.values():\n        for (level, badge) in enumerate(badge_cls.levels):\n            badge_count = badges_counts.get('{0}_{1}'.format(badge_cls.slug, level), 0)\n            badges_list.append({\n                'slug': badge_cls.slug,\n                'level': level,\n                'name': badge.name,\n                'description': badge.description,\n                'count': badge_count,\n                'percentage': (((badge_count / float(user_count)) * 100) if user_count else 0),\n                'user_has': ((badge_cls.slug, level) in user_badges),\n            })\n    return {\n        'badges_list': badges_list,\n        'user_count': user_count,\n    }\n", "label": 1}
{"function": "\n\ndef OnKeyPressed(self, event):\n    if self.CallTipActive():\n        self.CallTipCancel()\n    key = event.GetKeyCode()\n    if ((key == 32) and event.ControlDown()):\n        pos = self.GetCurrentPos()\n        if event.ShiftDown():\n            self.CallTipSetBackground('yellow')\n            self.CallTipShow(pos, 'lots of of text: blah, blah, blah\\n\\nshow some suff, maybe parameters..\\n\\nfubar(param1, param2)')\n        else:\n            kw = keyword.kwlist[:]\n            kw.sort()\n            self.AutoCompSetIgnoreCase(False)\n            for i in range(len(kw)):\n                if (kw[i] in keyword.kwlist):\n                    kw[i] = kw[i]\n            self.AutoCompShow(0, ' '.join(kw))\n    else:\n        event.Skip()\n", "label": 1}
{"function": "\n\ndef __call__(self, parser, namespace, value, option_string=None):\n    try:\n        assert (self.dest == 'spam'), ('dest: %s' % self.dest)\n        assert (option_string == '-s'), ('flag: %s' % option_string)\n        expected_ns = NS(spam=0.25)\n        if (value in [0.125, 0.625]):\n            expected_ns.badger = 2\n        elif (value in [2.0]):\n            expected_ns.badger = 84\n        else:\n            raise AssertionError(('value: %s' % value))\n        assert (expected_ns == namespace), ('expected %s, got %s' % (expected_ns, namespace))\n    except AssertionError:\n        e = sys.exc_info()[1]\n        raise ArgumentParserError(('opt_action failed: %s' % e))\n    setattr(namespace, 'spam', value)\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    for (name, value) in self._iter_contents():\n        if (key == name):\n            break\n    else:\n        raise KeyError(('Folder entry %s not found' % repr(key)))\n    if (value == '/'):\n        qname = quote(name)\n        child_ls = self.sub_tree[(qname + '.ls')]\n        try:\n            child_sub = self.sub_tree[(qname + '.sub')]\n        except KeyError:\n            child_sub = self.sub_tree.new_tree((qname + '.sub'))\n            self.storage._autocommit()\n        return StorageDir(name, child_ls, child_sub, ((self.path + name) + '/'), self.storage, self)\n    else:\n        inode = self.storage.get_inode(value)\n        return StorageFile(name, inode, self)\n", "label": 1}
{"function": "\n\ndef _LoadArtifactsFromDatastore(self, source_urns=None, token=None, overwrite_if_exists=True):\n    'Load artifacts from the data store.'\n    if (token is None):\n        token = access_control.ACLToken(username='GRRArtifactRegistry', reason='Managing Artifacts.')\n    loaded_artifacts = []\n    for artifact_coll_urn in (source_urns or []):\n        with aff4.FACTORY.Create(artifact_coll_urn, aff4_type='RDFValueCollection', token=token, mode='rw') as artifact_coll:\n            for artifact_value in artifact_coll:\n                self.RegisterArtifact(artifact_value, source=('datastore:%s' % artifact_coll_urn), overwrite_if_exists=overwrite_if_exists)\n                loaded_artifacts.append(artifact_value)\n                logging.debug('Loaded artifact %s from %s', artifact_value.name, artifact_coll_urn)\n    revalidate = True\n    while revalidate:\n        revalidate = False\n        for artifact_obj in loaded_artifacts[:]:\n            try:\n                artifact_obj.Validate()\n            except ArtifactDefinitionError as e:\n                logging.error('Artifact %s did not validate: %s', artifact_obj.name, e)\n                artifact_obj.error_message = utils.SmartStr(e)\n                loaded_artifacts.remove(artifact_obj)\n                revalidate = True\n", "label": 1}
{"function": "\n\ndef kmeans_init(X, Y, n_labels, n_hidden_states, latent_node_features=False):\n    all_feats = []\n    for (x, y) in zip(X, Y):\n        if (len(x) == 3):\n            (features, edges, n_hidden) = x\n        elif (len(x) == 4):\n            (features, edges, _, n_hidden) = x\n        else:\n            raise ValueError('Something is fishy!')\n        n_visible = features.shape[0]\n        if latent_node_features:\n            n_visible -= n_hidden\n        if (np.max(edges) != ((n_hidden + n_visible) - 1)):\n            raise ValueError(\"Edges don't add up\")\n        labels_one_hot = np.zeros((n_visible, n_labels), dtype=np.int)\n        y = y.ravel()\n        gx = np.ogrid[:n_visible]\n        labels_one_hot[(gx, y)] = 1\n        graph = sparse.coo_matrix((np.ones(edges.shape[0]), edges.T), ((n_visible + n_hidden), (n_visible + n_hidden)))\n        graph = (graph + graph.T)[(- n_hidden):, :n_visible]\n        neighbors = (graph * labels_one_hot.reshape(n_visible, (- 1)))\n        neighbors /= np.maximum(neighbors.sum(axis=1)[:, np.newaxis], 1)\n        all_feats.append(neighbors)\n    all_feats_stacked = np.vstack(all_feats)\n    try:\n        km = KMeans(n_clusters=n_hidden_states)\n    except TypeError:\n        km = KMeans(k=n_hidden_states)\n    km.fit(all_feats_stacked)\n    H = []\n    for (y, feats) in zip(Y, all_feats):\n        H.append(np.hstack([y, (km.predict(feats) + n_labels)]))\n    return H\n", "label": 1}
{"function": "\n\ndef to_terminal(self):\n    'Yield lines to be printed in a terminal.'\n    average_cc = 0.0\n    analyzed = 0\n    for (name, blocks) in self.results:\n        if ('error' in blocks):\n            (yield (name, (blocks['error'],), {\n                'error': True,\n            }))\n            continue\n        (res, cc, n) = cc_to_terminal(blocks, self.config.show_complexity, self.config.min, self.config.max, self.config.total_average)\n        average_cc += cc\n        analyzed += n\n        if res:\n            (yield (name, (), {\n                \n            }))\n            (yield (res, (), {\n                'indent': 1,\n            }))\n    if ((self.config.average or self.config.total_average) and analyzed):\n        cc = (average_cc / analyzed)\n        ranked_cc = cc_rank(cc)\n        (yield ('\\n{0} blocks (classes, functions, methods) analyzed.', (analyzed,), {\n            \n        }))\n        (yield ('Average complexity: {0}{1} ({2}){3}', (RANKS_COLORS[ranked_cc], ranked_cc, cc, RESET), {\n            \n        }))\n", "label": 1}
{"function": "\n\ndef send_status(test_result, status_url, repo_token, pending=False):\n    if (status_url and repo_token):\n        commit_id = status_url.rstrip('/').split('/')[(- 1)]\n        log_url = (cfg.CONF.worker.log_url_prefix + commit_id)\n        headers = {\n            'Authorization': ('token ' + repo_token),\n            'Content-Type': 'application/json',\n        }\n        if pending:\n            data = {\n                'state': 'pending',\n                'description': 'Solum says: Testing in progress',\n                'target_url': log_url,\n            }\n        elif (test_result == 0):\n            data = {\n                'state': 'success',\n                'description': 'Solum says: Tests passed',\n                'target_url': log_url,\n            }\n        else:\n            data = {\n                'state': 'failure',\n                'description': 'Solum says: Tests failed',\n                'target_url': log_url,\n            }\n        try:\n            conn = get_http_connection()\n            (resp, _) = conn.request(status_url, 'POST', headers=headers, body=json.dumps(data))\n            if (resp['status'] != '201'):\n                LOG.debug(('Failed to send back status. Error code %s,status_url %s, repo_token %s' % (resp['status'], status_url, repo_token)))\n        except (httplib2.HttpLib2Error, socket.error) as ex:\n            LOG.warning(('Error in sending status, status url: %s, repo token: %s, error: %s' % (status_url, repo_token, ex)))\n    else:\n        LOG.debug('No url or token available to send back status')\n", "label": 1}
{"function": "\n\ndef do_migration(records):\n    database['boxnodesettings'].update({\n        'user_settings': {\n            '$type': 2,\n        },\n    }, {\n        '$rename': {\n            'user_settings': 'foreign_user_settings',\n        },\n    }, multi=True)\n    for user_addon in records:\n        user = user_addon.owner\n        old_account = user_addon.oauth_settings\n        logger.info('Record found for user {}'.format(user._id))\n        try:\n            account = ExternalAccount(provider='box', provider_name='Box', display_name=old_account.username, oauth_key=old_account.access_token, refresh_token=old_account.refresh_token, provider_id=old_account.user_id, expires_at=old_account.expires_at)\n            account.save()\n        except KeyExistsException:\n            account = ExternalAccount.find_one((Q('provider', 'eq', 'box') & Q('provider_id', 'eq', old_account.user_id)))\n            assert (account is not None)\n        user.external_accounts.append(account)\n        user.save()\n        user_addon.oauth_settings = None\n        user_addon.save()\n        logger.info('Added external account {0} to user {1}'.format(account._id, user._id))\n    for node in BoxNodeSettings.find():\n        if (node.foreign_user_settings is None):\n            continue\n        logger.info('Migrating user_settings for box {}'.format(node._id))\n        node.user_settings = node.foreign_user_settings\n        node.save()\n", "label": 1}
{"function": "\n\ndef test_uncollectable(self):\n    'Test uncollectable object tracking.\\n\\n        This is fixed in Python 3.4 (PEP 442).\\n        '\n    foo = Foo()\n    foo.parent = foo\n    enemy = Enemy()\n    enemy.parent = enemy\n    idfoo = id(foo)\n    idenemy = id(enemy)\n    del foo\n    del enemy\n    gb = GarbageGraph(collectable=0)\n    gfoo = [x for x in gb.metadata if (x.id == idfoo)]\n    self.assertEqual(len(gfoo), 0)\n    genemy = [x for x in gb.metadata if (x.id == idenemy)]\n    if (sys.version_info < (3, 4)):\n        self.assertEqual(len(genemy), 1)\n    self.assertEqual(gb.reduce_to_cycles(), None)\n", "label": 1}
{"function": "\n\ndef _swap_generator(self, coro):\n    'Internal use only.\\n        '\n    self._lock.acquire()\n    cid = coro._id\n    coro = self._coros.get(cid, None)\n    if (coro is None):\n        logger.warning('invalid coroutine %s to swap', cid)\n        self._lock.release()\n        return (- 1)\n    if (coro._callers or (not coro._hot_swappable)):\n        logger.debug('postponing hot swapping of %s', str(coro))\n        self._lock.release()\n        return 0\n    else:\n        coro._timeout = None\n        if (coro._state is None):\n            coro._generator = coro._swap_generator\n            coro._value = None\n            if (coro._complete == 0):\n                coro._complete = None\n            elif isinstance(coro._complete, Event):\n                coro._complete.clear()\n            self._scheduled.add(cid)\n            coro._state = AsynCoro._Scheduled\n            coro._hot_swappable = False\n        else:\n            coro._exceptions.append((HotSwapException, HotSwapException(coro._swap_generator)))\n            if (coro._state in (AsynCoro._Suspended, AsynCoro._AwaitMsg_)):\n                self._suspended.discard(cid)\n                self._scheduled.add(cid)\n                coro._state = AsynCoro._Scheduled\n        coro._swap_generator = None\n        if (self._polling and (len(self._scheduled) == 1)):\n            self._notifier.interrupt()\n    self._lock.release()\n    return 0\n", "label": 1}
{"function": "\n\ndef __init__(self, parameters=None, return_annotation=_empty, __validate_parameters__=True):\n    \"Constructs Signature from the given list of Parameter\\n        objects and 'return_annotation'.  All arguments are optional.\\n        \"\n    if (parameters is None):\n        params = OrderedDict()\n    elif __validate_parameters__:\n        params = OrderedDict()\n        top_kind = _POSITIONAL_ONLY\n        for (idx, param) in enumerate(parameters):\n            kind = param.kind\n            if (kind < top_kind):\n                msg = 'wrong parameter order: {0} before {1}'\n                msg = msg.format(top_kind, param.kind)\n                raise ValueError(msg)\n            else:\n                top_kind = kind\n            name = param.name\n            if (name is None):\n                name = str(idx)\n                param = param.replace(name=name)\n            if (name in params):\n                msg = 'duplicate parameter name: {0!r}'.format(name)\n                raise ValueError(msg)\n            params[name] = param\n    else:\n        params = OrderedDict(((param.name, param) for param in parameters))\n    self._parameters = params\n    self._return_annotation = return_annotation\n", "label": 1}
{"function": "\n\ndef format(self, record):\n    'Uses contextstring if request_id is set, otherwise default.'\n    record.project = self.project\n    record.version = self.version\n    context = getattr(local.store, 'context', None)\n    if context:\n        d = _dictify_context(context)\n        for (k, v) in d.items():\n            setattr(record, k, v)\n    for key in ('instance', 'color'):\n        if (key not in record.__dict__):\n            record.__dict__[key] = ''\n    if record.__dict__.get('request_id', None):\n        self._fmt = CONF.logging_context_format_string\n    else:\n        self._fmt = CONF.logging_default_format_string\n    if ((record.levelno == logging.DEBUG) and CONF.logging_debug_format_suffix):\n        self._fmt += (' ' + CONF.logging_debug_format_suffix)\n    if record.exc_info:\n        record.exc_text = self.formatException(record.exc_info, record)\n    return logging.Formatter.format(self, record)\n", "label": 1}
{"function": "\n\ndef handle_class(val, class_name):\n    cls_errors = []\n    docstring = inspect.getdoc(val)\n    if (docstring is None):\n        cls_errors.append((class_name, '**missing** class-level docstring'))\n    else:\n        cls_errors = [(e,) for e in NumpyClassDocString(docstring, class_name, val).get_errors()]\n        methods = dict(((name, func) for (name, func) in inspect.getmembers(val) if ((not name.startswith('_')) and callable(func) and (type(func) is not type))))\n        for (m_name, method) in six.iteritems(methods):\n            if (inspect.getmodule(method) is not None):\n                continue\n            cls_errors.extend(handle_method(method, m_name, class_name))\n    return cls_errors\n", "label": 1}
{"function": "\n\ndef get_features(self, context_obj):\n    if (('source_token' not in context_obj) or (len(context_obj['source_token']) == 0)):\n        return [0.0 for i in range((len(self.thresholds) * 2))]\n    (translations, translations_weighted) = ([], [])\n    for thr in self.thresholds:\n        (all_words, all_words_weighted) = ([], [])\n        for word in context_obj['source_token']:\n            trans = [fl for fl in self.lex_prob[word] if (fl >= thr)]\n            all_words.append(len(trans))\n            all_words_weighted.append((len(trans) * self.corpus_freq.freq(word)))\n        translations.append(np.average(all_words))\n        translations_weighted.append(np.average(all_words_weighted))\n    return (translations + translations_weighted)\n", "label": 1}
{"function": "\n\ndef safely_reserve_a_username(cursor, gen_usernames=gen_random_usernames, reserve=insert_into_participants):\n    'Safely reserve a username.\\n\\n    :param cursor: a :py:class:`psycopg2.cursor` managed as a :py:mod:`postgres`\\n        transaction\\n    :param gen_usernames: a generator of usernames to try\\n    :param reserve: a function that takes the cursor and does the SQL\\n        stuff\\n    :database: one ``INSERT`` on average\\n    :returns: a 12-hex-digit unicode\\n    :raises: :py:class:`FailedToReserveUsername` if no acceptable username is found\\n        within 100 attempts, or :py:class:`RanOutOfUsernameAttempts` if the username\\n        generator runs out first\\n\\n    The returned value is guaranteed to have been reserved in the database.\\n\\n    '\n    cursor.execute('SAVEPOINT safely_reserve_a_username')\n    seatbelt = 0\n    for username in gen_usernames():\n        seatbelt += 1\n        if (seatbelt > 100):\n            raise FailedToReserveUsername\n        try:\n            check = reserve(cursor, username)\n        except IntegrityError:\n            cursor.execute('ROLLBACK TO safely_reserve_a_username')\n            continue\n        else:\n            assert (check == username)\n            break\n    else:\n        raise RanOutOfUsernameAttempts\n    cursor.execute('RELEASE safely_reserve_a_username')\n    return username\n", "label": 1}
{"function": "\n\ndef save_font(self, name, family, generic):\n    \" It is possible that the HTML browser doesn't know how to\\n            show a particular font. Fortunately ODF provides generic fallbacks.\\n            Unfortunately they are not the same as CSS2.\\n            CSS2: serif, sans-serif, cursive, fantasy, monospace\\n            ODF: roman, swiss, modern, decorative, script, system\\n            This method put the font and fallback into a dictionary\\n        \"\n    htmlgeneric = 'sans-serif'\n    if (generic == 'roman'):\n        htmlgeneric = 'serif'\n    elif (generic == 'swiss'):\n        htmlgeneric = 'sans-serif'\n    elif (generic == 'modern'):\n        htmlgeneric = 'monospace'\n    elif (generic == 'decorative'):\n        htmlgeneric = 'sans-serif'\n    elif (generic == 'script'):\n        htmlgeneric = 'monospace'\n    elif (generic == 'system'):\n        htmlgeneric = 'serif'\n    self.fontdict[name] = (family, htmlgeneric)\n", "label": 1}
{"function": "\n\ndef send_data(sock, data, address=None):\n    if (address is None):\n        _rep = []\n    else:\n        _rep = list(address)\n    for a in data:\n        if isinstance(a, str):\n            _rep.append(a.encode('utf-8'))\n        else:\n            _rep.append(a)\n    while True:\n        try:\n            sock.send_multipart(_rep, zmq.NOBLOCK)\n        except zmq.ZMQError as e:\n            if (e.errno == errno.EAGAIN):\n                gethub().do_write(sock)\n                continue\n            elif (e.errno == errno.EINTR):\n                continue\n            else:\n                raise\n        else:\n            break\n", "label": 1}
{"function": "\n\ndef LoadAppInclude(app_include):\n    'Load a single AppInclude object where one and only one is expected.\\n\\n  Args:\\n    app_include: A file-like object or string.  If it is a string, parse it as\\n    a configuration file.  If it is a file-like object, read in data and\\n    parse.\\n\\n  Returns:\\n    An instance of AppInclude as loaded from a YAML file.\\n\\n  Raises:\\n    EmptyConfigurationFile: when there are no documents in YAML file.\\n    MultipleConfigurationFile: when there is more than one document in YAML\\n    file.\\n  '\n    builder = yaml_object.ObjectBuilder(AppInclude)\n    handler = yaml_builder.BuilderHandler(builder)\n    listener = yaml_listener.EventListener(handler)\n    listener.Parse(app_include)\n    includes = handler.GetResults()\n    if (len(includes) < 1):\n        raise appinfo_errors.EmptyConfigurationFile()\n    if (len(includes) > 1):\n        raise appinfo_errors.MultipleConfigurationFile()\n    includeyaml = includes[0]\n    if includeyaml.handlers:\n        for handler in includeyaml.handlers:\n            handler.FixSecureDefaults()\n            handler.WarnReservedURLs()\n    if includeyaml.builtins:\n        BuiltinHandler.Validate(includeyaml.builtins)\n    return includeyaml\n", "label": 1}
{"function": "\n\ndef _loop(self):\n    import jedi\n    while True:\n        data = stream_read(sys.stdin)\n        if (not isinstance(data, tuple)):\n            break\n        (source, line, col, filename) = data\n        log.debug('Line: %r, Col: %r, Filename: %r', line, col, filename)\n        completions = jedi.Script(source, line, col, filename).completions()\n        out = []\n        tmp_filecache = {\n            \n        }\n        for c in completions:\n            (name, type_, desc, abbr) = self.parse_completion(c, tmp_filecache)\n            kind = (type_ if (not self.use_short_types) else (_types.get(type_) or type_))\n            out.append((c.module_path, name, type_, desc, abbr, kind))\n        stream_write(sys.stdout, out)\n", "label": 1}
{"function": "\n\ndef walk(self, node_type=None):\n    'Walk through the query tree, returning nodes of a specific type.'\n    pending = [self._root]\n    while pending:\n        node = pending.pop()\n        if ((not node_type) or isinstance(node, node_type)):\n            (yield node)\n        if isinstance(node, nodes.UnaryOp):\n            pending.append(node.node)\n        elif isinstance(node, nodes.BinaryOp):\n            pending.extend([node.left, node.right])\n", "label": 1}
{"function": "\n\ndef validate(self, obj, value):\n    if (value is None):\n        if self._allow_none:\n            return value\n    if (not isinstance(value, basestring)):\n        self.error(obj, value)\n    for v in self.values:\n        if (v.lower() == value.lower()):\n            return v\n    self.error(obj, value)\n", "label": 1}
{"function": "\n\ndef url_for_static(filename=None, **kwargs):\n    from uliweb import settings, application\n    from uliweb.core.SimpleFrame import get_url_adapter\n    from urlparse import urlparse, urlunparse, urljoin\n    import urllib\n    domain = application.domains.get('static', {\n        \n    })\n    ver = settings.GLOBAL.STATIC_VER\n    if ver:\n        kwargs['ver'] = ver\n    external = kwargs.pop('_external', False)\n    if (not external):\n        external = domain.get('display', False)\n    if filename.startswith('/'):\n        if filename.endswith('/'):\n            filename = filename[:(- 1)]\n        if kwargs:\n            filename += ('?' + urllib.urlencode(kwargs))\n        if external:\n            return urljoin(domain.get('domain', ''), filename)\n        return filename\n    r = urlparse(filename)\n    if (r.scheme or r.netloc):\n        x = list(r)\n        if kwargs:\n            x[4] = urllib.urlencode(kwargs)\n            return urlunparse(x)\n        else:\n            return filename\n    kwargs['filename'] = filename\n    url_adapter = get_url_adapter('static')\n    return url_adapter.build('uliweb.contrib.staticfiles.static', kwargs, force_external=external)\n", "label": 1}
{"function": "\n\ndef filter_items(value, startswith=None, strip_prefix=False):\n    'Jinja2 filter used to filter a dictionary\\'s keys by specifying a\\n    required prefix.\\n\\n    Returns a list of key/value tuples.\\n\\n    Usage:\\n        {{ my_dict|filter_items }}\\n        {{ my_dict|filter_items(\"MY_PREFIX_\") }}\\n        {{ my_dict|filter_items(\"MY_PREFIX_\", True) }}\\n    '\n    if (startswith is not None):\n        value = [x for x in value.items() if x[0].startswith(startswith)]\n    else:\n        value = value.items()\n    if ((startswith is not None) and strip_prefix):\n        value = [(x[0].replace(startswith, '', 1), x[1]) for x in value]\n    return value\n", "label": 1}
{"function": "\n\ndef _build_resource(self):\n    'Generate a resource for :meth:`begin`.'\n    resource = {\n        'query': self.query,\n    }\n    if (self.default_dataset is not None):\n        resource['defaultDataset'] = {\n            'projectId': self.project,\n            'datasetId': self.default_dataset.name,\n        }\n    if (self.max_results is not None):\n        resource['maxResults'] = self.max_results\n    if (self.preserve_nulls is not None):\n        resource['preserveNulls'] = self.preserve_nulls\n    if (self.timeout_ms is not None):\n        resource['timeoutMs'] = self.timeout_ms\n    if (self.use_query_cache is not None):\n        resource['useQueryCache'] = self.use_query_cache\n    return resource\n", "label": 1}
{"function": "\n\ndef test_initialization_legacy(self, NeuralNet):\n    input = Mock(__name__='InputLayer', __bases__=(InputLayer,))\n    (hidden1, hidden2, output) = [Mock(__name__='MockLayer', __bases__=(Layer,)) for i in range(3)]\n    nn = NeuralNet(layers=[('input', input), ('hidden1', hidden1), ('hidden2', hidden2), ('output', output)], input_shape=(10, 10), hidden1_some='param')\n    out = nn.initialize_layers(nn.layers)\n    input.assert_called_with(name='input', shape=(10, 10))\n    assert (nn.layers_['input'] is input.return_value)\n    hidden1.assert_called_with(incoming=input.return_value, name='hidden1', some='param')\n    assert (nn.layers_['hidden1'] is hidden1.return_value)\n    hidden2.assert_called_with(incoming=hidden1.return_value, name='hidden2')\n    assert (nn.layers_['hidden2'] is hidden2.return_value)\n    output.assert_called_with(incoming=hidden2.return_value, name='output')\n    assert (out is nn.layers_['output'])\n", "label": 1}
{"function": "\n\ndef parse(self, string, name='<string>'):\n    '\\n        Divide the given string into examples and intervening text,\\n        and return them as a list of alternating Examples and strings.\\n        Line numbers for the Examples are 0-based.  The optional\\n        argument `name` is a name identifying this string, and is only\\n        used for error messages.\\n        '\n    string = string.expandtabs()\n    min_indent = self._min_indent(string)\n    if (min_indent > 0):\n        string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n    output = []\n    (charno, lineno) = (0, 0)\n    for m in self._EXAMPLE_RE.finditer(string):\n        output.append(string[charno:m.start()])\n        lineno += string.count('\\n', charno, m.start())\n        (source, options, want, exc_msg) = self._parse_example(m, name, lineno)\n        if (not self._IS_BLANK_OR_COMMENT(source)):\n            output.append(doctest.Example(source, want, exc_msg, lineno=lineno, indent=(min_indent + len((m.group('indent') or m.group('runindent')))), options=options))\n        lineno += string.count('\\n', m.start(), m.end())\n        charno = m.end()\n    output.append(string[charno:])\n    return output\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TListSentryRolesRequest')\n    if (self.protocol_version is not None):\n        oprot.writeFieldBegin('protocol_version', TType.I32, 1)\n        oprot.writeI32(self.protocol_version)\n        oprot.writeFieldEnd()\n    if (self.requestorUserName is not None):\n        oprot.writeFieldBegin('requestorUserName', TType.STRING, 2)\n        oprot.writeString(self.requestorUserName)\n        oprot.writeFieldEnd()\n    if (self.groupName is not None):\n        oprot.writeFieldBegin('groupName', TType.STRING, 3)\n        oprot.writeString(self.groupName)\n        oprot.writeFieldEnd()\n    if (self.component is not None):\n        oprot.writeFieldBegin('component', TType.STRING, 4)\n        oprot.writeString(self.component)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef test_indexSearch(self, dataFrame):\n    datasearch = DataSearch('Test', dataFrame=dataFrame)\n    filterString = 'indexSearch([0])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 1)\n    filterString = 'indexSearch([0, 2])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 2)\n    filterString = 'indexSearch([0, 1, 2])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 3)\n    filterString = 'indexSearch([99])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 0)\n", "label": 1}
{"function": "\n\ndef lineReceived(self, line):\n    line = line.strip()\n    self.log.debug('[sref:%s] Received line: %s', self.sessionRef, line)\n    (cmd, arg, line) = self.parseline(line)\n    if (self.sessionLineCallback is not None):\n        return self.sessionLineCallback(cmd, arg, line)\n    if (not line):\n        return self.sendData()\n    if ((cmd is None) or (cmd not in self.findCommands())):\n        return self.default(line)\n    funcName = ('do_' + cmd)\n    try:\n        func = getattr(self, funcName)\n    except AttributeError:\n        return self.default(line)\n    self.log.debug('[sref:%s] Running %s with arg:%s', self.sessionRef, funcName, arg)\n    return func(arg)\n", "label": 1}
{"function": "\n\ndef _set_item(self, item_id, value):\n    'Set the current value of an item in the dialog\\n        '\n    item_hwnd = wrapped(win32gui.GetDlgItem, self.hwnd, item_id)\n    class_name = wrapped(win32gui.GetClassName, item_hwnd)\n    styles = wrapped(win32gui.GetWindowLong, self.hwnd, win32con.GWL_STYLE)\n    if (class_name == 'Edit'):\n        if isinstance(value, datetime.date):\n            value = value.strftime('%d %b %Y')\n        value = unicode(value).replace('\\r\\n', '\\n').replace('\\n', '\\r\\n')\n        wrapped(win32gui.SetDlgItemText, self.hwnd, item_id, value)\n    elif (class_name == 'Button'):\n        SendMessage(item_hwnd, win32con.BM_SETCHECK, int(value), 0)\n    elif (class_name == 'ComboBox'):\n        for item in value:\n            if isinstance(item, tuple):\n                item = item[0]\n            SendMessage(item_hwnd, win32con.CB_ADDSTRING, 0, utils.string_as_pointer(str(item)))\n        SendMessage(item_hwnd, win32con.CB_SETCURSEL, 0, 0)\n    elif (class_name == 'Static'):\n        wrapped(win32gui.SetDlgItemText, self.hwnd, item_id, unicode(value))\n    else:\n        raise RuntimeError(('Unknown class: %s' % class_name))\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    print(('Querying database at %s' % CURRENCY_API_URL))\n    currencies = []\n    if len(args):\n        currencies = list(args)\n    api = urlopen(CURRENCY_API_URL)\n    d = json.loads(api.read())\n    i = 0\n    for currency in sorted(d.keys()):\n        if ((not currencies) or (currency in currencies)):\n            if (not Currency.objects.filter(code=currency)):\n                print(('Creating %r (%s)' % (d[currency], currency)))\n                Currency(code=currency, name=d[currency], factor=1.0, is_active=False).save()\n                i += 1\n    if (i == 1):\n        print(('%i new currency' % i))\n    else:\n        print(('%i new currencies' % i))\n", "label": 1}
{"function": "\n\ndef doNick(self, msg):\n    'Handles NICK messages.'\n    if (msg.nick == self.nick):\n        newNick = msg.args[0]\n        self.nick = newNick\n        (nick, user, domain) = ircutils.splitHostmask(msg.prefix)\n        self.prefix = ircutils.joinHostmask(self.nick, user, domain)\n    elif conf.supybot.followIdentificationThroughNickChanges():\n        try:\n            id = ircdb.users.getUserId(msg.prefix)\n            u = ircdb.users.getUser(id)\n        except KeyError:\n            return\n        if u.auth:\n            (_, user, host) = ircutils.splitHostmask(msg.prefix)\n            newhostmask = ircutils.joinHostmask(msg.args[0], user, host)\n            for (i, (when, authmask)) in enumerate(u.auth[:]):\n                if ircutils.strEqual(msg.prefix, authmask):\n                    log.info('Following identification for %s: %s -> %s', u.name, authmask, newhostmask)\n                    u.auth[i] = (u.auth[i][0], newhostmask)\n                    ircdb.users.setUser(u)\n", "label": 1}
{"function": "\n\ndef acquire(self, blocking=True):\n    \"Must be used with 'yield' as 'yield cv.acquire()'.\\n        \"\n    coro = AsynCoro.cur_coro()\n    if (self._owner == coro):\n        self._depth += 1\n        raise StopIteration(True)\n    if ((not blocking) and (self._owner is not None)):\n        raise StopIteration(False)\n    while (self._owner is not None):\n        self._waitlist.append(coro)\n        (yield coro._await_())\n    assert (self._depth == 0)\n    self._owner = coro\n    self._depth = 1\n    raise StopIteration(True)\n", "label": 1}
{"function": "\n\ndef ToPrimitiveDict(self):\n    'Handle dict generation specifically for Artifacts.'\n    artifact_dict = super(Artifact, self).ToPrimitiveDict()\n    artifact_dict['name'] = utils.SmartStr(self.name)\n    for source in artifact_dict['sources']:\n        if ('type' in source):\n            source['type'] = str(source['type'])\n        if ('key_value_pairs' in source['attributes']):\n            outarray = []\n            for indict in source['attributes']['key_value_pairs']:\n                outarray.append(dict(indict.items()))\n            source['attributes']['key_value_pairs'] = outarray\n    for field in self.required_repeated_fields:\n        if (field not in artifact_dict):\n            artifact_dict[field] = []\n    return artifact_dict\n", "label": 1}
{"function": "\n\n@login_required\ndef new(request, form_class=BlogForm, template_name='blog/new.html'):\n    if (request.method == 'POST'):\n        if (request.POST['action'] == 'create'):\n            blog_form = form_class(request.user, request.POST)\n            if blog_form.is_valid():\n                blog = blog_form.save(commit=False)\n                blog.author = request.user\n                if getattr(settings, 'BEHIND_PROXY', False):\n                    blog.creator_ip = request.META['HTTP_X_FORWARDED_FOR']\n                else:\n                    blog.creator_ip = request.META['REMOTE_ADDR']\n                blog.save()\n                messages.add_message(request, messages.SUCCESS, (ugettext(\"Successfully saved post '%s'\") % blog.title))\n                if notification:\n                    if (blog.status == 2):\n                        if friends:\n                            notification.send((x['friend'] for x in Friendship.objects.friends_for_user(blog.author)), 'blog_friend_post', {\n                                'post': blog,\n                            })\n                return HttpResponseRedirect(reverse('blog_list_yours'))\n        else:\n            blog_form = form_class()\n    else:\n        blog_form = form_class()\n    return render_to_response(template_name, {\n        'blog_form': blog_form,\n    }, context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\n@property\ndef initial_selection(self):\n    'Initial selection matrix'\n    if (not (self.state_regression and self.time_varying_regression)):\n        if (self.k_posdef > 0):\n            selection = np.r_[(([0] * self._k_states_diff), ([1] * (self._k_order > 0)), ([0] * (self._k_order - 1)), ([0] * ((1 - self.mle_regression) * self.k_exog)))][:, None]\n        else:\n            selection = np.zeros((self.k_states, 0))\n    else:\n        selection = np.zeros((self.k_states, self.k_posdef))\n        if (self._k_order > 0):\n            selection[(0, 0)] = 1\n        for i in range(self.k_exog, 0, (- 1)):\n            selection[((- i), (- i))] = 1\n    return selection\n", "label": 1}
{"function": "\n\ndef update(self, **kw):\n    \"\\n        Shortcut for doing an UPDATE on this object.\\n\\n        If _signal=False is in ``kw`` the post_save signal won't be sent.\\n        \"\n    signal = kw.pop('_signal', True)\n    cls = self.__class__\n    for (k, v) in kw.items():\n        setattr(self, k, v)\n    if signal:\n        attrs = dict(self.__dict__)\n        models.signals.pre_save.send(sender=cls, instance=self)\n        for (k, v) in self.__dict__.items():\n            if (attrs[k] != v):\n                kw[k] = v\n                setattr(self, k, v)\n    cls.objects.filter(pk=self.pk).update(**kw)\n    if signal:\n        models.signals.post_save.send(sender=cls, instance=self, created=False)\n", "label": 1}
{"function": "\n\ndef dedup_views(window):\n    group = window.active_group()\n    for g in range(window.num_groups()):\n        found = dict()\n        views = window.views_in_group(g)\n        active = window.active_view_in_group(g)\n        for v in views:\n            if v.is_dirty():\n                continue\n            id = v.buffer_id()\n            if (id in found):\n                if (v == active):\n                    before = found[id]\n                    found[id] = v\n                    v = before\n                window.focus_view(v)\n                window.run_command('close')\n            else:\n                found[id] = v\n        window.focus_view(active)\n    window.focus_group(group)\n", "label": 1}
{"function": "\n\ndef resolve_promise(o):\n    if isinstance(o, dict):\n        for (k, v) in o.items():\n            o[k] = resolve_promise(v)\n    elif isinstance(o, (list, tuple)):\n        o = [resolve_promise(x) for x in o]\n    elif isinstance(o, Promise):\n        try:\n            o = force_unicode(o)\n        except:\n            try:\n                o = [resolve_promise(x) for x in o]\n            except:\n                raise Exception(('Unable to resolve lazy object %s' % o))\n    elif callable(o):\n        o = o()\n    return o\n", "label": 1}
{"function": "\n\n@verbose\ndef save(self, fname, ftype='stc', verbose=None):\n    'Save the source estimates to a file\\n\\n        Parameters\\n        ----------\\n        fname : string\\n            The stem of the file name. The stem is extended with \"-vl.stc\"\\n            or \"-vl.w\".\\n        ftype : string\\n            File format to use. Allowed values are \"stc\" (default) and \"w\".\\n            The \"w\" format only supports a single time point.\\n        verbose : bool, str, int, or None\\n            If not None, override default verbose level (see mne.verbose).\\n            Defaults to self.verbose.\\n        '\n    if (ftype not in ['stc', 'w']):\n        raise ValueError(('ftype must be \"stc\" or \"w\", not \"%s\"' % ftype))\n    if (ftype == 'stc'):\n        logger.info('Writing STC to disk...')\n        if (not (fname.endswith('-vl.stc') or fname.endswith('-vol.stc'))):\n            fname += '-vl.stc'\n        _write_stc(fname, tmin=self.tmin, tstep=self.tstep, vertices=self.vertices, data=self.data)\n    elif (ftype == 'w'):\n        logger.info('Writing STC to disk (w format)...')\n        if (not (fname.endswith('-vl.w') or fname.endswith('-vol.w'))):\n            fname += '-vl.w'\n        _write_w(fname, vertices=self.vertices, data=self.data)\n    logger.info('[done]')\n", "label": 1}
{"function": "\n\ndef __call__(self, event):\n    '\\n        Write event to file.\\n\\n        @param event: An event.\\n        @type event: L{dict}\\n        '\n    text = self.formatEvent(event)\n    if (text is None):\n        text = ''\n    if ('log_failure' in event):\n        try:\n            traceback = event['log_failure'].getTraceback()\n        except Exception:\n            traceback = '(UNABLE TO OBTAIN TRACEBACK FROM EVENT)\\n'\n        text = '\\n'.join((text, traceback))\n    if (self._encoding is not None):\n        text = text.encode(self._encoding)\n    if text:\n        self._outFile.write(text)\n        self._outFile.flush()\n", "label": 1}
{"function": "\n\ndef add_image_info_cb(self, viewer, channel, info):\n    save_thumb = self.settings.get('cache_thumbs', False)\n    chname = channel.name\n    thumbkey = self.get_thumb_key(chname, info.name, info.path)\n    thumbpath = self.get_thumbpath(info.path)\n    with self.thmblock:\n        try:\n            bnch = self.thumbDict[thumbkey]\n            if (bnch.thumbpath == thumbpath):\n                return\n        except KeyError:\n            pass\n    image = None\n    if ((thumbpath is not None) and os.path.exists(thumbpath)):\n        save_thumb = False\n        try:\n            image = self.fv.load_image(thumbpath)\n        except Exception as e:\n            pass\n    try:\n        if (image is None):\n            image = info.image_loader(info.path)\n        image.set(name=info.name)\n        self.fv.gui_do(self._make_thumb, chname, image, info.name, info.path, thumbkey, info.image_future, save_thumb=save_thumb, thumbpath=thumbpath)\n    except Exception as e:\n        self.logger.error((\"Error generating thumbnail for '%s': %s\" % (info.path, str(e))))\n", "label": 1}
{"function": "\n\ndef decrypt(self, encBytes):\n    'Decrypt the passed-in bytes.\\n\\n        This requires the key to have a private component.  It performs\\n        PKCS1 decryption of the passed-in data.\\n\\n        @type encBytes: L{bytearray} of unsigned bytes\\n        @param encBytes: The value which will be decrypted.\\n\\n        @rtype: L{bytearray} of unsigned bytes or None.\\n        @return: A PKCS1 decryption of the passed-in data or None if\\n        the data is not properly formatted.\\n        '\n    if (not self.hasPrivateKey()):\n        raise AssertionError()\n    if (len(encBytes) != numBytes(self.n)):\n        return None\n    c = bytesToNumber(encBytes)\n    if (c >= self.n):\n        return None\n    m = self._rawPrivateKeyOp(c)\n    decBytes = numberToBytes(m, numBytes(self.n))\n    if ((decBytes[0] != 0) or (decBytes[1] != 2)):\n        return None\n    for x in range(1, (len(decBytes) - 1)):\n        if (decBytes[x] == 0):\n            break\n    else:\n        return None\n    return decBytes[(x + 1):]\n", "label": 1}
{"function": "\n\ndef run(self):\n    isIdle = False\n    while (not self._finish):\n        (state, nextCheck, idle) = self._tracker.check_idle()\n        if ((state == 'idle') and (not isIdle) and (idle >= (self._idleSeconds * 1000))):\n            self.emit(QtCore.SIGNAL('idle()'))\n        elif ((state is not None) and isIdle):\n            self.emit(QtCore.SIGNAL('active()'))\n        if (state is not None):\n            isIdle = (state == 'idle')\n        self.usleep((nextCheck * 1000))\n", "label": 1}
{"function": "\n\ndef prepareQsub(self, cpu, mem, jobID):\n    qsubline = ['qsub', '-b', 'y', '-terse', '-j', 'y', '-cwd', '-o', '/dev/null', '-e', '/dev/null', '-N', ('toil_job_' + str(jobID))]\n    if self.boss.environment:\n        qsubline.append('-v')\n        qsubline.append(','.join((((k + '=') + quote((os.environ[k] if (v is None) else v))) for (k, v) in self.boss.environment.iteritems())))\n    reqline = list()\n    if (mem is not None):\n        memStr = (str((mem / 1024)) + 'K')\n        reqline += [('vf=' + memStr), ('h_vmem=' + memStr)]\n    if (len(reqline) > 0):\n        qsubline.extend(['-hard', '-l', ','.join(reqline)])\n    if ((cpu is not None) and (math.ceil(cpu) > 1)):\n        peConfig = (os.getenv('TOIL_GRIDENGINE_PE') or 'shm')\n        qsubline.extend(['-pe', peConfig, str(int(math.ceil(cpu)))])\n    return qsubline\n", "label": 1}
{"function": "\n\ndef extract_images(self):\n    from frappe.utils.file_manager import extract_images_from_html\n    if self.format_data:\n        data = json.loads(self.format_data)\n        for df in data:\n            if (df.get('fieldtype') and (df['fieldtype'] in ('HTML', 'Custom HTML')) and df.get('options')):\n                df['options'] = extract_images_from_html(self, df['options'])\n        self.format_data = json.dumps(data)\n", "label": 1}
{"function": "\n\n@classmethod\ndef email_url_config(cls, url, backend=None):\n    'Parses an email URL.'\n    config = {\n        \n    }\n    url = (urlparse.urlparse(url) if (not isinstance(url, cls.URL_CLASS)) else url)\n    path = url.path[1:]\n    path = path.split('?', 2)[0]\n    config.update({\n        'EMAIL_FILE_PATH': path,\n        'EMAIL_HOST_USER': url.username,\n        'EMAIL_HOST_PASSWORD': url.password,\n        'EMAIL_HOST': url.hostname,\n        'EMAIL_PORT': _cast_int(url.port),\n    })\n    if backend:\n        config['EMAIL_BACKEND'] = backend\n    elif (url.scheme not in cls.EMAIL_SCHEMES):\n        raise ImproperlyConfigured(('Invalid email schema %s' % url.scheme))\n    elif (url.scheme in cls.EMAIL_SCHEMES):\n        config['EMAIL_BACKEND'] = cls.EMAIL_SCHEMES[url.scheme]\n    if (url.scheme in ('smtps', 'smtp+tls')):\n        config['EMAIL_USE_TLS'] = True\n    elif (url.scheme == 'smtp+ssl'):\n        config['EMAIL_USE_SSL'] = True\n    if url.query:\n        config_options = {\n            \n        }\n        for (k, v) in urlparse.parse_qs(url.query).items():\n            opt = {\n                k.upper(): _cast_int(v[0]),\n            }\n            if (k.upper() in cls._EMAIL_BASE_OPTIONS):\n                config.update(opt)\n            else:\n                config_options.update(opt)\n        config['OPTIONS'] = config_options\n    return config\n", "label": 1}
{"function": "\n\ndef onEnterNode(self, node):\n    for variable in node.getClosureVariables():\n        assert (not variable.isModuleVariable())\n        current = node\n        while (current is not variable.getOwner()):\n            if current.isParentVariableProvider():\n                if (variable not in current.getClosureVariables()):\n                    current.addClosureVariable(variable)\n            assert (current.getParentVariableProvider() is not current)\n            current = current.getParentVariableProvider()\n            assert (current is not None), variable\n", "label": 1}
{"function": "\n\n@register(_dump_registry, bytes)\ndef _dump_bytes(obj, stream):\n    l = len(obj)\n    if (l == 0):\n        stream.append(TAG_EMPTY_STR)\n    elif (l == 1):\n        stream.append((TAG_STR1 + obj))\n    elif (l == 2):\n        stream.append((TAG_STR2 + obj))\n    elif (l == 3):\n        stream.append((TAG_STR3 + obj))\n    elif (l == 4):\n        stream.append((TAG_STR4 + obj))\n    elif (l < 256):\n        stream.append(((TAG_STR_L1 + I1.pack(l)) + obj))\n    else:\n        stream.append(((TAG_STR_L4 + I4.pack(l)) + obj))\n", "label": 1}
{"function": "\n\ndef on_widget(self, instance, value):\n    stack = self.ids.stack\n    prefs = [btn.widget_ref() for btn in self.parents]\n    if (value in prefs):\n        index = prefs.index(value)\n        for btn in self.parents:\n            btn.state = 'normal'\n        self.parents[index].state = 'down'\n        return\n    stack.clear_widgets()\n    if (not value):\n        return\n    widget = value\n    parents = []\n    while True:\n        btn = ConsoleButton(text=widget.__class__.__name__)\n        btn.widget_ref = weakref.ref(widget)\n        btn.bind(on_release=self.highlight_widget)\n        parents.append(btn)\n        if (widget == widget.parent):\n            break\n        widget = widget.parent\n    for btn in reversed(parents):\n        stack.add_widget(btn)\n    self.ids.sv.scroll_x = 1\n    self.parents = parents\n    btn.state = 'down'\n", "label": 1}
{"function": "\n\ndef log_int_fixed(n, prec, ln2=None):\n    '\\n    Fast computation of log(n), caching the value for small n,\\n    intended for zeta sums.\\n    '\n    if (n in log_int_cache):\n        (value, vprec) = log_int_cache[n]\n        if (vprec >= prec):\n            return (value >> (vprec - prec))\n    wp = (prec + 10)\n    if (wp <= LOG_TAYLOR_SHIFT):\n        if (ln2 is None):\n            ln2 = ln2_fixed(wp)\n        r = bitcount(n)\n        x = (n << (wp - r))\n        v = (log_taylor_cached(x, wp) + (r * ln2))\n    else:\n        v = to_fixed(mpf_log(from_int(n), (wp + 5)), wp)\n    if (n < MAX_LOG_INT_CACHE):\n        log_int_cache[n] = (v, wp)\n    return (v >> (wp - prec))\n", "label": 1}
{"function": "\n\ndef print_scoreboard(self):\n    'Print object as a scoreboard.'\n    output = ''\n    innings = []\n    away = []\n    home = []\n    for x in self:\n        innings.append(x['inning'])\n        away.append(x['away'])\n        home.append(x['home'])\n    output += 'Inning\\t'\n    for x in innings:\n        output += (str(x) + ' ')\n    output += '\\n'\n    for x in innings:\n        output += '---'\n    output += '\\nAway\\t'\n    for (y, x) in enumerate(away, start=1):\n        if (y >= 10):\n            output += (str(x) + '  ')\n        else:\n            output += (str(x) + ' ')\n    output += '\\nHome\\t'\n    for (y, x) in enumerate(home, start=1):\n        if (y >= 10):\n            output += (str(x) + '  ')\n        else:\n            output += (str(x) + ' ')\n    return output\n", "label": 1}
{"function": "\n\ndef getCovMatrix(self, x=None, z=None, mode=None):\n    self.checkInputGetCovMatrix(x, z, mode)\n    c = np.exp(self.hyp[0])\n    sf2 = np.exp((2.0 * self.hyp[1]))\n    ord = self.para[0]\n    if (np.abs((ord - np.round(ord))) < 1e-08):\n        ord = int(round(ord))\n    assert (ord >= 1.0)\n    ord = int(ord)\n    if (mode == 'self_test'):\n        (nn, D) = z.shape\n        A = np.reshape(np.sum((z * z), 1), (nn, 1))\n    elif (mode == 'train'):\n        (n, D) = x.shape\n        A = (np.dot(x, x.T) + (np.eye(n) * 1e-10))\n    elif (mode == 'cross'):\n        A = np.dot(x, z.T)\n    A = (sf2 * ((c + A) ** ord))\n    return A\n", "label": 1}
{"function": "\n\ndef test_upload_image(client, mocker):\n    today = datetime.date.today()\n    mocker.patch('requests.get')\n    mocker.patch('redwind.tasks.create_queue')\n    rv = client.post('/save_new', data={\n        'photo': (open('tests/image.jpg', 'rb'), 'image.jpg', 'image/jpeg'),\n        'post_type': 'photo',\n        'content': 'High score',\n        'action': 'publish_quietly',\n    })\n    assert (rv.status_code == 302)\n    assert (rv.location == 'http://example.com/{}/{:02d}/high-score'.format(today.year, today.month))\n    permalink = rv.location\n    rv = client.get(permalink)\n    assert (rv.status_code == 200)\n    content = rv.get_data(as_text=True)\n    assert ('High score' in content)\n    assert ('<img' in content)\n    rv = client.get((permalink + '/files/image.jpg'))\n    assert (rv.status_code == 200)\n", "label": 1}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    (images, hidacts, frows, fcols) = node.inputs\n    (ishape, hshape, frowshp, fcolshp) = shapes\n    (igroups, icolors_per_group, irows, icols, icount) = ishape\n    (hgroups, hcolors_per_group, hrows, hcols, hcount) = hshape\n    fmodulesR = hrows\n    fmodulesC = hcols\n    fcolors = icolors_per_group\n    fgroups = hgroups\n    filters_per_group = hcolors_per_group\n    fshape = (fmodulesR, fmodulesC, fcolors, frows, fcols, fgroups, filters_per_group)\n    if (not_symbolic(irows, icols) and (irows != icols)):\n        raise NotImplementedError('non-square image argument', (irows, icols))\n    if (not_symbolic(hrows, hcols) and (hrows != hcols)):\n        raise NotImplementedError('non-square filter shape', (hrows, hcols))\n    if (not_symbolic(icount, hcount) and (icount != hcount)):\n        raise NotImplementedError('different number of images', (icount, hcount))\n    if (not_symbolic(igroups, hgroups) and (igroups != hgroups)):\n        raise ValueError('hgroups must match igroups', igroups, hgroups)\n    return [fshape]\n", "label": 1}
{"function": "\n\ndef _get_text(self, encoding):\n    lines = self.editor.toPlainText().splitlines()\n    if self.clean_trailing_whitespaces:\n        lines = [l.rstrip() for l in lines]\n    try:\n        last_line = lines[(- 1)]\n    except IndexError:\n        pass\n    else:\n        while (last_line == ''):\n            try:\n                lines.pop()\n                last_line = lines[(- 1)]\n            except IndexError:\n                last_line = None\n    text = (self._eol.join(lines) + self._eol)\n    return text.encode(encoding)\n", "label": 1}
{"function": "\n\ndef mon_status(conn, logger, hostname, args, silent=False):\n    '\\n    run ``ceph daemon mon.`hostname` mon_status`` on the remote end and provide\\n    not only the output, but be able to return a boolean status of what is\\n    going on.\\n    ``False`` represents a monitor that is not doing OK even if it is up and\\n    running, while ``True`` would mean the monitor is up and running correctly.\\n    '\n    mon = ('mon.%s' % hostname)\n    try:\n        out = mon_status_check(conn, logger, hostname, args)\n        if (not out):\n            logger.warning(('monitor: %s, might not be running yet' % mon))\n            return False\n        if (not silent):\n            logger.debug(('*' * 80))\n            logger.debug(('status for monitor: %s' % mon))\n            for line in json.dumps(out, indent=2, sort_keys=True).split('\\n'):\n                logger.debug(line)\n            logger.debug(('*' * 80))\n        if (out['rank'] >= 0):\n            logger.info(('monitor: %s is running' % mon))\n            return True\n        if ((out['rank'] == (- 1)) and out['state']):\n            logger.info(('monitor: %s is currently at the state of %s' % (mon, out['state'])))\n            return True\n        logger.info(('monitor: %s is not running' % mon))\n        return False\n    except RuntimeError:\n        logger.info(('monitor: %s is not running' % mon))\n        return False\n", "label": 1}
{"function": "\n\ndef execute(self, cluster, commands):\n    pipes = {\n        \n    }\n    pool = self.get_pool(commands)\n    for (db_num, command_list) in commands.iteritems():\n        pipes[db_num] = cluster[db_num].get_pipeline()\n        for command in command_list:\n            pipes[db_num].add(command.clone())\n    for (db_num, pipe) in pipes.iteritems():\n        pool.add(db_num, pipe.execute, (), {\n            \n        })\n    db_result_map = pool.join()\n    results = defaultdict(list)\n    for (db_num, db_results) in db_result_map.iteritems():\n        assert (len(db_results) == 1)\n        db_results = db_results[0]\n        if isinstance(db_results, Exception):\n            for command in commands[db_num]:\n                results[command].append(db_results)\n            continue\n        for (command, result) in db_results.iteritems():\n            results[command].append(result)\n    return results\n", "label": 1}
{"function": "\n\ndef _init_request_urls(self, api_urls):\n    '\\n        Returns a list of the API URLs.\\n        '\n    if (not isinstance(api_urls, (str, list, tuple))):\n        raise TypeError('api_urls needs to be string or iterable!')\n    if isinstance(api_urls, str):\n        api_urls = (api_urls,)\n    api_urls = list(api_urls)\n    for url in api_urls:\n        if ((not url.startswith('http://')) and (not url.startswith('https://'))):\n            raise ValueError(('URL \"%s\" contains an invalid or missing scheme' % url))\n    return list(api_urls)\n", "label": 1}
{"function": "\n\ndef _HasHeaders(self, args):\n    'Look at the args and decided if we expect headers or not.'\n    headers = True\n    for arg in args:\n        if (arg in ['--no-headers', 'h', '--no-heading']):\n            headers = False\n        elif (arg in ['--headers']):\n            headers = True\n        elif (('h' in arg) and (not arg.startswith('-')) and (',' not in arg)):\n            headers = False\n    return headers\n", "label": 1}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.set_package(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            self.set_capability(d.getPrefixedString())\n            continue\n        if (tt == 24):\n            self.set_status(d.getVarInt32())\n            continue\n        if (tt == 34):\n            self.set_internal_message(d.getPrefixedString())\n            continue\n        if (tt == 42):\n            self.set_admin_message(d.getPrefixedString())\n            continue\n        if (tt == 50):\n            self.set_error_message(d.getPrefixedString())\n            continue\n        if (tt == 58):\n            self.set_scheduled_time(d.getPrefixedString())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef delete_buf(self, path):\n    if (not utils.is_shared(path)):\n        msg.error(('Skipping deleting %s because it is not in shared path %s.' % (path, G.PROJECT_PATH)))\n        return\n    if os.path.isdir(path):\n        for (dirpath, dirnames, filenames) in os.walk(path):\n            dirnames[:] = [d for d in dirnames if (d[0] != '.')]\n            for f in filenames:\n                f_path = os.path.join(dirpath, f)\n                if (f[0] == '.'):\n                    msg.log(('Not deleting buf for hidden file %s' % f_path))\n                else:\n                    self.delete_buf(f_path)\n        return\n    buf_to_delete = self.get_buf_by_path(path)\n    if (buf_to_delete is None):\n        msg.error(('%s is not in this workspace' % path))\n        return\n    msg.log('deleting buffer ', utils.to_rel_path(path))\n    event = {\n        'name': 'delete_buf',\n        'id': buf_to_delete['id'],\n    }\n    G.AGENT.send(event)\n", "label": 1}
{"function": "\n\ndef finalize(self):\n    if (self.ct == 0):\n        return\n    elif (self.ct == 1):\n        return 0\n    prev = min_diff = None\n    while self.heap:\n        if (min_diff is None):\n            if (prev is None):\n                prev = heapq.heappop(self.heap)\n                continue\n        curr = heapq.heappop(self.heap)\n        diff = (curr - prev)\n        if ((min_diff is None) or (min_diff > diff)):\n            min_diff = diff\n        prev = curr\n    return min_diff\n", "label": 1}
{"function": "\n\ndef load_ctfs(saved_data, volume_property):\n    ' Given the saved data produced via `save_ctfs`, this sets the\\n    state of the passed volume_property appropriately.\\n\\n    It returns the new color transfer function and piecewise function.\\n    '\n    rgb = saved_data['rgb']\n    a = saved_data['alpha']\n    new_ctf = True\n    ctf = volume_property.rgb_transfer_function\n    if isinstance(ctf, ColorTransferFunction):\n        new_ctf = False\n        ctf.remove_all_points()\n    else:\n        ctf = ColorTransferFunction()\n    nc = len(rgb)\n    for i in range(nc):\n        ctf.add_rgb_point(rgb[i][0], *rgb[i][1:])\n    if new_ctf:\n        volume_property.set_color(ctf)\n    try:\n        ctf.range = saved_data['range']\n    except Exception:\n        pass\n    na = len(a)\n    new_otf = True\n    otf = volume_property.get_scalar_opacity()\n    if isinstance(otf, PiecewiseFunction):\n        new_otf = False\n        otf.remove_all_points()\n    else:\n        otf = PiecewiseFunction()\n    for i in range(na):\n        otf.add_point(a[i][0], a[i][1])\n    if new_otf:\n        volume_property.set_scalar_opacity(otf)\n    return (ctf, otf)\n", "label": 1}
{"function": "\n\ndef subset_byname(self, variables=None, samples=None):\n    'Returns a subset of the dataset (and metadata).\\n\\n        Same as Dataset.subset() except that variables and samples can be\\n        specified by their names.  \\n        \\n        Some examples:\\n\\n            - d.subset(variables=[\\'shh\\', \\'genex\\'])\\n            - s.subset(samples=[\"control%d\" % i for i in xrange(10)])\\n\\n        '\n    vardict = dict(((v.name, i) for (i, v) in enumerate(self.variables)))\n    sampledict = dict(((s.name, i) for (i, s) in enumerate(self.samples)))\n    variables = ([vardict[v] for v in variables] if variables else variables)\n    samples = ([sampledict[s] for s in samples] if samples else samples)\n    return self.subset(variables, samples)\n", "label": 1}
{"function": "\n\ndef _validate_ip(af, ip):\n    '\\n    Common logic for IPv4\\n    '\n    if ((af == socket.AF_INET6) and (not socket.has_ipv6)):\n        raise RuntimeError('IPv6 not supported')\n    try:\n        socket.inet_pton(af, ip)\n    except AttributeError:\n        if (af == socket.AF_INET6):\n            raise RuntimeError('socket.inet_pton not available')\n        try:\n            socket.inet_aton(ip)\n        except socket.error:\n            return False\n    except (socket.error, TypeError):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef run(self, ctx):\n    argv = ctx.command_argv\n    p = ctx.options_context.parser\n    (o, a) = p.parse_args(argv)\n    if o.help:\n        p.print_help()\n        return\n    n = ctx.build_node.make_node(BUILD_MANIFEST_PATH)\n    build_manifest = BuildManifest.from_file(n.abspath())\n    scheme = ctx.retrieve_configured_scheme()\n    build_manifest.update_paths(scheme)\n    node_sections = build_manifest.resolve_paths_with_destdir(ctx.build_node)\n    if o.list_files:\n        for (kind, source, target) in iter_files(node_sections):\n            print(target.abspath())\n        return\n    if o.transaction:\n        trans = TransactionLog('transaction.log')\n        try:\n            for (kind, source, target) in iter_files(node_sections):\n                trans.copy(source.abspath(), target.abspath(), kind)\n        finally:\n            trans.close()\n    else:\n        for (kind, source, target) in iter_files(node_sections):\n            copy_installer(source.abspath(), target.abspath(), kind)\n", "label": 1}
{"function": "\n\ndef init_asd_db():\n    global asd_db\n    if (not asd_db):\n        try:\n            response = requests.get('https://raw.github.com/stefanschmidt/warranty/master/asdcheck')\n            for (model, val) in [model_str.strip().split(':') for model_str in response.content.split('\\n') if model_str.strip()]:\n                asd_db[model] = val\n        except:\n            asd_db = {\n                \n            }\n", "label": 1}
{"function": "\n\ndef load(s):\n    try:\n        yml_dict = yaml.load(s, yaml_loader)\n    except yaml.YAMLError as exc:\n        msg = 'An error occurred during YAML parsing.'\n        if hasattr(exc, 'problem_mark'):\n            msg += (' Error position: (%s:%s)' % ((exc.problem_mark.line + 1), (exc.problem_mark.column + 1)))\n        raise ValueError(msg)\n    if ((not isinstance(yml_dict, dict)) and (not isinstance(yml_dict, list))):\n        raise ValueError('The source is not a YAML mapping or list.')\n    if (isinstance(yml_dict, dict) and (len(yml_dict) < 1)):\n        raise ValueError('Could not find any element in your YAML mapping.')\n    return yml_dict\n", "label": 1}
{"function": "\n\ndef initialize(self, taskParent, override=None):\n    \"This method initializes a task for (re)use.  taskParent is the\\n        object instance of the parent task, or a 'task environment' (something\\n        that runs tasks).\\n        If subclass overrides this method, it should call the superclass\\n        method at some point.\\n\\n        - Copy shared data from taskParent, overriding items from _override_\\n          if they are present there ('contagion' of task values).\\n        - Generate a unique tag, to be used with the Gen2 Monitor.\\n        - Clear done event, initialize times and result.\\n        \"\n    if (taskParent and hasattr(taskParent, 'shares')):\n        for var in taskParent.shares:\n            if (override and (var in override)):\n                self.__dict__[var] = override[var]\n            else:\n                self.__dict__[var] = taskParent.__dict__[var]\n    else:\n        pass\n    if (not self.tag):\n        try:\n            self.tag = ((str(taskParent) + '.') + self.tagger.get_tag(self))\n        except:\n            self.tag = get_tag(taskParent)\n    self.ev_done.clear()\n    self.starttime = time.time()\n    self.endtime = 0\n    self.totaltime = 0\n    self.result = None\n    return self.tag\n", "label": 1}
{"function": "\n\n@decorator\ndef retry(call, tries, errors=Exception, timeout=0):\n    if isinstance(errors, list):\n        errors = tuple(errors)\n    for attempt in xrange(tries):\n        try:\n            return call()\n        except errors:\n            if ((attempt + 1) == tries):\n                raise\n            else:\n                timeout_value = (timeout(attempt) if callable(timeout) else timeout)\n                if (timeout_value > 0):\n                    time.sleep(timeout_value)\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    if ((self._mock_name is None) and (self._spec_class is None)):\n        return object.__repr__(self)\n    name_string = ''\n    spec_string = ''\n    if (self._mock_name is not None):\n\n        def get_name(name):\n            if (name is None):\n                return 'mock'\n            return name\n        parent = self._mock_parent\n        name = self._mock_name\n        while (parent is not None):\n            name = ((get_name(parent._mock_name) + '.') + name)\n            parent = parent._mock_parent\n        name_string = (' name=%r' % name)\n    if (self._spec_class is not None):\n        spec_string = ' spec=%r'\n        if self._spec_set:\n            spec_string = ' spec_set=%r'\n        spec_string = (spec_string % self._spec_class.__name__)\n    return (\"<%s%s%s id='%s'>\" % (type(self).__name__, name_string, spec_string, id(self)))\n", "label": 1}
{"function": "\n\ndef _hostmaskPatternEqual(pattern, hostmask):\n    try:\n        return (_patternCache[pattern](hostmask) is not None)\n    except KeyError:\n        fd = minisix.io.StringIO()\n        for c in pattern:\n            if (c == '*'):\n                fd.write('.*')\n            elif (c == '?'):\n                fd.write('.')\n            elif (c in '[{'):\n                fd.write('[[{]')\n            elif (c in '}]'):\n                fd.write('[}\\\\]]')\n            elif (c in '|\\\\'):\n                fd.write('[|\\\\\\\\]')\n            elif (c in '^~'):\n                fd.write('[~^]')\n            else:\n                fd.write(re.escape(c))\n        fd.write('$')\n        f = re.compile(fd.getvalue(), re.I).match\n        _patternCache[pattern] = f\n        return (f(hostmask) is not None)\n", "label": 1}
{"function": "\n\ndef __str__(self, prefix='', printElemNumber=0):\n    res = ''\n    if self.has_class_or_file_name_:\n        res += (prefix + ('class_or_file_name: %s\\n' % self.DebugFormatString(self.class_or_file_name_)))\n    if self.has_line_number_:\n        res += (prefix + ('line_number: %s\\n' % self.DebugFormatInt32(self.line_number_)))\n    if self.has_function_name_:\n        res += (prefix + ('function_name: %s\\n' % self.DebugFormatString(self.function_name_)))\n    cnt = 0\n    for e in self.variables_:\n        elm = ''\n        if printElemNumber:\n            elm = ('(%d)' % cnt)\n        res += (prefix + ('variables%s <\\n' % elm))\n        res += e.__str__((prefix + '  '), printElemNumber)\n        res += (prefix + '>\\n')\n        cnt += 1\n    return res\n", "label": 1}
{"function": "\n\n@staticmethod\ndef canMatch(l1, l2):\n    if ((l1 is None) and (l2 is None)):\n        return True\n    elif ((l1 is None) or (l2 is None)):\n        return False\n    try:\n        l1.intersect(l2)\n        return True\n    except EmptyLabelSet:\n        return False\n", "label": 1}
{"function": "\n\ndef tograph(self, filename='fsm.png', nolabel=False):\n    import pygraphviz as pgv\n    graph = pgv.AGraph(directed=True)\n    for (src, dstdict) in self.fsm.items():\n        graph.add_node(str(src), label=str(src))\n        for (cond, dst) in dstdict.items():\n            graph.add_node(str(dst), label=str(dst))\n            if nolabel:\n                graph.add_edge(str(src), str(dst), label='')\n            else:\n                graph.add_edge(str(src), str(dst), label=str(cond))\n    srcs = self.fsm.keys()\n    for src in srcs:\n        for (cond, dst) in self.any.items():\n            graph.add_node(str(dst), label=str(dst))\n            if nolabel:\n                graph.add_edge(str(src), str(dst), label='')\n            else:\n                graph.add_edge(str(src), str(dst), label=str(cond))\n    graph.write('file.dot')\n    graph.layout(prog='dot')\n    graph.draw(filename)\n", "label": 1}
{"function": "\n\ndef arg_opts(self, name, default, taken_names):\n    opts = {\n        \n    }\n    opts['positional'] = (name in self.positional)\n    opts['optional'] = (name in self.optional)\n    if ('_' in name):\n        opts['attr_name'] = name\n        name = translate_underscores(name)\n    names = [name]\n    if self.auto_shortflags:\n        for char in name:\n            if (not ((char == name) or (char in taken_names))):\n                names.append(char)\n                break\n    opts['names'] = names\n    if (default not in (None, NO_DEFAULT)):\n        opts['kind'] = type(default)\n        opts['default'] = default\n    if (name in self.help):\n        opts['help'] = self.help[name]\n    return opts\n", "label": 1}
{"function": "\n\ndef add_website_theme(context):\n    bootstrap = frappe.get_hooks('bootstrap')[0]\n    web_include_css = context.web_include_css\n    context.theme = frappe._dict()\n    if (not context.disable_website_theme):\n        website_theme = get_active_theme()\n        context.theme = ((website_theme and website_theme.as_dict()) or frappe._dict())\n        if website_theme:\n            if website_theme.bootstrap:\n                bootstrap = website_theme.bootstrap\n            context.no_sidebar = website_theme.no_sidebar\n            context.web_include_css = (['website_theme.css'] + context.web_include_css)\n    context.web_include_css = ([bootstrap] + context.web_include_css)\n", "label": 1}
{"function": "\n\ndef __init__(self, config_files=None, refresh=False, private=False, config_key=None, config_defaults=None, cloud=None):\n    if (config_files is None):\n        config_files = []\n    config = os_client_config.config.OpenStackConfig(config_files=(os_client_config.config.CONFIG_FILES + config_files))\n    self.extra_config = config.get_extra_config(config_key, config_defaults)\n    if (cloud is None):\n        self.clouds = [shade.OpenStackCloud(cloud_config=cloud_config) for cloud_config in config.get_all_clouds()]\n    else:\n        try:\n            self.clouds = [shade.OpenStackCloud(cloud_config=config.get_one_cloud(cloud))]\n        except os_client_config.exceptions.OpenStackConfigException as e:\n            raise shade.OpenStackCloudException(e)\n    if private:\n        for cloud in self.clouds:\n            cloud.private = True\n    if refresh:\n        for cloud in self.clouds:\n            cloud._cache.invalidate()\n", "label": 1}
{"function": "\n\ndef format(self, record):\n    'Uses contextstring if request_id is set, otherwise default.'\n    for key in ('instance', 'color'):\n        if (key not in record.__dict__):\n            record.__dict__[key] = ''\n    if record.__dict__.get('request_id', None):\n        self._fmt = CONF.logging_context_format_string\n    else:\n        self._fmt = CONF.logging_default_format_string\n    if ((record.levelno == logging.DEBUG) and CONF.logging_debug_format_suffix):\n        self._fmt += (' ' + CONF.logging_debug_format_suffix)\n    if record.exc_info:\n        record.exc_text = self.formatException(record.exc_info, record)\n    return logging.Formatter.format(self, record)\n", "label": 1}
{"function": "\n\ndef __init__(self, uri, **config):\n    self.uri = uri\n    prefix = config.pop('prefix', '')\n    log.info(\"Connecting MongoEngine to '%s'.\", _safe_uri_replace.sub('\\\\1://\\\\2@', uri))\n    connection = self.connection = dict(tz_aware=True)\n    (scheme, parts) = uri.split('://', 1)\n    (parts, self.db) = parts.split('/', 1)\n    (auth, host) = (parts.split('@', 1) if ('@' in parts) else (None, parts))\n    if (scheme != 'mongodb'):\n        raise Exception(\"The URL must begin with 'mongodb://'!\")\n    (connection['host'], connection['port']) = (host.split(':') if (':' in host) else (host, '27017'))\n    connection['port'] = int(connection['port'])\n    if auth:\n        (connection['username'], _, connection['password']) = auth.partition(':')\n    for (k, v) in items(config):\n        (pfx, _, k) = k.rpartition('.')\n        if ((pfx != prefix) or (k in ('engine', 'model', 'ready'))):\n            continue\n        connection[k] = (int(v) if v.isdigit() else v)\n    self.cb = config.get('ready', None)\n", "label": 1}
{"function": "\n\ndef test_ignore_table_fields(self):\n    c1 = Column('A', format='L', array=[True, False])\n    c2 = Column('B', format='X', array=[[0], [1]])\n    c3 = Column('C', format='4I', dim='(2, 2)', array=[[0, 1, 2, 3], [4, 5, 6, 7]])\n    c4 = Column('B', format='X', array=[[1], [0]])\n    c5 = Column('C', format='4I', dim='(2, 2)', array=[[1, 2, 3, 4], [5, 6, 7, 8]])\n    ta = BinTableHDU.from_columns([c1, c2, c3])\n    tb = BinTableHDU.from_columns([c1, c4, c5])\n    diff = TableDataDiff(ta.data, tb.data, ignore_fields=['B', 'C'])\n    assert diff.identical\n    assert (len(diff.common_columns) == 1)\n    assert (diff.common_column_names == set(['a']))\n    assert (diff.diff_ratio == 0)\n    assert (diff.diff_total == 0)\n", "label": 1}
{"function": "\n\ndef _assert_contains(needle, haystack, invert):\n    matched = re.search(needle, haystack, re.M)\n    if ((invert and matched) or ((not invert) and (not matched))):\n        raise AssertionError((\"r'%s' %sfound in '%s'\" % (needle, ('' if invert else 'not '), haystack)))\n", "label": 1}
{"function": "\n\ndef checkArguments(self, category, arguments, allowedAttributes, isInput, isOutput):\n    for argumentInformation in arguments:\n        helpInfo = (self.name, 'arguments', category)\n        attributes = argumentAttributes()\n        (self.success, attributes) = methods.checkAttributes(argumentInformation, allowedAttributes, attributes, self.allowTermination, helpInfo)\n        if (attributes.longFormArgument in self.arguments):\n            if self.allowTermination:\n                self.errors.repeatedLongFormArgument(helpInfo, attributes.longFormArgument)\n            else:\n                self.success = False\n                return\n        if (attributes.shortFormArgument in self.shortFormArguments):\n            if self.allowTermination:\n                self.errors.repeatedShortFormArgument(helpInfo, attributes.longFormArgument, attributes.shortFormArgument)\n            else:\n                self.success = False\n                return\n        attributes.category = category\n        if isInput:\n            attributes.isInput = True\n        elif isOutput:\n            attributes.isOutput = True\n        self.arguments[attributes.longFormArgument] = attributes\n        self.shortFormArguments.append(attributes.shortFormArgument)\n", "label": 1}
{"function": "\n\ndef val(self, env):\n    'Returns the value of this variable'\n    if self.name:\n        return env[self.name]\n    if (self.constuction[0] == 'Gen+'):\n        gather = [v.val(env) for v in self.constuction[1:]]\n        Sum = None\n        for v in gather:\n            if (Sum is None):\n                Sum = v\n            else:\n                Sum = (v + Sum)\n        return Sum\n    if (self.constuction[0] == 'Gen*'):\n        base = self.constuction[1].val(env)\n        exps = [v.val(env) for v in self.constuction[2:]]\n        Prod = 1\n        for v in exps:\n            Prod = (v * Prod)\n        return (Prod * base)\n    raise Exception('Unknown case')\n", "label": 1}
{"function": "\n\ndef add_data(self, group, name, data):\n    if self.isgroup(data):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'group',\n            'class': data.__class__.__name__,\n        })\n        for comp in dir(data):\n            self.add_data(g, comp, getattr(data, comp))\n    elif isinstance(data, (list, tuple)):\n        dtype = 'list'\n        if isinstance(data, tuple):\n            dtype = 'tuple'\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': dtype,\n        })\n        for (ix, comp) in enumerate(data):\n            iname = ('item%i' % ix)\n            self.add_data(g, iname, comp)\n    elif isinstance(data, dict):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'dict',\n        })\n        for (key, val) in data.items():\n            self.add_data(g, key, val)\n    elif isParameter(data):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'parameter',\n        })\n        self.add_h5dataset(g, 'json', data.asjson())\n    else:\n        d = self.add_h5dataset(group, name, data)\n", "label": 1}
{"function": "\n\ndef get_definition(self, value):\n    '\\n        Get the definition site for the given variable name or instance.\\n        A Expr instance is returned.\\n        '\n    while True:\n        if isinstance(value, ir.Var):\n            name = value.name\n        elif isinstance(value, str):\n            name = value\n        else:\n            return value\n        defs = self.definitions[name]\n        if (len(defs) == 0):\n            raise KeyError(('no definition for %r' % (name,)))\n        if (len(defs) > 1):\n            raise KeyError(('more than one definition for %r' % (name,)))\n        value = defs[0]\n", "label": 1}
{"function": "\n\n@patch('nefertari.view.BaseView._run_init_actions')\ndef test_init(self, run):\n    request = Mock(content_type='application/json', json={\n        'param1.foo': 'val1',\n        'param3': 'val3',\n    }, method='POST', accept=[''])\n    request.params.mixed.return_value = {\n        'param2.foo': 'val2',\n    }\n    view = DummyBaseView(context={\n        'foo': 'bar',\n    }, request=request)\n    run.assert_called_once_with()\n    assert (request.override_renderer == 'nefertari_json')\n    assert (list(sorted(view._params.keys())) == ['param1', 'param2', 'param3'])\n    assert (view._params['param1'] == {\n        'foo': 'val1',\n    })\n    assert (view._params['param2'] == {\n        'foo': 'val2',\n    })\n    assert (view._params['param3'] == 'val3')\n    assert (view.request == request)\n    assert (view.context == {\n        'foo': 'bar',\n    })\n    assert (view._before_calls == {\n        \n    })\n    assert (view._after_calls == {\n        \n    })\n", "label": 1}
{"function": "\n\ndef __init__(self, *fields, **attributes):\n    if self.abstract:\n        raise TypeError('abstract nodes are not instanciable')\n    if fields:\n        if (len(fields) != len(self.fields)):\n            if (not self.fields):\n                raise TypeError(('%r takes 0 arguments' % self.__class__.__name__))\n            raise TypeError(('%r takes 0 or %d argument%s' % (self.__class__.__name__, len(self.fields), (((len(self.fields) != 1) and 's') or ''))))\n        for (name, arg) in izip(self.fields, fields):\n            setattr(self, name, arg)\n    for attr in self.attributes:\n        setattr(self, attr, attributes.pop(attr, None))\n    if attributes:\n        raise TypeError(('unknown attribute %r' % iter(attributes).next()))\n", "label": 1}
{"function": "\n\ndef add_proof(self, lhs, rhs):\n    'Adds a proof obligation to show the rhs is the representation of the lhs'\n    assert isinstance(lhs, Gen)\n    assert (lhs.prove == False)\n    assert isinstance(rhs, Gen)\n    assert (rhs.prove == True)\n    assert (self == lhs.zkp == rhs.zkp)\n    self.proofs += [(lhs, rhs)]\n", "label": 1}
{"function": "\n\ndef seed(self, seed=None):\n    '\\n        Re-initialize each random stream.\\n\\n        Parameters\\n        ----------\\n        seed : None or integer in range 0 to 2**30\\n            Each random stream will be assigned a unique state that depends\\n            deterministically on this value.\\n\\n        Returns\\n        -------\\n        None\\n\\n        '\n    if (seed is None):\n        seed = self.default_instance_seed\n    self.set_rstate(seed)\n    for (old_r, new_r, size, nstreams) in self.state_updates:\n        if (nstreams is None):\n            nstreams = self.n_streams(size)\n        rstates = self.get_substream_rstates(nstreams, new_r.owner.outputs[1].dtype)\n        assert (old_r.get_value(borrow=True, return_internal_type=True).shape == rstates.shape)\n        assert (rstates.dtype == old_r.dtype)\n        old_r.set_value(rstates, borrow=True)\n", "label": 1}
{"function": "\n\ndef load(self, fname=None, name=None):\n    if (name is None):\n        self._mName = fname\n    else:\n        self._mName = name\n    if (fname is not None):\n        if os.path.exists(fname):\n            self._fhandle = os.path.abspath(fname)\n        else:\n            self._fhandle = os.path.join(LAUNCH_PATH, 'Features', 'HaarCascades', fname)\n            if (not os.path.exists(self._fhandle)):\n                logger.warning(('Could not find Haar Cascade file ' + fname))\n                logger.warning('Try running the function img.listHaarFeatures() to see what is available')\n                return None\n        self._mCascade = cv.Load(self._fhandle)\n        if HaarCascade._cache.has_key(self._fhandle):\n            self._mCascade = HaarCascade._cache[fname]\n            return\n        HaarCascade._cache[self._fhandle] = self._mCascade\n    else:\n        logger.warning('No file path mentioned.')\n", "label": 1}
{"function": "\n\ndef rType(value):\n    if (not isinstance(value, list)):\n        value = [value]\n    t = commonType(value)\n    if (t == list):\n        v = rinterface.SexpVector(map(rType, value), rinterface.LISTSXP)\n    elif (t == float):\n        v = rinterface.FloatSexpVector(value)\n    elif (t == int):\n        v = rinterface.IntSexpVector(value)\n    elif (t == bool):\n        v = rinterface.BoolSexpVector(value)\n    elif (t == str):\n        v = rinterface.StrSexpVector(value)\n    else:\n        v = None\n    return v\n", "label": 1}
{"function": "\n\ndef __init__(self, *items):\n    DiagramItem.__init__(self, 'g')\n    self.items = [wrapString(item) for item in items]\n    self.width = sum(((item.width + (20 if item.needsSpace else 0)) for item in self.items))\n    self.up = 0\n    self.down = 0\n    for item in self.items:\n        self.up = max(self.up, (item.up - self.yAdvance))\n        self.down = max(self.down, (item.down + self.yAdvance))\n        self.yAdvance += item.yAdvance\n    if DEBUG:\n        self.attrs['data-updown'] = '{0} {1}'.format(self.up, self.down)\n        self.attrs['data-type'] = 'sequence'\n", "label": 1}
{"function": "\n\ndef Parse(self, cmd, args, stdout, stderr, return_val, time_taken, knowledge_base):\n    'Parse the dpkg output.'\n    _ = (stderr, time_taken, args, knowledge_base)\n    self.CheckReturn(cmd, return_val)\n    column_lengths = []\n    i = 0\n    for (i, line) in enumerate(stdout.splitlines()):\n        if line.startswith('+++-'):\n            for col in line.split('-')[1:]:\n                if (not re.match('=*', col)):\n                    raise parsers.ParseError(('Invalid header parsing for %s at line %s' % (cmd, i)))\n                column_lengths.append(len(col))\n            break\n    if column_lengths:\n        remaining_lines = stdout.splitlines()[(i + 1):]\n        for (i, line) in enumerate(remaining_lines):\n            cols = line.split(None, len(column_lengths))\n            (status, name, version, arch, desc) = cols\n            if (status[1] == 'i'):\n                status = rdf_client.SoftwarePackage.InstallState.INSTALLED\n            else:\n                status = rdf_client.SoftwarePackage.InstallState.UNKNOWN\n            (yield rdf_client.SoftwarePackage(name=name, description=desc, version=version, architecture=arch, install_state=status))\n", "label": 1}
{"function": "\n\ndef parse_vexrc(inp, environ):\n    'Iterator yielding key/value pairs from given stream.\\n\\n    yields tuples of heading, key, value.\\n    '\n    heading = None\n    errors = []\n    with inp:\n        for (line_number, line) in enumerate(inp):\n            line = line.decode('utf-8')\n            if (not line.strip()):\n                continue\n            extracted_heading = extract_heading(line)\n            if (extracted_heading is not None):\n                heading = extracted_heading\n                continue\n            kv_tuple = extract_key_value(line, environ)\n            if (kv_tuple is None):\n                errors.append((line_number, line))\n                continue\n            try:\n                (yield (heading, kv_tuple[0], kv_tuple[1]))\n            except GeneratorExit:\n                break\n    if errors:\n        raise InvalidConfigError(inp.name, errors)\n", "label": 1}
{"function": "\n\ndef compose_doc(self, current_line, edit):\n    params_match = re.search('def +[^ (]+[ (]*([^)]*)\\\\)?', current_line)\n    if params_match:\n        if re.search('def initialize*', current_line):\n            return self.initialize_doc(params_match, current_line)\n        else:\n            return self.method_doc(params_match, current_line)\n    params_match = re.search('class | module', current_line)\n    if params_match:\n        return self.class_doc(params_match, current_line)\n    params_match = re.search('[A-Z]+[ ]+=', current_line)\n    if params_match:\n        return self.const_doc(params_match, current_line)\n    params_match = re.search('attr_reader | attr_writer | attr_accessor ', current_line)\n    if params_match:\n        return self.attributes_doc(params_match, current_line)\n", "label": 1}
{"function": "\n\ndef main(args=None):\n    'Run the main command-line interface for beets. Includes top-level\\n    exception handlers that print friendly error messages.\\n    '\n    try:\n        _raw_main(args)\n    except UserError as exc:\n        message = (exc.args[0] if exc.args else None)\n        log.error('error: {0}', message)\n        sys.exit(1)\n    except util.HumanReadableException as exc:\n        exc.log(log)\n        sys.exit(1)\n    except library.FileOperationError as exc:\n        log.debug('{}', traceback.format_exc())\n        log.error('{}', exc)\n        sys.exit(1)\n    except confit.ConfigError as exc:\n        log.error('configuration error: {0}', exc)\n        sys.exit(1)\n    except db_query.InvalidQueryError as exc:\n        log.error('invalid query: {0}', exc)\n        sys.exit(1)\n    except IOError as exc:\n        if (exc.errno == errno.EPIPE):\n            pass\n        else:\n            raise\n    except KeyboardInterrupt:\n        log.debug('{}', traceback.format_exc())\n", "label": 1}
{"function": "\n\ndef test_merge_tiny_output_opt(tiffs):\n    outputname = str(tiffs.join('merged.tif'))\n    inputs = [str(x) for x in tiffs.listdir()]\n    inputs.sort()\n    runner = CliRunner()\n    result = runner.invoke(merge, (inputs + ['-o', outputname]))\n    assert (result.exit_code == 0)\n    with rasterio.open(outputname) as src:\n        data = src.read()\n        assert (data[0][0:2, 1] == 120).all()\n        assert (data[0][0:2, 2:4] == 90).all()\n        assert (data[0][2][1] == 60)\n        assert (data[0][3][0] == 40)\n", "label": 1}
{"function": "\n\ndef test_graph_equality_attributes(self):\n    '\\n        Graph equality test. This one checks node equality. \\n        '\n    gr = graph()\n    gr.add_nodes([0, 1, 2])\n    gr.add_edge((0, 1))\n    gr.add_node_attribute(1, ('a', 'x'))\n    gr.add_node_attribute(2, ('b', 'y'))\n    gr.add_edge_attribute((0, 1), ('c', 'z'))\n    gr2 = deepcopy(gr)\n    gr3 = deepcopy(gr)\n    gr3.del_edge((0, 1))\n    gr3.add_edge((0, 1))\n    gr4 = deepcopy(gr)\n    gr4.del_edge((0, 1))\n    gr4.add_edge((0, 1))\n    gr4.add_edge_attribute((0, 1), ('d', 'k'))\n    gr5 = deepcopy(gr)\n    gr5.del_node(2)\n    gr5.add_node(2)\n    gr5.add_node_attribute(0, ('d', 'k'))\n    assert (gr == gr2)\n    assert (gr2 == gr)\n    assert (gr != gr3)\n    assert (gr3 != gr)\n    assert (gr != gr4)\n    assert (gr4 != gr)\n    assert (gr != gr5)\n    assert (gr5 != gr)\n", "label": 1}
{"function": "\n\ndef get_function_signature(function, method=True):\n    signature = inspect.getargspec(function)\n    defaults = signature.defaults\n    if method:\n        args = signature.args[1:]\n    else:\n        args = signature.args\n    if defaults:\n        kwargs = zip(args[(- len(defaults)):], defaults)\n        args = args[:(- len(defaults))]\n    else:\n        kwargs = []\n    st = ('%s.%s(' % (function.__module__, function.__name__))\n    for a in args:\n        st += (str(a) + ', ')\n    for (a, v) in kwargs:\n        if (type(v) == str):\n            v = ((\"'\" + v) + \"'\")\n        st += (((str(a) + '=') + str(v)) + ', ')\n    if (kwargs or args):\n        return (st[:(- 2)] + ')')\n    else:\n        return (st + ')')\n", "label": 1}
{"function": "\n\ndef user_has_any_permission_for_instance(self, user, actions, instance):\n    if (('change' in actions) or ('delete' in actions)):\n        if self._check_perm(user, ['change'], collection=instance.collection):\n            return True\n        elif (self._check_perm(user, ['add'], collection=instance.collection) and (getattr(instance, self.owner_field_name) == user)):\n            return True\n        else:\n            return False\n    else:\n        return (user.is_active and user.is_superuser)\n", "label": 1}
{"function": "\n\ndef run(self, name, board_id, list_id, api_key=None, token=None):\n    if api_key:\n        self._set_creds(api_key=api_key, token=token)\n    cards = []\n    board = self._client().get_board(board_id)\n    lst = board.get_list(list_id)\n    for card in lst.list_cards():\n        if ((card.name == name) and (not card.closed)):\n            cards.append(card.id)\n    if (len(cards) == 0):\n        return False\n    else:\n        return cards\n", "label": 1}
{"function": "\n\ndef cmd(self, *args, **kwargs):\n    data = self.main(*args, **kwargs)\n    result = CommandResult()\n    for (key, value) in data.items():\n        result = result.add_line('Ran build plugin {plugin}', plugin=key)\n        if (not value):\n            continue\n        if (not isinstance(value, basestring)):\n            value = json.dumps(value, indent=4, sort_keys=True)\n        for line in value.split('\\n'):\n            if (not line.strip()):\n                continue\n            result = result.add_line('\\t{line}', line=line)\n    return result\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('RemoteControlInstance')\n    if (self.device is not None):\n        oprot.writeFieldBegin('device', TType.STRUCT, 1)\n        self.device.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.timeout is not None):\n        oprot.writeFieldBegin('timeout', TType.I32, 2)\n        oprot.writeI32(self.timeout)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef update_record(self, record, name=None, type=None, data=None, extra=None):\n    extra = (extra or {\n        \n    })\n    params = {\n        'z': record.zone.domain,\n        'id': record.id,\n    }\n    params['name'] = (name or record.name)\n    params['type'] = (type or record.type)\n    params['content'] = (data or record.data)\n    params['ttl'] = (extra.get('ttl', None) or record.extra['ttl'])\n    self.connection.set_context({\n        'zone_domain': record.zone.domain,\n    })\n    self.connection.set_context({\n        'record_id': record.id,\n    })\n    resp = self.connection.request(action='rec_edit', params=params)\n    item = resp.object['response']['rec']['obj']\n    record = self._to_record(zone=record.zone, item=item)\n    return record\n", "label": 1}
{"function": "\n\ndef write_fasta(ofile, s, chunk=60, id=None, reformatter=None):\n    'Trivial FASTA output'\n    if (id is None):\n        try:\n            id = str(s.id)\n        except AttributeError:\n            id = new_seq_id()\n    ofile.write((('>' + id) + '\\n'))\n    seq = str(s)\n    if (reformatter is not None):\n        seq = reformatter(seq)\n    end = len(seq)\n    pos = 0\n    while 1:\n        ofile.write((seq[pos:(pos + chunk)] + '\\n'))\n        pos += chunk\n        if (pos >= end):\n            break\n    return id\n", "label": 1}
{"function": "\n\ndef get_recipient_info(self, message, contact_cache):\n    recipient_id = message.couch_recipient\n    if (recipient_id in contact_cache):\n        return contact_cache[recipient_id]\n    doc = None\n    if (recipient_id not in [None, '']):\n        try:\n            if (message.couch_recipient_doc_type == 'CommCareCase'):\n                doc = CommCareCase.get(recipient_id)\n            else:\n                doc = CouchUser.get_by_user_id(recipient_id)\n        except Exception:\n            pass\n    if doc:\n        doc_info = get_doc_info(doc.to_json(), self.domain)\n    else:\n        doc_info = None\n    contact_cache[recipient_id] = doc_info\n    return doc_info\n", "label": 1}
{"function": "\n\ndef check_hbase_cluster_status(self, cluster):\n    job = cluster.jobs['master']\n    if (job.running_tasks_count < 1):\n        job.last_status = Status.ERROR\n        job.last_message = 'No running masters!'\n    else:\n        active = 0\n        for task in job.running_tasks.itervalues():\n            if self.is_master_active(task):\n                cluster.entry = ('%s:%d' % (task.host, task.port))\n                version = self.get_latest_metric(task, 'hadoop:service=HBase,name=Info', 'version')\n                revision = self.get_latest_metric(task, 'hadoop:service=HBase,name=Info', 'revision')\n                cluster.version = ('%s, r%s' % (version, revision))\n                active += 1\n        if (active > 1):\n            job.last_status = Status.ERROR\n            job.last_message = 'Too many active masters!'\n        elif (active < 1):\n            job.last_status = Status.ERROR\n            job.last_message = 'No active masters!'\n        elif (job.running_tasks_count < 2):\n            pass\n    job = cluster.jobs['regionserver']\n    if (job.running_tasks_count < 3):\n        job.last_status = Status.ERROR\n        job.last_message = 'Too few running regionservers!'\n    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])\n", "label": 1}
{"function": "\n\ndef mkdir(self, path, parents=True, raise_if_exists=False):\n    if (raise_if_exists and self.isdir(path)):\n        raise FileAlreadyExists()\n    (_, key) = self._path_to_bucket_and_key(path)\n    if self._is_root(key):\n        return\n    key = self._add_path_delimiter(key)\n    if ((not parents) and (not self.isdir(os.path.dirname(key)))):\n        raise MissingParentDirectory()\n    return self.put_string('', self._add_path_delimiter(path))\n", "label": 1}
{"function": "\n\ndef copy_file(self, path, prefixed_path, source_storage, **options):\n    '\\n        Attempt to copy ``path`` with storage\\n        '\n    if (prefixed_path in self.copied_files):\n        return self.log((\"Skipping '%s' (already copied earlier)\" % path))\n    if (not self.delete_file(path, prefixed_path, source_storage, **options)):\n        return\n    source_path = source_storage.path(path)\n    if options['dry_run']:\n        self.log((\"Pretending to copy '%s'\" % source_path), level=1)\n    else:\n        self.log((\"Copying '%s'\" % source_path), level=1)\n        if self.local:\n            full_path = self.storage.path(prefixed_path)\n            try:\n                os.makedirs(os.path.dirname(full_path))\n            except OSError:\n                pass\n            shutil.copy2(source_path, full_path)\n        else:\n            source_file = source_storage.open(path)\n            self.storage.save(prefixed_path, source_file)\n    if (not (prefixed_path in self.copied_files)):\n        self.copied_files.append(prefixed_path)\n", "label": 1}
{"function": "\n\ndef _check_criterion(self, criterion_k, criterion_v, payload_lookup):\n    if ('type' not in criterion_v):\n        return False\n    criteria_operator = criterion_v['type']\n    criteria_pattern = criterion_v.get('pattern', None)\n    try:\n        criteria_pattern = self._render_criteria_pattern(criteria_pattern=criteria_pattern)\n    except Exception:\n        LOG.exception(('Failed to render pattern value \"%s\" for key \"%s\"' % (criteria_pattern, criterion_k)), extra=self._base_logger_context)\n        return False\n    try:\n        matches = payload_lookup.get_value(criterion_k)\n        if matches:\n            payload_value = (matches[0] if (len(matches) > 0) else matches)\n        else:\n            payload_value = None\n    except:\n        LOG.exception('Failed transforming criteria key %s', criterion_k, extra=self._base_logger_context)\n        return False\n    op_func = criteria_operators.get_operator(criteria_operator)\n    try:\n        result = op_func(value=payload_value, criteria_pattern=criteria_pattern)\n    except:\n        LOG.exception('There might be a problem with critera in rule %s.', self.rule, extra=self._base_logger_context)\n        return False\n    return (result, payload_value, criteria_pattern)\n", "label": 1}
{"function": "\n\ndef status_done(self, result):\n    if (result is False):\n        self.view.set_status('git-status-index', '')\n        self.view.set_status('git-status-working', '')\n    else:\n        lines = [line for line in result.splitlines() if re.match('^[ MADRCU?!]{1,2}\\\\s+.*', line)]\n        index = [line[0] for line in lines if (not line[0].isspace())]\n        working = [line[1] for line in lines if (not line[1].isspace())]\n        self.view.set_status('git-status-index', ('index: ' + self.status_string(index)))\n        self.view.set_status('git-status-working', ('working: ' + self.status_string(working)))\n", "label": 1}
{"function": "\n\ndef cleanup(self, _warn=False):\n    if (self.name and (not self._closed)):\n        try:\n            self._rmtree(self.name)\n        except (TypeError, AttributeError) as ex:\n            if ('None' not in str(ex)):\n                raise\n            print_('ERROR: {!r} while cleaning up {!r}'.format(ex, self), file=_sys.stderr)\n            return\n        self._closed = True\n        if _warn:\n            self._warn('Implicitly cleaning up {!r}'.format(self), Warning)\n", "label": 1}
{"function": "\n\ndef _absolute_path(path, relative_to=None):\n    '\\n    Return an absolute path. In case ``relative_to`` is passed and ``path`` is\\n    not an absolute path, we try to prepend ``relative_to`` to ``path``and if\\n    that path exists, return that one\\n    '\n    if (path and os.path.isabs(path)):\n        return path\n    if (path and (relative_to is not None)):\n        _abspath = os.path.join(relative_to, path)\n        if os.path.isfile(_abspath):\n            log.debug(\"Relative path '{0}' converted to existing absolute path '{1}'\".format(path, _abspath))\n            return _abspath\n    return path\n", "label": 1}
{"function": "\n\ndef run(self, count=None, region=None, metadata=None):\n    if region:\n        cs = self.pyrax.connect_to_cloudservers(region=region)\n    else:\n        cs = self.pyrax.cloudservers\n    servers = cs.list()\n    result = []\n    for server in servers:\n        item = to_server_dict(server=server)\n        if metadata:\n            include = self._metadata_intersection(server=item, metadata=metadata)\n            if (not include):\n                continue\n        result.append(item['name'])\n    if count:\n        return result[0:count]\n    else:\n        return result\n", "label": 1}
{"function": "\n\n@throttle(0.1)\ndef update_panel(self):\n    window = sublime.active_window()\n    try:\n        result_panel = window.create_output_panel('test_runner')\n    except:\n        result_panel = window.get_output_panel('test_runner')\n    result_panel.set_syntax_file('Packages/Test Runner/TestRunnerOutput.tmLanguage')\n    result_panel.run_command('update_panel', {\n        'message': self.result['message'],\n    })\n    if ((self.result['failed'] > 0) or settings.get('show_panel_default', False)):\n        window.run_command('show_panel', {\n            'panel': 'output.test_runner',\n        })\n    elif (self.result['status'] == 'executed'):\n        window.run_command('hide_panel', {\n            'panel': 'output.test_runner',\n        })\n    if self.is_alive():\n        self.update_panel()\n", "label": 1}
{"function": "\n\ndef _process_dependent_arguments(self):\n    'Convert incoming configuration arguments to their\\n        proper form.\\n\\n        Callables are resolved, ORM annotations removed.\\n\\n        '\n    for attr in ('order_by', 'primaryjoin', 'secondaryjoin', 'secondary', '_user_defined_foreign_keys', 'remote_side'):\n        attr_value = getattr(self, attr)\n        if util.callable(attr_value):\n            setattr(self, attr, attr_value())\n    for attr in ('primaryjoin', 'secondaryjoin'):\n        val = getattr(self, attr)\n        if (val is not None):\n            setattr(self, attr, _orm_deannotate(expression._only_column_elements(val, attr)))\n    if ((self.order_by is not False) and (self.order_by is not None)):\n        self.order_by = [expression._only_column_elements(x, 'order_by') for x in util.to_list(self.order_by)]\n    self._user_defined_foreign_keys = util.column_set((expression._only_column_elements(x, 'foreign_keys') for x in util.to_column_set(self._user_defined_foreign_keys)))\n    self.remote_side = util.column_set((expression._only_column_elements(x, 'remote_side') for x in util.to_column_set(self.remote_side)))\n    self.target = self.mapper.mapped_table\n", "label": 1}
{"function": "\n\ndef test_assert_has_calls(self):\n    kalls1 = [call(1, 2), ({\n        'a': 3,\n    },), ((3, 4),), call(b=6), ('', (1,), {\n        'b': 6,\n    })]\n    kalls2 = [call.foo(), call.bar(1)]\n    kalls2.extend(call.spam().baz(a=3).call_list())\n    kalls2.extend(call.bam(set(), foo={\n        \n    }).fish([1]).call_list())\n    mocks = []\n    for mock in (Mock(), MagicMock()):\n        mock(1, 2)\n        mock(a=3)\n        mock(3, 4)\n        mock(b=6)\n        mock(1, b=6)\n        mocks.append((mock, kalls1))\n    mock = Mock()\n    mock.foo()\n    mock.bar(1)\n    mock.spam().baz(a=3)\n    mock.bam(set(), foo={\n        \n    }).fish([1])\n    mocks.append((mock, kalls2))\n    for (mock, kalls) in mocks:\n        for i in range(len(kalls)):\n            for step in (1, 2, 3):\n                these = kalls[i:(i + step)]\n                mock.assert_has_calls(these)\n                if (len(these) > 1):\n                    self.assertRaises(AssertionError, mock.assert_has_calls, list(reversed(these)))\n", "label": 1}
{"function": "\n\ndef test_caching(self):\n    changed = False\n\n    class TestLoader(loaders.BaseLoader):\n\n        def get_source(self, environment, template):\n            return ('foo', None, (lambda : (not changed)))\n    env = Environment(loader=TestLoader(), cache_size=(- 1))\n    tmpl = env.get_template('template')\n    assert (tmpl is env.get_template('template'))\n    changed = True\n    assert (tmpl is not env.get_template('template'))\n    changed = False\n    env = Environment(loader=TestLoader(), cache_size=0)\n    assert (env.get_template('template') is not env.get_template('template'))\n    env = Environment(loader=TestLoader(), cache_size=2)\n    t1 = env.get_template('one')\n    t2 = env.get_template('two')\n    assert (t2 is env.get_template('two'))\n    assert (t1 is env.get_template('one'))\n    t3 = env.get_template('three')\n    assert ('one' in env.cache)\n    assert ('two' not in env.cache)\n    assert ('three' in env.cache)\n", "label": 1}
{"function": "\n\ndef fragment_count(self):\n    table = self.fragmentruntable.payload.fragment_run_entry_table\n    (first_fragment, end_fragment) = (None, None)\n    for (i, fragmentrun) in enumerate(table):\n        if (fragmentrun.discontinuity_indicator is not None):\n            if (fragmentrun.discontinuity_indicator == 0):\n                break\n            elif (fragmentrun.discontinuity_indicator > 0):\n                continue\n        if (first_fragment is None):\n            first_fragment = fragmentrun.first_fragment\n        end_fragment = fragmentrun.first_fragment\n        fragment_duration = (fragmentrun.first_fragment_timestamp + fragmentrun.fragment_duration)\n        if (self.timestamp > fragment_duration):\n            offset = ((self.timestamp - fragment_duration) / fragmentrun.fragment_duration)\n            end_fragment += int(offset)\n    if (first_fragment is None):\n        first_fragment = 1\n    if (end_fragment is None):\n        end_fragment = 1\n    return (first_fragment, end_fragment)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if isinstance(other, datetime):\n        other = Date(other)\n    if isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return (self.date == 'infinity')\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) == other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date == other.date.replace(tzinfo=self.tz))\n        return (self.date == other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            return False\n        else:\n            return self.__eq__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef performAction(action, workflow):\n    if (action.actionType == 'add'):\n        for operation in action.db_operations:\n            workflow.db_add_object(operation.db_data, operation.db_parentObjType, operation.db_parentObjId)\n    elif (action.actionType == 'change'):\n        for operation in action.db_operations:\n            workflow.db_change_object(operation.db_data, operation.db_parentObjType, operation.db_parentObjId)\n    elif (action.actionType == 'delete'):\n        for operation in action.operations:\n            workflow.db_delete_object(operation.db_objectId, operation.db_what, operation.db_parentObjType, operation.db_parentObjId)\n    else:\n        msg = (\"Unrecognized action type '%s'\" % action.db_actionType)\n        raise TypeError(msg)\n", "label": 1}
{"function": "\n\ndef onSelectDet(self, event=None, index=0, init=False, **kws):\n    if (index > 0):\n        self.det_fore = index\n    self.det_back = self.wids['bkg_det'].GetSelection()\n    if (self.det_fore == self.det_back):\n        self.det_back = 0\n    for i in range(1, (self.nmca + 1)):\n        dname = ('det%i' % i)\n        bcol = (220, 220, 220)\n        fcol = (0, 0, 0)\n        if (i == self.det_fore):\n            bcol = (60, 50, 245)\n            fcol = (240, 230, 100)\n        if (i == self.det_back):\n            bcol = (80, 200, 20)\n        self.wids[dname].SetBackgroundColour(bcol)\n        self.wids[dname].SetForegroundColour(fcol)\n    self.clear_mcas()\n    self.show_mca(init=init)\n    self.Refresh()\n", "label": 1}
{"function": "\n\ndef _ensure_holidays_span_datetime(self, dt):\n    if callable(self.holidays):\n        if ((self._holidaysGeneratorStart is None) or (dt < self._holidaysGeneratorStart)):\n            self._holidaysGeneratorStart = dt\n            self._holidaysGenerator = self.holidays(dt)\n        while ((len(self._holidays) == 0) or (dt > self._holidays[(- 1)])):\n            self._holidays.append(next(self._holidaysGenerator))\n", "label": 1}
{"function": "\n\ndef __init__(self, cgroups, kill_process_fn, hardtimelimit, softtimelimit, walltimelimit, pid_to_kill, cores, callbackFn=(lambda reason: None)):\n    super(_TimelimitThread, self).__init__()\n    if (hardtimelimit or softtimelimit):\n        assert (CPUACCT in cgroups)\n    assert (walltimelimit is not None)\n    if cores:\n        self.cpuCount = len(cores)\n    else:\n        try:\n            self.cpuCount = multiprocessing.cpu_count()\n        except NotImplementedError:\n            self.cpuCount = 1\n    self.daemon = True\n    self.cgroups = cgroups\n    self.timelimit = (hardtimelimit or ((((60 * 60) * 24) * 365) * 100))\n    self.softtimelimit = (softtimelimit or ((((60 * 60) * 24) * 365) * 100))\n    self.latestKillTime = (util.read_monotonic_time() + walltimelimit)\n    self.pid_to_kill = pid_to_kill\n    self.callback = callbackFn\n    self.kill_process = kill_process_fn\n    self.finished = threading.Event()\n", "label": 1}
{"function": "\n\ndef doAuthenticate(self, msg):\n    if (not self.authenticate_decoder):\n        self.authenticate_decoder = ircutils.AuthenticateDecoder()\n    self.authenticate_decoder.feed(msg)\n    if (not self.authenticate_decoder.ready):\n        return\n    string = self.authenticate_decoder.get()\n    self.authenticate_decoder = None\n    mechanism = self.sasl_current_mechanism\n    if (mechanism == 'ecdsa-nist256p-challenge'):\n        if (string == b''):\n            self.sendSaslString(self.sasl_username.encode('utf-8'))\n            return\n        try:\n            with open(self.sasl_ecdsa_key) as fd:\n                private_key = SigningKey.from_pem(fd.read())\n            authstring = private_key.sign(base64.b64decode(msg.args[0].encode()))\n            self.sendSaslString(authstring)\n        except (BadDigestError, OSError, ValueError):\n            self.sendMsg(ircmsgs.IrcMsg(command='AUTHENTICATE', args=('*',)))\n            self.tryNextSaslMechanism()\n    elif (mechanism == 'external'):\n        self.sendSaslString(b'')\n    elif (mechanism == 'plain'):\n        authstring = b'\\x00'.join([self.sasl_username.encode('utf-8'), self.sasl_username.encode('utf-8'), self.sasl_password.encode('utf-8')])\n        self.sendSaslString(authstring)\n", "label": 1}
{"function": "\n\ndef add_block_proposal(self, p):\n    assert isinstance(p, BlockProposal)\n    if self.has_blockproposal(p.blockhash):\n        self.log('known block_proposal')\n        return\n    assert p.signing_lockset.has_quorum\n    assert (p.signing_lockset.height == (p.height - 1))\n    for v in p.signing_lockset:\n        self.add_vote(v)\n    self.block_candidates[p.blockhash] = p\n", "label": 1}
{"function": "\n\ndef render_image(self, rgbobj, dst_x, dst_y):\n    for ax in self.figure.axes:\n        if (not (ax in (self.ax_img, self.ax_util))):\n            if hasattr(ax, 'lines'):\n                for line in ax.lines:\n                    try:\n                        line._transformed_path.invalidate()\n                    except AttributeError:\n                        pass\n    if self.in_axes:\n        self.render_image2(rgbobj, dst_x, dst_y)\n    else:\n        self.render_image1(rgbobj, dst_x, dst_y)\n    self.ax_util.cla()\n    if self.t_['show_pan_position']:\n        self.ax_util.add_line(self.cross1)\n        self.ax_util.add_line(self.cross2)\n    if self.message:\n        self.draw_message(self.message)\n    self.figure.canvas.draw()\n", "label": 1}
{"function": "\n\ndef dates(self):\n    'returns the years and months for which there are posts'\n    if o_settings.CACHE_ENABLED:\n        key = get_key('posts_dates')\n        cached = cache.get(key)\n        if cached:\n            return cached\n    posts = self.published()\n    dates = OrderedDict()\n    for post in posts:\n        key = post.created.strftime('%Y_%m')\n        try:\n            dates[key][1] = (dates[key][1] + 1)\n        except KeyError:\n            dates[key] = [post.created, 1]\n    if o_settings.CACHE_ENABLED:\n        cache.set(key, dates, o_settings.CACHE_TIMEOUT)\n    return dates\n", "label": 1}
{"function": "\n\ndef proxy_request(service_name, instance_name, method, path, body=None, headers=None):\n    target = get_env('TSURU_TARGET').rstrip('/')\n    token = get_env('TSURU_TOKEN')\n    if ((not target.startswith('http://')) and (not target.startswith('https://'))):\n        target = 'http://{}'.format(target)\n    url = '{}/services/{}/proxy/{}?callback={}'.format(target, service_name, instance_name, path)\n    if body:\n        body = json.dumps(body)\n    request = Request(method, url, data=body)\n    request.add_header('Authorization', ('bearer ' + token))\n    if headers:\n        for (header, value) in headers.items():\n            request.add_header(header, value)\n    return urllib2.urlopen(request, timeout=30)\n", "label": 1}
{"function": "\n\ndef gf_add(f, g, p, K):\n    '\\n    Add polynomials in ``GF(p)[x]``.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys.domains import ZZ\\n    >>> from sympy.polys.galoistools import gf_add\\n\\n    >>> gf_add([3, 2, 4], [2, 2, 2], 5, ZZ)\\n    [4, 1]\\n\\n    '\n    if (not f):\n        return g\n    if (not g):\n        return f\n    df = gf_degree(f)\n    dg = gf_degree(g)\n    if (df == dg):\n        return gf_strip([((a + b) % p) for (a, b) in zip(f, g)])\n    else:\n        k = abs((df - dg))\n        if (df > dg):\n            (h, f) = (f[:k], f[k:])\n        else:\n            (h, g) = (g[:k], g[k:])\n        return (h + [((a + b) % p) for (a, b) in zip(f, g)])\n", "label": 1}
{"function": "\n\ndef to_obj(self, return_obj=None, ns_info=None):\n    self._collect_ns_info(ns_info)\n    artifact_obj = artifact_binding.ArtifactObjectType()\n    super(Artifact, self).to_obj(return_obj=artifact_obj, ns_info=ns_info)\n    if self.packaging:\n        packaging = artifact_binding.PackagingType()\n        for p in self.packaging:\n            p_obj = p.to_obj(ns_info=ns_info)\n            if isinstance(p, Compression):\n                packaging.add_Compression(p_obj)\n            elif isinstance(p, Encryption):\n                packaging.add_Encryption(p_obj)\n            elif isinstance(p, Encoding):\n                packaging.add_Encoding(p_obj)\n            else:\n                raise ValueError(('Unsupported Packaging Type: %s' % type(p)))\n        artifact_obj.Packaging = packaging\n    if self.packed_data:\n        artifact_obj.Raw_Artifact = RawArtifact(self.packed_data).to_obj(ns_info=ns_info)\n    artifact_obj.type_ = self.type_\n    return artifact_obj\n", "label": 1}
{"function": "\n\ndef to_python(self, data):\n    '\\n        Checks that the file-upload field data contains a valid image (GIF, JPG,\\n        PNG, possibly others -- whatever the Python Imaging Library supports).\\n        '\n    f = super(ImageField, self).to_python(data)\n    if (f is None):\n        return None\n    from PIL import Image\n    if hasattr(data, 'temporary_file_path'):\n        file = data.temporary_file_path()\n    elif hasattr(data, 'read'):\n        file = BytesIO(data.read())\n    else:\n        file = BytesIO(data['content'])\n    try:\n        image = Image.open(file)\n        image.verify()\n        f.image = image\n        f.content_type = Image.MIME.get(image.format)\n    except Exception:\n        six.reraise(ValidationError, ValidationError(self.error_messages['invalid_image'], code='invalid_image'), sys.exc_info()[2])\n    if (hasattr(f, 'seek') and callable(f.seek)):\n        f.seek(0)\n    return f\n", "label": 1}
{"function": "\n\ndef _verify_source_estimate_compat(a, b):\n    'Make sure two SourceEstimates are compatible for arith. operations'\n    compat = False\n    if (len(a.vertices) == len(b.vertices)):\n        if all((np.array_equal(av, vv) for (av, vv) in zip(a.vertices, b.vertices))):\n            compat = True\n    if (not compat):\n        raise ValueError('Cannot combine SourceEstimates that do not have the same vertices. Consider using stc.expand().')\n    if (a.subject != b.subject):\n        raise ValueError(('source estimates do not have the same subject names, %r and %r' % (a.subject, b.subject)))\n", "label": 1}
{"function": "\n\ndef test_repeat_get_url_interval(url_prefix):\n    session = RateLimitRequests(url_interval=1)\n    t = datetime.now()\n    resp1 = session.get((url_prefix + '/cookies'))\n    session.get((url_prefix + '/cookies/set?a=b'))\n    resp2 = session.get((url_prefix + '/cookies'))\n    assert ((datetime.now() - t) < timedelta(seconds=1))\n    assert (resp1 == resp2)\n    assert (resp1.json()['cookies'] == resp2.json()['cookies'])\n    time.sleep(1)\n    resp3 = session.get((url_prefix + '/cookies'))\n    assert (resp1 != resp3)\n    assert (resp1.json()['cookies'] != resp3.json()['cookies'])\n", "label": 1}
{"function": "\n\ndef _create_test_db(self, verbosity, autoclobber, keepdb=False):\n    '\\n        Internal implementation - creates the test db tables.\\n        '\n    suffix = self.sql_table_creation_suffix()\n    test_database_name = self._get_test_db_name()\n    qn = self.connection.ops.quote_name\n    with self._nodb_connection.cursor() as cursor:\n        try:\n            cursor.execute(('CREATE DATABASE %s %s' % (qn(test_database_name), suffix)))\n        except Exception as e:\n            if keepdb:\n                return test_database_name\n            sys.stderr.write(('Got an error creating the test database: %s\\n' % e))\n            if (not autoclobber):\n                confirm = input((\"Type 'yes' if you would like to try deleting the test database '%s', or 'no' to cancel: \" % test_database_name))\n            if (autoclobber or (confirm == 'yes')):\n                try:\n                    if (verbosity >= 1):\n                        print(('Destroying old test database for alias %s...' % (self._get_database_display_str(verbosity, test_database_name),)))\n                    cursor.execute(('DROP DATABASE %s' % qn(test_database_name)))\n                    cursor.execute(('CREATE DATABASE %s %s' % (qn(test_database_name), suffix)))\n                except Exception as e:\n                    sys.stderr.write(('Got an error recreating the test database: %s\\n' % e))\n                    sys.exit(2)\n            else:\n                print('Tests cancelled.')\n                sys.exit(1)\n    return test_database_name\n", "label": 1}
{"function": "\n\ndef processLine(result, line):\n    initLine(result, line)\n    if (result['mode'] == INDENT_MODE):\n        result['trackingIndent'] = ((len(result['parenStack']) != 0) and (not result['isInStr']))\n    elif (result['mode'] == PAREN_MODE):\n        result['trackingIndent'] = (not result['isInStr'])\n    chars = (line + NEWLINE)\n    for c in chars:\n        processChar(result, c)\n    if (result['lineNo'] == result['parenTrail']['lineNo']):\n        finishNewParenTrail(result)\n", "label": 1}
{"function": "\n\ndef get_variable_name(self, abbreviated_xpath):\n    '\\n        If the abbreviated_xpath has been renamed in\\n        self.variable_names_json return that new name, otherwise\\n        return the original abbreviated_xpath.\\n        '\n    if (not hasattr(self, '_keys')):\n        self._keys = self.get_keys()\n    if (not hasattr(self, '_headers')):\n        self._headers = self.get_headers()\n    assert (abbreviated_xpath in self._keys), abbreviated_xpath\n    i = self._keys.index(abbreviated_xpath)\n    header = self._headers[i]\n    if (not hasattr(self, '_variable_names')):\n        self._variable_names = ColumnRename.get_dict()\n        assert (type(self._variable_names) == dict)\n    if ((header in self._variable_names) and self._variable_names[header]):\n        return self._variable_names[header]\n    return header\n", "label": 1}
{"function": "\n\ndef export_to_csv(self, model_name):\n    self.header('Exporting models ...')\n    today = datetime.datetime.today()\n    model = get_model('calaccess_campaign_browser', model_name)\n    fieldnames = ([f.name for f in model._meta.fields] + ['committee_name', 'filer_name', 'filer_id', 'filer_id_raw'])\n    relation_names = ([f.name for f in model._meta.fields] + ['committee__name', 'committee__filer__name', 'committee__filer__id', 'committee__filer__filer_id_raw'])\n    filename = '{}-{}-{}-{}.csv'.format(today.year, today.month, today.day, model_name.lower())\n    filepath = os.path.join(self.data_dir, filename)\n    self.header('  Exporting {} model ...'.format(model_name.capitalize()))\n    with open(filepath, 'wb') as csvfile:\n        writer = csv.writer(csvfile, delimiter='\\t')\n        writer.writerow(fieldnames)\n        if (model_name != 'summary'):\n            for cycle in Cycle.objects.all():\n                self.log('    Looking at cycle {} ...'.format(cycle.name))\n                rows = model.objects.filter(cycle=cycle).exclude(is_duplicate=True).values_list(*relation_names)\n                if (not rows):\n                    self.failure('      No data for {}'.format(cycle.name))\n                else:\n                    rows = self.encoded(rows)\n                    writer.writerows(rows)\n                    self.success('      Added {} {} data'.format(cycle.name, model_name))\n        else:\n            rows = self.encoded(model.objects.values_list())\n            writer.writerows(rows)\n    self.success('  Exported {}!'.format(model_name.capitalize()))\n", "label": 1}
{"function": "\n\ndef do_copy(self, new_ids=False, id_scope=None, id_remap=None):\n    cp = DBWorkflowExec()\n    cp._db_id = self._db_id\n    cp._db_user = self._db_user\n    cp._db_ip = self._db_ip\n    cp._db_vt_version = self._db_vt_version\n    cp._db_ts_start = self._db_ts_start\n    cp._db_ts_end = self._db_ts_end\n    cp._db_parent_id = self._db_parent_id\n    cp._db_parent_type = self._db_parent_type\n    cp._db_parent_version = self._db_parent_version\n    cp._db_name = self._db_name\n    if (self._db_module_execs is None):\n        cp._db_module_execs = []\n    else:\n        cp._db_module_execs = [v.do_copy(new_ids, id_scope, id_remap) for v in self._db_module_execs]\n    if new_ids:\n        new_id = id_scope.getNewId(self.vtType)\n        if (self.vtType in id_scope.remap):\n            id_remap[(id_scope.remap[self.vtType], self._db_id)] = new_id\n        else:\n            id_remap[(self.vtType, self._db_id)] = new_id\n        cp._db_id = new_id\n    for v in cp._db_module_execs:\n        cp.db_module_execs_id_index[v._db_id] = v\n    cp.is_dirty = self.is_dirty\n    cp.is_new = self.is_new\n    return cp\n", "label": 1}
{"function": "\n\ndef bind_port(self, context):\n    LOG.debug('Attempting to bind port %(port)s on network %(network)s', {\n        'port': context.current['id'],\n        'network': context.network.current['id'],\n    })\n    vnic_type = context.current.get(portbindings.VNIC_TYPE, portbindings.VNIC_NORMAL)\n    if (vnic_type not in self.supported_vnic_types):\n        LOG.debug('Refusing to bind due to unsupported vnic_type: %s', vnic_type)\n        return\n    vif_type = self.vnic_type_for_vif_type.get(vnic_type, VIF_TYPE_HW_VEB)\n    if (not self._check_supported_pci_vendor_device(context)):\n        LOG.debug('Refusing to bind due to unsupported pci_vendor device')\n        return\n    if (vnic_type == portbindings.VNIC_DIRECT_PHYSICAL):\n        self.try_to_bind(context, None, vif_type)\n        return\n    for agent in context.host_agents(self.agent_type):\n        LOG.debug('Checking agent: %s', agent)\n        if agent['alive']:\n            if self.try_to_bind(context, agent, vif_type):\n                return\n        else:\n            LOG.warning(_LW('Attempting to bind with dead agent: %s'), agent)\n", "label": 1}
{"function": "\n\ndef on_query_context(self, view, key, operator, operand, match_all):\n\n    def test(a):\n        if (operator == sublime.OP_EQUAL):\n            return (a == operand)\n        if (operator == sublime.OP_NOT_EQUAL):\n            return (a != operand)\n        return False\n    if (key == 'i_search_active'):\n        return test((isearch_info_for(view) is not None))\n    if (key == 'sbp_has_visible_mark'):\n        if (not SettingsManager.get('sbp_cancel_mark_enabled', False)):\n            return False\n        return (CmdUtil(view).state.mark_ring.has_visible_mark() == operand)\n    if (key == 'sbp_use_alt_bindings'):\n        return test(SettingsManager.get('sbp_use_alt_bindings'))\n    if (key == 'sbp_use_super_bindings'):\n        return test(SettingsManager.get('sbp_use_super_bindings'))\n    if (key == 'sbp_alt+digit_inserts'):\n        return test((SettingsManager.get('sbp_alt+digit_inserts') or (not SettingsManager.get('sbp_use_alt_bindings'))))\n    if (key == 'sbp_has_prefix_argument'):\n        return test(CmdUtil(view).has_prefix_arg())\n", "label": 1}
{"function": "\n\ndef respond(self, message, user=None):\n    if ('brb' in message.lower()):\n        matches = re.findall(self.regex, message.lower())\n        if matches:\n            now = datetime.now()\n            due = (now + timedelta(minutes=int(matches[0])))\n            self.memory[user] = due\n    elif ('all: back' in message.lower()):\n        if (user in self.memory.keys()):\n            due = self.memory[user]\n            now = datetime.now()\n            if (now > due):\n                message = ((user + ': ') + 'You are late. :)')\n                self.speak(message)\n            self.memory.pop(user)\n    else:\n        pass\n", "label": 1}
{"function": "\n\ndef call_for_each_element(data, function, args=[], data_type='sequential'):\n    if (data_type == 'plain'):\n        return function(data, *args)\n    elif (data_type == 'sequential'):\n        assert list_of_lists(data)\n        return [function(d, *args) for d in data]\n    elif (data_type == 'token'):\n        assert (type(data) == dict)\n    return {token: function(contexts, *args) for (token, contexts) in data.items()}\n", "label": 1}
{"function": "\n\ndef headers_received(self, start_line, headers):\n    if (start_line.code != 101):\n        return super(WebSocketClientConnection, self).headers_received(start_line, headers)\n    self.headers = headers\n    assert (self.headers['Upgrade'].lower() == 'websocket')\n    assert (self.headers['Connection'].lower() == 'upgrade')\n    accept = WebSocketProtocol13.compute_accept_value(self.key)\n    assert (self.headers['Sec-Websocket-Accept'] == accept)\n    self.protocol = WebSocketProtocol13(self, mask_outgoing=True)\n    self.protocol._receive_frame()\n    if (self._timeout is not None):\n        self.io_loop.remove_timeout(self._timeout)\n        self._timeout = None\n    self.stream = self.connection.detach()\n    self.stream.set_close_callback(self.on_connection_close)\n    self.final_callback = None\n    self.connect_future.set_result(self)\n", "label": 1}
{"function": "\n\ndef get_recent(thing1=None, thing2=None):\n    ctx = context.get_context()\n    if (thing1 is None):\n        return [k for k in ctx.recent.keys()]\n    if (thing2 is None):\n        if isinstance(ctx.recent[thing1], (dict, cache.Cache)):\n            return dict([(repr(k), list(v)) for (k, v) in ctx.recent[thing1].items()])\n        else:\n            return list(ctx.recent[thing1])\n    elif isinstance(ctx.recent[thing1], (dict, cache.Cache)):\n        return dict([(repr(k), list(v)) for (k, v) in ctx.recent[thing1].items() if (thing2 in str(k))])\n    else:\n        return (('{\"error\":\"' + thing1) + ' recent data is not dict.\"}')\n", "label": 1}
{"function": "\n\n@classmethod\n@postonly\n@multiplayer_service\n@jsonify\ndef client_leave(cls):\n    remote_addr = get_remote_addr(request)\n    params = request.params\n    try:\n        session_id = params['session']\n        player_id = params['client']\n        hmac = params['hmac']\n    except KeyError:\n        response.status_int = 400\n        return {\n            'ok': False,\n            'msg': 'Missing session information.',\n        }\n    calculated_hmac = _calculate_client_hmac(cls.secret, remote_addr, session_id, player_id)\n    if (hmac != calculated_hmac):\n        response.status_int = 400\n        return {\n            'ok': False,\n            'msg': 'Invalid server information.',\n        }\n    sessions = cls.sessions\n    try:\n        session = sessions[session_id]\n    except KeyError:\n        response.status_int = 404\n        return {\n            'ok': False,\n            'msg': 'Unknown session.',\n        }\n    with cls.lock:\n        if session.has_player(player_id):\n            request_ip = get_remote_addr(request)\n            stored_ip = session.get_player_ip(player_id)\n            if ((stored_ip is not None) and (request_ip != stored_ip)):\n                response.status_int = 401\n                return {\n                    'ok': False,\n                }\n            LOG.info('Player %s left session %s', player_id, session_id)\n            session.remove_player(player_id)\n            cls._clean_empty_sessions()\n    return {\n        'ok': True,\n    }\n", "label": 1}
{"function": "\n\ndef read(self, expressions):\n    data = {\n        \n    }\n    for i in expressions.split('|'):\n        (name, type) = i.split(':')\n        if (type == 'byte'):\n            data[name] = self.read_byte()\n        if (type == 'ubyte'):\n            data[name] = self.read_ubyte()\n        if (type == 'short'):\n            data[name] = self.read_short()\n        if (type == 'ushort'):\n            data[name] = self.read_ushort()\n        if (type == 'double'):\n            data[name] = self.read_double()\n        if (type == 'position'):\n            data[name] = self.read_position()\n    return data\n", "label": 1}
{"function": "\n\ndef _var_beta_panel(y, x, beta, xx, rmse, cluster_axis, nw_lags, nobs, df, nw_overlap):\n    xx_inv = math.inv(xx)\n    yv = y.values\n    if (cluster_axis is None):\n        if (nw_lags is None):\n            return (xx_inv * (rmse ** 2))\n        else:\n            resid = (yv - np.dot(x.values, beta))\n            m = (x.values.T * resid).T\n            xeps = math.newey_west(m, nw_lags, nobs, df, nw_overlap)\n            return np.dot(xx_inv, np.dot(xeps, xx_inv))\n    else:\n        Xb = np.dot(x.values, beta).reshape((len(x.values), 1))\n        resid = DataFrame((yv[:, None] - Xb), index=y.index, columns=['resid'])\n        if (cluster_axis == 1):\n            x = x.swaplevel(0, 1).sortlevel(0)\n            resid = resid.swaplevel(0, 1).sortlevel(0)\n        m = _group_agg((x.values * resid.values), x.index._bounds, (lambda x: np.sum(x, axis=0)))\n        if (nw_lags is None):\n            nw_lags = 0\n        xox = 0\n        for i in range(len(x.index.levels[0])):\n            xox += math.newey_west(m[i:(i + 1)], nw_lags, nobs, df, nw_overlap)\n        return np.dot(xx_inv, np.dot(xox, xx_inv))\n", "label": 1}
{"function": "\n\ndef test_listen_targets_per_subclass(self):\n    'test that listen() called on a subclass remains specific to\\n        that subclass.'\n    canary = []\n\n    def listen_one(*args):\n        canary.append('listen_one')\n\n    def listen_two(*args):\n        canary.append('listen_two')\n\n    def listen_three(*args):\n        canary.append('listen_three')\n    event.listen(pool.Pool, 'connect', listen_one)\n    event.listen(pool.QueuePool, 'connect', listen_two)\n    event.listen(pool.SingletonThreadPool, 'connect', listen_three)\n    p1 = pool.QueuePool(creator=MockDBAPI().connect)\n    p2 = pool.SingletonThreadPool(creator=MockDBAPI().connect)\n    assert (listen_one in p1.dispatch.connect)\n    assert (listen_two in p1.dispatch.connect)\n    assert (listen_three not in p1.dispatch.connect)\n    assert (listen_one in p2.dispatch.connect)\n    assert (listen_two not in p2.dispatch.connect)\n    assert (listen_three in p2.dispatch.connect)\n    p1.connect()\n    eq_(canary, ['listen_one', 'listen_two'])\n    p2.connect()\n    eq_(canary, ['listen_one', 'listen_two', 'listen_one', 'listen_three'])\n", "label": 1}
{"function": "\n\ndef parse(self, data):\n    new = []\n    for part in data.split(self.sep):\n        if (part == '*'):\n            new.append((self.MIN, self.MAX))\n            break\n        tmplist = []\n        for subp in part.split(self.rng):\n            s = subp.strip()\n            tmplist.append(int(s))\n        if (len(tmplist) == 1):\n            new.append((tmplist[0], tmplist[0]))\n        elif ((len(tmplist) == 2) and (tmplist[0] <= tmplist[1])):\n            new.append((tmplist[0], tmplist[1]))\n        else:\n            raise ValueError(('Unable to parse: %r' % (data,)))\n    self.pairs.extend(new)\n    self.normalize()\n", "label": 1}
{"function": "\n\ndef __init__(self, module, module_filename=None, template=None, template_filename=None, module_source=None, template_source=None, output_encoding=None, encoding_errors='strict', disable_unicode=False, bytestring_passthrough=False, format_exceptions=False, error_handler=None, lookup=None, cache_args=None, cache_impl='beaker', cache_enabled=True, cache_type=None, cache_dir=None, cache_url=None):\n    self.module_id = re.sub('\\\\W', '_', module._template_uri)\n    self.uri = module._template_uri\n    self.input_encoding = module._source_encoding\n    self.output_encoding = output_encoding\n    self.encoding_errors = encoding_errors\n    self.disable_unicode = disable_unicode\n    self.bytestring_passthrough = (bytestring_passthrough or disable_unicode)\n    self.enable_loop = module._enable_loop\n    if (compat.py3k and disable_unicode):\n        raise exceptions.UnsupportedError('Mako for Python 3 does not support disabling Unicode')\n    elif (output_encoding and disable_unicode):\n        raise exceptions.UnsupportedError('output_encoding must be set to None when disable_unicode is used.')\n    self.module = module\n    self.filename = template_filename\n    ModuleInfo(module, module_filename, self, template_filename, module_source, template_source)\n    self.callable_ = self.module.render_body\n    self.format_exceptions = format_exceptions\n    self.error_handler = error_handler\n    self.lookup = lookup\n    self._setup_cache_args(cache_impl, cache_enabled, cache_args, cache_type, cache_dir, cache_url)\n", "label": 1}
{"function": "\n\ndef getexecutable(name, cache={\n    \n}):\n    try:\n        return cache[name]\n    except KeyError:\n        executable = py.path.local.sysfind(name)\n        if executable:\n            if (name == 'jython'):\n                import subprocess\n                popen = subprocess.Popen([str(executable), '--version'], universal_newlines=True, stderr=subprocess.PIPE)\n                (out, err) = popen.communicate()\n                if ((not err) or ('2.5' not in err)):\n                    executable = None\n                if ('2.5.2' in err):\n                    executable = None\n        cache[name] = executable\n        return executable\n", "label": 1}
{"function": "\n\ndef update(self, keys):\n    '\\n        Update arrow position.\\n        '\n    if self.allow_input:\n        if (keys[pg.K_DOWN] and (not keys[pg.K_UP]) and (self.index == 0)):\n            self.index = 1\n            self.allow_input = False\n            self.notify(c.CLICK)\n        elif (keys[pg.K_UP] and (not keys[pg.K_DOWN]) and (self.index == 1)):\n            self.index = 0\n            self.allow_input = False\n            self.notify(c.CLICK)\n        self.rect.y = self.pos_list[self.index]\n    if ((not keys[pg.K_DOWN]) and (not keys[pg.K_UP])):\n        self.allow_input = True\n", "label": 1}
{"function": "\n\ndef post(self, message, card=None, color=None, notify=False):\n    params = {\n        'notify': notify,\n    }\n    if self.author:\n        params['from'] = self.author[:25]\n    if (notify and self.targets):\n        message = ((self.targets + ': ') + message)\n    if card:\n        message += '\\n[card attached]'\n    params['message'] = message\n    params['message_format'] = 'text'\n    if card:\n        card['id'] = uuid.uuid4().urn\n        params['card'] = card\n    if color:\n        params['color'] = color\n    headers = {\n        'Authorization': ('Bearer ' + self.token),\n    }\n    r = requests.post('https://api.hipchat.com/v2/room/{0}/notification'.format(self.room_id), json=params, headers=headers)\n    r.raise_for_status()\n", "label": 1}
{"function": "\n\ndef convert_mode(in_text, str_len=0):\n    arduino_settings = settings.get_arduino_settings()\n    text = ''\n    display_mode = arduino_settings.get('display_mode', 'Text')\n    if (display_mode == 'Ascii'):\n        for character in in_text:\n            text += chr(character)\n    elif (display_mode == 'Hex'):\n        for (index, character) in enumerate(in_text):\n            text += ('%02X ' % character)\n            if ((((index + str_len) + 1) % 8) == 0):\n                text += '\\t'\n            if ((((index + str_len) + 1) % 16) == 0):\n                text += '\\n'\n    else:\n        text = in_text.decode('utf-8', 'replace')\n    return text\n", "label": 1}
{"function": "\n\ndef find_consecutive_slots(self, num_consecutive):\n    if (num_consecutive == 1):\n        return self.find_slot()\n    for i in range(len(self._data)):\n        any_taken = False\n        for k in range(num_consecutive):\n            if self._data[(i + k)]:\n                any_taken = True\n                break\n        if (not any_taken):\n            return i\n    return (- 1)\n", "label": 1}
{"function": "\n\ndef heuristic_search(graph, start, goal, heuristic):\n    '\\n    A* search algorithm.\\n    \\n    A set of heuristics is available under C{graph.algorithms.heuristics}. User-created heuristics\\n    are allowed too.\\n    \\n    @type graph: graph, digraph\\n    @param graph: Graph\\n    \\n    @type start: node\\n    @param start: Start node\\n    \\n    @type goal: node\\n    @param goal: Goal node\\n    \\n    @type heuristic: function\\n    @param heuristic: Heuristic function\\n    \\n    @rtype: list\\n    @return: Optimized path from start to goal node \\n    '\n    queue = [(0, start, 0, None)]\n    g = {\n        \n    }\n    explored = {\n        \n    }\n    while queue:\n        (_, current, dist, parent) = heappop(queue)\n        if (current == goal):\n            path = ([current] + [n for n in _reconstruct_path(parent, explored)])\n            path.reverse()\n            return path\n        if (current in explored):\n            continue\n        explored[current] = parent\n        for neighbor in graph[current]:\n            if (neighbor in explored):\n                continue\n            ncost = (dist + graph.edge_weight((current, neighbor)))\n            if (neighbor in g):\n                (qcost, h) = g[neighbor]\n                if (qcost <= ncost):\n                    continue\n            else:\n                h = heuristic(neighbor, goal)\n            g[neighbor] = (ncost, h)\n            heappush(queue, ((ncost + h), neighbor, ncost, current))\n    raise NodeUnreachable(start, goal)\n", "label": 1}
{"function": "\n\ndef parse_test(self, node):\n    token = next(self.stream)\n    if self.stream.current.test('name:not'):\n        next(self.stream)\n        negated = True\n    else:\n        negated = False\n    name = self.stream.expect('name').value\n    while (self.stream.current.type == 'dot'):\n        next(self.stream)\n        name += ('.' + self.stream.expect('name').value)\n    dyn_args = dyn_kwargs = None\n    kwargs = []\n    if (self.stream.current.type == 'lparen'):\n        (args, kwargs, dyn_args, dyn_kwargs) = self.parse_call(None)\n    elif ((self.stream.current.type in ('name', 'string', 'integer', 'float', 'lparen', 'lbracket', 'lbrace')) and (not self.stream.current.test_any('name:else', 'name:or', 'name:and'))):\n        if self.stream.current.test('name:is'):\n            self.fail('You cannot chain multiple tests with is')\n        args = [self.parse_expression()]\n    else:\n        args = []\n    node = nodes.Test(node, name, args, kwargs, dyn_args, dyn_kwargs, lineno=token.lineno)\n    if negated:\n        node = nodes.Not(node, lineno=token.lineno)\n    return node\n", "label": 1}
{"function": "\n\ndef delete_peer(self, addr):\n    if _debug:\n        BIPBBMD._debug('delete_peer %r', addr)\n    if isinstance(addr, Address):\n        pass\n    elif isinstance(addr, str):\n        addr = LocalStation(addr)\n    else:\n        raise TypeError('addr must be a string or an Address')\n    for i in range((len(self.bbmdBDT) - 1), (- 1), (- 1)):\n        if (addr == self.bbmdBDT[i]):\n            del self.bbmdBDT[i]\n            break\n    else:\n        pass\n", "label": 1}
{"function": "\n\ndef rectangle(self, rect=None, duration=1.0, block=True, color=(1, 1, 1, 1), parent=None, depth=0):\n    'Draw a single-colored rectangle.'\n    if (duration == 0):\n        block = False\n    l = rect[0]\n    r = rect[1]\n    t = rect[2]\n    b = rect[3]\n    obj = self._engine.direct.gui.OnscreenImage.OnscreenImage(image='blank.tga', pos=(((l + r) / 2), depth, ((b + t) / 2)), scale=(((r - l) / 2), 1, ((b - t) / 2)), color=color, parent=parent)\n    self._to_destroy.append(obj)\n    obj.setTransparency(self._engine.pandac.TransparencyAttrib.MAlpha)\n    if self.implicit_markers:\n        self.marker(250)\n    if block:\n        if ((type(duration) == list) or (type(duration) == tuple)):\n            self.sleep(duration[0])\n            self.waitfor(duration[1])\n        elif (type(duration) == str):\n            self.waitfor(duration)\n        else:\n            self.sleep(duration)\n        self._destroy_object(obj, 251)\n    else:\n        if (duration > 0):\n            self._engine.base.taskMgr.doMethodLater(duration, self._destroy_object, 'ConvenienceFunctions, remove_rect', extraArgs=[obj, 251])\n        return obj\n", "label": 1}
{"function": "\n\ndef add_type_view(self, request, form_url=''):\n    '\\n        Display a choice form to select which page type to add.\\n        '\n    if (not self.has_add_permission(request)):\n        raise PermissionDenied\n    extra_qs = ''\n    if request.META['QUERY_STRING']:\n        extra_qs = ('&' + request.META['QUERY_STRING'])\n    choices = self.get_child_type_choices(request, 'add')\n    if (len(choices) == 1):\n        return HttpResponseRedirect('?ct_id={0}{1}'.format(choices[0][0], extra_qs))\n    form = self.add_type_form(data=(request.POST if (request.method == 'POST') else None), initial={\n        'ct_id': choices[0][0],\n    })\n    form.fields['ct_id'].choices = choices\n    if form.is_valid():\n        return HttpResponseRedirect('?ct_id={0}{1}'.format(form.cleaned_data['ct_id'], extra_qs))\n    fieldsets = ((None, {\n        'fields': ('ct_id',),\n    }),)\n    adminForm = AdminForm(form, fieldsets, {\n        \n    }, model_admin=self)\n    media = (self.media + adminForm.media)\n    opts = self.model._meta\n    context = {\n        'title': (_('Add %s') % force_text(opts.verbose_name)),\n        'adminform': adminForm,\n        'is_popup': (('_popup' in request.POST) or ('_popup' in request.GET)),\n        'media': mark_safe(media),\n        'errors': AdminErrorList(form, ()),\n        'app_label': opts.app_label,\n    }\n    return self.render_add_type_form(request, context, form_url)\n", "label": 1}
{"function": "\n\ndef mod_pi2(man, exp, mag, wp):\n    if (mag > 0):\n        i = 0\n        while 1:\n            cancellation_prec = (20 << i)\n            wpmod = ((wp + mag) + cancellation_prec)\n            pi2 = pi_fixed((wpmod - 1))\n            pi4 = (pi2 >> 1)\n            offset = (wpmod + exp)\n            if (offset >= 0):\n                t = (man << offset)\n            else:\n                t = (man >> (- offset))\n            (n, y) = divmod(t, pi2)\n            if (y > pi4):\n                small = (pi2 - y)\n            else:\n                small = y\n            if (small >> ((wp + mag) - 10)):\n                n = int(n)\n                t = (y >> mag)\n                wp = (wpmod - mag)\n                break\n            i += 1\n    else:\n        wp += (- mag)\n        offset = (exp + wp)\n        if (offset >= 0):\n            t = (man << offset)\n        else:\n            t = (man >> (- offset))\n        n = 0\n    return (t, n, wp)\n", "label": 1}
{"function": "\n\ndef get_field_value(self, field_name, field_type, value):\n    if (value is None):\n        return None\n    values_map = self._import_config.get_value_mapping(field_name)\n    if isinstance(value, list):\n        return [self.get_field_value(field_name, field_type, v) for v in value]\n    if field_type.startswith('user'):\n        return self._to_yt_user(value)\n    if (field_type.lower() == 'date'):\n        return self.to_unix_date(value)\n    if isinstance(value, basestring):\n        return values_map.get(value, value)\n    if isinstance(value, int):\n        return values_map.get(value, str(value))\n", "label": 1}
{"function": "\n\ndef test_postgres_search_path_parsing(self):\n    url = 'postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn?currentSchema=otherschema'\n    url = dj_database_url.parse(url)\n    assert (url['ENGINE'] == 'django.db.backends.postgresql_psycopg2')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 5431)\n    assert (url['OPTIONS']['options'] == '-c search_path=otherschema')\n", "label": 1}
{"function": "\n\n@fails('aminator.provisioner.provision_scripts.error')\n@lapse('aminator.provisioner.provision_scripts.duration')\ndef _run_provision_scripts(self, scripts_dir):\n    '\\n        execute every python or shell script found in scripts_dir\\n            1. run python or shell scripts in lexical order\\n\\n        :param scripts_dir: path in chroot to look for python and shell scripts\\n        :return: None\\n        '\n    script_files = sorted((glob((scripts_dir + '/*.py')) + glob((scripts_dir + '/*.sh'))))\n    if (not script_files):\n        log.debug('no python or shell scripts found in {0}'.format(scripts_dir))\n    else:\n        log.debug('found scripts {0} in {1}'.format(script_files, scripts_dir))\n        for script in script_files:\n            log.debug('executing script {0}'.format(script))\n            if os.access(script, os.X_OK):\n                result = run_script(script)\n            elif script.endswith('.py'):\n                result = run_script(['python', script])\n            else:\n                result = run_script(['sh', script])\n            if (not result.success):\n                log.critical('script failed: {0}: {1.std_err}'.format(script, result.result))\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef _get_test_labels(test_modules):\n    test_labels = []\n    for test_module in test_modules:\n        for module in [name for (_, name, _) in pkgutil.iter_modules([os.path.join(test_module, 'tests')])]:\n            clsmembers = pyclbr.readmodule(('%s.tests.%s' % (test_module, module)))\n            for (clsname, cls) in clsmembers.items():\n                for (method, _) in cls.methods.items():\n                    if method.startswith('test_'):\n                        test_labels.append(('%s.%s.%s' % (test_module, clsname, method)))\n    return test_labels\n", "label": 1}
{"function": "\n\ndef _autodiscover(self):\n    'Discovers modules to register from ``settings.INSTALLED_APPS``.\\n\\n        This makes sure that the appropriate modules get imported to register\\n        themselves with Horizon.\\n        '\n    if (not getattr(self, '_registerable_class', None)):\n        raise ImproperlyConfigured('You must set a \"_registerable_class\" property in order to use autodiscovery.')\n    for mod_name in ('dashboard', 'panel'):\n        for app in settings.INSTALLED_APPS:\n            mod = import_module(app)\n            try:\n                before_import_registry = copy.copy(self._registry)\n                import_module(('%s.%s' % (app, mod_name)))\n            except Exception:\n                self._registry = before_import_registry\n                if module_has_submodule(mod, mod_name):\n                    raise\n", "label": 1}
{"function": "\n\ndef _get_rows(self, options):\n    'Return only those data rows that should be printed, based on slicing and sorting.\\n\\n        Arguments:\\n\\n        options - dictionary of option settings.'\n    if options['oldsortslice']:\n        rows = copy.deepcopy(self._rows[options['start']:options['end']])\n    else:\n        rows = copy.deepcopy(self._rows)\n    if options['sortby']:\n        sortindex = self._field_names.index(options['sortby'])\n        rows = [([row[sortindex]] + row) for row in rows]\n        rows.sort(reverse=options['reversesort'], key=options['sort_key'])\n        rows = [row[1:] for row in rows]\n    if (not options['oldsortslice']):\n        rows = rows[options['start']:options['end']]\n    return rows\n", "label": 1}
{"function": "\n\ndef AddAll(self, rdf_values, callback=None):\n    'Adds a list of rdfvalues to the collection.'\n    for rdf_value in rdf_values:\n        if (rdf_value is None):\n            raise ValueError(\"Can't add None to the collection via AddAll.\")\n        if (self._rdf_type and (not isinstance(rdf_value, self._rdf_type))):\n            raise ValueError(('This collection only accepts values of type %s' % self._rdf_type.__name__))\n        if (not rdf_value.age):\n            rdf_value.age.Now()\n    buf = cStringIO.StringIO()\n    for (index, rdf_value) in enumerate(rdf_values):\n        data = rdf_protodict.EmbeddedRDFValue(payload=rdf_value).SerializeToString()\n        buf.write(struct.pack('<i', len(data)))\n        buf.write(data)\n        self.size += 1\n        if callback:\n            callback(index, rdf_value)\n    self.fd.Seek(0, 2)\n    self.fd.Write(buf.getvalue())\n    self.stream_dirty = True\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef get_permissions(doctype=None, role=None):\n    frappe.only_for('System Manager')\n    out = frappe.db.sql(('select * from tabDocPerm\\n\\t\\twhere %s%s order by parent, permlevel, role' % (((doctype and (\" parent='%s'\" % frappe.db.escape(doctype))) or ''), ((role and (((doctype and ' and ') or '') + (\" role='%s'\" % frappe.db.escape(role)))) or ''))), as_dict=True)\n    linked_doctypes = {\n        \n    }\n    for d in out:\n        d.linked_doctypes = linked_doctypes.setdefault(d.parent, get_linked_doctypes(d.parent))\n    return out\n", "label": 1}
{"function": "\n\ndef _fix_slicing_order(self, outer_fields, inner_select, order, inner_table_name):\n    '\\n        Apply any necessary fixes to the outer_fields, inner_select, and order \\n        strings due to slicing.\\n        '\n    if (order is None):\n        meta = self.query.get_meta()\n        column = (meta.pk.db_column or meta.pk.get_attname())\n        order = '{0}.{1} ASC'.format(inner_table_name, self.connection.ops.quote_name(column))\n    else:\n        alias_id = 0\n        new_order = []\n        for x in order.split(','):\n            m = _re_find_order_direction.search(x)\n            if m:\n                direction = m.groups()[0]\n            else:\n                direction = 'ASC'\n            x = _re_find_order_direction.sub('', x)\n            col = x.rsplit('.', 1)[(- 1)]\n            if (x not in inner_select):\n                alias_id += 1\n                col = '{left_sql_quote}{0}___o{1}{right_sql_quote}'.format(col.strip((self.connection.ops.left_sql_quote + self.connection.ops.right_sql_quote)), alias_id, left_sql_quote=self.connection.ops.left_sql_quote, right_sql_quote=self.connection.ops.right_sql_quote)\n                inner_select = '({0}) AS {1}, {2}'.format(x, col, inner_select)\n            new_order.append('{0}.{1} {2}'.format(inner_table_name, col, direction))\n        order = ', '.join(new_order)\n    return (outer_fields, inner_select, order)\n", "label": 1}
{"function": "\n\n@then('the tag expression selects model elements with')\ndef step_given_named_model_elements_with_tags(context):\n    '\\n    .. code-block:: gherkin\\n\\n        Then the tag expression select model elements with:\\n            | tag expression | selected?    |\\n            |  @foo          | S1, S3       |\\n            | -@foo          | S0, S2, S3   |\\n    '\n    assert context.model_elements, 'REQUIRE: context attribute'\n    assert context.table, 'REQUIRE: context.table'\n    context.table.require_columns(['tag expression', 'selected?'])\n    for (row_index, row) in enumerate(context.table.rows):\n        tag_expression_text = row['tag expression']\n        tag_expression = convert_tag_expression(tag_expression_text)\n        expected_selected_names = convert_comma_list(row['selected?'])\n        actual_selected = []\n        for model_element in context.model_elements:\n            if tag_expression.check(model_element.tags):\n                actual_selected.append(model_element.name)\n        assert_that(actual_selected, equal_to(expected_selected_names), ('tag_expression=%s (row=%s)' % (tag_expression_text, row_index)))\n", "label": 1}
{"function": "\n\ndef strategy(self, opponent):\n    '\\n        Check whether the number of cooperations in the first and second halves\\n        of the history are close. The variance of the uniform distribution (1/4)\\n        is a reasonable delta but use something lower for certainty and avoiding\\n        false positives. This approach will also detect a lot of random players.\\n        '\n    n = len(self.history)\n    if ((n >= 8) and opponent.cooperations and opponent.defections):\n        (start1, end1) = (0, (n // 2))\n        (start2, end2) = ((n // 4), ((3 * n) // 4))\n        (start3, end3) = ((n // 2), n)\n        count1 = (opponent.history[start1:end1].count(C) + self.history[start1:end1].count(C))\n        count2 = (opponent.history[start2:end2].count(C) + self.history[start2:end2].count(C))\n        count3 = (opponent.history[start3:end3].count(C) + self.history[start3:end3].count(C))\n        ratio1 = ((0.5 * count1) / (end1 - start1))\n        ratio2 = ((0.5 * count2) / (end2 - start2))\n        ratio3 = ((0.5 * count3) / (end3 - start3))\n        if ((abs((ratio1 - ratio2)) < 0.2) and (abs((ratio1 - ratio3)) < 0.2)):\n            return D\n    return C\n", "label": 1}
{"function": "\n\ndef inet_ntop(af, packed_ip):\n    'Convert an packed IP address of the given family to string format.'\n    if (af == AF_INET):\n        return inet_ntoa(packed_ip)\n    elif (af == AF_INET6):\n        if ((len(packed_ip) != 16) or (not _is_str(packed_ip))):\n            raise ValueError('invalid length of packed IP address string')\n        tokens = [('%x' % i) for i in _unpack('>8H', packed_ip)]\n        words = list(_unpack('>8H', packed_ip))\n        int_val = 0\n        for (i, num) in enumerate(reversed(words)):\n            word = num\n            word = (word << (16 * i))\n            int_val = (int_val | word)\n        if ((65535 < int_val <= 4294967295) or ((int_val >> 32) == 65535)):\n            packed_ipv4 = _pack('>2H', *[int(i, 16) for i in tokens[(- 2):]])\n            ipv4_str = inet_ntoa(packed_ipv4)\n            tokens = (tokens[0:(- 2)] + [ipv4_str])\n        return ':'.join(_compact_ipv6_tokens(tokens))\n    else:\n        raise ValueError(('unknown address family %d' % af))\n", "label": 1}
{"function": "\n\ndef _setupTypeInfo(self, test, maskPattern):\n    data = ((self.errorCorrectLevel << 3) | maskPattern)\n    bits = QRUtil.getBCHTypeInfo(data)\n    for i in range(15):\n        mod = ((not test) and (((bits >> i) & 1) == 1))\n        if (i < 6):\n            self.modules[i][8] = mod\n        elif (i < 8):\n            self.modules[(i + 1)][8] = mod\n        else:\n            self.modules[((self.moduleCount - 15) + i)][8] = mod\n    for i in range(15):\n        mod = ((not test) and (((bits >> i) & 1) == 1))\n        if (i < 8):\n            self.modules[8][((self.moduleCount - i) - 1)] = mod\n        elif (i < 9):\n            self.modules[8][(((15 - i) - 1) + 1)] = mod\n        else:\n            self.modules[8][((15 - i) - 1)] = mod\n    self.modules[(self.moduleCount - 8)][8] = (not test)\n", "label": 1}
{"function": "\n\ndef submit(obj, username=None, email=None, password=None, formid=None, tos=False):\n    '\\n    Submit form for register new user. Note - if you have captcha\\n    in the form, you need to mock it.\\n    '\n    formid = (formid or 1)\n    obj.formvalue(formid, 'username', (username or USERNAME))\n    obj.formvalue(formid, 'email', (email or EMAIL))\n    obj.formvalue(formid, 'password1', (password or PASSWORD))\n    obj.formvalue(formid, 'password2', (password or PASSWORD))\n    if tos:\n        obj.formvalue(formid, 'tos', 'on')\n    obj.submit200()\n", "label": 1}
{"function": "\n\ndef rob(self, nums):\n    '\\n        Two cases: cannot touch 1st element vs. cannot touch 2nd element.\\n        There are two cases here 1) 1st element is included and last is not included 2) 1st is not included and last is\\n        included.\\n        :type nums: list\\n        :rtype: int\\n        '\n    n = len(nums)\n    if (n < 2):\n        return sum(nums)\n    dp = [0 for _ in xrange(((n - 1) + 2))]\n    for i in xrange(2, (n + 1)):\n        dp[i] = max(dp[(i - 1)], (dp[(i - 2)] + nums[(i - 2)]))\n    ret = dp[(- 1)]\n    dp = [0 for _ in xrange(((n - 1) + 2))]\n    for i in xrange(2, (n + 1)):\n        dp[i] = max(dp[(i - 1)], (dp[(i - 2)] + nums[(i - 1)]))\n    ret = max(ret, dp[(- 1)])\n    return ret\n", "label": 1}
{"function": "\n\ndef get_module(os_mapping, dirpath):\n    mapping_config = ConfigParser.RawConfigParser()\n    mapping_config.readfp(StringIO.StringIO(os_mapping))\n    opts = config.controller\n    dist = opts['distribution_name']\n    ver = opts['distribution_version']\n    combinations = ((dist, ver), (dist, 'default'), ('default', 'default'))\n    mod_name = None\n    for comb in combinations:\n        mod_name = get_module_name(mapping_config, comb[0], comb[1])\n        if mod_name:\n            break\n    if ((mod_name != 'default') and (mod_name is not None)):\n        (fp, path, desc) = imp.find_module(mod_name, [dirpath])\n        return imp.load_module(mod_name, fp, path, desc)\n    elif (mod_name == 'default'):\n        return None\n", "label": 1}
{"function": "\n\ndef sync_ldap_groups(self, ldap_groups):\n    '\\n        Synchronize LDAP groups with local group database.\\n        '\n    attributes = getattr(settings, 'LDAP_SYNC_GROUP_ATTRIBUTES', None)\n    groupname_field = 'name'\n    if (groupname_field not in attributes.values()):\n        error_msg = (\"LDAP_SYNC_GROUP_ATTRIBUTES must contain the group name field '%s'\" % groupname_field)\n        raise ImproperlyConfigured(error_msg)\n    for (cname, attrs) in ldap_groups:\n        try:\n            items = attrs.items()\n        except AttributeError:\n            continue\n        group_attr = {\n            \n        }\n        for (name, attr) in items:\n            group_attr[attributes[name]] = attr[0].decode('utf-8')\n        try:\n            groupname = group_attr[groupname_field]\n            group_attr[groupname_field] = groupname.lower()\n        except KeyError:\n            logger.warning((\"Group is missing a required attribute '%s'\" % groupname_field))\n            continue\n        kwargs = {\n            (groupname_field + '__iexact'): groupname,\n            'defaults': group_attr,\n        }\n        try:\n            (group, created) = Group.objects.get_or_create(**kwargs)\n        except IntegrityError as e:\n            logger.error(('Error creating group %s' % e))\n        else:\n            if created:\n                logger.debug(('Created group %s' % groupname))\n    logger.info('Groups are synchronized')\n", "label": 1}
{"function": "\n\ndef test_apply(self):\n    '\\n        Tests marking migrations as applied/unapplied.\\n        '\n    recorder = MigrationRecorder(connection)\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), set())\n    recorder.record_applied('myapp', '0432_ponies')\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), {('myapp', '0432_ponies')})\n    recorder_other = MigrationRecorder(connections['other'])\n    self.assertEqual(set(((x, y) for (x, y) in recorder_other.applied_migrations() if (x == 'myapp'))), set())\n    recorder.record_unapplied('myapp', '0432_ponies')\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), set())\n", "label": 1}
{"function": "\n\ndef get_table_list(self, cursor):\n    'Returns a list of table names in the current database and schema.'\n    cursor.execute((\"\\n            SELECT c.relname, c.relkind\\n            FROM pg_catalog.pg_class c\\n            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\\n            WHERE c.relkind IN ('r', 'v', '')\\n                AND n.nspname = '%s'\\n                AND pg_catalog.pg_table_is_visible(c.oid)\" % self.connection.schema_name))\n    if (django.VERSION >= (1, 8, 0)):\n        return [TableInfo(row[0], {\n            'r': 't',\n            'v': 'v',\n        }.get(row[1])) for row in cursor.fetchall() if (row[0] not in self.ignored_tables)]\n    else:\n        return [row[0] for row in cursor.fetchall() if (row[0] not in self.ignored_tables)]\n", "label": 1}
{"function": "\n\ndef _camel_case_to_underscores(self, text):\n    result = []\n    pos = 0\n    while (pos < len(text)):\n        if text[pos].isupper():\n            if ((((pos - 1) > 0) and text[(pos - 1)].islower()) or (((pos - 1) > 0) and ((pos + 1) < len(text)) and text[(pos + 1)].islower())):\n                result.append(('_%s' % text[pos].lower()))\n            else:\n                result.append(text[pos].lower())\n        else:\n            result.append(text[pos])\n        pos += 1\n    return ''.join(result)\n", "label": 1}
{"function": "\n\ndef test_process_commit():\n    (m, ctl, config) = init()\n    config['numprocesses'] = 0\n    m.load(config, start=False)\n    cmd = TestCommand('commit', ['dummy'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    state = m._get_locked_state('dummy')\n    assert (len(state.running) == 0)\n    assert (state.numprocesses == 0)\n    assert (len(state.running_out) == 1)\n    assert (m.pids() == [1])\n    m.stop()\n    m.run()\n    assert (cmd.result['pid'] == 1)\n", "label": 1}
{"function": "\n\ndef bind_params(self, I, E, O, alpha):\n    assert (I.dtype == E.dtype)\n    if (not self.initialized):\n        self.initialized = True\n        self.autotune(I, E, O)\n    if ((O.dtype.type is not np.float32) or self.determ_size):\n        updat_temp = self.lib.scratch_buffer(self.output_size)\n        image_temp = self.lib.scratch_buffer_offset(self.image_size)\n        delta_temp = self.lib.scratch_buffer_offset(self.delta_size)\n        self.convert_args = [updat_temp, 'f4', O, self.determ_shape]\n    else:\n        updat_temp = O.gpudata\n        image_temp = self.lib.scratch_buffer(self.image_size)\n        delta_temp = self.lib.scratch_buffer_offset(self.delta_size)\n        self.convert_args = False\n    self.image_args[2:5] = (self.lib.stream, image_temp, I.gpudata)\n    self.delta_args[2:5] = (self.lib.stream, delta_temp, E.gpudata)\n    if self.zero:\n        self.zero_args = [updat_temp, 0, O.size, self.lib.stream]\n    self.kernel[3:8] = (self.lib.stream, updat_temp, image_temp, delta_temp, alpha)\n", "label": 1}
{"function": "\n\ndef validate(self, object, name, value):\n    ' Validates that the value is a valid font descriptor string.\\n        '\n    try:\n        point_size = family = style = weight = underline = ''\n        facename = ['']\n        for word in value.split():\n            lword = word.lower()\n            if (lword in font_families):\n                family = (' ' + lword)\n            elif (lword in font_styles):\n                style = (' ' + lword)\n            elif (lword in font_weights):\n                weight = (' ' + lword)\n            elif (lword == 'underline'):\n                underline = (' ' + lword)\n            elif (lword not in font_noise):\n                try:\n                    int(lword)\n                    point_size = (lword + ' pt')\n                except:\n                    facename.append(word)\n        fontstr = ('%s%s%s%s%s%s' % (point_size, family, style, weight, underline, ' '.join(facename))).strip()\n        return fontstr\n    except Exception:\n        pass\n    raise TraitError(object, name, 'a font descriptor string', repr(value))\n", "label": 1}
{"function": "\n\ndef _update_no_improvement_count(self, early_stopping, X_val, y_val):\n    if early_stopping:\n        self.validation_scores_.append(self.score(X_val, y_val))\n        if self.verbose:\n            print(('Validation score: %f' % self.validation_scores_[(- 1)]))\n        last_valid_score = self.validation_scores_[(- 1)]\n        if (last_valid_score < (self.best_validation_score_ + self.tol)):\n            self._no_improvement_count += 1\n        else:\n            self._no_improvement_count = 0\n        if (last_valid_score > self.best_validation_score_):\n            self.best_validation_score_ = last_valid_score\n            self._best_coefs = [c.copy() for c in self.coefs_]\n            self._best_intercepts = [i.copy() for i in self.intercepts_]\n    else:\n        if (self.loss_curve_[(- 1)] > (self.best_loss_ - self.tol)):\n            self._no_improvement_count += 1\n        else:\n            self._no_improvement_count = 0\n        if (self.loss_curve_[(- 1)] < self.best_loss_):\n            self.best_loss_ = self.loss_curve_[(- 1)]\n", "label": 1}
{"function": "\n\ndef test_mixedset_cache_miss(self, backend, base_set, base_set2):\n    ms = op2.MixedSet([base_set, base_set2])\n    ms2 = op2.MixedSet([base_set2, base_set])\n    assert (ms is not ms2)\n    assert (ms != ms2)\n    assert (not (ms == ms2))\n    ms3 = op2.MixedSet([base_set, base_set2])\n    assert (ms is ms3)\n    assert (not (ms != ms3))\n    assert (ms == ms3)\n", "label": 1}
{"function": "\n\ndef shell_exec_monitor(self, Command, PluginInfo):\n    CommandInfo = self.StartCommand(Command, Command)\n    (Target, CanRun) = self.CanRunCommand(CommandInfo)\n    if (not CanRun):\n        Message = ('The command was already run for target: ' + str(Target))\n        return Message\n    logging.info('')\n    logging.info('Executing :\\n\\n%s\\n\\n', Command)\n    logging.info('')\n    logging.info(('------> Execution Start Date/Time: ' + self.timer.get_start_date_time_as_str('Command')))\n    logging.info('')\n    Output = ''\n    Cancelled = False\n    try:\n        proc = self.create_subprocess(Command)\n        while True:\n            line = proc.stdout.readline()\n            if (not line):\n                break\n            logging.warn(line.strip())\n            Output += line\n    except KeyboardInterrupt:\n        os.killpg(proc.pid, signal.SIGINT)\n        (outdata, errdata) = proc.communicate()\n        logging.warn(outdata)\n        Output += outdata\n        try:\n            os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n        except OSError:\n            pass\n        Cancelled = True\n        Output += self.error_handler.UserAbort('Command', Output)\n    finally:\n        self.FinishCommand(CommandInfo, Cancelled, PluginInfo)\n    return scrub_output(Output)\n", "label": 1}
{"function": "\n\ndef information(self, b, ties='breslow'):\n    info = 0\n    score = 0\n    for t in iterkeys(self.failures):\n        fail = self.failures[t]\n        d = len(fail)\n        risk = self.risk[t]\n        Z = self.design[t]\n        if (ties == 'breslow'):\n            w = np.exp(np.dot(Z, b))\n            rv = Discrete(Z[risk], w=w[risk])\n            info += rv.cov()\n        elif (ties == 'efron'):\n            w = np.exp(np.dot(Z, b))\n            score += Z[fail].sum()\n            for j in range(d):\n                efron_w = w\n                efron_w[fail] -= ((i * w[fail]) / d)\n                rv = Discrete(Z[risk], w=efron_w[risk])\n                info += rv.cov()\n        elif (ties == 'cox'):\n            raise NotImplementedError('Cox tie breaking method not implemented')\n        else:\n            raise NotImplementedError('tie breaking method not recognized')\n    return score\n", "label": 1}
{"function": "\n\ndef on_status(self, status):\n    if (status.in_reply_to_status_id == tweetID):\n        parsedNumbers = ''.join(status.text.split(' ')[1:]).replace(' ', '').replace('[', '').replace(']', '').replace('(', '').replace(')', '').split(',')\n        givenNumbers = list(map(int, parsedNumbers))\n        areSorted = True\n        if (len(givenNumbers) != len(numbers)):\n            areSorted = False\n        for n in givenNumbers:\n            if (givenNumbers.count(n) != numbers.count(n)):\n                areSorted = False\n                break\n        for i in range(len(givenNumbers)):\n            if (i > 0):\n                if (not (givenNumbers[i] >= givenNumbers[(i - 1)])):\n                    areSorted = False\n                    break\n        if areSorted:\n            print(givenNumbers)\n            api.update_status((('@' + status.author.screen_name) + ' Awesome! Thanks!'), in_reply_to_status_id=status.id)\n            return False\n        else:\n            api.update_status((('@' + status.author.screen_name) + \" Those numbers aren't sorted!\"), in_reply_to_status_id=status.id)\n            return True\n    else:\n        return True\n", "label": 1}
{"function": "\n\ndef _after_flush(app, changes):\n    bytype = {\n        \n    }\n    for change in changes:\n        update = (change[1] in ('update', 'insert'))\n        if hasattr(change[0].__class__, __searchable__):\n            bytype.setdefault(change[0].__class__.__name__, []).append((update, change[0]))\n    for (model, values) in bytype.iteritems():\n        index = whoosh_index(app, values[0][1].__class__)\n        with index.writer() as writer:\n            primary_field = values[0][1].pure_whoosh.primary_key_name\n            searchable = values[0][1].__searchable__\n            for (update, v) in values:\n                if update:\n                    attrs = {\n                        \n                    }\n                    for key in searchable:\n                        try:\n                            attrs[key] = unicode(getattr(v, key))\n                        except AttributeError:\n                            raise AttributeError('{0} does not have {1} field {2}'.format(model, __searchable__, key))\n                    attrs[primary_field] = unicode(getattr(v, primary_field))\n                    writer.update_document(**attrs)\n                else:\n                    writer.delete_by_term(primary_field, unicode(getattr(v, primary_field)))\n", "label": 1}
{"function": "\n\ndef update(self, key):\n    if ((self.plot.dynamic == 'bounded') and (not isinstance(key, int))):\n        key = tuple(((dim.values[k] if dim.values else k) for (dim, k) in zip(self.mock_obj.kdims, tuple(key))))\n    if (self.renderer.mode == 'nbagg'):\n        if (not self.manager._shown):\n            self.comm.start()\n            self.manager.add_web_socket(self.comm)\n            self.manager._shown = True\n        fig = self.plot[key]\n        fig.canvas.draw_idle()\n        return ''\n    frame = self._plot_figure(key)\n    if (self.renderer.mode == 'mpld3'):\n        frame = self.encode_frames({\n            0: frame,\n        })\n    return frame\n", "label": 1}
{"function": "\n\ndef _update_an_article(self, postid):\n    (afile, aname) = self.conf.get_article(postid, self.args.type)\n    if self.args.output:\n        self._write_html_file(afile)\n        return\n    (html, meta, txt, medias) = self._get_and_update_article_content(afile)\n    if (not html):\n        return\n    resultclass = WordPressPost\n    if (self.args.type == 'page'):\n        postid = meta.postid\n        resultclass = WordPressPage\n    post = self.wpcall(GetPost(postid, result_class=resultclass))\n    if (not post):\n        slog.warning(('No post \"%s\"!' % postid))\n        return\n    slog.info('Old article:')\n    self.print_results(post)\n    post.title = meta.title\n    post.user = meta.author\n    post.slug = meta.nicename\n    post.date = meta.date\n    post.content = html\n    post.post_status = meta.poststatus\n    if meta.modified:\n        post.date_modified = meta.modified\n    terms = self.cache.get_terms_from_meta(meta.category, meta.tags)\n    if terms:\n        post.terms = terms\n    elif (self.args.type == 'post'):\n        slog.warning('Please provide some terms.')\n        return\n    succ = self.wpcall(EditPost(postid, post))\n    if (succ == None):\n        return\n    if succ:\n        slog.info(('Update %s successfully!' % postid))\n    else:\n        slog.info(('Update %s fail!' % postid))\n", "label": 1}
{"function": "\n\ndef test_nested_block(self, space):\n    bc = self.assert_compiles(space, '\\n        sums = []\\n        [].each do |x|\\n            [].each do |y|\\n                sums << x + y\\n            end\\n        end\\n        ', '\\n        BUILD_ARRAY 0\\n        STORE_DEREF 0\\n        DISCARD_TOP\\n\\n        BUILD_ARRAY 0\\n        LOAD_CONST 0\\n        LOAD_CLOSURE 0\\n        BUILD_BLOCK 1\\n        SEND_BLOCK 1 1\\n\\n        RETURN\\n        ')\n    assert (bc.freevars == [])\n    assert (bc.cellvars == ['sums'])\n    self.assert_compiled(bc.consts_w[0], '\\n        BUILD_ARRAY 0\\n        LOAD_CONST 0\\n        LOAD_CLOSURE 0\\n        LOAD_CLOSURE 1\\n        BUILD_BLOCK 2\\n        SEND_BLOCK 1 1\\n        RETURN\\n        ')\n    assert (bc.consts_w[0].freevars == ['sums'])\n    assert (bc.consts_w[0].cellvars == ['x'])\n    self.assert_compiled(bc.consts_w[0].consts_w[0], '\\n        LOAD_DEREF 1\\n        LOAD_DEREF 2\\n        LOAD_DEREF 0\\n        SEND 0 1\\n        SEND 1 1\\n        RETURN\\n        ')\n    assert (bc.consts_w[0].consts_w[0].freevars == ['sums', 'x'])\n    assert (bc.consts_w[0].consts_w[0].cellvars == ['y'])\n", "label": 1}
{"function": "\n\ndef _filter_requirements(lines_iter, filter_names=None, filter_sys_version=False):\n    for line in lines_iter:\n        line = line.strip()\n        if ((not line) or line.startswith('#')):\n            continue\n        match = REQ_PATTERN.match(line)\n        if (match is None):\n            raise AssertionError((\"Could not parse requirement: '%s'\" % line))\n        name = match.group('name')\n        if ((filter_names is not None) and (name not in filter_names)):\n            continue\n        if (filter_sys_version and match.group('pyspec')):\n            (pycomp, pyspec) = match.group('pycomp', 'pyspec')\n            comp = STR_TO_CMP[pycomp]\n            pyver_spec = StrictVersion(pyspec)\n            if comp(SYS_VERSION, pyver_spec):\n                (yield line.split(';')[0])\n            continue\n        (yield line)\n", "label": 1}
{"function": "\n\n@classmethod\ndef validate(cls, data):\n    try:\n        su.schema_validate(data, cls.SCHEMA)\n    except su.ValidationError as e:\n        cls_name = reflection.get_class_name(cls, fully_qualified=False)\n        excp.raise_with_cause(excp.InvalidFormat, ('%s message response data not of the expected format: %s' % (cls_name, e.message)), cause=e)\n    else:\n        failures = []\n        if ('failures' in data):\n            failures.extend(six.itervalues(data['failures']))\n        result = data.get('result')\n        if (result is not None):\n            (result_data_type, result_data) = result\n            if (result_data_type == 'failure'):\n                failures.append(result_data)\n        for fail_data in failures:\n            ft.Failure.validate(fail_data)\n", "label": 1}
{"function": "\n\ndef set(self, name, value, retries=3, wait_ready=False):\n    if wait_ready:\n        self.wait_ready()\n    name = name.upper()\n    value = float(value)\n    success = False\n    remaining = retries\n    while True:\n        self._vehicle._master.param_set_send(name, value)\n        tstart = monotonic.monotonic()\n        if (remaining == 0):\n            break\n        remaining -= 1\n        while ((monotonic.monotonic() - tstart) < 1):\n            if ((name in self._vehicle._params_map) and (self._vehicle._params_map[name] == value)):\n                return True\n            time.sleep(0.1)\n    if (retries > 0):\n        errprinter(('timeout setting parameter %s to %f' % (name, value)))\n    return False\n", "label": 1}
{"function": "\n\ndef get(self):\n    'Handle GET.'\n    (pkgs, unused_dt) = models.ReportsCache.GetInstallCounts()\n    for p in models.PackageInfo.all():\n        if (not p.plist):\n            continue\n        if (p.munki_name not in pkgs):\n            continue\n        elif (not pkgs[p.munki_name].get('duration_seconds_avg', None)):\n            continue\n        lock = ('pkgsinfo_%s' % p.filename)\n        if (not gae_util.ObtainLock(lock, timeout=5.0)):\n            continue\n        old_desc = p.plist['description']\n        avg_duration_text = (models.PackageInfo.AVG_DURATION_TEXT % (pkgs[p.munki_name]['duration_count'], pkgs[p.munki_name]['duration_seconds_avg']))\n        p.description = ('%s\\n\\n%s' % (p.description, avg_duration_text))\n        if (p.plist['description'] != old_desc):\n            p.put()\n        gae_util.ReleaseLock(lock)\n    delay = 0\n    for track in common.TRACKS:\n        delay += 5\n        models.Catalog.Generate(track, delay=delay)\n", "label": 1}
{"function": "\n\ndef parse_candidates(self, result):\n    completion = {\n        'dup': 1,\n    }\n    _type = ''\n    word = ''\n    placeholder = ''\n    for chunk in [x for x in result.string if x.spelling]:\n        chunk_spelling = chunk.spelling\n        if chunk.isKindTypedText():\n            word += chunk_spelling\n            placeholder += chunk_spelling\n            continue\n        elif chunk.isKindResultType():\n            _type += chunk_spelling\n        else:\n            placeholder += chunk_spelling\n    completion['word'] = word\n    completion['abbr'] = completion['info'] = placeholder\n    completion['kind'] = ' '.join([(index_h.kinds[result.cursorKind] if (result.cursorKind in index_h.kinds) else str(result.cursorKind)), _type])\n    return completion\n", "label": 1}
{"function": "\n\ndef isDBPort(host, port, db, timeout=10):\n    if (host == JSONFILE_HOSTNAME):\n        return True\n    t = 2\n    while (t < timeout):\n        try:\n            conn = urllib.request.urlopen('http://{0}:{1}/{2}/status'.format(host, (port or '80'), db))\n            return True\n        except HTTPError:\n            return False\n        except URLError:\n            return False\n        except socket.timeout:\n            t = (t + 2)\n    return False\n", "label": 1}
{"function": "\n\ndef test_mysql_database_url_with_sslca_options(self):\n    os.environ['DATABASE_URL'] = 'mysql://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:3306/d8r82722r2kuvn?ssl-ca=rds-combined-ca-bundle.pem'\n    url = dj_database_url.config()\n    assert (url['ENGINE'] == 'django.db.backends.mysql')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 3306)\n    assert (url['OPTIONS'] == {\n        'ssl': {\n            'ca': 'rds-combined-ca-bundle.pem',\n        },\n    })\n    os.environ['DATABASE_URL'] = 'mysql://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:3306/d8r82722r2kuvn?'\n    url = dj_database_url.config()\n    assert ('OPTIONS' not in url)\n", "label": 1}
{"function": "\n\ndef __init__(self, params=None, responses=None):\n    super(MetaModel, self).__init__()\n    if ((params is None) or (not isinstance(params, tuple))):\n        msg = ('Metamodel params argument needs to be a tuple of ' + 'variable names.')\n        self.raise_exception(msg, ValueError)\n    if ((responses is None) or (not isinstance(responses, tuple))):\n        msg = ('Metamodel responses argument needs to be a tuple of ' + 'variable names.')\n        self.raise_exception(msg, ValueError)\n    input_tree = self.get('params')\n    self._param_data = []\n    for name in params:\n        self.add(name, Float(0.0, iotype='in', desc='metamodel param'))\n        input_tree.add(name, List([], desc='training param'))\n    output_tree = self.get('responses')\n    self._response_data = {\n        \n    }\n    for name in responses:\n        self.add(name, Float(0.0, iotype='out', desc='metamodel response'))\n        output_tree.add(name, List([], desc='training response'))\n        self._response_data[name] = []\n        self.surrogates[name] = None\n    self._surrogate_input_names = params\n    self._surrogate_output_names = responses\n    self._train = True\n    self._surrogate_overrides = set()\n    self._default_surrogate_copies = {\n        \n    }\n    self.on_trait_change(self._surrogate_updated, 'surrogates_items')\n", "label": 1}
{"function": "\n\ndef test_flatline_query_key():\n    rules = {\n        'timeframe': datetime.timedelta(seconds=30),\n        'threshold': 1,\n        'use_query_key': True,\n        'query_key': 'qk',\n        'timestamp_field': '@timestamp',\n    }\n    rule = FlatlineRule(rules)\n    rule.add_data(hits(1, qk='key1'))\n    rule.add_data(hits(1, qk='key2'))\n    rule.add_data(hits(1, qk='key3'))\n    assert (rule.matches == [])\n    rule.garbage_collect(ts_to_dt('2014-09-26T12:00:11Z'))\n    assert (rule.matches == [])\n    rule.add_data([create_event(ts_to_dt('2014-09-26T12:00:20Z'), qk='key3')])\n    timestamp = '2014-09-26T12:00:45Z'\n    rule.garbage_collect(ts_to_dt(timestamp))\n    assert (len(rule.matches) == 2)\n    assert (set(['key1', 'key2']) == set([m['key'] for m in rule.matches if (m['@timestamp'] == timestamp)]))\n    timestamp = '2014-09-26T12:01:20Z'\n    rule.garbage_collect(ts_to_dt(timestamp))\n    assert (len(rule.matches) == 3)\n    assert (set(['key3']) == set([m['key'] for m in rule.matches if (m['@timestamp'] == timestamp)]))\n", "label": 1}
{"function": "\n\ndef parse_data(infile):\n    'Parse data from `infile`.'\n    blocks = re.compile(' '.join(([('=' * 9)] * 8)))\n    dashes = re.compile('^-{79}$')\n    title = re.compile('^Timings for (.*)$')\n    row = re.compile((' '.join((['(.{9})'] * 7)) + ' (.{8,9})'))\n    lines = infile.readlines()\n    data = co.OrderedDict()\n    index = 0\n    while (index < len(lines)):\n        line = lines[index]\n        if blocks.match(line):\n            try:\n                name = title.match(lines[(index + 1)]).group(1)\n            except:\n                index += 1\n                continue\n            data[name] = {\n                \n            }\n            assert dashes.match(lines[(index + 2)])\n            cols = parse_row(row, lines[(index + 3)])\n            assert blocks.match(lines[(index + 4)])\n            get_row = parse_row(row, lines[(index + 5)])\n            assert (get_row[0] == 'get')\n            set_row = parse_row(row, lines[(index + 6)])\n            assert (set_row[0] == 'set')\n            delete_row = parse_row(row, lines[(index + 7)])\n            assert (delete_row[0] == 'delete')\n            assert blocks.match(lines[(index + 9)])\n            data[name]['get'] = dict(zip(cols, get_row))\n            data[name]['set'] = dict(zip(cols, set_row))\n            data[name]['delete'] = dict(zip(cols, delete_row))\n            index += 10\n        else:\n            index += 1\n    return data\n", "label": 1}
{"function": "\n\ndef _find_exe_in_registry(self):\n    try:\n        from _winreg import OpenKey, QueryValue, HKEY_LOCAL_MACHINE, HKEY_CURRENT_USER\n    except ImportError:\n        from winreg import OpenKey, QueryValue, HKEY_LOCAL_MACHINE, HKEY_CURRENT_USER\n    import shlex\n    keys = ('SOFTWARE\\\\Classes\\\\FirefoxHTML\\\\shell\\\\open\\\\command', 'SOFTWARE\\\\Classes\\\\Applications\\\\firefox.exe\\\\shell\\\\open\\\\command')\n    command = ''\n    for path in keys:\n        try:\n            key = OpenKey(HKEY_LOCAL_MACHINE, path)\n            command = QueryValue(key, '')\n            break\n        except OSError:\n            try:\n                key = OpenKey(HKEY_CURRENT_USER, path)\n                command = QueryValue(key, '')\n                break\n            except OSError:\n                pass\n    else:\n        return ''\n    if (not command):\n        return ''\n    return shlex.split(command)[0]\n", "label": 1}
{"function": "\n\ndef pp_options_list(keys, width=80, _print=False):\n    ' Builds a concise listing of available options, grouped by prefix '\n    from textwrap import wrap\n    from itertools import groupby\n\n    def pp(name, ks):\n        pfx = ((('- ' + name) + '.[') if name else '')\n        ls = wrap(', '.join(ks), width, initial_indent=pfx, subsequent_indent='  ', break_long_words=False)\n        if (ls and ls[(- 1)] and name):\n            ls[(- 1)] = (ls[(- 1)] + ']')\n        return ls\n    ls = []\n    singles = [x for x in sorted(keys) if (x.find('.') < 0)]\n    if singles:\n        ls += pp('', singles)\n    keys = [x for x in keys if (x.find('.') >= 0)]\n    for (k, g) in groupby(sorted(keys), (lambda x: x[:x.rfind('.')])):\n        ks = [x[(len(k) + 1):] for x in list(g)]\n        ls += pp(k, ks)\n    s = '\\n'.join(ls)\n    if _print:\n        print(s)\n    else:\n        return s\n", "label": 1}
{"function": "\n\n@decorators.which_bin(['lsblk', 'df'])\ndef fstype(device):\n    \"\\n    Return the filesystem name of a block device\\n\\n    .. versionadded:: 2015.8.2\\n\\n    device\\n        The name of the block device\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' blockdev.fstype /dev/sdX1\\n    \"\n    if salt.utils.which('lsblk'):\n        lsblk_out = __salt__['cmd.run']('lsblk -o fstype {0}'.format(device)).splitlines()\n        if (len(lsblk_out) > 1):\n            fs_type = lsblk_out[1].strip()\n            if fs_type:\n                return fs_type\n    if salt.utils.which('df'):\n        df_out = __salt__['cmd.run']('df -T {0}'.format(device)).splitlines()\n        if (len(df_out) > 1):\n            fs_type = df_out[1]\n            if fs_type:\n                return fs_type\n    return ''\n", "label": 1}
{"function": "\n\ndef _add_container_actions(self, container):\n    title_group_map = {\n        \n    }\n    for group in self._action_groups:\n        if (group.title in title_group_map):\n            msg = _('cannot merge actions - two groups are named %r')\n            raise ValueError((msg % group.title))\n        title_group_map[group.title] = group\n    group_map = {\n        \n    }\n    for group in container._action_groups:\n        if (group.title not in title_group_map):\n            title_group_map[group.title] = self.add_argument_group(title=group.title, description=group.description, conflict_handler=group.conflict_handler)\n        for action in group._group_actions:\n            group_map[action] = title_group_map[group.title]\n    for group in container._mutually_exclusive_groups:\n        mutex_group = self.add_mutually_exclusive_group(required=group.required)\n        for action in group._group_actions:\n            group_map[action] = mutex_group\n    for action in container._actions:\n        group_map.get(action, self)._add_action(action)\n", "label": 1}
{"function": "\n\ndef remove_results_below_q_threshold(search_results):\n    \"removes buildings if total count of buildings grouped by org is less\\n    than their org's public query threshold\\n\\n    :param list/queryset search_results: search results\\n    :returns: list or queryset\\n    \"\n    manual_group_by = {\n        \n    }\n    thresholds = {\n        \n    }\n    for b in search_results:\n        parent_org = b.super_organization.get_parent()\n        if (parent_org.id not in manual_group_by):\n            manual_group_by[parent_org.id] = 1\n            thresholds[parent_org.id] = parent_org.query_threshold\n        else:\n            manual_group_by[parent_org.id] += 1\n    orgs_below_threshold = []\n    for (org_id, count) in manual_group_by.items():\n        if (count < thresholds[org_id]):\n            orgs_below_threshold.append(org_id)\n    results = []\n    for sr in search_results:\n        if (sr.super_organization.get_parent() not in orgs_below_threshold):\n            results.append(sr)\n    return results\n", "label": 1}
{"function": "\n\ndef flush_user_profile(sender, **kwargs):\n    user_profile = kwargs['instance']\n    update_user_profile_caches([user_profile])\n    if ((kwargs.get('update_fields') is None) or (len((set(['full_name', 'short_name', 'email', 'is_active']) & set(kwargs['update_fields']))) > 0)):\n        cache_delete(active_user_dicts_in_realm_cache_key(user_profile.realm))\n    bot_fields = {'full_name', 'api_key', 'avatar_source', 'default_all_public_streams', 'is_active', 'default_sending_stream', 'default_events_register_stream'}\n    if (user_profile.is_bot and ((kwargs['update_fields'] is None) or (bot_fields & set(kwargs['update_fields'])))):\n        cache_delete(active_bot_dicts_in_realm_cache_key(user_profile.realm))\n    if ((kwargs.get('update_fields') is None) or ('alert_words' in kwargs['update_fields'])):\n        cache_delete(realm_alert_words_cache_key(user_profile.realm))\n", "label": 1}
{"function": "\n\n@register(_dump_registry, tuple)\ndef _dump_tuple(obj, stream):\n    l = len(obj)\n    if (l == 0):\n        stream.append(TAG_EMPTY_TUPLE)\n    elif (l == 1):\n        stream.append(TAG_TUP1)\n    elif (l == 2):\n        stream.append(TAG_TUP2)\n    elif (l == 3):\n        stream.append(TAG_TUP3)\n    elif (l == 4):\n        stream.append(TAG_TUP4)\n    elif (l < 256):\n        stream.append((TAG_TUP_L1 + I1.pack(l)))\n    else:\n        stream.append((TAG_TUP_L4 + I4.pack(l)))\n    for item in obj:\n        _dump(item, stream)\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    data = DefaultDict(None)\n    key = None\n    for line in self.f:\n        heading = self.isSectionHeading(line)\n        if heading:\n            if (data and (heading == self.newTestHeading)):\n                data[key] = data[key][:(- 1)]\n                (yield self.normaliseOutput(data))\n                data = DefaultDict(None)\n            key = heading\n            data[key] = ''\n        elif (key is not None):\n            data[key] += line\n    if data:\n        (yield self.normaliseOutput(data))\n", "label": 1}
{"function": "\n\n@requires_admin\ndef post(self, project_id):\n    project = Project.get(project_id)\n    if (project is None):\n        return ('', 404)\n    args = self.post_parser.parse_args()\n    if args.name:\n        project.name = args.name\n    if args.slug:\n        match = Project.query.filter((Project.slug == args.slug), (Project.id != project.id)).first()\n        if match:\n            return (('{\"error\": \"Project with slug %r already exists\"}' % (args.slug,)), 400)\n        project.slug = args.slug\n    if args.repository:\n        repository = Repository.get(args.repository)\n        if (repository is None):\n            return (('{\"error\": \"Repository with url %r does not exist\"}' % (args.repository,)), 400)\n        project.repository = repository\n    if (args.status == 'inactive'):\n        project.status = ProjectStatus.inactive\n    elif (args.status == 'active'):\n        project.status = ProjectStatus.active\n    db.session.add(project)\n    data = self.serialize(project)\n    data['repository'] = self.serialize(project.repository)\n    return self.respond(data, serialize=False)\n", "label": 1}
{"function": "\n\ndef fuzzy_load(name, merge_inherited=True):\n    localedata._cache_lock.acquire()\n    try:\n        data = localedata._cache.get(name)\n        if (not data):\n            if ((name == 'root') or (not merge_inherited)):\n                data = {\n                    \n                }\n            else:\n                parts = name.split('_')\n                if (len(parts) == 1):\n                    parent = 'root'\n                else:\n                    parent = '_'.join(parts[:(- 1)])\n                data = fuzzy_load(parent).copy()\n            filename = os.path.join(localedata._dirname, ('%s.dat' % name))\n            try:\n                fileobj = open(filename, 'rb')\n                try:\n                    if ((name != 'root') and merge_inherited):\n                        localedata.merge(data, pickle.load(fileobj))\n                    else:\n                        data = pickle.load(fileobj)\n                    localedata._cache[name] = data\n                finally:\n                    fileobj.close()\n            except IOError:\n                pass\n        return data\n    finally:\n        localedata._cache_lock.release()\n", "label": 1}
{"function": "\n\ndef _calc_correction(self):\n    '\\n        if self.tau > 0 this will be used in the correction factor calculation\\n        if self.tau = 0 then we assume ocr = icr in the correction factor calculation,\\n                      ie only lt correction\\n                     (note deadtime.calc_icr handles above two conditions)\\n        if self.tau < 0 (or None):\\n           if input_counts > 0  this will be used for icr in the factor calculation\\n           if input_counts <= 0 we assume ocr = icr in the correction factor calculation,\\n                                ie only lt correction\\n        '\n    if ((self.live_time <= 0) or (self.real_time <= 0)):\n        self.dt_factor = 1.0\n        return\n    if (self.total_counts > 0):\n        ocr = (self.total_counts / self.live_time)\n    else:\n        ocr = None\n    if (self.tau >= 0):\n        icr = calc_icr(ocr, self.tau)\n        if (icr is None):\n            icr = 0\n        self.icr_calc = icr\n    elif (self.input_counts > 0):\n        icr = (self.input_counts / self.live_time)\n    else:\n        icr = ocr = None\n    self.dt_factor = correction_factor(self.real_time, self.live_time, icr=icr, ocr=ocr)\n    if (self.dt_factor <= 0):\n        print('Error computing counts correction factor --> setting to 1')\n        self.dt_factor = 1.0\n", "label": 1}
{"function": "\n\ndef _calculateVectors(self):\n    log.msg('* Calculating shortest-path vectors', debug=True, system=LOG_SYSTEM)\n    paths = {\n        \n    }\n    for (port, vectors) in self.vectors.items():\n        for (network, cost) in vectors.items():\n            if (network in self.local_networks):\n                continue\n            if (network in self.blacklist_networks):\n                log.msg(('Skipping network %s in vector calculation, is blacklisted' % network), system=LOG_SYSTEM)\n                continue\n            if (cost > self.max_cost):\n                log.msg(('Skipping network %s in vector calculation, cost %i exceeds max cost %i' % (network, cost, self.max_cost)), system=LOG_SYSTEM)\n                continue\n            if (not (network in paths)):\n                paths[network] = (port, cost)\n                log.msg(('Added path to %s via %s. Cost %i' % (network, port, cost)), debug=True, system=LOG_SYSTEM)\n            elif (cost < paths[network][1]):\n                paths[network] = (port, cost)\n                log.msg(('Updated path to %s via %s. Cost %i' % (network, port, cost)), debug=True, system=LOG_SYSTEM)\n    self._shortest_paths = paths\n", "label": 1}
{"function": "\n\n@app.route('/division', methods=['GET', 'POST', 'OPTIONS'])\n@crossdomain(origin='*', headers=['Content-Type', 'X-Requested-With'])\ndef division_lookup():\n    try:\n        if ((request.json is None) and (request.method == 'POST')):\n            abort(400, 'Must provide JSON (did you set Content-type?)')\n        elif (request.method == 'POST'):\n            args = request.json\n        else:\n            args = request.args\n        if ('latitude' not in args):\n            abort(400, 'Most provide latitude and longitude')\n        if ('longitude' not in args):\n            abort(400, 'Most provide latitude and longitude')\n        conn = psycopg2.connect(host=dbcreds.HOSTNAME, database=dbcreds.DATABASE, user=dbcreds.USERNAME, password=dbcreds.PASSWORD)\n        cursor = conn.cursor()\n        cursor.execute(QUERY_FORMAT.format(latitude=float(args['latitude']), longitude=float(args['longitude'])))\n        result = cursor.fetchone()\n        if (result is None):\n            name = None\n        else:\n            name = result[0].lower().translate(None, \" -'\")\n        return jsonify({\n            'division': name,\n        })\n    except:\n        rollbar.report_exc_info()\n        raise\n", "label": 1}
{"function": "\n\ndef visit_label(self, label, add_to_result_map=None, within_label_clause=False, within_columns_clause=False, render_label_as_label=None, **kw):\n    render_label_with_as = (within_columns_clause and (not within_label_clause))\n    render_label_only = (render_label_as_label is label)\n    if (render_label_only or render_label_with_as):\n        if isinstance(label.name, elements._truncated_label):\n            labelname = self._truncated_identifier('colident', label.name)\n        else:\n            labelname = label.name\n    if render_label_with_as:\n        if (add_to_result_map is not None):\n            add_to_result_map(labelname, label.name, ((label, labelname) + label._alt_names), label.type)\n        return ((label.element._compiler_dispatch(self, within_columns_clause=True, within_label_clause=True, **kw) + OPERATORS[operators.as_]) + self.preparer.format_label(label, labelname))\n    elif render_label_only:\n        return self.preparer.format_label(label, labelname)\n    else:\n        return label.element._compiler_dispatch(self, within_columns_clause=False, **kw)\n", "label": 1}
{"function": "\n\ndef OnAuthTumblr(self):\n    self.User = self.le_mail.text().trimmed()\n    self.Password = self.le_password.text()\n    self.Blog = self.le_url.text().trimmed()\n    self.error = None\n    if (not ((self.User.isEmpty() | self.Password.isEmpty()) | self.Blog.isEmpty())):\n        self.api = Api(self.Blog, self.User, self.Password)\n        try:\n            self.auth = self.api.auth_check()\n            if QtGui.QSystemTrayIcon.isSystemTrayAvailable():\n                self.hide()\n                tray = TumblrTray(self)\n            else:\n                dashboard = Dashboard(self)\n                self.hide()\n                dashboard.show()\n            if (self.rememberme.checkState() == 2):\n                file = open((QtCore.QDir().homePath() + '/.opentumblr'), 'w')\n                file.write(self.le_mail.text())\n                file.write(self.le_url.text())\n        except TumblrAuthError:\n            self.error = errors['403']\n        except urllib2.HTTPError:\n            self.error = errors['404']\n        except urllib2.URLError:\n            self.error = errors['urlopen']\n        finally:\n            if (self.error != None):\n                QtGui.QMessageBox.warning(self, 'Error', ('Occurrio un error: \\n' + self.error), QtGui.QMessageBox.Ok)\n    else:\n        QtGui.QMessageBox.warning(self, 'Error', 'Todos los Campos son necesarios', QtGui.QMessageBox.Ok)\n", "label": 1}
{"function": "\n\ndef encode_features(X, enc_map=None):\n    'Converts categorical values in each column of X to integers in the range\\n    [0, n_unique_values_in_column - 1], if X is not already of integer type.\\n    Unknown values get a value of -1.\\n\\n    If mapping is not provided, it is calculated based on the valus in X.\\n    '\n    if np.issubdtype(X.dtype, np.integer):\n        return (X, enc_map)\n    if (enc_map is None):\n        fit = True\n        enc_map = []\n    else:\n        fit = False\n    Xenc = np.zeros(X.shape).astype('int')\n    for ii in range(X.shape[1]):\n        if fit:\n            enc_map.append({val: jj for (jj, val) in enumerate(np.unique(X[:, ii]))})\n        Xenc[:, ii] = np.array([enc_map[ii].get(x, (- 1)) for x in X[:, ii]])\n    return (Xenc, enc_map)\n", "label": 1}
{"function": "\n\ndef optimizeJumps(irdata):\n    instrs = irdata.flat_instructions\n    jump_instrs = [ins for ins in instrs if isinstance(ins, ir.LazyJumpBase)]\n    while 1:\n        done = True\n        (posd, _) = _calcMinimumPositions(instrs)\n        for ins in jump_instrs:\n            if ((ins.min < ins.max) and ins.widenIfNecessary(irdata.labels, posd)):\n                done = False\n        if done:\n            break\n    for ins in jump_instrs:\n        assert (ins.min <= ins.max)\n        ins.max = ins.min\n", "label": 1}
{"function": "\n\ndef handle(self):\n    message_format = '%s     %3d%%     [ %d / %d ]'\n    last_status_message_len = 0\n    status_message = ''\n    message_sent = False\n    self.server.binwalk.status.running = True\n    while True:\n        time.sleep(0.1)\n        try:\n            self.request.send(binwalk.core.compat.str2bytes(('\\x08' * last_status_message_len)))\n            self.request.send(binwalk.core.compat.str2bytes((' ' * last_status_message_len)))\n            self.request.send(binwalk.core.compat.str2bytes(('\\x08' * last_status_message_len)))\n            if self.server.binwalk.status.shutdown:\n                self.server.binwalk.status.finished = True\n                break\n            if (self.server.binwalk.status.total != 0):\n                percentage = ((float(self.server.binwalk.status.completed) / float(self.server.binwalk.status.total)) * 100)\n                status_message = (message_format % (self.server.binwalk.status.fp.path, percentage, self.server.binwalk.status.completed, self.server.binwalk.status.total))\n            elif (not message_sent):\n                status_message = 'No status information available at this time!'\n            else:\n                continue\n            last_status_message_len = len(status_message)\n            self.request.send(binwalk.core.compat.str2bytes(status_message))\n            message_sent = True\n        except IOError as e:\n            if (e.errno == errno.EPIPE):\n                break\n        except Exception as e:\n            binwalk.core.common.debug((('StatusRequestHandler exception: ' + str(e)) + '\\n'))\n        except KeyboardInterrupt as e:\n            raise e\n    self.server.binwalk.status.running = False\n    return\n", "label": 1}
{"function": "\n\ndef _getFileSizeQuota(self, model, resource):\n    '\\n        Get the current fileSizeQuota for a resource.  This takes the default\\n        quota into account if necessary.\\n\\n        :param model: the type of resource (e.g., user or collection)\\n        :param resource: the resource document.\\n        :returns: the fileSizeQuota.  None for no quota (unlimited), otherwise\\n                 a positive integer.\\n        '\n    useDefault = resource[QUOTA_FIELD].get('useQuotaDefault', True)\n    quota = resource[QUOTA_FIELD].get('fileSizeQuota', None)\n    if useDefault:\n        if (model == 'user'):\n            key = constants.PluginSettings.QUOTA_DEFAULT_USER_QUOTA\n        elif (model == 'collection'):\n            key = constants.PluginSettings.QUOTA_DEFAULT_COLLECTION_QUOTA\n        else:\n            key = None\n        if key:\n            quota = self.model('setting').get(key, None)\n    if ((not quota) or (quota < 0) or (not isinstance(quota, six.integer_types))):\n        return None\n    return quota\n", "label": 1}
{"function": "\n\ndef _get_constraint_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances', 1)\n    solver_cache = filter_properties['solver_cache']\n    constraint_matrix = [[True for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n    solver_cache['constraint_matrix'] = constraint_matrix\n    constraint_objects = [cons() for cons in self.constraint_classes]\n    constraint_objects.sort(key=(lambda cons: cons.precedence))\n    precedence_level = 0\n    for constraint_object in constraint_objects:\n        if (constraint_object.precedence > precedence_level):\n            solver_cache['constraint_matrix'] = constraint_matrix\n            precedence_level = constraint_object.precedence\n        this_cons_mat = constraint_object.get_constraint_matrix(hosts, filter_properties)\n        if (not this_cons_mat):\n            continue\n        for i in xrange(num_hosts):\n            constraint_matrix[i][1:] = [(constraint_matrix[i][(j + 1)] & this_cons_mat[i][j]) for j in xrange(num_instances)]\n    solver_cache['constraint_matrix'] = constraint_matrix\n    return constraint_matrix\n", "label": 1}
{"function": "\n\ndef local_add_mul_fusion(node):\n    'Fuse consecutive add or mul in one such node with more inputs.\\n\\n    It is better to fuse add/mul that way then in a Composite node as\\n    this make the inner graph of the Compiste smaller. This allow to\\n    put more computation in a Composite before hitting the max\\n    recusion limit when pickling Composite.\\n\\n    '\n    if ((not isinstance(node.op, Elemwise)) or (not isinstance(node.op.scalar_op, (scalar.Add, scalar.Mul)))):\n        return False\n    s_op = node.op.scalar_op.__class__\n    for inp in node.inputs:\n        if (inp.owner and isinstance(inp.owner.op, Elemwise) and isinstance(inp.owner.op.scalar_op, s_op)):\n            l = list(node.inputs)\n            l.remove(inp)\n            output_node = node.op(*(l + inp.owner.inputs))\n            copy_stack_trace(node.outputs[0], output_node)\n            return [output_node]\n", "label": 1}
{"function": "\n\n@classmethod\ndef load_dotenv(cls):\n    '\\n        Pulled from Honcho code with minor updates, reads local default\\n        environment variables from a .env file located in the project root\\n        or provided directory.\\n\\n        http://www.wellfireinteractive.com/blog/easier-12-factor-django/\\n        https://gist.github.com/bennylope/2999704\\n        '\n    dotenv = getattr(cls, 'DOTENV', None)\n    if (not dotenv):\n        return\n    try:\n        with open(dotenv, 'r') as f:\n            content = f.read()\n    except IOError as e:\n        raise ImproperlyConfigured(\"Couldn't read .env file with the path {}. Error: {}\".format(dotenv, e))\n    else:\n        for line in content.splitlines():\n            m1 = re.match('\\\\A([A-Za-z_0-9]+)=(.*)\\\\Z', line)\n            if (not m1):\n                continue\n            (key, val) = (m1.group(1), m1.group(2))\n            m2 = re.match(\"\\\\A'(.*)'\\\\Z\", val)\n            if m2:\n                val = m2.group(1)\n            m3 = re.match('\\\\A\"(.*)\"\\\\Z', val)\n            if m3:\n                val = re.sub('\\\\\\\\(.)', '\\\\1', m3.group(1))\n            os.environ.setdefault(key, val)\n        cls.DOTENV_LOADED = dotenv\n", "label": 1}
{"function": "\n\ndef clean(self):\n    username = self.cleaned_data.get('username')\n    password = self.cleaned_data.get('password')\n    message = ERROR_MESSAGE\n    if (username and password):\n        self.user_cache = authenticate(username=username, password=password)\n        if (self.user_cache is None):\n            if ('@' in username):\n                try:\n                    user = User.objects.get(email=username)\n                except (User.DoesNotExist, User.MultipleObjectsReturned):\n                    pass\n                else:\n                    if user.check_password(password):\n                        message = (_(\"Your e-mail address is not your username. Try '%s' instead.\") % user.username)\n            raise forms.ValidationError(message)\n        elif ((not self.user_cache.is_active) or (not self.user_cache.is_staff)):\n            raise forms.ValidationError(message)\n    self.check_for_test_cookie()\n    return self.cleaned_data\n", "label": 1}
{"function": "\n\ndef findStyleName(element, style):\n    oldStyle = DOM.getAttribute(element, 'className')\n    if (oldStyle is None):\n        return (- 1)\n    idx = oldStyle.find(style)\n    lastPos = len(oldStyle)\n    while (idx != (- 1)):\n        if ((idx == 0) or (oldStyle[(idx - 1)] == ' ')):\n            last = (idx + len(style))\n            if ((last == lastPos) or ((last < lastPos) and (oldStyle[last] == ' '))):\n                break\n        idx = oldStyle.find(style, (idx + 1))\n    return idx\n", "label": 1}
{"function": "\n\n@never_cache\n@login_required\ndef delete(request, app_label, model_name, instance_id, delete_form=DeleteRequestForm):\n    instance_return = _get_instance(request, 'delete', app_label, model_name, instance_id)\n    if isinstance(instance_return, HttpResponseForbidden):\n        return instance_return\n    (model, instance_form, instance) = instance_return\n    cancel = _handle_cancel(request)\n    if cancel:\n        return cancel\n    if (request.method == 'POST'):\n        form = delete_form(request.POST)\n        if form.is_valid():\n            instance.delete()\n            msg = (_('Your %(model_name)s was deleted.') % {\n                'model_name': model._meta.verbose_name,\n            })\n            try:\n                request.user.message_set.create(message=msg)\n            except AttributeError:\n                messages.success(request, msg)\n            if request.is_ajax():\n                return success_delete(request)\n            return _handle_response(request, instance)\n    else:\n        form = delete_form()\n    template_context = {\n        'action': 'delete',\n        'action_url': request.get_full_path(),\n        'model_title': model._meta.verbose_name,\n        'form': form,\n    }\n    return render_to_response(_get_template(request, None, None), template_context, RequestContext(request))\n", "label": 1}
{"function": "\n\ndef bfs_deque(self, rooms, x, y):\n    from collections import deque\n    m = len(rooms)\n    n = len(rooms[0])\n    q = deque()\n    q.append((x, y, 0))\n    while q:\n        (i, j, level) = q.popleft()\n        rooms[i][j] = min(rooms[i][j], level)\n        for d in self.dirs:\n            (i_t, j_t) = ((i + d[0]), (j + d[1]))\n            if ((0 <= i_t < m) and (0 <= j_t < n) and (rooms[i_t][j_t] != (- 1)) and (rooms[i_t][j_t] >= (level + 1))):\n                q.append((i_t, j_t, (level + 1)))\n", "label": 1}
{"function": "\n\ndef dialect_of(data, **kwargs):\n    ' CSV dialect of a CSV file stored in SSH, HDFS, or a Directory. '\n    keys = set(['delimiter', 'doublequote', 'escapechar', 'lineterminator', 'quotechar', 'quoting', 'skipinitialspace', 'strict', 'has_header'])\n    if isinstance(data, (HDFS(CSV), SSH(CSV))):\n        with sample(data) as fn:\n            d = dialect_of(CSV(fn, **data.dialect))\n    elif isinstance(data, (HDFS(Directory(CSV)), SSH(Directory(CSV)))):\n        with sample(data) as fn:\n            d = dialect_of(CSV(fn, **data.kwargs))\n    elif isinstance(data, Directory(CSV)):\n        d = dialect_of(next(data))\n    else:\n        assert isinstance(data, CSV)\n        with open(data.path, 'r') as f:\n            text = f.read()\n        result = dict()\n        d = sniffer.sniff(text)\n        d = dict(((k, getattr(d, k)) for k in keys if hasattr(d, k)))\n        if (data.has_header is None):\n            d['has_header'] = sniffer.has_header(text)\n        else:\n            d['has_header'] = data.has_header\n        d.update(data.dialect)\n    d.update(kwargs)\n    d = dict(((k, v) for (k, v) in d.items() if (k in keys)))\n    return d\n", "label": 1}
{"function": "\n\ndef _structured_bootstrap(args, n_boot, units, func, func_kwargs, rs):\n    'Resample units instead of datapoints.'\n    unique_units = np.unique(units)\n    n_units = len(unique_units)\n    args = [[a[(units == unit)] for unit in unique_units] for a in args]\n    boot_dist = []\n    for i in range(int(n_boot)):\n        resampler = rs.randint(0, n_units, n_units)\n        sample = [np.take(a, resampler, axis=0) for a in args]\n        lengths = map(len, sample[0])\n        resampler = [rs.randint(0, n, n) for n in lengths]\n        sample = [[c.take(r, axis=0) for (c, r) in zip(a, resampler)] for a in sample]\n        sample = list(map(np.concatenate, sample))\n        boot_dist.append(func(*sample, **func_kwargs))\n    return np.array(boot_dist)\n", "label": 1}
{"function": "\n\ndef test_benchmark():\n    TRIALS = range(1000000)\n    integer = 1\n    float = 3.0\n    long = 293203948032948023984023948023957245\n    stime = time.clock()\n    for i in TRIALS:\n        answer = isinstance(integer, (int, float, long, complex))\n        ok_(answer)\n    print(('isinstance, %s' % (time.clock() - stime)))\n    stime = time.clock()\n    for i in TRIALS:\n        answer = (integer.__class__ in set((int, float, long, complex)))\n        ok_(answer)\n    print(('__class__, set, %s' % (time.clock() - stime)))\n    stime = time.clock()\n    for i in TRIALS:\n        answer = ((integer.__class__ == int) or (integer.__class__ == float) or (integer.__class__ == long) or (integer.__class__ == complex))\n        ok_(answer)\n    print(('__class__, or, %s' % (time.clock() - stime)))\n", "label": 1}
{"function": "\n\ndef _on_calculate_verts(self):\n    self.u_interval = self.intervals[0]\n    self.u_set = list(self.u_interval.frange())\n    self.v_interval = self.intervals[1]\n    self.v_set = list(self.v_interval.frange())\n    self.bounds = [[S.Infinity, (- S.Infinity), 0], [S.Infinity, (- S.Infinity), 0], [S.Infinity, (- S.Infinity), 0]]\n    evaluate = self._get_evaluator()\n    self._calculating_verts_pos = 0.0\n    self._calculating_verts_len = float((self.u_interval.v_len * self.v_interval.v_len))\n    verts = list()\n    b = self.bounds\n    for u in self.u_set:\n        column = list()\n        for v in self.v_set:\n            try:\n                _e = evaluate(u, v)\n            except ZeroDivisionError:\n                _e = None\n            if (_e is not None):\n                for axis in range(3):\n                    b[axis][0] = min([b[axis][0], _e[axis]])\n                    b[axis][1] = max([b[axis][1], _e[axis]])\n            column.append(_e)\n            self._calculating_verts_pos += 1.0\n        verts.append(column)\n    for axis in range(3):\n        b[axis][2] = (b[axis][1] - b[axis][0])\n        if (b[axis][2] == 0.0):\n            b[axis][2] = 1.0\n    self.verts = verts\n    self.push_wireframe(self.draw_verts(False, False))\n    self.push_solid(self.draw_verts(False, True))\n", "label": 1}
{"function": "\n\ndef addFile(self, relPath, fullPath, distname, override=False):\n    fileName = os.path.basename(relPath)\n    fileExtension = os.path.splitext(fileName)[1]\n    if self.__package:\n        fileId = ('%s/' % self.__package)\n    else:\n        fileId = ''\n    if ((fileExtension in classExtensions) and (distname == 'classes')):\n        fileId += os.path.splitext(relPath)[0]\n        construct = jasy.item.Class.ClassItem\n        dist = self.classes\n    elif ((fileExtension in translationExtensions) and (distname == 'translations')):\n        fileId += os.path.splitext(relPath)[0]\n        construct = jasy.item.Translation.TranslationItem\n        dist = self.translations\n    elif (fileName in docFiles):\n        fileId += os.path.dirname(relPath)\n        fileId = fileId.strip('/')\n        construct = jasy.item.Doc.DocItem\n        dist = self.docs\n    else:\n        fileId += relPath\n        construct = jasy.item.Asset.AssetItem\n        dist = self.assets\n    if (construct != jasy.item.Asset.AssetItem):\n        fileId = fileId.replace('/', '.')\n    if ((fileId in dist) and (not override)):\n        raise UserError(('Item ID was registered before: %s' % fileId))\n    item = construct(self, fileId).attach(fullPath)\n    Console.debug(('Registering %s %s' % (item.kind, fileId)))\n    dist[fileId] = item\n", "label": 1}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    cls._meta = self\n    self.parent = cls\n    self.object_name = cls.__name__\n    self.type_name = self.object_name\n    self.description = cls.__doc__\n    self.original_attrs = {\n        \n    }\n    if self.meta:\n        meta_attrs = self.meta.__dict__.copy()\n        for name in self.meta.__dict__:\n            if name.startswith('_'):\n                del meta_attrs[name]\n        for attr_name in self.valid_attrs:\n            if (attr_name in meta_attrs):\n                setattr(self, attr_name, meta_attrs.pop(attr_name))\n                self.original_attrs[attr_name] = getattr(self, attr_name)\n            elif hasattr(self.meta, attr_name):\n                setattr(self, attr_name, getattr(self.meta, attr_name))\n                self.original_attrs[attr_name] = getattr(self, attr_name)\n        del self.valid_attrs\n        if (meta_attrs != {\n            \n        }):\n            raise TypeError((\"'class Meta' got invalid attribute(s): %s\" % ','.join(meta_attrs.keys())))\n    del self.meta\n", "label": 1}
{"function": "\n\ndef safe_create_instance(username, xml_file, media_files, uuid, request):\n    'Create an instance and catch exceptions.\\n\\n    :returns: A list [error, instance] where error is None if there was no\\n        error.\\n    '\n    error = instance = None\n    try:\n        instance = create_instance(username, xml_file, media_files, uuid=uuid, request=request)\n    except InstanceInvalidUserError:\n        error = OpenRosaResponseBadRequest(_('Username or ID required.'))\n    except InstanceEmptyError:\n        error = OpenRosaResponseBadRequest(_('Received empty submission. No instance was created'))\n    except FormInactiveError:\n        error = OpenRosaResponseNotAllowed(_('Form is not active'))\n    except XForm.DoesNotExist:\n        error = OpenRosaResponseNotFound(_('Form does not exist on this account'))\n    except ExpatError as e:\n        error = OpenRosaResponseBadRequest(_('Improperly formatted XML.'))\n    except DuplicateInstance:\n        response = OpenRosaResponse(_('Duplicate submission'))\n        response.status_code = 202\n        response['Location'] = request.build_absolute_uri(request.path)\n        error = response\n    except PermissionDenied as e:\n        error = OpenRosaResponseForbidden(e)\n    except InstanceMultipleNodeError as e:\n        error = OpenRosaResponseBadRequest(e)\n    except DjangoUnicodeDecodeError:\n        error = OpenRosaResponseBadRequest(_('File likely corrupted during transmission, please try later.'))\n    return [error, instance]\n", "label": 1}
{"function": "\n\ndef _async_recv_msg(self):\n    \"Internal use only; use 'recv_msg' with 'yield' instead.\\n\\n        Message is tagged with length of the payload (data). This\\n        method receives length of payload, then the payload and\\n        returns the payload.\\n        \"\n    n = struct.calcsize('>L')\n    try:\n        data = (yield self.recvall(n))\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            raise StopIteration('')\n        else:\n            raise\n    if (len(data) != n):\n        raise StopIteration('')\n    n = struct.unpack('>L', data)[0]\n    assert (n >= 0)\n    try:\n        data = (yield self.recvall(n))\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            raise StopIteration('')\n        else:\n            raise\n    if (len(data) != n):\n        raise StopIteration('')\n    raise StopIteration(data)\n", "label": 1}
{"function": "\n\ndef test_delete_user_in_backends_by_username(self):\n    ' Delete a user previously registered user by username\\n        '\n    try:\n        self.msu.create_user('bootsy', 'collins', 'funk@mothership.com')\n    except NotImplementedError:\n        skip('user management not supported in this version of managesf')\n    self.logout()\n    self.login('bootsy', 'collins', config.GATEWAY_URL)\n    self.assertEqual('funk@mothership.com', self.gu.get_account('bootsy').get('email'))\n    if has_issue_tracker():\n        users = self.rm.active_users()\n        users = [u for u in users if (u[0] == 'bootsy')]\n        self.assertEqual(1, len(users))\n        user = users[0]\n        self.assertEqual('funk@mothership.com', user[1])\n    del_url = (config.GATEWAY_URL + 'manage/services_users/?username=bootsy')\n    auth_cookie = get_cookie(config.GATEWAY_HOST, 'user5', config.ADMIN_PASSWORD)\n    d = requests.delete(del_url, cookies={\n        'auth_pubtkt': auth_cookie,\n    })\n    self.assertEqual(401, int(d.status_code))\n    auth_cookie = config.USERS[config.ADMIN_USER]['auth_cookie']\n    d = requests.delete(del_url, cookies={\n        'auth_pubtkt': auth_cookie,\n    })\n    self.assertTrue((int(d.status_code) < 400), d.status_code)\n    self.assertEqual(False, self.gu.get_account('bootsy'))\n    if has_issue_tracker():\n        users = self.rm.active_users()\n        self.assertEqual(0, len([u for u in users if (u[0] == 'bootsy')]))\n", "label": 1}
{"function": "\n\ndef _fill_remote(cur, keep_files):\n    'Add references to remote Keep files if present and not local.\\n    '\n    if isinstance(cur, (list, tuple)):\n        return [_fill_remote(x, keep_files) for x in cur]\n    elif isinstance(cur, dict):\n        out = {\n            \n        }\n        for (k, v) in cur.items():\n            out[k] = _fill_remote(v, keep_files)\n        return out\n    elif (isinstance(cur, basestring) and os.path.splitext(cur)[(- 1)] and (not os.path.exists(cur))):\n        for test_keep in keep_files:\n            if test_keep.endswith(cur):\n                return test_keep\n        return cur\n    else:\n        return cur\n", "label": 1}
{"function": "\n\ndef _check_array_dimensions(self, array=None):\n    \"\\n        Check that a grid's array dimensions agree with this grid's metadata\\n\\n        Parameters\\n        ----------\\n        array : np.ndarray or list of np.ndarray, optional\\n            The array for which to test the dimensions. If this is not\\n            specified, this method performs a self-consistency check of array\\n            dimensions and meta-data.\\n        \"\n    n_pop_ref = None\n    if isinstance(array, OctreeGridView):\n        array = array.quantities[array.viewed_quantity]\n    for quantity in self.quantities:\n        if (array is None):\n            (n_pop, shape) = single_grid_dims(self.quantities[quantity], ndim=1)\n        else:\n            (n_pop, shape) = single_grid_dims(array, ndim=1)\n        if (shape != self.shape):\n            raise ValueError(('Quantity arrays do not have the right dimensions: %s instead of %s' % (shape, self.shape)))\n        if (n_pop is not None):\n            if (n_pop_ref is None):\n                n_pop_ref = n_pop\n            elif (n_pop != n_pop_ref):\n                raise ValueError('Not all dust lists in the grid have the same size')\n", "label": 1}
{"function": "\n\ndef _create_x509_extension(self, handlers, extension):\n    if isinstance(extension.value, x509.UnrecognizedExtension):\n        obj = _txt2obj_gc(self, extension.oid.dotted_string)\n        value = _encode_asn1_str_gc(self, extension.value.value, len(extension.value.value))\n        return self._lib.X509_EXTENSION_create_by_OBJ(self._ffi.NULL, obj, (1 if extension.critical else 0), value)\n    else:\n        try:\n            encode = handlers[extension.oid]\n        except KeyError:\n            raise NotImplementedError('Extension not supported: {0}'.format(extension.oid))\n        ext_struct = encode(self, extension.value)\n        nid = self._lib.OBJ_txt2nid(extension.oid.dotted_string.encode('ascii'))\n        backend.openssl_assert((nid != self._lib.NID_undef))\n        x509_extension = self._lib.X509V3_EXT_i2d(nid, (1 if extension.critical else 0), ext_struct)\n        if ((x509_extension == self._ffi.NULL) and (extension.oid == x509.OID_CERTIFICATE_ISSUER)):\n            self._consume_errors()\n            pp = backend._ffi.new('unsigned char **')\n            r = self._lib.i2d_GENERAL_NAMES(ext_struct, pp)\n            backend.openssl_assert((r > 0))\n            pp = backend._ffi.gc(pp, (lambda pointer: backend._lib.OPENSSL_free(pointer[0])))\n            obj = _txt2obj_gc(self, extension.oid.dotted_string)\n            return self._lib.X509_EXTENSION_create_by_OBJ(self._ffi.NULL, obj, (1 if extension.critical else 0), _encode_asn1_str_gc(self, pp[0], r))\n        return x509_extension\n", "label": 1}
{"function": "\n\ndef __init__(self, path, upload_to=None, preview_w=None, preview_h=None):\n    self.upload_to = upload_to\n    self.preview_width = preview_w\n    self.preview_height = preview_h\n    self.metadata = {\n        \n    }\n    if (not path):\n        self.name = None\n        return\n    if ('%' in path):\n        path = urlunquote_plus(path)\n    if path.startswith(settings.MEDIA_URL):\n        self._path = get_relative_media_url(path, clean_slashes=False)\n    elif re.search('^(?:http(?:s)?:)?//', path):\n        self._path = self.download_image_url(path)\n    else:\n        abs_path = get_media_path(path)\n        if os.path.exists(abs_path):\n            self._path = get_relative_media_url(abs_path)\n    if ((not self._path) or (not os.path.exists(os.path.join(settings.MEDIA_ROOT, self._path)))):\n        self.name = None\n        return\n    super(ImageFile, self).__init__(self._path)\n    if self:\n        self.preview_image = self.get_for_size('preview')\n", "label": 1}
{"function": "\n\ndef _dirichlet_check_input(alpha, x):\n    x = np.asarray(x)\n    if (((x.shape[0] + 1) != alpha.shape[0]) and (x.shape[0] != alpha.shape[0])):\n        raise ValueError((\"Vector 'x' must have either the same number of entries as, or one entry fewer than, parameter vector 'a', but alpha.shape = %s and x.shape = %s.\" % (alpha.shape, x.shape)))\n    if (x.shape[0] != alpha.shape[0]):\n        xk = np.array([(1 - np.sum(x, 0))])\n        if (xk.ndim == 1):\n            x = np.append(x, xk)\n        elif (xk.ndim == 2):\n            x = np.vstack((x, xk))\n        else:\n            raise ValueError('The input must be one dimensional or a two dimensional matrix containing the entries.')\n    if (np.min(x) <= 0):\n        raise ValueError(\"Each entry in 'x' must be greater than zero.\")\n    if (np.max(x) > 1):\n        raise ValueError(\"Each entry in 'x' must be smaller or equal one.\")\n    if (np.abs((np.sum(x, 0) - 1.0)) > 1e-09).any():\n        raise ValueError((\"The input vector 'x' must lie within the normal simplex. but np.sum(x, 0) = %s.\" % np.sum(x, 0)))\n    return x\n", "label": 1}
{"function": "\n\ndef serialize_options(opts):\n    '\\n    A helper method to serialize and processes the options dictionary.\\n    '\n    options = (opts or {\n        \n    }).copy()\n    for key in opts.keys():\n        if (key not in DEFAULT_OPTIONS):\n            LOG.warn('Unknown option passed to Flask-CORS: %s', key)\n    options['origins'] = sanitize_regex_param(options.get('origins'))\n    options['allow_headers'] = sanitize_regex_param(options.get('allow_headers'))\n    if (('.*' in options['origins']) and options['supports_credentials'] and options['send_wildcard']):\n        raise ValueError(\"Cannot use supports_credentials in conjunction withan origin string of '*'. See: http://www.w3.org/TR/cors/#resource-requests\")\n    serialize_option(options, 'expose_headers')\n    serialize_option(options, 'methods', upper=True)\n    if isinstance(options.get('max_age'), timedelta):\n        options['max_age'] = str(int(options['max_age'].total_seconds()))\n    return options\n", "label": 1}
{"function": "\n\ndef guess_seqtype(s):\n    dna_letters = 'AaTtUuGgCcNn'\n    ndna = 0\n    nU = 0\n    nT = 0\n    for l in s:\n        if (l in dna_letters):\n            ndna += 1\n        if ((l == 'U') or (l == 'u')):\n            nU += 1\n        elif ((l == 'T') or (l == 't')):\n            nT += 1\n    ratio = (ndna / float(len(s)))\n    if (ratio > 0.85):\n        if (nT > nU):\n            return DNA_SEQTYPE\n        else:\n            return RNA_SEQTYPE\n    else:\n        return PROTEIN_SEQTYPE\n", "label": 1}
{"function": "\n\ndef attach(self, canvas):\n    super(Win32ARBContext, self).attach(canvas)\n    share = self.context_share\n    if share:\n        if (not share.canvas):\n            raise RuntimeError('Share context has no canvas.')\n        share = share._context\n    attribs = []\n    if (self.config.major_version is not None):\n        attribs.extend([wglext_arb.WGL_CONTEXT_MAJOR_VERSION_ARB, self.config.major_version])\n    if (self.config.minor_version is not None):\n        attribs.extend([wglext_arb.WGL_CONTEXT_MINOR_VERSION_ARB, self.config.minor_version])\n    flags = 0\n    if self.config.forward_compatible:\n        flags |= wglext_arb.WGL_CONTEXT_FORWARD_COMPATIBLE_BIT_ARB\n    if self.config.debug:\n        flags |= wglext_arb.WGL_DEBUG_BIT_ARB\n    if flags:\n        attribs.extend([wglext_arb.WGL_CONTEXT_FLAGS_ARB, flags])\n    attribs.append(0)\n    attribs = (c_int * len(attribs))(*attribs)\n    self.config._set_pixel_format(canvas)\n    self._context = wglext_arb.wglCreateContextAttribsARB(canvas.hdc, share, attribs)\n", "label": 1}
{"function": "\n\ndef iter_version_links(html, name):\n    '\\n    Iterate through version links (in order) within HTML.\\n\\n    Filtering out links that don\\'t \"look\" like versions.\\n\\n    Either yields hrefs to be recursively searches or tuples of (name, href)\\n    that match the given name.\\n    '\n    soup = BeautifulSoup(html)\n    for node in soup.findAll('a'):\n        if (node.get('href') is None):\n            continue\n        try:\n            (guessed_name, _) = guess_name_and_version(node.text)\n        except ValueError:\n            href = node['href']\n            for extension in ['.tar.gz', '.zip']:\n                if href.endswith(extension):\n                    (yield (basename(href), href))\n                    break\n            else:\n                if (node.get('rel') == 'download'):\n                    (yield href)\n        else:\n            if (guessed_name.replace('_', '-').lower() != name.replace('_', '-').lower()):\n                continue\n            (yield (node.text, node['href']))\n", "label": 1}
{"function": "\n\ndef consolidate_listing(self, con, current_artifacts):\n    (server_artifacts, duplicates) = self.read_existing_artifacts(con)\n    known_folders = set()\n    for artifact_name in iterkeys(current_artifacts):\n        known_folders.add(posixpath.dirname(artifact_name))\n    for (artifact_name, checksum) in iteritems(server_artifacts):\n        if (artifact_name not in current_artifacts):\n            con.log_buffer.append(('000 Deleting %s' % artifact_name))\n            con.delete_file(artifact_name)\n            folder = posixpath.dirname(artifact_name)\n            if (folder not in known_folders):\n                con.log_buffer.append(('000 Deleting %s' % folder))\n                con.delete_folder(folder)\n    if (duplicates or (server_artifacts != current_artifacts)):\n        listing = []\n        for (artifact_name, checksum) in iteritems(current_artifacts):\n            listing.append(('%s|%s\\n' % (artifact_name, checksum)))\n        listing.sort()\n        con.upload_file('.lektor/.listing.tmp', ''.join(listing))\n        con.rename_file('.lektor/.listing.tmp', '.lektor/listing')\n", "label": 1}
{"function": "\n\ndef __init__(self, node):\n    'Create the basic structures to keep the attribute information.\\n\\n        Reads all the HDF5 attributes (if any) on disk for the node \"node\".\\n\\n        Parameters\\n        ----------\\n        node\\n            The parent node\\n\\n        '\n    if (not node._v_isopen):\n        raise ClosedNodeError('the node for attribute set is closed')\n    dict_ = self.__dict__\n    self._g_new(node)\n    dict_['_v__nodefile'] = node._v_file\n    dict_['_v__nodepath'] = node._v_pathname\n    dict_['_v_attrnames'] = self._g_list_attr(node)\n    dict_['_v_unimplemented'] = []\n    try:\n        format_version = node._v_file.format_version\n    except AttributeError:\n        parsed_version = None\n    else:\n        if (format_version == 'unknown'):\n            parsed_version = None\n        else:\n            parsed_version = tuple(map(int, format_version.split('.')))\n    dict_['_v__format_version'] = parsed_version\n    dict_['_v_attrnamessys'] = []\n    dict_['_v_attrnamesuser'] = []\n    for attr in self._v_attrnames:\n        self.__getattr__(attr)\n        if issysattrname(attr):\n            self._v_attrnamessys.append(attr)\n        else:\n            self._v_attrnamesuser.append(attr)\n    self._v_attrnames.sort()\n    self._v_attrnamessys.sort()\n    self._v_attrnamesuser.sort()\n", "label": 1}
{"function": "\n\ndef _compare_files(self, filename1, filename2):\n    '\\n        Compares two files, giving precedence to header files over source\\n        files. This allows the resulting list of files to be more\\n        intelligently sorted.\\n        '\n    if ((filename1.find('.') != (- 1)) and (filename2.find('.') != (- 1))):\n        (basename1, ext1) = filename1.rsplit('.', 1)\n        (basename2, ext2) = filename2.rsplit('.', 1)\n        if (basename1 == basename2):\n            if ((ext1 in self.HEADER_EXTENSIONS) and (ext2 in self.IMPL_EXTENSIONS)):\n                return (- 1)\n            elif ((ext1 in self.IMPL_EXTENSIONS) and (ext2 in self.HEADER_EXTENSIONS)):\n                return 1\n    return cmp(filename1, filename2)\n", "label": 1}
{"function": "\n\ndef post(self, user_id=None, name=None):\n    user = User.get(self.session, user_id, name)\n    if (not user):\n        return self.notfound()\n    if ((user.name != self.current_user.name) and (not self.current_user.user_admin)):\n        return self.forbidden()\n    form = PublicKeyForm(self.request.arguments)\n    if (not form.validate()):\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    try:\n        pubkey = public_key.add_public_key(self.session, user, form.data['public_key'])\n    except public_key.PublicKeyParseError:\n        form.public_key.errors.append('Key failed to parse and is invalid.')\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    except public_key.DuplicateKey:\n        form.public_key.errors.append('Key already in use. Public keys must be unique.')\n        return self.render('public-key-add.html', form=form, user=user, alerts=self.get_form_alerts(form.errors))\n    AuditLog.log(self.session, self.current_user.id, 'add_public_key', 'Added public key: {}'.format(pubkey.fingerprint), on_user_id=user.id)\n    email_context = {\n        'actioner': self.current_user.name,\n        'changed_user': user.name,\n        'action': 'added',\n    }\n    send_email(self.session, [user.name], 'Public SSH key added', 'ssh_keys_changed', settings, email_context)\n    return self.redirect('/users/{}?refresh=yes'.format(user.name))\n", "label": 1}
{"function": "\n\ndef _on_headers(self, data):\n    try:\n        data = native_str(data.decode('latin1'))\n        eol = data.find('\\r\\n')\n        start_line = data[:eol]\n        try:\n            (method, uri, version) = start_line.split(' ')\n        except ValueError:\n            raise _BadRequestException('Malformed HTTP request line')\n        if (not version.startswith('HTTP/')):\n            raise _BadRequestException('Malformed HTTP version in HTTP Request-Line')\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            raise _BadRequestException('Malformed HTTP headers')\n        if (self.address_family in (socket.AF_INET, socket.AF_INET6)):\n            remote_ip = self.address[0]\n        else:\n            remote_ip = '0.0.0.0'\n        self._request = HTTPRequest(connection=self, method=method, uri=uri, version=version, headers=headers, remote_ip=remote_ip, protocol=self.protocol)\n        content_length = headers.get('Content-Length')\n        if content_length:\n            content_length = int(content_length)\n            if (content_length > self.stream.max_buffer_size):\n                raise _BadRequestException('Content-Length too long')\n            if (headers.get('Expect') == '100-continue'):\n                self.stream.write(b'HTTP/1.1 100 (Continue)\\r\\n\\r\\n')\n            self.stream.read_bytes(content_length, self._on_request_body)\n            return\n        self.request_callback(self._request)\n    except _BadRequestException as e:\n        gen_log.info('Malformed HTTP request from %s: %s', self.address[0], e)\n        self.close()\n        return\n", "label": 1}
{"function": "\n\n@expose('/delete/', methods=('POST',))\ndef delete(self):\n    '\\n            Delete view method\\n        '\n    form = self.delete_form()\n    path = form.path.data\n    if path:\n        return_url = self._get_dir_url('.index', op.dirname(path))\n    else:\n        return_url = self.get_url('.index')\n    if self.validate_form(form):\n        (base_path, full_path, path) = self._normalize_path(path)\n        if (not self.can_delete):\n            flash(gettext('Deletion is disabled.'), 'error')\n            return redirect(return_url)\n        if (not self.is_accessible_path(path)):\n            flash(gettext('Permission denied.'), 'error')\n            return redirect(self._get_dir_url('.index'))\n        if op.isdir(full_path):\n            if (not self.can_delete_dirs):\n                flash(gettext('Directory deletion is disabled.'), 'error')\n                return redirect(return_url)\n            try:\n                shutil.rmtree(full_path)\n                self.on_directory_delete(full_path, path)\n                flash(gettext('Directory \"%(path)s\" was successfully deleted.', path=path))\n            except Exception as ex:\n                flash(gettext('Failed to delete directory: %(error)s', error=ex), 'error')\n        else:\n            try:\n                os.remove(full_path)\n                self.on_file_delete(full_path, path)\n                flash(gettext('File \"%(name)s\" was successfully deleted.', name=path))\n            except Exception as ex:\n                flash(gettext('Failed to delete file: %(name)s', name=ex), 'error')\n    else:\n        helpers.flash_errors(form, message='Failed to delete file. %(error)s')\n    return redirect(return_url)\n", "label": 1}
{"function": "\n\ndef _app_hook_handler(self, alert_body, hook_headers):\n    if (not alert_body['application_name']):\n        self._log.info('No application found for alert %s. Will Ignore.', alert_body)\n        return\n    long_description = alert_body['long_description']\n    if (self._is_alert_opened(long_description) or self._is_escalated_downtime(long_description)):\n        payload = {\n            'alert': alert_body,\n            'header': hook_headers,\n        }\n        self._dispatch_trigger(WEB_APP_ALERT_TRIGGER_REF, payload)\n    elif (self._is_alert_closed(long_description) or self._is_downtime_recovered(long_description)):\n        payload = {\n            'alert': alert_body,\n            'header': hook_headers,\n        }\n        self._log.info('App alert closed. Delay.')\n        eventlet.spawn_after(self._normal_report_delay, self._dispatch_application_normal, payload)\n    elif (self._is_alert_canceled(long_description) or self._is_alert_acknowledged(long_description)):\n        self._log.info('Ignored alert : %s.', alert_body)\n", "label": 1}
{"function": "\n\ndef stop(self, signum=None, _unused=None):\n    'Stop the consumer from consuming by calling BasicCancel and setting\\n        our state.\\n\\n        :param int signum: The signal received\\n        :param frame _unused: The stack frame from when the signal was called\\n\\n        '\n    LOGGER.debug('Stop called in state: %s', self.state_description)\n    if self.is_stopped:\n        LOGGER.warning('Stop requested but consumer is already stopped')\n        return\n    elif self.is_shutting_down:\n        LOGGER.warning('Stop requested, consumer is already shutting down')\n        return\n    elif self.is_waiting_to_shutdown:\n        LOGGER.warning('Stop requested but already waiting to shut down')\n        return\n    self.cancel_consumer_with_rabbitmq()\n    if self.is_processing:\n        LOGGER.info('Waiting for consumer to finish processing')\n        self.set_state(self.STATE_STOP_REQUESTED)\n        if (signum == signal.SIGTERM):\n            signal.siginterrupt(signal.SIGTERM, False)\n        return\n    if self.stats.statsd:\n        self.stats.statsd.stop()\n    self.on_ready_to_stop()\n", "label": 1}
{"function": "\n\ndef format_params(self, params):\n    fp = []\n    for p in params:\n        if isinstance(p, text_type):\n            if (not self.driver_supports_utf8):\n                fp.append(p.encode('utf-8'))\n            else:\n                fp.append(p)\n        elif isinstance(p, binary_type):\n            if (not self.driver_supports_utf8):\n                fp.append(p.decode(self.encoding).encode('utf-8'))\n            else:\n                fp.append(p)\n        elif isinstance(p, type(True)):\n            if p:\n                fp.append(1)\n            else:\n                fp.append(0)\n        else:\n            fp.append(p)\n    return tuple(fp)\n", "label": 1}
{"function": "\n\ndef CallExpression(traverser, node):\n    args = [traverser.traverse_node(a) for a in node['arguments']]\n    member = traverser.traverse_node(node['callee'])\n    if ((node['callee']['type'] == 'MemberExpression') and (node['callee']['property']['type'] == 'Identifier')):\n        identifier_name = node['callee']['property']['name']\n        if (identifier_name in instanceactions.INSTANCE_DEFINITIONS):\n            traverser._debug('Calling instance action...')\n            result = instanceactions.INSTANCE_DEFINITIONS[identifier_name](args, traverser, traverser.traverse_node(node['callee']['object']))\n            if (result is not None):\n                return result\n    if (isinstance(member, JSGlobal) and ('return' in member.global_data)):\n        traverser._debug('EVALUATING RETURN...')\n        output = member.global_data['return'](wrapper=member, arguments=args, traverser=traverser)\n        if (output is not None):\n            return output\n    return JSObject(traverser=traverser)\n", "label": 1}
{"function": "\n\ndef _prepare_plots(self, mixing=False, unmixing=False):\n    if (self.locations_ is None):\n        raise RuntimeError('Need sensor locations for plotting')\n    if (self.topo_ is None):\n        from scot.eegtopo.topoplot import Topoplot\n        self.topo_ = Topoplot(clipping=self.topo_clipping)\n        self.topo_.set_locations(self.locations_)\n    if (mixing and (not self.mixmaps_)):\n        premix = (self.premixing_ if (self.premixing_ is not None) else np.eye(self.mixing_.shape[1]))\n        self.mixmaps_ = self.plotting.prepare_topoplots(self.topo_, np.dot(self.mixing_, premix))\n    if (unmixing and (not self.unmixmaps_)):\n        preinv = (np.linalg.pinv(self.premixing_) if (self.premixing_ is not None) else np.eye(self.unmixing_.shape[0]))\n        self.unmixmaps_ = self.plotting.prepare_topoplots(self.topo_, np.dot(preinv, self.unmixing_).T)\n", "label": 1}
{"function": "\n\ndef read_fasta_lengths(ifile):\n    'Generate sequence ID,length from stream ifile'\n    id = None\n    seqLength = 0\n    isEmpty = True\n    for line in ifile:\n        if ('>' == line[0]):\n            if ((id is not None) and (seqLength > 0)):\n                (yield (id, seqLength))\n                isEmpty = False\n            id = line[1:].split()[0]\n            seqLength = 0\n        elif (id is not None):\n            for word in line.split():\n                seqLength += len(word)\n    if ((id is not None) and (seqLength > 0)):\n        (yield (id, seqLength))\n    elif isEmpty:\n        raise IOError('no readable sequence in FASTA file!')\n", "label": 1}
{"function": "\n\ndef _match_task_syslog_path(path, application_id=None, job_id=None):\n    'Is this the path/URI of a task syslog?\\n\\n    If so, return a dictionary containing application_id and container_id\\n    (on YARN) or attempt_id (on pre-YARN Hadoop). Otherwise, return None\\n\\n    Optionally, filter by application_id (YARN) or job_id (pre-YARN).\\n    '\n    m = _PRE_YARN_TASK_SYSLOG_PATH_RE.match(path)\n    if m:\n        if (job_id and (job_id != _to_job_id(m.group('attempt_id')))):\n            return None\n        return dict(attempt_id=m.group('attempt_id'))\n    m = _YARN_TASK_SYSLOG_PATH_RE.match(path)\n    if m:\n        if (application_id and (application_id != m.group('application_id'))):\n            return None\n        return dict(application_id=m.group('application_id'), container_id=m.group('container_id'))\n    return None\n", "label": 1}
{"function": "\n\ndef mayRaiseException(self, exception_type):\n    for default in self.getDefaults():\n        if default.mayRaiseException(exception_type):\n            return True\n    kw_defaults = self.getKwDefaults()\n    if ((kw_defaults is not None) and kw_defaults.mayRaiseException(exception_type)):\n        return True\n    annotations = self.getAnnotations()\n    if ((annotations is not None) and annotations.mayRaiseException(exception_type)):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef test_new_term_with_terms():\n    rules = {\n        'fields': ['a'],\n        'timestamp_field': '@timestamp',\n        'es_host': 'example.com',\n        'es_port': 10,\n        'index': 'logstash',\n        'query_key': 'a',\n    }\n    mock_res = {\n        'aggregations': {\n            'filtered': {\n                'values': {\n                    'buckets': [{\n                        'key': 'key1',\n                        'doc_count': 1,\n                    }, {\n                        'key': 'key2',\n                        'doc_count': 5,\n                    }],\n                },\n            },\n        },\n    }\n    with mock.patch('elastalert.ruletypes.Elasticsearch') as mock_es:\n        mock_es.return_value = mock.Mock()\n        mock_es.return_value.search.return_value = mock_res\n        rule = NewTermsRule(rules)\n        assert (rule.es.search.call_count == 1)\n    terms = {\n        ts_now(): [{\n            'key': 'key1',\n            'doc_count': 1,\n        }, {\n            'key': 'key2',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (rule.matches == [])\n    terms = {\n        ts_now(): [{\n            'key': 'key3',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (len(rule.matches) == 1)\n    assert (rule.matches[0]['new_field'] == 'a')\n    assert (rule.matches[0]['a'] == 'key3')\n    rule.matches = []\n    terms = {\n        ts_now(): [{\n            'key': 'key3',\n            'doc_count': 1,\n        }],\n    }\n    rule.add_terms_data(terms)\n    assert (rule.matches == [])\n", "label": 1}
{"function": "\n\ndef mouseMoveEvent(self, event):\n    self.oldPosition = self.pos()\n    if (self.isSideClicked and self.isCusorRightSide):\n        w = max(self.minimumWidth(), ((self.currentWidth + event.x()) - self.rdragx))\n        h = self.currentHeight\n        self.resize(w, h)\n    elif (self.isSideClicked and self.isCusorDownSide):\n        w = self.currentWidth\n        h = max(self.minimumHeight(), ((self.currentHeight + event.y()) - self.rdragy))\n        self.resize(w, h)\n    elif self.isMaximized():\n        event.ignore()\n    elif (not self.isLocked()):\n        if hasattr(self, 'dragPosition'):\n            if (event.buttons() == Qt.LeftButton):\n                self.move((event.globalPos() - self.dragPosition))\n                event.accept()\n", "label": 1}
{"function": "\n\ndef fetch_art(self, session, task):\n    'Find art for the album being imported.'\n    if task.is_album:\n        if (task.album.artpath and os.path.isfile(task.album.artpath)):\n            return\n        if (task.choice_flag == importer.action.ASIS):\n            local = True\n        elif (task.choice_flag == importer.action.APPLY):\n            local = False\n        else:\n            return\n        candidate = self.art_for_album(task.album, task.paths, local)\n        if candidate:\n            self.art_candidates[task] = candidate\n", "label": 1}
{"function": "\n\ndef test_virtual_memory(self):\n    mem = psutil.virtual_memory()\n    assert (mem.total > 0), mem\n    assert (mem.available > 0), mem\n    assert (0 <= mem.percent <= 100), mem\n    assert (mem.used > 0), mem\n    assert (mem.free >= 0), mem\n    for name in mem._fields:\n        if (name != 'total'):\n            value = getattr(mem, name)\n            if (not (value >= 0)):\n                self.fail(('%r < 0 (%s)' % (name, value)))\n            if (value > mem.total):\n                self.fail(('%r > total (total=%s, %s=%s)' % (name, mem.total, name, value)))\n", "label": 1}
{"function": "\n\ndef static(environ, start_response, path):\n    logger.info(('[static]sending: %s' % (path,)))\n    try:\n        text = open(path).read()\n        if path.endswith('.ico'):\n            start_response('200 OK', [('Content-Type', 'image/x-icon')])\n        elif path.endswith('.html'):\n            start_response('200 OK', [('Content-Type', 'text/html')])\n        elif path.endswith('.json'):\n            start_response('200 OK', [('Content-Type', 'application/json')])\n        elif path.endswith('.txt'):\n            start_response('200 OK', [('Content-Type', 'text/plain')])\n        elif path.endswith('.css'):\n            start_response('200 OK', [('Content-Type', 'text/css')])\n        else:\n            start_response('200 OK', [('Content-Type', 'text/xml')])\n        return [text]\n    except IOError:\n        resp = NotFound()\n        return resp(environ, start_response)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef shutdown(dbpool, filename, archive_db_dir):\n    '\\n        Facilitates cleaning up filesystem objects after the last\\n        user of the connection pool goes away.\\n        '\n    try:\n        dbpool.close()\n    except Exception as e:\n        logging.info('SqlConnections: dbpool.close failed in shutdown: %s', e)\n    if ((filename != ':memory:') and os.path.exists(filename)):\n        if archive_db_dir:\n            logging.info(\"SqlConnections: Archiving db file '%s' to '%s'\", filename, archive_db_dir)\n            try:\n                os.renames(filename, archive_db_dir)\n            except Exception as e:\n                logging.info('SqlConnections: ERROR: Archive failed! %s', e)\n        else:\n            logging.info(\"SqlConnections: Removing db file '%s'\", filename)\n            try:\n                os.remove(filename)\n            except Exception as e:\n                logging.info('SqlConnections: ERROR: Could not remove db file! %s', e)\n", "label": 1}
{"function": "\n\ndef main():\n    (options, args) = parse_args()\n    names = sorted((name for name in resource_listdir('trac.wiki', 'default-pages') if (not name.startswith('.'))))\n    if args:\n        args = sorted((set(names) & set(map(os.path.basename, args))))\n    else:\n        args = names\n    if options.download:\n        download_default_pages(args, options.prefix)\n    env = EnvironmentStub(disable=['trac.mimeview.pygments.*'])\n    load_components(env)\n    with env.db_transaction:\n        for name in names:\n            wiki = WikiPage(env, name)\n            wiki.text = resource_string('trac.wiki', ('default-pages/' + name)).decode('utf-8')\n            if wiki.text:\n                wiki.save('trac', '')\n            else:\n                printout(('%s: Skipped empty page' % name))\n    req = Mock(href=Href('/'), abs_href=Href('http://localhost/'), perm=MockPerm())\n    for name in args:\n        wiki = WikiPage(env, name)\n        if (not wiki.exists):\n            continue\n        context = web_context(req, wiki.resource)\n        out = DummyIO()\n        DefaultWikiChecker(env, context, name).format(wiki.text, out)\n", "label": 1}
{"function": "\n\ndef __gen_opts(self, opts_in, grains, saltenv=None, ext=None, pillarenv=None):\n    '\\n        The options need to be altered to conform to the file client\\n        '\n    opts = copy.deepcopy(opts_in)\n    opts['file_roots'] = opts['pillar_roots']\n    opts['file_client'] = 'local'\n    if (not grains):\n        opts['grains'] = {\n            \n        }\n    else:\n        opts['grains'] = grains\n    if ('environment' not in opts):\n        opts['environment'] = saltenv\n    opts['id'] = self.minion_id\n    if ('pillarenv' not in opts):\n        opts['pillarenv'] = pillarenv\n    if opts['state_top'].startswith('salt://'):\n        opts['state_top'] = opts['state_top']\n    elif opts['state_top'].startswith('/'):\n        opts['state_top'] = salt.utils.url.create(opts['state_top'][1:])\n    else:\n        opts['state_top'] = salt.utils.url.create(opts['state_top'])\n    if self.__valid_ext(ext):\n        if ('ext_pillar' in opts):\n            opts['ext_pillar'].append(ext)\n        else:\n            opts['ext_pillar'] = [ext]\n    return opts\n", "label": 1}
{"function": "\n\ndef regroup_commands(commands):\n    '\\n    Returns a list of tuples:\\n\\n        [(command_to_run, [list, of, commands])]\\n\\n    If the list of commands has a single item, the command was not grouped.\\n    '\n    grouped = []\n    pending = []\n\n    def group_pending():\n        if (not pending):\n            return\n        new_command = grouped_command(pending)\n        result = []\n        while pending:\n            result.append(pending.pop(0))\n        grouped.append((new_command, result))\n    for (command, next_command) in peek(commands):\n        if can_group_commands(command, next_command):\n            if (pending and (not can_group_commands(pending[0], command))):\n                group_pending()\n            pending.append(command)\n        else:\n            if (pending and can_group_commands(pending[0], command)):\n                pending.append(command)\n            else:\n                grouped.append((command.clone(), [command]))\n            group_pending()\n    group_pending()\n    return grouped\n", "label": 1}
{"function": "\n\ndef fix_lib64(lib_dir):\n    \"\\n    Some platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y\\n    instead of lib/pythonX.Y.  If this is such a platform we'll just create a\\n    symlink so lib64 points to lib\\n    \"\n    if [p for p in distutils.sysconfig.get_config_vars().values() if (isinstance(p, basestring) and ('lib64' in p))]:\n        logger.debug('This system uses lib64; symlinking lib64 to lib')\n        assert (os.path.basename(lib_dir) == ('python%s' % sys.version[:3])), ('Unexpected python lib dir: %r' % lib_dir)\n        lib_parent = os.path.dirname(lib_dir)\n        assert (os.path.basename(lib_parent) == 'lib'), ('Unexpected parent dir: %r' % lib_parent)\n        copyfile(lib_parent, os.path.join(os.path.dirname(lib_parent), 'lib64'))\n", "label": 1}
{"function": "\n\ndef get_form(self, form_class=None):\n    '\\n        Returns an instance of the form to be used in this view.\\n        '\n    self.form = super(SmartFormMixin, self).get_form(form_class)\n    fields = list(self.derive_fields())\n    exclude = self.derive_exclude()\n    exclude += self.derive_readonly()\n    for field in exclude:\n        if (field in self.form.fields):\n            del self.form.fields[field]\n    if (fields is not None):\n        for (name, field) in self.form.fields.items():\n            if (not (name in fields)):\n                del self.form.fields[name]\n    location = forms.CharField(widget=forms.widgets.HiddenInput(), required=False)\n    if ('HTTP_REFERER' in self.request.META):\n        location.initial = self.request.META['HTTP_REFERER']\n    self.form.fields['loc'] = location\n    if fields:\n        fields.append('loc')\n    for (name, field) in self.form.fields.items():\n        field = self.customize_form_field(name, field)\n        self.form.fields[name] = field\n    return self.form\n", "label": 1}
{"function": "\n\ndef render_change_form(self, request, context, add=False, change=False, form_url='', obj=None):\n    deployment_nodes = []\n    for node in Node.objects.order_by('name'):\n        deployment_nodes.append((node.site_deployment.deployment.id, node.id, node.name))\n    deployment_flavors = []\n    for flavor in Flavor.objects.all():\n        for deployment in flavor.deployments.all():\n            deployment_flavors.append((deployment.id, flavor.id, flavor.name))\n    deployment_images = []\n    for image in Image.objects.all():\n        for deployment_image in image.imagedeployments.all():\n            deployment_images.append((deployment_image.deployment.id, image.id, image.name))\n    site_login_bases = []\n    for site in Site.objects.all():\n        site_login_bases.append((site.id, site.login_base))\n    context['deployment_nodes'] = deployment_nodes\n    context['deployment_flavors'] = deployment_flavors\n    context['deployment_images'] = deployment_images\n    context['site_login_bases'] = site_login_bases\n    return super(InstanceAdmin, self).render_change_form(request, context, add, change, form_url, obj)\n", "label": 1}
{"function": "\n\ndef main(argv=None):\n    args = docopt.docopt(__doc__, argv=argv)\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(name)s -- %(message)s')\n    revert_radius = int(args['--revert-radius'])\n    revert_window = (int(args['--revert-window']) * (60 * 60))\n    if args['--host']:\n        session = mwapi.Session(args['--host'], user_agent='ORES revert labeling utility')\n    else:\n        session = None\n    dumps = args['<dump-file>']\n    verbose = args['--verbose']\n    start = args['--start']\n    if start:\n        start = Timestamp(start)\n    end = args['--end']\n    if end:\n        end = Timestamp(end)\n    reverted_only = args['--reverted-only']\n    trusted_groups = args['--trusted-groups']\n    if trusted_groups:\n        trusted_groups = trusted_groups.split(',')\n        trusted_users = load_user_group_members(trusted_groups, session)\n    else:\n        trusted_users = None\n    trusted_edits = args['--trusted-edits']\n    if trusted_edits:\n        trusted_edits = int(trusted_edits)\n    if (args['--rev-reverteds'] == '<stdout>'):\n        rev_reverteds = mysqltsv.Writer(sys.stdout)\n    else:\n        rev_reverteds = mysqltsv.Writer(open(args['--rev-reverteds'], 'w'))\n    check_blocked = args['--check-blocked']\n    run(dumps, session, start, end, revert_radius, revert_window, reverted_only, trusted_users, trusted_edits, rev_reverteds, check_blocked, verbose=verbose)\n", "label": 1}
{"function": "\n\ndef DeclareLocks(self, level):\n    if (level == locking.LEVEL_NODEGROUP):\n        assert (not self.needed_locks[locking.LEVEL_NODEGROUP])\n        if self.req_target_uuids:\n            lock_groups = set(self.req_target_uuids)\n            instance_groups = self.cfg.GetInstanceNodeGroups(self.op.instance_uuid)\n            lock_groups.update(instance_groups)\n        else:\n            lock_groups = locking.ALL_SET\n        self.needed_locks[locking.LEVEL_NODEGROUP] = lock_groups\n    elif (level == locking.LEVEL_NODE):\n        if self.req_target_uuids:\n            self.recalculate_locks[locking.LEVEL_NODE] = constants.LOCKS_APPEND\n            self._LockInstancesNodes()\n            lock_groups = (frozenset(self.owned_locks(locking.LEVEL_NODEGROUP)) | self.cfg.GetInstanceNodeGroups(self.op.instance_uuid))\n            member_nodes = [node_uuid for group in lock_groups for node_uuid in self.cfg.GetNodeGroup(group).members]\n            self.needed_locks[locking.LEVEL_NODE].extend(member_nodes)\n        else:\n            self.needed_locks[locking.LEVEL_NODE] = locking.ALL_SET\n", "label": 1}
{"function": "\n\ndef test_admin_inheritor_documents(suite, administrator, docs):\n    jwt = _login(administrator)\n    headers = _auth_header(jwt)\n    docs = [{\n        'name': 'Added Document {0}'.format(x),\n        'date': datetime.utcnow().isoformat(),\n    } for x in range(101)]\n    inheritor_documents = 'http://localhost:5000/api/authorized-app/simple-documents'\n    confirmed_dangerous_delete = requests.delete(inheritor_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    post = requests.post(inheritor_documents, headers=headers, data=json.dumps(docs))\n    assert post.ok\n    get_all = requests.get((inheritor_documents + ';json'), headers=headers)\n    assert get_all.ok\n    confirmed_dangerous_delete = requests.delete(inheritor_documents, headers=headers, params={\n        'delete_all': True,\n    })\n    assert confirmed_dangerous_delete.ok\n    get_empty = requests.get((inheritor_documents + ';json'), headers=headers)\n    assert get_empty.ok\n    _logout(jwt)\n", "label": 1}
{"function": "\n\ndef RGS_unrank(rank, m):\n    '\\n    Gives the unranked restricted growth string for a given\\n    superset size.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.combinatorics.partitions import RGS_unrank\\n    >>> RGS_unrank(14, 4)\\n    [0, 1, 2, 3]\\n    >>> RGS_unrank(0, 4)\\n    [0, 0, 0, 0]\\n    '\n    if (m < 1):\n        raise ValueError('The superset size must be >= 1')\n    if ((rank < 0) or (RGS_enum(m) <= rank)):\n        raise ValueError('Invalid arguments')\n    L = ([1] * (m + 1))\n    j = 1\n    D = RGS_generalized(m)\n    for i in range(2, (m + 1)):\n        v = D[((m - i), j)]\n        cr = (j * v)\n        if (cr <= rank):\n            L[i] = (j + 1)\n            rank -= cr\n            j += 1\n        else:\n            L[i] = int(((rank / v) + 1))\n            rank %= v\n    return [(x - 1) for x in L[1:]]\n", "label": 1}
{"function": "\n\ndef get_ambient_temperature(self, n=5):\n    '\\n        Populates the self.ambient_temp property\\n\\n        Calculation is taken from Rs232_Comms_v100.pdf section \"Converting values\\n        sent by the device to meaningful units\" item 5.\\n        '\n    if self.logger:\n        self.logger.info('Getting ambient temperature')\n    values = []\n    for i in range(0, n):\n        try:\n            value = (float(self.query('!T')[0]) / 100.0)\n        except:\n            pass\n        else:\n            if self.logger:\n                self.logger.debug('  Ambient Temperature Query = {:.1f}'.format(value))\n            values.append(value)\n    if (len(values) >= (n - 1)):\n        self.ambient_temp = (np.median(values) * u.Celsius)\n        if self.logger:\n            self.logger.info('  Ambient Temperature = {:.1f}'.format(self.ambient_temp))\n    else:\n        self.ambient_temp = None\n        if self.logger:\n            self.logger.info('  Failed to Read Ambient Temperature')\n    return self.ambient_temp\n", "label": 1}
{"function": "\n\ndef prepare_on_all_hosts(self, query, excluded_host):\n    '\\n        Prepare the given query on all hosts, excluding ``excluded_host``.\\n        Intended for internal use only.\\n        '\n    futures = []\n    for host in self._pools.keys():\n        if ((host != excluded_host) and host.is_up):\n            future = ResponseFuture(self, PrepareMessage(query=query), None, self.default_timeout)\n            try:\n                request_id = future._query(host)\n            except Exception:\n                log.exception('Error preparing query for host %s:', host)\n                continue\n            if (request_id is None):\n                log.debug('Failed to prepare query for host %s: %r', host, future._errors.get(host))\n                continue\n            futures.append((host, future))\n    for (host, future) in futures:\n        try:\n            future.result()\n        except Exception:\n            log.exception('Error preparing query for host %s:', host)\n", "label": 1}
{"function": "\n\ndef g(self, x):\n    g1 = ((x[0] + x[1]) - 2.0)\n    g2 = ((6.0 - x[0]) - x[1])\n    g3 = ((2.0 - x[1]) + x[0])\n    g4 = ((1.0 - x[0]) + (3.0 * x[1]))\n    g5 = ((4.0 - ((x[2] - 3) ** 2)) - x[3])\n    g6 = ((((x[4] - 3) ** 2) + x[5]) - 4.0)\n    if ((g1 >= 0) and (g2 >= 0) and (g3 >= 0) and (g4 >= 0) and (g5 >= 0) and (g6 >= 0)):\n        return (True, array([0.0, 0.0]))\n    return (False, array([g1, g2, g3, g4, g5, g6]))\n", "label": 1}
{"function": "\n\ndef put(self, objects, force=False, commit=True):\n    if self.stopped:\n        return\n    self.init()\n    if (isinstance(objects, basestring) or (not iterable(objects))):\n        return self.put_one(objects, force, commit)\n    remains = []\n    for obj in objects:\n        result = self.put_one(obj, force, commit=False)\n        if (result is not None):\n            remains.append(result)\n    m = self.map_handles[WRITE_ENTRANCE]\n    if ((len(remains) > 0) and (m is not None)):\n        with self.lock:\n            m.flush()\n    return remains\n", "label": 1}
{"function": "\n\n@classmethod\ndef MergeAppYamlAppInclude(cls, appyaml, appinclude):\n    'This function merges an app.yaml file with referenced builtins/includes.\\n    '\n    if (not appinclude):\n        return appyaml\n    if appinclude.handlers:\n        tail = (appyaml.handlers or [])\n        appyaml.handlers = []\n        for h in appinclude.handlers:\n            if ((not h.position) or (h.position == 'head')):\n                appyaml.handlers.append(h)\n            else:\n                tail.append(h)\n            h.position = None\n        appyaml.handlers.extend(tail)\n    AppInclude.MergeManualScaling(appyaml, appinclude)\n    appyaml.admin_console = AdminConsole.Merge(appyaml.admin_console, appinclude.admin_console)\n    appyaml.vm_settings = VmSettings.Merge(appyaml.vm_settings, appinclude.vm_settings)\n    return appyaml\n", "label": 1}
{"function": "\n\ndef validate_dict(template, to_be_checked, parent, missing_log, extras_log):\n    '\\n    Given a dictionary as a template will check that the to_be_checked\\n    dictionary.\\n    '\n    for k in to_be_checked:\n        k = k.strip()\n        if (not (k in template)):\n            extras_log.append(('Field %r in record %r' % (k, parent)))\n    for k in template:\n        k = k.strip()\n        if (not (k in to_be_checked)):\n            missing_log.append(('Field %r in record %r' % (k, parent)))\n    for (key, val) in template.iteritems():\n        if isinstance(val, dict):\n            validate_dict(val, to_be_checked[key], parent, missing_log, extras_log)\n", "label": 1}
{"function": "\n\ndef uninstall(self):\n    platform = self.get_type()\n    installed_platforms = PlatformFactory.get_platforms(installed=True).keys()\n    if (platform not in installed_platforms):\n        raise exception.PlatformNotInstalledYet(platform)\n    deppkgs = set()\n    for item in installed_platforms:\n        if (item == platform):\n            continue\n        p = PlatformFactory.newPlatform(item)\n        deppkgs = deppkgs.union(set(p.get_packages().keys()))\n    pm = PackageManager()\n    for name in self.get_packages().keys():\n        if ((not pm.is_installed(name)) or (name in deppkgs)):\n            continue\n        pm.uninstall(name)\n    installed_platforms.remove(platform)\n    set_state_item('installed_platforms', installed_platforms)\n    return True\n", "label": 1}
{"function": "\n\ndef get_language_from_extension(file_name):\n    'Returns a matching language for the given file extension.\\n    '\n    (filepart, extension) = os.path.splitext(file_name)\n    if (os.path.exists(u('{0}{1}').format(u(filepart), u('.c'))) or os.path.exists(u('{0}{1}').format(u(filepart), u('.C')))):\n        return 'C'\n    extension = extension.lower()\n    if (extension == '.h'):\n        directory = os.path.dirname(file_name)\n        available_files = os.listdir(directory)\n        available_extensions = list(zip(*map(os.path.splitext, available_files)))[1]\n        available_extensions = [ext.lower() for ext in available_extensions]\n        if ('.cpp' in available_extensions):\n            return 'C++'\n        if ('.c' in available_extensions):\n            return 'C'\n    return None\n", "label": 1}
{"function": "\n\ndef info(self, query):\n    response = self.command('remote-info', query)[0].decode(*enc)\n    if (response and ('Unknown package' not in response)):\n        from re import match\n        from string import strip\n        res = [map(strip, match('(\\\\w*)(.*$)', line.replace('\\n', '')).groups()) for line in response.splitlines() if (line and ('PACKAGE' not in line) and ('====' not in line) and match('(\\\\w*)(.*$)', line))]\n        return self.munge_lines(res)\n    return ['Aborted: No info available']\n", "label": 1}
{"function": "\n\ndef add_task(self, task, raise_error=False):\n    '\\n        Add task to the task queue.\\n        '\n    if self.parser_mode:\n        self.parser_result_queue.put((task, None))\n        return\n    if (self.task_queue is None):\n        raise SpiderMisuseError('You should configure task queue before adding tasks. Use `setup_queue` method.')\n    if ((task.priority is None) or (not task.priority_is_custom)):\n        task.priority = self.generate_task_priority()\n        task.priority_is_custom = False\n    else:\n        task.priority_is_custom = True\n    try:\n        if (not task.url.startswith(('http://', 'https://', 'ftp://', 'file://', 'feed://'))):\n            if (self.base_url is None):\n                msg = ('Could not resolve relative URL because base_url is not specified. Task: %s, URL: %s' % (task.name, task.url))\n                raise SpiderError(msg)\n            else:\n                warn('Class attribute `Spider::base_url` is deprecated. Use Task objects with absolute URLs')\n                task.url = urljoin(self.base_url, task.url)\n                if task.grab_config:\n                    task.grab_config['url'] = task.url\n    except Exception as ex:\n        self.stat.collect('task-with-invalid-url', task.url)\n        if raise_error:\n            raise\n        else:\n            logger.error('', exc_info=ex)\n            return False\n    self.task_queue.put(task, task.priority, schedule_time=task.schedule_time)\n    return True\n", "label": 1}
{"function": "\n\ndef check_protected_from_update(object, data):\n    if (object.is_protected and data.get('is_protected', True)):\n        if ('is_public' in data):\n            obj = object.to_dict()\n            if all((((k == 'is_public') or ((k in obj) and (obj[k] == v))) for (k, v) in six.iteritems(data))):\n                return\n        raise ex.UpdateFailedException(object.id, _(\"{object} with id '%s' could not be updated because it's marked as protected\").format(object=type(object).__name__))\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('LoginUser_args')\n    if (self.username is not None):\n        oprot.writeFieldBegin('username', TType.STRING, 1)\n        oprot.writeString(self.username)\n        oprot.writeFieldEnd()\n    if (self.Pass is not None):\n        oprot.writeFieldBegin('Pass', TType.STRING, 2)\n        oprot.writeString(self.Pass)\n        oprot.writeFieldEnd()\n    if (self.Remember is not None):\n        oprot.writeFieldBegin('Remember', TType.BOOL, 3)\n        oprot.writeBool(self.Remember)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef transform_Assign(self, stmt):\n    lhs = stmt.lhs\n    lhs_class = lhs.__class__\n    rhs = self.transform_expr(stmt.rhs)\n    if (lhs_class is Tuple):\n        for (i, _) in enumerate(lhs.type.elt_types):\n            lhs_i = self.tuple_proj(lhs, i)\n            rhs_i = self.tuple_proj(rhs, i)\n            assert (lhs_i.__class__ not in (ArrayView, Tuple))\n            self.assign(lhs_i, rhs_i)\n        return None\n    elif (lhs_class is Index):\n        lhs = self.transform_Index(lhs)\n        if (lhs.__class__ is ArrayView):\n            copy_loop = self.array_copy(src=rhs, dest=lhs, return_stmt=True)\n            copy_loop = self.transform_stmt(copy_loop)\n            return copy_loop\n    elif ((lhs_class is Var) and (stmt.rhs.__class__ in (Slice, Struct, ArrayView, Tuple))):\n        self.bindings[lhs.name] = rhs\n    return Assign(lhs, rhs)\n", "label": 1}
{"function": "\n\ndef pil_from_ndarray(ndarray):\n    '\\n    Converts an ndarray to a PIL image.\\n\\n    Parameters\\n    ----------\\n    ndarray : ndarray\\n        An ndarray containing an image.\\n\\n    Returns\\n    -------\\n    pil : PIL Image\\n        A PIL Image containing the image.\\n    '\n    try:\n        if ((ndarray.dtype == 'float32') or (ndarray.dtype == 'float64')):\n            assert (ndarray.min() >= 0.0)\n            assert (ndarray.max() <= 1.0)\n            ndarray = np.cast['uint8']((ndarray * 255))\n            if ((len(ndarray.shape) == 3) and (ndarray.shape[2] == 1)):\n                ndarray = ndarray[:, :, 0]\n        ensure_Image()\n        rval = Image.fromarray(ndarray)\n        return rval\n    except Exception as e:\n        logger.exception('original exception: ')\n        logger.exception(e)\n        logger.exception('ndarray.dtype: {0}'.format(ndarray.dtype))\n        logger.exception('ndarray.shape: {0}'.format(ndarray.shape))\n        raise\n    assert False\n", "label": 1}
{"function": "\n\ndef test_SeqExprOp():\n    form = SeqFormula((n ** 2), (n, 0, 10))\n    per = SeqPer((1, 2, 3), (m, 5, 10))\n    s = SeqExprOp(form, per)\n    assert (s.gen == ((n ** 2), (1, 2, 3)))\n    assert (s.interval == Interval(5, 10))\n    assert (s.start == 5)\n    assert (s.stop == 10)\n    assert (s.length == 6)\n    assert (s.variables == (n, m))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef get_reasons(reasons):\n    'return string with description of reason task is not up-to-date'\n    lines = []\n    if reasons['has_no_dependencies']:\n        lines.append(' * The task has no dependencies.')\n    if reasons['uptodate_false']:\n        lines.append(' * The following uptodate objects evaluate to false:')\n        for (utd, utd_args, utd_kwargs) in reasons['uptodate_false']:\n            msg = '    - {} (args={}, kwargs={})'\n            lines.append(msg.format(utd, utd_args, utd_kwargs))\n    if reasons['checker_changed']:\n        msg = ' * The file_dep checker changed from {0} to {1}.'\n        lines.append(msg.format(*reasons['checker_changed']))\n    sentences = {\n        'missing_target': 'The following targets do not exist:',\n        'changed_file_dep': 'The following file dependencies have changed:',\n        'missing_file_dep': 'The following file dependencies are missing:',\n        'removed_file_dep': 'The following file dependencies were removed:',\n        'added_file_dep': 'The following file dependencies were added:',\n    }\n    for (reason, sentence) in sentences.items():\n        entries = reasons.get(reason)\n        if entries:\n            lines.append(' * {}'.format(sentence))\n            for item in entries:\n                lines.append('    - {}'.format(item))\n    return '\\n'.join(lines)\n", "label": 1}
{"function": "\n\ndef cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n    'Returns a CookieJar from a key/value dictionary.\\n\\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\\n    :param cookiejar: (optional) A cookiejar to add the cookies to.\\n    :param overwrite: (optional) If False, will not replace cookies\\n        already in the jar with new ones.\\n    '\n    if (cookiejar is None):\n        cookiejar = RequestsCookieJar()\n    if (cookie_dict is not None):\n        names_from_jar = [cookie.name for cookie in cookiejar]\n        for name in cookie_dict:\n            if (overwrite or (name not in names_from_jar)):\n                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n    return cookiejar\n", "label": 1}
{"function": "\n\ndef levenshtein_distance(first, second):\n    'Find the Levenshtein distance between two strings.'\n    if (len(first) > len(second)):\n        (first, second) = (second, first)\n    if (len(second) == 0):\n        return len(first)\n    first_length = (len(first) + 1)\n    second_length = (len(second) + 1)\n    distance_matrix = [([0] * second_length) for x in range(first_length)]\n    for i in range(first_length):\n        distance_matrix[i][0] = i\n    for j in range(second_length):\n        distance_matrix[0][j] = j\n    for i in xrange(1, first_length):\n        for j in range(1, second_length):\n            deletion = (distance_matrix[(i - 1)][j] + 1)\n            insertion = (distance_matrix[i][(j - 1)] + 1)\n            substitution = distance_matrix[(i - 1)][(j - 1)]\n            if (first[(i - 1)] != second[(j - 1)]):\n                substitution += 1\n            distance_matrix[i][j] = min(insertion, deletion, substitution)\n    return distance_matrix[(first_length - 1)][(second_length - 1)]\n", "label": 1}
{"function": "\n\ndef doctree_read(app, doctree):\n    secnums = app.builder.env.toc_secnumbers\n    for node in doctree.traverse(subfigstart):\n        parentloc = node.parent.children.index(node)\n        subfigendloc = parentloc\n        while (subfigendloc < len(node.parent.children)):\n            n = node.parent.children[subfigendloc]\n            if isinstance(n, subfigend):\n                break\n            subfigendloc += 1\n        if (subfigendloc == len(node.parent.children)):\n            return\n        between_nodes = node.parent.children[parentloc:subfigendloc]\n        subfigend_node = node.parent.children[subfigendloc]\n        node['subfigend'] = subfigend_node\n        for (i, n) in enumerate(between_nodes):\n            if isinstance(n, nodes.figure):\n                children = [n]\n                prevnode = between_nodes[(i - 1)]\n                if isinstance(prevnode, nodes.target):\n                    node.parent.children.remove(prevnode)\n                    children.insert(0, prevnode)\n                nodeloc = node.parent.children.index(n)\n                node.parent.children[nodeloc] = subfig('', *children)\n                node.parent.children[nodeloc]['width'] = subfigend_node['width']\n                node.parent.children[nodeloc]['mainfigid'] = subfigend_node['ids'][0]\n", "label": 1}
{"function": "\n\ndef _new_actor(self, actor_type, actor_id=None, credentials=None):\n    \"Return a 'bare' actor of actor_type, raises an exception on failure.\"\n    if (credentials is not None):\n        sec = Security()\n        sec.set_principal(credentials)\n        sec.authenticate_principal()\n    else:\n        sec = None\n    (found, is_primitive, class_) = ActorStore(security=sec).lookup(actor_type)\n    if (not found):\n        _log.analyze(self.node.id, '+ NOT FOUND CREATE SHADOW ACTOR', {\n            'class': class_,\n        })\n        found = True\n        is_primitive = True\n        class_ = ShadowActor\n    if ((not found) or (not is_primitive)):\n        _log.error(('Requested actor %s is not available' % actor_type))\n        raise Exception('ERROR_NOT_FOUND')\n    try:\n        a = class_(actor_type, actor_id=actor_id)\n    except Exception as e:\n        _log.exception('')\n        _log.error((\"The actor %s(%s) can't be instantiated.\" % (actor_type, class_.__init__)))\n        raise e\n    try:\n        a.set_credentials(credentials, security=sec)\n        a._calvinsys = self.node.calvinsys()\n        a.check_requirements()\n    except Exception as e:\n        _log.exception('Catched new from state')\n        _log.analyze(self.node.id, '+ FAILED REQS CREATE SHADOW ACTOR', {\n            'class': class_,\n        })\n        a = ShadowActor(actor_type, actor_id=actor_id)\n        a.set_credentials(credentials, security=sec)\n        a._calvinsys = self.node.calvinsys()\n    return a\n", "label": 1}
{"function": "\n\ndef test_fermionoperator():\n    c = FermionOp('c')\n    d = FermionOp('d')\n    assert isinstance(c, FermionOp)\n    assert isinstance(Dagger(c), FermionOp)\n    assert c.is_annihilation\n    assert (not Dagger(c).is_annihilation)\n    assert (FermionOp('c') == FermionOp('c'))\n    assert (FermionOp('c') != FermionOp('d'))\n    assert (FermionOp('c', True) != FermionOp('c', False))\n    assert (AntiCommutator(c, Dagger(c)).doit() == 1)\n    assert (AntiCommutator(c, Dagger(d)).doit() == ((c * Dagger(d)) + (Dagger(d) * c)))\n", "label": 1}
{"function": "\n\ndef _fetch(self, model_cls, query=None, sort=None):\n    'Fetch the objects of type `model_cls` matching the given\\n        query. The query may be given as a string, string sequence, a\\n        Query object, or None (to fetch everything). `sort` is an\\n        `Sort` object.\\n        '\n    query = (query or TrueQuery())\n    sort = (sort or NullSort())\n    (where, subvals) = query.clause()\n    order_by = sort.order_clause()\n    sql = 'SELECT * FROM {0} WHERE {1} {2}'.format(model_cls._table, (where or '1'), ('ORDER BY {0}'.format(order_by) if order_by else ''))\n    with self.transaction() as tx:\n        rows = tx.query(sql, subvals)\n    return Results(model_cls, rows, self, (None if where else query), (sort if sort.is_slow() else None))\n", "label": 1}
{"function": "\n\ndef check(self):\n    if (not self.graph.is_acyclic()):\n        err = 'Graph cannot be processed because it contains cycles in it:'\n        err += ', '.join(six.moves.map(str, nx.simple_cycles(nx.DiGraph(self.graph))))\n        err += '\\n'\n        raise errors.InvalidData(err)\n    non_existing_tasks = []\n    invalid_tasks = []\n    for (node_key, node_value) in six.iteritems(self.graph.node):\n        if (not node_value.get('id')):\n            successors = self.graph.successors(node_key)\n            predecessors = self.graph.predecessors(node_key)\n            neighbors = (successors + predecessors)\n            non_existing_tasks.append(node_key)\n            invalid_tasks.extend(neighbors)\n    if non_existing_tasks:\n        raise errors.InvalidData(\"Tasks '{non_existing_tasks}' can't be in requires|required_for|groups|tasks for [{invalid_tasks}] because they don't exist in the graph\".format(non_existing_tasks=', '.join((str(x) for x in sorted(non_existing_tasks))), invalid_tasks=', '.join((str(x) for x in sorted(set(invalid_tasks))))))\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    r = []\n    try:\n        for el in self:\n            c = el.get('class')\n            c = ((c and ('.' + '.'.join(c.split(' ')))) or '')\n            id = el.get('id')\n            id = ((id and ('#' + id)) or '')\n            r.append(('<%s%s%s>' % (el.tag, id, c)))\n        return (('[' + ', '.join(r)) + ']')\n    except AttributeError:\n        if PY3k:\n            return list.__repr__(self)\n        else:\n            for el in self:\n                if isinstance(el, unicode):\n                    r.append(el.encode('utf-8'))\n                else:\n                    r.append(el)\n            return repr(r)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef find_parent_scope(block):\n    '\\n        Find parent scope, if the block is not a fold trigger.\\n\\n        '\n    original = block\n    if (not TextBlockHelper.is_fold_trigger(block)):\n        while ((block.text().strip() == '') and block.isValid()):\n            block = block.next()\n        ref_lvl = (TextBlockHelper.get_fold_lvl(block) - 1)\n        block = original\n        while (block.blockNumber() and ((not TextBlockHelper.is_fold_trigger(block)) or (TextBlockHelper.get_fold_lvl(block) > ref_lvl))):\n            block = block.previous()\n    return block\n", "label": 1}
{"function": "\n\ndef _configure_port_entries(self, port, vlan_id, device_id, host_id, vni, is_provider_vlan):\n    'Create a nexus switch entry.\\n\\n        if needed, create a VLAN in the appropriate switch or port and\\n        configure the appropriate interfaces for this VLAN.\\n\\n        Called during update postcommit port event.\\n        '\n    connections = self._get_active_port_connections(port, host_id)\n    vlan_already_created = []\n    starttime = time.time()\n    for (switch_ip, intf_type, nexus_port, is_native) in connections:\n        all_bindings = nxos_db.get_nexusvlan_binding(vlan_id, switch_ip)\n        previous_bindings = [row for row in all_bindings if (row.instance_id != device_id)]\n        if (previous_bindings and (switch_ip in vlan_already_created)):\n            duplicate_type = const.DUPLICATE_VLAN\n        else:\n            vlan_already_created.append(switch_ip)\n            duplicate_type = const.NO_DUPLICATE\n        port_starttime = time.time()\n        try:\n            self._configure_port_binding(is_provider_vlan, duplicate_type, is_native, switch_ip, vlan_id, intf_type, nexus_port, vni)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                self.driver.capture_and_print_timeshot(port_starttime, 'port_configerr', switch=switch_ip)\n                self.driver.capture_and_print_timeshot(starttime, 'configerr', switch=switch_ip)\n        self.driver.capture_and_print_timeshot(port_starttime, 'port_config', switch=switch_ip)\n    self.driver.capture_and_print_timeshot(starttime, 'config')\n", "label": 1}
{"function": "\n\ndef _create_initial_state(self, ip, jumpkind):\n    '\\n        Obtain a SimState object for a specific address\\n\\n        Fastpath means the CFG generation will work in an IDA-like way, in which it will not try to execute every\\n        single statement in the emulator, but will just do the decoding job. This is much faster than the old way.\\n\\n        :param int ip: The instruction pointer\\n        :param str jumpkind: The jumpkind upon executing the block\\n        :return: The newly-generated state\\n        :rtype: simuvex.SimState\\n        '\n    jumpkind = ('Ijk_Boring' if (jumpkind is None) else jumpkind)\n    if (self._initial_state is None):\n        state = self.project.factory.entry_state(addr=ip, mode='fastpath')\n    else:\n        state = self._initial_state\n        state.scratch.jumpkind = jumpkind\n        state.set_mode('fastpath')\n        state.ip = state.se.BVV(ip, self.project.arch.bits)\n    if (jumpkind is not None):\n        state.scratch.jumpkind = jumpkind\n    state_info = None\n    if ((ip is not None) and (self.project.arch.name in ('MIPS32', 'MIPS64'))):\n        state_info = {\n            't9': state.se.BVV(ip, self.project.arch.bits),\n        }\n    elif ((ip is not None) and (self.project.arch.name == 'PPC64')):\n        state_info = {\n            'r2': state.registers.load('r2'),\n        }\n    state = self.project.arch.prepare_state(state, state_info)\n    return state\n", "label": 1}
{"function": "\n\ndef load_middleware(self):\n    '\\n        Populate middleware lists from settings.MIDDLEWARE_CLASSES.\\n\\n        Must be called after the environment is fixed (see __call__ in subclasses).\\n        '\n    self._view_middleware = []\n    self._template_response_middleware = []\n    self._response_middleware = []\n    self._exception_middleware = []\n    request_middleware = []\n    for middleware_path in settings.MIDDLEWARE_CLASSES:\n        mw_class = import_string(middleware_path)\n        try:\n            mw_instance = mw_class()\n        except MiddlewareNotUsed as exc:\n            if settings.DEBUG:\n                if six.text_type(exc):\n                    logger.debug('MiddlewareNotUsed(%r): %s', middleware_path, exc)\n                else:\n                    logger.debug('MiddlewareNotUsed: %r', middleware_path)\n            continue\n        if hasattr(mw_instance, 'process_request'):\n            request_middleware.append(mw_instance.process_request)\n        if hasattr(mw_instance, 'process_view'):\n            self._view_middleware.append(mw_instance.process_view)\n        if hasattr(mw_instance, 'process_template_response'):\n            self._template_response_middleware.insert(0, mw_instance.process_template_response)\n        if hasattr(mw_instance, 'process_response'):\n            self._response_middleware.insert(0, mw_instance.process_response)\n        if hasattr(mw_instance, 'process_exception'):\n            self._exception_middleware.insert(0, mw_instance.process_exception)\n    self._request_middleware = request_middleware\n", "label": 1}
{"function": "\n\n@luminosity.setter\ndef luminosity(self, value):\n    if (value is not None):\n        if is_numpy_array(value):\n            if (value.ndim != 1):\n                raise ValueError('luminosity should be a 1-D array')\n            if (not np.all((value > 0.0))):\n                raise ValueError('luminosity should be positive')\n            if ((self.position is not None) and (value.shape[0] != self.position.shape[0])):\n                raise ValueError('luminosity should be a 1-D array with the same number of rows as position')\n        else:\n            raise ValueError('luminosity should be a Numpy array')\n    self._luminosity = value\n", "label": 1}
{"function": "\n\ndef add_spatial_info(features, add_x=True, add_y=True, inplace=False, dtype=None):\n    \"\\n    Adds spatial information to image features (which should contain a frames\\n    attribute in the format created by extract_image_features).\\n\\n    Adds a feature for x (if add_x) and y (if add_y), which are relative (x, y)\\n    locations within the image of the feature between 0 and 1 (inclusive).\\n\\n    Returns a new Features object with these additional features, or modifies\\n    features and returns None if inplace is True.\\n\\n    If dtype is not None, the resulting array will have that dtype. Otherwise,\\n    it will maintain features.dtype if it's a float type, or float32 if not.\\n    \"\n    if ((not add_x) and (not add_y)):\n        return (None if inplace else features)\n    indices = []\n    if add_x:\n        indices.append(0)\n    if add_y:\n        indices.append(1)\n    if (dtype is None):\n        dtype = features.dtype\n        if (dtype.kind != 'f'):\n            dtype = np.float32\n    spatial = np.asarray(np.vstack(features.frames)[:, indices], dtype=dtype)\n    spatial /= spatial.max(axis=0)\n    new_feats = np.hstack((features._features, spatial))\n    if inplace:\n        features._features = new_feats\n        features._refresh_features()\n    else:\n        return Features(new_feats, n_pts=features._n_pts, categories=features.categories, names=features.names, **dict(((k, features.data[k]) for k in features._extra_names)))\n", "label": 1}
{"function": "\n\ndef output_dim(self, X, S, padding, strides, pooling=False):\n    '\\n        compute along 1 dimension, with these sizes, what will be the output dimension\\n\\n        Arguments:\\n            X (int): input data dimension\\n            S (int): filter dimension\\n            padding (int): padding on each side\\n            strides (int): striding\\n            pooling (bool): flag for setting pooling layer size\\n        '\n    if (self.check_caffe_compat() and pooling):\n        size = (int(ceil((float(((X - S) + (2 * padding))) / strides))) + 1)\n        if ((padding > 0) and (((size - 1) * strides) >= (X + padding))):\n            size -= 1\n    else:\n        size = ((((X - S) + (2 * padding)) / strides) + 1)\n    if (pooling and (padding >= S)):\n        raise ValueError(('Padding dim %d incompatible with filter size %d' % (padding, S)))\n    return size\n", "label": 1}
{"function": "\n\ndef lookup(self, *features):\n    if (len(self.builders) == 0):\n        return None\n    if (len(features) == 0):\n        return self.builders[0]\n    features = list(features)\n    features.reverse()\n    candidates = None\n    candidate_set = None\n    while (len(features) > 0):\n        feature = features.pop()\n        we_have_the_feature = self.builders_for_feature.get(feature, [])\n        if (len(we_have_the_feature) > 0):\n            if (candidates is None):\n                candidates = we_have_the_feature\n                candidate_set = set(candidates)\n            else:\n                candidate_set = candidate_set.intersection(set(we_have_the_feature))\n    if (candidate_set is None):\n        return None\n    for candidate in candidates:\n        if (candidate in candidate_set):\n            return candidate\n    return None\n", "label": 1}
{"function": "\n\ndef salvage_broken_user_settings_document(document):\n    if ((not document['access_token']) or (not document['dropbox_id'])):\n        return False\n    if ((not document['owner']) or (not User.load(document['owner']).is_active)):\n        return False\n    if document['deleted']:\n        return False\n    if ((not document.get('dropbox_info')) or (not document['dropbox_info']['display_name'])):\n        logger.info('Attempting dropbox_info population for document (id:{0})'.format(document['_id']))\n        client = DropboxClient(document['access_token'])\n        document['dropbox_info'] = {\n            \n        }\n        try:\n            database['dropboxusersettings'].find_and_modify({\n                '_id': document['_id'],\n            }, {\n                '$set': {\n                    'dropbox_info': client.account_info(),\n                },\n            })\n        except Exception:\n            return True\n        else:\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef __combineVarStatements(node):\n    'Top level method called to optimize a script node'\n    if (len(node.scope.declared) == 0):\n        return\n    firstVar = __findFirstVarStatement(node)\n    if (not firstVar):\n        firstVar = Node.Node(None, 'var')\n        node.insert(0, firstVar)\n    __patchVarStatements(node, firstVar)\n    __cleanFirst(firstVar)\n    if (len(firstVar) == 0):\n        firstVar.parent.remove(firstVar)\n    else:\n        firstVarParent = firstVar.parent\n        firstVarPos = firstVarParent.index(firstVar)\n        if (len(firstVarParent) > (firstVarPos + 1)):\n            possibleForStatement = firstVarParent[(firstVarPos + 1)]\n            if ((possibleForStatement.type == 'for') and (not hasattr(possibleForStatement, 'setup'))):\n                possibleForStatement.append(firstVar, 'setup')\n", "label": 1}
{"function": "\n\ndef finalize(genomes, env):\n    'Provide symlinks back to reference genomes so tophat avoids generating FASTA genomes.\\n    '\n    genome_dir = os.path.join(env.data_files, 'genomes')\n    for (orgname, gid, manager) in genomes:\n        org_dir = os.path.join(genome_dir, orgname)\n        for aligner in ['bowtie', 'bowtie2']:\n            aligner_dir = os.path.join(org_dir, gid, aligner)\n            if env.safe_exists(aligner_dir):\n                with cd(aligner_dir):\n                    for ext in ['', '.fai']:\n                        orig_seq = os.path.join(os.pardir, 'seq', ('%s.fa%s' % (gid, ext)))\n                        if (env.safe_exists(orig_seq) and (not env.safe_exists(os.path.basename(orig_seq)))):\n                            env.safe_run(('ln -sf %s' % orig_seq))\n", "label": 1}
{"function": "\n\n@login_required\ndef profileupdaterequest_details(request, request_id):\n    update_request = get_object_or_404(ProfileUpdateRequest, pk=request_id)\n    person_selected = False\n    person = None\n    form = None\n    try:\n        person = Person.objects.get(email=update_request.email)\n    except Person.DoesNotExist:\n        try:\n            person = Person.objects.get(personal=update_request.personal, family=update_request.family)\n        except (Person.DoesNotExist, Person.MultipleObjectsReturned):\n            try:\n                form = PersonLookupForm(request.GET)\n                person = Person.objects.get(pk=int(request.GET['person_1']))\n                person_selected = True\n            except KeyError:\n                person = None\n                form = PersonLookupForm()\n            except (ValueError, Person.DoesNotExist):\n                person = None\n    if person:\n        person.has_instructor_badge = Award.objects.filter(badge__in=Badge.objects.instructor_badges(), person=person).exists()\n    try:\n        airport = Airport.objects.get(iata=update_request.airport_iata)\n    except Airport.DoesNotExist:\n        airport = None\n    context = {\n        'title': 'Instructor profile update request #{}'.format(update_request.pk),\n        'new': update_request,\n        'old': person,\n        'person_form': form,\n        'person_selected': person_selected,\n        'form_helper': bootstrap_helper_get,\n        'airport': airport,\n    }\n    return render(request, 'workshops/profileupdaterequest.html', context)\n", "label": 1}
{"function": "\n\n@login_required\n@transaction.atomic\ndef delete_posts(request, topic_id):\n    topic = Topic.objects.select_related().get(pk=topic_id)\n    if forum_moderated_by(topic, request.user):\n        deleted = False\n        post_list = request.POST.getlist('post')\n        for post_id in post_list:\n            if (not deleted):\n                deleted = True\n            delete_post(request, post_id)\n        if deleted:\n            messages.success(request, _('Post deleted.'))\n            return HttpResponseRedirect(topic.get_absolute_url())\n    last_post = topic.posts.latest()\n    if request.user.is_authenticated():\n        topic.update_read(request.user)\n    posts = topic.posts.all().select_related()\n    initial = {\n        \n    }\n    if request.user.is_authenticated():\n        initial = {\n            'markup': request.user.forum_profile.markup,\n        }\n    form = AddPostForm(topic=topic, initial=initial)\n    moderator = (request.user.is_superuser or (request.user in topic.forum.moderators.all()))\n    if (request.user.is_authenticated() and (request.user in topic.subscribers.all())):\n        subscribed = True\n    else:\n        subscribed = False\n    return render(request, 'djangobb_forum/delete_posts.html', {\n        'topic': topic,\n        'last_post': last_post,\n        'form': form,\n        'moderator': moderator,\n        'subscribed': subscribed,\n        'posts_page': get_page(posts, request, forum_settings.TOPIC_PAGE_SIZE),\n    })\n", "label": 1}
{"function": "\n\ndef write_csv(outfile, data):\n    'generate csv output'\n    output = []\n    headers = []\n    keys = ['uu', 'us', 'upl', 'ueid', 'peid', 'uipl', 'pid', 'pmrp', 'ptrp', 'upa', 'pda', 'ut']\n    for k in keys:\n        try:\n            headers.append(FIELDS[k]['human'])\n        except:\n            headers.append(k)\n    output.append(headers)\n    for record in data:\n        line = []\n        try:\n            for k in keys:\n                if (k in record):\n                    line.append(record[k])\n                else:\n                    line.append('')\n            output.append(line)\n        except:\n            output.append(['Error parsing line.'])\n    writer = csv.writer(outfile, delimiter=',', quotechar='\"', dialect='excel', quoting=csv.QUOTE_ALL)\n    writer.writerows(output)\n", "label": 1}
{"function": "\n\n@data.setter\ndef data(self, data):\n    numpy = import_module('numpy')\n    data = _TensorDataLazyEvaluator.parse_data(data)\n    if (data.ndim > 2):\n        raise ValueError('data have to be of rank 1 (diagonal metric) or 2.')\n    if (data.ndim == 1):\n        if (self.dim is not None):\n            nda_dim = data.shape[0]\n            if (nda_dim != self.dim):\n                raise ValueError('Dimension mismatch')\n        dim = data.shape[0]\n        newndarray = numpy.zeros((dim, dim), dtype=object)\n        for (i, val) in enumerate(data):\n            newndarray[(i, i)] = val\n        data = newndarray\n    (dim1, dim2) = data.shape\n    if (dim1 != dim2):\n        raise ValueError('Non-square matrix tensor.')\n    if (self.dim is not None):\n        if (self.dim != dim1):\n            raise ValueError('Dimension mismatch')\n    _tensor_data_substitution_dict[self] = data\n    _tensor_data_substitution_dict.add_metric_data(self.metric, data)\n    delta = self.get_kronecker_delta()\n    i1 = TensorIndex('i1', self)\n    i2 = TensorIndex('i2', self)\n    delta(i1, (- i2)).data = _TensorDataLazyEvaluator.parse_data(eye(dim1))\n", "label": 1}
{"function": "\n\ndef render(self, context):\n    generator_id = (self._generator_id.resolve(context) if self._generator_id else DEFAULT_THUMBNAIL_GENERATOR)\n    dimensions = parse_dimensions(self._dimensions.resolve(context))\n    kwargs = dict(((k, v.resolve(context)) for (k, v) in self._generator_kwargs.items()))\n    kwargs['source'] = self._source.resolve(context)\n    kwargs.update(dimensions)\n    generator = generator_registry.get(generator_id, **kwargs)\n    file = ImageCacheFile(generator)\n    attrs = dict(((k, v.resolve(context)) for (k, v) in self._html_attrs.items()))\n    if ((not ('width' in attrs)) and (not ('height' in attrs))):\n        attrs.update(width=file.width, height=file.height)\n    attrs['src'] = file.url\n    attr_str = ' '.join((('%s=\"%s\"' % (escape(k), escape(v))) for (k, v) in attrs.items()))\n    return mark_safe(('<img %s />' % attr_str))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef budget_monitoring_percentage(row):\n    '\\n            Virtual Field to show the percentage used of the Budget\\n        '\n    if hasattr(row, 'budget_monitoring'):\n        row = row.budget_monitoring\n    if hasattr(row, 'planned'):\n        planned = row.planned\n        if (planned == 0.0):\n            return current.messages['NONE']\n    else:\n        planned = None\n    if hasattr(row, 'value'):\n        actual = row.value\n    else:\n        actual = None\n    if ((planned is not None) and (actual is not None)):\n        percentage = ((actual / planned) * 100)\n        return ('%s %%' % '{0:.2f}'.format(percentage))\n    if hasattr(row, 'id'):\n        table = current.s3db.budget_monitoring\n        r = current.db((table.id == row.id)).select(table.planned, table.value, limitby=(0, 1)).first()\n        if r:\n            planned = r.planned\n            if (planned == 0.0):\n                return current.messages['NONE']\n            percentage = ((r.value / planned) * 100)\n            return ('%s %%' % percentage)\n    return current.messages['NONE']\n", "label": 1}
{"function": "\n\ndef run(self, sync):\n    app = sync.app\n    with app.db.getSession() as session:\n        for c in session.getPendingTopics():\n            sync.submitTask(SetTopicTask(c.key, self.priority))\n        for c in session.getPendingRebases():\n            sync.submitTask(RebaseChangeTask(c.key, self.priority))\n        for c in session.getPendingStatusChanges():\n            sync.submitTask(ChangeStatusTask(c.key, self.priority))\n        for c in session.getPendingStarred():\n            sync.submitTask(ChangeStarredTask(c.key, self.priority))\n        for c in session.getPendingCherryPicks():\n            sync.submitTask(SendCherryPickTask(c.key, self.priority))\n        for r in session.getPendingCommitMessages():\n            sync.submitTask(ChangeCommitMessageTask(r.key, self.priority))\n        for m in session.getPendingMessages():\n            sync.submitTask(UploadReviewTask(m.key, self.priority))\n", "label": 1}
{"function": "\n\ndef _InvokeGitkitApi(self, method, params=None, need_service_account=True):\n    'Invokes Gitkit API, with optional access token for service account.\\n\\n    Args:\\n      method: string, the api method name.\\n      params: dict of optional parameters for the API.\\n      need_service_account: false if service account is not needed.\\n\\n    Raises:\\n      GitkitClientError: if the request is bad.\\n      GitkitServerError: if Gitkit can not handle the request.\\n\\n    Returns:\\n      API response as dict.\\n    '\n    body = (simplejson.dumps(params) if params else None)\n    req = urllib_request.Request((self.google_api_url + method))\n    req.add_header('Content-type', 'application/json')\n    if need_service_account:\n        if self.credentials:\n            access_token = self.credentials.get_access_token().access_token\n        elif (self.service_account_email and self.service_account_key):\n            access_token = self._GetAccessToken()\n        else:\n            raise errors.GitkitClientError('Missing service account credentials')\n        req.add_header('Authorization', ('Bearer ' + access_token))\n    try:\n        binary_body = (body.encode('utf-8') if body else None)\n        raw_response = urllib_request.urlopen(req, binary_body).read()\n    except urllib_request.HTTPError as err:\n        if (err.code == 400):\n            raw_response = err.read()\n        else:\n            raise\n    return self._CheckGitkitError(raw_response)\n", "label": 1}
{"function": "\n\ndef __setupConnections(self, reuseUntil=None):\n    if (self.__graphComponent is None):\n        self.__connections = []\n        return\n    updatedConnections = []\n    n = 0\n    g = self.__graphComponent\n    reuse = (reuseUntil is not None)\n    while ((g is not None) and (n < self.__numComponents)):\n        if reuse:\n            updatedConnections.extend(self.__connections[(n * 2):((n * 2) + 2)])\n        else:\n            updatedConnections.append(g.nameChangedSignal().connect(Gaffer.WeakMethod(self.__setText)))\n            if (n < (self.__numComponents - 1)):\n                updatedConnections.append(g.parentChangedSignal().connect(Gaffer.WeakMethod(self.__parentChanged)))\n        if g.isSame(reuseUntil):\n            reuse = False\n        g = g.parent()\n        n += 1\n    self.__connections = updatedConnections\n", "label": 1}
{"function": "\n\ndef nome(ctx, m):\n    m = ctx.convert(m)\n    if (not m):\n        return m\n    if (m == ctx.one):\n        return m\n    if ctx.isnan(m):\n        return m\n    if ctx.isinf(m):\n        if (m == ctx.ninf):\n            return type(m)((- 1))\n        else:\n            return ctx.mpc((- 1))\n    a = ctx.ellipk((ctx.one - m))\n    b = ctx.ellipk(m)\n    v = ctx.exp((((- ctx.pi) * a) / b))\n    if ((not ctx._im(m)) and (ctx._re(m) < 1)):\n        if ctx._is_real_type(m):\n            return v.real\n        else:\n            return (v.real + 0j)\n    elif (m == 2):\n        v = ctx.mpc(0, v.imag)\n    return v\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _tensAdd_check_automatrix(args):\n    if (not args):\n        return args\n    auto_left_types = set([])\n    auto_right_types = set([])\n    args_auto_left_types = []\n    args_auto_right_types = []\n    for (i, arg) in enumerate(args):\n        arg_auto_left_types = set([])\n        arg_auto_right_types = set([])\n        for index in get_indices(arg):\n            if (index in (index._tensortype.auto_left, (- index._tensortype.auto_left))):\n                auto_left_types.add(index._tensortype)\n                arg_auto_left_types.add(index._tensortype)\n            if (index in (index._tensortype.auto_right, (- index._tensortype.auto_right))):\n                auto_right_types.add(index._tensortype)\n                arg_auto_right_types.add(index._tensortype)\n        args_auto_left_types.append(arg_auto_left_types)\n        args_auto_right_types.append(arg_auto_right_types)\n    for (arg, aas_left, aas_right) in zip(args, args_auto_left_types, args_auto_right_types):\n        missing_left = (auto_left_types - aas_left)\n        missing_right = (auto_right_types - aas_right)\n        missing_intersection = (missing_left & missing_right)\n        for j in missing_intersection:\n            args[i] *= j.delta(j.auto_left, (- j.auto_right))\n        if (missing_left != missing_right):\n            raise ValueError('cannot determine how to add auto-matrix indices on some args')\n    return args\n", "label": 1}
{"function": "\n\ndef getValueAt(self, rowIndex, columnIndex):\n    messageEntry = self._db.getMessageByRow(rowIndex)\n    if messageEntry:\n        if (columnIndex == 0):\n            return str(messageEntry.getTableRow())\n        elif (columnIndex == 1):\n            return messageEntry._name\n        elif (columnIndex == 2):\n            return messageEntry._successRegex\n        else:\n            roleEntry = self._db.getRoleByMessageTableColumn(columnIndex)\n            if roleEntry:\n                roleIndex = roleEntry._index\n                return ((roleIndex in messageEntry._roles) and messageEntry._roles[roleIndex])\n    return ''\n", "label": 1}
{"function": "\n\ndef __init__(self, *args):\n    if (len(args) == 0):\n        self._empty = True\n        self._head = None\n        self._tail = None\n    elif ((len(args) == 1) and isinstance(args[0], (list, tuple))):\n        if (len(args[0]) == 0):\n            self._empty = True\n            self._head = None\n            self._tail = None\n        else:\n            self._head = args[0][0]\n            if (len(args[0]) > 1):\n                self._tail = ImmutableList(args[0][1:])\n            else:\n                self._tail = ImmutableList()\n            self._empty = False\n    elif ((len(args) == 2) and isinstance(args[1], ImmutableList)):\n        self._head = args[0]\n        self._tail = args[1]\n        self._empty = False\n", "label": 1}
{"function": "\n\ndef rectangularize_featureset(featureset):\n    'Convert xarray.Dataset into (2d) Pandas.DataFrame for use with sklearn.'\n    featureset = featureset.drop([coord for coord in ['target', 'class'] if (coord in featureset)])\n    feature_df = featureset.to_dataframe()\n    if ('channel' in featureset):\n        feature_df = feature_df.unstack(level='channel')\n        if (len(featureset.channel) == 1):\n            feature_df.columns = [pair[0] for pair in feature_df.columns]\n        else:\n            feature_df.columns = ['_'.join([str(el) for el in pair]) for pair in feature_df.columns]\n    return feature_df.loc[featureset.name]\n", "label": 1}
{"function": "\n\ndef recv_frame(self):\n    '\\n        recieve data as frame from server.\\n\\n        return value: ABNF frame object.\\n        '\n    header_bytes = self._recv_strict(2)\n    if (not header_bytes):\n        return None\n    b1 = header_bytes[0]\n    fin = ((b1 >> 7) & 1)\n    rsv1 = ((b1 >> 6) & 1)\n    rsv2 = ((b1 >> 5) & 1)\n    rsv3 = ((b1 >> 4) & 1)\n    opcode = (b1 & 15)\n    b2 = header_bytes[1]\n    mask = ((b2 >> 7) & 1)\n    length = (b2 & 127)\n    length_data = b''\n    if (length == 126):\n        length_data = self._recv_strict(2)\n        length = struct.unpack('!H', length_data)[0]\n    elif (length == 127):\n        length_data = self._recv_strict(8)\n        length = struct.unpack('!Q', length_data)[0]\n    mask_key = b''\n    if mask:\n        mask_key = self._recv_strict(4)\n    data = self._recv_strict(length)\n    if traceEnabled:\n        recieved = (((header_bytes + length_data) + mask_key) + data)\n        logger.debug(('recv: ' + repr(recieved)))\n    if mask:\n        data = ABNF.mask(mask_key, data)\n    frame = ABNF(fin, rsv1, rsv2, rsv3, opcode, mask, data)\n    return frame\n", "label": 1}
{"function": "\n\ndef check(self, instance):\n    btrfs_devices = {\n        \n    }\n    excluded_devices = instance.get('excluded_devices', [])\n    for p in psutil.disk_partitions():\n        if ((p.fstype == 'btrfs') and (p.device not in btrfs_devices) and (p.device not in excluded_devices)):\n            btrfs_devices[p.device] = p.mountpoint\n    if (len(btrfs_devices) == 0):\n        raise Exception('No btrfs device found')\n    for (device, mountpoint) in btrfs_devices.iteritems():\n        for (flags, total_bytes, used_bytes) in self.get_usage(mountpoint):\n            (replication_type, usage_type) = FLAGS_MAPPER[flags]\n            tags = ['usage_type:{0}'.format(usage_type), 'replication_type:{0}'.format(replication_type)]\n            free = (total_bytes - used_bytes)\n            usage = (float(free) / float(total_bytes))\n            self.gauge('system.disk.btrfs.total', total_bytes, tags=tags, device_name=device)\n            self.gauge('system.disk.btrfs.used', used_bytes, tags=tags, device_name=device)\n            self.gauge('system.disk.btrfs.free', free, tags=tags, device_name=device)\n            self.gauge('system.disk.btrfs.usage', usage, tags=tags, device_name=device)\n", "label": 1}
{"function": "\n\ndef gpg_exception_factory(returncode, message):\n    if (returncode == 2):\n        if (b'decryption failed: bad key' in message):\n            return IncorrectPasswordException(message)\n        if (b'CRC error;' in message):\n            return FileCorruptionException(message)\n        if (b'fatal: zlib inflate problem: invalid distance' in message):\n            return FileCorruptionException(message)\n        if (b'decryption failed: invalid packet' in message):\n            return FileCorruptionException(message)\n        if b'no valid OpenPGP data found':\n            return InvalidEncryptedFileException(message)\n    return Exception('unkown error', returncode, message)\n", "label": 1}
{"function": "\n\ndef setPosition(self, value):\n    if ((value is None) or (value == '')):\n        return\n    posisize = value.split(' ')\n    textpos = posisize[0]\n    if (textpos.find('%') == (- 1)):\n        if (textpos == 'sub'):\n            self.superscript = False\n            self.subscript = True\n        elif (textpos == 'super'):\n            self.superscript = True\n            self.subscript = False\n    else:\n        itextpos = int(textpos[:textpos.find('%')])\n        if (itextpos > 10):\n            self.superscript = False\n            self.subscript = True\n        elif (itextpos < (- 10)):\n            self.superscript = True\n            self.subscript = False\n", "label": 1}
{"function": "\n\ndef _add_value_to_fields_in_project(self, project_id):\n    for field in self._get_fields_with_values(project_id):\n        field_name = self._get_field_name(field[NAME], project_id)\n        pcf = self._target.getProjectCustomField(project_id, field_name)\n        if hasattr(pcf, 'bundle'):\n            field_type = pcf.type[0:(- 3)]\n            bundle = self._target.getBundle(field_type, pcf.bundle)\n            yt_values = [v for v in [field['converter'](value, bundle, (lambda name, value_name: self._import_config.get_field_value(name, field_type, value_name))) for value in field['values']] if len(v)]\n            for value in yt_values:\n                self._target.addValueToBundle(bundle, value)\n", "label": 1}
{"function": "\n\ndef extract_test_sentences(string, comment_chars='#%;', encoding=None):\n    '\\n    Parses a string with one test sentence per line.\\n    Lines can optionally begin with:\\n      - a bool, saying if the sentence is grammatical or not, or\\n      - an int, giving the number of parse trees is should have,\\n    The result information is followed by a colon, and then the sentence.\\n    Empty lines and lines beginning with a comment char are ignored.\\n\\n    :return: a list of tuple of sentences and expected results,\\n        where a sentence is a list of str,\\n        and a result is None, or bool, or int\\n\\n    :param comment_chars: ``str`` of possible comment characters.\\n    :param encoding: the encoding of the string, if it is binary\\n    '\n    if (encoding is not None):\n        string = string.decode(encoding)\n    sentences = []\n    for sentence in string.split('\\n'):\n        if ((sentence == '') or (sentence[0] in comment_chars)):\n            continue\n        split_info = sentence.split(':', 1)\n        result = None\n        if (len(split_info) == 2):\n            if (split_info[0] in ['True', 'true', 'False', 'false']):\n                result = (split_info[0] in ['True', 'true'])\n                sentence = split_info[1]\n            else:\n                result = int(split_info[0])\n                sentence = split_info[1]\n        tokens = sentence.split()\n        if (tokens == []):\n            continue\n        sentences += [(tokens, result)]\n    return sentences\n", "label": 1}
{"function": "\n\ndef _populate_vars(self):\n    if ('variables' in self.config.keys()):\n        for (var, question) in self.config['variables'].items():\n            if (self.app.pargs.defaults is True):\n                try:\n                    res = self.app.config.get('answers', var)\n                except ConfigParser.NoOptionError as e:\n                    res = 'MISSING VARIABLE'\n            elif (var.lower() in self.app.config.keys('answers')):\n                default = self.app.config.get('answers', var.lower())\n                res = input(('%s: [%s] ' % (question, default)))\n                if (len(res) == 0):\n                    res = default\n            else:\n                res = input(('%s: ' % question))\n            self._vars[var] = res.strip()\n", "label": 1}
{"function": "\n\ndef apply(self, fgraph):\n    '\\n        WRITEME\\n\\n        Applies each L{Optimizer} in self in turn.\\n\\n        '\n    l = []\n    if fgraph.profile:\n        validate_before = fgraph.profile.validate_time\n        sub_validate_time = [validate_before]\n    else:\n        sub_validate_time = []\n    callback_before = fgraph.execute_callbacks_time\n    nb_node_before = len(fgraph.apply_nodes)\n    sub_profs = []\n    for optimizer in self:\n        try:\n            t0 = time.time()\n            sub_prof = optimizer.optimize(fgraph)\n            l.append(float((time.time() - t0)))\n            sub_profs.append(sub_prof)\n            if fgraph.profile:\n                sub_validate_time.append(fgraph.profile.validate_time)\n        except AssertionError:\n            raise\n        except Exception as e:\n            if self.failure_callback:\n                self.failure_callback(e, self, optimizer)\n                continue\n            else:\n                raise\n    if fgraph.profile:\n        validate_time = (fgraph.profile.validate_time - validate_before)\n    else:\n        validate_time = None\n    callback_time = (fgraph.execute_callbacks_time - callback_before)\n    return (self, l, validate_time, callback_time, nb_node_before, len(fgraph.apply_nodes), sub_profs, sub_validate_time)\n", "label": 1}
{"function": "\n\ndef keyPressEvent(self, ev):\n    '\\n        Re-implemented to handle the user input a key at a time.\\n        \\n        @param ev key event (QKeyEvent)\\n        '\n    txt = ev.text()\n    key = ev.key()\n    ctrl = (ev.modifiers() & Qt.ControlModifier)\n    shift = (ev.modifiers() & Qt.ShiftModifier)\n    if (self.keymap.has_key(key) and (not shift) and (not ctrl)):\n        self.keymap[key]()\n    elif (ev == QtGui.QKeySequence.Paste):\n        self.paste()\n    elif (self.__isCursorOnLastLine() and txt.length()):\n        QsciScintilla.keyPressEvent(self, ev)\n        self.incrementalSearchActive = True\n        if (txt == '.'):\n            self.__showDynCompletion()\n    elif (ctrl or shift):\n        QsciScintilla.keyPressEvent(self, ev)\n    else:\n        ev.ignore()\n", "label": 1}
{"function": "\n\ndef iterbusinessdays(self, d1, d2):\n    '\\n        Date iterator returning dates in d1 <= x < d2, excluding weekends and holidays\\n        '\n    assert (d2 >= d1)\n    if ((d1.date() == d2.date()) and (d2.time() < self.business_hours[0])):\n        return\n    first = True\n    for dt in self.iterdays(d1, d2):\n        if (first and (d1.time() > self.business_hours[1])):\n            first = False\n            continue\n        first = False\n        if ((not self.isweekend(dt)) and (not self.isholiday(dt))):\n            (yield dt)\n", "label": 1}
{"function": "\n\ndef transaction(self, func, *watches, **kwargs):\n    \"\\n        Convenience method for executing the callable `func` as a transaction\\n        while watching all keys specified in `watches`. The 'func' callable\\n        should expect a single argument which is a Pipeline object.\\n        \"\n    shard_hint = kwargs.pop('shard_hint', None)\n    value_from_callable = kwargs.pop('value_from_callable', False)\n    watch_delay = kwargs.pop('watch_delay', None)\n    with self.pipeline(True, shard_hint) as pipe:\n        while 1:\n            try:\n                if watches:\n                    pipe.watch(*watches)\n                func_value = func(pipe)\n                exec_value = pipe.execute()\n                return (func_value if value_from_callable else exec_value)\n            except WatchError:\n                if ((watch_delay is not None) and (watch_delay > 0)):\n                    time.sleep(watch_delay)\n                continue\n", "label": 1}
{"function": "\n\ndef transform_Index(self, expr):\n    arr = self.transform_expr(expr.value)\n    idx = self.transform_expr(expr.index)\n    idx = self.assign_name(idx, 'idx')\n    arr_t = arr.type\n    if (arr_t.__class__ is PtrT):\n        assert isinstance(idx.type, IntT)\n        return expr\n    assert (arr_t.__class__ is ArrayT), ('Unexpected array %s : %s' % (arr, arr.type))\n    if self.is_tuple(idx):\n        indices = self.tuple_elts(idx)\n    else:\n        indices = [idx]\n    n_given = len(indices)\n    n_required = arr_t.rank\n    if (n_given < n_required):\n        extra_indices = ([syntax.helpers.slice_none] * (n_required - n_given))\n        indices.extend(extra_indices)\n    if syntax.helpers.all_scalars(indices):\n        data_ptr = self.attr(arr, 'data')\n        strides = self.attr(arr, 'strides')\n        offset_elts = self.attr(arr, 'offset')\n        for (i, idx_i) in enumerate(indices):\n            stride_i = self.tuple_proj(strides, i)\n            elts_i = self.mul(stride_i, idx_i, ('offset_elts_%d' % i))\n            offset_elts = self.add(offset_elts, elts_i, 'total_offset')\n        return self.index(data_ptr, offset_elts, temp=False)\n    else:\n        return self.array_slice(arr, indices)\n", "label": 1}
{"function": "\n\ndef test_scale():\n    (m, ctl, config) = init()\n    m.load(config)\n    pids = m.pids()\n    cmd = TestCommand('scale', ['dummy', 1])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids1 = m.pids()\n    cmd = TestCommand('scale', ['dummy', (- 1)])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids2 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '+4'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids3 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '-1'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids4 = m.pids()\n    cmd = TestCommand('scale', ['dummy', '=1'])\n    ctl.process_command(cmd)\n    time.sleep(0.1)\n    pids5 = m.pids()\n    m.stop()\n    m.run()\n    assert (len(pids) == 1)\n    assert (len(pids1) == 2)\n    assert (len(pids2) == 1)\n    assert (len(pids3) == 5)\n    assert (len(pids4) == 4)\n    assert (len(pids5) == 1)\n", "label": 1}
{"function": "\n\ndef test_load_from_entry_point():\n    from giblets.core import ComponentManager, Component, ExtensionPoint\n    from giblets.search import find_plugins_by_entry_point\n\n    class PluginFinder(Component):\n        found_plugins = ExtensionPoint(TestEggInterface)\n    mgr = ComponentManager()\n    pf = PluginFinder(mgr)\n    assert (len(pf.found_plugins) == 0)\n    find_plugins_by_entry_point('giblets_load_from_entry_point_test')\n    expected_plugins = ['TestEggPlugin1', 'TestEggPlugin2', 'TestEggPlugin3']\n    got_plugins = set()\n    assert (len(pf.found_plugins) == len(expected_plugins))\n    for plugin in pf.found_plugins:\n        plugin_name = plugin.__class__.__name__\n        assert (plugin_name in expected_plugins)\n        got_plugins.add(plugin_name)\n    for plugin_name in expected_plugins:\n        assert (plugin_name in got_plugins)\n", "label": 1}
{"function": "\n\ndef network(ip, netmask, gateway):\n    '\\n    Ensure the DRAC network settings are consistent\\n    '\n    ret = {\n        'name': ip,\n        'result': True,\n        'changes': {\n            \n        },\n        'comment': '',\n    }\n    current_network = __salt__['drac.network_info']()\n    new_network = {\n        \n    }\n    if (ip != current_network['IPv4 settings']['IP Address']):\n        ret['changes'].update({\n            'IP Address': {\n                'Old': current_network['IPv4 settings']['IP Address'],\n                'New': ip,\n            },\n        })\n    if (netmask != current_network['IPv4 settings']['Subnet Mask']):\n        ret['changes'].update({\n            'Netmask': {\n                'Old': current_network['IPv4 settings']['Subnet Mask'],\n                'New': netmask,\n            },\n        })\n    if (gateway != current_network['IPv4 settings']['Gateway']):\n        ret['changes'].update({\n            'Gateway': {\n                'Old': current_network['IPv4 settings']['Gateway'],\n                'New': gateway,\n            },\n        })\n    if __opts__['test']:\n        ret['result'] = None\n        return ret\n    if __salt__['drac.set_network'](ip, netmask, gateway):\n        if (not ret['changes']):\n            ret['comment'] = 'Network is in the desired state'\n        return ret\n    ret['result'] = False\n    ret['comment'] = 'unable to configure network'\n    return ret\n", "label": 1}
{"function": "\n\ndef contains(self, val):\n    '\\n        Check if given value or range is present.\\n\\n        Parameters\\n        ----------\\n        val : int or tuple or list or range\\n            Range or integer being checked.\\n\\n        Returns\\n        -------\\n        retlen : int\\n            Length of overlapping with `val` subranges.\\n        '\n    (start, end) = self.__val_convert(val)\n    retlen = 0\n    for r in self.__has:\n        if ((start < r[1]) and (end > r[0])):\n            retlen += ((((end < r[1]) and end) or r[1]) - (((start > r[0]) and start) or r[0]))\n    return retlen\n", "label": 1}
{"function": "\n\n@ignore_warnings\ndef test_warm_start():\n    (X, y) = (iris.data, iris.target)\n    solvers = ['newton-cg', 'sag']\n    if (sp_version >= (0, 12)):\n        solvers.append('lbfgs')\n    for warm_start in [True, False]:\n        for fit_intercept in [True, False]:\n            for solver in solvers:\n                for multi_class in ['ovr', 'multinomial']:\n                    clf = LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept)\n                    clf.fit(X, y)\n                    coef_1 = clf.coef_\n                    clf.max_iter = 1\n                    with ignore_warnings():\n                        clf.fit(X, y)\n                    cum_diff = np.sum(np.abs((coef_1 - clf.coef_)))\n                    msg = ('Warm starting issue with %s solver in %s mode with fit_intercept=%s and warm_start=%s' % (solver, multi_class, str(fit_intercept), str(warm_start)))\n                    if warm_start:\n                        assert_greater(2.0, cum_diff, msg)\n                    else:\n                        assert_greater(cum_diff, 2.0, msg)\n", "label": 1}
{"function": "\n\ndef _addedDataToIncoming(self, inc, skipFinish=False):\n    if (not inc.receivedAllData()):\n        return inc\n    (rdata, extra) = ('', '')\n    try:\n        (rdata, extra) = inc.data\n        if isControlMessage(rdata):\n            raise ValueError(('Error: received control message \"%s\"; expecting incoming data.' % str(rdata)))\n        rEnv = ReceiveEnvelope(*rdata)\n    except Exception:\n        import traceback\n        thesplog('OUCH!  Error deserializing received data: %s  (rdata=\"%s\", extra=\"%s\")', traceback.format_exc(), rdata, extra)\n        try:\n            inc.socket.sendall(ackDataErrMsg)\n        except Exception:\n            pass\n        inc.close()\n        return None\n    inc.socket.sendall(ackMsg)\n    inc.fromAddress = rdata[0]\n    self._processReceivedEnvelope(rEnv)\n    if (extra and isinstance(inc, TCPIncomingPersistent)):\n        newinc = TCPIncomingPersistent(inc.fromAddress, inc.socket)\n        newinc.addData(rdata)\n        return self._addedDataToIncoming(newinc)\n    if (not skipFinish):\n        self._finishIncoming(inc, rEnv.sender)\n    return None\n", "label": 1}
{"function": "\n\ndef Decider(self, function):\n    copy_function = self._copy2_from_cache\n    if (function in ('MD5', 'content')):\n        if (not SCons.Util.md5):\n            raise UserError('MD5 signatures are not available in this version of Python.')\n        function = self._changed_content\n    elif (function == 'MD5-timestamp'):\n        function = self._changed_timestamp_then_content\n    elif (function in ('timestamp-newer', 'make')):\n        function = self._changed_timestamp_newer\n        copy_function = self._copy_from_cache\n    elif (function == 'timestamp-match'):\n        function = self._changed_timestamp_match\n    elif (not callable(function)):\n        raise UserError(('Unknown Decider value %s' % repr(function)))\n    self.decide_target = function\n    self.decide_source = function\n    self.copy_from_cache = copy_function\n", "label": 1}
{"function": "\n\n@periodic_task.periodic_task(spacing=CONF.instance_delete_interval)\ndef _cleanup_incomplete_migrations(self, context):\n    'Delete instance files on failed resize/revert-resize operation\\n\\n        During resize/revert-resize operation, if that instance gets deleted\\n        in-between then instance files might remain either on source or\\n        destination compute node because of race condition.\\n        '\n    LOG.debug('Cleaning up deleted instances with incomplete migration ')\n    migration_filters = {\n        'host': CONF.host,\n        'status': 'error',\n    }\n    migrations = objects.MigrationList.get_by_filters(context, migration_filters)\n    if (not migrations):\n        return\n    inst_uuid_from_migrations = set([migration.instance_uuid for migration in migrations])\n    inst_filters = {\n        'deleted': True,\n        'soft_deleted': False,\n        'uuid': inst_uuid_from_migrations,\n        'host': CONF.host,\n    }\n    attrs = ['info_cache', 'security_groups', 'system_metadata']\n    with utils.temporary_mutation(context, read_deleted='yes'):\n        instances = objects.InstanceList.get_by_filters(context, inst_filters, expected_attrs=attrs, use_slave=True)\n    for instance in instances:\n        for migration in migrations:\n            if (instance.uuid == migration.instance_uuid):\n                self.driver.delete_instance_files(instance)\n                try:\n                    migration.status = 'failed'\n                    with migration.obj_as_admin():\n                        migration.save()\n                except exception.MigrationNotFound:\n                    LOG.warning(_LW('Migration %s is not found.'), migration.id, context=context, instance=instance)\n                break\n", "label": 1}
{"function": "\n\n@contextmanager\ndef build_with_altered_context(self, name, conf, context, stream, dockerfile, volumes_from=None, command=None, tag=False):\n    conf_image_name = conf.prefixed_image_name\n    new_conf = conf.clone()\n    if (name is not None):\n        if tag:\n            new_name = '{0}-{1}'.format(conf.prefixed_image_name, name)\n        else:\n            new_name = None\n        new_conf.name = name\n        new_conf.image_name = new_name\n        new_conf.container_id = None\n        new_conf.container_name = '{0}-{1}'.format(new_name, str(uuid.uuid1())).replace('/', '__')\n    else:\n        new_name = conf.image_name\n    new_conf.bash = NotSpecified\n    new_conf.command = NotSpecified\n    if (command is not None):\n        new_conf.bash = command\n    if volumes_from:\n        new_conf.volumes = new_conf.volumes.clone()\n        new_conf.volumes.share_with = (list(conf.volumes.share_with) + volumes_from)\n    if (context is not None):\n        maker = context.clone_with_new_dockerfile(conf, dockerfile)\n    else:\n        new_conf.context = Context(enabled=False, parent_dir=new_conf.context.parent_dir)\n        maker = new_conf.make_context(docker_file=dockerfile)\n\n    @contextmanager\n    def remover(conf):\n        (yield)\n    if (new_name is not None):\n        remover = self.remove_replaced_images\n    with remover(new_conf):\n        cached = False\n        with maker as new_context:\n            cached = NormalBuilder(new_name).build(new_conf, new_context, stream)\n            new_conf.image_name = stream.current_container\n    (yield (new_conf, cached))\n", "label": 1}
{"function": "\n\ndef _learnStep(self):\n    if self.evaluatorIsNoisy:\n        self.bestEvaluation = self._oneEvaluation(self.bestEvaluable)\n    challenger = self.bestEvaluable.copy()\n    challenger.mutate()\n    newEval = self._oneEvaluation(challenger)\n    if (((not self.minimize) and (newEval < self.bestEvaluation)) or (self.minimize and (newEval > self.bestEvaluation))):\n        acceptProbability = exp(((- abs((newEval - self.bestEvaluation))) / self.temperature))\n        if (random() < acceptProbability):\n            (self.bestEvaluable, self.bestEvaluation) = (challenger, newEval)\n", "label": 1}
{"function": "\n\ndef _update(self, frame_no):\n    self._frame_no = frame_no\n    if ((self._clear and (frame_no == (self._stop_frame - 1))) or (self._delete_count == 1)):\n        for i in range(0, self._renderer.max_height):\n            self._screen.print_at((' ' * self._renderer.max_width), self._x, (self._y + i), bg=self._bg)\n    elif ((frame_no % self._speed) == 0):\n        (image, colours) = self._renderer.rendered_text\n        for (i, line) in enumerate(image):\n            self._screen.paint(line, self._x, (self._y + i), self._colour, attr=self._attr, bg=self._bg, transparent=self._transparent, colour_map=colours[i])\n", "label": 1}
{"function": "\n\ndef filter(self, data):\n    resp = util.http.parse_response(data)\n    content_type = (resp.getheader('content-type', '').strip() if resp else '')\n    return (resp and (((resp.status == 200) and any((re.match(type, content_type) for type in SSLStrip.content_types))) or (((resp.status / 100) == 3) and resp.getheader('location', '').startswith('https://'))))\n", "label": 1}
{"function": "\n\ndef match(self, other):\n    'Return true if this MediaType satisfies the given MediaType.'\n    for key in self.params.keys():\n        if ((key != 'q') and (other.params.get(key, None) != self.params.get(key, None))):\n            return False\n    if ((self.sub_type != '*') and (other.sub_type != '*') and (other.sub_type != self.sub_type)):\n        return False\n    if ((self.main_type != '*') and (other.main_type != '*') and (other.main_type != self.main_type)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_success(self):\n    count = CompatReport.objects.count()\n    r = self.client.post(self.url, self.json, content_type='application/json')\n    assert (r.status_code == 204)\n    assert (CompatReport.objects.count() == (count + 1))\n    cr = CompatReport.objects.order_by('-id')[0]\n    assert (cr.app_build == incoming_data['appBuild'])\n    assert (cr.app_guid == incoming_data['appGUID'])\n    assert (cr.works_properly == incoming_data['worksProperly'])\n    assert (cr.comments == incoming_data['comments'])\n    assert (cr.client_ip == '127.0.0.1')\n    vals = CompatReport.objects.filter(id=cr.id).values('other_addons')\n    assert (vals[0]['other_addons'] == json.dumps(incoming_data['otherAddons'], separators=(',', ':')))\n", "label": 1}
{"function": "\n\n@classmethod\ndef on_change(cls, seconds, now):\n    'Change the theme and get the next time point to change themes.'\n    cls.busy = True\n    if ((cls.next_change is not None) and ((cls.next_change.theme != cls.current_theme) or (cls.next_change.msg != cls.current_msg) or (cls.next_change.filters != cls.current_filters) or (cls.next_change.ui_theme != cls.current_ui_theme) or (cls.next_change.command is not None))):\n        debug_log('Change needed!')\n        update = True\n    else:\n        debug_log('Change not needed!')\n        update = False\n    cls.update_theme(seconds, now, update)\n    cls.busy = False\n", "label": 1}
{"function": "\n\ndef get_prep_lookup(self, lookup_type, value, prepared=False):\n    ' Cleanup value for the jsonb lookup types\\n\\n        contains requires json encoded string\\n        has_any and has_all require array of string_types\\n        has requires string, but we can easily convert int to string\\n\\n        '\n    if (lookup_type in ['jcontains']):\n        if (not isinstance(value, six.string_types)):\n            value = json.dumps(value, cls=get_encoder_class(), **self._options)\n    if (lookup_type in ['jhas_any', 'jhas_all']):\n        if isinstance(value, six.string_types):\n            value = [value]\n        value = [('%s' % v) for v in value]\n    elif ((lookup_type in ['jhas']) and (not isinstance(value, six.string_types))):\n        if isinstance(value, six.integer_types):\n            value = str(value)\n        else:\n            raise TypeError('jhas lookup requires str or int')\n    return value\n", "label": 1}
{"function": "\n\ndef client_update_all_switches(self):\n    ' Updates all the switch states on the OSC client.'\n    if (self.client_mode == 'name'):\n        for switch in self.machine.switches:\n            if self.machine.switch_controller.is_active(switch.name):\n                data = 1\n            else:\n                data = 0\n            self.client_send_osc_message('sw', switch.name, data)\n    elif (self.client_mode == 'wpc'):\n        for switch in self.machine.switches:\n            if self.machine.switch_controller.is_active(switch.name):\n                data = 1\n            else:\n                data = 0\n            self.client_send_osc_message('sw', switch.config['number_str'].lower(), data)\n", "label": 1}
{"function": "\n\ndef _sync_recv_msg(self):\n    \"Internal use only; use 'recv_msg' instead.\\n\\n        Synchronous version of async_recv_msg.\\n        \"\n    n = struct.calcsize('>L')\n    try:\n        data = self._sync_recvall(n)\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            return ''\n        else:\n            raise\n    if (len(data) != n):\n        return ''\n    n = struct.unpack('>L', data)[0]\n    assert (n >= 0)\n    try:\n        data = self._sync_recvall(n)\n    except socket.error as err:\n        if (err.args[0] == 'hangup'):\n            return ''\n        else:\n            raise\n    if (len(data) != n):\n        return ''\n    return data\n", "label": 1}
{"function": "\n\ndef get_header(self, create=True):\n    try:\n        hdr = self.metadata['header']\n        if (self.inherit_primary_header and (self._primary_hdr is not None)):\n            displayhdr = AstroHeader()\n            for key in hdr.keyorder:\n                card = hdr.get_card(key)\n                bnch = displayhdr.__setitem__(card.key, card.value)\n                bnch.comment = card.comment\n            for key in self._primary_hdr.keyorder:\n                if (key not in hdr):\n                    card = self._primary_hdr.get_card(key)\n                    bnch = displayhdr.__setitem__(card.key, card.value)\n                    bnch.comment = card.comment\n        else:\n            displayhdr = hdr\n    except KeyError as e:\n        if (not create):\n            raise e\n        hdr = AstroHeader()\n        self.metadata['header'] = hdr\n        displayhdr = hdr\n    return displayhdr\n", "label": 1}
{"function": "\n\n@classmethod\ndef cli_resp_formatter(cls, resp):\n    'Override this method to provide custom formatting of cli response.\\n        '\n    if (not resp.value):\n        return ''\n    if (resp.status == STATUS_OK):\n        if (type(resp.value) in (str, bool, int, float, six.text_type)):\n            return str(resp.value)\n        ret = ''\n        val = resp.value\n        if (not isinstance(val, list)):\n            val = [val]\n        for line in val:\n            for (k, v) in line.items():\n                if isinstance(v, dict):\n                    ret += cls.cli_resp_line_template.format(k, ('\\n' + pprint.pformat(v)))\n                else:\n                    ret += cls.cli_resp_line_template.format(k, v)\n        return ret\n    else:\n        return 'Error: {0}'.format(resp.value)\n", "label": 1}
{"function": "\n\ndef _read_structure(f, array_desc, struct_desc):\n    '\\n    Read a structure, with the array and structure descriptors given as\\n    `array_desc` and `structure_desc` respectively.\\n    '\n    nrows = array_desc['nelements']\n    columns = struct_desc['tagtable']\n    dtype = []\n    for col in columns:\n        if (col['structure'] or col['array']):\n            dtype.append(((col['name'].lower(), col['name']), np.object_))\n        elif (col['typecode'] in DTYPE_DICT):\n            dtype.append(((col['name'].lower(), col['name']), DTYPE_DICT[col['typecode']]))\n        else:\n            raise Exception(('Variable type %i not implemented' % col['typecode']))\n    structure = np.recarray((nrows,), dtype=dtype)\n    for i in range(nrows):\n        for col in columns:\n            dtype = col['typecode']\n            if col['structure']:\n                structure[col['name']][i] = _read_structure(f, struct_desc['arrtable'][col['name']], struct_desc['structtable'][col['name']])\n            elif col['array']:\n                structure[col['name']][i] = _read_array(f, dtype, struct_desc['arrtable'][col['name']])\n            else:\n                structure[col['name']][i] = _read_data(f, dtype)\n    if (array_desc['ndims'] > 1):\n        dims = array_desc['dims'][:int(array_desc['ndims'])]\n        dims.reverse()\n        structure = structure.reshape(dims)\n    return structure\n", "label": 1}
{"function": "\n\n@property\ndef template_width(self):\n    '\\n        Set column width to accommodate widest image.\\n        '\n    width = 0\n    if self.app.enable_case_list_icon_dynamic_width:\n        for (i, item) in enumerate(self.column.enum):\n            for path in item.value.values():\n                map_item = self.app.multimedia_map[path]\n                if (map_item is not None):\n                    image = CommCareMultimedia.get(map_item.multimedia_id)\n                    if (image is not None):\n                        for media in image.aux_media:\n                            width = max(width, media.media_meta['size']['width'])\n    if (width == 0):\n        return '13%'\n    return str(width)\n", "label": 1}
{"function": "\n\ndef _apply_path_joins(self, query, joins, path, inner_join=True):\n    '\\n            Apply join path to the query.\\n\\n            :param query:\\n                Query to add joins to\\n            :param joins:\\n                List of current joins. Used to avoid joining on same relationship more than once\\n            :param path:\\n                Path to be joined\\n            :param fn:\\n                Join function\\n        '\n    last = None\n    if path:\n        for item in path:\n            key = (inner_join, item)\n            alias = joins.get(key)\n            if (key not in joins):\n                if (not isinstance(item, Table)):\n                    alias = aliased(item.property.mapper.class_)\n                fn = (query.join if inner_join else query.outerjoin)\n                if (last is None):\n                    query = (fn(item) if (alias is None) else fn(alias, item))\n                else:\n                    prop = getattr(last, item.key)\n                    query = (fn(prop) if (alias is None) else fn(alias, prop))\n                joins[key] = alias\n            last = alias\n    return (query, joins, last)\n", "label": 1}
{"function": "\n\ndef _check_send(self):\n    if (self._active < self._max):\n        return\n    while (self._active >= self._max):\n        first_q = min(self._queues.itervalues(), key=self._key_getter)\n        key = first_q[2]\n        second_q = min((q for (k, q) in self._queues.iteritems() if (k != key)), key=self._key_getter)\n        switch_grpos = second_q[0]\n        frame_queue = first_q[1]\n        while frame_queue:\n            if (frame_queue[0][0] > switch_grpos):\n                break\n            (grpos, frame) = frame_queue.popleft()\n            self._callback(grpos, key, frame)\n        if (not frame_queue):\n            self._active -= 1\n        else:\n            first_q[0] = frame_queue[0][0]\n", "label": 1}
{"function": "\n\ndef mapKeyword2Script(path):\n    'collect keywords from scripts.'\n    map_keyword2script = collections.defaultdict(list)\n    for script in glob.glob(os.path.join(path, '*.py')):\n        s = os.path.basename(script)[:(- 3)]\n        with open(script, 'r') as inf:\n            data = [x for x in inf.readlines(10000) if x.startswith(':Tags:')]\n            if data:\n                keywords = [x.strip() for x in data[0][6:].split(' ')]\n                for x in keywords:\n                    if x:\n                        map_keyword2script[x].append(s)\n    return map_keyword2script\n", "label": 1}
{"function": "\n\ndef is_nash(self, action_profile):\n    '\\n        Return True if `action_profile` is a Nash equilibrium.\\n\\n        Parameters\\n        ----------\\n        action_profile : array_like(int or array_like(float))\\n            An array of N objects, where each object must be an integer\\n            (pure action) or an array of floats (mixed action).\\n\\n        Returns\\n        -------\\n        bool\\n            True if `action_profile` is a Nash equilibrium; False\\n            otherwise.\\n\\n        '\n    if (self.N == 2):\n        for (i, player) in enumerate(self.players):\n            (own_action, opponent_action) = (action_profile[i], action_profile[(1 - i)])\n            if (not player.is_best_response(own_action, opponent_action)):\n                return False\n    elif (self.N >= 3):\n        for (i, player) in enumerate(self.players):\n            own_action = action_profile[i]\n            opponents_actions = (tuple(action_profile[(i + 1):]) + tuple(action_profile[:i]))\n            if (not player.is_best_response(own_action, opponents_actions)):\n                return False\n    elif (not self.players[0].is_best_response(action_profile[0], None)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef diagonalize(S, n, m):\n    for k in range(min(n, m)):\n        (val, i, j) = max(((abs(S[i][j]), i, j) for i in range(k, m) for j in range(k, n)))\n        if is_zero(val):\n            return k\n        (S[i], S[k]) = (S[k], S[i])\n        for r in range((m + 1)):\n            (S[r][j], S[r][k]) = (S[r][k], S[r][j])\n        pivot = float(S[k][k])\n        for j in range(k, (n + 1)):\n            S[k][j] /= pivot\n        for i in range(m):\n            if (i != k):\n                fact = S[i][k]\n                for j in range(k, (n + 1)):\n                    S[i][j] -= (fact * S[k][j])\n    return min(n, m)\n", "label": 1}
{"function": "\n\ndef rule_params_SA(parser, node, children):\n    params = {\n        \n    }\n    for (name, value) in children[0].items():\n        if (name not in ['skipws', 'ws']):\n            raise TextXSyntaxError('Invalid rule param \"{}\" at {}.'.format(name, parser.pos_to_linecol(node.position)))\n        if ((name == 'ws') and ('\\\\' in value)):\n            new_value = ''\n            if ('\\\\n' in value):\n                new_value += '\\n'\n            if ('\\\\r' in value):\n                new_value += '\\r'\n            if ('\\\\t' in value):\n                new_value += '\\t'\n            if (' ' in value):\n                new_value += ' '\n            value = new_value\n        params[name] = value\n    return params\n", "label": 1}
{"function": "\n\ndef test_define(self):\n    assert (self.l2_net_dev.forward.mode == 'nat')\n    self.l2_net_dev.define()\n    assert isinstance(self.l2_net_dev.uuid, str)\n    assert (len(self.l2_net_dev.uuid) == 36)\n    assert (self.l2_net_dev.network_name() == 'test_env_test_l2_net_dev')\n    assert (self.l2_net_dev.exists() is True)\n    assert (self.l2_net_dev.is_active() == 0)\n    assert (self.l2_net_dev.bridge_name() == 'virbr1')\n    assert (self.l2_net_dev._libvirt_network.autostart() == 1)\n    xml = self.l2_net_dev._libvirt_network.XMLDesc(0)\n    assert (xml == \"<network>\\n  <name>test_env_test_l2_net_dev</name>\\n  <uuid>{0}</uuid>\\n  <forward mode='nat'/>\\n  <bridge name='virbr1' stp='on' delay='0'/>\\n  <ip address='172.0.0.1' prefix='24'>\\n  </ip>\\n</network>\\n\".format(self.l2_net_dev.uuid))\n", "label": 1}
{"function": "\n\ndef sudoku(G):\n    'Solving Sudoku\\n\\n    :param G: integer matrix with 0 at empty cells\\n    :returns bool: True if grid could be solved\\n    :modifies: G will contain the solution\\n    :complexity: huge, but linear for usual published 9x9 grids\\n    '\n    global N, N2, N4\n    if (len(G) == 16):\n        (N, N2, N4) = (4, 16, 256)\n    e = (4 * N4)\n    univers = (e + 1)\n    S = [[rc(a), rv(a), cv(a), bv(a)] for a in range((N4 * N2))]\n    A = [e]\n    for r in range(N2):\n        for c in range(N2):\n            if (G[r][c] != 0):\n                a = assignation(r, c, (G[r][c] - 1))\n                A += S[a]\n    sol = dancing_links(univers, (S + [A]))\n    if sol:\n        for a in sol:\n            if (a < len(S)):\n                G[row(a)][col(a)] = (val(a) + 1)\n        return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef get_placeholders(template_name):\n    'Return a list of PlaceholderNode found in the given template.\\n\\n    :param template_name: the name of the template file\\n    '\n    dummy_context.template = template.Template('')\n    try:\n        temp_wrapper = template.loader.get_template(template_name)\n    except template.TemplateDoesNotExist:\n        return []\n    (plist, blist) = ([], [])\n    temp = temp_wrapper.template\n    _placeholders_recursif(temp.nodelist, plist, blist)\n    previous = {\n        \n    }\n    block_to_remove = []\n    for block in blist:\n        if (block.name in previous):\n            if (not hasattr(block, 'has_super_var')):\n                block_to_remove.append(previous[block.name])\n        previous[block.name] = block\n\n    def keep(p):\n        return (not (p.found_in_block in block_to_remove))\n    placeholders = [p for p in plist if keep(p)]\n    names = []\n    pfiltered = []\n    for p in placeholders:\n        if (p.ctype not in names):\n            pfiltered.append(p)\n            names.append(p.ctype)\n    return pfiltered\n", "label": 1}
{"function": "\n\ndef _is_removing_self_admin_role(self, request, project_id, user_id, available_roles, current_role_ids):\n    is_current_user = (user_id == request.user.id)\n    is_current_project = (project_id == request.user.tenant_id)\n    _admin_roles = self.get_admin_roles()\n    available_admin_role_ids = [role.id for role in available_roles if (role.name.lower() in _admin_roles)]\n    admin_roles = [role for role in current_role_ids if (role in available_admin_role_ids)]\n    if len(admin_roles):\n        removing_admin = any([(role in current_role_ids) for role in admin_roles])\n    else:\n        removing_admin = False\n    if (is_current_user and is_current_project and removing_admin):\n        msg = _('You cannot revoke your administrative privileges from the project you are currently logged into. Please switch to another project with administrative privileges or remove the administrative role manually via the CLI.')\n        messages.warning(request, msg)\n        return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef test_generate_with_params(self):\n    api = APISpecification(version='v1', base_url='http://api.globo.com')\n    api.add_resource(Resource('dogs', paths=[Path('/dogs/{key}', params=[Param('key')], methods=[Method('POST'), Method('GET')])]))\n    result = self.gen(api)\n    doc = ElementTree.fromstring(result)\n    resources = doc.getchildren()[0]\n    resource = resources.getchildren()[0]\n    param = resource.getchildren()[0]\n    assert param.tag.endswith('param')\n    assert (param.get('name') == 'key')\n    assert (param.get('required') == 'true')\n    assert (param.get('type') == 'xsd:string')\n    assert (param.get('style') == 'template')\n    method_1 = resource.getchildren()[1]\n    assert method_1.tag.endswith('method')\n    assert (method_1.get('name') == 'POST')\n    method_2 = resource.getchildren()[2]\n    assert method_2.tag.endswith('method')\n    assert (method_2.get('name') == 'GET')\n", "label": 1}
{"function": "\n\ndef _list_xml_members(cls):\n    \"Generator listing all members which are XML elements or attributes.\\n    \\n    The following members would be considered XML members:\\n    foo = 'abc' - indicates an XML attribute with the qname abc\\n    foo = SomeElement - indicates an XML child element\\n    foo = [AnElement] - indicates a repeating XML child element, each instance\\n        will be stored in a list in this member\\n    foo = ('att1', '{http://example.com/namespace}att2') - indicates an XML\\n        attribute which has different parsing rules in different versions of \\n        the protocol. Version 1 of the XML parsing rules will look for an\\n        attribute with the qname 'att1' but verion 2 of the parsing rules will\\n        look for a namespaced attribute with the local name of 'att2' and an\\n        XML namespace of 'http://example.com/namespace'.\\n    \"\n    members = []\n    for pair in inspect.getmembers(cls):\n        if ((not pair[0].startswith('_')) and (pair[0] != 'text')):\n            member_type = pair[1]\n            if (isinstance(member_type, tuple) or isinstance(member_type, list) or isinstance(member_type, (str, unicode)) or (inspect.isclass(member_type) and issubclass(member_type, XmlElement))):\n                members.append(pair)\n    return members\n", "label": 1}
{"function": "\n\ndef values_from_response(self, response, sreg_names=None, ax_names=None):\n    'Return values from SimpleRegistration response or\\n        AttributeExchange response if present.\\n\\n        @sreg_names and @ax_names must be a list of name and aliases\\n        for such name. The alias will be used as mapping key.\\n        '\n    values = {\n        \n    }\n    if sreg_names:\n        resp = sreg.SRegResponse.fromSuccessResponse(response)\n        if resp:\n            values.update(((alias, (resp.get(name) or '')) for (name, alias) in sreg_names))\n    if ax_names:\n        resp = ax.FetchResponse.fromSuccessResponse(response)\n        if resp:\n            for (src, alias) in ax_names:\n                name = alias.replace('old_', '')\n                values[name] = (resp.getSingle(src, '') or values.get(name))\n    return values\n", "label": 1}
{"function": "\n\ndef check_permissions(self, request):\n    '\\n        Retrieve the controlled object and perform the permissions check.\\n        '\n    obj = ((hasattr(self, 'get_controlled_object') and self.get_controlled_object()) or (hasattr(self, 'get_object') and self.get_object()) or getattr(self, 'object', None))\n    user = request.user\n    perms = self.get_required_permissions(self)\n    has_permissions = self.perform_permissions_check(user, obj, perms)\n    if ((not has_permissions) and (not user.is_authenticated())):\n        return HttpResponseRedirect('{}?{}={}'.format(self.login_url, self.redirect_field_name, urlquote(request.get_full_path())))\n    elif (not has_permissions):\n        raise PermissionDenied\n", "label": 1}
{"function": "\n\ndef set_content(self, *values):\n    ' Sets the content of a view.\\n        '\n    content = []\n    accum = []\n    for value in values:\n        if isinstance(value, ViewSubElement):\n            content.append(value)\n        elif (type(value) in SequenceTypes):\n            content.append(Group(*value))\n        elif (isinstance(value, basestring) and (value[:1] == '<') and (value[(- 1):] == '>')):\n            content.append(Include(value[1:(- 1)].strip()))\n        else:\n            content.append(Item(value))\n    for item in content:\n        if isinstance(item, Item):\n            content = [Group(*content)]\n            break\n    self.content = Group(*content, container=self)\n", "label": 1}
{"function": "\n\ndef test_reset_repeating(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, repeat=True, _time_function=time)\n    time.time = 12\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 8)\n    timer.reset()\n    assert (timer.sleep_time() == 10)\n    time.time = 34\n    assert (timer.sleep_time() == 0)\n    timer.reset()\n    assert (timer.sleep_time() == 10)\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 10)\n", "label": 1}
{"function": "\n\ndef import_(module, objects=None, via=None):\n    '\\n    :param module: py3 compatiable module path\\n    :param objects: objects want to imported, it should be a list\\n    :param via: for some py2 module, you should give the import path according the\\n        objects which you want to imported\\n    :return: object or module\\n    '\n    if PY3:\n        mod = __import__(module, fromlist=['*'])\n    else:\n        path = modules_mapping.get(module)\n        if (not path):\n            raise Exception((\"Can't find the module %s in mappings.\" % module))\n        if isinstance(path, list):\n            if (not via):\n                raise Exception('You should give a via parameter to enable import from py2.')\n            path = via\n        mod = __import__(path, fromlist=['*'])\n    if objects:\n        if (not isinstance(objects, (list, tuple))):\n            raise Exception('objects parameter should be a list or tuple.')\n        r = []\n        for x in objects:\n            r.append(getattr(mod, x))\n        if (len(r) > 1):\n            return tuple(r)\n        else:\n            return r[0]\n    else:\n        return mod\n", "label": 1}
{"function": "\n\ndef _process_axis_and_grid(plot, axis_type, axis_location, minor_ticks, axis_label, rng, dim):\n    axiscls = _get_axis_class(axis_type, rng)\n    if axiscls:\n        if (axiscls is LogAxis):\n            if (dim == 0):\n                plot.x_mapper_type = 'log'\n            elif (dim == 1):\n                plot.y_mapper_type = 'log'\n            else:\n                raise ValueError(('received invalid dimension value: %r' % dim))\n        axis = axiscls(plot=(plot if axis_location else None))\n        if isinstance(axis.ticker, ContinuousTicker):\n            axis.ticker.num_minor_ticks = _get_num_minor_ticks(axiscls, minor_ticks)\n        axis_label = axis_label\n        if axis_label:\n            axis.axis_label = axis_label\n        grid = Grid(plot=plot, dimension=dim, ticker=axis.ticker)\n        grid\n        if (axis_location is not None):\n            getattr(plot, axis_location).append(axis)\n", "label": 1}
{"function": "\n\ndef _packagePaths(self):\n    '\\n        Yield a sequence of FilePath-like objects which represent path segments.\\n        '\n    if (not self.isPackage()):\n        return\n    if self.isLoaded():\n        load = self.load()\n        if hasattr(load, '__path__'):\n            for fn in load.__path__:\n                if (fn == self.parentPath.path):\n                    assert self.parentPath.exists()\n                    (yield self.parentPath)\n                else:\n                    smp = self.pathEntry.pythonPath._smartPath(fn)\n                    if smp.exists():\n                        (yield smp)\n    else:\n        (yield self.parentPath)\n", "label": 1}
{"function": "\n\ndef parse(self, path):\n    try:\n        if (path.startswith('http://') or path.startswith('https://')):\n            if ('requests' not in IMPORTS):\n                e = 'HTTP library not found: requests'\n                raise ImportError(e)\n            headers = {\n                'User-Agent': 'Mozilla/5.0 Gecko Firefox',\n            }\n            r = requests.get(path, headers=headers)\n            r.raise_for_status()\n            f = StringIO(r.content)\n            self.parser_func(f, path)\n            return\n        elif os.path.isfile(path):\n            with open(path, 'rb') as f:\n                self.parser_func(f, path)\n            return\n        elif os.path.isdir(path):\n            for (walk_root, walk_dirs, walk_files) in os.walk(path):\n                for walk_file in fnmatch.filter(walk_files, self.ext_filter):\n                    fpath = os.path.join(walk_root, walk_file)\n                    with open(fpath, 'rb') as f:\n                        self.parser_func(f, fpath)\n            return\n        e = ('File path is not a file, directory or URL: %s' % path)\n        raise IOError(e)\n    except (KeyboardInterrupt, SystemExit):\n        raise\n    except Exception as e:\n        self.handler.print_error(path, e)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _item_style_sheet_changed(cls, item):\n    item_styles = cls._item_styles\n    item_sheets = cls._item_style_sheets\n    items = [i for i in item.traverse() if (i in cls._queried_items)]\n    for item in items:\n        sheets = item_sheets.pop(item, None)\n        if (sheets is not None):\n            sheet_items = cls._style_sheet_items\n            for sheet in sheets:\n                if (sheet in sheet_items):\n                    sheet_items[sheet].discard(item)\n        styles = item_styles.pop(item, None)\n        if (styles is not None):\n            style_items = cls._style_items\n            for style in styles:\n                if (style in style_items):\n                    style_items[style].discard(item)\n    cls._request_restyle(items)\n", "label": 1}
{"function": "\n\ndef require_instances(self, instances=(), instance_ids=()):\n    used = {(instance.id, instance.src) for instance in self.instances}\n    for instance in instances:\n        if ((instance.id, instance.src) not in used):\n            self.instances.append(Instance(id=instance.id, src=instance.src))\n            if (len(self.instances) == 1):\n                instance_node = self.node.find('instance')\n                command_node = self.node.find('command')\n                self.node.remove(instance_node)\n                self.node.insert((self.node.index(command_node) + 1), instance_node)\n    covered_ids = {instance_id for (instance_id, _) in used}\n    for instance_id in instance_ids:\n        if (instance_id not in covered_ids):\n            raise UnknownInstanceError('Instance reference not recognized: {} in xpath \"{}\"'.format(instance_id, getattr(instance_id, 'xpath', '(Xpath Unknown)')))\n    sorted_instances = sorted(self.instances, key=(lambda instance: instance.id))\n    if (sorted_instances != self.instances):\n        self.instances = sorted_instances\n", "label": 1}
{"function": "\n\ndef testCrossBelowMany(self):\n    count = 100\n    values1 = [0 for i in range(count)]\n    values2 = [((- 1) if ((i % 2) == 0) else 1) for i in range(count)]\n    self.assertEqual(cross.cross_below(values1, values2, 0, 0), 0)\n    period = 2\n    for i in range(1, count):\n        if ((i % 2) == 0):\n            self.assertEqual(cross.cross_below(values1, values2, ((i - period) + 1), (i + 1)), 0)\n        else:\n            self.assertEqual(cross.cross_below(values1, values2, ((i - period) + 1), (i + 1)), 1)\n    period = 4\n    for i in range(3, count):\n        if ((i % 2) == 0):\n            self.assertEqual(cross.cross_below(values1, values2, ((i - period) + 1), (i + 1)), 1)\n        else:\n            self.assertEqual(cross.cross_below(values1, values2, ((i - period) + 1), (i + 1)), 2)\n    self.assertEqual(cross.cross_below(values1, values2, 0, count), (count / 2))\n", "label": 1}
{"function": "\n\ndef minion_config(path, env_var='SALT_MINION_CONFIG', defaults=None, cache_minion_id=False):\n    \"\\n    Reads in the minion configuration file and sets up special options\\n\\n    This is useful for Minion-side operations, such as the\\n    :py:class:`~salt.client.Caller` class, and manually running the loader\\n    interface.\\n\\n    .. code-block:: python\\n\\n        import salt.client\\n        minion_opts = salt.config.minion_config('/etc/salt/minion')\\n    \"\n    if (defaults is None):\n        defaults = DEFAULT_MINION_OPTS\n    if ((path is not None) and path.endswith('proxy')):\n        defaults.update(DEFAULT_PROXY_MINION_OPTS)\n    if (not os.environ.get(env_var, None)):\n        salt_config_dir = os.environ.get('SALT_CONFIG_DIR', None)\n        if salt_config_dir:\n            env_config_file_path = os.path.join(salt_config_dir, 'minion')\n            if (salt_config_dir and os.path.isfile(env_config_file_path)):\n                os.environ[env_var] = env_config_file_path\n    overrides = load_config(path, env_var, DEFAULT_MINION_OPTS['conf_file'])\n    default_include = overrides.get('default_include', defaults['default_include'])\n    include = overrides.get('include', [])\n    overrides.update(include_config(default_include, path, verbose=False))\n    overrides.update(include_config(include, path, verbose=True))\n    opts = apply_minion_config(overrides, defaults, cache_minion_id=cache_minion_id)\n    _validate_opts(opts)\n    return opts\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, n):\n    n = sympify(n)\n    if n.is_Number:\n        if (n is S.Zero):\n            return S.One\n        elif (n is S.Infinity):\n            return S.Infinity\n        elif n.is_Integer:\n            if n.is_negative:\n                return S.ComplexInfinity\n            else:\n                n = n.p\n                if (n < 20):\n                    if (not cls._small_factorials):\n                        result = 1\n                        for i in range(1, 20):\n                            result *= i\n                            cls._small_factorials.append(result)\n                    result = cls._small_factorials[(n - 1)]\n                else:\n                    bits = bin(n).count('1')\n                    result = (cls._recursive(n) * (2 ** (n - bits)))\n                return Integer(result)\n", "label": 1}
{"function": "\n\ndef __exit__(self, type, value, tb):\n    if (value == None):\n        return True\n    if isinstance(value, NoPerm):\n        self.svc.return_error(403, value.args[0])\n    elif isinstance(value, WrongArgs):\n        self.svc.return_error(400, value.args[0])\n    elif isinstance(value, Unauthorized):\n        self.svc.return_error(401, value.args[0])\n    elif isinstance(value, NotFound):\n        self.svc.return_error(404, value.args[0])\n    elif isinstance(value, OutOfRange):\n        self.svc.return_error(416, value.args[0])\n    elif isinstance(value, ServerError):\n        self.svc.return_error(500, value.args[0])\n    else:\n        info = ''.join(traceback.format_exception(type, value, tb))\n        Log.error(('Unexpected exception! %s' % info))\n        self.svc.return_error(500, 'Unexpected server exception', info)\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef _CorrectOrientation(self, image, orientation):\n    'Use PIL to correct the image orientation based on its EXIF.\\n\\n    See JEITA CP-3451 at http://www.exif.org/specifications.html,\\n    Exif 2.2, page 18.\\n\\n    Args:\\n      image: source PIL.Image.Image object.\\n      orientation: integer in range (1,8) inclusive, corresponding the image\\n        orientation from EXIF.\\n\\n    Returns:\\n      PIL.Image.Image with transforms performed on it. If no correction was\\n        done, it returns the input image.\\n    '\n    if (orientation == 2):\n        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n    elif (orientation == 3):\n        image = image.rotate(180)\n    elif (orientation == 4):\n        image = image.transpose(Image.FLIP_TOP_BOTTOM)\n    elif (orientation == 5):\n        image = image.transpose(Image.FLIP_TOP_BOTTOM)\n        image = image.rotate(270)\n    elif (orientation == 6):\n        image = image.rotate(270)\n    elif (orientation == 7):\n        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n        image = image.rotate(270)\n    elif (orientation == 8):\n        image = image.rotate(90)\n    return image\n", "label": 1}
{"function": "\n\ndef email(request, form_class=AddEmailForm, template_name='account/email.html'):\n    if ((request.method == 'POST') and request.user.is_authenticated()):\n        if (request.POST['action'] == 'add'):\n            add_email_form = form_class(request.user, request.POST)\n            if add_email_form.is_valid():\n                add_email_form.save()\n                add_email_form = form_class()\n        else:\n            add_email_form = form_class()\n            if (request.POST['action'] == 'send'):\n                email = request.POST['email']\n                try:\n                    email_address = EmailAddress.objects.get(user=request.user, email=email)\n                    request.user.message_set.create(message=('Confirmation email sent to %s' % email))\n                    EmailConfirmation.objects.send_confirmation(email_address)\n                except EmailAddress.DoesNotExist:\n                    pass\n            elif (request.POST['action'] == 'remove'):\n                email = request.POST['email']\n                try:\n                    email_address = EmailAddress.objects.get(user=request.user, email=email)\n                    email_address.delete()\n                    request.user.message_set.create(message=('Removed email address %s' % email))\n                except EmailAddress.DoesNotExist:\n                    pass\n            elif (request.POST['action'] == 'primary'):\n                email = request.POST['email']\n                email_address = EmailAddress.objects.get(user=request.user, email=email)\n                email_address.set_as_primary()\n    else:\n        add_email_form = form_class()\n    return render_to_response(template_name, {\n        'add_email_form': add_email_form,\n    }, context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef disable(**kwargs):\n    \"\\n    Disable all beaconsd jobs on the minion\\n\\n    :return:                Boolean and status message on success or failure of disable.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' beacons.disable\\n    \"\n    ret = {\n        'comment': [],\n        'result': True,\n    }\n    if (('test' in kwargs) and kwargs['test']):\n        ret['comment'] = 'Beacons would be disabled.'\n    else:\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({\n                'func': 'disable',\n            }, 'manage_beacons')\n            if res:\n                event_ret = eventer.get_event(tag='/salt/minion/minion_beacons_disabled_complete', wait=30)\n                log.debug('event_ret {0}'.format(event_ret))\n                if (event_ret and event_ret['complete']):\n                    beacons = event_ret['beacons']\n                    if (('enabled' in beacons) and (not beacons['enabled'])):\n                        ret['result'] = True\n                        ret['comment'] = 'Disabled beacons on minion.'\n                    else:\n                        ret['result'] = False\n                        ret['comment'] = 'Failed to disable beacons on minion.'\n                    return ret\n        except KeyError:\n            ret['comment'] = 'Event module not available. Beacons enable job failed.'\n    return ret\n", "label": 1}
{"function": "\n\ndef NormalizeVmSettings(self):\n    'Normalize Vm settings.\\n    '\n    if self.IsVm():\n        if (not self.vm_settings):\n            self.vm_settings = VmSettings()\n        if ('vm_runtime' not in self.vm_settings):\n            self.SetEffectiveRuntime(self.runtime)\n        if (hasattr(self, 'beta_settings') and self.beta_settings):\n            for field in ['vm_runtime', 'has_docker_image', 'image', 'module_yaml_path']:\n                if ((field not in self.beta_settings) and (field in self.vm_settings)):\n                    self.beta_settings[field] = self.vm_settings[field]\n", "label": 1}
{"function": "\n\ndef test_assertRaises(self):\n\n    def _raise(e):\n        raise e\n    self.assertRaises(KeyError, _raise, KeyError)\n    self.assertRaises(KeyError, _raise, KeyError('key'))\n    try:\n        self.assertRaises(KeyError, (lambda : None))\n    except self.failureException as e:\n        self.assertIn('KeyError not raised', str(e))\n    else:\n        self.fail(\"assertRaises() didn't fail\")\n    try:\n        self.assertRaises(KeyError, _raise, ValueError)\n    except ValueError:\n        pass\n    else:\n        self.fail(\"assertRaises() didn't let exception pass through\")\n    with self.assertRaises(KeyError) as cm:\n        try:\n            raise KeyError\n        except Exception as e:\n            exc = e\n            raise\n    self.assertIs(cm.exception, exc)\n    with self.assertRaises(KeyError):\n        raise KeyError('key')\n    try:\n        with self.assertRaises(KeyError):\n            pass\n    except self.failureException as e:\n        self.assertIn('KeyError not raised', str(e))\n    else:\n        self.fail(\"assertRaises() didn't fail\")\n    try:\n        with self.assertRaises(KeyError):\n            raise ValueError\n    except ValueError:\n        pass\n    else:\n        self.fail(\"assertRaises() didn't let exception pass through\")\n", "label": 1}
{"function": "\n\ndef import_string(import_name, silent=False):\n    'Imports an object based on a string.  This is useful if you want to\\n    use import paths as endpoints or something similar.  An import path can\\n    be specified either in dotted notation (``xml.sax.saxutils.escape``)\\n    or with a colon as object delimiter (``xml.sax.saxutils:escape``).\\n\\n    If `silent` is True the return value will be `None` if the import fails.\\n\\n    :param import_name: the dotted name for the object to import.\\n    :param silent: if set to `True` import errors are ignored and\\n                   `None` is returned instead.\\n    :return: imported object\\n    '\n    assert isinstance(import_name, string_types)\n    import_name = str(import_name)\n    try:\n        if (':' in import_name):\n            (module, obj) = import_name.split(':', 1)\n        elif ('.' in import_name):\n            (module, obj) = import_name.rsplit('.', 1)\n        else:\n            return __import__(import_name)\n        if (PY2 and isinstance(obj, unicode)):\n            obj = obj.encode('utf-8')\n        try:\n            return getattr(__import__(module, None, None, [obj]), obj)\n        except (ImportError, AttributeError):\n            modname = ((module + '.') + obj)\n            __import__(modname)\n            return sys.modules[modname]\n    except ImportError as e:\n        if (not silent):\n            raise\n", "label": 1}
{"function": "\n\ndef match(self, left, collected=None):\n    collected = ([] if (collected is None) else collected)\n    args = [l for l in left if (type(l) is Argument)]\n    if (not len(args)):\n        return (False, left, collected)\n    left.remove(args[0])\n    if (type(self.value) is not list):\n        return (True, left, (collected + [Argument(self.name, args[0].value)]))\n    same_name = [a for a in collected if ((type(a) is Argument) and (a.name == self.name))]\n    if len(same_name):\n        same_name[0].value += [args[0].value]\n        return (True, left, collected)\n    else:\n        return (True, left, (collected + [Argument(self.name, [args[0].value])]))\n", "label": 1}
{"function": "\n\ndef ensure_new_type(obj):\n    from future.types.newbytes import newbytes\n    from future.types.newstr import newstr\n    from future.types.newint import newint\n    from future.types.newdict import newdict\n    native_type = type(native(obj))\n    if issubclass(native_type, type(obj)):\n        if (native_type == str):\n            return newbytes(obj)\n        elif (native_type == unicode):\n            return newstr(obj)\n        elif (native_type == int):\n            return newint(obj)\n        elif (native_type == long):\n            return newint(obj)\n        elif (native_type == dict):\n            return newdict(obj)\n        else:\n            return NotImplementedError(('type %s not supported' % type(obj)))\n    else:\n        assert (type(obj) in [newbytes, newstr])\n        return obj\n", "label": 1}
{"function": "\n\ndef single_step(s, x86_64=False):\n    i = fetch_instruction(s, x86_64)\n    if (i is None):\n        return []\n    if (i.address in _hit_count):\n        hc = _hit_count[i.address] = (_hit_count[i.address] + 1)\n    else:\n        hc = _hit_count[i.address] = 1\n    if (i.address in s.symbols):\n        symbol = s.symbols[i.address]\n        if (s.symbols[i.address] in s.function_hooks):\n            ss = s.function_hooks[symbol](s)\n            for s in ss:\n                if (s.solver.solve_time() > 30):\n                    s.log.warning('concretising (last solve took: {}s)', s.solver.solve_time)\n                    s.solver.concretise()\n                s.clear_il_state()\n            return ss\n        else:\n            s.log.function_call(None, symbol)\n    s.log.native_instruction(hc, i, x86_64)\n    max_il_index = len(i.il_instructions)\n    states = [s]\n    exit_states = []\n    while (len(states) > 0):\n        s = states.pop()\n        if (s.il_index >= max_il_index):\n            if (s.solver.solve_time() > 30):\n                s.log.warning('concretising (last solve took: {}s)'.format(s.solver.solve_time()))\n                s.solver.concretise()\n            s.clear_il_state()\n            exit_states.append(s)\n            continue\n        ri = i.il_instructions[s.il_index]\n        s.il_index += 1\n        s.log.reil_instruction(ri)\n        states += reil_single_step(ri, s)\n    return exit_states\n", "label": 1}
{"function": "\n\ndef __new__(cls, inputs=inputs, outputs=outputs, window_length=window_length, mask=mask, *args, **kwargs):\n    if (inputs is NotSpecified):\n        inputs = cls.inputs\n    if (inputs is not NotSpecified):\n        inputs = tuple(inputs)\n    if (outputs is NotSpecified):\n        outputs = cls.outputs\n    if (outputs is not NotSpecified):\n        outputs = tuple(outputs)\n    if (mask is NotSpecified):\n        mask = cls.mask\n    if (mask is NotSpecified):\n        mask = AssetExists()\n    if (window_length is NotSpecified):\n        window_length = cls.window_length\n    return super(ComputableTerm, cls).__new__(cls, *args, inputs=inputs, outputs=outputs, mask=mask, window_length=window_length, **kwargs)\n", "label": 1}
{"function": "\n\ndef update_site(env, debug):\n    'Run through commands to update this site.'\n    error_updating = False\n    here = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n    project_branch = {\n        'branch': ENV_BRANCH[env][PROJECT],\n    }\n    commands = [(CHDIR, here), (EXEC, (GIT_PULL % project_branch)), (EXEC, GIT_SYNC), (EXEC, GIT_SUBMODULE)]\n    commands += [(EXEC, 'python2.6 manage.py collectstatic --noinput'), (EXEC, 'python2.6 manage.py syncdb'), (EXEC, 'python2.6 manage.py migrate'), (EXEC, '/etc/init.d/httpd restart')]\n    for (cmd, cmd_args) in commands:\n        if (CHDIR == cmd):\n            if debug:\n                sys.stdout.write(('cd %s\\n' % cmd_args))\n            os.chdir(cmd_args)\n        elif (EXEC == cmd):\n            if debug:\n                sys.stdout.write(('%s\\n' % cmd_args))\n            if (not (0 == os.system(cmd_args))):\n                error_updating = True\n                break\n        else:\n            raise Exception(('Unknown type of command %s' % cmd))\n    if error_updating:\n        sys.stderr.write('There was an error while updating. Please try again later. Aborting.\\n')\n", "label": 1}
{"function": "\n\ndef _runner(init, shape, target_mean=None, target_std=None, target_max=None, target_min=None):\n    variable = init(shape)\n    output = K.get_value(variable)\n    lim = 0.01\n    if (target_std is not None):\n        assert (abs((output.std() - target_std)) < lim)\n    if (target_mean is not None):\n        assert (abs((output.mean() - target_mean)) < lim)\n    if (target_max is not None):\n        assert (abs((output.max() - target_max)) < lim)\n    if (target_min is not None):\n        assert (abs((output.min() - target_min)) < lim)\n", "label": 1}
{"function": "\n\ndef validate_params(valid_options, params):\n    '\\n    Helps us validate the parameters for the request\\n\\n    :param valid_options: a list of strings of valid options for the\\n                          api request\\n    :param params: a dict, the key-value store which we really only care about\\n                   the key which has tells us what the user is using for the\\n                   API request\\n\\n    :returns: None or throws an exception if the validation fails\\n    '\n    if (not params):\n        return\n    data_filter = ['data', 'source', 'external_url', 'embed']\n    multiple_data = [key for key in params.keys() if (key in data_filter)]\n    if (len(multiple_data) > 1):\n        raise Exception(\"You can't mix and match data parameters\")\n    disallowed_fields = [key for key in params.keys() if (key not in valid_options)]\n    if disallowed_fields:\n        field_strings = ','.join(disallowed_fields)\n        raise Exception('{0} are not allowed fields'.format(field_strings))\n", "label": 1}
{"function": "\n\ndef write_worksheet_cols(doc, worksheet):\n    'Write worksheet columns to xml.'\n    if worksheet.column_dimensions:\n        start_tag(doc, 'cols')\n        for (column_string, columndimension) in worksheet.column_dimensions.items():\n            col_index = column_index_from_string(column_string)\n            col_def = {\n                \n            }\n            col_def['collapsed'] = str(columndimension.style_index)\n            col_def['min'] = str(col_index)\n            col_def['max'] = str(col_index)\n            if (columndimension.width != worksheet.default_column_dimension.width):\n                col_def['customWidth'] = 'true'\n            if (not columndimension.visible):\n                col_def['hidden'] = 'true'\n            if (columndimension.outline_level > 0):\n                col_def['outlineLevel'] = str(columndimension.outline_level)\n            if columndimension.collapsed:\n                col_def['collapsed'] = 'true'\n            if columndimension.auto_size:\n                col_def['bestFit'] = 'true'\n            if (columndimension.width > 0):\n                col_def['width'] = str(columndimension.width)\n            else:\n                col_def['width'] = '9.10'\n            tag(doc, 'col', col_def)\n        end_tag(doc, 'cols')\n", "label": 1}
{"function": "\n\ndef execute(self, ns, package, _type=None):\n    properties = ['Name', ('Type', (lambda i: (software.FILE_TYPES[i.FileType] if i.FileExists else 'Missing'))), ('FileSize', (lambda i: i.FileSize)), ('Passed', (lambda i: (len(i.FailedFlags) < 1)))]\n    if (_type is not None):\n        del properties[1]\n    pkgs = list((p.to_instance() for p in software.find_package(ns, pkg_spec=package)))\n    pkgs = [p for p in pkgs if software.is_package_installed(p)]\n    if (len(pkgs) < 1):\n        raise errors.LmiFailed(('No package matching \"%s\" found.' % package))\n    if (len(pkgs) > 1):\n        LOG().warn('More than one package found for \"%s\": %s', package, ', '.join((p.ElementName for p in pkgs)))\n    return (properties, software.list_package_files(ns, pkgs[(- 1)], file_type=_type))\n", "label": 1}
{"function": "\n\ndef edit_record(self, record):\n    new_record = list(record)\n    new_record[0] = (self.prompt(('Name [%s]: ' % record[0]), required=False) or record[0])\n    new_record[1] = (self.prompt(('Username [%s]: ' % record[1]), required=False) or record[1])\n    pw = (self.prompt('Password []/g: ', required=False, password=True) or record[2])\n    if (pw == 'g'):\n        new_record[2] = gen_password_entropy(128)\n    elif pw:\n        new_record[2] = pw\n    self.output.write(('Notes: %s\\n' % record[3]))\n    edit = (self.prompt('Edit? [n]: ', required=False) or 'n')\n    if (edit[0] == 'y'):\n        new_record[3] = edit_in_editor(record[3])\n    return tuple(new_record)\n", "label": 1}
{"function": "\n\ndef find_square(frame_in):\n    frame_out = frame_in.copy()\n    frame_gray = cv2.cvtColor(frame_in, cv2.COLOR_BGR2GRAY)\n    thresh = adap_threshold(frame_gray)\n    frame_blur = cv2.blur(thresh, (3, 3))\n    (contours, hry) = cv2.findContours(frame_blur, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    squares = []\n    for cnt in contours:\n        cnt_len = cv2.arcLength(cnt, True)\n        cnt = cv2.approxPolyDP(cnt, (0.01 * cnt_len), True)\n        if ((cv2.contourArea(cnt) > 10) and (len(cnt) == 4) and cv2.isContourConvex(cnt)):\n            cnt = cnt.reshape((- 1), 2)\n            max_cos = np.max([angle_cos(cnt[i], cnt[((i + 1) % 4)], cnt[((i + 2) % 4)]) for i in xrange(4)])\n            if ((max_cos < 0.1) and equal(cnt[0], cnt[1], cnt[2], 0.2)):\n                squares.append(cnt)\n    cv2.drawContours(frame_out, squares, (- 1), (0, 255, 0), 2)\n    return frame_out\n", "label": 1}
{"function": "\n\ndef _eval_expand_func(self, **hints):\n    '\\n        Function to expand binomial(n,k) when m is positive integer\\n        Also,\\n        n is self.args[0] and k is self.args[1] while using binomial(n, k)\\n        '\n    n = self.args[0]\n    if n.is_Number:\n        return binomial(*self.args)\n    k = self.args[1]\n    if (k.is_Add and (n in k.args)):\n        k = (n - k)\n    if k.is_Integer:\n        if (k == S.Zero):\n            return S.One\n        elif (k < 0):\n            return S.Zero\n        else:\n            n = self.args[0]\n            result = ((n - k) + 1)\n            for i in range(2, (k + 1)):\n                result *= ((n - k) + i)\n                result /= i\n            return result\n    else:\n        return binomial(*self.args)\n", "label": 1}
{"function": "\n\ndef remove_tech(self):\n    if (self.selected_tech is None):\n        return\n    item = self.selected_tech\n    visible = [x['name'] for x in self.player.get_visible_techs()]\n    enabled = [x['name'] for x in self.player.get_enabled_techs()]\n    self.player.set_visible_techs(make_tech_list([x for x in visible if (x != item)]))\n    self.player.set_enabled_techs(make_tech_list([x for x in enabled if (x != item)]))\n    self.update_lists()\n    self.update_selection()\n", "label": 1}
{"function": "\n\ndef attach(self, timeout, wait=True):\n    if (self.attachment_status in (apiAttachPendingAuthorization, apiAttachSuccess)):\n        return\n    self.acquire()\n    try:\n        try:\n            self.start()\n        except AssertionError:\n            pass\n        t = threading.Timer(timeout2float(timeout), (lambda : setattr(self, 'wait', False)))\n        try:\n            self.init_observer()\n            self.client_id = (- 1)\n            self.set_attachment_status(apiAttachPendingAuthorization)\n            self.post('SKSkypeAPIAttachRequest')\n            self.wait = True\n            if wait:\n                t.start()\n            while (self.wait and (self.attachment_status == apiAttachPendingAuthorization)):\n                if self.run_main_loop:\n                    time.sleep(1.0)\n                else:\n                    EventLoop.run(1.0)\n        finally:\n            t.cancel()\n        if (not self.wait):\n            self.set_attachment_status(apiAttachUnknown)\n            raise SkypeAPIError('Skype attach timeout')\n    finally:\n        self.release()\n    command = Command(('PROTOCOL %s' % self.protocol), Blocking=True)\n    self.send_command(command)\n    self.protocol = int(command.Reply.rsplit(None, 1)[(- 1)])\n", "label": 1}
{"function": "\n\ndef collect(self):\n    '\\n        Collect memory stats of LXCs.\\n        '\n    lxc_metrics = ['memory.usage_in_bytes', 'memory.limit_in_bytes']\n    if (os.path.isdir(self.config['sys_path']) is False):\n        self.log.debug(\"sys_path '%s' isn't directory.\", self.config['sys_path'])\n        return {\n            \n        }\n    collected = {\n        \n    }\n    for item in os.listdir(self.config['sys_path']):\n        fpath = ('%s/%s' % (self.config['sys_path'], item))\n        if (os.path.isdir(fpath) is False):\n            continue\n        for lxc_metric in lxc_metrics:\n            filename = ('%s/%s' % (fpath, lxc_metric))\n            metric_name = ('%s.%s' % (item.replace('.', '_'), lxc_metric.replace('_in_bytes', '')))\n            self.log.debug('Trying to collect from %s', filename)\n            collected[metric_name] = self._read_file(filename)\n    for key in collected.keys():\n        if (collected[key] is None):\n            continue\n        for unit in self.config['byte_unit']:\n            value = diamond.convertor.binary.convert(collected[key], oldUnit='B', newUnit=unit)\n            new_key = ('%s_in_%ss' % (key, unit))\n            self.log.debug(\"Publishing '%s %s'\", new_key, value)\n            self.publish(new_key, value, metric_type='GAUGE')\n", "label": 1}
{"function": "\n\ndef execute():\n    for table in frappe.db.get_tables():\n        doctype = table[3:]\n        if frappe.db.exists('DocType', doctype):\n            fieldnames = [df['fieldname'] for df in frappe.get_all('DocField', fields=['fieldname'], filters={\n                'parent': doctype,\n            })]\n            custom_fieldnames = [df['fieldname'] for df in frappe.get_all('Custom Field', fields=['fieldname'], filters={\n                'dt': doctype,\n            })]\n        else:\n            fieldnames = custom_fieldnames = []\n        for column in frappe.db.sql('desc `{0}`'.format(table), as_dict=True):\n            if (column['Type'] == 'int(1)'):\n                fieldname = column['Field']\n                if (not ((fieldname in default_fields) or (fieldname in fieldnames) or (fieldname in custom_fieldnames))):\n                    continue\n                frappe.db.sql('update `{table}` set `{column}`=0 where `{column}` is null'.format(table=table, column=fieldname))\n                frappe.db.commit()\n                frappe.db.sql_ddl('alter table `{table}`\\n\\t\\t\\t\\t\\tmodify `{column}` int(1) not null default {default}'.format(table=table, column=fieldname, default=cint(column['Default'])))\n", "label": 1}
{"function": "\n\ndef get_errors(self, check_order=True):\n    errors = []\n    self._doc.reset()\n    for (j, line) in enumerate(self._doc):\n        if (len(line) > 75):\n            if hasattr(self, 'name'):\n                errors.append(('%s: Line %d exceeds 75 chars: \"%s\"...' % (self.name, (j + 1), line[:30])))\n            else:\n                errors.append(('Line %d exceeds 75 chars: \"%s\"...' % ((j + 1), line[:30])))\n    if check_order:\n        canonical_order = ['Signature', 'Summary', 'Extended Summary', 'Attributes', 'Methods', 'Parameters', 'Other Parameters', 'Returns', 'Raises', 'Warns', 'See Also', 'Notes', 'References', 'Examples', 'index']\n        canonical_order_copy = list(canonical_order)\n        for s in self.section_order:\n            while (canonical_order_copy and (s != canonical_order_copy[0])):\n                canonical_order_copy.pop(0)\n                if (not canonical_order_copy):\n                    errors.append(('Sections in wrong order (starting at %s). The right order is %s' % (s, canonical_order)))\n    return errors\n", "label": 1}
{"function": "\n\ndef kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB_ALIAS):\n    \"\\n    This view generates KML for the given app label, model, and field name.\\n\\n    The model's default manager must be GeoManager, and the field name\\n    must be that of a geographic field.\\n    \"\n    placemarks = []\n    klass = get_model(label, model)\n    if (not klass):\n        raise Http404(('You must supply a valid app label and module name.  Got \"%s.%s\"' % (label, model)))\n    if field_name:\n        try:\n            info = klass._meta.get_field_by_name(field_name)\n            if (not isinstance(info[0], GeometryField)):\n                raise Exception\n        except:\n            raise Http404('Invalid geometry field.')\n    connection = connections[using]\n    if connection.ops.postgis:\n        placemarks = klass._default_manager.using(using).kml(field_name=field_name)\n    else:\n        placemarks = []\n        if connection.ops.oracle:\n            qs = klass._default_manager.using(using).transform(4326, field_name=field_name)\n        else:\n            qs = klass._default_manager.using(using).all()\n        for mod in qs:\n            mod.kml = getattr(mod, field_name).kml\n            placemarks.append(mod)\n    if compress:\n        render = render_to_kmz\n    else:\n        render = render_to_kml\n    return render('gis/kml/placemarks.kml', {\n        'places': placemarks,\n    })\n", "label": 1}
{"function": "\n\ndef parse_parameters_from_response(self, response):\n    \"\\n        Returns a response signature and query string generated from the\\n        server response. 'h' aka signature argument is stripped from the\\n        returned query string.\\n        \"\n    lines = response.splitlines()\n    pairs = [line.strip().split('=', 1) for line in lines if ('=' in line)]\n    pairs = sorted(pairs)\n    signature = ([unquote(v) for (k, v) in pairs if (k == 'h')] or [None])[0]\n    query_string = '&'.join([((k + '=') + v) for (k, v) in pairs if (k != 'h')])\n    return (signature, query_string)\n", "label": 1}
{"function": "\n\ndef parse_csv(self, data):\n    if ((data == None) or (data == '')):\n        return\n    if isinstance(data, str):\n        str_data = data.split('\\n')\n    else:\n        str_data = data.decode(encoding='utf-8').split('\\n')\n    for f in str_data:\n        if (';' in str(f)):\n            s = f.split(';')[0]\n            ns = f.split(';')[1]\n            if ((ns is not None) and (ns != '')):\n                ns = int(re.sub('\\\\D', '', ns))\n                s = int(re.sub('\\\\D', '', s))\n                self.data.append(((s * 1000000000.0) + ns))\n", "label": 1}
{"function": "\n\ndef server_list(request, search_opts=None, all_tenants=False):\n    page_size = utils.get_page_size(request)\n    c = novaclient(request)\n    paginate = False\n    if (search_opts is None):\n        search_opts = {\n            \n        }\n    elif ('paginate' in search_opts):\n        paginate = search_opts.pop('paginate')\n        if paginate:\n            search_opts['limit'] = (page_size + 1)\n    if all_tenants:\n        search_opts['all_tenants'] = True\n    else:\n        search_opts['project_id'] = request.user.tenant_id\n    servers = [Server(s, request) for s in c.servers.list(True, search_opts)]\n    has_more_data = False\n    if (paginate and (len(servers) > page_size)):\n        servers.pop((- 1))\n        has_more_data = True\n    elif (paginate and (len(servers) == getattr(settings, 'API_RESULT_LIMIT', 1000))):\n        has_more_data = True\n    return (servers, has_more_data)\n", "label": 1}
{"function": "\n\n@view_config(renderer='ban.mak', route_name='ban')\ndef ban(request):\n    r = request\n    s = request.session\n    p = s['safe_post']\n    if (('logged_in_admin' not in s) or (s['logged_in_admin'] == False)):\n        return HTTPNotFound()\n    if ('ip' in p):\n        if (p['ip'].strip() == ''):\n            ip = None\n        else:\n            ip = p['ip']\n        if (p['username'].strip() == ''):\n            username = None\n            user_id = None\n        else:\n            username = p['username']\n        if (p['duration'].strip() == 'infinite'):\n            duration = None\n        else:\n            duration = 'timedelta({0})'.format(p['duration'])\n            duration = eval(duration)\n        if username:\n            user_id = users.get_user_by_name(username).id\n        b = Ban(ip=ip, username=username, duration=duration, user_id=user_id, added_by=s['users.id'])\n        dbsession = DBSession()\n        dbsession.add(b)\n    bans = general.list_bans()\n    return {\n        'bans': bans,\n    }\n", "label": 1}
{"function": "\n\ndef _check_available_product_reminder(self, card, last_state):\n    ' Check if card has new products '\n    current_prod = (card.products if card.products else [])\n    old_prod = (last_state.products if last_state.products else [])\n    for product in current_prod:\n        if (product not in old_prod):\n            return True\n    current_pending = (card.pending if card.pending else [])\n    old_pending = (last_state.pending if last_state.pending else [])\n    for pending in current_pending:\n        if (pending in old_pending):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef preprocessOptions(self):\n    'Processes \"action\" oriented options.'\n    if self.getOption('runit_install'):\n        self._install()\n    if HAS_DAEMONIZE:\n        if self.status:\n            if daemon.status(pidfile=self.pidfile, logger=self.logger):\n                sys.exit(0)\n            else:\n                sys.exit(1)\n        if self.kill:\n            if daemon.kill(pidfile=self.pidfile, logger=self.logger):\n                sys.exit(0)\n            else:\n                sys.exit(1)\n    if (self.options.tasks == []):\n        print('Available Tasks:')\n        for t in self.tasks:\n            print((' - %s' % t.__name__))\n        sys.exit(1)\n", "label": 1}
{"function": "\n\ndef login(self, server, username, password='', original_prompt='[#$]', login_timeout=10, auto_prompt_reset=True, sync_multiplier=1, port=23):\n    cmd = 'telnet -l {} {} {}'.format(username, server, port)\n    spawn._spawn(self, cmd)\n    try:\n        i = self.expect('(?i)(?:password)', timeout=login_timeout)\n        if (i == 0):\n            self.sendline(password)\n            i = self.expect([original_prompt, 'Login incorrect'], timeout=login_timeout)\n        if i:\n            raise pxssh.ExceptionPxssh('could not log in: password was incorrect')\n    except TIMEOUT:\n        if (not password):\n            pass\n        else:\n            raise pxssh.ExceptionPxssh('could not log in: did not see a password prompt')\n    if (not self.sync_original_prompt(sync_multiplier)):\n        self.close()\n        raise pxssh.ExceptionPxssh('could not synchronize with original prompt')\n    if auto_prompt_reset:\n        if (not self.set_unique_prompt()):\n            self.close()\n            message = 'could not set shell prompt (recieved: {}, expected: {}).'\n            raise pxssh.ExceptionPxssh(message.format(self.before, self.PROMPT))\n    return True\n", "label": 1}
{"function": "\n\ndef calc_max_bits(self, signed, values):\n    ' Calculates the maximim needed bits to represent a value '\n    b = 0\n    vmax = (- 10000000)\n    for val in values:\n        if signed:\n            b = ((b | val) if (val >= 0) else (b | ((~ val) << 1)))\n            vmax = (val if (vmax < val) else vmax)\n        else:\n            b |= val\n    bits = 0\n    if (b > 0):\n        bits = (len(self.bin(b)) - 2)\n        if (signed and (vmax > 0) and ((len(self.bin(vmax)) - 2) >= bits)):\n            bits += 1\n    return bits\n", "label": 1}
{"function": "\n\ndef _addNote(self, nick, whence, text, at=None, maximum=None):\n    if (at is None):\n        at = time.time()\n    if (maximum is None):\n        maximum = self.registryValue('maximum')\n    try:\n        notes = self._notes[nick]\n        if (maximum and (len(notes) >= maximum)):\n            raise QueueIsFull()\n        else:\n            notes.append((at, whence, text))\n    except KeyError:\n        self._notes[nick] = [(at, whence, text)]\n    if (('?' in nick) or (('*' in nick) and (nick not in self.wildcards))):\n        self.wildcards.append(nick)\n    self._flushNotes()\n", "label": 1}
{"function": "\n\n@login_required\n@permission_required('workshops.add_person', raise_exception=True)\ndef person_bulk_add(request):\n    if (request.method == 'POST'):\n        form = PersonBulkAddForm(request.POST, request.FILES)\n        if form.is_valid():\n            charset = (request.FILES['file'].charset or settings.DEFAULT_CHARSET)\n            stream = io.TextIOWrapper(request.FILES['file'].file, charset)\n            try:\n                (persons_tasks, empty_fields) = upload_person_task_csv(stream)\n            except csv.Error as e:\n                messages.add_message(request, messages.ERROR, 'Error processing uploaded .CSV file: {}'.format(e))\n            except UnicodeDecodeError as e:\n                messages.add_message(request, messages.ERROR, 'Please provide a file in {} encoding.'.format(charset))\n            else:\n                if empty_fields:\n                    msg_template = 'The following required fields were not found in the uploaded file: {}'\n                    msg = msg_template.format(', '.join(empty_fields))\n                    messages.add_message(request, messages.ERROR, msg)\n                else:\n                    request.session['bulk-add-people'] = persons_tasks\n                    return redirect('person_bulk_add_confirmation')\n    else:\n        form = PersonBulkAddForm()\n    context = {\n        'title': 'Bulk Add People',\n        'form': form,\n        'charset': settings.DEFAULT_CHARSET,\n    }\n    return render(request, 'workshops/person_bulk_add_form.html', context)\n", "label": 1}
{"function": "\n\ndef get_distance(self, dist_val, lookup_type):\n    '\\n        Returns a distance number in units of the field.  For example, if\\n        `D(km=1)` was passed in and the units of the field were in meters,\\n        then 1000 would be returned.\\n        '\n    if (len(dist_val) == 1):\n        (dist, option) = (dist_val[0], None)\n    else:\n        (dist, option) = dist_val\n    if isinstance(dist, Distance):\n        if self.geodetic:\n            if (SpatialBackend.postgis and (lookup_type == 'dwithin')):\n                raise TypeError('Only numeric values of degree units are allowed on geographic DWithin queries.')\n            dist_param = dist.m\n        else:\n            dist_param = getattr(dist, Distance.unit_attname(self.units_name))\n    else:\n        dist_param = dist\n    if (SpatialBackend.postgis and self.geodetic and (lookup_type != 'dwithin') and (option == 'spheroid')):\n        return [gqn(self._spheroid), dist_param]\n    else:\n        return [dist_param]\n", "label": 1}
{"function": "\n\ndef set_PWM(self, percent, ntries=15):\n    '\\n        '\n    count = 0\n    success = False\n    if (percent < 0.0):\n        percent = 0.0\n    if (percent > 100.0):\n        percent = 100.0\n    while ((not success) and (count <= ntries)):\n        if self.logger:\n            self.logger.info('Setting PWM value to {:.1f} %'.format(percent))\n        send_digital = int(((1023.0 * float(percent)) / 100.0))\n        send_string = 'P{:04d}!'.format(send_digital)\n        result = self.query(send_string)\n        count += 1\n        if result:\n            self.PWM = ((float(result[0]) * 100.0) / 1023.0)\n            if (abs((self.PWM - percent)) > 5.0):\n                if self.logger:\n                    self.logger.warning('  Failed to set PWM value!')\n                time.sleep(2)\n            else:\n                success = True\n            if self.logger:\n                self.logger.info('  PWM Value = {:.1f}'.format(self.PWM))\n", "label": 1}
{"function": "\n\ndef updateDropEffect(self, dataTransfer, event_type):\n    '\\n        http://dev.w3.org/html5/spec/dnd.html#dragevent\\n        '\n    dropEffect = 'none'\n    if (event_type in ['dragover', 'dragenter']):\n        ea = dataTransfer.getEffectAllowed()\n        if (ea == 'none'):\n            dropEffect = 'none'\n        elif (ea.startswith('copy') or (ea == 'all')):\n            dropEffect = 'copy'\n        elif ea.startswith('link'):\n            dropEffect = 'link'\n        elif (ea == 'move'):\n            dropEffect = 'move'\n        else:\n            dropEffect = 'copy'\n    elif (event_type in ['drop', 'dragend']):\n        dropEffect = self.currentDragOperation\n    dataTransfer.dropEffect = dropEffect\n", "label": 1}
{"function": "\n\ndef mustcontain(self, *strings, **kw):\n    '\\n        Assert that the response contains all of the strings passed\\n        in as arguments.\\n\\n        Equivalent to::\\n\\n            assert string in res\\n        '\n    if ('no' in kw):\n        no = kw['no']\n        del kw['no']\n        if isinstance(no, (six.binary_type, six.text_type)):\n            no = [no]\n    else:\n        no = []\n    if kw:\n        raise TypeError(\"The only keyword argument allowed is 'no'\")\n    for s in strings:\n        if (not (s in self)):\n            print(('Actual response (no %r):' % s), file=sys.stderr)\n            print(self, file=sys.stderr)\n            raise IndexError(('Body does not contain string %r' % s))\n    for no_s in no:\n        if (no_s in self):\n            print(('Actual response (has %r)' % no_s), file=sys.stderr)\n            print(self, file=sys.stderr)\n            raise IndexError(('Body contains string %r' % s))\n", "label": 1}
{"function": "\n\ndef lamdaContainer(x):\n    f = (lambda c: c)\n    g = (lambda c: (c if x else (c * c)))\n    y = f(x)\n    z = g(4)\n    print('Lambda with conditional expression gives', z)\n    if ('a' <= x <= y <= 'z'):\n        print('Four')\n    if ('a' <= x <= 'z'):\n        print('Yes')\n    if ('a' <= x > 'z'):\n        print('Yes1')\n    if ('a' <= ('1' if x else '2') > 'z'):\n        print('Yes2')\n    if ('a' <= ('1' if x else '2') > 'z' > undefined_global):\n        print('Yes3')\n    z = (lambda huhu=y: huhu)\n    print('Lambda defaulted gives', z())\n", "label": 1}
{"function": "\n\ndef parse_headers(message):\n    '\\n    Turn a Message object into a list of WSGI-style headers.\\n    '\n    headers_out = []\n    if six.PY3:\n        for (header, value) in message.items():\n            if (header.lower() not in filtered_headers):\n                headers_out.append((header, value))\n    else:\n        for full_header in message.headers:\n            if (not full_header):\n                continue\n            if full_header[0].isspace():\n                if (not headers_out):\n                    raise ValueError(('First header starts with a space (%r)' % full_header))\n                (last_header, last_value) = headers_out.pop()\n                value = ((last_value + ' ') + full_header.strip())\n                headers_out.append((last_header, value))\n                continue\n            try:\n                (header, value) = full_header.split(':', 1)\n            except:\n                raise ValueError(('Invalid header: %r' % full_header))\n            value = value.strip()\n            if (header.lower() not in filtered_headers):\n                headers_out.append((header, value))\n    return headers_out\n", "label": 1}
{"function": "\n\ndef _next_method(self):\n    'Read the next method from the source and process it\\n\\n        Once one complete method has been assembled, it is placed in the internal queue. This\\n        method will block until a complete `Method` has been constructed, which may consist of one\\n        or more frames.\\n        '\n    while (not self.method_queue):\n        try:\n            frame = self.transport.read_frame()\n        except Exception as exc:\n            if six.PY2:\n                (_, _, tb) = sys.exc_info()\n                exc.tb = tb\n            self.method_queue.append(exc)\n            break\n        self.frames_recv += 1\n        if (frame.frame_type not in (self.expected_types[frame.channel], 8)):\n            msg = 'Received frame type {} while expecting type: {}'.format(frame.frame_type, self.expected_types[frame.channel])\n            self.method_queue.append(UnexpectedFrame(msg, channel_id=frame.channel))\n        elif (frame.frame_type == FrameType.METHOD):\n            self._process_method_frame(frame)\n        elif (frame.frame_type == FrameType.HEADER):\n            self._process_content_header(frame)\n        elif (frame.frame_type == FrameType.BODY):\n            self._process_content_body(frame)\n", "label": 1}
{"function": "\n\ndef __unicode__(self):\n    qs = ['DELETE']\n    if self.fields:\n        qs += [', '.join(['{0}'.format(f) for f in self.fields])]\n    qs += ['FROM', self.table]\n    delete_option = []\n    if self.timestamp:\n        delete_option += ['TIMESTAMP {0}'.format(self.timestamp_normalized)]\n    if delete_option:\n        qs += [' USING {0} '.format(' AND '.join(delete_option))]\n    if self.where_clauses:\n        qs += [self._where]\n    if self.conditionals:\n        qs += [self._get_conditionals()]\n    if self.if_exists:\n        qs += ['IF EXISTS']\n    return ' '.join(qs)\n", "label": 1}
{"function": "\n\ndef InstallLibrary(name, version, explicit=True):\n    'Install a package.\\n\\n  If the installation is explicit then the user made the installation request,\\n  not a package as a dependency. Explicit installation leads to stricter\\n  version checking.\\n\\n  Args:\\n    name: Name of the requested package (already validated as available).\\n    version: The desired version (already validated as available).\\n    explicit: Explicitly requested by the user or implicitly because of a\\n      dependency.\\n  '\n    (installed_version, explicitly_installed) = installed.get(name, ([None] * 2))\n    if (name in sys.modules):\n        if explicit:\n            CheckInstalledVersion(name, version, explicit=True)\n        return\n    elif installed_version:\n        if (version == installed_version):\n            return\n        if explicit:\n            if explicitly_installed:\n                raise ValueError(('%s %s requested, but %s already in use' % (name, version, installed_version)))\n            RemoveLibrary(name)\n        else:\n            version_ob = distutils.version.LooseVersion(version)\n            installed_ob = distutils.version.LooseVersion(installed_version)\n            if (version_ob <= installed_ob):\n                return\n            else:\n                RemoveLibrary(name)\n    AddLibrary(name, version, explicit)\n    dep_details = PACKAGES[name][1][version]\n    if (not dep_details):\n        return\n    for (dep_name, dep_version) in dep_details:\n        InstallLibrary(dep_name, dep_version, explicit=False)\n", "label": 1}
{"function": "\n\ndef __init__(self, optimizers, failure_callback=None, ignore_newtrees=True, max_use_ratio=None, final_optimizers=None, cleanup_optimizers=None):\n    super(EquilibriumOptimizer, self).__init__(None, ignore_newtrees=ignore_newtrees, failure_callback=failure_callback)\n    self.local_optimizers_map = OrderedDict()\n    self.local_optimizers_all = []\n    self.global_optimizers = []\n    self.final_optimizers = []\n    self.cleanup_optimizers = []\n    for opt in optimizers:\n        if isinstance(opt, LocalOptimizer):\n            if (opt.tracks() is None):\n                self.local_optimizers_all.append(opt)\n            else:\n                for c in opt.tracks():\n                    self.local_optimizers_map.setdefault(c, []).append(opt)\n        else:\n            self.global_optimizers.append(opt)\n    if final_optimizers:\n        self.final_optimizers = final_optimizers\n    if cleanup_optimizers:\n        self.cleanup_optimizers = cleanup_optimizers\n    self.max_use_ratio = max_use_ratio\n    assert (self.max_use_ratio is not None), 'max_use_ratio has to be a number'\n", "label": 1}
{"function": "\n\ndef launchFile(path, line=(- 1), col1=(- 1), col2=(- 1)):\n    if (sys.platform == 'darwin'):\n        cleanPath = os.path.abspath(path)\n        if os.path.isdir(cleanPath):\n            os.system(('open %s' % cleanPath))\n        elif os.path.isfile(cleanPath):\n            if (line >= 0):\n                os.system(('mate -l %s \"%s\"' % (line, cleanPath)))\n            else:\n                os.system(('mate \"%s\"' % cleanPath))\n    elif (sys.platform == 'win32'):\n        cleanPath = os.path.abspath(path)\n        if os.path.isdir(cleanPath):\n            os.system(('explorer %s' % cleanPath))\n        elif os.path.isfile(cleanPath):\n            scriptPath = 'editMSVC.py'\n            command = ('%s \"%s\" %s %s %s' % (scriptPath, cleanPath, line, col1, col2))\n            os.system(command)\n", "label": 1}
{"function": "\n\ndef process_regular_2d_scalars(*args, **kwargs):\n    ' Converts different signatures to (x, y, s). '\n    args = convert_to_arrays(args)\n    for (index, arg) in enumerate(args):\n        if (not callable(arg)):\n            args[index] = np.atleast_2d(arg)\n    if (len(args) == 1):\n        s = args[0]\n        assert (len(s.shape) == 2), '2D array required'\n        (x, y) = np.indices(s.shape)\n    elif (len(args) == 3):\n        (x, y, s) = args\n        if callable(s):\n            s = s(x, y)\n    else:\n        raise ValueError('wrong number of arguments')\n    assert (len(s.shape) == 2), '2D array required'\n    if ('mask' in kwargs):\n        mask = kwargs['mask']\n        s[mask.astype('bool')] = np.nan\n        s = s.astype('float')\n    return (x, y, s)\n", "label": 1}
{"function": "\n\n@classmethod\ndef create(cls, host=None, binary=None, topic=None, manager=None, report_interval=None, periodic_interval=None, periodic_fuzzy_delay=None):\n    \"Instantiates class and passes back application object.\\n\\n        :param host: defaults to CONF.host\\n        :param binary: defaults to basename of executable\\n        :param topic: defaults to bin_name - 'neutron-' part\\n        :param manager: defaults to CONF.<topic>_manager\\n        :param report_interval: defaults to CONF.report_interval\\n        :param periodic_interval: defaults to CONF.periodic_interval\\n        :param periodic_fuzzy_delay: defaults to CONF.periodic_fuzzy_delay\\n\\n        \"\n    if (not host):\n        host = CONF.host\n    if (not binary):\n        binary = os.path.basename(inspect.stack()[(- 1)][1])\n    if (not topic):\n        topic = binary.rpartition('neutron-')[2]\n        topic = topic.replace('-', '_')\n    if (not manager):\n        manager = CONF.get(('%s_manager' % topic), None)\n    if (report_interval is None):\n        report_interval = CONF.report_interval\n    if (periodic_interval is None):\n        periodic_interval = CONF.periodic_interval\n    if (periodic_fuzzy_delay is None):\n        periodic_fuzzy_delay = CONF.periodic_fuzzy_delay\n    service_obj = cls(host, binary, topic, manager, report_interval=report_interval, periodic_interval=periodic_interval, periodic_fuzzy_delay=periodic_fuzzy_delay)\n    return service_obj\n", "label": 1}
{"function": "\n\ndef norm_cspace_id(cspace):\n    try:\n        cspace = ALIASES[cspace]\n    except (KeyError, TypeError):\n        pass\n    if isinstance(cspace, str):\n        if _CIECAM02_axes.issuperset(cspace):\n            return {\n                'name': 'CIECAM02-subset',\n                'ciecam02_space': CIECAM02Space.sRGB,\n                'axes': cspace,\n            }\n        else:\n            return {\n                'name': cspace,\n            }\n    elif isinstance(cspace, CIECAM02Space):\n        return {\n            'name': 'CIECAM02',\n            'ciecam02_space': cspace,\n        }\n    elif isinstance(cspace, LuoEtAl2006UniformSpace):\n        return {\n            'name': \"J'a'b'\",\n            'ciecam02_space': CIECAM02Space.sRGB,\n            'luoetal2006_space': cspace,\n        }\n    elif isinstance(cspace, dict):\n        if (cspace['name'] in ALIASES):\n            base = ALIASES[cspace['name']]\n            if (isinstance(base, dict) and (base['name'] == cspace['name'])):\n                return cspace\n            else:\n                base = norm_cspace_id(base)\n                cspace = dict(cspace)\n                del cspace['name']\n                base = dict(base)\n                base.update(cspace)\n                return base\n        return cspace\n    else:\n        raise ValueError(('unrecognized color space %r' % (cspace,)))\n", "label": 1}
{"function": "\n\ndef repeatable_expr_SA(parser, node, children):\n    expr = children[0]\n    rule = expr\n    if (len(children) > 1):\n        repeat_op = children[1]\n        if (len(repeat_op) > 1):\n            (repeat_op, modifiers) = repeat_op\n        else:\n            repeat_op = repeat_op[0]\n            modifiers = None\n        if (repeat_op == '?'):\n            rule = Optional(nodes=[expr])\n        elif (repeat_op == '*'):\n            rule = ZeroOrMore(nodes=[expr])\n        else:\n            rule = OneOrMore(nodes=[expr])\n        if modifiers:\n            (modifiers, position) = modifiers\n            if (repeat_op == '?'):\n                (line, col) = parser.pos_to_linecol(position)\n                raise TextXSyntaxError('Modifiers are not allowed for \"?\" operator at {}'.format(text((line, col))), line, col)\n            if ('sep' in modifiers):\n                sep = modifiers['sep']\n                rule = Sequence(nodes=[expr, ZeroOrMore(nodes=[Sequence(nodes=[sep, expr])])])\n                if (repeat_op == '*'):\n                    rule = Optional(nodes=[rule])\n            if ('eolterm' in modifiers):\n                rule.eolterm = True\n    return rule\n", "label": 1}
{"function": "\n\ndef __setitem__(self, key, value):\n    if (isinstance(value, bytearray) or isinstance(value, str)):\n        start_addr = 0\n        stop_addr = 0\n        if isinstance(key, slice):\n            start_addr = int(key.start)\n            stop_addr = int(key.stop)\n        else:\n            start_addr = int(key)\n            stop_addr = (int(key) + 1)\n        self.mem[start_addr:stop_addr] = value\n    else:\n        start_addr = 0\n        num_bytes = 0\n        if isinstance(key, slice):\n            start_addr = int(key.start)\n            num_bytes = (int(key.stop) - int(key.start))\n        else:\n            start_addr = int(key)\n            num_bytes = 1\n        if isinstance(value, Bits):\n            bits = value\n            assert ((value.nbits % 8) == 0)\n        else:\n            bits = Bits((num_bytes * 8), value)\n        for i in range(num_bytes):\n            self.mem[(start_addr + i)] = bits[(i * 8):((i * 8) + 8)]\n", "label": 1}
{"function": "\n\ndef formfield_for_dbfield(self, db_field, **kwargs):\n    \"\\n        Hook for specifying the form Field instance for a given database Field\\n        instance.\\n\\n        If kwargs are given, they're passed to the form Field's constructor.\\n        \"\n    request = kwargs.pop('request', None)\n    if db_field.choices:\n        return self.formfield_for_choice_field(db_field, request, **kwargs)\n    if isinstance(db_field, (models.ForeignKey, models.ManyToManyField)):\n        if (db_field.__class__ in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[db_field.__class__], **kwargs)\n        if isinstance(db_field, models.ForeignKey):\n            formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)\n        elif isinstance(db_field, models.ManyToManyField):\n            formfield = self.formfield_for_manytomany(db_field, request, **kwargs)\n        if (formfield and (db_field.name not in self.raw_id_fields)):\n            formfield.widget = widgets.RelatedFieldWidgetWrapper(formfield.widget, db_field.rel, self.admin_site)\n        return formfield\n    for klass in db_field.__class__.mro():\n        if (klass in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[klass], **kwargs)\n            return db_field.formfield(**kwargs)\n    return db_field.formfield(**kwargs)\n", "label": 1}
{"function": "\n\ndef test_get_open_files(self):\n    p = psutil.Process(os.getpid())\n    files = p.get_open_files()\n    self.assertFalse((TESTFN in files))\n    f = open(TESTFN, 'w')\n    call_until(p.get_open_files, ('len(ret) != %i' % len(files)))\n    filenames = [x.path for x in p.get_open_files()]\n    self.assertIn(TESTFN, filenames)\n    f.close()\n    for file in filenames:\n        assert os.path.isfile(file), file\n    cmdline = (\"import time; f = open(r'%s', 'r'); time.sleep(100);\" % TESTFN)\n    sproc = get_test_subprocess([PYTHON, '-c', cmdline], wait=True)\n    p = psutil.Process(sproc.pid)\n    for x in range(100):\n        filenames = [x.path for x in p.get_open_files()]\n        if (TESTFN in filenames):\n            break\n        time.sleep(0.01)\n    else:\n        self.assertIn(TESTFN, filenames)\n    for file in filenames:\n        assert os.path.isfile(file), file\n", "label": 1}
{"function": "\n\ndef expand_macro(self, formatter, name, content, args=None):\n    args = (args or {\n        \n    })\n    reponame = (args.get('repository') or '')\n    rev = args.get('revision')\n    repos = RepositoryManager(self.env).get_repository(reponame)\n    try:\n        changeset = repos.get_changeset(rev)\n        message = changeset.message\n        rev = changeset.rev\n        resource = repos.resource\n    except Exception:\n        message = content\n        resource = Resource('repository', reponame)\n    if (formatter.context.resource.realm == 'ticket'):\n        ticket_re = CommitTicketUpdater.ticket_re\n        if (not any(((int(tkt_id) == int(formatter.context.resource.id)) for tkt_id in ticket_re.findall(message)))):\n            return tag.p(_(\"(The changeset message doesn't reference this ticket)\"), class_='hint')\n    if ChangesetModule(self.env).wiki_format_messages:\n        return tag.div(format_to_html(self.env, formatter.context.child('changeset', rev, parent=resource), message, escape_newlines=True), class_='message')\n    else:\n        return tag.pre(message, class_='message')\n", "label": 1}
{"function": "\n\ndef run(self, command_line=''):\n    assert command_line, 'expected non-empty command_line'\n    parsed = parse_command_line(command_line)\n    global_range = None\n    if parsed.line_range.is_empty:\n        global_range = R(0, self._view.size())\n    else:\n        global_range = parsed.line_range.resolve(self._view)\n    pattern = parsed.command.pattern\n    if pattern:\n        ExGlobal.most_recent_pat = pattern\n    else:\n        pattern = ExGlobal.most_recent_pat\n    subcmd = parsed.command.subcommand\n    try:\n        matches = find_all_in_range(self._view, pattern, global_range.begin(), global_range.end())\n    except Exception as e:\n        msg = (\"Vintageous (global): %s ... in pattern '%s'\" % (str(e), pattern))\n        sublime.status_message(msg)\n        print(msg)\n        return\n    if ((not matches) or (not parsed.command.subcommand.cooperates_with_global)):\n        return\n    matches = [self._view.full_line(r.begin()) for r in matches]\n    matches = [[r.a, r.b] for r in matches]\n    self.window.run_command(subcmd.target_command, {\n        'command_line': str(subcmd),\n        'global_lines': matches,\n    })\n", "label": 1}
{"function": "\n\ndef yield_sentences(self):\n    test_file = io.open(self.test_file, 'r').read()\n    inst2ans = self.get_answers()\n    for text in bsoup(test_file).findAll('text'):\n        if (not text):\n            continue\n        textid = text['id']\n        context_doc = ' '.join([remove_tags(i) for i in str(text).split('\\n') if remove_tags(i)])\n        for sent in text.findAll('sentence'):\n            context_sent = ' '.join([remove_tags(i) for i in str(sent).split('\\n') if remove_tags(i)])\n            (yield (sent, context_sent, context_doc, inst2ans, textid))\n", "label": 1}
{"function": "\n\ndef _group_matching(tlist, start_ttype, start_value, end_ttype, end_value, cls, include_semicolon=False, recurse=False):\n\n    def _find_matching(i, tl, stt, sva, ett, eva):\n        depth = 1\n        for n in range(i, len(tl.tokens)):\n            t = tl.tokens[n]\n            if t.match(stt, sva):\n                depth += 1\n            elif t.match(ett, eva):\n                depth -= 1\n                if (depth == 1):\n                    return t\n        return None\n    [_group_matching(sgroup, start_ttype, start_value, end_ttype, end_value, cls, include_semicolon) for sgroup in tlist.get_sublists() if recurse]\n    if isinstance(tlist, cls):\n        idx = 1\n    else:\n        idx = 0\n    token = tlist.token_next_match(idx, start_ttype, start_value)\n    while token:\n        tidx = tlist.token_index(token)\n        end = _find_matching(tidx, tlist, start_ttype, start_value, end_ttype, end_value)\n        if (end is None):\n            idx = (tidx + 1)\n        else:\n            if include_semicolon:\n                next_ = tlist.token_next(tlist.token_index(end))\n                if (next_ and next_.match(T.Punctuation, ';')):\n                    end = next_\n            group = tlist.group_tokens(cls, tlist.tokens_between(token, end))\n            _group_matching(group, start_ttype, start_value, end_ttype, end_value, cls, include_semicolon)\n            idx = (tlist.token_index(group) + 1)\n        token = tlist.token_next_match(idx, start_ttype, start_value)\n", "label": 1}
{"function": "\n\ndef _k_modes_iter(X, centroids, cl_attr_freq, membship):\n    'Single iteration of k-modes clustering algorithm'\n    moves = 0\n    for (ipoint, curpoint) in enumerate(X):\n        clust = np.argmin(matching_dissim(centroids, curpoint))\n        if membship[(clust, ipoint)]:\n            continue\n        moves += 1\n        old_clust = np.argwhere(membship[:, ipoint])[0][0]\n        (cl_attr_freq, membship) = move_point_cat(curpoint, ipoint, clust, old_clust, cl_attr_freq, membship)\n        for iattr in range(len(curpoint)):\n            for curc in (clust, old_clust):\n                centroids[(curc, iattr)] = get_max_value_key(cl_attr_freq[curc][iattr])\n        if (sum(membship[old_clust, :]) == 0):\n            from_clust = membship.sum(axis=1).argmax()\n            choices = [ii for (ii, ch) in enumerate(membship[from_clust, :]) if ch]\n            rindx = np.random.choice(choices)\n            (cl_attr_freq, membship) = move_point_cat(X[rindx], rindx, old_clust, from_clust, cl_attr_freq, membship)\n    return (centroids, moves)\n", "label": 1}
{"function": "\n\ndef _get_ssl_kwargs(ssl=False, ssl_keyfile=None, ssl_certfile=None, ssl_cert_reqs=None, ssl_ca_certs=None, ssl_match_hostname=True):\n    ssl_kwargs = {\n        'ssl': ssl,\n    }\n    if ssl_keyfile:\n        ssl_kwargs['ssl'] = True\n        ssl_kwargs['ssl_keyfile'] = ssl_keyfile\n    if ssl_certfile:\n        ssl_kwargs['ssl'] = True\n        ssl_kwargs['ssl_certfile'] = ssl_certfile\n    if ssl_cert_reqs:\n        if (ssl_cert_reqs is 'none'):\n            ssl_cert_reqs = ssl_lib.CERT_NONE\n        elif (ssl_cert_reqs is 'optional'):\n            ssl_cert_reqs = ssl_lib.CERT_OPTIONAL\n        elif (ssl_cert_reqs is 'required'):\n            ssl_cert_reqs = ssl_lib.CERT_REQUIRED\n        ssl_kwargs['ssl_cert_reqs'] = ssl_cert_reqs\n    if ssl_ca_certs:\n        ssl_kwargs['ssl'] = True\n        ssl_kwargs['ssl_ca_certs'] = ssl_ca_certs\n    if ssl_kwargs.get('ssl', False):\n        ssl_kwargs['ssl_match_hostname'] = ssl_match_hostname\n    return ssl_kwargs\n", "label": 1}
{"function": "\n\ndef test_NOT_purge_duplicate_comments(self):\n    self.explore(self.traj)\n    self.traj.f_get('purge_duplicate_comments').f_unlock()\n    self.traj.hdf5.purge_duplicate_comments = 0\n    self.traj.f_get('results_summary').f_unlock()\n    self.traj.overview.results_summary = 0\n    self.make_run()\n    hdf5file = pt.openFile(self.filename, mode='a')\n    ncomments = {\n        \n    }\n    try:\n        traj_group = hdf5file.getNode(where='/', name=self.traj.v_name)\n        for node in traj_group._f_walkGroups():\n            if (('/derived_parameters/' in node._v_pathname) or ('/results/' in node._v_pathname)):\n                if ('SRVC_LEAF' in node._v_attrs):\n                    if ('SRVC_INIT_COMMENT' in node._v_attrs):\n                        comment = node._v_attrs['SRVC_INIT_COMMENT']\n                        if (comment not in ncomments):\n                            ncomments[comment] = 0\n                        ncomments[comment] += 1\n    finally:\n        hdf5file.close()\n    self.assertGreaterEqual(len(ncomments), 1)\n    self.assertTrue(any(((x > 1) for x in ncomments.values())))\n", "label": 1}
{"function": "\n\ndef convertObjectArgs(self, objectArgs):\n    if isinstance(objectArgs, list):\n        return [self.convertObjectArgs(x) for x in objectArgs]\n    if isinstance(objectArgs, dict):\n        if ('objectDefinition_' in objectArgs):\n            obj = self.extractObjectDefinition(objectArgs['objectDefinition_'])\n            if ('objectId_' in objectArgs):\n                self.incomingObjectCache.addObjectById(objectArgs['objectId_'], obj)\n            return obj\n        if ('objectId_' in objectArgs):\n            obj = self.incomingObjectCache.lookupObjectById(objectArgs['objectId_'])\n            return obj\n        tr = {\n            \n        }\n        for (k, v) in objectArgs.iteritems():\n            tr[k] = self.convertObjectArgs(v)\n        return tr\n    return objectArgs\n", "label": 1}
{"function": "\n\n@classmethod\ndef _generate_jar_template(cls, jars):\n    global_dep_attributes = set((Dependency(org=jar.org, name=jar.name, rev=jar.rev, mutable=jar.mutable, force=jar.force, transitive=jar.transitive) for jar in jars))\n    if (len(global_dep_attributes) != 1):\n        conflicting_dependencies = sorted((str(g) for g in global_dep_attributes))\n        raise cls.IvyResolveConflictingDepsError('Found conflicting dependencies:\\n\\t{}'.format('\\n\\t'.join(conflicting_dependencies)))\n    jar_attributes = global_dep_attributes.pop()\n    excludes = set()\n    for jar in jars:\n        excludes.update(jar.excludes)\n    any_have_url = False\n    artifacts = OrderedDict()\n    for jar in jars:\n        ext = jar.ext\n        url = jar.url\n        if url:\n            any_have_url = True\n        classifier = jar.classifier\n        artifact = Artifact(name=jar.name, type_=(ext or 'jar'), ext=ext, url=url, classifier=classifier)\n        artifacts[(ext, url, classifier)] = artifact\n    template = TemplateData(org=jar_attributes.org, module=jar_attributes.name, version=jar_attributes.rev, mutable=jar_attributes.mutable, force=jar_attributes.force, transitive=jar_attributes.transitive, artifacts=artifacts.values(), any_have_url=any_have_url, excludes=[cls._generate_exclude_template(exclude) for exclude in excludes])\n    return template\n", "label": 1}
{"function": "\n\ndef vsphere(account):\n    myCredAccount = CredAccountVSphere()\n    if (not ('hostname' in account)):\n        printer.out('hostname in vcenter account not found', printer.ERROR)\n        return\n    if (not ('username' in account)):\n        printer.out('username in vcenter account not found', printer.ERROR)\n        return\n    if (not ('password' in account)):\n        printer.out('password in vcenter account not found', printer.ERROR)\n        return\n    if (not ('name' in account)):\n        printer.out('name in vcenter account not found', printer.ERROR)\n        return\n    if ('proxyHostname' in account):\n        myCredAccount.proxyHost = account['proxyHostname']\n    if ('proxyPort' in account):\n        myCredAccount.proxyPort = account['proxyPort']\n    if ('port' in account):\n        port = int(account['port'])\n    else:\n        port = 443\n    myCredAccount.name = account['name']\n    myCredAccount.login = account['username']\n    myCredAccount.password = account['password']\n    myCredAccount.hostname = account['hostname']\n    myCredAccount.port = port\n    return myCredAccount\n", "label": 1}
{"function": "\n\ndef as_sql(self, compiler, connection):\n    if isinstance(self.lhs, MultiColSource):\n        from django.db.models.sql.where import WhereNode, SubqueryConstraint, AND, OR\n        root_constraint = WhereNode(connector=OR)\n        if self.rhs_is_direct_value():\n            values = [get_normalized_value(value, self.lhs) for value in self.rhs]\n            for value in values:\n                value_constraint = WhereNode()\n                for (source, target, val) in zip(self.lhs.sources, self.lhs.targets, value):\n                    lookup_class = target.get_lookup('exact')\n                    lookup = lookup_class(target.get_col(self.lhs.alias, source), val)\n                    value_constraint.add(lookup, AND)\n                root_constraint.add(value_constraint, OR)\n        else:\n            root_constraint.add(SubqueryConstraint(self.lhs.alias, [target.column for target in self.lhs.targets], [source.name for source in self.lhs.sources], self.rhs), AND)\n        return root_constraint.as_sql(compiler, connection)\n    else:\n        return super(RelatedIn, self).as_sql(compiler, connection)\n", "label": 1}
{"function": "\n\ndef __init__(self, string):\n    chunks = string.split('\\x1b')\n    self.chars = []\n    chars = list(chunks[0])\n    if (len(chunks) > 1):\n        for chunk in chunks[1:]:\n            if (chunk == '(B'):\n                chars.append(('\\x1b' + chunk))\n            else:\n                p = chunk.find('m')\n                if (p > 0):\n                    chars.append(('\\x1b' + chunk[:(p + 1)]))\n                    chars.extend(list(chunk[(p + 1):]))\n                else:\n                    chars.extend(list(chunk))\n    ansi = []\n    for char in chars:\n        if (char[0] == '\\x1b'):\n            ansi.append(char)\n        else:\n            self.chars.append((''.join(ansi) + char))\n            ansi = []\n    if (len(self.chars) > 2):\n        if (self.chars[(- 1)][0] == '\\x1b'):\n            self.chars[(- 2)] = (self.chars[(- 2)] + self.chars[(- 1)])\n            self.chars = self.chars[:(- 1)]\n", "label": 1}
{"function": "\n\ndef test_call_some_more():\n    from commonast import Name, Num, Starred, Keyword\n    code = 'foo(1, a, *b, c=3, **d)'\n    node = commonast.parse(code).body_nodes[0].value_node\n    assert isinstance(node, commonast.Call)\n    assert (len(node.arg_nodes) == 3)\n    assert (len(node.kwarg_nodes) == 2)\n    for (arg, cls) in zip((node.arg_nodes + node.kwarg_nodes), [Num, Name, Starred, Num, None.__class__]):\n        isinstance(arg, cls)\n    assert (node.arg_nodes[2].value_node.name == 'b')\n    assert (node.kwarg_nodes[1].name is None)\n    assert (node.kwarg_nodes[1].value_node.name == 'd')\n", "label": 1}
{"function": "\n\ndef test_edit_strings(self):\n    '\\n        Check access to view lotte for a resource in a private project\\n        '\n    URL = self.urls['translate_private']\n    for user in ['anonymous']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 302)\n    for user in ['registered']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 403)\n    for user in ['maintainer', 'team_coordinator', 'team_member']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 200)\n    URL = reverse('translate_resource', kwargs={\n        'project_slug': self.project_private.slug,\n        'resource_slug': self.resource_private.slug,\n        'lang_code': self.language_ar.code,\n    })\n    for user in ['anonymous']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 302)\n    for user in ['registered']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 403)\n    for user in ['team_coordinator', 'team_member']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 403)\n    for user in ['maintainer']:\n        response = self.client[user].get(URL)\n        self.failUnlessEqual(response.status_code, 200)\n", "label": 1}
{"function": "\n\ndef add(self, regions, forward, join):\n    total_bytes = sum((len(c) for c in regions))\n    if (total_bytes == 0):\n        return\n    index = self.index\n    try:\n        if (not join):\n            if (self.entries[index] and self.entries[index].same_as(regions)):\n                return\n        elif (self.entries[index] and self.entries[index].join_if_possible(regions, forward)):\n            return\n        index = ((index + 1) % self.KILL_RING_SIZE)\n        self.entries[index] = KillRing.Kill(regions)\n    finally:\n        self.index = index\n        self.entries[index].set_clipboard()\n", "label": 1}
{"function": "\n\ndef _handle(self, data):\n    self._buf_in += data\n    if self._handling:\n        return\n    self._handling = True\n    while True:\n        (before, sep, after) = self._buf_in.partition(b'\\n')\n        if (not sep):\n            break\n        try:\n            before = before.decode('utf-8', 'ignore')\n            data = json.loads(before)\n        except Exception as e:\n            msg.error('Unable to parse json: ', str_e(e))\n            msg.error('Data: ', before)\n            self._buf_in = after\n            continue\n        name = data.get('name')\n        self._buf_in = after\n        try:\n            msg.debug(('got data ' + (name or 'no name')))\n            self.emit('data', name, data)\n        except Exception as e:\n            api.send_error(('Error handling %s event.' % name), str_e(e))\n            if (name == 'room_info'):\n                editor.error_message(('Error joining workspace: %s' % str_e(e)))\n                self.stop()\n    self._handling = False\n", "label": 1}
{"function": "\n\ndef body_languages(self, msg):\n    \"\\n        Find the language related headers in the message and return a\\n        string suitable for the 'body language' element of a\\n        bodystructure reply.\\n\\n        Arguments:\\n        - `msg`: the message we are looking in..\\n        \"\n    langs = []\n    for (hdr, value) in msg.items():\n        if (hdr[(- 9):].lower() != '-language'):\n            continue\n        if (',' in value):\n            langs.extend(value.split(','))\n        elif (';' in value):\n            langs.extend(value.split(';'))\n        else:\n            langs.append(value)\n    if (len(langs) == 0):\n        return 'NIL'\n    elif (len(langs) == 1):\n        return ('\"%s\"' % langs[0])\n    else:\n        return ('(%s)' % ' '.join([str(x).strip() for x in langs]))\n", "label": 1}
{"function": "\n\ndef _get_matches(self, key_presses):\n    '\\n        For a list of :class:`KeyPress` instances. Give the matching handlers\\n        that would handle this.\\n        '\n    keys = tuple((k.key for k in key_presses))\n    cli = self._cli_ref()\n    with_mode = [b for b in self._registry.get_bindings_for_keys(keys) if b.filter(cli)]\n    if with_mode:\n        return with_mode\n    keys_any = tuple((keys[:(- 1)] + (Keys.Any,)))\n    with_mode_any = [b for b in self._registry.get_bindings_for_keys(keys_any) if b.filter(cli)]\n    if with_mode_any:\n        return with_mode_any\n    return []\n", "label": 1}
{"function": "\n\ndef file_filter(state, dirname, fnames):\n    if (args.dir_masks and (not any([re.search(x, dirname) for x in args.dir_masks]))):\n        return\n    for f in fnames:\n        p = os.path.abspath(os.path.join(os.path.realpath(dirname), f))\n        if (any([re.search(x, f) for x in args.file_masks]) or any([re.search(x, p) for x in args.path_masks])):\n            if os.path.isfile(p):\n                state['files'].append(p)\n", "label": 1}
{"function": "\n\n@wsgi.extends\ndef detail(self, req, resp_obj):\n    context = req.environ['nova.context']\n    authorize_extend = False\n    authorize_host_status = False\n    if authorize(context):\n        authorize_extend = True\n    if (api_version_request.is_supported(req, min_version='2.16') and soft_authorize(context, action='show:host_status')):\n        authorize_host_status = True\n    if (authorize_extend or authorize_host_status):\n        servers = list(resp_obj.obj['servers'])\n        instances = req.get_db_instances()\n        if authorize_host_status:\n            host_statuses = self.compute_api.get_instances_host_statuses(instances.values())\n        for server in servers:\n            if authorize_extend:\n                instance = instances[server['id']]\n                self._extend_server(context, server, instance, req)\n            if authorize_host_status:\n                server['host_status'] = host_statuses[server['id']]\n", "label": 1}
{"function": "\n\ndef test_main():\n    tests = []\n    test_suite = unittest.TestSuite()\n    tests.append(TestCase)\n    tests.append(TestFetchAllProcesses)\n    if POSIX:\n        from _posix import PosixSpecificTestCase\n        tests.append(PosixSpecificTestCase)\n    if LINUX:\n        from _linux import LinuxSpecificTestCase as stc\n    elif WINDOWS:\n        from _windows import WindowsSpecificTestCase as stc\n        from _windows import TestDualProcessImplementation\n        tests.append(TestDualProcessImplementation)\n    elif OSX:\n        from _osx import OSXSpecificTestCase as stc\n    elif BSD:\n        from _bsd import BSDSpecificTestCase as stc\n    tests.append(stc)\n    if hasattr(os, 'getuid'):\n        if (os.getuid() == 0):\n            tests.append(LimitedUserTestCase)\n        else:\n            atexit.register(warn, \"Couldn't run limited user tests (super-user privileges are required)\")\n    for test_class in tests:\n        test_suite.addTest(unittest.makeSuite(test_class))\n    unittest.TextTestRunner(verbosity=2).run(test_suite)\n", "label": 1}
{"function": "\n\ndef __init__(self, RGB1, RGB2, numColors=33.0, divide=255.0, method='moreland', filename=''):\n    self.numColors = numColors\n    assert (np.mod(numColors, 2) == 1), 'For diverging colormaps odd numbers of colors are desireable!'\n    knownMethods = ['moreland', 'lab']\n    assert (method in knownMethods), 'Unknown method was specified!'\n    if (method == knownMethods[0]):\n        self.colorMap = self.generateColorMap(RGB1, RGB2, divide)\n    elif (method == knownMethods[1]):\n        self.colorMap = self.generateColorMapLab(RGB1, RGB2, divide)\n    if (filename == ''):\n        for c in self.colorMap:\n            pass\n    else:\n        with open(filename, 'w') as f:\n            for c in self.colorMap:\n                f.write('{0}, {1}, {2}\\n'.format(c[0], c[1], c[2]))\n", "label": 1}
{"function": "\n\ndef test_values_list_form_has_metadata(self):\n    'Test default results form has metadata.'\n    searcher = list(self.get_s().query(foo='bar').values_list('id'))\n    assert hasattr(searcher[0], '_id')\n    assert hasattr(searcher[0].es_meta, 'id')\n    assert hasattr(searcher[0].es_meta, 'score')\n    assert hasattr(searcher[0].es_meta, 'source')\n    assert hasattr(searcher[0].es_meta, 'type')\n    assert hasattr(searcher[0].es_meta, 'explanation')\n    assert hasattr(searcher[0].es_meta, 'highlight')\n", "label": 1}
{"function": "\n\ndef CheckRemoteGitState(self):\n    'Checks the state of the remote git repository.\\n\\n    Returns:\\n      A boolean value to indicate if the state is sane.\\n    '\n    if (self._command == 'close'):\n        if (not self._git_helper.SynchronizeWithUpstream()):\n            print('{0:s} aborted - unable to synchronize with upstream/master.'.format(self._command.title()))\n            return False\n    elif (self._command in ('create', 'update')):\n        if (not self._git_helper.CheckSynchronizedWithUpstream()):\n            if (not self._git_helper.SynchronizeWithUpstream()):\n                print('{0:s} aborted - unable to synchronize with upstream/master.'.format(self._command.title()))\n                return False\n            force_push = True\n        else:\n            force_push = False\n        if (not self._git_helper.PushToOrigin(self._active_branch, force=force_push)):\n            print('{0:s} aborted - unable to push updates to origin/{1:s}.'.format(self._command.title(), self._active_branch))\n            return False\n    elif (self._command == 'lint'):\n        self._git_helper.CheckSynchronizedWithUpstream()\n    elif (self._command == 'merge'):\n        if (not self._git_helper.SynchronizeWithOrigin()):\n            print('{0:s} aborted - unable to synchronize with origin/master.'.format(self._command.title()))\n            return False\n    return True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _validate_publish_parameters(body, exchange, immediate, mandatory, properties, routing_key):\n    'Validate Publish Parameters.\\n\\n        :param str|unicode body:\\n        :param str routing_key:\\n        :param str exchange:\\n        :param dict properties:\\n        :param bool mandatory:\\n        :param bool immediate:\\n        :raises  AMQPInvalidArgument: Invalid Parameters\\n        :return:\\n        '\n    if (not compatibility.is_string(body)):\n        raise AMQPInvalidArgument('body should be a string')\n    elif (not compatibility.is_string(routing_key)):\n        raise AMQPInvalidArgument('routing_key should be a string')\n    elif (not compatibility.is_string(exchange)):\n        raise AMQPInvalidArgument('exchange should be a string')\n    elif (properties and (not isinstance(properties, dict))):\n        raise AMQPInvalidArgument('properties should be a dict or None')\n    elif (not isinstance(mandatory, bool)):\n        raise AMQPInvalidArgument('mandatory should be a boolean')\n    elif (not isinstance(immediate, bool)):\n        raise AMQPInvalidArgument('immediate should be a boolean')\n", "label": 1}
{"function": "\n\ndef handle_form_and_email(request, form=None, form_template='', message_template='', subject='', recipients=None, redirect_to='/thanks/', sender=None, uses_captcha=True, *args, **kwargs):\n    '\\n    Abstracts the rendering and processing of e-mail forms.\\n    '\n    if uses_captcha:\n        remote_ip = request.META['REMOTE_ADDR']\n    if (request.method == 'POST'):\n        if uses_captcha:\n            form = form(remote_ip, request.POST)\n        else:\n            form = form(request.POST)\n        if form.is_valid():\n            try:\n                if form.cleaned_data['cc_self']:\n                    recipients.append(form.cleaned_data['self_email'])\n                if (sender == 'self'):\n                    sender = form.cleaned_data['self_email']\n            except:\n                pass\n            render_email_and_send(message_template=message_template, context=form.cleaned_data, subject=subject, recipients=recipients, sender=sender)\n            return HttpResponseRedirect('/thanks/')\n    elif uses_captcha:\n        form = form(remote_ip)\n    else:\n        form = form()\n    page = {\n        'form': form,\n    }\n    return render_to_response(form_template, page, context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef send_subscription_change_alert(domain, new_subscription, old_subscription, internal_change):\n    billing_account = (new_subscription.account if new_subscription else (old_subscription.account if old_subscription else None))\n    request = get_request()\n    email_context = {\n        'domain': domain,\n        'domain_url': get_default_domain_url(domain),\n        'old_plan': (old_subscription.plan_version if old_subscription else None),\n        'new_plan': (new_subscription.plan_version if new_subscription else None),\n        'old_subscription': old_subscription,\n        'new_subscription': new_subscription,\n        'billing_account': billing_account,\n        'username': (request.couch_user.username if getattr(request, 'couch_user', None) else None),\n        'referer': (request.META.get('HTTP_REFERER') if request else None),\n    }\n    email_subject = '{env}Subscription Change Alert: {domain} from {old_plan} to {new_plan}'.format(env=('[{}] '.format(settings.SERVER_ENVIRONMENT.upper()) if (settings.SERVER_ENVIRONMENT == 'staging') else ''), domain=email_context['domain'], old_plan=email_context['old_plan'], new_plan=email_context['new_plan'])\n    sub_change_email_address = (settings.INTERNAL_SUBSCRIPTION_CHANGE_EMAIL if internal_change else settings.SUBSCRIPTION_CHANGE_EMAIL)\n    send_html_email_async.delay(email_subject, sub_change_email_address, render_to_string('accounting/subscription_change_email.html', email_context), text_content=render_to_string('accounting/subscription_change_email.txt', email_context))\n", "label": 1}
{"function": "\n\ndef send_frame(self, cmd, headers={\n    \n}, body=''):\n    frame = utils.Frame(cmd, headers, body)\n    if (cmd == CMD_BEGIN):\n        trans = headers[HDR_TRANSACTION]\n        if (trans in self.transactions):\n            self.notify('error', {\n                \n            }, ('Transaction %s already started' % trans))\n        else:\n            self.transactions[trans] = []\n    elif (cmd == CMD_COMMIT):\n        trans = headers[HDR_TRANSACTION]\n        if (trans not in self.transactions):\n            self.notify('error', {\n                \n            }, ('Transaction %s not started' % trans))\n        else:\n            for f in self.transactions[trans]:\n                self.transport.transmit(f)\n            del self.transactions[trans]\n    elif (cmd == CMD_ABORT):\n        trans = headers['transaction']\n        del self.transactions[trans]\n    elif ('transaction' in headers):\n        trans = headers['transaction']\n        if (trans not in self.transactions):\n            self.transport.notify('error', {\n                \n            }, ('Transaction %s not started' % trans))\n            return\n        else:\n            self.transactions[trans].append(frame)\n    else:\n        self.transport.transmit(frame)\n", "label": 1}
{"function": "\n\ndef get_query_string(self, new_params=None, remove=None):\n    if (new_params is None):\n        new_params = {\n            \n        }\n    if (remove is None):\n        remove = []\n    p = copy(self.params)\n    for r in remove:\n        for k in p.keys():\n            if k.startswith(r):\n                del p[k]\n    for (k, v) in new_params.items():\n        if (v is None):\n            if (k in p):\n                del p[k]\n        else:\n            p[k] = v\n    if hasattr(p, 'urlencode'):\n        return ('?%s' % p.urlencode())\n    return ('?%s' % urlencode(p))\n", "label": 1}
{"function": "\n\ndef minimalBases(classes):\n    'Reduce a list of base classes to its ordered minimum equivalent'\n    classes = [c for c in classes if (not _py2to3.is_old_style_class(c))]\n    candidates = []\n    for m in classes:\n        for n in classes:\n            if (issubclass(n, m) and (m is not n)):\n                break\n        else:\n            if (m in candidates):\n                candidates.remove(m)\n            candidates.append(m)\n    return candidates\n", "label": 1}
{"function": "\n\ndef __init__(self, name, fields, options=None, bases=None, managers=None):\n    self.fields = fields\n    self.options = (options or {\n        \n    })\n    self.bases = (bases or (models.Model,))\n    self.managers = (managers or [])\n    super(CreateModel, self).__init__(name)\n    _check_for_duplicates('fields', (name for (name, _) in self.fields))\n    _check_for_duplicates('bases', ((base._meta.label_lower if hasattr(base, '_meta') else (base.lower() if isinstance(base, six.string_types) else base)) for base in self.bases))\n    _check_for_duplicates('managers', (name for (name, _) in self.managers))\n", "label": 1}
{"function": "\n\n@mock.patch((MODPATH + '.BaseOpenStackService._get_meta_data'))\n@mock.patch((MODPATH + '.BaseOpenStackService.get_user_data'))\ndef _test_get_client_auth_certs(self, mock_get_user_data, mock_get_meta_data, meta_data, ret_value=None):\n    mock_get_meta_data.return_value = meta_data\n    mock_get_user_data.side_effect = [ret_value]\n    response = self._service.get_client_auth_certs()\n    mock_get_meta_data.assert_called_once_with()\n    if (isinstance(ret_value, bytes) and ret_value.startswith(x509constants.PEM_HEADER.encode())):\n        mock_get_user_data.assert_called_once_with()\n        self.assertEqual([ret_value.decode()], response)\n    elif (ret_value is base.NotExistingMetadataException):\n        self.assertFalse(response)\n    else:\n        expected = []\n        expectation = {\n            'meta': 'fake cert',\n            'keys': [key['data'].strip() for key in self._fake_keys if (key['type'] == 'x509')],\n        }\n        for (field, value) in expectation.items():\n            if (field in meta_data):\n                expected.extend((value if isinstance(value, list) else [value]))\n        self.assertEqual(sorted(list(set(expected))), sorted(response))\n", "label": 1}
{"function": "\n\ndef _run(self):\n    args = self.args\n\n    def notes(noteArgs, **kwargs):\n        gitArgs = ('notes --ref=%s %s %s' % (noteNamespace, noteArgs, args.commit))\n        return git(gitArgs, **kwargs)\n\n    def add(note):\n        notes(('add -fm \"%s\"' % note))\n    note = notes('show', errorValue='')\n    if args.delete:\n        if (not note):\n            return\n        workitem = args.workitem\n        if workitem:\n            items = note.split(',')\n            try:\n                items.remove(str(workitem))\n            except ValueError:\n                fail(('Workitem %s is not associated with %s' % (workitem, args.commit)))\n            if items:\n                add(','.join(items))\n                return\n        notes('remove')\n    elif args.workitem:\n        if note:\n            note += ','\n        add((note + str(args.workitem)))\n    elif note:\n        print(note)\n", "label": 1}
{"function": "\n\ndef setup_databases(verbosity, interactive, **kwargs):\n    from django.db import connections, DEFAULT_DB_ALIAS\n    mirrored_aliases = {\n        \n    }\n    test_databases = {\n        \n    }\n    dependencies = {\n        \n    }\n    default_sig = connections[DEFAULT_DB_ALIAS].creation.test_db_signature()\n    for alias in connections:\n        connection = connections[alias]\n        if connection.settings_dict['TEST_MIRROR']:\n            mirrored_aliases[alias] = connection.settings_dict['TEST_MIRROR']\n        else:\n            item = test_databases.setdefault(connection.creation.test_db_signature(), (connection.settings_dict['NAME'], set()))\n            item[1].add(alias)\n            if ('TEST_DEPENDENCIES' in connection.settings_dict):\n                dependencies[alias] = connection.settings_dict['TEST_DEPENDENCIES']\n            elif ((alias != DEFAULT_DB_ALIAS) and (connection.creation.test_db_signature() != default_sig)):\n                dependencies[alias] = connection.settings_dict.get('TEST_DEPENDENCIES', [DEFAULT_DB_ALIAS])\n    old_names = []\n    mirrors = []\n    for (signature, (db_name, aliases)) in dependency_ordered(test_databases.items(), dependencies):\n        test_db_name = None\n        for alias in aliases:\n            connection = connections[alias]\n            if (test_db_name is None):\n                test_db_name = connection.creation.create_test_db(verbosity, autoclobber=(not interactive))\n                destroy = True\n            else:\n                connection.settings_dict['NAME'] = test_db_name\n                destroy = False\n            old_names.append((connection, db_name, destroy))\n    for (alias, mirror_alias) in mirrored_aliases.items():\n        mirrors.append((alias, connections[alias].settings_dict['NAME']))\n        connections[alias].settings_dict['NAME'] = connections[mirror_alias].settings_dict['NAME']\n    return (old_names, mirrors)\n", "label": 1}
{"function": "\n\ndef _read_unlocked(self, n=None):\n    nodata_val = b''\n    empty_values = (b'', None)\n    buf = self._read_buf\n    pos = self._read_pos\n    if ((n is None) or (n == (- 1))):\n        self._reset_read_buf()\n        chunks = [buf[pos:]]\n        current_size = 0\n        while True:\n            chunk = self.raw.read()\n            if (chunk in empty_values):\n                nodata_val = chunk\n                break\n            current_size += len(chunk)\n            chunks.append(chunk)\n        return (b''.join(chunks) or nodata_val)\n    avail = (len(buf) - pos)\n    if (n <= avail):\n        self._read_pos += n\n        return buf[pos:(pos + n)]\n    chunks = [buf[pos:]]\n    wanted = max(self.buffer_size, n)\n    while (avail < n):\n        chunk = self.raw.read(wanted)\n        if (chunk in empty_values):\n            nodata_val = chunk\n            break\n        avail += len(chunk)\n        chunks.append(chunk)\n    n = min(n, avail)\n    out = b''.join(chunks)\n    self._read_buf = out[n:]\n    self._read_pos = 0\n    return (out[:n] if out else nodata_val)\n", "label": 1}
{"function": "\n\ndef _parse_waf(context, repos, record, identifier):\n    recobjs = []\n    content = util.http_request('GET', record)\n    LOGGER.debug(content)\n    try:\n        parser = etree.HTMLParser()\n        tree = etree.fromstring(content, parser)\n    except Exception as err:\n        raise Exception(('Could not parse WAF: %s' % str(err)))\n    up = urlparse(record)\n    links = []\n    LOGGER.debug('collecting links')\n    for link in tree.xpath('//a/@href'):\n        link = link.strip()\n        if (not link):\n            continue\n        if (link.find('?') != (- 1)):\n            continue\n        if (not link.endswith('.xml')):\n            LOGGER.debug('Skipping, not .xml')\n            continue\n        if ('/' in link):\n            if (link[(- 1)] == '/'):\n                continue\n            if (link[0] == '/'):\n                link = ('%s://%s%s' % (up.scheme, up.netloc, link))\n        else:\n            link = ('%s/%s' % (record, link))\n        LOGGER.debug('URL is: %s', link)\n        links.append(link)\n    LOGGER.debug('%d links found', len(links))\n    for link in links:\n        LOGGER.debug('Processing link %s', link)\n        linkcontent = util.http_request('GET', link)\n        recobj = _parse_metadata(context, repos, linkcontent)[0]\n        recobj.source = link\n        recobj.mdsource = link\n        recobjs.append(recobj)\n    return recobjs\n", "label": 1}
{"function": "\n\ndef _get_article_metadata(self, meta):\n    adict = PYConf()\n    adict.title = meta['title'][0]\n    adict.postid = meta['postid'][0]\n    adict.nicename = meta['nicename'][0]\n    adict.slug = meta['slug'][0]\n    adict.date = self.get_datetime(meta['date'][0])\n    adict.author = meta['author'][0]\n    tags = meta.get('tags')\n    if tags:\n        adict.tags = [tag.strip() for tag in tags[0].split(',')]\n    category = meta.get('category')\n    if category:\n        adict.category = [cat.strip() for cat in category[0].split(',')]\n    modified = meta.get('modified')\n    if modified:\n        adict.modified = self.get_datetime(modified[0])\n    posttype = meta.get('posttype')\n    if posttype:\n        adict.posttype = posttype[0]\n    else:\n        adict.posttype = 'post'\n    poststatus = meta.get('poststatus')\n    if poststatus:\n        adict.poststatus = poststatus[0]\n    else:\n        adict.poststatus = 'publish'\n    attachments = meta.get('attachments')\n    if attachments:\n        adict.attachments = [att.strip() for att in attachments[0].split(',')]\n    return adict\n", "label": 1}
{"function": "\n\ndef process_view(self, request, view_func, view_args, view_kwargs):\n    'Forwards unauthenticated requests to the admin page to the CAS\\n        login URL, as well as calls to django.contrib.auth.views.login and\\n        logout.\\n        '\n    if (view_func == login):\n        return cas_login(request, *view_args, **view_kwargs)\n    elif (view_func == logout):\n        return cas_logout(request, *view_args, **view_kwargs)\n    if (view_func in (cas_login, cas_logout)):\n        return None\n    if settings.CAS_ADMIN_PREFIX:\n        if (not request.path.startswith(settings.CAS_ADMIN_PREFIX)):\n            return None\n    elif (not view_func.__module__.startswith('django.contrib.admin.')):\n        return None\n    if request.user.is_authenticated():\n        if request.user.is_staff:\n            return None\n        else:\n            error = '<h1>Forbidden</h1><p>You do not have staff privileges.</p>'\n            return HttpResponseForbidden(error)\n    params = urllib_parse.urlencode({\n        REDIRECT_FIELD_NAME: request.get_full_path(),\n    })\n    return HttpResponseRedirect(((reverse(cas_login) + '?') + params))\n", "label": 1}
{"function": "\n\ndef load_profile(self, profile_directory):\n    list_files = os.listdir(profile_directory)\n    if (not list_files):\n        raise LangDetectException(ErrorCode.NeedLoadProfileError, ('Not found profile: ' + profile_directory))\n    (langsize, index) = (len(list_files), 0)\n    for filename in list_files:\n        if filename.startswith('.'):\n            continue\n        filename = path.join(profile_directory, filename)\n        if (not path.isfile(filename)):\n            continue\n        f = None\n        try:\n            if (sys.version_info[0] < 3):\n                f = open(filename, 'r')\n            else:\n                f = open(filename, 'r', encoding='utf-8')\n            json_data = json.load(f)\n            profile = LangProfile(**json_data)\n            self.add_profile(profile, index, langsize)\n            index += 1\n        except IOError:\n            raise LangDetectException(ErrorCode.FileLoadError, ('Cannot open \"%s\"' % filename))\n        except:\n            raise LangDetectException(ErrorCode.FormatError, ('Profile format error in \"%s\"' % filename))\n        finally:\n            if f:\n                f.close()\n", "label": 1}
{"function": "\n\ndef get_aruba_data(self):\n    'Retrieve data from Aruba Access Point and return parsed result.'\n    import pexpect\n    connect = 'ssh {}@{}'\n    ssh = pexpect.spawn(connect.format(self.username, self.host))\n    query = ssh.expect(['password:', pexpect.TIMEOUT, pexpect.EOF, 'continue connecting (yes/no)?', 'Host key verification failed.', 'Connection refused', 'Connection timed out'], timeout=120)\n    if (query == 1):\n        _LOGGER.error('Timeout')\n        return\n    elif (query == 2):\n        _LOGGER.error('Unexpected response from router')\n        return\n    elif (query == 3):\n        ssh.sendline('yes')\n        ssh.expect('password:')\n    elif (query == 4):\n        _LOGGER.error('Host key Changed')\n        return\n    elif (query == 5):\n        _LOGGER.error('Connection refused by server')\n        return\n    elif (query == 6):\n        _LOGGER.error('Connection timed out')\n        return\n    ssh.sendline(self.password)\n    ssh.expect('#')\n    ssh.sendline('show clients')\n    ssh.expect('#')\n    devices_result = ssh.before.split(b'\\r\\n')\n    ssh.sendline('exit')\n    devices = {\n        \n    }\n    for device in devices_result:\n        match = _DEVICES_REGEX.search(device.decode('utf-8'))\n        if match:\n            devices[match.group('ip')] = {\n                'ip': match.group('ip'),\n                'mac': match.group('mac').upper(),\n                'name': match.group('name'),\n            }\n    return devices\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TAlterSentryRoleRevokePrivilegeRequest')\n    if (self.protocol_version is not None):\n        oprot.writeFieldBegin('protocol_version', TType.I32, 1)\n        oprot.writeI32(self.protocol_version)\n        oprot.writeFieldEnd()\n    if (self.requestorUserName is not None):\n        oprot.writeFieldBegin('requestorUserName', TType.STRING, 2)\n        oprot.writeString(self.requestorUserName)\n        oprot.writeFieldEnd()\n    if (self.roleName is not None):\n        oprot.writeFieldBegin('roleName', TType.STRING, 3)\n        oprot.writeString(self.roleName)\n        oprot.writeFieldEnd()\n    if (self.component is not None):\n        oprot.writeFieldBegin('component', TType.STRING, 4)\n        oprot.writeString(self.component)\n        oprot.writeFieldEnd()\n    if (self.privilege is not None):\n        oprot.writeFieldBegin('privilege', TType.STRUCT, 5)\n        self.privilege.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef handleJsonMessage(self, incomingJsonMessage):\n    if (not isinstance(incomingJsonMessage, dict)):\n        raise MalformedMessageException(('Incoming message was not a dictionary: %s' % incomingJsonMessage))\n    if (not ('messageId' in incomingJsonMessage)):\n        raise MalformedMessageException(('Invalid incoming message id: %s' % incomingJsonMessage))\n    if (incomingJsonMessage['messageId'] != self.expectedMessageId):\n        raise MalformedMessageException(('Invalid incoming message id: expected %s, but got %s. %s' % (self.expectedMessageId, incomingJsonMessage['messageId'], incomingJsonMessage)))\n    try:\n        self.expectedMessageId += 1\n        if (incomingJsonMessage['messageType'] not in self.messageTypeHandlers):\n            raise MalformedMessageException('Invalid incoming messageType')\n        if (not ('objectDefinition' in incomingJsonMessage)):\n            raise MalformedMessageException('No object definition given')\n        if (incomingJsonMessage['messageType'] != 'ServerFlushObjectIdsBelow'):\n            obj = self.extractObjectDefinition(incomingJsonMessage['objectDefinition'])\n        else:\n            obj = None\n        return self.messageTypeHandlers[incomingJsonMessage['messageType']](incomingJsonMessage, obj)\n    except MalformedMessageException:\n        raise\n    except Exception as e:\n        return [unexpectedExceptionJson(incomingJsonMessage, Exceptions.wrapException(e).message)]\n", "label": 1}
{"function": "\n\ndef _check_1d(self, routine, dtype, shape, axis, overwritable_dtypes):\n    np.random.seed(1234)\n    if np.issubdtype(dtype, np.complexfloating):\n        data = (np.random.randn(*shape) + (1j * np.random.randn(*shape)))\n    else:\n        data = np.random.randn(*shape)\n    data = data.astype(dtype)\n    for type in [1, 2, 3]:\n        for overwrite_x in [True, False]:\n            for norm in [None, 'ortho']:\n                if ((type == 1) and (norm == 'ortho')):\n                    continue\n                should_overwrite = (overwrite_x and (dtype in overwritable_dtypes) and ((len(shape) == 1) or ((axis % len(shape)) == (len(shape) - 1))))\n                self._check(data, routine, type, None, axis, norm, overwrite_x, should_overwrite)\n", "label": 1}
{"function": "\n\ndef process_pass_fail(self):\n    '\\n        :return: etree element\\n        '\n    mods = (self.engine.reporters + self.engine.services)\n    pass_fail_objects = [_x for _x in mods if isinstance(_x, PassFailStatus)]\n    self.log.debug('Processing passfail objects: %s', pass_fail_objects)\n    fail_criterias = []\n    for pf_obj in pass_fail_objects:\n        if pf_obj.criterias:\n            for _fc in pf_obj.criterias:\n                fail_criterias.append(_fc)\n    root_xml_element = etree.Element('testsuite', name='bzt_pass_fail', package='bzt')\n    bza_report_info = self.get_bza_report_info()\n    classname = (bza_report_info[0][1] if bza_report_info else ('bzt-' + str(self.__hash__())))\n    report_urls = [info_item[0] for info_item in bza_report_info]\n    for fc_obj in fail_criterias:\n        testcase_etree = self.__process_criteria(classname, fc_obj, report_urls)\n        root_xml_element.append(testcase_etree)\n    return root_xml_element\n", "label": 1}
{"function": "\n\ndef field_repr(field, force_many=False):\n    kwargs = field._kwargs\n    if force_many:\n        kwargs = kwargs.copy()\n        kwargs['many'] = True\n        kwargs.pop('child', None)\n    if (kwargs.get('label', None) is None):\n        kwargs.pop('label', None)\n    if (kwargs.get('help_text', None) is None):\n        kwargs.pop('help_text', None)\n    arg_string = ', '.join([smart_repr(val) for val in field._args])\n    kwarg_string = ', '.join([('%s=%s' % (key, smart_repr(val))) for (key, val) in sorted(kwargs.items())])\n    if (arg_string and kwarg_string):\n        arg_string += ', '\n    if force_many:\n        class_name = force_many.__class__.__name__\n    else:\n        class_name = field.__class__.__name__\n    return ('%s(%s%s)' % (class_name, arg_string, kwarg_string))\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize(['gevent_count', 'subpool_size', 'iterations', 'expected_clients'], [(None, None, 1, 1), (None, None, 2, 1), (None, 10, 1, 10), (None, 10, 2, 10), (None, 200, 1, 100), (None, 200, 2, 100), (4, None, 1, 4), (4, None, 2, 4), (2, 2, 1, 4), (2, 2, 2, 4)])\ndef test_redis_disconnections(gevent_count, subpool_size, iterations, expected_clients, worker):\n    \" mrq.context.connections is not the actual connections pool that the worker uses.\\n        this worker's pool is not accessible from here, since it runs in a different thread.\\n    \"\n    from mrq.context import connections\n    worker.start_deps()\n    gevent_count = (gevent_count if (gevent_count is not None) else 1)\n    get_clients = (lambda : [c for c in connections.redis.client_list() if (c.get('cmd') != 'client')])\n    assert (len(get_clients()) == 0)\n    kwargs = {\n        'flags': '--redis_max_connections 100',\n        'deps': False,\n    }\n    if gevent_count:\n        kwargs['flags'] += (' --gevent %s' % gevent_count)\n    worker.start(**kwargs)\n    for i in range(0, iterations):\n        worker.send_tasks('tests.tasks.redis.Disconnections', ([{\n            'subpool_size': subpool_size,\n        }] * gevent_count))\n    assert (len(get_clients()) == expected_clients)\n    worker.stop(deps=False)\n    assert (len(get_clients()) == 0)\n    worker.stop_deps()\n", "label": 1}
{"function": "\n\ndef merge(a, b):\n    'return merged tuples or lists without duplicates\\n    note: ensure if admin theme is before admin\\n    '\n    if (isinstance(a, CONFIG_VALID) and isinstance(b, CONFIG_VALID)):\n        if (isinstance(a, dict) and isinstance(b, dict)):\n            a.update(b)\n            return a\n        _a = list(a)\n        for x in list(b):\n            if (x not in _a):\n                _a.append(x)\n        return _a\n    if (a and b):\n        raise Exception('Cannot merge')\n    raise NotImplementedError\n", "label": 1}
{"function": "\n\ndef __init__(self, topic, exchange, type_handlers=None, on_wait=None, url=None, transport=None, transport_options=None, retry_options=None):\n    self._topic = topic\n    self._exchange_name = exchange\n    self._on_wait = on_wait\n    self._running = threading.Event()\n    self._dispatcher = dispatcher.TypeDispatcher(requeue_filters=[(lambda data, message: (not self.is_running))], type_handlers=type_handlers)\n    ensure_options = self.DEFAULT_RETRY_OPTIONS.copy()\n    if (retry_options is not None):\n        for k in set(six.iterkeys(ensure_options)):\n            if (k in retry_options):\n                val = retry_options[k]\n                if (k in self._RETRY_INT_OPTS):\n                    tmp_val = int(val)\n                else:\n                    tmp_val = float(val)\n                if (tmp_val < 0):\n                    raise ValueError((\"Expected value greater or equal to zero for 'retry_options' %s; got %s instead\" % (k, val)))\n                ensure_options[k] = tmp_val\n    self._ensure_options = ensure_options\n    self._drain_events_timeout = DRAIN_EVENTS_PERIOD\n    if ((transport == 'memory') and transport_options):\n        polling_interval = transport_options.get('polling_interval')\n        if (polling_interval is not None):\n            self._drain_events_timeout = polling_interval\n    self._conn = kombu.Connection(url, transport=transport, transport_options=transport_options)\n    self._exchange = kombu.Exchange(name=self._exchange_name, durable=False, auto_delete=True)\n", "label": 1}
{"function": "\n\ndef _build_root_message(self, message_cls=None, **kw):\n    msg = (message_cls or SafeMIMEMultipart)(**kw)\n    if self.policy:\n        msg.policy = self.policy\n    msg.preamble = self.ROOT_PREAMBLE\n    self.set_header(msg, 'Date', self.date, encode=False)\n    self.set_header(msg, 'Message-ID', self.message_id, encode=False)\n    if self._headers:\n        for (name, value) in self._headers.items():\n            self.set_header(msg, name, value)\n    subject = self.subject\n    if (subject is not None):\n        self.set_header(msg, 'Subject', subject)\n    self.set_header(msg, 'From', self.encode_address_header(self._mail_from), encode=False)\n    if self._mail_to:\n        self.set_header(msg, 'To', ', '.join([self.encode_address_header(addr) for addr in self._mail_to]), encode=False)\n    if self._cc:\n        self.set_header(msg, 'Cc', ', '.join([self.encode_address_header(addr) for addr in self._cc]), encode=False)\n    return msg\n", "label": 1}
{"function": "\n\ndef __expandparamstodict(self, params, kw):\n    self_parameters = self.parameters\n    parametervalues = dict(zip((p['name'] for p in self_parameters), params))\n    for (kw, kwval) in kw.items():\n        if (kw in parametervalues):\n            raise KeyError(('Multiple definitions of parameter %r' % kw))\n        parametervalues[kw] = kwval\n    for param_to_convert in self_parameters:\n        if (param_to_convert['name'] in parametervalues):\n            val = parametervalues[param_to_convert['name']]\n            if (val is None):\n                parametervalues[param_to_convert['name']] = ''\n            elif (not isinstance(val, param_to_convert['datatype'])):\n                conversion = param_to_convert['datatype'](val)\n                parametervalues[param_to_convert['name']] = getattr(conversion, '_json_struct', conversion)\n        elif (param_to_convert['parameterType'] != 'esriGPParameterTypeDerived'):\n            parametervalues[param_to_convert['name']] = ''\n    return parametervalues\n", "label": 1}
{"function": "\n\ndef attributeAsLDIF(attribute, value):\n    if (value.startswith('\\x00') or value.startswith('\\n') or value.startswith('\\r') or value.startswith(' ') or value.startswith(':') or value.startswith('<') or value.endswith(' ') or containsNonprintable(value)):\n        return attributeAsLDIF_base64(attribute, value)\n    else:\n        return ('%s: %s\\n' % (attribute, value))\n", "label": 1}
{"function": "\n\ndef volume_create_attach(name, call=None, **kwargs):\n    '\\n    Create and attach volumes to created node\\n    '\n    if (call == 'function'):\n        raise SaltCloudSystemExit('The create_attach_volumes action must be called with -a or --action.')\n    if (type(kwargs['volumes']) is str):\n        volumes = yaml.safe_load(kwargs['volumes'])\n    else:\n        volumes = kwargs['volumes']\n    ret = []\n    for volume in volumes:\n        created = False\n        volume_dict = {\n            'name': volume['name'],\n        }\n        if ('volume_id' in volume):\n            volume_dict['volume_id'] = volume['volume_id']\n        elif ('snapshot' in volume):\n            volume_dict['snapshot'] = volume['snapshot']\n        else:\n            volume_dict['size'] = volume['size']\n            if ('type' in volume):\n                volume_dict['type'] = volume['type']\n            if ('iops' in volume):\n                volume_dict['iops'] = volume['iops']\n        if ('id' not in volume_dict):\n            created_volume = create_volume(**volume_dict)\n            created = True\n            volume_dict.update(created_volume)\n        attach = attach_volume(name=volume['name'], server_name=name, device=volume.get('device', None), call='action')\n        if attach:\n            msg = '{0} attached to {1} (aka {2})'.format(volume_dict['id'], name, volume_dict['name'])\n            log.info(msg)\n            ret.append(msg)\n    return ret\n", "label": 1}
{"function": "\n\ndef _check_true_dir(self, text):\n    is_rtl = False\n    is_ltr = False\n    quoted_text = False\n    last_inline_html_char_pos = text.rfind('>')\n    if (last_inline_html_char_pos > (- 1)):\n        it_here = text[(last_inline_html_char_pos + 1):]\n    else:\n        it_here = text\n    for ch in it_here:\n        res = UD.bidirectional(ch)\n        if (ch == '\"'):\n            quoted_text = (not quoted_text)\n        elif ((not quoted_text) and (res in {'R', 'AL'})):\n            is_rtl = True\n        elif ((not quoted_text) and (res == 'L')):\n            is_ltr = True\n    if is_rtl:\n        return 'rtl'\n    elif is_ltr:\n        return 'ltr'\n    else:\n        return 'auto'\n", "label": 1}
{"function": "\n\ndef _get_pages(self, locations, project_name):\n    '\\n        Yields (page, page_url) from the given locations, skipping\\n        locations that have errors, and adding download/homepage links\\n        '\n    all_locations = list(locations)\n    seen = set()\n    normalized = normalize_name(project_name)\n    while all_locations:\n        location = all_locations.pop(0)\n        if (location in seen):\n            continue\n        seen.add(location)\n        page = self._get_page(location)\n        if (page is None):\n            continue\n        (yield page)\n        for link in page.rel_links():\n            if ((normalized not in self.allow_external) and (not self.allow_all_external)):\n                self.need_warn_external = True\n                logger.debug('Not searching %s for files because external urls are disallowed.', link)\n                continue\n            if ((link.trusted is not None) and (not link.trusted) and (normalized not in self.allow_unverified)):\n                logger.debug('Not searching %s for urls, it is an untrusted link and cannot produce safe or verifiable files.', link)\n                self.need_warn_unverified = True\n                continue\n            all_locations.append(link)\n", "label": 1}
{"function": "\n\ndef resolve_rosdep_key(key, os_name, os_version, ros_distro=None, ignored=None, retry=True):\n    ignored = (ignored or [])\n    ctx = create_default_installer_context()\n    try:\n        installer_key = ctx.get_default_os_installer_key(os_name)\n    except KeyError:\n        BloomGenerator.exit(\"Could not determine the installer for '{0}'\".format(os_name))\n    installer = ctx.get_installer(installer_key)\n    ros_distro = (ros_distro or DEFAULT_ROS_DISTRO)\n    view = get_view(os_name, os_version, ros_distro)\n    try:\n        return resolve_more_for_os(key, view, installer, os_name, os_version)\n    except (KeyError, ResolutionError) as exc:\n        debug(traceback.format_exc())\n        if (key in ignored):\n            return (None, None, None)\n        if isinstance(exc, KeyError):\n            error(\"Could not resolve rosdep key '{0}'\".format(key))\n            returncode = code.GENERATOR_NO_SUCH_ROSDEP_KEY\n        else:\n            error(\"Could not resolve rosdep key '{0}' for distro '{1}':\".format(key, os_version))\n            info(str(exc), use_prefix=False)\n            returncode = code.GENERATOR_NO_ROSDEP_KEY_FOR_DISTRO\n        if retry:\n            error('Try to resolve the problem with rosdep and then continue.')\n            if maybe_continue():\n                update_rosdep()\n                invalidate_view_cache()\n                return resolve_rosdep_key(key, os_name, os_version, ros_distro, ignored, retry=True)\n        BloomGenerator.exit(\"Failed to resolve rosdep key '{0}', aborting.\".format(key), returncode=returncode)\n", "label": 1}
{"function": "\n\n@login_required\ndef rule_new(request, testplan_id):\n    try:\n        testplan = TestPlan(auth_token=request.user.password).get(testplan_id)\n    except UnauthorizedException:\n        logger.warning('User unauthorized. Signing out...')\n        return signout(request)\n    except NotFoundException:\n        return render(request, '404.html')\n    except Exception as inst:\n        logger.error('Unexpected exception', exc_info=True)\n        messages.error(request, (inst.message if inst.message else 'Unexpected error'))\n        return HttpResponseRedirect(reverse('testplan_list'))\n    form = RuleForm((request.POST or None))\n    if form.is_valid():\n        try:\n            rule_id = Rule(testplan_id, auth_token=request.user.password).create(form.cleaned_data)\n            return HttpResponseRedirect(reverse('rule_details', args=(str(testplan_id), str(rule_id))))\n        except UnauthorizedException:\n            logger.warning('User unauthorized. Signing out...')\n            return signout(request)\n        except Exception as inst:\n            messages.error(request, (inst.message if inst.message else 'Unexpected error'))\n            return HttpResponseRedirect(reverse('rule_new', args=(str(testplan_id),)))\n    return render(request, 'rules/rule_new.html', {\n        'form': form,\n        'testplan': testplan,\n    })\n", "label": 1}
{"function": "\n\ndef test_update(self):\n    kc = KeyBundle([{\n        'kty': 'oct',\n        'key': 'supersecret',\n        'use': 'sig',\n    }])\n    assert (len(kc.get('oct')) == 1)\n    assert (len(kc.get('rsa')) == 0)\n    assert (kc.remote is False)\n    assert (kc.source is None)\n    kc.update()\n    assert (len(kc.get('oct')) == 1)\n    assert (len(kc.get('rsa')) == 0)\n    assert (kc.remote is False)\n    assert (kc.source is None)\n", "label": 1}
{"function": "\n\n@login_required\ndef vote(request):\n    env = {\n        \n    }\n    try:\n        online = VoteStatus.objects.all()[0].online\n    except:\n        online = False\n    if (not online):\n        return render(request, 'voting/vote_offline.html')\n    try:\n        cart = VoteCart.objects.get(user=request.user.id)\n    except:\n        cart = VoteCart(user=request.user, status=STATUS.ACTIVE)\n        cart.save()\n    if (cart.status == STATUS.COMPLETED):\n        return render(request, 'voting/vote_complete.html')\n    if (request.method == 'POST'):\n        form = VoteForm(request.POST)\n        if form.is_valid():\n            for (key, team) in form.cleaned_data.items():\n                category_id = key.replace('cat_', '')\n                category = Category.objects.get(id=category_id)\n                _insert_or_update_vote(cart, category, team)\n            return render(request, 'voting/vote_complete.html')\n    else:\n        votes = Vote.objects.filter(cart=cart.id)\n        form_init = {\n            \n        }\n        for vote in votes:\n            form_init[('cat_%s' % vote.category.id)] = vote.team.id\n        form = VoteForm(form_init)\n    env['form'] = form\n    return render(request, 'voting/vote.html', env)\n", "label": 1}
{"function": "\n\ndef _get_empty_context(self, context, instance, edit_fields, language, view_url, view_method, editmode=True):\n    '\\n        Inject in a copy of the context the data requested to trigger the edit.\\n\\n        `content` and `rendered_content` is emptied.\\n        '\n    if (not language):\n        language = get_language_from_request(context['request'])\n    if ((not instance) and editmode):\n        return context\n    extra_context = copy(context)\n    if (instance and isinstance(instance, Page)):\n        if (edit_fields == 'titles'):\n            edit_fields = 'title,page_title,menu_title'\n        view_url = 'admin:cms_page_edit_title_fields'\n    if (edit_fields == 'changelist'):\n        view_url = ('admin:%s_%s_changelist' % (instance._meta.app_label, instance._meta.model_name))\n    querystring = OrderedDict((('language', language),))\n    if edit_fields:\n        extra_context['edit_fields'] = edit_fields.strip().split(',')\n    if self._is_editable(context.get('request', None)):\n        extra_context.update(self._get_editable_context(extra_context, instance, language, edit_fields, view_method, view_url, querystring, editmode))\n    extra_context['content'] = ''\n    extra_context['rendered_content'] = ''\n    return extra_context\n", "label": 1}
{"function": "\n\ndef render(self):\n    met = (' metadata,' if (_flask_prepend == '') else '')\n    text = 't_{0} = {1}Table(\\n    {0!r},{2}\\n'.format(self.table.name, _flask_prepend, met)\n    for column in self.table.columns:\n        text += '    {0},\\n'.format(_render_column(column, True))\n    for constraint in sorted(self.table.constraints, key=_get_constraint_sort_key):\n        if isinstance(constraint, PrimaryKeyConstraint):\n            continue\n        if (isinstance(constraint, (ForeignKeyConstraint, UniqueConstraint)) and (len(constraint.columns) == 1)):\n            continue\n        text += '    {0},\\n'.format(_render_constraint(constraint))\n    for index in self.table.indexes:\n        if (len(index.columns) > 1):\n            text += '    {0},\\n'.format(_render_index(index))\n    if self.schema:\n        text += \"    schema='{0}',\\n\".format(self.schema)\n    return (text.rstrip('\\n,') + '\\n)')\n", "label": 1}
{"function": "\n\ndef test_cardinality_qk():\n    rules = {\n        'max_cardinality': 2,\n        'timeframe': datetime.timedelta(minutes=10),\n        'cardinality_field': 'foo',\n        'timestamp_field': '@timestamp',\n        'query_key': 'user',\n    }\n    rule = CardinalityRule(rules)\n    users = ['foo', 'bar', 'baz']\n    for user in users:\n        event = {\n            '@timestamp': datetime.datetime.now(),\n            'user': user,\n            'foo': ('foo' + user),\n        }\n        rule.add_data([event])\n        assert (len(rule.matches) == 0)\n    rule.garbage_collect(datetime.datetime.now())\n    values = ['faz', 'fuz', 'fiz']\n    for value in values:\n        event = {\n            '@timestamp': (datetime.datetime.now() + datetime.timedelta(minutes=5)),\n            'user': 'baz',\n            'foo': value,\n        }\n        rule.add_data([event])\n    rule.garbage_collect((datetime.datetime.now() + datetime.timedelta(minutes=5)))\n    assert (len(rule.matches) == 2)\n    assert (rule.matches[0]['user'] == 'baz')\n    assert (rule.matches[1]['user'] == 'baz')\n    assert (rule.matches[0]['foo'] == 'fuz')\n    assert (rule.matches[1]['foo'] == 'fiz')\n", "label": 1}
{"function": "\n\ndef get_methods(self, include_default=False):\n    for method in self.handler.allowed_methods:\n        met = getattr(self.handler, CsrfExemptResource.callmap.get(method))\n        stale = (inspect.getmodule(met) is handler)\n        if (not self.handler.is_anonymous):\n            if (met and ((not stale) or include_default)):\n                (yield HandlerMethod(met, stale))\n        elif ((not stale) or ((met.__name__ == 'read') and ('GET' in self.allowed_methods))):\n            (yield HandlerMethod(met, stale))\n", "label": 1}
{"function": "\n\ndef pre_process_json(obj):\n    '\\n    Preprocess items in a dictionary or list and prepare them to be json serialized.\\n    '\n    if (type(obj) is dict):\n        new_dict = {\n            \n        }\n        for (key, value) in obj.items():\n            new_dict[key] = pre_process_json(value)\n        return new_dict\n    elif (type(obj) is list):\n        new_list = []\n        for item in obj:\n            new_list.append(pre_process_json(item))\n        return new_list\n    elif hasattr(obj, 'todict'):\n        return dict(obj.todict())\n    else:\n        try:\n            json.dumps(obj)\n        except TypeError:\n            try:\n                json.dumps(obj.__dict__)\n            except TypeError:\n                return str(obj)\n            else:\n                return obj.__dict__\n        else:\n            return obj\n", "label": 1}
{"function": "\n\ndef handle_save_embeds(sender, instance, **kwargs):\n    embedded_media_fields = FieldRegistry.get_fields(sender)\n    if (not embedded_media_fields):\n        return\n    urls = []\n    for field in instance._meta.fields:\n        if isinstance(field, TextField):\n            urls.extend(re.findall(URL_RE, getattr(instance, field.name)))\n    urls = set(urls)\n    for embedded_field in embedded_media_fields:\n        m2m = getattr(instance, embedded_field.name)\n        m2m.clear()\n        for url in urls:\n            try:\n                provider = oembed.site.provider_for_url(url)\n            except OEmbedMissingEndpoint:\n                pass\n            else:\n                if ((not embedded_field.media_type) or (provider.resource_type in embedded_field.media_type)):\n                    (media_obj, created) = AggregateMedia.objects.get_or_create(url=url)\n                    m2m.add(media_obj)\n", "label": 1}
{"function": "\n\ndef get_ordering(self, request, queryset, view):\n    '\\n        Return a tuple of strings, that may be used in an `order_by` method.\\n        '\n    ordering_filters = [filter_cls for filter_cls in getattr(view, 'filter_backends', []) if hasattr(filter_cls, 'get_ordering')]\n    if ordering_filters:\n        filter_cls = ordering_filters[0]\n        filter_instance = filter_cls()\n        ordering = filter_instance.get_ordering(request, queryset, view)\n        assert (ordering is not None), 'Using cursor pagination, but filter class {filter_cls} returned a `None` ordering.'.format(filter_cls=filter_cls.__name__)\n    else:\n        ordering = self.ordering\n        assert (ordering is not None), 'Using cursor pagination, but no ordering attribute was declared on the pagination class.'\n        assert ('__' not in ordering), 'Cursor pagination does not support double underscore lookups for orderings. Orderings should be an unchanging, unique or nearly-unique field on the model, such as \"-created\" or \"pk\".'\n    assert isinstance(ordering, (six.string_types, list, tuple)), 'Invalid ordering. Expected string or tuple, but got {type}'.format(type=type(ordering).__name__)\n    if isinstance(ordering, six.string_types):\n        return (ordering,)\n    return tuple(ordering)\n", "label": 1}
{"function": "\n\ndef _get_non_unique_error(self, queryset):\n    '\\n        Generate error message when some of the field has more than one value.\\n        '\n    non_unique = {\n        \n    }\n    for field in self._invoice_report_common_fields:\n        items = queryset.values(field).distinct()\n        if (items.count() != 1):\n            if (field == 'invoice_date'):\n                data = ', '.join((item[field].strftime('%Y-%m-%d') for item in items if item[field]))\n            else:\n                data = ', '.join((item[field] for item in items if item[field]))\n            non_unique[field] = data\n    non_unique_items = ' '.join(['{}: {}'.format(key, value) for (key, value) in non_unique.items() if value])\n    return '{}: {}'.format(_('Selected items have different'), non_unique_items)\n", "label": 1}
{"function": "\n\ndef list_tasks(self, app_id=None, **kwargs):\n    'List running tasks, optionally filtered by app_id.\\n\\n        :param str app_id: if passed, only show tasks for this application\\n        :param kwargs: arbitrary search filters\\n\\n        :returns: list of tasks\\n        :rtype: list[:class:`marathon.models.task.MarathonTask`]\\n        '\n    response = self._do_request('GET', '/v2/tasks')\n    tasks = self._parse_response(response, MarathonTask, is_list=True, resource_name='tasks')\n    if app_id:\n        tasks = [task for task in tasks if (task.app_id.lstrip('/') == app_id.lstrip('/'))]\n    [setattr(t, 'app_id', app_id) for t in tasks if (app_id and (t.app_id is None))]\n    for (k, v) in kwargs.items():\n        tasks = [o for o in tasks if (getattr(o, k) == v)]\n    return tasks\n", "label": 1}
{"function": "\n\n@classmethod\ndef decode_obj(cls, obj):\n    if isinstance(obj, dict):\n        obj = dict(((key, cls.decode_obj(val)) for (key, val) in obj.items()))\n    elif isinstance(obj, list):\n        obj = list((cls.decode_obj(val) for val in obj))\n    if ((not isinstance(obj, dict)) or (len(obj) != 1)):\n        return obj\n    obj_tag = next(iter(obj.keys()))\n    if (not obj_tag.startswith('!')):\n        return obj\n    if (obj_tag not in json_tags):\n        raise ValueError('Unknown tag', obj_tag)\n    obj_cls = json_tags[obj_tag]\n    return obj_cls.decode_json_obj(obj[obj_tag])\n", "label": 1}
{"function": "\n\ndef enableFutureFeature(object_name, future_spec, source_ref):\n    if (object_name == 'unicode_literals'):\n        future_spec.enableUnicodeLiterals()\n    elif (object_name == 'absolute_import'):\n        future_spec.enableAbsoluteImport()\n    elif (object_name == 'division'):\n        future_spec.enableFutureDivision()\n    elif (object_name == 'print_function'):\n        future_spec.enableFuturePrint()\n    elif ((object_name == 'barry_as_FLUFL') and (python_version >= 300)):\n        future_spec.enableBarry()\n    elif (object_name == 'generator_stop'):\n        future_spec.enableGeneratorStop()\n    elif (object_name == 'braces'):\n        SyntaxErrors.raiseSyntaxError('not a chance', source_ref)\n    elif (object_name in ('nested_scopes', 'generators', 'with_statement')):\n        pass\n    else:\n        SyntaxErrors.raiseSyntaxError(('future feature %s is not defined' % object_name), source_ref)\n", "label": 1}
{"function": "\n\ndef pretty(self, indent=0):\n    lines = list()\n    lines.append(('resource', self.uri))\n    if self.name:\n        lines.append(('identifier', repr(self.name)))\n    if self.resource_type:\n        lines.append(('resource type', self.resource_type))\n    if self.resource_size:\n        lines.append(('resource size', str(self.resource_size)))\n    for lang in sorted(self.title):\n        lines.append((('title[%s]' % lang), self.title[lang]))\n    for icon in sorted(self.icons):\n        info = '{0} ... ({1} bytes)'.format(repr(self.icons[icon][:10]).strip(\"'\"), len(self.icons[icon]))\n        lines.append((('icon[%s]' % icon), info))\n    lines.append(('action', self.action))\n    indent = (indent * ' ')\n    lwidth = max([len(line[0]) for line in lines])\n    lines = [((line[0].ljust(lwidth) + ' = ') + line[1]) for line in lines]\n    return '\\n'.join([(indent + line) for line in lines])\n", "label": 1}
{"function": "\n\ndef _restore_state(self, obj, instance):\n    state = self._restore(obj[tags.STATE])\n    has_slots = (isinstance(state, tuple) and (len(state) == 2) and isinstance(state[1], dict))\n    has_slots_and_dict = (has_slots and isinstance(state[0], dict))\n    if hasattr(instance, '__setstate__'):\n        instance.__setstate__(state)\n    elif isinstance(state, dict):\n        self._restore_from_dict(state, instance, ignorereserved=False)\n    elif has_slots:\n        self._restore_from_dict(state[1], instance, ignorereserved=False)\n        if has_slots_and_dict:\n            self._restore_from_dict(state[0], instance, ignorereserved=False)\n    elif ((not hasattr(instance, '__getnewargs__')) and (not hasattr(instance, '__getnewargs_ex__'))):\n        instance = state\n    return instance\n", "label": 1}
{"function": "\n\ndef generate_global_orchestrate_file(self):\n    accounts = set([host.cloud_image.account for host in self.hosts.all()])\n    orchestrate = {\n        \n    }\n    for account in accounts:\n        target = 'G@stack_id:{0} and G@cloud_account:{1}'.format(self.id, account.slug)\n        groups = {\n            \n        }\n        for component in account.formula_components.all():\n            groups.setdefault(component.order, set()).add(component.sls_path)\n        for order in sorted(groups.keys()):\n            for role in groups[order]:\n                state_title = '{0}_{1}'.format(account.slug, role)\n                orchestrate[state_title] = {\n                    'salt.state': [{\n                        'tgt': target,\n                    }, {\n                        'tgt_type': 'compound',\n                    }, {\n                        'sls': role,\n                    }],\n                }\n                depend = (order - 1)\n                while (depend >= 0):\n                    if (depend in groups.keys()):\n                        orchestrate[role]['salt.state'].append({\n                            'require': [{\n                                'salt': req,\n                            } for req in groups[depend]],\n                        })\n                        break\n                    depend -= 1\n    yaml_data = yaml.safe_dump(orchestrate, default_flow_style=False)\n    if (not self.global_orchestrate_file):\n        self.global_orchestrate_file.save('global_orchestrate.sls', ContentFile(yaml_data))\n    else:\n        with open(self.global_orchestrate_file.path, 'w') as f:\n            f.write(yaml_data)\n", "label": 1}
{"function": "\n\ndef assert_attr_equal(attr, left, right, obj='Attributes'):\n    \"checks attributes are equal. Both objects must have attribute.\\n\\n    Parameters\\n    ----------\\n    attr : str\\n        Attribute name being compared.\\n    left : object\\n    right : object\\n    obj : str, default 'Attributes'\\n        Specify object name being compared, internally used to show appropriate\\n        assertion message\\n    \"\n    left_attr = getattr(left, attr)\n    right_attr = getattr(right, attr)\n    if (left_attr is right_attr):\n        return True\n    elif (is_number(left_attr) and np.isnan(left_attr) and is_number(right_attr) and np.isnan(right_attr)):\n        return True\n    try:\n        result = (left_attr == right_attr)\n    except TypeError:\n        result = False\n    if (not isinstance(result, bool)):\n        result = result.all()\n    if result:\n        return True\n    else:\n        raise_assert_detail(obj, 'Attribute \"{0}\" are different'.format(attr), left_attr, right_attr)\n", "label": 1}
{"function": "\n\ndef __lt__(self, other):\n    \" Used for custom sort on note's timestamp\\n        :param other: NoteListWidgetItem to compare against\\n        :return: boolean True if should be sorted below\\n        \"\n    if (other.notemodel.pinned and self.notemodel.pinned):\n        return (self.notemodel.timestamp < other.notemodel.timestamp)\n    elif ((not other.notemodel.pinned) and (not self.notemodel.pinned)):\n        return (self.notemodel.timestamp < other.notemodel.timestamp)\n    elif (other.notemodel.pinned and (not self.notemodel.pinned)):\n        return True\n    elif ((not other.notemodel.pinned) and self.notemodel.pinned):\n        return False\n", "label": 1}
{"function": "\n\ndef run(options):\n    options = opt_validate(options)\n    info('Read and build bedGraph...')\n    bio = BedGraphIO.bedGraphIO(options.ifile)\n    btrack = bio.build_bdgtrack(baseline_value=0)\n    info('Modify bedGraph...')\n    if (options.method.lower() == 'p2q'):\n        btrack.p2q()\n    elif (options.method.lower() == 'analen'):\n        btrack.analen()\n    else:\n        extraparam = float(options.extraparam[0])\n        if (options.method.lower() == 'multiply'):\n            btrack.apply_func((lambda x: (x * extraparam)))\n        elif (options.method.lower() == 'add'):\n            btrack.apply_func((lambda x: (x + extraparam)))\n        elif (options.method.lower() == 'max'):\n            btrack.apply_func((lambda x: (x if (x > extraparam) else extraparam)))\n        elif (options.method.lower() == 'min'):\n            btrack.apply_func((lambda x: (x if (x < extraparam) else extraparam)))\n    ofile = os.path.join(options.outdir, options.ofile)\n    info('Write bedGraph of modified scores...')\n    ofhd = open(ofile, 'wb')\n    btrack.write_bedGraph(ofhd, name=('%s_modified_scores' % options.method.upper()), description=('Scores calculated by %s' % options.method.upper()))\n    info((\"Finished '%s'! Please check '%s'!\" % (options.method, ofile)))\n", "label": 1}
{"function": "\n\ndef config_file_opt(self, path2json):\n    if os.path.exists(path2json):\n        with codecs.open(opt.config, 'r', encoding='ascii') as f:\n            json_con = json_loads(f.read())\n        self.server = (self.server or json_con.get('server', None))\n        self.password = (self.password or json_con.get('password', None))\n        self.server_port = (self.server_port or json_con.get('server_port', None))\n        self.local_port = (self.local_port or json_con.get('local_port', None))\n        self.method = (self.method or json_con.get('method', 'table'))\n        self.timeout = (self.timeout or json_con.get('timeout', None))\n        self.debug = (self.debug or json_con.get('debug', False))\n    else:\n        logger.warning(('the json file path `%s` is not exists' % path2json))\n", "label": 1}
{"function": "\n\ndef collect(self, objs, source=None, nullable=False, collect_related=True, source_attr=None, reverse_dependency=False):\n    if self.can_fast_delete(objs):\n        self.fast_deletes.append(objs)\n        return\n    new_objs = self.add(objs, source, nullable, reverse_dependency=reverse_dependency)\n    if (not new_objs):\n        return\n    model = new_objs[0].__class__\n    if collect_related:\n        for related in model._meta.get_all_related_objects(include_hidden=True, include_proxy_eq=True, local_only=True):\n            field = related.field\n            if (field.rel.on_delete == DO_NOTHING):\n                continue\n            sub_objs = self.related_objects(related, new_objs)\n            if self.can_fast_delete(sub_objs, from_field=field):\n                self.fast_deletes.append(sub_objs)\n            elif sub_objs:\n                field.rel.on_delete(self, field, sub_objs, self.using)\n        for field in model._meta.virtual_fields:\n            if hasattr(field, 'bulk_related_objects'):\n                sub_objs = field.bulk_related_objects(new_objs, self.using)\n                self.collect(sub_objs, source=model, source_attr=field.rel.related_name, nullable=True)\n", "label": 1}
{"function": "\n\ndef _update_dbo_fields(cls, new_attrs):\n    for (name, attr) in new_attrs.items():\n        if hasattr(attr, 'hydrate'):\n            old_attr = cls.dbo_fields.get(name)\n            if (old_attr != attr):\n                if (old_attr and (old_attr.default == attr.default)):\n                    log.warn('Unnecessary override of attr {} in class {}', name, cls.__name__)\n                elif (old_attr and old_attr.default):\n                    log.info('Overriding default value of attr{} in class {}', name, cls.__name__)\n                cls.dbo_fields[name] = attr\n        elif isinstance(attr, DBOTField):\n            cls.dbot_fields[name] = attr\n    load_func = new_attrs.get('on_loaded')\n    if load_func:\n        cls.load_funcs.append(load_func)\n", "label": 1}
{"function": "\n\ndef _read_object_from_repo(self, rev=None, relpath=None, sha=None):\n    'Read an object from the git repo.\\n    This is implemented via a pipe to git cat-file --batch\\n    '\n    if sha:\n        spec = (sha + '\\n')\n    else:\n        assert (rev is not None)\n        assert (relpath is not None)\n        relpath = self._fixup_dot_relative(relpath)\n        spec = '{}:{}\\n'.format(rev, relpath)\n    self._maybe_start_cat_file_process()\n    self._cat_file_process.stdin.write(spec)\n    self._cat_file_process.stdin.flush()\n    header = None\n    while (not header):\n        header = self._cat_file_process.stdout.readline()\n        if (self._cat_file_process.poll() is not None):\n            raise self.GitDiedException(\"Git cat-file died while trying to read '{}'.\".format(spec))\n    header = header.rstrip()\n    parts = header.rsplit(SPACE, 2)\n    if (len(parts) == 2):\n        assert (parts[1] == 'missing')\n        raise self.MissingFileException(rev, relpath)\n    (_, object_type, object_len) = parts\n    blob = self._cat_file_process.stdout.read(int(object_len))\n    assert (self._cat_file_process.stdout.read(1) == '\\n')\n    assert (len(blob) == int(object_len))\n    return (object_type, blob)\n", "label": 1}
{"function": "\n\ndef _port_action_vxlan(self, port, segment, func):\n    'Verify configuration and then process event.'\n    if (segment is None):\n        self._log_missing_segment()\n        return\n    device_id = port.get('device_id')\n    mcast_group = segment.get(api.PHYSICAL_NETWORK)\n    host_id = port.get(portbindings.HOST_ID)\n    vni = segment.get(api.SEGMENTATION_ID)\n    if (vni and device_id and mcast_group and host_id):\n        func(vni, device_id, mcast_group, host_id)\n        return vni\n    else:\n        fields = ('vni ' if (not vni) else '')\n        fields += ('device_id ' if (not device_id) else '')\n        fields += ('mcast_group ' if (not mcast_group) else '')\n        fields += ('host_id' if (not host_id) else '')\n        raise excep.NexusMissingRequiredFields(fields=fields)\n", "label": 1}
{"function": "\n\ndef read_results(results_fpath, lamb):\n    last = {\n        \n    }\n    classes = {\n        \n    }\n    objs = set()\n    users = set()\n    joint = defaultdict((lambda : defaultdict(int)))\n    glob = defaultdict((lambda : defaultdict(int)))\n    with open(results_fpath) as results_file:\n        results_file.readline()\n        for line in results_file:\n            spl = line.strip().split()\n            user = spl[0]\n            class_ = spl[1]\n            last_stage = spl[(- 1)].split('-')[(- 1)]\n            last[user] = last_stage\n            classes[user] = class_\n            for item in spl[2:]:\n                (obj, stage) = item.split('-')\n                joint[class_][(stage, obj)] += 1\n                glob[class_][stage] += 1\n                objs.add(obj)\n            users.add(user)\n    probs = {\n        \n    }\n    for u in last:\n        probs[u] = {\n            \n        }\n        class_ = classes[u]\n        for o in objs:\n            probs[u][o] = ((lamb + joint[class_][(last[u], o)]) / ((len(objs) * lamb) + glob[class_][last[u]]))\n    for u in probs:\n        sum_ = sum(probs[u].values())\n        for o in objs:\n            probs[u][o] = ((probs[u][o] / sum_) if (sum_ > 0) else 0)\n    sorted_probs = {\n        \n    }\n    for u in probs:\n        sorted_probs[u] = sorted([(v, o) for (o, v) in probs[u].items()], reverse=True)\n    return (sorted_probs, users, objs)\n", "label": 1}
{"function": "\n\ndef _create_control(self, parent):\n    if ((len(self.default_path) != 0) and (len(self.default_directory) == 0) and (len(self.default_filename) == 0)):\n        (default_directory, default_filename) = os.path.split(self.default_path)\n    else:\n        default_directory = self.default_directory\n        default_filename = self.default_filename\n    filters = []\n    for filter_list in self.wildcard.split('|')[::2]:\n        filter_list = filter_list.replace(';', ' ')\n        filters.append(filter_list)\n    if (not default_directory):\n        default_directory = QtCore.QDir.currentPath()\n    dlg = QtGui.QFileDialog(parent, self.title, default_directory)\n    dlg.setViewMode(QtGui.QFileDialog.Detail)\n    dlg.selectFile(default_filename)\n    dlg.setNameFilters(filters)\n    if (self.wildcard_index < len(filters)):\n        dlg.selectNameFilter(filters[self.wildcard_index])\n    if (self.action == 'open'):\n        dlg.setAcceptMode(QtGui.QFileDialog.AcceptOpen)\n        dlg.setFileMode(QtGui.QFileDialog.ExistingFile)\n    elif (self.action == 'open files'):\n        dlg.setAcceptMode(QtGui.QFileDialog.AcceptOpen)\n        dlg.setFileMode(QtGui.QFileDialog.ExistingFiles)\n    else:\n        dlg.setAcceptMode(QtGui.QFileDialog.AcceptSave)\n        dlg.setFileMode(QtGui.QFileDialog.AnyFile)\n    return dlg\n", "label": 1}
{"function": "\n\ndef CalculateSlices(self, maxWidth, columnWidths):\n    '\\n        Return a list of integer pairs, where each pair represents\\n        the left and right columns that can fit into the width of one page\\n        '\n    firstColumn = 0\n    if (hasattr(self.lv, 'useExpansionColumn') and self.lv.useExpansionColumn):\n        firstColumn = 1\n    if (self.IsShrinkToFit() or (sum(columnWidths) <= maxWidth)):\n        return [[firstColumn, (len(columnWidths) - 1)]]\n    pairs = list()\n    left = firstColumn\n    right = firstColumn\n    while (right < len(columnWidths)):\n        if (sum(columnWidths[left:(right + 1)]) > maxWidth):\n            if (left == right):\n                pairs.append([left, right])\n                left += 1\n                right += 1\n            else:\n                pairs.append([left, (right - 1)])\n                left = right\n        else:\n            right += 1\n    if (left < len(columnWidths)):\n        pairs.append([left, (right - 1)])\n    return pairs\n", "label": 1}
{"function": "\n\ndef inputhook_wx3():\n    'Run the wx event loop by processing pending events only.\\n    \\n    This is like inputhook_wx1, but it keeps processing pending events\\n    until stdin is ready.  After processing all pending events, a call to \\n    time.sleep is inserted.  This is needed, otherwise, CPU usage is at 100%.\\n    This sleep time should be tuned though for best performance.\\n    '\n    try:\n        app = wx.GetApp()\n        if (app is not None):\n            assert wx.Thread_IsMain()\n            if (not isinstance(signal.getsignal(signal.SIGINT), collections.Callable)):\n                signal.signal(signal.SIGINT, signal.default_int_handler)\n            evtloop = wx.EventLoop()\n            ea = wx.EventLoopActivator(evtloop)\n            t = clock()\n            while (not stdin_ready()):\n                while evtloop.Pending():\n                    t = clock()\n                    evtloop.Dispatch()\n                app.ProcessIdle()\n                used_time = (clock() - t)\n                if (used_time > (5 * 60.0)):\n                    time.sleep(5.0)\n                elif (used_time > 10.0):\n                    time.sleep(1.0)\n                elif (used_time > 0.1):\n                    time.sleep(0.05)\n                else:\n                    time.sleep(0.001)\n            del ea\n    except KeyboardInterrupt:\n        pass\n    return 0\n", "label": 1}
{"function": "\n\ndef get_file_content(url, comes_from=None):\n    'Gets the content of a file; it may be a filename, file: URL, or\\n    http: URL.  Returns (location, content)'\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if ((scheme == 'file') and comes_from and comes_from.startswith('http')):\n            raise InstallationError(('Requirements file %s references URL %s, which is local' % (comes_from, url)))\n        if (scheme == 'file'):\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = ((match.group(1) + ':') + path.split('|', 1)[1])\n            path = urllib.unquote(path)\n            if path.startswith('/'):\n                path = ('/' + path.lstrip('/'))\n            url = path\n        else:\n            resp = urlopen(url)\n            return (geturl(resp), resp.read())\n    try:\n        f = open(url)\n        content = f.read()\n    except IOError:\n        e = sys.exc_info()[1]\n        raise InstallationError(('Could not open requirements file: %s' % str(e)))\n    else:\n        f.close()\n    return (url, content)\n", "label": 1}
{"function": "\n\ndef expand_variant(self, variant_call):\n    if (not self.is_snp(variant_call)):\n        return variant_call\n    expansion_calls = []\n    for sample_id in self.sample_refs.keys():\n        ref_call = self.sample_refs[sample_id]\n        if ((self.get_start(ref_call) <= self.get_start(variant_call)) and (self.get_end(ref_call) >= (self.get_start(variant_call) + 1))):\n            expansion_calls.extend(ref_call['call'])\n        else:\n            del self.sample_refs[sample_id]\n    if self.filter_ref_matches:\n        variant_sample_names = [call['call_set_name'] for call in variant_call['call']]\n        variant_call['call'].extend([call for call in expansion_calls if (call['call_set_name'] not in variant_sample_names)])\n    else:\n        variant_call['call'].extend(expansion_calls)\n    return variant_call\n", "label": 1}
{"function": "\n\ndef db_children(self, parent=(None, None), orphan=False):\n    children = []\n    if (self._db_location is not None):\n        children.extend(self._db_location.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_functions:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_annotations:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    for child in self._db_portSpecs:\n        children.extend(child.db_children((self.vtType, self._db_id), orphan))\n    if orphan:\n        for child in self._db_functions[:]:\n            self.db_delete_function(child)\n        for child in self._db_annotations[:]:\n            self.db_delete_annotation(child)\n        for child in self._db_portSpecs[:]:\n            self.db_delete_portSpec(child)\n        self._db_location = None\n    children.append((self, parent[0], parent[1]))\n    return children\n", "label": 1}
{"function": "\n\ndef match_rating_codex(s):\n    if isinstance(s, bytes):\n        raise TypeError(_no_bytes_err)\n    s = s.upper()\n    codex = []\n    prev = None\n    for (i, c) in enumerate(s):\n        if (((c != ' ') and ((i == 0) and (c in 'AEIOU'))) or ((c not in 'AEIOU') and (c != prev))):\n            codex.append(c)\n        prev = c\n    if (len(codex) > 6):\n        return ''.join((codex[:3] + codex[(- 3):]))\n    else:\n        return ''.join(codex)\n", "label": 1}
{"function": "\n\ndef exchange_declare(self, exchange=None, type='direct', durable=False, auto_delete=False, arguments=None, nowait=False, passive=False):\n    'Declare exchange.'\n    type = (type or 'direct')\n    exchange = (exchange or ('amq.%s' % type))\n    if passive:\n        if (exchange not in self.state.exchanges):\n            raise ChannelError('NOT_FOUND - no exchange {0!r} in vhost {1!r}'.format(exchange, (self.connection.client.virtual_host or '/')), (50, 10), 'Channel.exchange_declare', '404')\n        return\n    try:\n        prev = self.state.exchanges[exchange]\n        if (not self.typeof(exchange).equivalent(prev, exchange, type, durable, auto_delete, arguments)):\n            raise NotEquivalentError(NOT_EQUIVALENT_FMT.format(exchange, (self.connection.client.virtual_host or '/')))\n    except KeyError:\n        self.state.exchanges[exchange] = {\n            'type': type,\n            'durable': durable,\n            'auto_delete': auto_delete,\n            'arguments': (arguments or {\n                \n            }),\n            'table': [],\n        }\n", "label": 1}
{"function": "\n\ndef extractParagraphProperties(self, style, parent=None):\n    ' Extracts paragraph properties from a style element. '\n    paraProps = ParagraphProps()\n    name = style.getAttribute('style:name')\n    if name.startswith('Heading_20_'):\n        level = name[11:]\n        try:\n            level = int(level)\n            paraProps.setHeading(level)\n        except:\n            level = 0\n    if (name == 'Title'):\n        paraProps.setTitle(True)\n    paraPropEl = style.getElementsByTagName('style:paragraph-properties')\n    if paraPropEl:\n        paraPropEl = paraPropEl[0]\n        leftMargin = paraPropEl.getAttribute('fo:margin-left')\n        if leftMargin:\n            try:\n                leftMargin = float(leftMargin[:(- 2)])\n                if (leftMargin > 0.01):\n                    paraProps.setIndented(True)\n            except:\n                pass\n    textProps = self.extractTextProperties(style)\n    if textProps.fixed:\n        paraProps.setCode(True)\n    return paraProps\n", "label": 1}
{"function": "\n\ndef add_palette(self, color, palette_type, palette_name):\n    'Add pallete.'\n    if (palette_type == '__special__'):\n        if (palette_name == 'Favorites'):\n            favs = util.get_favs()['colors']\n            if (color not in favs):\n                favs.append(color)\n            util.save_palettes(favs, favs=True)\n            self.show_color_info(update=True)\n    elif (palette_type in ('__global__', '__project__')):\n        if (palette_type == '__global__'):\n            color_palettes = util.get_palettes()\n        else:\n            color_palettes = util.get_project_palettes(self.view.window())\n        for palette in color_palettes:\n            if (palette_name == palette['name']):\n                if (color not in palette['colors']):\n                    palette['colors'].append(color)\n                    if (palette_type == '__global__'):\n                        util.save_palettes(color_palettes)\n                    else:\n                        util.save_project_palettes(self.view.window(), color_palettes)\n                    self.show_color_info(update=True)\n                    break\n", "label": 1}
{"function": "\n\ndef test_ed25519py():\n    kp0 = ed25519py.crypto_sign_keypair(binary((' ' * 32)))\n    kp = ed25519py.crypto_sign_keypair()\n    signed = ed25519py.crypto_sign(binary('test'), kp.sk)\n    ed25519py.crypto_sign_open(signed, kp.vk)\n    try:\n        ed25519py.crypto_sign_open(signed, kp0.vk)\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign_keypair(binary((' ' * 33)))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign(binary(''), (binary(' ') * 31))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n    try:\n        ed25519py.crypto_sign_open(binary(''), (binary(' ') * 31))\n    except ValueError:\n        pass\n    else:\n        raise Exception('Expected ValueError')\n", "label": 1}
{"function": "\n\ndef whataremyips(bind_ip=None):\n    '\\n    Get \"our\" IP addresses (\"us\" being the set of services configured by\\n    one `*.conf` file). If our REST listens on a specific address, return it.\\n    Otherwise, if listen on \\'0.0.0.0\\' or \\'::\\' return all addresses, including\\n    the loopback.\\n\\n    :param str bind_ip: Optional bind_ip from a config file; may be IP address\\n                        or hostname.\\n    :returns: list of Strings of ip addresses\\n    '\n    if bind_ip:\n        try:\n            (_, _, _, _, sockaddr) = socket.getaddrinfo(bind_ip, None, 0, socket.SOCK_STREAM, 0, socket.AI_NUMERICHOST)[0]\n            if (sockaddr[0] not in ('0.0.0.0', '::')):\n                return [bind_ip]\n        except socket.gaierror:\n            pass\n    addresses = []\n    for interface in netifaces.interfaces():\n        try:\n            iface_data = netifaces.ifaddresses(interface)\n            for family in iface_data:\n                if (family not in (netifaces.AF_INET, netifaces.AF_INET6)):\n                    continue\n                for address in iface_data[family]:\n                    addr = address['addr']\n                    if (family == netifaces.AF_INET6):\n                        addr = expand_ipv6(addr.split('%')[0])\n                    addresses.append(addr)\n        except ValueError:\n            pass\n    return addresses\n", "label": 1}
{"function": "\n\ndef handle(self, fn_name, action, *args, **kwds):\n    self.parent.calls.append((self, fn_name, args, kwds))\n    if (action is None):\n        return None\n    elif (action == 'return self'):\n        return self\n    elif (action == 'return response'):\n        res = MockResponse(200, 'OK', {\n            \n        }, '')\n        return res\n    elif (action == 'return request'):\n        return Request('http://blah/')\n    elif action.startswith('error'):\n        code = action[(action.rfind(' ') + 1):]\n        try:\n            code = int(code)\n        except ValueError:\n            pass\n        res = MockResponse(200, 'OK', {\n            \n        }, '')\n        return self.parent.error('http', args[0], res, code, '', {\n            \n        })\n    elif (action == 'raise'):\n        raise urllib.error.URLError('blah')\n    assert False\n", "label": 1}
{"function": "\n\ndef generate_stylesheet(self):\n    for name in self.stylestack:\n        styles = self.styledict.get(name)\n        if (('__style-family' in styles) and (styles['__style-family'] in self.styledict)):\n            familystyle = self.styledict[styles['__style-family']].copy()\n            del styles['__style-family']\n            for (style, val) in list(styles.items()):\n                familystyle[style] = val\n            styles = familystyle\n        while (('__parent-style-name' in styles) and (styles['__parent-style-name'] in self.styledict)):\n            parentstyle = self.styledict[styles['__parent-style-name']].copy()\n            del styles['__parent-style-name']\n            for (style, val) in list(styles.items()):\n                parentstyle[style] = val\n            styles = parentstyle\n        self.styledict[name] = styles\n    self.writeout(self.default_styles)\n    for name in self.stylestack:\n        styles = self.styledict.get(name)\n        css2 = self.cs.convert_styles(styles)\n        self.writeout(('%s {\\n' % name))\n        for (style, val) in list(css2.items()):\n            self.writeout(('\\t%s: %s;\\n' % (style, val)))\n        self.writeout('}\\n')\n", "label": 1}
{"function": "\n\ndef escape(text, newline=False):\n    'Escape special html characters.'\n    if isinstance(text, str):\n        if ('&' in text):\n            text = text.replace('&', '&amp;')\n        if ('>' in text):\n            text = text.replace('>', '&gt;')\n        if ('<' in text):\n            text = text.replace('<', '&lt;')\n        if ('\"' in text):\n            text = text.replace('\"', '&quot;')\n        if (\"'\" in text):\n            text = text.replace(\"'\", '&quot;')\n        if newline:\n            if ('\\n' in text):\n                text = text.replace('\\n', '<br>')\n    return text\n", "label": 1}
{"function": "\n\ndef blockedSample(sampler, sample_size, predicates, *args):\n    blocked_sample = set()\n    remaining_sample = (sample_size - len(blocked_sample))\n    previous_sample_size = 0\n    while (remaining_sample and predicates):\n        random.shuffle(predicates)\n        new_sample = sampler(remaining_sample, predicates, *args)\n        filtered_sample = (subsample for subsample in new_sample if subsample)\n        blocked_sample.update(itertools.chain.from_iterable(filtered_sample))\n        growth = (len(blocked_sample) - previous_sample_size)\n        growth_rate = (growth / remaining_sample)\n        remaining_sample = (sample_size - len(blocked_sample))\n        previous_sample_size = len(blocked_sample)\n        if (growth_rate < 0.001):\n            warnings.warn(('%s blocked samples were requested, but only able to sample %s' % (sample_size, len(blocked_sample))))\n            break\n        predicates = [pred for (pred, pred_sample) in zip(predicates, new_sample) if (pred_sample or (pred_sample is None))]\n    return blocked_sample\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_deployment_tasks(cls, instance, graph_type=None):\n    'Get deployment graph based on release version.\\n\\n        :param instance: Release instance\\n        :type instance: models.Release\\n        :param graph_type: deployment graph type\\n        :type graph_type: basestring|None\\n        :returns: list of deployment tasks\\n        :rtype: list\\n        '\n    if (graph_type is None):\n        graph_type = consts.DEFAULT_DEPLOYMENT_GRAPH_TYPE\n    env_version = instance.environment_version\n    deployment_graph = DeploymentGraph.get_for_model(instance, graph_type)\n    if deployment_graph:\n        deployment_tasks = DeploymentGraph.get_tasks(deployment_graph)\n    else:\n        deployment_tasks = []\n    if ((graph_type == consts.DEFAULT_DEPLOYMENT_GRAPH_TYPE) and (not deployment_tasks)):\n        if env_version.startswith('5.0'):\n            deployment_tasks = yaml.load(graph_configuration.DEPLOYMENT_50)\n        elif (env_version.startswith('5.1') or env_version.startswith('6.0')):\n            deployment_tasks = yaml.load(graph_configuration.DEPLOYMENT_51_60)\n        if deployment_graph:\n            if deployment_tasks:\n                DeploymentGraph.update(deployment_graph, {\n                    'tasks': deployment_tasks,\n                })\n        else:\n            DeploymentGraph.create_for_model({\n                'tasks': deployment_tasks,\n            }, instance)\n    return deployment_tasks\n", "label": 1}
{"function": "\n\ndef reachable(stepFunction, start, destinations, _alreadyseen=None):\n    ' Determines the subset of destinations that can be reached from a set of starting positions,\\n    while using stepFunction (which produces a list of neighbor states) to navigate.\\n    Uses breadth-first search.\\n    Returns a dictionary with reachable destinations and their distances.\\n    '\n    if ((len(start) == 0) or (len(destinations) == 0)):\n        return {\n            \n        }\n    if (_alreadyseen is None):\n        _alreadyseen = []\n    _alreadyseen.extend(start)\n    res = {\n        \n    }\n    for s in start:\n        if (s in destinations):\n            res[s] = 0\n            start.remove(s)\n    new = set()\n    for s in start:\n        new.update(stepFunction(s))\n    new.difference_update(_alreadyseen)\n    ndestinations = list(destinations)\n    for s in list(new):\n        if (s in destinations):\n            res[s] = 1\n            new.remove(s)\n            ndestinations.remove(s)\n            _alreadyseen.append(s)\n    deeper = reachable(stepFunction, new, ndestinations, _alreadyseen)\n    for (k, val) in list(deeper.items()):\n        res[k] = (val + 1)\n    return res\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('grid_type', ALL_GRID_TYPES)\ndef test_append_two_quantities(self, grid_type):\n    g = self.grid[grid_type]\n    g['density'] = []\n    g['energy'] = []\n    g['density'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    assert g['density'][0]\n    assert g['density'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['density'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert g['energy'][0]\n    assert g['energy'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['energy'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert (g.n_dust == 1)\n", "label": 1}
{"function": "\n\ndef clean(self, value):\n    super(DEIdentityCardNumberField, self).clean(value)\n    if (value in EMPTY_VALUES):\n        return ''\n    match = re.match(id_re, value)\n    if (not match):\n        raise ValidationError(self.error_messages['invalid'])\n    gd = match.groupdict()\n    (residence, origin) = (gd['residence'], gd['origin'])\n    (birthday, validity, checksum) = (gd['birthday'], gd['validity'], gd['checksum'])\n    if ((residence == '0000000000') or (birthday == '0000000') or (validity == '0000000')):\n        raise ValidationError(self.error_messages['invalid'])\n    all_digits = ('%s%s%s%s' % (residence, birthday, validity, checksum))\n    if ((not self.has_valid_checksum(residence)) or (not self.has_valid_checksum(birthday)) or (not self.has_valid_checksum(validity)) or (not self.has_valid_checksum(all_digits))):\n        raise ValidationError(self.error_messages['invalid'])\n    return ('%s%s-%s-%s-%s' % (residence, origin, birthday, validity, checksum))\n", "label": 1}
{"function": "\n\ndef __init__(self, desired_capabilities=None, executable_path=None, port=0, service_log_path=None, service_args=None, opera_options=None):\n    engine = (desired_capabilities.get('engine', None) if desired_capabilities else None)\n    if ((engine == WebDriver.ServiceType.CHROMIUM) or (opera_options and opera_options.android_package_name)):\n        OperaDriver.__init__(self, executable_path=executable_path, port=port, opera_options=opera_options, service_args=service_args, desired_capabilities=desired_capabilities, service_log_path=service_log_path)\n    else:\n        if service_log_path:\n            print((\"Warning! service_log_path shouldn't be used \" + 'with Presto based Opera'))\n        if service_args:\n            print((\"Warning! service_args shouldn't be used with \" + 'Presto based Opera'))\n        if opera_options:\n            print((\"Warning! opera_options shouldn't be used with \" + 'Presto based Opera'))\n        if (not desired_capabilities):\n            desired_capabilities = DesiredCapabilities.OPERA\n        PrestoDriver.__init__(self, executable_path=executable_path, port=port, desired_capabilities=desired_capabilities)\n", "label": 1}
{"function": "\n\ndef receiveMessage(self, msg, sender):\n    if isinstance(msg, ActorExitRequest):\n        pass\n    elif ('what light' in msg):\n        self.send(sender, 'Ay me!')\n    elif (msg == 'She speaks!'):\n        self.send(sender, 'O Romeo, Romeo! wherefore art thou Romeo?')\n    elif (msg == 'Shall I hear more, or shall I speak at this?'):\n        self.send(sender, \"What's in a name? That which we call a rose\")\n        self.send(sender, 'By any other name would smell as sweet')\n    elif (msg == 'Like softest music to attending ears!'):\n        if self.nurse:\n            self.send(self.nurse, 'Anon, good nurse!')\n        else:\n            self.recalled = True\n    elif (msg == 'Mistress!'):\n        self.nurse = sender\n        if self.recalled:\n            self.send(self.nurse, 'Anon, good nurse!')\n    elif ('who_are_you' == msg):\n        self.send(sender, self.myAddress)\n", "label": 1}
{"function": "\n\n@transaction.atomic\ndef moderate(request, forum_id):\n    forum = get_object_or_404(Forum, pk=forum_id)\n    topics = forum.topics.order_by('-sticky', '-updated').select_related()\n    if (request.user.is_superuser or (request.user in forum.moderators.all())):\n        topic_ids = request.POST.getlist('topic_id')\n        if ('move_topics' in request.POST):\n            return render(request, 'djangobb_forum/move_topic.html', {\n                'categories': Category.objects.all(),\n                'topic_ids': topic_ids,\n                'exclude_forum': forum,\n            })\n        elif ('delete_topics' in request.POST):\n            for topic_id in topic_ids:\n                topic = get_object_or_404(Topic, pk=topic_id)\n                topic.delete()\n            messages.success(request, _('Topics deleted'))\n            return HttpResponseRedirect(reverse('djangobb:index'))\n        elif ('open_topics' in request.POST):\n            for topic_id in topic_ids:\n                open_close_topic(request, topic_id, 'o')\n            messages.success(request, _('Topics opened'))\n            return HttpResponseRedirect(reverse('djangobb:index'))\n        elif ('close_topics' in request.POST):\n            for topic_id in topic_ids:\n                open_close_topic(request, topic_id, 'c')\n            messages.success(request, _('Topics closed'))\n            return HttpResponseRedirect(reverse('djangobb:index'))\n        return render(request, 'djangobb_forum/moderate.html', {\n            'forum': forum,\n            'topics_page': get_page(topics, request, forum_settings.FORUM_PAGE_SIZE),\n            'posts': forum.posts.count(),\n        })\n    else:\n        raise Http404\n", "label": 1}
{"function": "\n\ndef ns_checker(self, i3s_output_list, i3s_config):\n    response = {\n        'full_text': '',\n    }\n    counter = 0\n    error = False\n    nameservers = []\n    if (not isinstance(self.nameservers, list)):\n        self.nameservers = self.nameservers.split(',')\n    if (not isinstance(self.resolvers, list)):\n        self.resolvers = self.resolvers.split(',')\n    my_resolver = dns.resolver.Resolver()\n    my_resolver.lifetime = self.lifetime\n    if self.resolvers:\n        my_resolver.nameservers = self.resolvers\n    my_ns = my_resolver.query(self.domain, 'NS')\n    for ns in my_ns:\n        nameservers.append(str(socket.gethostbyname(str(ns))))\n    for ns in self.nameservers:\n        nameservers.append(str(ns))\n    for ns in nameservers:\n        my_resolver.nameservers = [ns]\n        counter += 1\n        try:\n            my_resolver.query(self.domain, 'A')\n        except:\n            error = True\n    if error:\n        response['full_text'] = (str(counter) + ' NS NOK')\n        response['color'] = i3s_config['color_bad']\n    else:\n        response['full_text'] = (str(counter) + ' NS OK')\n        response['color'] = i3s_config['color_good']\n    return response\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@register_specialize\n@gof.local_optimizer([T.Rebroadcast])\ndef local_rebroadcast_lift(node):\n    '\\n    Lifts Rebroadcast through unary Elemwise operations,\\n    and merges consecutive Rebroadcasts.\\n\\n    Rebroadcast(Elemwise(x)) => Elemwise(Rebroadcast(x))\\n    Rebroadcast(Rebroadcast(x)) => Rebroadcast(x)\\n\\n    '\n    op = node.op\n    if (not isinstance(op, T.Rebroadcast)):\n        return False\n    input = node.inputs[0]\n    inode = input.owner\n    if (inode and isinstance(inode.op, Elemwise) and (len(inode.inputs) == 1)):\n        if (hasattr(input, 'clients') and (len(input.clients) == 1)):\n            rebroadcasted = T.Rebroadcast(*list(op.axis.items()))(inode.inputs[0])\n            copy_stack_trace(node.outputs, rebroadcasted)\n            rval = inode.op.make_node(rebroadcasted).outputs\n            copy_stack_trace((node.outputs + node.inputs), rval)\n            return rval\n    if (inode and isinstance(inode.op, T.Rebroadcast)):\n        axis = inode.op.axis.copy()\n        axis.update(op.axis)\n        iinput = inode.inputs[0]\n        rval = [T.Rebroadcast(*list(axis.items()))(iinput)]\n        copy_stack_trace((node.outputs + node.inputs), rval)\n        return rval\n", "label": 1}
{"function": "\n\ndef Buffer(self, geometries, distances, unit=None, unionResults=False, inSR=None, outSR=None, bufferSR=None):\n    'The buffer operation is performed on a geometry service resource.\\n           The result of this operation is buffer polygons at the specified\\n           distances for the input geometry array. An option is available to\\n           union buffers at each distance.'\n    if isinstance(geometries, geometry.Geometry):\n        geometries = [geometries]\n    if isinstance(distances, (list, tuple)):\n        distances = ','.join((str(distance) for distance in distances))\n    geometry_types = set([x.__geometry_type__ for x in geometries])\n    assert (len(geometry_types) == 1), 'Too many geometry types'\n    geo_json = json.dumps({\n        'geometryType': list(geometry_types)[0],\n        'geometries': [geo._json_struct_without_sr for geo in geometries],\n    })\n    if (inSR is None):\n        inSR = geometries[0].spatialReference.wkid\n    if (outSR is None):\n        outSR = geometries[0].spatialReference.wkid\n    if (bufferSR is None):\n        bufferSR = geometries[0].spatialReference.wkid\n    return self._get_subfolder('buffer', GeometryResult, {\n        'geometries': geo_json,\n        'distances': distances,\n        'unit': unit,\n        'unionResults': unionResults,\n        'inSR': inSR,\n        'outSR': outSR,\n        'bufferSR': bufferSR,\n    })\n", "label": 1}
{"function": "\n\ndef _validatePolicy(self, policy):\n    '\\n        Validate a policy JSON object.  Only a limited set of keys is\\n        supported, and each of them has a restricted data type.\\n\\n        :param policy: JSON object to validate.  This may also be a Python\\n                           dictionary as if the JSON was already decoded.\\n        :returns: a validate policy dictionary.\\n        '\n    if (not isinstance(policy, dict)):\n        try:\n            policy = json.loads(policy)\n        except ValueError:\n            raise RestException('The policy parameter must be JSON.')\n    if (not isinstance(policy, dict)):\n        raise RestException('The policy parameter must be a dictionary.')\n    validKeys = []\n    for key in dir(self):\n        if key.startswith('_validate_'):\n            validKeys.append(key.split('_validate_', 1)[1])\n    for key in list(policy):\n        if key.startswith('_'):\n            del policy[key]\n    for key in policy:\n        if (key not in validKeys):\n            raise RestException(('%s is not a valid quota policy key.  Valid keys are %s.' % (key, ', '.join(sorted(validKeys)))))\n        funcName = ('_validate_' + key)\n        policy[key] = getattr(self, funcName)(policy[key])\n    return policy\n", "label": 1}
{"function": "\n\ndef get_result(self):\n    op = self.expr.op()\n    if isinstance(op, ops.Join):\n        self._walk_join_tree(op)\n    else:\n        self.join_tables.append(self._format_table(self.expr))\n    result = self.join_tables[0]\n    for (jtype, table, preds) in zip(self.join_types, self.join_tables[1:], self.join_predicates):\n        if len(preds):\n            sqla_preds = [self._translate(pred) for pred in preds]\n            onclause = _and_all(sqla_preds)\n        else:\n            onclause = None\n        if (jtype in (ops.InnerJoin, ops.CrossJoin)):\n            result = result.join(table, onclause)\n        elif (jtype is ops.LeftJoin):\n            result = result.join(table, onclause, isouter=True)\n        elif (jtype is ops.RightJoin):\n            result = table.join(result, onclause, isouter=True)\n        elif (jtype is ops.OuterJoin):\n            result = result.outerjoin(table, onclause)\n        else:\n            raise NotImplementedError(jtype)\n    return result\n", "label": 1}
{"function": "\n\ndef python(self, options):\n    import code\n    imported_objects = {\n        \n    }\n    try:\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter\n        readline.set_completer(rlcompleter.Completer(imported_objects).complete)\n        readline_doc = getattr(readline, '__doc__', '')\n        if ((readline_doc is not None) and ('libedit' in readline_doc)):\n            readline.parse_and_bind('bind ^I rl_complete')\n        else:\n            readline.parse_and_bind('tab:complete')\n    if (not options['no_startup']):\n        for pythonrc in (os.environ.get('PYTHONSTARTUP'), '~/.pythonrc.py'):\n            if (not pythonrc):\n                continue\n            pythonrc = os.path.expanduser(pythonrc)\n            if (not os.path.isfile(pythonrc)):\n                continue\n            try:\n                with open(pythonrc) as handle:\n                    exec(compile(handle.read(), pythonrc, 'exec'), imported_objects)\n            except NameError:\n                pass\n    code.interact(local=imported_objects)\n", "label": 1}
{"function": "\n\ndef __init__(self, default=missing_, attribute=None, load_from=None, dump_to=None, error=None, validate=None, required=False, allow_none=None, load_only=False, dump_only=False, missing=missing_, error_messages=None, **metadata):\n    self.default = default\n    self.attribute = attribute\n    self.load_from = load_from\n    self.dump_to = dump_to\n    self.validate = validate\n    if utils.is_iterable_but_not_string(validate):\n        if (not utils.is_generator(validate)):\n            self.validators = validate\n        else:\n            self.validators = list(validate)\n    elif callable(validate):\n        self.validators = [validate]\n    elif (validate is None):\n        self.validators = []\n    else:\n        raise ValueError(\"The 'validate' parameter must be a callable or a collection of callables.\")\n    self.required = required\n    if (allow_none is None):\n        if (missing is None):\n            self.allow_none = True\n        else:\n            self.allow_none = False\n    else:\n        self.allow_none = allow_none\n    self.load_only = load_only\n    self.dump_only = dump_only\n    self.missing = missing\n    self.metadata = metadata\n    self._creation_index = Field._creation_index\n    Field._creation_index += 1\n    messages = {\n        \n    }\n    for cls in reversed(self.__class__.__mro__):\n        messages.update(getattr(cls, 'default_error_messages', {\n            \n        }))\n    messages.update((error_messages or {\n        \n    }))\n    self.error_messages = messages\n", "label": 1}
{"function": "\n\ndef get_contact_info(domain):\n    cache_key = ('sms-chat-contact-list-%s' % domain)\n    cache_expiration = (30 * 60)\n    try:\n        client = cache_core.get_redis_client()\n        cached_data = client.get(cache_key)\n        if cached_data:\n            return json.loads(cached_data)\n    except:\n        pass\n    verified_number_ids = VerifiedNumber.by_domain(domain, ids_only=True)\n    domain_obj = Domain.get_by_name(domain, strict=True)\n    case_ids = []\n    mobile_worker_ids = []\n    data = []\n    for doc in iter_docs(VerifiedNumber.get_db(), verified_number_ids):\n        owner_id = doc['owner_id']\n        if (doc['owner_doc_type'] == 'CommCareCase'):\n            case_ids.append(owner_id)\n            data.append([None, 'case', doc['phone_number'], owner_id, doc['_id']])\n        elif (doc['owner_doc_type'] == 'CommCareUser'):\n            mobile_worker_ids.append(owner_id)\n            data.append([None, 'mobile_worker', doc['phone_number'], owner_id, doc['_id']])\n    contact_data = get_case_contact_info(domain_obj, case_ids)\n    contact_data.update(get_mobile_worker_contact_info(domain_obj, mobile_worker_ids))\n    for row in data:\n        contact_info = contact_data.get(row[3])\n        row[0] = (contact_info[0] if contact_info else _('(unknown)'))\n    try:\n        client.set(cache_key, json.dumps(data))\n        client.expire(cache_key, cache_expiration)\n    except:\n        pass\n    return data\n", "label": 1}
{"function": "\n\n@staticmethod\ndef validate_type(value, type_name, force=False):\n    is_ok = False\n    if (not force):\n        if ((type_name == 'any') or (type_name == 'void') or (value is None)):\n            return\n        clazz = type(value)\n        if (type_name == 'function'):\n            is_ok = isinstance(value, types.FunctionType)\n        elif (type_name == 'str'):\n            is_ok = isinstance(value, (six.text_type, six.string_types))\n        elif (type_name in {'float', 'int', 'bool', 'dict', 'list'}):\n            is_ok = (type(value).__name__ == type_name)\n        else:\n            p = type_name.split('.')\n            if (1 < len(p)):\n                c = p.pop()\n                m = __import__('.'.join(p), fromlist=[c])\n                is_ok = isinstance(value, getattr(m, c))\n            else:\n                c = globals()['__builtins__'][type_name]\n                is_ok = isinstance(value, c)\n    if (not is_ok):\n        raise SaklientException('argument_type_mismatch', (((('Argument type mismatch (expected ' + type_name) + ', got ') + str(type(value))) + ')'))\n", "label": 1}
{"function": "\n\ndef compat(self, token, iterable):\n    startline = False\n    indents = []\n    toks_append = self.tokens.append\n    (toknum, tokval) = token\n    if (toknum in (NAME, NUMBER)):\n        tokval += ' '\n    if (toknum in (NEWLINE, NL)):\n        startline = True\n    for tok in iterable:\n        (toknum, tokval) = tok[:2]\n        if (toknum in (NAME, NUMBER)):\n            tokval += ' '\n        if (toknum == INDENT):\n            indents.append(tokval)\n            continue\n        elif (toknum == DEDENT):\n            indents.pop()\n            continue\n        elif (toknum in (NEWLINE, NL)):\n            startline = True\n        elif (startline and indents):\n            toks_append(indents[(- 1)])\n            startline = False\n        toks_append(tokval)\n", "label": 1}
{"function": "\n\ndef compare(self, label, model, field):\n    return ((self.label or self.model or self.field) and ((self.label is None) or (self.label == label)) and ((self.model is None) or self.match_model(model)) and ((self.field is None) or (self.field == field)))\n", "label": 1}
{"function": "\n\ndef select_treasury_duration(start_date, end_date):\n    td = (end_date - start_date)\n    if (td.days <= 31):\n        treasury_duration = '1month'\n    elif (td.days <= 93):\n        treasury_duration = '3month'\n    elif (td.days <= 186):\n        treasury_duration = '6month'\n    elif (td.days <= 366):\n        treasury_duration = '1year'\n    elif (td.days <= ((365 * 2) + 1)):\n        treasury_duration = '2year'\n    elif (td.days <= ((365 * 3) + 1)):\n        treasury_duration = '3year'\n    elif (td.days <= ((365 * 5) + 2)):\n        treasury_duration = '5year'\n    elif (td.days <= ((365 * 7) + 2)):\n        treasury_duration = '7year'\n    elif (td.days <= ((365 * 10) + 2)):\n        treasury_duration = '10year'\n    else:\n        treasury_duration = '30year'\n    return treasury_duration\n", "label": 1}
{"function": "\n\ndef headerData(self, section, orientation, role=Qt.DisplayRole):\n    if (role == Qt.TextAlignmentRole):\n        if (orientation == Qt.Horizontal):\n            return to_qvariant(int((Qt.AlignHCenter | Qt.AlignVCenter)))\n        return to_qvariant(int((Qt.AlignRight | Qt.AlignVCenter)))\n    if (role != Qt.DisplayRole):\n        return to_qvariant()\n    if (orientation == Qt.Horizontal):\n        if (section == NAME):\n            return to_qvariant('Name')\n        elif (section == VERSION):\n            return to_qvariant('Version')\n        elif (section == ACTION):\n            return to_qvariant('Action')\n        elif (section == DESCRIPTION):\n            return to_qvariant('Description')\n    return to_qvariant()\n", "label": 1}
{"function": "\n\ndef replace(self, col):\n    if ((not self.magic_flag) and isinstance(col, FromClause) and self.selectable.is_derived_from(col)):\n        return self.selectable\n    elif (not isinstance(col, ColumnElement)):\n        return None\n    elif (self.include_fn and (not self.include_fn(col))):\n        return None\n    elif (self.exclude_fn and self.exclude_fn(col)):\n        return None\n    else:\n        return self._corresponding_column(col, True)\n", "label": 1}
{"function": "\n\ndef calculate_hash(self, callback=None):\n    cookie_string = ''\n    try:\n        if self.blacklist:\n            string_with_spaces = re.sub(self.cookie_regex, '', self.request.headers['Cookie']).strip()\n            cookie_string = ''.join(string_with_spaces.split(' '))\n        else:\n            cookies_matrix = re.findall(self.cookie_regex, self.request.headers['Cookie'])\n            for cookie_tuple in cookies_matrix:\n                for item in cookie_tuple:\n                    if item:\n                        cookie_string += item.strip()\n    except KeyError:\n        pass\n    request_mod = ((self.request.method + self.request.url) + self.request.version)\n    request_mod = ((request_mod + self.request.body) + cookie_string)\n    try:\n        request_mod = (request_mod + self.request.headers['User-Agent'])\n    except KeyError:\n        pass\n    try:\n        request_mod = (request_mod + self.request.headers['Sec-Websocket-Key'])\n    except KeyError:\n        pass\n    md5_hash = hashlib.md5()\n    md5_hash.update(request_mod)\n    self.request_hash = md5_hash.hexdigest()\n    self.file_path = os.path.join(self.cache_dir, self.request_hash)\n    if callback:\n        callback(self.request_hash)\n", "label": 1}
{"function": "\n\ndef paste_config(gconfig, config_url, relative_to, global_conf=None):\n    sys.path.insert(0, relative_to)\n    pkg_resources.working_set.add_entry(relative_to)\n    config_url = config_url.split('#')[0]\n    cx = loadwsgi.loadcontext(SERVER, config_url, relative_to=relative_to, global_conf=global_conf)\n    (gc, lc) = (cx.global_conf.copy(), cx.local_conf.copy())\n    cfg = {\n        \n    }\n    (host, port) = (lc.pop('host', ''), lc.pop('port', ''))\n    if (host and port):\n        cfg['bind'] = ('%s:%s' % (host, port))\n    elif host:\n        cfg['bind'] = host.split(',')\n    cfg['default_proc_name'] = gc.get('__file__')\n    config_file = config_url.split(':')[1]\n    if _has_logging_config(config_file):\n        cfg.setdefault('logconfig', config_file)\n    for (k, v) in gc.items():\n        if (k not in gconfig.settings):\n            continue\n        cfg[k] = v\n    for (k, v) in lc.items():\n        if (k not in gconfig.settings):\n            continue\n        cfg[k] = v\n    return cfg\n", "label": 1}
{"function": "\n\ndef createLimitOrder(self, action, instrument, limitPrice, quantity):\n    if (instrument != common.btc_symbol):\n        raise Exception('Only BTC instrument is supported')\n    if (action == broker.Order.Action.BUY_TO_COVER):\n        action = broker.Order.Action.BUY\n    elif (action == broker.Order.Action.SELL_SHORT):\n        action = broker.Order.Action.SELL\n    if ((limitPrice * quantity) < BacktestingBroker.MIN_TRADE_USD):\n        raise Exception(('Trade must be >= %s' % BacktestingBroker.MIN_TRADE_USD))\n    if (action == broker.Order.Action.BUY):\n        fee = self.getCommission().calculate(None, limitPrice, quantity)\n        cashRequired = ((limitPrice * quantity) + fee)\n        if (cashRequired > self.getCash(False)):\n            raise Exception('Not enough cash')\n    elif (action == broker.Order.Action.SELL):\n        if (quantity > self.getShares(common.btc_symbol)):\n            raise Exception(('Not enough %s' % common.btc_symbol))\n    else:\n        raise Exception('Only BUY/SELL orders are supported')\n    return super(BacktestingBroker, self).createLimitOrder(action, instrument, limitPrice, quantity)\n", "label": 1}
{"function": "\n\ndef build(self, wait_time=60):\n    for container in self.start_order:\n        if (not self.config['containers'][container]):\n            sys.stderr.write((('Error: no configuration found for container: ' + container) + '\\n'))\n            exit(1)\n        config = self.config['containers'][container]\n        if ('base_image' not in config):\n            sys.stderr.write(('Error: no base image specified for container: ' + container))\n            exit(1)\n        base = config['base_image']\n        self._handleRequire(container, wait_time)\n        count = tag_name = 1\n        if ('count' in config):\n            count = tag_name = config['count']\n        while (count > 0):\n            name = container\n            if (tag_name > 1):\n                name = ((name + '__') + str(count))\n            self.log.info('Building container: %s using base template %s', name, base)\n            build = Container(name, copy.deepcopy(config))\n            dockerfile = None\n            if ('buildspec' in config):\n                if ('dockerfile' in config['buildspec']):\n                    dockerfile = config['buildspec']['dockerfile']\n                if ('url' in config['buildspec']):\n                    dockerfile_url = config['buildspec']['url']\n            build.build(dockerfile)\n            self.containers[name] = build\n            count = (count - 1)\n", "label": 1}
{"function": "\n\ndef fetch_item_bodies(self, item_ids, format='json', media_rss=False, authenticated=True):\n    query_params = {\n        'output': format,\n        'ann': 'false',\n        'likes': 'true',\n    }\n    if media_rss:\n        query_params['mediaRss'] = 'true'\n    post_params = {\n        'i': [i.decimal_form for i in item_ids],\n    }\n    result_text = self._fetch('stream/items/contents', query_params, post_params, authenticated=authenticated)\n    result = {\n        \n    }\n    if format.startswith('atom'):\n        feed = base.atom.parse(result_text)\n        for entry in feed.entries:\n            result[entry.item_id] = entry\n    else:\n        item_bodies_json = json.loads(result_text)\n        for item_body_json in item_bodies_json['items']:\n            item_id = item_id_from_atom_form(item_body_json['id'])\n            result[item_id] = item_body_json\n    for item_id in item_ids:\n        if ((item_id not in result) and (item_id not in not_found_items_ids_to_ignore)):\n            logging.warning('Requested item id %s (%s), but it was not found in the result', item_id.atom_form, item_id.decimal_form)\n    return result\n", "label": 1}
{"function": "\n\ndef isAuthorized(port, security_attributes, request_info, stp, start_time, end_time):\n    '\\n    Check if a request is authorized to use a certain port within the given criteria.\\n    '\n    default = (False if port.authz else True)\n    for rule in port.authz:\n        if (rule.type_ in HEADER_ATTRIBUTES):\n            if any([rule.match(sa) for sa in security_attributes]):\n                log.msg(('AuthZ granted for port %s: Using %s attribute' % (port.name, rule.type_)), system=LOG_SYSTEM)\n                return True\n        elif ((rule.type_ in REQUEST_ATTRIBUTES) and (rule.type_ == HOST_DN)):\n            if (rule.value == request_info.cert_host_dn):\n                log.msg(('AuthZ granted for port %s: Using certificate dn %s' % (port.name, request_info.cert_host_dn)), system=LOG_SYSTEM)\n                return True\n        else:\n            log.msg((\"Couldn't figure out what to do with rule of type %s\" % rule.type_), system='AuthZ')\n    return default\n", "label": 1}
{"function": "\n\ndef ssl_options_to_context(ssl_options):\n    'Try to convert an ``ssl_options`` dictionary to an\\n    `~ssl.SSLContext` object.\\n\\n    The ``ssl_options`` dictionary contains keywords to be passed to\\n    `ssl.wrap_socket`.  In Python 3.2+, `ssl.SSLContext` objects can\\n    be used instead.  This function converts the dict form to its\\n    `~ssl.SSLContext` equivalent, and may be used when a component which\\n    accepts both forms needs to upgrade to the `~ssl.SSLContext` version\\n    to use features like SNI or NPN.\\n    '\n    if isinstance(ssl_options, dict):\n        assert all(((k in _SSL_CONTEXT_KEYWORDS) for k in ssl_options)), ssl_options\n    if ((not hasattr(ssl, 'SSLContext')) or isinstance(ssl_options, ssl.SSLContext)):\n        return ssl_options\n    context = ssl.SSLContext(ssl_options.get('ssl_version', ssl.PROTOCOL_SSLv23))\n    if ('certfile' in ssl_options):\n        context.load_cert_chain(ssl_options['certfile'], ssl_options.get('keyfile', None))\n    if ('cert_reqs' in ssl_options):\n        context.verify_mode = ssl_options['cert_reqs']\n    if ('ca_certs' in ssl_options):\n        context.load_verify_locations(ssl_options['ca_certs'])\n    if ('ciphers' in ssl_options):\n        context.set_ciphers(ssl_options['ciphers'])\n    if hasattr(ssl, 'OP_NO_COMPRESSION'):\n        context.options |= ssl.OP_NO_COMPRESSION\n    return context\n", "label": 1}
{"function": "\n\ndef __init__(self, global_conf, script, path=None, include_os_environ=True, query_string=None):\n    if global_conf:\n        raise NotImplemented('global_conf is no longer supported for CGIApplication (use make_cgi_application); please pass None instead')\n    self.script_filename = script\n    if (path is None):\n        path = os.environ.get('PATH', '').split(':')\n    self.path = path\n    if ('?' in script):\n        assert (query_string is None), (\"You cannot have '?' in your script name (%r) and also give a query_string (%r)\" % (script, query_string))\n        (script, query_string) = script.split('?', 1)\n    if (os.path.abspath(script) != script):\n        for path_dir in self.path:\n            if os.path.exists(os.path.join(path_dir, script)):\n                self.script = os.path.join(path_dir, script)\n                break\n        else:\n            raise CGIError(('Script %r not found in path %r' % (script, self.path)))\n    else:\n        self.script = script\n    self.include_os_environ = include_os_environ\n    self.query_string = query_string\n", "label": 1}
{"function": "\n\ndef _workaround_for_old_pycparser(csource):\n    parts = []\n    while True:\n        match = _r_star_const_space.search(csource)\n        if (not match):\n            break\n        parts.append(csource[:match.start()])\n        parts.append('(')\n        closing = ')'\n        parts.append(match.group())\n        endpos = match.end()\n        if csource.startswith('*', endpos):\n            parts.append('(')\n            closing += ')'\n        level = 0\n        i = endpos\n        while (i < len(csource)):\n            c = csource[i]\n            if (c == '('):\n                level += 1\n            elif (c == ')'):\n                if (level == 0):\n                    break\n                level -= 1\n            elif (c in ',;='):\n                if (level == 0):\n                    break\n            i += 1\n        csource = ((csource[endpos:i] + closing) + csource[i:])\n    parts.append(csource)\n    return ''.join(parts)\n", "label": 1}
{"function": "\n\n@get_twit\ndef print_tweets(twit=None, settings=None):\n    'Find some tweets and print them to console.'\n    if (settings['stream'] and (settings['search'] or settings['user'])):\n        return streamed_search(twit, settings)\n    elif settings.get('uid'):\n        tweets = twit.statuses.user_timeline(user_id=settings['uid'])\n    elif settings['search']:\n        tweets = twit.search.tweets(q=','.join(settings['search']))['statuses']\n    else:\n        tweets = twit.statuses.home_timeline()\n    max_tweets = settings['max']\n    for (i, tweet) in enumerate(tweets):\n        if (not print_tweet(tweet, settings)):\n            max_tweets += 1\n        if ((i + 1) >= max_tweets):\n            break\n", "label": 1}
{"function": "\n\ndef get_all_permissions(self, user_obj, obj=None):\n    if ((not user_obj.is_active) or user_obj.is_anonymous()):\n        return set()\n    disable_cache = getattr(settings, 'DISABLE_GENERIC_PERMISSION_CACHE', False)\n    if user_obj.is_superuser:\n        perms = getattr(self, '_all_permissions', None)\n        if (perms is None):\n            perms = generate_perm_list(Permission.objects.all())\n            if (not disable_cache):\n                setattr(self, '_all_permissions', perms)\n        return perms\n    if (obj is None):\n        cache_field_name = ALL_PERMS_CACHE_FIELD\n    else:\n        cache_field_name = '{field}_{pk}_{ctype_id}'.format(field=ALL_PERMS_CACHE_FIELD, pk=obj.pk, ctype_id=ContentType.objects.get_for_model(obj).id)\n    if ((not hasattr(user_obj, cache_field_name)) or disable_cache):\n        perms = get_all_user_permissions(user_obj, obj)\n        setattr(user_obj, cache_field_name, perms)\n    return getattr(user_obj, cache_field_name)\n", "label": 1}
{"function": "\n\n@responder(pattern='^(\\\\?|help) ?(?P<regex>\\\\S+)?', form='? [regex]', auth_required=False, help='Display help, optionally filtered')\ndef help(conversation, regex):\n    help_text = ''\n    groups = {\n        \n    }\n    for func in responders.values():\n        if (func.module not in groups):\n            groups[func.module] = []\n        groups[func.module].append(func)\n    for (group, funcs) in sorted(groups.items()):\n        help_text += ('\\n<<< %s commands >>>\\n' % group)\n        for func in funcs:\n            help_text += ('%24s  %s\\n' % (func.form, func.help))\n    if regex:\n        regex = re.compile(regex)\n        output = ''\n        for line in help_text.split('\\n'):\n            if regex.search(line):\n                output += (line + '\\n')\n    else:\n        output = help_text\n    conversation.say(('\\n' + output), useHTML=False)\n    if (not regex):\n        doc_url = ('http://%s:%d/doc/index.html' % (config.HOSTNAME, config.DRONED_PORT))\n        conversation.say(('For more information read my online documentation at %s' % doc_url))\n", "label": 1}
{"function": "\n\n@classmethod\ndef build_network_xml(cls, network_name, bridge_name, addresses=None, forward=None, ip_network_address=None, ip_network_prefixlen=None, stp=True, has_pxe_server=False, has_dhcp_server=False, dhcp_range_start=None, dhcp_range_end=None, tftp_root_dir=None):\n    'Generate network XML\\n\\n        :type network: Network\\n            :rtype : String\\n        '\n    if (addresses is None):\n        addresses = []\n    network_xml = XMLGenerator('network')\n    network_xml.name(cls._crop_name(network_name))\n    network_xml.bridge(name=bridge_name, stp=('on' if stp else 'off'), delay='0')\n    if forward:\n        network_xml.forward(mode=forward)\n    if (ip_network_address is None):\n        return str(network_xml)\n    with network_xml.ip(address=ip_network_address, prefix=ip_network_prefixlen):\n        if (has_pxe_server and tftp_root_dir):\n            network_xml.tftp(root=tftp_root_dir)\n        if has_dhcp_server:\n            with network_xml.dhcp:\n                network_xml.range(start=dhcp_range_start, end=dhcp_range_end)\n                for address in addresses:\n                    network_xml.host(mac=address['mac'], ip=address['ip'], name=address['name'])\n                if has_pxe_server:\n                    network_xml.bootp(file='pxelinux.0')\n    return str(network_xml)\n", "label": 1}
{"function": "\n\ndef cascade_visibility_down(element, visibility_mode):\n    'Sets visibility for all descendents of an element. (cascades down).'\n    links = [rel.get_accessor_name() for rel in element._meta.get_all_related_objects()]\n    for link in links:\n        objects = getattr(element, link).all()\n        for object in objects:\n            try:\n                if (visibility_mode == 'private'):\n                    if object.public:\n                        object.public = False\n                        object.save()\n                elif (visibility_mode == 'public'):\n                    if (not object.public):\n                        object.public = True\n                        object.save()\n            except Exception as e:\n                pass\n            if object._meta.get_all_related_objects():\n                cascade_visibility_down(object, visibility_mode)\n", "label": 1}
{"function": "\n\ndef handle_status_result(self, cluster):\n    (cluster_status, status_times) = self.last_status.setdefault(str(cluster), (Status.OK, 0))\n    need_send_alert = False\n    if (cluster.last_status != cluster_status):\n        self.last_status[str(cluster)] = (cluster.last_status, 1)\n        if ((cluster.last_status == Status.OK) and (status_times >= ERROR_TIMES_FOR_ALERT)):\n            need_send_alert = True\n    else:\n        self.last_status[str(cluster)] = (cluster.last_status, (status_times + 1))\n        if ((cluster.last_status != Status.OK) and ((status_times + 1) == ERROR_TIMES_FOR_ALERT)):\n            need_send_alert = True\n    if need_send_alert:\n        self.alert_msg += ('[%s]Cluster[%s]\\n' % (('OK' if (cluster.last_status == Status.OK) else 'PROBLEM'), cluster))\n        for job in cluster.jobs.itervalues():\n            if (job.last_status != Status.OK):\n                self.alert_msg += ('Job[%s] not healthy: %s\\n' % (job.name, job.last_message))\n        self.alert_msg += '******\\n'\n", "label": 1}
{"function": "\n\ndef delete(self):\n    for prop in self.schema_visitor.properties:\n        if prop.is_value:\n            continue\n        if (prop.is_array and prop.items.inline):\n            for item in getattr(self, prop.name):\n                item.delete()\n        if (prop.is_object and prop.inline):\n            value = getattr(self, prop.name)\n            if (value is not None):\n                value.delete()\n    super(SchemaModel, self).delete()\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    started = time()\n    frequency = None\n    if (len(args) > 0):\n        frequency = int(args[0])\n    self.stdout.write('[*] Starting...\\n')\n    while True:\n        jobs = Cronjob.objects.all()\n        for job in jobs:\n            if (not job.need_run()):\n                continue\n            self.stdout.write((('[+] Running ' + job.description) + '...\\n'))\n            for entry in job.entry_set.filter(approved=True).exclude(travis_token=''):\n                self.stdout.write((('[+] Ping ' + entry.gh_project) + '\\n'))\n                try:\n                    ping(entry)\n                except:\n                    print_exc()\n            job.run_now()\n            job.save()\n        self.stdout.write('[-] Pinging myself as a Dead Man Snitch...\\n')\n        ping(Entry.objects.get(gh_project='FiloSottile/travis-cron'))\n        before_next = min(map((lambda job: job.before_next_run()), jobs))\n        before_next = (0 if (before_next < 0) else int((before_next + 1)))\n        if (frequency and ((time() + before_next) > (started + frequency))):\n            self.stdout.write('[-] Closing...\\n')\n            break\n        self.stdout.write((('[-] Sleeping ' + str(before_next)) + ' seconds...\\n'))\n        self.stdout.flush()\n        sleep(before_next)\n", "label": 1}
{"function": "\n\ndef find_binary(locations):\n    searchpath_sep = (';' if (sys.platform == 'win32') else ':')\n    searchpaths = os.environ['PATH'].split(searchpath_sep)\n    for location in locations:\n        if ('{PATH}' in location):\n            for searchpath in searchpaths:\n                s = location.replace('{PATH}', searchpath)\n                if (os.path.isfile(s) and os.access(s, os.X_OK)):\n                    (yield s)\n        elif (os.path.isfile(location) and os.access(location, os.X_OK)):\n            (yield location)\n", "label": 1}
{"function": "\n\ndef _create_control(self, parent):\n    dlg = QtGui.QMessageBox(parent)\n    dlg.setWindowTitle(self.title)\n    dlg.setText(self.message)\n    dlg.setInformativeText(self.informative)\n    dlg.setDetailedText(self.detail)\n    if (self.image is None):\n        dlg.setIcon(QtGui.QMessageBox.Warning)\n    else:\n        dlg.setIconPixmap(self.image.create_image())\n    if self.yes_label:\n        btn = dlg.addButton(self.yes_label, QtGui.QMessageBox.YesRole)\n    else:\n        btn = dlg.addButton(QtGui.QMessageBox.Yes)\n    self._button_result_map[btn] = YES\n    if (self.default == YES):\n        dlg.setDefaultButton(btn)\n    if self.no_label:\n        btn = dlg.addButton(self.no_label, QtGui.QMessageBox.NoRole)\n    else:\n        btn = dlg.addButton(QtGui.QMessageBox.No)\n    self._button_result_map[btn] = NO\n    if (self.default == NO):\n        dlg.setDefaultButton(btn)\n    if self.cancel:\n        if self.cancel_label:\n            btn = dlg.addButton(self.cancel_label, QtGui.QMessageBox.RejectRole)\n        else:\n            btn = dlg.addButton(QtGui.QMessageBox.Cancel)\n        self._button_result_map[btn] = CANCEL\n        if (self.default == CANCEL):\n            dlg.setDefaultButton(btn)\n    return dlg\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = RemoteDirectoryListingResult()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\n@lockutils.synchronized('cisco-nexus-portlock')\ndef update_port_postcommit(self, context):\n    'Update port non-database commit event.'\n    (vlan_segment, vxlan_segment) = self._get_segments(context.top_bound_segment, context.bottom_bound_segment)\n    (orig_vlan_segment, orig_vxlan_segment) = self._get_segments(context.original_top_bound_segment, context.original_bottom_bound_segment)\n    if self._is_vm_migrating(context, vlan_segment, orig_vlan_segment):\n        vni = (self._port_action_vxlan(context.original, orig_vxlan_segment, self._delete_nve_member) if orig_vxlan_segment else 0)\n        self._port_action_vlan(context.original, orig_vlan_segment, self._delete_switch_entry, vni)\n    elif (self._is_supported_deviceowner(context.current) and self._is_status_active(context.current)):\n        if self._is_baremetal(context.current):\n            self._init_baremetal_trunk_interfaces(context.current, vlan_segment, 0)\n            host_id = ''\n            (all_switches, active_switches) = self._get_baremetal_switches(context.current)\n        else:\n            host_id = context.current.get(portbindings.HOST_ID)\n            (all_switches, active_switches) = self._get_host_switches(host_id)\n        if ((not active_switches) and all_switches):\n            raise excep.NexusConnectFailed(nexus_host=all_switches[0], config='None', exc='Update Port Failed: Nexus Switch is down or replay in progress')\n        vni = (self._port_action_vxlan(context.current, vxlan_segment, self._configure_nve_member) if vxlan_segment else 0)\n        self._port_action_vlan(context.current, vlan_segment, self._configure_port_entries, vni)\n", "label": 1}
{"function": "\n\ndef __init__(self, country=None, state=None, locality=None, organization=None, organization_unit=None, name=None, email=None, digest='sha1', filename=None):\n    if (filename is None):\n        req = crypto.X509Req()\n        subject = req.get_subject()\n        if country:\n            subject.C = country\n        if state:\n            subject.ST = state\n        if locality:\n            subject.L = locality\n        if organization:\n            subject.O = organization\n        if organization_unit:\n            subject.OU = organization_unit\n        if name:\n            subject.CN = name\n        if email:\n            subject.emailAddress = email\n    else:\n        (ftype, text) = get_type_and_text(filename)\n        req = crypto.load_certificate_request(ftype, text)\n    self._req = req\n", "label": 1}
{"function": "\n\n@inherit_docstring_from(rv_continuous)\ndef fit(self, data, *args, **kwds):\n    f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))\n    floc = kwds.get('floc', None)\n    fscale = kwds.get('fscale', None)\n    if (floc is None):\n        return super(gamma_gen, self).fit(data, *args, **kwds)\n    if ((f0 is not None) and (fscale is not None)):\n        raise ValueError('All parameters fixed. There is nothing to optimize.')\n    data = np.asarray(data)\n    if np.any((data <= floc)):\n        raise FitDataError('gamma', lower=floc, upper=np.inf)\n    if (floc != 0):\n        data = (data - floc)\n    xbar = data.mean()\n    if (fscale is None):\n        if (f0 is not None):\n            a = f0\n        else:\n            s = (log(xbar) - log(data).mean())\n            func = (lambda a: ((log(a) - special.digamma(a)) - s))\n            aest = (((3 - s) + np.sqrt((((s - 3) ** 2) + (24 * s)))) / (12 * s))\n            xa = (aest * (1 - 0.4))\n            xb = (aest * (1 + 0.4))\n            a = optimize.brentq(func, xa, xb, disp=0)\n        scale = (xbar / a)\n    else:\n        c = (log(data).mean() - log(fscale))\n        a = _digammainv(c)\n        scale = fscale\n    return (a, floc, scale)\n", "label": 1}
{"function": "\n\ndef validates_log(obj):\n    checker = Checker()\n    check = True\n    _ = obj._\n    checker.errors = []\n    if is_param(obj.input, 's'):\n        check = (checker.check_datetime_string(_('Start Date'), obj.input.s, (CHECK_EMPTY | CHECK_VALID), obj.me.languages) and check)\n    if is_param(obj.input, 'e'):\n        check = (checker.check_datetime_string(_('End Date'), obj.input.e, (CHECK_EMPTY | CHECK_VALID), obj.me.languages) and check)\n    if is_param(obj.input, 'st'):\n        check = (checker.check_time_string(_('Start Time'), obj.input.st, (CHECK_EMPTY | CHECK_VALID)) and check)\n    if is_param(obj.input, 'et'):\n        check = (checker.check_time_string(_('End Time'), obj.input.et, (CHECK_EMPTY | CHECK_VALID)) and check)\n    obj.view.alert = checker.errors\n    return check\n", "label": 1}
{"function": "\n\ndef make_temp_dir(filename, signal=False):\n    'Creates a temporary folder and returns the joined filename'\n    try:\n        if (((testParams['user_tempdir'] is not None) and (testParams['user_tempdir'] != '')) and (testParams['actual_tempdir'] == '')):\n            testParams['actual_tempdir'] = testParams['user_tempdir']\n        if (not os.path.isdir(testParams['actual_tempdir'])):\n            os.makedirs(testParams['actual_tempdir'])\n        return os.path.join(testParams['actual_tempdir'], filename)\n    except OSError as exc:\n        actual_tempdir = os.path.join(tempfile.gettempdir(), testParams['tempdir'])\n        if signal:\n            errwrite(('I used `tempfile.gettempdir()` to create the temporary folder `%s`.' % actual_tempdir))\n        testParams['actual_tempdir'] = actual_tempdir\n        if (not os.path.isdir(testParams['actual_tempdir'])):\n            try:\n                os.makedirs(testParams['actual_tempdir'])\n            except Exception:\n                pass\n        return os.path.join(actual_tempdir, filename)\n    except:\n        get_root_logger().error('Could not create a directory.')\n        raise\n", "label": 1}
{"function": "\n\ndef copy(x):\n    new_x = pycopy.copy(x)\n    new_x.params = [x for x in new_x.params]\n    new_x.params_grad_scale = [x for x in new_x.params_grad_scale]\n    new_x.noise_params = [x for x in new_x.noise_params]\n    new_x.noise_params_shape_fn = [x for x in new_x.noise_params_shape_fn]\n    new_x.updates = [x for x in new_x.updates]\n    new_x.additional_gradients = [x for x in new_x.additional_gradients]\n    new_x.inputs = [x for x in new_x.inputs]\n    new_x.schedules = [x for x in new_x.schedules]\n    new_x.properties = [x for x in new_x.properties]\n    return new_x\n", "label": 1}
{"function": "\n\ndef register(self, app=None, discovering_apps=False):\n    if (app is None):\n        return (lambda app: self.register(app, discovering_apps))\n    if (app.__module__.split('.')[(- 1)] == 'cms_app'):\n        warnings.warn('cms_app.py filename is deprecated, and it will be removed in version 3.4; please rename it to cms_apps.py', DeprecationWarning)\n    if (self.apphooks and (not discovering_apps)):\n        return app\n    if (app.__name__ in self.apps):\n        raise AppAlreadyRegistered(('A CMS application %r is already registered' % app.__name__))\n    if (not issubclass(app, CMSApp)):\n        raise ImproperlyConfigured(('CMS application must inherit from cms.app_base.CMSApp, but %r does not' % app.__name__))\n    if ((not hasattr(app, 'menus')) and hasattr(app, 'menu')):\n        warnings.warn((\"You define a 'menu' attribute on CMS application %r, but the 'menus' attribute is empty, did you make a typo?\" % app.__name__))\n    self.apps[app.__name__] = app()\n    return app\n", "label": 1}
{"function": "\n\ndef _validate_path_args(self):\n    params = self.parameters\n    if ((self.cmd == 'mv') and self._same_path(params['src'], params['dest'])):\n        raise ValueError((\"Cannot mv a file onto itself: '%s' - '%s'\" % (params['src'], params['dest'])))\n    if (('locals3' == params['paths_type']) and (not params['is_stream'])):\n        if (not os.path.exists(params['src'])):\n            raise RuntimeError(('The user-provided path %s does not exist.' % params['src']))\n    elif (('s3local' == params['paths_type']) and params['dir_op']):\n        if (not os.path.exists(params['dest'])):\n            os.makedirs(params['dest'])\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_json(name, random=[], **json):\n    cls = None\n    if (name == 'play'):\n        cls = PlayMove\n    elif (name == 'attack'):\n        cls = AttackMove\n    elif (name == 'power'):\n        cls = PowerMove\n    elif (name == 'end'):\n        cls = TurnEndMove\n    elif (name == 'start'):\n        cls = TurnStartMove\n    elif (name == 'concede'):\n        cls = ConcedeMove\n    obj = cls.__new__(cls)\n    cls.__from_json__(obj, **json)\n    obj.random_numbers = []\n    for num in random:\n        if isinstance(num, dict):\n            obj.random_numbers.append(hearthbreaker.proxies.ProxyCharacter.from_json(**num))\n        else:\n            obj.random_numbers.append(num)\n    return obj\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.LIST):\n                self.success = []\n                (_etype31, _size28) = iprot.readListBegin()\n                for _i32 in xrange(_size28):\n                    _elem33 = Certificate()\n                    _elem33.read(iprot)\n                    self.success.append(_elem33)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\n@ComputedGraph.Function\ndef extractVectorDataAsNumpyArrayInChunks(self, stepSize=100000):\n    \"Return the data as a sequence of numpy arrays each of which is no larger than 'stepSize'.\\n\\n        This is used to prevent us from creating memory fragmentation when we are loading\\n        lots of arrays of different sizes.\\n        \"\n    if (self.computedValueVector.vectorImplVal is None):\n        return None\n    if ((len(self.vectorDataIds) > 0) and (not self.isLoaded)):\n        return None\n    if (not self.vdmThinksIsLoaded()):\n        return None\n    result = []\n    index = self.lowIndex\n    while ((index < self.highIndex) and (result is not None)):\n        tailResult = ComputedValueGateway.getGateway().extractVectorDataAsNumpyArray(self.computedValueVector, index, min(self.highIndex, (index + stepSize)))\n        index += stepSize\n        if (tailResult is not None):\n            result.append(tailResult)\n        else:\n            result = None\n    if ((result is None) and (not self.vdmThinksIsLoaded())):\n        logging.info('CumulusClient: %s was marked loaded but returned None', self)\n        self.isLoaded = False\n        ComputedValueGateway.getGateway().reloadVector(self)\n    return result\n", "label": 1}
{"function": "\n\ndef is_cached(self, path, saltenv='base', cachedir=None):\n    '\\n        Returns the full path to a file if it is cached locally on the minion\\n        otherwise returns a blank string\\n        '\n    if path.startswith('salt://'):\n        (path, senv) = salt.utils.url.parse(path)\n        if senv:\n            saltenv = senv\n    escaped = (True if salt.utils.url.is_escaped(path) else False)\n    localsfilesdest = os.path.join(self.opts['cachedir'], 'localfiles', path.lstrip('|/'))\n    filesdest = os.path.join(self.opts['cachedir'], 'files', saltenv, path.lstrip('|/'))\n    extrndest = self._extrn_path(path, saltenv, cachedir=cachedir)\n    if os.path.exists(filesdest):\n        return (salt.utils.url.escape(filesdest) if escaped else filesdest)\n    elif os.path.exists(localsfilesdest):\n        return (salt.utils.url.escape(localsfilesdest) if escaped else localsfilesdest)\n    elif os.path.exists(extrndest):\n        return extrndest\n    return ''\n", "label": 1}
{"function": "\n\n@staticmethod\ndef zip_loader(filename, **kwargs):\n    'Read images from an zip file.\\n\\n        .. versionadded:: 1.0.8\\n\\n        Returns an Image with a list of type ImageData stored in Image._data\\n        '\n    _file = BytesIO(open(filename, 'rb').read())\n    z = zipfile.ZipFile(_file)\n    image_data = []\n    znamelist = z.namelist()\n    znamelist.sort()\n    image = None\n    for zfilename in znamelist:\n        try:\n            tmpfile = BytesIO(z.read(zfilename))\n            ext = zfilename.split('.')[(- 1)].lower()\n            im = None\n            for loader in ImageLoader.loaders:\n                if ((ext not in loader.extensions()) or (not loader.can_load_memory())):\n                    continue\n                Logger.debug(('Image%s: Load <%s> from <%s>' % (loader.__name__[11:], zfilename, filename)))\n                try:\n                    im = loader(zfilename, ext=ext, rawdata=tmpfile, inline=True, **kwargs)\n                except:\n                    continue\n                break\n            if (im is not None):\n                image_data.append(im._data[0])\n                image = im\n        except:\n            Logger.warning(('Image: Unable to load image<%s> in zip <%s> trying to continue...' % (zfilename, filename)))\n    z.close()\n    if (len(image_data) == 0):\n        raise Exception(('no images in zip <%s>' % filename))\n    image._data = image_data\n    image.filename = filename\n    return image\n", "label": 1}
{"function": "\n\ndef read_regexp_block(stream, start_re, end_re=None):\n    '\\n    Read a sequence of tokens from a stream, where tokens begin with\\n    lines that match ``start_re``.  If ``end_re`` is specified, then\\n    tokens end with lines that match ``end_re``; otherwise, tokens end\\n    whenever the next line matching ``start_re`` or EOF is found.\\n    '\n    while True:\n        line = stream.readline()\n        if (not line):\n            return []\n        if re.match(start_re, line):\n            break\n    lines = [line]\n    while True:\n        oldpos = stream.tell()\n        line = stream.readline()\n        if (not line):\n            return [''.join(lines)]\n        if ((end_re is not None) and re.match(end_re, line)):\n            return [''.join(lines)]\n        if ((end_re is None) and re.match(start_re, line)):\n            stream.seek(oldpos)\n            return [''.join(lines)]\n        lines.append(line)\n", "label": 1}
{"function": "\n\ndef __poisson_cdf_Q_large_lambda(k, a):\n    'Slower internal Poisson CDF evaluater for upper tail with large\\n    lambda.\\n    \\n    '\n    if (k < 0):\n        return 1\n    num_parts = int((a / LSTEP))\n    last_part = (a % LSTEP)\n    lastexp = exp((- last_part))\n    next = EXPSTEP\n    num_parts -= 1\n    for i in xrange(1, (k + 1)):\n        last = next\n        next = ((last * a) / i)\n        if (next > EXPTHRES):\n            if (num_parts >= 1):\n                next *= EXPSTEP\n                num_parts -= 1\n            else:\n                cdf *= lastexp\n                lastexp = 1\n    cdf = 0\n    i = (k + 1)\n    while (next > 0):\n        last = next\n        next = ((last * a) / i)\n        cdf += next\n        i += 1\n        if ((next > EXPTHRES) or (cdf > EXPTHRES)):\n            if (num_parts >= 1):\n                cdf *= EXPSTEP\n                next *= EXPSTEP\n                num_parts -= 1\n            else:\n                cdf *= lastexp\n                lastexp = 1\n    for i in xrange(num_parts):\n        cdf *= EXPSTEP\n    cdf *= lastexp\n    return cdf\n", "label": 1}
{"function": "\n\ndef makedirs_count(path, count=0):\n    \"\\n    Same as os.makedirs() except that this method returns the number of\\n    new directories that had to be created.\\n\\n    Also, this does not raise an error if target directory already exists.\\n    This behaviour is similar to Python 3.x's os.makedirs() called with\\n    exist_ok=True. Also similar to swift.common.utils.mkdirs()\\n\\n    https://hg.python.org/cpython/file/v3.4.2/Lib/os.py#l212\\n    \"\n    (head, tail) = os.path.split(path)\n    if (not tail):\n        (head, tail) = os.path.split(head)\n    if (head and tail and (not os.path.exists(head))):\n        count = makedirs_count(head, count)\n        if (tail == os.path.curdir):\n            return\n    try:\n        os.mkdir(path)\n    except OSError as e:\n        if ((e.errno != errno.EEXIST) or (not os.path.isdir(path))):\n            raise\n    else:\n        count += 1\n    return count\n", "label": 1}
{"function": "\n\ndef _configure(self, qtile, bar):\n    base._Widget._configure(self, qtile, bar)\n    if (not self.filename):\n        raise ValueError('Filename not set!')\n    self.filename = os.path.expanduser(self.filename)\n    try:\n        self.image = cairocffi.ImageSurface.create_from_png(self.filename)\n    except MemoryError:\n        raise ValueError((\"The image '%s' doesn't seem to be a valid PNG\" % self.filename))\n    self.pattern = cairocffi.SurfacePattern(self.image)\n    self.image_width = self.image.get_width()\n    self.image_height = self.image.get_height()\n    if self.scale:\n        if self.bar.horizontal:\n            new_height = (self.bar.height - (self.margin_y * 2))\n            if (new_height and (self.image_height != new_height)):\n                scaler = cairocffi.Matrix()\n                sp = (self.image_height / new_height)\n                self.image_height = new_height\n                self.image_width = int((self.image_width / sp))\n                scaler.scale(sp, sp)\n                self.pattern.set_matrix(scaler)\n        else:\n            new_width = (self.bar.width - (self.margin_x * 2))\n            if (new_width and (self.image_width != new_width)):\n                scaler = cairocffi.Matrix()\n                sp = (self.image_width / new_width)\n                self.image_width = new_width\n                self.image_height = int((self.image_height / sp))\n                scaler.scale(sp, sp)\n                self.pattern.set_matrix(scaler)\n", "label": 1}
{"function": "\n\ndef data_to_field(field, data):\n    if isinstance(field, fields.EmbeddedDocumentField):\n        return data_to_document(field.document_type_obj, data)\n    elif isinstance(field, (fields.ListField, fields.SequenceField, fields.SortedListField)):\n        l = []\n        for d in data:\n            l.append(data_to_field(field.field, d))\n        return l\n    elif isinstance(field, fields.FileField):\n        if data.filename:\n            gfs = field.proxy_class(db_alias=field.db_alias, collection_name=field.collection_name, instance=field.owner_document(), key=field.name)\n            gfs.put(data.stream, filename=secure_filename(data.filename), content_type=data.mimetype)\n            return gfs\n        elif data.clear:\n            return _remove_file_value\n        return _unset_value\n    elif (isinstance(field, (fields.ReferenceField, fields.ObjectIdField)) and isinstance(data, basestring)):\n        from bson.objectid import ObjectId\n        return ObjectId(data)\n    else:\n        return data\n", "label": 1}
{"function": "\n\ndef _set_hash_type(entity, value):\n    'Callback hook to set the hash type based on the length of the value.\\n\\n    If the `Hash` object already has a type, it is not changed.\\n\\n    Args:\\n        entity (Hash): The Hash object being modified.\\n        value (str): The hash value\\n    '\n    if entity.type_:\n        return\n    if (not value):\n        return\n    hashlen = len(value.value)\n    if (hashlen == 32):\n        entity.type_ = Hash.TYPE_MD5\n    elif (hashlen == 40):\n        entity.type_ = Hash.TYPE_SHA1\n    elif (hashlen == 56):\n        entity.type_ = Hash.TYPE_SHA224\n    elif (hashlen == 64):\n        entity.type_ = Hash.TYPE_SHA256\n    elif (hashlen == 96):\n        entity.type_ = Hash.TYPE_SHA384\n    elif (hashlen == 128):\n        entity.type_ = Hash.TYPE_SHA512\n    else:\n        entity.type_ = Hash.TYPE_OTHER\n", "label": 1}
{"function": "\n\ndef __init__(self, v, aspectValues=None):\n    global Aspect\n    if (Aspect is None):\n        from arelle.ModelFormulaObject import Aspect\n    self.modelXbrl = v.modelXbrl\n    if (aspectValues is None):\n        aspectValues = {\n            \n        }\n    self.aspectEntryObjectId = aspectValues.get('aspectEntryObjectId', None)\n    if (Aspect.CONCEPT in aspectValues):\n        qname = aspectValues[Aspect.CONCEPT]\n        self.qname = qname\n        self.concept = v.modelXbrl.qnameConcepts.get(qname)\n        self.isItem = ((self.concept is not None) and self.concept.isItem)\n        self.isTuple = ((self.concept is not None) and self.concept.isTuple)\n    else:\n        self.qname = None\n        self.concept = None\n        self.isItem = False\n        self.isTuple = False\n    if (Aspect.LOCATION in aspectValues):\n        self.parent = aspectValues[Aspect.LOCATION]\n        try:\n            self.isTuple = self.parent.isTuple\n        except AttributeError:\n            self.isTuple = False\n    else:\n        self.parent = v.modelXbrl.modelDocument.xmlRootElement\n    self.isNumeric = ((self.concept is not None) and self.concept.isNumeric)\n    self.context = ContextPrototype(v, aspectValues)\n    if (Aspect.UNIT in aspectValues):\n        self.unit = UnitPrototype(v, aspectValues)\n    else:\n        self.unit = None\n    self.factObjectId = None\n", "label": 1}
{"function": "\n\ndef tearDown(self):\n    'Runs after each test method to tear down test environment.'\n    try:\n        self.mox.UnsetStubs()\n        self.stubs.UnsetAll()\n        self.stubs.SmartUnsetAll()\n        self.mox.VerifyAll()\n        super(TestCase, self).tearDown()\n    finally:\n        if FLAGS.fake_rabbit:\n            fakerabbit.reset_all()\n        if (FLAGS.connection_type == 'fake'):\n            if hasattr(fake.FakeConnection, '_instance'):\n                del fake.FakeConnection._instance\n        if (FLAGS.image_service == 'nova.image.fake.FakeImageService'):\n            nova.image.fake.FakeImageService_reset()\n        self.reset_flags()\n        for x in self.injected:\n            try:\n                x.stop()\n            except AssertionError:\n                pass\n        for x in self._services:\n            try:\n                x.kill()\n            except Exception:\n                pass\n", "label": 1}
{"function": "\n\n@register_specialize\n@gof.local_optimizer([T.neg])\ndef local_neg_div_neg(node):\n    '\\n    - (-a / b) -> a / b\\n\\n    Also performs - (c / b) -> ((-c) / b) when c is a scalar constant.\\n\\n    '\n    if (node.op == T.neg):\n        if (node.inputs[0].owner and (node.inputs[0].owner.op == T.true_div)):\n            frac = node.inputs[0]\n            (num, denom) = frac.owner.inputs\n            if (num.owner and (num.owner.op == T.neg)):\n                if (len(frac.clients) == 1):\n                    new_num = num.owner.inputs[0]\n                    return [T.true_div(new_num, denom)]\n            elif (numpy.all(num.broadcastable) and isinstance(num, Constant)):\n                if (len(frac.clients) == 1):\n                    new_num = (- num.data)\n                    return [T.true_div(new_num, denom)]\n", "label": 1}
{"function": "\n\ndef __init__(self, credentials, subscription_id, api_version='2016-03-30', accept_language='en-US', long_running_operation_retry_timeout=30, generate_client_request_id=True, base_url=None, filepath=None):\n    if (credentials is None):\n        raise ValueError(\"Parameter 'credentials' must not be None.\")\n    if (subscription_id is None):\n        raise ValueError(\"Parameter 'subscription_id' must not be None.\")\n    if (not isinstance(subscription_id, str)):\n        raise TypeError(\"Parameter 'subscription_id' must be str.\")\n    if ((api_version is not None) and (not isinstance(api_version, str))):\n        raise TypeError(\"Optional parameter 'api_version' must be str.\")\n    if ((accept_language is not None) and (not isinstance(accept_language, str))):\n        raise TypeError(\"Optional parameter 'accept_language' must be str.\")\n    if (not base_url):\n        base_url = 'https://management.azure.com'\n    super(NetworkManagementClientConfiguration, self).__init__(base_url, filepath)\n    self.add_user_agent('networkmanagementclient/{}'.format(VERSION))\n    self.add_user_agent('Azure-SDK-For-Python')\n    self.credentials = credentials\n    self.subscription_id = subscription_id\n    self.api_version = api_version\n    self.accept_language = accept_language\n    self.long_running_operation_retry_timeout = long_running_operation_retry_timeout\n    self.generate_client_request_id = generate_client_request_id\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    \"Emmulate GNU diff's format.\\n    Header: @@ -382,8 +481,9 @@\\n    Indicies are printed as 1-based, not 0-based.\\n\\n    Returns:\\n      The GNU diff string.\\n    \"\n    if (self.length1 == 0):\n        coords1 = (str(self.start1) + ',0')\n    elif (self.length1 == 1):\n        coords1 = str((self.start1 + 1))\n    else:\n        coords1 = ((str((self.start1 + 1)) + ',') + str(self.length1))\n    if (self.length2 == 0):\n        coords2 = (str(self.start2) + ',0')\n    elif (self.length2 == 1):\n        coords2 = str((self.start2 + 1))\n    else:\n        coords2 = ((str((self.start2 + 1)) + ',') + str(self.length2))\n    text = ['@@ -', coords1, ' +', coords2, ' @@\\n']\n    for (op, data) in self.diffs:\n        if (op == diff_match_patch.DIFF_INSERT):\n            text.append('+')\n        elif (op == diff_match_patch.DIFF_DELETE):\n            text.append('-')\n        elif (op == diff_match_patch.DIFF_EQUAL):\n            text.append(' ')\n        data = data.encode('utf-8')\n        text.append((urllib.quote(data, \"!~*'();/?:@&=+$,# \") + '\\n'))\n    return ''.join(text)\n", "label": 1}
{"function": "\n\ndef _translations(self):\n    '\\n            Render the translations for all configured languages\\n\\n            @returns: translation tags\\n        '\n    T = current.T\n    translations = TAG['']()\n    strings = self._strings()\n    if (self.translate and strings):\n        append_translation = translations.append\n        languages = [l for l in current.response.s3.l10n_languages if (l != 'en')]\n        languages.insert(0, 'en')\n        for language in languages:\n            translation = TAG['translation'](_lang=language)\n            append_string = translation.append\n            for (key, string) in strings.items():\n                tstr = T((string.m if hasattr(string, 'm') else string), language=language)\n                append_string(TAG['text'](TAG['value'](tstr), _id=key))\n            if len(translation):\n                append_translation(translation)\n    return TAG['itext'](translations)\n", "label": 1}
{"function": "\n\ndef run_job(self, job_name, unzip=False, wait_for_workers=False):\n    if wait_for_workers:\n        while (not self.stopped.is_set()):\n            if (len(self.worker_tracker.workers) > 0):\n                break\n            stopped = self.stopped.wait(3)\n            if stopped:\n                return\n    if unzip:\n        self._unzip(job_name)\n    job_path = os.path.join(self.job_dir, job_name)\n    job_desc = import_job_desc(job_path)\n    job_master = JobMaster(self.ctx, job_name, job_desc, self.worker_tracker.workers.keys())\n    job_master.init()\n    self.job_tracker.register_job(job_name, job_master)\n    self._register_runned_job(job_name, job_desc)\n    zip_file = os.path.join(self.zip_dir, (job_name + '.zip'))\n    for worker in job_master.workers:\n        FileTransportClient(worker, zip_file).send_file()\n    self.logger.debug(('entering the master prepare stage, job id: %s' % job_name))\n    self.logger.debug(('job available workers: %s' % job_master.workers))\n    stage = Stage(job_master.workers, 'prepare', logger=self.logger)\n    prepared_ok = stage.barrier(True, job_name)\n    if (not prepared_ok):\n        self.logger.error('prepare for running failed')\n        return\n    self.logger.debug(('entering the master run_job stage, job id: %s' % job_name))\n    stage = Stage(job_master.workers, 'run_job', logger=self.logger)\n    run_ok = stage.barrier(True, job_name)\n    if (not run_ok):\n        self.logger.error(('run job failed, job id: %s' % job_name))\n", "label": 1}
{"function": "\n\ndef _create_request(self, uri, http_method, body, headers):\n    headers = (headers or {\n        \n    })\n    if (('Content-Type' in headers) and (CONTENT_TYPE_FORM_URLENCODED in headers['Content-Type'])):\n        request = Request(uri, http_method, body, headers)\n    else:\n        request = Request(uri, http_method, '', headers)\n    (signature_type, params, oauth_params) = self._get_signature_type_and_params(request)\n    if (len(dict(oauth_params)) != len(oauth_params)):\n        raise errors.InvalidRequestError(description='Duplicate OAuth1 entries.')\n    oauth_params = dict(oauth_params)\n    request.signature = oauth_params.get('oauth_signature')\n    request.client_key = oauth_params.get('oauth_consumer_key')\n    request.resource_owner_key = oauth_params.get('oauth_token')\n    request.nonce = oauth_params.get('oauth_nonce')\n    request.timestamp = oauth_params.get('oauth_timestamp')\n    request.redirect_uri = oauth_params.get('oauth_callback')\n    request.verifier = oauth_params.get('oauth_verifier')\n    request.signature_method = oauth_params.get('oauth_signature_method')\n    request.realm = dict(params).get('realm')\n    request.oauth_params = oauth_params\n    request.params = [(k, v) for (k, v) in params if (k != 'oauth_signature')]\n    if ('realm' in request.headers.get('Authorization', '')):\n        request.params = [(k, v) for (k, v) in request.params if (k != 'realm')]\n    return request\n", "label": 1}
{"function": "\n\ndef assertFormErrors(self, response, count=0, message=None, context_name='form'):\n    \"\\n        Asserts that the response does contain a form in it's\\n        context, and that form has errors, if count were given,\\n        it must match the exact numbers of errors\\n        \"\n    context = getattr(response, 'context', {\n        \n    })\n    assert (context and (context_name in context)), 'The response did not contain a form.'\n    errors = response.context[context_name]._errors\n    if count:\n        assert (len(errors) == count), ('%d errors were found on the form, %d expected' % (len(errors), count))\n        if (message and (message not in unicode(errors))):\n            self.fail(('Expected message not found, instead found: %s' % [('%s: %s' % (key, [e for e in field_errors])) for (key, field_errors) in errors.items()]))\n    else:\n        assert (len(errors) > 0), 'No errors were found on the form'\n", "label": 1}
{"function": "\n\ndef __init__(self, model=None, path=None, actions=None):\n    if model:\n        self.model = model\n    if (not self.model_name):\n        self.model_name = self.model._meta.object_name\n    if (not self.app_name):\n        self.app_name = self.model._meta.app_label\n    if ((not path) and (not self.path)):\n        self.path = self.model_name.lower()\n    if (not self.module_name):\n        parts = self.__class__.__module__.split('.')\n        self.module_name = parts[(- 2)]\n        if ((self.module_name == 'views') and (len(parts) >= 3)):\n            self.module_name = parts[(- 3)]\n    if actions:\n        self.actions = actions\n", "label": 1}
{"function": "\n\ndef getData(self, key, create_sub_data=False):\n    self.need_open()\n    if (key in self.db):\n        if (self.verbose > 1):\n            print('getData key exists')\n        (t, v) = self.get_value_and_value_type(key)\n        if (t == TYPE_SUB):\n            sub_db_name = v['name']\n            if (self.verbose > 1):\n                print('return subData stored as key', key, 'using name', sub_db_name)\n            return PersistentDataStructure(name=sub_db_name, path=os.path.join(self._dirname), verbose=self.verbose)\n        elif (t == TYPE_NPA):\n            if (self.verbose > 1):\n                print('return nparray value')\n            return self._loadNPA(v['fname'])\n        else:\n            if (self.verbose > 1):\n                print('return normal value')\n            return v\n    elif (not create_sub_data):\n        raise KeyError('key not found\\n{}'.format(key_to_str(key)))\n    else:\n        if (self.verbose > 1):\n            print('getData key does NOT exists -> create subData')\n        return self.newSubData(key)\n", "label": 1}
{"function": "\n\n@attr(speed='fast')\ndef test_2d_freq():\n    ' reading/writing of 2D RNMRTK frequency domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_2d', 'freq_2d.sec'))\n    assert (data.shape == (2048, 4096))\n    assert (np.abs((data[(0, 1)] - (- 0.19))) <= 0.01)\n    assert (np.abs((data[(10, 18)] - 0.88)) <= 0.01)\n    assert (dic['sw'][1] == 50000.0)\n    assert (dic['sf'][1] == 125.69)\n    assert (dic['ppm'][1] == 55.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 50.65)\n    assert (dic['ppm'][0] == 120.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef _print_execution_details(self, execution, args, **kwargs):\n    '\\n        Print the execution detail to stdout.\\n\\n        This method takes into account if an executed action was workflow or not\\n        and formats the output accordingly.\\n        '\n    runner_type = execution.action.get('runner_type', 'unknown')\n    is_workflow_action = (runner_type in WORKFLOW_RUNNER_TYPES)\n    show_tasks = getattr(args, 'show_tasks', False)\n    raw = getattr(args, 'raw', False)\n    detail = getattr(args, 'detail', False)\n    key = getattr(args, 'key', None)\n    attr = getattr(args, 'attr', [])\n    if (show_tasks and (not is_workflow_action)):\n        raise ValueError('--show-tasks option can only be used with workflow actions')\n    if ((not raw) and (not detail) and (show_tasks or is_workflow_action)):\n        self._run_and_print_child_task_list(execution=execution, args=args, **kwargs)\n    else:\n        instance = execution\n        if detail:\n            formatter = table.PropertyValueTable\n        else:\n            formatter = execution_formatter.ExecutionResult\n        if detail:\n            options = {\n                'attributes': copy.copy(self.display_attributes),\n            }\n        elif key:\n            options = {\n                'attributes': [('result.%s' % key)],\n                'key': key,\n            }\n        else:\n            options = {\n                'attributes': attr,\n            }\n        options['json'] = args.json\n        options['attribute_transform_functions'] = self.attribute_transform_functions\n        self.print_output(instance, formatter, **options)\n", "label": 1}
{"function": "\n\ndef _walk_TableConstructor(self, node):\n    (yield self._get_text(node, '{'))\n    self._indent += 1\n    if node.fields:\n        for t in self._walk(node.fields[0]):\n            (yield t)\n        if (len(node.fields) > 1):\n            for i in range(1, len(node.fields)):\n                (yield self._get_code_for_spaces(node))\n                if self._args.get('ignore_tokens'):\n                    (yield ', ')\n                else:\n                    (yield self._get_text(node, self._tokens[self._pos].code))\n                for t in self._walk(node.fields[i]):\n                    (yield t)\n    self._indent -= 1\n    (yield self._get_code_for_spaces(node))\n    if (not self._args.get('ignore_tokens')):\n        if (self._tokens[self._pos].matches(lexer.TokSymbol(',')) or self._tokens[self._pos].matches(lexer.TokSymbol(';'))):\n            (yield self._get_text(node, self._tokens[self._pos].code))\n    (yield self._get_text(node, '}'))\n", "label": 1}
{"function": "\n\ndef testParse(self):\n    s = 'foo = pkg_resources.tests.test_resources:TestEntryPoints [x]'\n    ep = EntryPoint.parse(s, self.dist)\n    self.assertfields(ep)\n    ep = EntryPoint.parse('bar baz=  spammity[PING]')\n    assert (ep.name == 'bar baz')\n    assert (ep.module_name == 'spammity')\n    assert (ep.attrs == ())\n    assert (ep.extras == ('ping',))\n    ep = EntryPoint.parse(' fizzly =  wocka:foo')\n    assert (ep.name == 'fizzly')\n    assert (ep.module_name == 'wocka')\n    assert (ep.attrs == ('foo',))\n    assert (ep.extras == ())\n    spec = 'html+mako = mako.ext.pygmentplugin:MakoHtmlLexer'\n    ep = EntryPoint.parse(spec)\n    assert (ep.name == 'html+mako')\n", "label": 1}
{"function": "\n\ndef code_to_func(lang, code, out_parms, func_name, func_parms, symb_replace):\n    if (not isinstance(code[1], list)):\n        code = (code[0], [code[1]])\n    if (not isinstance(out_parms, list)):\n        out_parms = [out_parms]\n    lang = lang.lower()\n    if (lang in ['python', 'py']):\n        gen_func = gen_py_func\n    elif (lang in ['c', 'c++']):\n        gen_func = gen_c_func\n    elif (lang in ['julia', 'jl']):\n        gen_func = gen_julia_func\n    else:\n        raise Exception('chosen language not supported.')\n    if symb_replace:\n        sympified_replace = {\n            \n        }\n        for (k, v) in symb_replace.items():\n            if isinstance(k, str):\n                k = sympy.Symbol(k)\n            if isinstance(v, str):\n                v = sympy.Symbol(v)\n            sympified_replace[k] = v\n        code = xreplace(code, sympified_replace)\n    return gen_func(code, out_parms, func_parms, func_name)\n", "label": 1}
{"function": "\n\ndef __init__(self, hidden_size, input_type='sequence', output_type='sequence', inner_activation='sigmoid', outer_activation='tanh', inner_init=None, outer_init=None, steps=None, go_backwards=False, persistent_state=False, batch_size=0, reset_state_for_input=None, forget_bias=1, mask=None, second_input=None, second_input_size=None):\n    super(LSTM, self).__init__('lstm')\n    self._hidden_size = hidden_size\n    self._input_type = input_type\n    self._output_type = output_type\n    self._inner_activation = inner_activation\n    self._outer_activation = outer_activation\n    self._inner_init = inner_init\n    self._outer_init = outer_init\n    self._steps = steps\n    self.persistent_state = persistent_state\n    self.reset_state_for_input = reset_state_for_input\n    self.batch_size = batch_size\n    self.go_backwards = go_backwards\n    mask = (mask.tensor if (type(mask) == NeuralVariable) else mask)\n    self.mask = (mask.dimshuffle((1, 0)) if mask else None)\n    self._sequence_map = OrderedDict()\n    if (type(second_input) == NeuralVariable):\n        second_input_size = second_input.dim()\n        second_input = second_input.tensor\n    self.second_input = second_input\n    self.second_input_size = second_input_size\n    self.forget_bias = forget_bias\n    if (input_type not in INPUT_TYPES):\n        raise Exception(('Input type of LSTM is wrong: %s' % input_type))\n    if (output_type not in OUTPUT_TYPES):\n        raise Exception(('Output type of LSTM is wrong: %s' % output_type))\n    if (self.persistent_state and (not self.batch_size)):\n        raise Exception('Batch size must be set for persistent state mode')\n    if (mask and (input_type == 'one')):\n        raise Exception('Mask only works with sequence input')\n", "label": 1}
{"function": "\n\ndef getnnz(self, axis=None):\n    if (axis is None):\n        nnz = len(self.data)\n        if ((nnz != len(self.row)) or (nnz != len(self.col))):\n            raise ValueError('row, column, and data array must all be the same length')\n        if ((self.data.ndim != 1) or (self.row.ndim != 1) or (self.col.ndim != 1)):\n            raise ValueError('row, column, and data arrays must be 1-D')\n        return int(nnz)\n    if (axis < 0):\n        axis += 2\n    if (axis == 0):\n        return np.bincount(downcast_intp_index(self.col), minlength=self.shape[1])\n    elif (axis == 1):\n        return np.bincount(downcast_intp_index(self.row), minlength=self.shape[0])\n    else:\n        raise ValueError('axis out of bounds')\n", "label": 1}
{"function": "\n\ndef _list_windows(self):\n    'Return list of windows in :py:obj:`dict` form.\\n\\n        Retrieved from ``$ tmux(1) list-windows`` stdout.\\n\\n        The :py:obj:`list` is derived from ``stdout`` in :class:`util.tmux_cmd`\\n        which wraps :py:class:`subprocess.Popen`.\\n\\n        :rtype: list\\n\\n        '\n    wformats = (['session_name', 'session_id'] + formats.WINDOW_FORMATS)\n    tmux_formats = [('#{%s}' % format) for format in wformats]\n    proc = self.cmd('list-windows', '-a', ('-F%s' % '\\t'.join(tmux_formats)))\n    if proc.stderr:\n        raise exc.TmuxpException(proc.stderr)\n    windows = proc.stdout\n    wformats = (['session_name', 'session_id'] + formats.WINDOW_FORMATS)\n    windows = [dict(zip(wformats, window.split('\\t'))) for window in windows]\n    windows = [dict(((k, v) for (k, v) in window.items() if v)) for window in windows]\n    for w in windows:\n        if (not ('window_id' in w)):\n            w['window_id'] = w['window_name']\n    if self._windows:\n        self._windows[:] = []\n    self._windows.extend(windows)\n    return self._windows\n", "label": 1}
{"function": "\n\ndef test_get_browsers():\n    browsers = utils.get_browsers()\n    browser_list = []\n    for browser in browsers:\n        if (browser[0] == 'Internet Explorer'):\n            assert (int(float(browser[1])) < 20)\n        browser_list.append(browser[0])\n    assert (len(browser_list) == 5)\n    assert ('Firefox' in browser_list)\n    assert ('Opera' in browser_list)\n    assert ('Chrome' in browser_list)\n    assert ('Internet Explorer' in browser_list)\n    assert ('Safari' in browser_list)\n    global browser_list\n", "label": 1}
{"function": "\n\ndef deselect(self, mode, data):\n    'Returns false if there was a data error when deselecting'\n    size = len(self.objects)\n    if (mode == 'None'):\n        return True\n    elif (mode == 'Mask'):\n        ' Deselect objects according to deselect mask '\n        mask = data\n        if (len(mask) < size):\n            mask = (mask + ([False] * (len(self.objects) - len(mask))))\n        self.logger.debug('Deselection Mask: {0}'.format(mask))\n        self.objects = [obj for (slct, obj) in filter((lambda slct_obj: (not slct_obj[0])), zip(mask, self.objects))]\n        return (len(mask) <= size)\n    elif (mode == 'OneIndices'):\n        ' Deselect objects according to indexes '\n        clean_data = list(filter((lambda i: (i < size)), data))\n        self.objects = [self.objects[i] for i in range(len(self.objects)) if (i not in clean_data)]\n        return (len(clean_data) == len(data))\n    elif (mode == 'ZeroIndices'):\n        ' Deselect objects according to indexes '\n        clean_data = list(filter((lambda i: (i < size)), data))\n        self.objects = [self.objects[i] for i in clean_data]\n        return (len(clean_data) == len(data))\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef _lift_TableColumn(self, expr, block=None):\n    node = expr.op()\n    tnode = node.table.op()\n    root = _base_table(tnode)\n    result = expr\n    if isinstance(root, ops.Selection):\n        can_lift = False\n        for val in root.selections:\n            if (isinstance(val.op(), ops.PhysicalTable) and (node.name in val.schema())):\n                can_lift = True\n                lifted_root = self.lift(val)\n            elif (isinstance(val.op(), ops.TableColumn) and (val.op().name == val.get_name()) and (node.name == val.get_name())):\n                can_lift = True\n                lifted_root = self.lift(val.op().table)\n        if (can_lift and (not block)):\n            lifted_node = ops.TableColumn(node.name, lifted_root)\n            result = expr._factory(lifted_node, name=expr._name)\n    return result\n", "label": 1}
{"function": "\n\ndef _handle_activating_mode(state, request_endpoint):\n    status = state['status']\n    if (request_endpoint == 'snapshots/<string:snapshot_id>'):\n        if (request.method in FORBIDDEN_METHODS):\n            return _return_maintenance_error(status)\n    if (request_endpoint == 'snapshots/<string:snapshot_id>/restore'):\n        return _return_maintenance_error(status)\n    if (request_endpoint == 'executions'):\n        if (request.method in FORBIDDEN_METHODS):\n            return _return_maintenance_error(status)\n    if (request_endpoint == 'deployments/<string:deployment_id>'):\n        if (request.method in FORBIDDEN_METHODS):\n            return _return_maintenance_error(status)\n    if (request_endpoint == 'deployment-modifications'):\n        if (request.method in FORBIDDEN_METHODS):\n            return _return_maintenance_error(status)\n", "label": 1}
{"function": "\n\ndef find_next_sym(next_bp, prev_bp, timeout):\n    iters = 100\n    found_sym = False\n    sample_time = 0\n    set_bp(next_bp, 0, 1)\n    set_bp(prev_bp, 1, iters)\n    while (not is_complete()):\n        pykd.go()\n        curr_bp = get_bp_hit()\n        target_time = get_proc_run_time()\n        log.debug('target time %0.2f', target_time)\n        if (curr_bp == 1):\n            if (target_time >= timeout):\n                break\n            iter_duration = (target_time - sample_time)\n            if (iter_duration < 0.5):\n                if (iters < 25600):\n                    iters *= 2\n                    log.debug('iter duration: %0.2f, (x2) prev_bp iters: %d', iter_duration, iters)\n            elif ((iter_duration >= 0.5) and (iter_duration < 0.85)):\n                iters += 100\n                log.debug('iter duration: %0.2f, (+100) prev_bp iters: %d', iter_duration, iters)\n            set_bp(prev_bp, 1, iters)\n        elif (curr_bp == 0):\n            found_sym = True\n            break\n        else:\n            log.debug('break not triggered by breakpoint')\n            if (pykd.dbgCommand('.lastevent').find('(!!! second chance !!!)') != (- 1)):\n                raise RuntimeError('Expected Timeout found Access violation!')\n        sample_time = target_time\n    pykd.removeBp(1)\n    pykd.removeBp(0)\n    return found_sym\n", "label": 1}
{"function": "\n\ndef compute(self):\n    ' compute() -> None\\n        Dispatch the vtkRenderer to the actual rendering widget\\n        '\n    if self.has_input('canvas'):\n        canvas = self.get_input('canvas')\n    else:\n        self.cellWidget = self.displayAndWait(QCDATWidget, (None,))\n        self.set_output('canvas', self.cellWidget.canvas)\n        return\n    self.set_output('canvas', canvas)\n    if (not self.has_input('gmName')):\n        return\n    if (not self.has_input('plotType')):\n        return\n    if (not self.has_input('slab1')):\n        return\n    if (not self.has_input('template')):\n        return\n    args = []\n    slab1 = self.get_input('slab1')\n    args.append(self.get_input('slab1'))\n    if self.has_input('slab2'):\n        args.append(self.get_input('slab2'))\n    args.append(self.get_input('template'))\n    args.append(self.get_input('plotType'))\n    args.append(self.get_input('gmName'))\n    kwargs = {\n        \n    }\n    if self.has_input('continents'):\n        kwargs['continents'] = self.get_input('continents')\n    self.location = CellLocation()\n    if self.has_input('row'):\n        self.location.row = self.get_input('row')\n    if self.has_input('col'):\n        self.location.col = self.get_input('col')\n    inputPorts = (canvas, args, kwargs)\n    self.displayAndWait(QCDATWidget, inputPorts)\n", "label": 1}
{"function": "\n\ndef run(self):\n    '\\n        Run the eventloop for the telnet server.\\n        '\n    listen_socket = self.create_socket(self.host, self.port)\n    logger.info('Listening for telnet connections on %s port %r', self.host, self.port)\n    try:\n        while True:\n            self.connections = set([c for c in self.connections if (not c.closed)])\n            connections = set([c for c in self.connections if (not c.handling_command)])\n            read_list = ([listen_socket, self._schedule_pipe[0]] + [c.conn for c in connections])\n            (read, _, _) = select.select(read_list, [], [])\n            for s in read:\n                if (s == listen_socket):\n                    self._accept(listen_socket)\n                elif (s == self._schedule_pipe[0]):\n                    self._process_callbacks()\n                else:\n                    self._handle_incoming_data(s)\n    finally:\n        listen_socket.close()\n", "label": 1}
{"function": "\n\ndef parse_datatypes(f):\n    dt = set()\n    re_entry = re.compile('\\\\s*<entry><type>([^<]+)</type></entry>')\n    for line in f:\n        if ('<sect1' in line):\n            break\n        if ('<entry><type>' not in line):\n            continue\n        line = re.sub('<replaceable>[^<]+</replaceable>', '', line)\n        line = re.sub('<[^>]+>', '', line)\n        for tmp in [t for tmp in line.split('[') for t in tmp.split(']') if ('(' not in t)]:\n            for t in tmp.split(','):\n                t = t.strip()\n                if (not t):\n                    continue\n                dt.add(' '.join(t.split()))\n    dt = list(dt)\n    dt.sort()\n    return dt\n", "label": 1}
{"function": "\n\ndef FindRendererForObject(rdf_obj):\n    'Find the appropriate renderer for an RDFValue object.'\n    if (not semantic_renderer_cache):\n        for cls in RDFValueRenderer.classes.values():\n            if aff4.issubclass(cls, RDFValueArrayRenderer):\n                repeated_renderer_cache[cls.classname] = cls\n            elif aff4.issubclass(cls, RDFValueRenderer):\n                semantic_renderer_cache[cls.classname] = cls\n    rdf_obj_classname = rdf_obj.__class__.__name__\n    if isinstance(rdf_obj, rdf_protodict.RDFValueArray):\n        return repeated_renderer_cache.get(rdf_obj_classname, RDFValueArrayRenderer)(rdf_obj)\n    if isinstance(rdf_obj, rdf_structs.RepeatedFieldHelper):\n        rdf_obj_classname = rdf_obj.type_descriptor.type.__name__\n        return repeated_renderer_cache.get(rdf_obj_classname, RDFValueArrayRenderer)(rdf_obj)\n    if isinstance(rdf_obj, rdf_structs.RDFProtoStruct):\n        return semantic_renderer_cache.get(rdf_obj_classname, RDFProtoRenderer)(rdf_obj)\n    if isinstance(rdf_obj, rdfvalue.RDFValue):\n        return semantic_renderer_cache.get(rdf_obj_classname, RDFValueRenderer)(rdf_obj)\n    elif isinstance(rdf_obj, dict):\n        return DictRenderer(rdf_obj)\n    return RDFValueRenderer(rdf_obj)\n", "label": 1}
{"function": "\n\ndef super_len(o):\n    total_length = 0\n    current_position = 0\n    if hasattr(o, '__len__'):\n        total_length = len(o)\n    elif hasattr(o, 'len'):\n        total_length = o.len\n    elif hasattr(o, 'getvalue'):\n        total_length = len(o.getvalue())\n    elif hasattr(o, 'fileno'):\n        try:\n            fileno = o.fileno()\n        except io.UnsupportedOperation:\n            pass\n        else:\n            total_length = os.fstat(fileno).st_size\n            if ('b' not in o.mode):\n                warnings.warn(\"Requests has determined the content-length for this request using the binary size of the file: however, the file has been opened in text mode (i.e. without the 'b' flag in the mode). This may lead to an incorrect content-length. In Requests 3.0, support will be removed for files in text mode.\", FileModeWarning)\n    if hasattr(o, 'tell'):\n        try:\n            current_position = o.tell()\n        except (OSError, IOError):\n            current_position = total_length\n    return max(0, (total_length - current_position))\n", "label": 1}
{"function": "\n\ndef get_next_fire_time(self, previous_fire_time, now):\n    if previous_fire_time:\n        start_date = min(now, (previous_fire_time + timedelta(microseconds=1)))\n    else:\n        start_date = (max(now, self.start_date) if self.start_date else now)\n    fieldnum = 0\n    next_date = datetime_ceil(start_date).astimezone(self.timezone)\n    while (0 <= fieldnum < len(self.fields)):\n        field = self.fields[fieldnum]\n        curr_value = field.get_value(next_date)\n        next_value = field.get_next_value(next_date)\n        if (next_value is None):\n            (next_date, fieldnum) = self._increment_field_value(next_date, (fieldnum - 1))\n        elif (next_value > curr_value):\n            if field.REAL:\n                next_date = self._set_field_value(next_date, fieldnum, next_value)\n                fieldnum += 1\n            else:\n                (next_date, fieldnum) = self._increment_field_value(next_date, fieldnum)\n        else:\n            fieldnum += 1\n        if (self.end_date and (next_date > self.end_date)):\n            return None\n    if (fieldnum >= 0):\n        return next_date\n", "label": 1}
{"function": "\n\ndef populate_functions(self, schema, filter_func):\n    'Returns a list of function names\\n\\n        filter_func is a function that accepts a FunctionMetadata namedtuple\\n        and returns a boolean indicating whether that function should be\\n        kept or discarded\\n        '\n    metadata = self.dbmetadata['functions']\n    if schema:\n        try:\n            return [func for (func, metas) in metadata[schema].items() for meta in metas if filter_func(meta)]\n        except KeyError:\n            return []\n    else:\n        return [func for schema in self.search_path for (func, metas) in metadata[schema].items() for meta in metas if filter_func(meta)]\n", "label": 1}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.set_query_kind(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            length = d.getVarInt32()\n            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), (d.pos() + length))\n            d.skip(length)\n            self.mutable_query_ancestor().TryMerge(tmp)\n            continue\n        if (tt == 25):\n            self.set_query_thiscursor(d.get64())\n            continue\n        if (tt == 33):\n            self.set_query_nextcursor(d.get64())\n            continue\n        if (tt == 40):\n            self.add_get_successful_fetch(d.getBoolean())\n            continue\n        if (tt == 50):\n            length = d.getVarInt32()\n            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), (d.pos() + length))\n            d.skip(length)\n            self.add_keys_read().TryMerge(tmp)\n            continue\n        if (tt == 58):\n            length = d.getVarInt32()\n            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), (d.pos() + length))\n            d.skip(length)\n            self.add_keys_written().TryMerge(tmp)\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef get_context_data(self, **kwargs):\n    context = kwargs\n    if ('view' not in context):\n        context['view'] = self\n    context['forum'] = self.get_forum()\n    context['topic'] = self.get_topic()\n    context['post'] = self.get_post()\n    if context['attachment_formset']:\n        if (hasattr(self, 'attachment_preview') and self.attachment_preview):\n            context['attachment_preview'] = self.attachment_preview\n            attachments = []\n            for form in context['attachment_formset'].forms:\n                if (form['DELETE'].value() or ((not (form['file'].html_name in self.request._files)) and (not form.instance.pk))):\n                    continue\n                attachments.append((form, (self.request._files[form['file'].html_name].name if (not form.instance) else form.instance.filename)))\n            context['attachment_file_previews'] = attachments\n    return context\n", "label": 1}
{"function": "\n\ndef finish(self):\n    self.done = True\n    if (self.has_trailers and hasattr(self.fp, 'read_trailer_lines')):\n        self.trailers = {\n            \n        }\n        try:\n            for line in self.fp.read_trailer_lines():\n                if (line[0] in ntob(' \\t')):\n                    v = line.strip()\n                else:\n                    try:\n                        (k, v) = line.split(ntob(':'), 1)\n                    except ValueError:\n                        raise ValueError('Illegal header line.')\n                    k = k.strip().title()\n                    v = v.strip()\n                if (k in comma_separated_headers):\n                    existing = self.trailers.get(envname)\n                    if existing:\n                        v = ntob(', ').join((existing, v))\n                self.trailers[k] = v\n        except Exception:\n            e = sys.exc_info()[1]\n            if (e.__class__.__name__ == 'MaxSizeExceeded'):\n                raise cherrypy.HTTPError(413, ('Maximum request length: %r' % e.args[1]))\n            else:\n                raise\n", "label": 1}
{"function": "\n\ndef process(self, challenge=None):\n    '\\n        '\n    if (challenge is None):\n        if self.has_values(['username', 'realm', 'nonce', 'key_hash', 'nc', 'cnonce', 'qops']):\n            self._qops = self.values['qops']\n            return self.response()\n        else:\n            return None\n    d = parse_challenge(challenge)\n    if (b'rspauth' in d):\n        self.mutual_auth(d[b'rspauth'])\n    else:\n        if (b'realm' not in d):\n            d[b'realm'] = self.sasl.def_realm\n        for key in ['nonce', 'realm']:\n            if (bytes(key) in d):\n                self.values[key] = d[bytes(key)]\n        self.values['nc'] = 0\n        self._qops = [b'auth']\n        if (b'qop' in d):\n            self._qops = [x.strip() for x in d[b'qop'].split(b',')]\n        self.values['qops'] = self._qops\n        if (b'maxbuf' in d):\n            self._max_buffer = int(d[b'maxbuf'])\n        return self.response()\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef _create_new_client_event(self, builder):\n    latest_ret = (yield self.store.get_latest_event_ids_and_hashes_in_room(builder.room_id))\n    if latest_ret:\n        depth = (max([d for (_, _, d) in latest_ret]) + 1)\n    else:\n        depth = 1\n    prev_events = [(event_id, prev_hashes) for (event_id, prev_hashes, _) in latest_ret]\n    builder.prev_events = prev_events\n    builder.depth = depth\n    state_handler = self.state_handler\n    context = (yield state_handler.compute_event_context(builder))\n    if ((not self.is_host_in_room(context.current_state)) and (builder.type == EventTypes.Member)):\n        prev_member_event = (yield self.store.get_room_member(builder.sender, builder.room_id))\n        context_event_ids = (e.event_id for e in context.current_state.values())\n        if (prev_member_event and (prev_member_event.event_id not in context_event_ids)):\n            builder.prev_events = (prev_member_event.event_id, prev_member_event.prev_events)\n            context = (yield state_handler.compute_event_context(builder, old_state=(prev_member_event,), outlier=True))\n    if builder.is_state():\n        builder.prev_state = (yield self.store.add_event_hashes(context.prev_state_events))\n    (yield self.auth.add_auth_events(builder, context))\n    add_hashes_and_signatures(builder, self.server_name, self.signing_key)\n    event = builder.build()\n    logger.debug('Created event %s with current state: %s', event.event_id, context.current_state)\n    defer.returnValue((event, context))\n", "label": 1}
{"function": "\n\ndef _plot_errorbars_trainset(self, canvas, which_data_rows='all', which_data_ycols='all', fixed_inputs=None, plot_raw=False, apply_link=False, label=None, projection='2d', predict_kw=None, **plot_kwargs):\n    ycols = get_which_data_ycols(self, which_data_ycols)\n    rows = get_which_data_rows(self, which_data_rows)\n    (X, _, Y) = get_x_y_var(self)\n    if (fixed_inputs is None):\n        fixed_inputs = []\n    free_dims = get_free_dims(self, None, fixed_inputs)\n    Xgrid = X.copy()\n    for (i, v) in fixed_inputs:\n        Xgrid[:, i] = v\n    plots = []\n    if ((len(free_dims) <= 2) and (projection == '2d')):\n        update_not_existing_kwargs(plot_kwargs, pl().defaults.yerrorbar)\n        if (predict_kw is None):\n            predict_kw = {\n                \n            }\n        if ('Y_metadata' not in predict_kw):\n            predict_kw['Y_metadata'] = (self.Y_metadata or {\n                \n            })\n        (mu, percs, _) = helper_predict_with_model(self, Xgrid, plot_raw, apply_link, (2.5, 97.5), ycols, predict_kw)\n        if (len(free_dims) == 1):\n            for d in ycols:\n                plots.append(pl().yerrorbar(canvas, X[(rows, free_dims[0])], mu[(rows, d)], np.vstack([(mu[(rows, d)] - percs[0][(rows, d)]), (percs[1][(rows, d)] - mu[(rows, d)])]), label=label, **plot_kwargs))\n    else:\n        raise NotImplementedError('Cannot plot in more then one dimensions, or 3d')\n    return dict(yerrorbars=plots)\n", "label": 1}
{"function": "\n\ndef send_ready(self, conn, count):\n    if (self.state == CLOSED):\n        self.logger.debug(('[%s] cannot send RDY (in state CLOSED)' % conn))\n        return\n    if (self.state == BACKOFF):\n        self.logger.debug(('[%s] cannot send RDY (in state BACKOFF)' % conn))\n        return\n    if ((self.state == THROTTLED) and self.total_in_flight_or_ready):\n        msg = '[%s] cannot send RDY (THROTTLED and %d in flight or ready)'\n        self.logger.debug((msg % (conn, self.total_in_flight_or_ready)))\n        return\n    if (not conn.is_connected):\n        self.logger.debug(('[%s] cannot send RDY (connection closed)' % conn))\n        return\n    total = ((self.total_ready_count - conn.ready_count) + count)\n    if (total > self.max_in_flight):\n        if (not (conn.ready_count or conn.in_flight)):\n            self.logger.debug(('[%s] sending later' % conn))\n            gevent.spawn_later(5, self.send_ready, conn, count)\n        return\n    self.logger.debug(('[%s] sending RDY %d' % (conn, count)))\n    try:\n        conn.ready(count)\n    except NSQSocketError as error:\n        self.logger.warn(('[%s] RDY %d failed (%r)' % (conn, count, error)))\n", "label": 1}
{"function": "\n\ndef applicationDataReceived(self, bytes):\n    '\\n        '\n    if (not self.interacting):\n        self.transport.write(bytes)\n        if (bytes in ('\\r', '\\n')):\n            self.transport.write('\\n')\n            pieces = self.cmdbuf.split(' ', 1)\n            self.cmdbuf = ''\n            (cmd, args) = (pieces[0], '')\n            if (len(pieces) > 1):\n                args = pieces[1]\n            try:\n                func = getattr(self, ('cmd_' + cmd))\n            except AttributeError:\n                self.transport.write('** Unknown command.\\r\\n')\n                return\n            func(args)\n        else:\n            self.cmdbuf += bytes\n    else:\n        for c in bytes:\n            if (ord(c) == 27):\n                self.interacting.terminal.delInteractor(self)\n                self.interacting = None\n                self.transport.write('\\r\\n** Interactive session closed.\\r\\n')\n                return\n        if (not self.readonly):\n            if (type(bytes) == type('')):\n                ttylog.ttylog_write(self.interacting.terminal.ttylog_file, len(bytes), ttylog.TYPE_INTERACT, time.time(), bytes)\n            for c in bytes:\n                recvline.HistoricRecvLine.keystrokeReceived(self.interacting, c, None)\n", "label": 1}
{"function": "\n\ndef extra_l33t_entropy(match):\n    if (('l33t' not in match) or (not match['l33t'])):\n        return 0\n    possibilities = 0\n    for (subbed, unsubbed) in match['sub'].items():\n        sub_len = len([x for x in match['token'] if (x == subbed)])\n        unsub_len = len([x for x in match['token'] if (x == unsubbed)])\n        possibilities += sum((binom((unsub_len + sub_len), i) for i in range(0, (min(unsub_len, sub_len) + 1))))\n    if (possibilities <= 1):\n        return 1\n    return lg(possibilities)\n", "label": 1}
{"function": "\n\ndef status_notifciation(changed_status):\n    notifications = {\n        \n    }\n    for (category, services) in changed_status.iteritems():\n        for (name, st) in services.iteritems():\n            if (name == 'GameServer'):\n                notifications[category] = st\n    for (category, st) in notifications.iteritems():\n        status = ('Available' if st else 'Unavailable')\n        offset = 0\n        limit = 200\n        while True:\n            subscribers = load_model('subscribers').get_subscribers(limit, offset)\n            if (not subscribers):\n                break\n            for subscribe in subscribers:\n                if (category in subscribe.categorys):\n                    alert = _trans_alert('Diablo3 %s server status has changed to %s', category, status, subscribe.locale)\n                    apns_tasks.apns_push_task.delay(subscribe.token, {\n                        \n                    }, alert=alert, badge=1, sound='default')\n            offset += len(subscribers)\n", "label": 1}
{"function": "\n\ndef interactive_running(args):\n    \"\\n    This function provides an interactive environment for running the system.\\n    It receives text from the standard input, tokenizes it, and calls the function\\n    given as a parameter to produce an answer.\\n    \\n    :param task: 'pos', 'srl' or 'dependency'\\n    :param use_tokenizer: whether to use built-in tokenizer\\n    \"\n    use_tokenizer = (not args.disable_tokenizer)\n    task_lower = args.task.lower()\n    if (task_lower == 'pos'):\n        tagger = nlpnet.taggers.POSTagger(language=args.lang)\n    elif (task_lower == 'srl'):\n        tagger = nlpnet.taggers.SRLTagger(language=args.lang)\n    elif (task_lower == 'dependency'):\n        tagger = nlpnet.taggers.DependencyParser(language=args.lang)\n    else:\n        raise ValueError(('Unknown task: %s' % args.task))\n    while True:\n        try:\n            text = raw_input()\n        except KeyboardInterrupt:\n            break\n        except EOFError:\n            break\n        if (type(text) is not unicode):\n            text = unicode(text, 'utf-8')\n        if use_tokenizer:\n            result = tagger.tag(text)\n        else:\n            tokens = text.split()\n            if (task_lower != 'dependency'):\n                result = [tagger.tag_tokens(tokens, True)]\n            else:\n                result = [tagger.tag_tokens(tokens)]\n        _print_tagged(result, task_lower)\n", "label": 1}
{"function": "\n\ndef add_vxlan(self, name, vni, group=None, dev=None, ttl=None, tos=None, local=None, port=None, proxy=False):\n    cmd = ['add', name, 'type', 'vxlan', 'id', vni]\n    if group:\n        cmd.extend(['group', group])\n    if dev:\n        cmd.extend(['dev', dev])\n    if ttl:\n        cmd.extend(['ttl', ttl])\n    if tos:\n        cmd.extend(['tos', tos])\n    if local:\n        cmd.extend(['local', local])\n    if proxy:\n        cmd.append('proxy')\n    if (port and (len(port) == 2)):\n        cmd.extend(['port', port[0], port[1]])\n    elif port:\n        raise n_exc.NetworkVxlanPortRangeError(vxlan_range=port)\n    self._as_root([], 'link', cmd)\n    return IPDevice(name, namespace=self.namespace)\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TAlterSentryRoleAddGroupsRequest')\n    if (self.protocol_version is not None):\n        oprot.writeFieldBegin('protocol_version', TType.I32, 1)\n        oprot.writeI32(self.protocol_version)\n        oprot.writeFieldEnd()\n    if (self.requestorUserName is not None):\n        oprot.writeFieldBegin('requestorUserName', TType.STRING, 2)\n        oprot.writeString(self.requestorUserName)\n        oprot.writeFieldEnd()\n    if (self.roleName is not None):\n        oprot.writeFieldBegin('roleName', TType.STRING, 3)\n        oprot.writeString(self.roleName)\n        oprot.writeFieldEnd()\n    if (self.component is not None):\n        oprot.writeFieldBegin('component', TType.STRING, 4)\n        oprot.writeString(self.component)\n        oprot.writeFieldEnd()\n    if (self.groups is not None):\n        oprot.writeFieldBegin('groups', TType.SET, 5)\n        oprot.writeSetBegin(TType.STRING, len(self.groups))\n        for iter13 in self.groups:\n            oprot.writeString(iter13)\n        oprot.writeSetEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\n@classmethod\ndef parse(cls, value):\n    '\\n        Parse this from a header value\\n        '\n    results = []\n    weak_results = []\n    while value:\n        if value.lower().startswith('w/'):\n            weak = True\n            value = value[2:]\n        else:\n            weak = False\n        if value.startswith('\"'):\n            try:\n                (etag, rest) = value[1:].split('\"', 1)\n            except ValueError:\n                etag = value.strip(' \",')\n                rest = ''\n            else:\n                rest = rest.strip(', ')\n        elif (',' in value):\n            (etag, rest) = value.split(',', 1)\n            rest = rest.strip()\n        else:\n            etag = value\n            rest = ''\n        if (etag == '*'):\n            return AnyETag\n        if etag:\n            if weak:\n                weak_results.append(etag)\n            else:\n                results.append(etag)\n        value = rest\n    return cls(results, weak_results)\n", "label": 1}
{"function": "\n\ndef main():\n    o = ord('x')\n    assert (o == 120)\n    n = float('1.1')\n    assert (n == 1.1)\n    n = float('NaN')\n    print(n)\n    assert (isNaN(n) == True)\n    r = round(1.1234, 2)\n    print(r)\n    assert (str(r) == '1.12')\n    x = chr(120)\n    print(x)\n    assert (x == 'x')\n    r = round(100.001, 2)\n    assert (r == 100)\n    i = int(100.1)\n    assert (i == 100)\n    r = round(5.49)\n    assert (r == 5)\n    r = round(5.49, 1)\n    assert (r == 5.5)\n", "label": 1}
{"function": "\n\ndef migrate_docker_facts(facts):\n    ' Apply migrations for docker facts '\n    params = {\n        'common': ('additional_registries', 'insecure_registries', 'blocked_registries', 'options'),\n        'node': ('log_driver', 'log_options'),\n    }\n    if ('docker' not in facts):\n        facts['docker'] = {\n            \n        }\n    for role in params.keys():\n        if (role in facts):\n            for param in params[role]:\n                old_param = ('docker_' + param)\n                if (old_param in facts[role]):\n                    facts['docker'][param] = facts[role].pop(old_param)\n    if (('node' in facts) and ('portal_net' in facts['node'])):\n        facts['docker']['hosted_registry_insecure'] = True\n        facts['docker']['hosted_registry_network'] = facts['node'].pop('portal_net')\n    if (('log_options' in facts['docker']) and isinstance(facts['docker']['log_options'], basestring)):\n        facts['docker']['log_options'] = facts['docker']['log_options'].split(',')\n    return facts\n", "label": 1}
{"function": "\n\ndef _ensure_inventory_uptodate(inventory, container_skel):\n    'Update inventory if needed.\\n\\n    Inspect the current inventory and ensure that all host items have all of\\n    the required entries.\\n\\n    :param inventory: ``dict`` Living inventory of containers and hosts\\n    '\n    for (key, value) in inventory['_meta']['hostvars'].iteritems():\n        if ('container_name' not in value):\n            value['container_name'] = key\n        for rh in REQUIRED_HOSTVARS:\n            if (rh not in value):\n                value[rh] = None\n                if (rh == 'container_networks'):\n                    value[rh] = {\n                        \n                    }\n    for (key, value) in container_skel.iteritems():\n        item = inventory.get(key)\n        hosts = item.get('hosts')\n        if hosts:\n            for host in hosts:\n                container = inventory['_meta']['hostvars'][host]\n                if ('properties' in value):\n                    container['properties'] = value['properties']\n", "label": 1}
{"function": "\n\ndef compress(self, node):\n    type = node.type\n    result = None\n    if (type in self.__simple):\n        result = type\n    elif (type in self.__prefixes):\n        if getattr(node, 'postfix', False):\n            result = (self.compress(node[0]) + self.__prefixes[node.type])\n        else:\n            result = (self.__prefixes[node.type] + self.compress(node[0]))\n    elif (type in self.__dividers):\n        first = self.compress(node[0])\n        second = self.compress(node[1])\n        divider = self.__dividers[node.type]\n        if (node.type not in ('plus', 'minus')):\n            result = ('%s%s%s' % (first, divider, second))\n        else:\n            result = first\n            if first.endswith(divider):\n                result += ' '\n            result += divider\n            if second.startswith(divider):\n                result += ' '\n            result += second\n    else:\n        try:\n            result = getattr(self, ('type_%s' % type))(node)\n        except KeyError:\n            print((\"Compressor does not support type '%s' from line %s in file %s\" % (type, node.line, node.getFileName())))\n            sys.exit(1)\n    if getattr(node, 'parenthesized', None):\n        return ('(%s)' % result)\n    else:\n        return result\n", "label": 1}
{"function": "\n\ndef walker(self, path=None, base_folder=None):\n    '\\n        This method walk a directory structure and create the\\n        Folders and Files as they appear.\\n        '\n    path = (path or self.path)\n    base_folder = (base_folder or self.base_folder)\n    path = os.path.normpath(upath(path))\n    if base_folder:\n        base_folder = os.path.normpath(upath(base_folder))\n        print(('The directory structure will be imported in %s' % (base_folder,)))\n    if (self.verbosity >= 1):\n        print(('Import the folders and files in %s' % (path,)))\n    root_folder_name = os.path.basename(path)\n    for (root, dirs, files) in os.walk(path):\n        rel_folders = root.partition(path)[2].strip(os.path.sep).split(os.path.sep)\n        while ('' in rel_folders):\n            rel_folders.remove('')\n        if base_folder:\n            folder_names = ((base_folder.split('/') + [root_folder_name]) + rel_folders)\n        else:\n            folder_names = ([root_folder_name] + rel_folders)\n        folder = self.get_or_create_folder(folder_names)\n        for file_obj in files:\n            dj_file = DjangoFile(open(os.path.join(root, file_obj), mode='rb'), name=file_obj)\n            self.import_file(file_obj=dj_file, folder=folder)\n    if (self.verbosity >= 1):\n        print((('folder_created #%s / file_created #%s / ' + 'image_created #%s') % (self.folder_created, self.file_created, self.image_created)))\n", "label": 1}
{"function": "\n\ndef skip(app, what, name, obj, skip, options):\n    if skip:\n        return True\n    if (name.startswith('_') and (name != '__init__')):\n        return True\n    if name.startswith('on_data'):\n        return True\n    if name.startswith('on_raw_'):\n        return True\n    if (name.startswith('on_ctcp') and (name not in ('on_ctcp', 'on_ctcp_reply'))):\n        return True\n    if name.startswith('on_isupport_'):\n        return True\n    if name.startswith('on_capability_'):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef _get_offsets(self, node):\n    '\\n        @type node: C{PyNode}\\n        @param node: a node in the Graf graph\\n        @return: the offsets contained by a given node\\n        @rtype: C{pair} of C{int}, or C{None}\\n        '\n    if ((len(node._links) == 0) and (node._outEdgeList != [])):\n        offsets = []\n        edge_list = node._outEdgeList\n        edge_list.reverse()\n        for edge in edge_list:\n            temp_offsets = self._get_offsets(edge._toNode)\n            if (temp_offsets is not None):\n                offsets.extend(self._get_offsets(edge._toNode))\n        if (len(offsets) == 0):\n            return None\n        offsets.sort()\n        start = offsets[0]\n        end = offsets[(len(offsets) - 1)]\n        return [start, end]\n    elif (len(node._links) != 0):\n        offsets = []\n        for link in node._links:\n            for region in link._regions:\n                for anchor in region._anchors:\n                    offsets.append(int(anchor._offset))\n        offsets.sort()\n        start = offsets[0]\n        end = offsets[(len(offsets) - 1)]\n        return [start, end]\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef begin_site(self):\n    for node in self.site.content.walk():\n        for resource in node.resources:\n            created = None\n            modified = None\n            try:\n                created = resource.meta.created\n                modified = resource.meta.modified\n            except AttributeError:\n                pass\n            if ((created != self.vcs_name) and (modified != self.vcs_name)):\n                continue\n            (date_created, date_modified) = self.get_dates(resource)\n            if (created == 'git'):\n                created = (date_created or datetime.utcfromtimestamp(os.path.getctime(resource.path)))\n                created = created.replace(tzinfo=None)\n                resource.meta.created = created\n            if (modified == 'git'):\n                modified = (date_modified or resource.source.last_modified)\n                modified = modified.replace(tzinfo=None)\n                resource.meta.modified = modified\n", "label": 1}
{"function": "\n\ndef embedly(url, max_width=None, key=None):\n    from embedly import Embedly\n    if (key is None):\n        key = settings.WAGTAILEMBEDS_EMBEDLY_KEY\n    client = Embedly(key=key)\n    if (max_width is not None):\n        oembed = client.oembed(url, maxwidth=max_width, better=False)\n    else:\n        oembed = client.oembed(url, better=False)\n    if oembed.get('error'):\n        if (oembed['error_code'] in [401, 403]):\n            raise AccessDeniedEmbedlyException\n        elif (oembed['error_code'] == 404):\n            raise EmbedNotFoundException\n        else:\n            raise EmbedlyException\n    if (oembed['type'] == 'photo'):\n        html = ('<img src=\"%s\" />' % (oembed['url'],))\n    else:\n        html = oembed.get('html')\n    return {\n        'title': (oembed['title'] if ('title' in oembed) else ''),\n        'author_name': (oembed['author_name'] if ('author_name' in oembed) else ''),\n        'provider_name': (oembed['provider_name'] if ('provider_name' in oembed) else ''),\n        'type': oembed['type'],\n        'thumbnail_url': oembed.get('thumbnail_url'),\n        'width': oembed.get('width'),\n        'height': oembed.get('height'),\n        'html': html,\n    }\n", "label": 1}
{"function": "\n\ndef test_repeating(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, repeat=True, _time_function=time)\n    time.time = 7\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 0)\n    assert (timer.sleep_time() == 3)\n    time.time = 34\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 6)\n    time.time = 40\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 2)\n    assert (timer.sleep_time() == 10)\n", "label": 1}
{"function": "\n\ndef request(self, method, path, body=None, headers={\n    \n}):\n    if (not self.port):\n        self.port = (443 if self.secure else 80)\n    if self.secure:\n        self.http_conn = httplib.HTTPSConnection(self.host, self.port, timeout=self.timeout)\n    else:\n        self.http_conn = httplib.HTTPConnection(self.host, self.port, timeout=self.timeout)\n    if self.authenticator:\n        self.authenticator.add_auth(headers)\n    self.http_conn.request(method, (self.root + path), body, headers=headers)\n    response = self.http_conn.getresponse()\n    body = response.read()\n    try:\n        data = json.loads(body)\n    except ValueError:\n        data = body\n    if (response.status == 403):\n        raise AuthenticationError()\n    elif (response.status == 500):\n        raise InternalServerError()\n    elif (response.status == 400):\n        raise BadRequestError(body)\n    elif (response.status == 404):\n        raise NotFoundError()\n    return (response, data)\n", "label": 1}
{"function": "\n\n@cache_readonly\ndef _var_beta_raw(self):\n    'Returns the raw covariance of beta.'\n    x = self._x\n    y = self._y\n    dates = x.index.levels[0]\n    cluster_axis = None\n    if (self._cluster == 'time'):\n        cluster_axis = 0\n    elif (self._cluster == 'entity'):\n        cluster_axis = 1\n    nobs = self._nobs\n    rmse = self._rmse_raw\n    beta = self._beta_raw\n    df = self._df_raw\n    window = self._window\n    if (not self._time_effects):\n        cum_xx = self._cum_xx(x)\n    results = []\n    for (n, i) in enumerate(self._valid_indices):\n        if (self._is_rolling and (i >= window)):\n            prior_date = dates[((i - window) + 1)]\n        else:\n            prior_date = dates[0]\n        date = dates[i]\n        x_slice = x.truncate(prior_date, date)\n        y_slice = y.truncate(prior_date, date)\n        if self._time_effects:\n            xx = _xx_time_effects(x_slice, y_slice)\n        else:\n            xx = cum_xx[i]\n            if (self._is_rolling and (i >= window)):\n                xx = (xx - cum_xx[(i - window)])\n        result = _var_beta_panel(y_slice, x_slice, beta[n], xx, rmse[n], cluster_axis, self._nw_lags, nobs[n], df[n], self._nw_overlap)\n        results.append(result)\n    return np.array(results)\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    if (not self.capture):\n        raise RuntimeError('Not capturing')\n    if (os.name == 'nt'):\n        q = Queue.Queue()\n\n        def reader(stream):\n            while 1:\n                line = stream.readline()\n                q.put(line)\n                if (not line):\n                    break\n        t1 = threading.Thread(target=reader, args=(self._cmd.stdout,))\n        t1.setDaemon(True)\n        t2 = threading.Thread(target=reader, args=(self._cmd.stderr,))\n        t2.setDaemon(True)\n        t1.start()\n        t2.start()\n        outstanding = 2\n        while outstanding:\n            item = q.get()\n            if (not item):\n                outstanding -= 1\n            else:\n                (yield item.rstrip().decode('utf-8', 'replace'))\n    else:\n        streams = [self._cmd.stdout, self._cmd.stderr]\n        while streams:\n            for l in select.select(streams, [], streams):\n                for stream in l:\n                    line = stream.readline()\n                    if (not line):\n                        if (stream in streams):\n                            streams.remove(stream)\n                        break\n                    (yield line.rstrip().decode('utf-8', 'replace'))\n", "label": 1}

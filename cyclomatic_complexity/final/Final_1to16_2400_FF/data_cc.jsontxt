{"function": "\n\ndef __init__(self, connection):\n    self.connection = connection\n", "label": 0}
{"function": "\n\ndef get_cdn_log_retention(self, container):\n    '\\n        Returns the status of the setting for CDN log retention for the\\n        specified container.\\n        '\n    return self._manager.get_cdn_log_retention(container)\n", "label": 0}
{"function": "\n\ndef test_multipath_joins():\n    (app, db, admin) = setup()\n\n    class Model1(db.Model):\n        id = db.Column(db.Integer, primary_key=True)\n        val1 = db.Column(db.String(20))\n        test = db.Column(db.String(20))\n\n    class Model2(db.Model):\n        id = db.Column(db.Integer, primary_key=True)\n        val2 = db.Column(db.String(20))\n        first_id = db.Column(db.Integer, db.ForeignKey(Model1.id))\n        first = db.relationship(Model1, backref='first', foreign_keys=[first_id])\n        second_id = db.Column(db.Integer, db.ForeignKey(Model1.id))\n        second = db.relationship(Model1, backref='second', foreign_keys=[second_id])\n    db.create_all()\n    view = CustomModelView(Model2, db.session, filters=['first.test'])\n    admin.add_view(view)\n    client = app.test_client()\n    rv = client.get('/admin/model2/')\n    eq_(rv.status_code, 200)\n", "label": 0}
{"function": "\n\ndef teetsv(table, source=None, encoding=None, errors='strict', write_header=True, **csvargs):\n    '\\n    Convenience function, as :func:`petl.io.csv.teecsv` but with different\\n    default dialect (tab delimited).\\n\\n    '\n    csvargs.setdefault('dialect', 'excel-tab')\n    return teecsv(table, source=source, encoding=encoding, errors=errors, write_header=write_header, **csvargs)\n", "label": 0}
{"function": "\n\ndef test_no_transaction_create(self):\n    manifest = shippo.Manifest.create(**self.create_mock_manifest())\n    self.assertEqual(manifest.object_status, 'NOTRANSACTIONS')\n", "label": 0}
{"function": "\n\ndef fps_return_url(self, request):\n    uri = request.build_absolute_uri()\n    parsed_url = urlparse(uri)\n    resp = self.fps_connection.verify_signature(UrlEndPoint=('%s://%s%s' % (parsed_url.scheme, parsed_url.netloc, parsed_url.path)), HttpParameters=parsed_url.query)\n    if (not (resp.VerifySignatureResult.VerificationStatus == 'Success')):\n        return HttpResponseForbidden()\n    return self.transaction(request)\n", "label": 0}
{"function": "\n\ndef __init__(self, channel, msg):\n    self._channel = channel\n    self._msg = msg\n    self._terminal_state = False\n", "label": 0}
{"function": "\n\n@utils.positional((1 + Property._positional))\ndef __init__(self, name=None, compressed=False, **kwds):\n    if compressed:\n        kwds.setdefault('indexed', False)\n    super(GenericProperty, self).__init__(name=name, **kwds)\n    self._compressed = compressed\n    if (compressed and self._indexed):\n        raise NotImplementedError(('GenericProperty %s cannot be compressed and indexed at the same time.' % self._name))\n", "label": 0}
{"function": "\n\ndef __init__(self, value, context):\n    self.value = value\n    super().__init__(context)\n", "label": 0}
{"function": "\n\ndef set_DBName(self, DBName):\n    self.add_query_param('DBName', DBName)\n", "label": 0}
{"function": "\n\ndef selflaunch(ip, line):\n    \" Launch python script with 'this' interpreter\\n    \\n    e.g. d:\\x0coo\\\\ipykit.exe a.py\\n    \\n    \"\n    tup = line.split(None, 1)\n    if (len(tup) == 1):\n        print('Launching nested ipython session')\n        os.system(sys.executable)\n        return\n    cmd = ((sys.executable + ' ') + tup[1])\n    print('>', cmd)\n    os.system(cmd)\n", "label": 0}
{"function": "\n\ndef DeferredSerializer(original, context):\n    '\\n    Serialize the result of the given Deferred without affecting its result.\\n\\n    @type original: L{defer.Deferred}\\n    @param original: The Deferred being serialized.\\n\\n    @rtype: L{defer.Deferred}\\n    @return: A Deferred which will be called back with the result of\\n        serializing the result of C{original} or which will errback if\\n        either C{original} errbacks or there is an error serializing the\\n        result of C{original}.\\n    '\n    d = defer.Deferred()\n\n    def cb(result):\n        d2 = defer.maybeDeferred(flat.serialize, result, context)\n        d2.chainDeferred(d)\n        return result\n\n    def eb(error):\n        d.errback(error)\n        return error\n    original.addCallbacks(cb, eb)\n    return d\n", "label": 0}
{"function": "\n\ndef handle_echo(remote_addr):\n    while True:\n        message = until('\\r\\n')\n        send(('you said: %s' % message))\n", "label": 0}
{"function": "\n\ndef GetCodeTypeFromDictionary(self, def_dict):\n    \"Convert a json schema type to a suitable Java type name.\\n\\n    Overrides the default.\\n\\n    Args:\\n      def_dict: (dict) A dictionary describing Json schema for this Property.\\n    Returns:\\n      A name suitable for use as a class in the generator's target language.\\n    \"\n    json_type = def_dict.get('type', 'string')\n    json_format = def_dict.get('format')\n    datatype = self.TYPE_FORMAT_TO_DATATYPE.get((json_type, json_format))\n    if datatype:\n        native_format = datatype\n    else:\n        native_format = utilities.CamelCase(json_type)\n    return native_format\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/bio_engineer/dna_template/shared_dna_template_dalyrake.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef set_node_maintenance(self, node_id, maintenance_mode, params=''):\n    self.ironic('node-set-maintenance', params='{0} {1} {2}'.format(node_id, maintenance_mode, params))\n", "label": 0}
{"function": "\n\ndef upload_artifacts_to_staging(self, local_artifacts, repo_id, print_created_artifacts=True, upload_filelist=False):\n    '\\n        :param local_artifacts: list[LocalArtifact]\\n        :param repo_id: name of staging repository\\n        :param print_created_artifacts: if True prints to stdout what was uploaded and where\\n        :param staging: bool\\n        :param upload_filelist: if True, creates and uploads a list of uploaded files\\n\\n        :return: list[RemoteArtifact]\\n        '\n    hostname_for_download = self._staging_repository_url\n    path_prefix = 'service/local/staging/deployByRepositoryId'\n    remote_artifacts = self.upload_artifacts(local_artifacts, repo_id, print_created_artifacts, hostname_for_download, path_prefix)\n    if upload_filelist:\n        coord_list = [a.get_coordinates_string() for a in remote_artifacts]\n        data = '\\n'.join(coord_list)\n        remote_path = '{path_prefix}/{repo_id}/{filelist_path}'.format(path_prefix=path_prefix, repo_id=repo_id, filelist_path=self._get_filelist_path(repo_id))\n        self._send(remote_path, method='POST', data=data, headers={\n            'Content-Type': 'text/csv',\n        })\n    return remote_artifacts\n", "label": 0}
{"function": "\n\ndef delete(self, count=100000):\n    start = _time.time()\n    for i in range(count):\n        self.r.delete(('key-%d' % i))\n    finish = _time.time()\n    return (start, finish)\n", "label": 0}
{"function": "\n\ndef test_sessionmanager_save(self):\n    '\\n        Test SessionManager.save method\\n        '\n    self.session['y'] = 1\n    self.session.save()\n    s = Session.objects.get(session_key=self.session.session_key)\n    Session.objects.save(s.session_key, {\n        'y': 2,\n    }, s.expire_date)\n    del self.session._session_cache\n    self.assertEqual(self.session['y'], 2)\n", "label": 0}
{"function": "\n\ndef test_Predict32(self):\n    theano.config.floatX = 'float32'\n    for t in SPARSE_TYPES:\n        sparse_matrix = getattr(scipy.sparse, t)\n        X = sparse_matrix((8, 4), dtype=numpy.float32)\n        yp = self.nn._predict(X)\n        assert_equal(yp.dtype, numpy.float32)\n", "label": 0}
{"function": "\n\ndef __init__(self, html=None, type=None, **kwargs):\n    if ((type is None) and html):\n        type = 'xhtml'\n    super(Content, self).__init__(type=type, **kwargs)\n    if (html is not None):\n        self.html = html\n", "label": 0}
{"function": "\n\n@contextlib.contextmanager\ndef bayesdb(metamodel=None, **kwargs):\n    if (metamodel is None):\n        crosscat = local_crosscat()\n        metamodel = CrosscatMetamodel(crosscat)\n    bdb = bayeslite.bayesdb_open(builtin_metamodels=False, **kwargs)\n    bayeslite.bayesdb_register_metamodel(bdb, metamodel)\n    try:\n        (yield bdb)\n    finally:\n        bdb.close()\n", "label": 0}
{"function": "\n\ndef unpackdict(table, field, keys=None, includeoriginal=False, samplesize=1000, missing=None):\n    \"\\n    Unpack dictionary values into separate fields. E.g.::\\n\\n        >>> import petl as etl\\n        >>> table1 = [['foo', 'bar'],\\n        ...           [1, {'baz': 'a', 'quux': 'b'}],\\n        ...           [2, {'baz': 'c', 'quux': 'd'}],\\n        ...           [3, {'baz': 'e', 'quux': 'f'}]]\\n        >>> table2 = etl.unpackdict(table1, 'bar')\\n        >>> table2\\n        +-----+-----+------+\\n        | foo | baz | quux |\\n        +=====+=====+======+\\n        |   1 | 'a' | 'b'  |\\n        +-----+-----+------+\\n        |   2 | 'c' | 'd'  |\\n        +-----+-----+------+\\n        |   3 | 'e' | 'f'  |\\n        +-----+-----+------+\\n\\n    See also :func:`petl.transform.unpacks.unpack`.\\n\\n    \"\n    return UnpackDictView(table, field, keys=keys, includeoriginal=includeoriginal, samplesize=samplesize, missing=missing)\n", "label": 0}
{"function": "\n\ndef get_pdu_json(self, time_now=None):\n    pdu_json = self.get_dict()\n    if ((time_now is not None) and ('age_ts' in pdu_json['unsigned'])):\n        age = (time_now - pdu_json['unsigned']['age_ts'])\n        pdu_json.setdefault('unsigned', {\n            \n        })['age'] = int(age)\n        del pdu_json['unsigned']['age_ts']\n    pdu_json['unsigned'].pop('redacted_because', None)\n    return pdu_json\n", "label": 0}
{"function": "\n\ndef __init__(self, consumer_key, consumer_secret, token, token_secret):\n    self.consumer = oauth2.Consumer(consumer_key, consumer_secret)\n    self.token = oauth2.Token(token, token_secret)\n", "label": 0}
{"function": "\n\ndef add_data(self, data):\n    'Add a new data set to the widget\\n\\n        :returns: True if the addition was expected, False otherwise\\n        '\n    if (data in self.client):\n        return\n    self.client.add_layer(data)\n    self._coordinator._add_data(data)\n    return True\n", "label": 0}
{"function": "\n\n@toolz.memoize\ndef table_of_metadata(metadata, name):\n    if (metadata.schema is not None):\n        name = '.'.join((metadata.schema, name))\n    if (name not in metadata.tables):\n        metadata.reflect(views=metadata.bind.dialect.supports_views)\n    return metadata.tables[name]\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Creature()\n    result.template = 'object/mobile/shared_dressed_imperial_admiral_m_2.iff'\n    result.attribute_template_id = 9\n    result.stfName('npc_name', 'human_base_male')\n    return result\n", "label": 0}
{"function": "\n\ndef _sub(self, match):\n    (before, prefix, name, postfix) = match.groups()\n    self.names.append(name)\n    if (name in self.builtins):\n        return (before + self.builtins[name])\n    elif ((prefix == '>') or (postfix == '<')):\n        return (before + self._textarea(name))\n    elif ('\"' in (prefix, postfix)):\n        self.attrs.append(name)\n        return (before + (self.require(name) or (('{{' + name) + '}}')))\n    else:\n        return (before + self._textarea(name))\n", "label": 0}
{"function": "\n\ndef test_verify_networks_resp_error(self):\n    cluster_db = self.env.create(cluster_kwargs={\n        \n    }, nodes_kwargs=[{\n        'api': False,\n    }, {\n        'api': False,\n    }])\n    (node1, node2) = self.env.nodes\n    nets_sent = [{\n        'iface': 'eth0',\n        'vlans': range(100, 105),\n    }]\n    nets_resp = [{\n        'iface': 'eth0',\n        'vlans': range(100, 104),\n    }]\n    task = Task(name='super', cluster_id=cluster_db.id)\n    task.cache = {\n        'args': {\n            'nodes': self.nodes_message((node1, node2), nets_sent),\n            'offline': 0,\n        },\n    }\n    self.db.add(task)\n    self.db.commit()\n    kwargs = {\n        'task_uuid': task.uuid,\n        'status': 'ready',\n        'nodes': self.nodes_message((node1, node2), nets_resp),\n    }\n    self.receiver.verify_networks_resp(**kwargs)\n    self.db.flush()\n    self.db.refresh(task)\n    self.assertEqual(task.status, 'error')\n    error_nodes = []\n    for node in self.env.nodes:\n        error_nodes.append({\n            'uid': node.id,\n            'interface': 'eth0',\n            'name': node.name,\n            'absent_vlans': [104],\n            'mac': node.interfaces[0].mac,\n        })\n    self.assertEqual(task.message, '')\n    self.assertEqual(task.result, error_nodes)\n", "label": 0}
{"function": "\n\ndef get_channel_list(self):\n    '\\n        Returns a sorted list of cached channels.\\n\\n        '\n    channels = listitems(self)\n    channels.sort()\n    return [value for (key, value) in channels]\n", "label": 0}
{"function": "\n\ndef get_object(self, bits):\n    slug = bits[0]\n    return Author.objects.get(slug=slug)\n", "label": 0}
{"function": "\n\ndef inner_view(request):\n    content = Template('{% url outer as outer_url %}outer:{{ outer_url }},{% url inner as inner_url %}inner:{{ inner_url }}').render(Context())\n    return HttpResponse(content)\n", "label": 0}
{"function": "\n\ndef read(self, inp):\n    '\\n        :param inp:\\n        :return: a tuple of (Enum-name, Enum-value-name)\\n        '\n    return tuple(inp.read_utf(), inp.read_utf())\n", "label": 0}
{"function": "\n\ndef gotCData(self, cdata):\n    self._gotStandalone(CDATASection, cdata)\n", "label": 0}
{"function": "\n\ndef test_super_in_class_methods_working(self):\n    d = D()\n    self.assertEqual(d.cm(), (d, (D, (D, (D, 'A'), 'B'), 'C'), 'D'))\n    e = E()\n    self.assertEqual(e.cm(), (e, (E, (E, (E, 'A'), 'B'), 'C'), 'D'))\n", "label": 0}
{"function": "\n\n@test.create_stubs({\n    neutronclient: ('create_ipsecpolicy',),\n})\ndef test_ipsecpolicy_create(self):\n    ipsecpolicy1 = self.api_ipsecpolicies.first()\n    form_data = {\n        'name': ipsecpolicy1['name'],\n        'description': ipsecpolicy1['description'],\n        'auth_algorithm': ipsecpolicy1['auth_algorithm'],\n        'encryption_algorithm': ipsecpolicy1['encryption_algorithm'],\n        'encapsulation_mode': ipsecpolicy1['encapsulation_mode'],\n        'lifetime': ipsecpolicy1['lifetime'],\n        'pfs': ipsecpolicy1['pfs'],\n        'transform_protocol': ipsecpolicy1['transform_protocol'],\n    }\n    ipsecpolicy = {\n        'ipsecpolicy': self.api_ipsecpolicies.first(),\n    }\n    neutronclient.create_ipsecpolicy({\n        'ipsecpolicy': form_data,\n    }).AndReturn(ipsecpolicy)\n    self.mox.ReplayAll()\n    ret_val = api.vpn.ipsecpolicy_create(self.request, **form_data)\n    self.assertIsInstance(ret_val, api.vpn.IPSecPolicy)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    return self.lst[key]\n", "label": 0}
{"function": "\n\ndef _make_base_dirs(self):\n    '\\n        Ensure that base dirs exists.\\n        '\n    for dir_ in [self.release_dir, self.pre_release_dir]:\n        if (not isdir(dir_)):\n            makedirs(dir_)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return (self.flickrid == other.flickrid)\n", "label": 0}
{"function": "\n\ndef _body_callback(self, buf):\n    if (self._logfile is not None):\n        self._logfile.write(buf)\n    if (self._downloadfile is not None):\n        self._downloadfile.write(buf)\n    else:\n        self._contents.append(buf)\n", "label": 0}
{"function": "\n\ndef __init__(self, server='localhost', port='8080', ssl=False):\n    '\\n        :type server: str\\n        :param server: the host to connect to that is running KairosDB\\n        :type port: str\\n        :param port: the port, as a string, that the KairosDB instance is running on\\n        :type ssl: bool\\n        :param ssl: Whether or not to use ssl for this connection.\\n        '\n    self.ssl = ssl\n    self.server = server\n    self.port = port\n    self._generate_urls()\n    metadata.get_server_version(self)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef selected_reporting_group_ids(mobile_user_and_group_slugs):\n    return [g[3:] for g in mobile_user_and_group_slugs if g.startswith('g__')]\n", "label": 0}
{"function": "\n\ndef handle_deploy_cb(self, handle, connection, status, deployer, **kwargs):\n    _log.analyze(self.node.id, '+ DEPLOYED', {\n        'status': status.status,\n    })\n    if status:\n        self.send_response(handle, connection, (json.dumps({\n            'application_id': deployer.app_id,\n            'actor_map': deployer.actor_map,\n            'placement': kwargs.get('placement', None),\n            'requirements_fulfilled': (status.status == calvinresponse.OK),\n        }) if deployer.app_id else None), status=status.status)\n    else:\n        self.send_response(handle, connection, None, status=status.status)\n", "label": 0}
{"function": "\n\ndef _combine(self, other, conn):\n    if (not isinstance(other, Q)):\n        raise TypeError(other)\n    obj = type(self)()\n    obj.add(self, conn)\n    obj.add(other, conn)\n    return obj\n", "label": 0}
{"function": "\n\ndef test_http_auth_issue_creation(self):\n    settings.REDMINE_USERNAME = 'sentry'\n    settings.REDMINE_PASSWORD = 'sentry'\n    group = GroupedMessage.objects.all()[0]\n    response = self.client.post(CreateRedmineIssue.get_url(group.pk), {\n        'subject': 'test',\n        'description': 'foo',\n    }, follow=True)\n    self.assertEquals(response.status_code, 200)\n    self.assertTemplateUsed(response, 'sentry/group/details.html')\n    self.assertTrue(RedmineIssue.objects.filter(group=group).exists())\n    group = GroupedMessage.objects.get(pk=group.pk)\n    self.assertTrue((group.data['redmine']['issue_id'] > 0))\n", "label": 0}
{"function": "\n\ndef __init__(self, stream, default_style=None, default_flow_style=None, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None, encoding=None, explicit_start=None, explicit_end=None, version=None, tags=None):\n    Emitter.__init__(self, stream, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break)\n    Serializer.__init__(self, encoding=encoding, explicit_start=explicit_start, explicit_end=explicit_end, version=version, tags=tags)\n    SafeRepresenter.__init__(self, default_style=default_style, default_flow_style=default_flow_style)\n    Resolver.__init__(self)\n", "label": 0}
{"function": "\n\ndef __init__(self, project='project', connection=None):\n    self.project = project\n    self.connection = connection\n", "label": 0}
{"function": "\n\ndef get_leaking_objects(objects=None):\n    'Return objects that do not have any referents.\\n\\n    These could indicate reference-counting bugs in C code.  Or they could\\n    be legitimate.\\n\\n    Note that the GC does not track simple objects like int or str.\\n\\n    .. versionadded:: 1.7\\n    '\n    if (objects is None):\n        gc.collect()\n        objects = gc.get_objects()\n    try:\n        ids = set((id(i) for i in objects))\n        for i in objects:\n            ids.difference_update((id(j) for j in gc.get_referents(i)))\n        return [i for i in objects if (id(i) in ids)]\n    finally:\n        del objects, i\n", "label": 0}
{"function": "\n\ndef close(self, callback=None):\n    'cleanly closes the connection to the server.\\n\\n        all pending tasks are flushed before connection shutdown'\n    if (self.status not in (status.OPENING, status.OPENED)):\n        raise AmqpStatusError('connection is not open')\n    self._close_callback = callback\n    self.status = status.CLOSING\n    for ch in self.channels.values():\n        if ((ch is not self) and (ch.status in (status.OPENING, status.OPENED))):\n            ch.close(self.close)\n    m = Close(reply_code=0, reply_text='', class_id=0, method_id=0)\n    self.send_method(m, self._close_callback)\n", "label": 0}
{"function": "\n\n@Appender(_index_shared_docs['_shallow_copy'])\ndef _shallow_copy(self, values=None, **kwargs):\n    if (values is None):\n        values = self.values\n    attributes = self._get_attributes_dict()\n    attributes.update(kwargs)\n    return self._simple_new(values, **attributes)\n", "label": 0}
{"function": "\n\ndef test_rst_render_empty(self):\n    self.create_file('empty.rst')\n    empty_rst = self.make_target(':empty_rst', target_type=Page, source='empty.rst')\n    task = self.create_task(self.context(target_roots=[empty_rst]))\n    task.execute()\n", "label": 0}
{"function": "\n\ndef get_messages_for_user(self, user):\n    '\\n        Get messages for a particular user.\\n        \\n        @param user: user object whose messages to get\\n        '\n    return self.filter(users=user)\n", "label": 0}
{"function": "\n\ndef test_simple(self):\n    xs = rules.cached_segment_enriched_tokens(self.s1)\n    self.assertTrue(isinstance(xs, list))\n    self.assertTrue(all((isinstance(x, RichToken) for x in xs)))\n", "label": 0}
{"function": "\n\ndef test_sadmin_removes_ownership(self):\n    self._create_domain('domain.tld', withtpl=True)\n    dom = Domain.objects.get(name='domain.tld')\n    self.client.logout()\n    self.client.login(username='admin', password='password')\n    self.ajax_get('{0}?domid={1}&daid={2}'.format(reverse('admin:permission_remove'), dom.id, self.user.id), {\n        \n    })\n    self._check_limit('domains', 0, 2)\n    self._check_limit('domain_admins', 0, 2)\n    self._check_limit('mailboxes', 0, 2)\n    self._check_limit('mailbox_aliases', 0, 2)\n", "label": 0}
{"function": "\n\ndef onPlayerMessage(self, payload):\n    player = self.filterName(payload['player'])\n    message = payload['message']\n    self.msgQueue.append(('<%s> %s' % (player, message)))\n", "label": 0}
{"function": "\n\n@_require_mech\ndef unwrap(self, incoming):\n    '\\n        Unwrap a message from the SASL server. Depending on the negotiated\\n        quality of protection, this may check a signature, decrypt the message,\\n        or leave the message unaltered.\\n        '\n    return self._chosen_mech.unwrap(incoming)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, label=None):\n    \"\\n        Returns the PointCloud that contains this label represents on the group.\\n        This will be a subset of the total landmark group PointCloud.\\n\\n        Parameters\\n        ----------\\n        label : `string`\\n            Label to filter on.\\n\\n        Returns\\n        -------\\n        pcloud : :map:`PointCloud`\\n            The PointCloud that this label represents. Will be a subset of the\\n            entire group's landmarks.\\n        \"\n    if (label is None):\n        return self.lms.copy()\n    return self._pointcloud.from_mask(self._labels_to_masks[label])\n", "label": 0}
{"function": "\n\ndef test_run_call_order__error_in_test(self):\n    events = []\n    result = LoggingResult(events)\n\n    def setUp():\n        events.append('setUp')\n\n    def test():\n        events.append('test')\n        raise RuntimeError('raised by test')\n\n    def tearDown():\n        events.append('tearDown')\n    expected = ['startTest', 'setUp', 'test', 'addError', 'tearDown', 'stopTest']\n    unittest2.FunctionTestCase(test, setUp, tearDown).run(result)\n    self.assertEqual(events, expected)\n", "label": 0}
{"function": "\n\ndef get_keys(self, list_names):\n    return [self.get_key(list_name) for list_name in list_names]\n", "label": 0}
{"function": "\n\ndef test_installer_unmatch(self):\n    test_unmatch = {\n        'os_installer_name': 'dummy',\n        'os_name': 'CentOS',\n    }\n    matcher = self.os_matcher.match(**test_unmatch)\n    self.assertFalse(matcher)\n", "label": 0}
{"function": "\n\ndef WithValidator(self, validator):\n    'Add a validator callback to this Measurement, chainable.'\n    if (not callable(validator)):\n        raise ValueError('Validator must be callable', validator)\n    self.validators.append(validator)\n    return self\n", "label": 0}
{"function": "\n\ndef test_client_getattr(self):\n    c = delicious.DeliciousClient('username', 'password')\n    self.assertEqual(c.username, 'username')\n    self.assertEqual(c.password, 'password')\n    self.assertEqual(c.method, 'v1')\n    c2 = c.foo.bar.baz\n    self.assertEqual(c2.username, 'username')\n    self.assertEqual(c2.password, 'password')\n    self.assertEqual(c2.method, 'v1/foo/bar/baz')\n", "label": 0}
{"function": "\n\ndef loopbackUNIX(server, client, noisy=True):\n    'Run session between server and client protocol instances over UNIX socket.'\n    path = tempfile.mktemp()\n    from twisted.internet import reactor\n    f = policies.WrappingFactory(protocol.Factory())\n    serverWrapper = _FireOnClose(f, server)\n    f.noisy = noisy\n    f.buildProtocol = (lambda addr: serverWrapper)\n    serverPort = reactor.listenUNIX(path, f)\n    clientF = LoopbackClientFactory(client)\n    clientF.noisy = noisy\n    reactor.connectUNIX(path, clientF)\n    d = clientF.deferred\n    d.addCallback((lambda x: serverWrapper.deferred))\n    d.addCallback((lambda x: serverPort.stopListening()))\n    return d\n", "label": 0}
{"function": "\n\ndef _get_container_info(self, env):\n    '\\n        Retrieves x-container-meta-web-index, x-container-meta-web-error,\\n        x-container-meta-web-listings, x-container-meta-web-listings-css,\\n        and x-container-meta-web-directory-type from memcache or from the\\n        cluster and stores the result in memcache and in self._index,\\n        self._error, self._listings, self._listings_css and self._dir_type.\\n\\n        :param env: The WSGI environment dict.\\n        :return container_info: The container_info dict.\\n        '\n    self._index = self._error = self._listings = self._listings_css = self._dir_type = None\n    container_info = get_container_info(env, self.app, swift_source='SW')\n    if is_success(container_info['status']):\n        meta = container_info.get('meta', {\n            \n        })\n        self._index = meta.get('web-index', '').strip()\n        self._error = meta.get('web-error', '').strip()\n        self._listings = meta.get('web-listings', '').strip()\n        self._listings_label = meta.get('web-listings-label', '').strip()\n        self._listings_css = meta.get('web-listings-css', '').strip()\n        self._dir_type = meta.get('web-directory-type', '').strip()\n    return container_info\n", "label": 0}
{"function": "\n\ndef look_at(self, pos):\n    delta = (Vector3(pos) - self.clientinfo.eye_pos)\n    if (delta.x or delta.z):\n        self.look_at_rel(delta)\n    else:\n        self.look(self.clientinfo.position.yaw, delta.yaw_pitch.pitch)\n", "label": 0}
{"function": "\n\ndef _to_sizes(self, object):\n    '\\n        Request a list of instance types and convert that list to a list of\\n        OpenNebulaNodeSize objects.\\n\\n        Request a list of instance types from the OpenNebula web interface,\\n        and issue a request to convert each XML object representation of an\\n        instance type to an OpenNebulaNodeSize object.\\n\\n        :return: List of instance types.\\n        :rtype:  ``list`` of :class:`OpenNebulaNodeSize`\\n        '\n    sizes = []\n    size_id = 1\n    attributes = [('name', str, None), ('ram', int, 'MEMORY'), ('cpu', float, None), ('vcpu', float, None), ('disk', str, None), ('bandwidth', float, None), ('price', float, None)]\n    for element in object.findall('INSTANCE_TYPE'):\n        element = self.connection.request(('/instance_type/%s' % element.attrib['name'])).object\n        size_kwargs = {\n            'id': size_id,\n            'driver': self,\n        }\n        values = self._get_attributes_values(attributes=attributes, element=element)\n        size_kwargs.update(values)\n        size = OpenNebulaNodeSize(**size_kwargs)\n        sizes.append(size)\n        size_id += 1\n    return sizes\n", "label": 0}
{"function": "\n\ndef test_init(self):\n    '\\n        Use git.init to init a new repo\\n        '\n    new_repo = tempfile.mkdtemp(dir=integration.TMP)\n    self.assertEqual(self.run_function('git.init', [new_repo]), 'Initialized empty Git repository in {0}/.git/'.format(new_repo))\n    shutil.rmtree(new_repo)\n", "label": 0}
{"function": "\n\ndef set_admin_password(self, ctxt, instance, new_pass):\n    cctxt = self.client.prepare(version='1.29')\n    cctxt.cast(ctxt, 'set_admin_password', instance=instance, new_pass=new_pass)\n", "label": 0}
{"function": "\n\ndef _translate_setnb(self, tb, instruction):\n    self._translate_set(tb, instruction, 'nb')\n", "label": 0}
{"function": "\n\ndef delete(self):\n    '\\n        Deletes the feed and its markers.\\n        '\n    super(BaseNotificationFeed, self).delete()\n    args = []\n    if self.track_unseen:\n        args.append('unseen')\n    if self.track_unread:\n        args.append('unread')\n    self.feed_markers.flush(*args)\n", "label": 0}
{"function": "\n\ndef check_notaction(self, iamgroup_item):\n    '\\n        alert when an IAM Group has a policy containing \\'NotAction\\'.\\n        NotAction combined with an \"Effect\": \"Allow\" often provides more privilege\\n        than is desired.\\n        '\n    self.library_check_iamobj_has_notaction(iamgroup_item, policies_key='grouppolicies')\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (self.msg != None):\n        self.reporter(((self.msg + ': ') + repr(other)))\n    else:\n        self.reporter(repr(other))\n    if (self.value is not Dumper.UNSET_VALUE):\n        return (self.value == other)\n    return True\n", "label": 0}
{"function": "\n\ndef process_precedence(option, option_str, option_value, parser, builder):\n    if (option_str == '--build'):\n        builder.allow_builds()\n    elif (option_str == '--no-build'):\n        builder.no_allow_builds()\n    elif (option_str == '--wheel'):\n        setattr(parser.values, option.dest, True)\n        builder.use_wheel()\n    elif (option_str in ('--no-wheel', '--no-use-wheel')):\n        setattr(parser.values, option.dest, False)\n        builder.no_use_wheel()\n    else:\n        raise OptionValueError\n", "label": 0}
{"function": "\n\ndef test_success(self):\n    app_id = 'app_id'\n    token = 'token'\n    response = facebook.save(app_id, token)\n    self.assertIsInstance(response, RedirectResponse)\n", "label": 0}
{"function": "\n\ndef get_remote():\n    repo_check(require_remote=True)\n    reader = repo.config_reader()\n    if (not reader.has_option('legit', 'remote')):\n        return repo.remotes[0]\n    remote_name = reader.get('legit', 'remote')\n    if (not (remote_name in [r.name for r in repo.remotes])):\n        raise ValueError('Remote \"{0}\" does not exist! Please update your git configuration.'.format(remote_name))\n    return repo.remote(remote_name)\n", "label": 0}
{"function": "\n\n@orca.column('tours')\ndef dest_density_index(tours, land_use):\n    return reindex(land_use.density_index, tours.destination)\n", "label": 0}
{"function": "\n\ndef insert_before(self, before, item):\n    ' Inserts an item into the group before the specified item.\\n\\n        Parameters\\n        ----------\\n        before : ActionManagerItem\\n            The item to insert before.\\n        item : ActionManagerItem, Action or callable\\n            The item to insert.\\n\\n        Returns\\n        -------\\n        index, item : int, ActionManagerItem\\n            The position inserted, and the item actually inserted.\\n\\n        Notes\\n        -----\\n\\n        If the item is an ActionManagerItem instance it is simply inserted.\\n        If the item is an Action instance, an ActionItem is created for the\\n        action, and that is inserted.  If the item is a callable, then an\\n        Action is created for the callable, and then that is handled as above.\\n        '\n    index = self._items.index(before)\n    self.insert(index, item)\n    return (index, item)\n", "label": 0}
{"function": "\n\ndef debug(self, text):\n    'Helper method for \"logging\" to the console.'\n    if self.PRINT_DEBUG:\n        print(('[FileHistory] ' + text))\n", "label": 0}
{"function": "\n\ndef test_parse_args_1(self):\n    test_data = {\n        'address': '192.168.1.2',\n        'user': 'admin',\n        'pswd': 'superpswd',\n        'port': '8010',\n        'type': 'foscam',\n    }\n    expeted_data = {\n        'address': '192.168.1.2:8010',\n        'user': 'admin',\n        'pswd': 'superpswd',\n        'name': None,\n    }\n    self.assertEqual(parse_args(test_data), expeted_data)\n", "label": 0}
{"function": "\n\ndef enqueue(self, item):\n    if (self.front is None):\n        new_node = QueueNode(item)\n        self.front = new_node\n        self.back = new_node\n    else:\n        new_back = QueueNode(item)\n        self.back.next = new_back\n        self.back = new_back\n    self.size += 1\n", "label": 0}
{"function": "\n\ndef _link_fields(self, field_refs, acount, progress_monitor):\n    count = 0\n    progress_monitor.start('Parsing fields', acount)\n    log = gl.LinkerLog(self, self.field_kind.kind)\n    for scode_reference in field_refs:\n        if self._reject_reference(scode_reference):\n            progress_monitor.work('Skipped reference', 1)\n            continue\n        (field_name, fqn_container) = self._get_field_name(scode_reference)\n        code_elements = []\n        code_elements.extend(self._get_field_elements(field_name, PREFIX_FIELD_LINKER, self.field_kind))\n        code_elements.extend(self._get_field_elements(field_name, PREFIX_ENUM_VAL_LINKER, self.enum_value_kind))\n        code_elements.extend(self._get_field_elements(field_name, PREFIX_ANN_FIELD_LINKER, self.ann_field_kind))\n        (code_element, potentials) = self.get_code_element(scode_reference, code_elements, field_name, fqn_container, log)\n        count += gl.save_link(scode_reference, code_element, potentials, self)\n        if (not log.custom_filtered):\n            reclassify_java(code_element, scode_reference)\n        progress_monitor.work('Processed field', 1)\n    log.close()\n    progress_monitor.done()\n    print('Associated {0} fields'.format(count))\n", "label": 0}
{"function": "\n\ndef _check_276(self, engine, data):\n    self.assertColumnExists(engine, 'instance_extra', 'vcpu_model')\n    self.assertColumnExists(engine, 'shadow_instance_extra', 'vcpu_model')\n    instance_extra = oslodbutils.get_table(engine, 'instance_extra')\n    shadow_instance_extra = oslodbutils.get_table(engine, 'shadow_instance_extra')\n    self.assertIsInstance(instance_extra.c.vcpu_model.type, sqlalchemy.types.Text)\n    self.assertIsInstance(shadow_instance_extra.c.vcpu_model.type, sqlalchemy.types.Text)\n", "label": 0}
{"function": "\n\ndef test_valid_lifecycle(self):\n    bucket_uri = self.CreateBucket()\n    fpath = self.CreateTempFile(contents=self.lifecycle_doc)\n    self.RunGsUtil(['lifecycle', 'set', fpath, suri(bucket_uri)])\n    stdout = self.RunGsUtil(['lifecycle', 'get', suri(bucket_uri)], return_stdout=True)\n    self.assertEqual(json.loads(stdout), self.lifecycle_json_obj)\n", "label": 0}
{"function": "\n\ndef addFailure(self, offset, line):\n    \"A 'failure:' directive has been read.\"\n    self._outcome(offset, line, self._failure, self.parser._reading_failure_details)\n", "label": 0}
{"function": "\n\ndef __init__(self, id, schedule, host, database, username, password, select_query, insert_query, table, depends_on=None):\n    'Constructor for the SqlNode class\\n\\n        Args:\\n            id(str): id of the object\\n            schedule(Schedule): pipeline schedule\\n            database(str): database name on the RDS host\\n            sql(str): sql to be executed\\n            table(str): table to be read\\n        '\n    if (not isinstance(schedule, Schedule)):\n        raise ETLInputError('Input schedule must be of the type Schedule')\n    if (not depends_on):\n        depends_on = list()\n    kwargs = {\n        'id': id,\n        'type': 'SqlDataNode',\n        'schedule': schedule,\n        'database': database,\n        'selectQuery': select_query,\n        'insertQuery': insert_query,\n        'table': table,\n        'dependsOn': depends_on,\n    }\n    super(PostgresNode, self).__init__(**kwargs)\n", "label": 0}
{"function": "\n\ndef destroy_node(self, node):\n    domain = self._get_domain_for_node(node=node)\n    return (domain.destroy() == 0)\n", "label": 0}
{"function": "\n\ndef get_parser(self, prog_name):\n    parser = super(UpdateNode, self).get_parser(prog_name)\n    parser.add_argument('--name', metavar='<name>', help=_('New name for the node'))\n    parser.add_argument('--profile', metavar='<profile-id>', help=_('ID of new profile to use'))\n    parser.add_argument('--role', metavar='<role>', help=_('Role for this node in the specific cluster'))\n    parser.add_argument('--metadata', metavar='<key1=value1;key2=value2...>', help=_('Metadata values to be attached to the node. Metadata can be specified multiple times, or once with key-value pairs separated by a semicolon'), action='append')\n    parser.add_argument('node', metavar='<node>', help=_('Name or ID of node to update'))\n    return parser\n", "label": 0}
{"function": "\n\ndef get_config(self):\n    config = {\n        'target_shape': self.target_shape,\n    }\n    base_config = super(Reshape, self).get_config()\n    return dict((list(base_config.items()) + list(config.items())))\n", "label": 0}
{"function": "\n\n@web.asynchronous\n@gen.engine\ndef delete(self, *args, **kwargs):\n    'Handle delete of an item\\n\\n        :param args:\\n        :param kwargs:\\n\\n        '\n    self.model = self.get_model(kwargs.get('id'))\n    result = (yield self.model.fetch())\n    if (not result):\n        self.not_found()\n        return\n    if (not self.has_delete_permission()):\n        self.permission_denied()\n        return\n    self.model.delete()\n    self.set_status(204)\n    self.finish()\n", "label": 0}
{"function": "\n\ndef test_container_create_fail(self):\n    CREATE_CONTAINER_FAIL = copy.deepcopy(CREATE_CONTAINER)\n    CREATE_CONTAINER_FAIL['wrong_key'] = 'wrong'\n    self.assertRaisesRegexp(exceptions.InvalidAttribute, ('Key must be in %s' % ','.join(containers.CREATION_ATTRIBUTES)), self.mgr.create, **CREATE_CONTAINER_FAIL)\n    self.assertEqual([], self.api.calls)\n", "label": 0}
{"function": "\n\ndef test_switch_to_new_window(self):\n    page = AlertPage()\n    handler = WindowsHandler()\n    page.open()\n    parent = handler.active_window\n    handler.save_window_set()\n    assert_false(handler.is_new_window_present())\n    page.open_new_window_link.click()\n    ok_(handler.is_new_window_present())\n    new = handler.new_window\n    handler.switch_to_new_window()\n    eq_(new, handler.active_window)\n    handler.drop_active_window()\n    eq_(parent, handler.active_window)\n", "label": 0}
{"function": "\n\ndef __init__(self, max_pool_size=MAX_POOL_SIZE):\n    'Constructor.\\n\\n    Args:\\n      max_pool_size: maximum pools size in bytes before flushing it to db.\\n    '\n    self.max_pool_size = max_pool_size\n    self.puts = ItemList()\n    self.deletes = ItemList()\n", "label": 0}
{"function": "\n\n@classmethod\ndef load_from_config(cls, config):\n    if ('env' in config):\n        config['env'] = parse_env_dict(config['env'])\n    cfg = config.copy()\n    w = cls(name=config.pop('name'), cmd=config.pop('cmd'), **config)\n    w._cfg = cfg\n    return w\n", "label": 0}
{"function": "\n\ndef test_constructor_headerlist_with_add_header(self):\n    response = Response(headers={\n        'key1': 'value1',\n    })\n    expected_content_type = ('key1', 'value1')\n    self.assertIn(expected_content_type, response.headerlist)\n", "label": 0}
{"function": "\n\ndef get_member(self, client):\n    matches = tuple(filter((lambda x: (x.client == client)), self.members))\n    return ((len(matches) > 0) and matches[0])\n", "label": 0}
{"function": "\n\ndef translate(self, x, y):\n    self.matrix[2] += ((self.matrix[0] * x) + (self.matrix[1] * y))\n    self.matrix[5] += ((self.matrix[3] * x) + (self.matrix[4] * y))\n", "label": 0}
{"function": "\n\n@decorators.json_view\n@account_decorators.login_required\ndef user_geoip(request):\n    'AJAX method\\n    Retrieves the estimated location of a given user using the IP address of\\n    the request.\\n    :param request: The GET HTTP request.\\n    :return: JSON object, { latitude: $lat, longitude: $lng }\\n    '\n    (lat, lng) = gis.get_remote_user_location(ip=ipware_get_ip(request))\n    return {\n        'latitude': lat,\n        'longitude': lng,\n    }\n", "label": 0}
{"function": "\n\ndef setUp(self, core_plugin=None, dm_plugin=None, ext_mgr=None):\n    super(HostingDeviceConfigAgentNotifierTestCase, self).setUp(core_plugin, dm_plugin, ext_mgr)\n    fake_notifier.reset()\n", "label": 0}
{"function": "\n\ndef __init__(self, type=None, keys=None, count=None, dynamic_count=None):\n    self.type = type\n    self.keys = keys\n    self.count = count\n    self.dynamic_count = dynamic_count\n", "label": 0}
{"function": "\n\ndef dialogSetItems(self, items):\n    '\\n        dialogSetItems(\\n          JSONArray items)\\n\\n        Set alert dialog list items.\\n        '\n    return self._rpc('dialogSetItems', items)\n", "label": 0}
{"function": "\n\n@delete.setter\ndef delete(self, value):\n    self['_delete'] = value\n", "label": 0}
{"function": "\n\ndef ex_remove_tag_key(self, tag_key):\n    '\\n        Modify a specific tag key\\n\\n        :param tag_key: The tag key you want to remove (required)\\n        :type  tag_key: :class:`DimensionDataTagKey` or ``str``\\n\\n        :rtype: ``bool``\\n        '\n    tag_key_id = self._tag_key_to_tag_key_id(tag_key)\n    remove_tag_key = ET.Element('deleteTagKey', {\n        'xmlns': TYPES_URN,\n        'id': tag_key_id,\n    })\n    response = self.connection.request_with_orgId_api_2('tag/deleteTagKey', method='POST', data=ET.tostring(remove_tag_key)).object\n    response_code = findtext(response, 'responseCode', TYPES_URN)\n    return (response_code in ['IN_PROGRESS', 'OK'])\n", "label": 0}
{"function": "\n\ndef participants(self, conference_sid):\n    '\\n        Return a :class:`~twilio.rest.resources.Participants` instance for the\\n        :class:`~twilio.rest.resources.Conference` with given conference_sid\\n        '\n    base_uri = ('%s/Conferences/%s' % (self.account_uri, conference_sid))\n    return Participants(base_uri, self.auth, self.timeout)\n", "label": 0}
{"function": "\n\ndef test_get_ongoing_events(self):\n    'Test the events manager can find all events overlapping today.\\n\\n        Include events that (according to the timestamp) are not ongoing,\\n        but which started or finished today.\\n        '\n    ongoing_events = Event.objects.ongoing_events()\n    event_slugs = [e.slug for e in ongoing_events]\n    correct_slugs = ['ends-tomorrow-ongoing', 'ends-today-ongoing']\n    self.assertCountEqual(event_slugs, correct_slugs)\n", "label": 0}
{"function": "\n\ndef test_conversation_report(self):\n    argv = ['conversation', 'report', 'ue90']\n    _cli = self.cli(argv)\n    parser_args = _cli.parser.parse_args(argv)\n    self.assertParser(_cli, parser_args, argv)\n    self.assertTrue(_cli.client.report_sender.called)\n", "label": 0}
{"function": "\n\ndef __init__(self, config):\n    self.keytree = Key(None)\n    self.commandmap = {\n        \n    }\n    self.multikeys = ''\n    self.update(DEFAULT_KEYMAP)\n    self.update(config)\n", "label": 0}
{"function": "\n\ndef closeEvent(self, event):\n    self.mainFrame.browser.CloseBrowser()\n", "label": 0}
{"function": "\n\ndef _is_not_a_sequence(self, value):\n    return (not isinstance(value, collections.Sequence))\n", "label": 0}
{"function": "\n\ndef post(self, request, *args, **kwargs):\n    user = self.get_object()\n    user.send_reset_password_email()\n    return Response({\n        \n    }, status=status.HTTP_200_OK)\n", "label": 0}
{"function": "\n\ndef test_clone(self):\n    '\\n        Test cloning an existing repo\\n        '\n    clone_parent_dir = tempfile.mkdtemp(dir=integration.TMP)\n    self.assertTrue(self.run_function('git.clone', [clone_parent_dir, self.repo]))\n    shutil.rmtree(clone_parent_dir)\n", "label": 0}
{"function": "\n\ndef render(self, name, value, attrs=None):\n    return mark_safe(('%s' % value))\n", "label": 0}
{"function": "\n\ndef _create_interfaces_from_meta(self, node):\n    for interface in node.meta['interfaces']:\n        interface = NodeNICInterface(mac=interface.get('mac'), name=interface.get('name'), ip_addr=interface.get('ip'), netmask=interface.get('netmask'))\n        self.db.add(interface)\n        node.nic_interfaces.append(interface)\n    self.db.flush()\n    if node.cluster_id:\n        self.network_manager.assign_networks_by_default(node)\n    if (node.nic_interfaces and (not filter((lambda i: (node.mac == i.mac)), node.nic_interfaces))):\n        node.nic_interfaces[0].mac = node.mac\n    self.db.commit()\n", "label": 0}
{"function": "\n\ndef __new__(cls, name, bases, args):\n\n    def gen_lex():\n\n        def test(self):\n            tokens = list(lexical(self.asm))\n            self.assertEquals(len(tokens), len(self.lex))\n            for (i, l) in enumerate(self.lex):\n                self.assertEquals(l[0], tokens[i]['type'])\n                self.assertEquals(l[1], tokens[i]['value'])\n        return test\n\n    def gen_syn():\n\n        def test(self):\n            tokens = [{\n                'type': l[0],\n                'value': l[1],\n            } for l in self.lex]\n            ast = syntax(tokens)\n            self.assertEquals(1, len(self.syn))\n        return test\n\n    def gen_sem():\n\n        def test(self):\n            tokens = [{\n                'type': l[0],\n                'value': l[1],\n            } for l in self.lex]\n            ast = [{\n                'type': self.syn[0],\n                'children': tokens,\n            }]\n            compiled = semantic(ast)\n            self.assertEquals(compiled, self.code)\n        return test\n    args['test_lexical'] = gen_lex()\n    args['test_syntax'] = gen_syn()\n    args['test__semantic'] = gen_sem()\n    return type.__new__(cls, name, bases, args)\n", "label": 0}
{"function": "\n\ndef close(self, streamgroup):\n    if isinstance(streamgroup, IMLiveStreamGroup):\n        server_sg = streamgroup._g\n        ns = self._store[server_sg.namespace]\n        del ns[server_sg.name]\n    return defer.succeed(None)\n", "label": 0}
{"function": "\n\ndef strptime(string, format='%a %b %d %H:%M:%S %Y', to_datetime=False):\n    date = TimeParser(format).match(string)\n    result = datetime.date(date[0], date[1], date[2])\n    if (to_datetime and (len(date) > 3)):\n        time = datetime.time(date[3], date[4], date[5])\n        result = datetime.datetime.combine(result, time)\n        result = result.replace(tzinfo=None)\n    return result\n", "label": 0}
{"function": "\n\ndef test_nodummy_dydxmean(self):\n    me = self.res1.get_margeff(at='mean')\n    assert_almost_equal(me.margeff, self.res2.margeff_nodummy_dydxmean, DECIMAL_4)\n    assert_almost_equal(me.margeff_se, self.res2.margeff_nodummy_dydxmean_se, DECIMAL_4)\n", "label": 0}
{"function": "\n\ndef trigger(self, event, *args, **kw):\n    'trigger all functions from an event'\n    if ((not (event in self.events)) or (not self.events[event])):\n        return False\n    list(map((lambda x: x(*args, **kw)), self.events[event]))\n    return True\n", "label": 0}
{"function": "\n\ndef update(self, testable_resource_id, patch):\n    return self._update(resource_id=testable_resource_id, patch=patch)\n", "label": 0}
{"function": "\n\ndef test_default_field_with_queryset(self):\n    qs = mock.NonCallableMock(spec=[])\n    f = ModelChoiceFilter(queryset=qs)\n    field = f.field\n    self.assertIsInstance(field, forms.ModelChoiceField)\n    self.assertEqual(field.queryset, qs)\n", "label": 0}
{"function": "\n\ndef create_pcap():\n    '\\n    Create a capture file from the test fixtures.\\n    '\n    tfile = tempfile.NamedTemporaryFile()\n    if (sys.version_info[0] >= 3):\n        capture = pickle.loads(base64.b64decode(fixture.TESTPCAP3))\n    else:\n        capture = pickle.loads(fixture.TESTPCAP2.decode('base64'))\n    open(tfile.name, 'wb').write(capture)\n    return tfile\n", "label": 0}
{"function": "\n\ndef delete_attachment(self, attachment_id=''):\n    try:\n        self.rr_store.delete_attachment(attachment_id, attachment_name=self.DEFAULT_ATTACHMENT_NAME)\n    finally:\n        return self.delete(attachment_id, del_associations=True)\n", "label": 0}
{"function": "\n\ndef __init__(self, command):\n    super(NixCompletion, self).__init__(command)\n", "label": 0}
{"function": "\n\ndef test_region_show(self):\n    region_id = self._create_dummy_region()\n    raw_output = self.openstack(('region show %s' % region_id))\n    region = self.parse_show_as_object(raw_output)\n    self.assertEqual(region_id, region['region'])\n    self.assertEqual('None', region['parent_region'])\n", "label": 0}
{"function": "\n\ndef get_indexer_for(self, target, **kwargs):\n    ' guaranteed return of an indexer even when non-unique '\n    if self.is_unique:\n        return self.get_indexer(target, **kwargs)\n    (indexer, _) = self.get_indexer_non_unique(target, **kwargs)\n    return indexer\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Tangible()\n    result.template = 'object/tangible/furniture/elegant/shared_end_table_s01.iff'\n    result.attribute_template_id = 6\n    result.stfName('frn_n', 'frn_end_table')\n    return result\n", "label": 0}
{"function": "\n\ndef _splitSpec(spec):\n    ' \\n        Takes an input specification, and returns a (path, pattern) tuple.\\n\\n        The splitting works as follows:\\n            - First, the spec is split on \".\".\\n            - We then try to maximally match as much as possible of the\\n              start of the spec to an existing file or directory.\\n            - The remainder is considered the mark pattern.\\n\\n        If no path is found, the first element of the return tuple is the empty\\n        string.\\n    '\n    parts = spec.split('.')\n    dirOffset = 0\n    fileOffset = 0\n    for i in range(1, (len(parts) + 1)):\n        if os.path.isdir('.'.join(parts[:i])):\n            dirOffset = i\n        elif os.path.isfile(('.'.join(parts[:i]) + '.py')):\n            fileOffset = i\n    if (dirOffset > fileOffset):\n        target = '.'.join(parts[:dirOffset])\n        pattern = '.'.join(parts[dirOffset:])\n    elif fileOffset:\n        target = ('.'.join(parts[:fileOffset]) + '.py')\n        pattern = '.'.join(parts[fileOffset:])\n    else:\n        target = ''\n        pattern = '.'.join(parts)\n    if (target and (pattern == 'py')):\n        pattern = ''\n    return (target, pattern)\n", "label": 0}
{"function": "\n\ndef process(conn, coro=None):\n    global n\n    if (sys.version_info.major >= 3):\n        eol = ord('/')\n    else:\n        eol = '/'\n    data = ''.encode()\n    while True:\n        data += (yield conn.recv(128))\n        if (data[(- 1)] == eol):\n            break\n    conn.close()\n    n += 1\n    print(('recieved: %s' % data))\n", "label": 0}
{"function": "\n\ndef test_signed_item_is_verifiable(self):\n    '\\n        Check that the resulting item is able to be verified.\\n        '\n    item = {\n        'foo': 'bar',\n        'baz': [1, 2, 3],\n    }\n    signed_item = get_signed_item(item, PUBLIC_KEY, PRIVATE_KEY)\n    self.assertTrue(verify_item(signed_item))\n", "label": 0}
{"function": "\n\n@classmethod\ndef initialize(cls, output_path, authority, username, begin=None):\n    '\\n        Generate a certificate signed by the supplied root certificate.\\n\\n        :param FilePath output_path: Directory where the certificate will be\\n            written.\\n        :param CertificateAuthority authority: The certificate authority with\\n            which this certificate will be signed.\\n        :param unicode username: A UTF-8 encoded username to be included in\\n            the certificate.\\n        :param datetime begin: The datetime from which the generated\\n            certificate should be valid.\\n        '\n    key_filename = (username + '.key')\n    cert_filename = (username + '.crt')\n    name = ('user-' + username)\n    organizational_unit = authority.organizational_unit\n    dn = DistinguishedName(commonName=name, organizationalUnitName=organizational_unit)\n    keypair = flocker_keypair()\n    request = keypair.keypair.requestObject(dn)\n    serial = os.urandom(16).encode(b'hex')\n    serial = int(serial, 16)\n    cert = sign_certificate_request(authority.credential.keypair.keypair, authority.credential.certificate.original.get_subject(), request, serial, EXPIRY_20_YEARS, b'sha256', start=begin, additional_extensions=[crypto.X509Extension(b'extendedKeyUsage', False, b'clientAuth')])\n    credential = FlockerCredential(path=output_path, keypair=keypair, certificate=cert)\n    credential.write_credential_files(key_filename, cert_filename)\n    instance = cls(credential=credential, username=username)\n    return instance\n", "label": 0}
{"function": "\n\ndef read_simple_binding(jboss_config, binding_name, profile=None):\n    '\\n    Read jndi binding in the running jboss instance\\n\\n    jboss_config\\n        Configuration dictionary with properties specified above.\\n    binding_name\\n        Binding name to be created\\n    profile\\n        The profile name (JBoss domain mode only)\\n\\n    CLI Example:\\n\\n        .. code-block:: bash\\n\\n        salt \\'*\\' jboss7.read_simple_binding \\'{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}\\' my_binding_name\\n       '\n    log.debug('======================== MODULE FUNCTION: jboss7.read_simple_binding, %s', binding_name)\n    return __read_simple_binding(jboss_config, binding_name, profile=profile)\n", "label": 0}
{"function": "\n\ndef set(name, value):\n    'To set a vim variable to a given value.'\n    if isinstance(value, basestring):\n        val = \"'{0}'\".format(value)\n    elif isinstance(value, bool):\n        val = '{0}'.format((1 if value else 0))\n    else:\n        val = value\n    vim.command('let {0} = {1}'.format((prefix + name), val))\n", "label": 0}
{"function": "\n\ndef filter(self, *data):\n    lens = [len(seq) for seq in data[0]]\n    if (self.percentile > 0):\n        max_len = np.percentile(lens, self.percentile)\n        max_len = np.clip(max_len, self.min_max_len, self.max_len)\n    else:\n        max_len = self.max_len\n    valid_idxs = [i for (i, l) in enumerate(lens) if (l <= max_len)]\n    if (len(data) == 1):\n        return list_index(data[0], valid_idxs)\n    else:\n        return tuple([list_index(d, valid_idxs) for d in data])\n", "label": 0}
{"function": "\n\ndef __init__(self, auth=None, client_id=None, email_address=None):\n    self.client_id = client_id\n    self.email_address = email_address\n    super(Person, self).__init__(auth)\n", "label": 0}
{"function": "\n\ndef setup_sahara_engine():\n    periodic.setup()\n    engine = _get_infrastructure_engine()\n    service_ops.setup_ops(engine)\n    remote_driver = _get_remote_driver()\n    remote.setup_remote(remote_driver, engine)\n", "label": 0}
{"function": "\n\n@utils.positional(1)\ndef __new__(cls, kind, properties, ancestor):\n    'Constructor.'\n    obj = object.__new__(cls)\n    obj.__kind = kind\n    obj.__properties = properties\n    obj.__ancestor = ancestor\n    return obj\n", "label": 0}
{"function": "\n\ndef _saveVideo(self, videoFields):\n    params = (videoFields['source_id'],)\n    self._testCase.assertIn(params, self._saveVideoReturn)\n    self.saveVideoLog.append(videoFields)\n    return self._saveVideoReturn[params]\n", "label": 0}
{"function": "\n\ndef get_db_prep_lookup(self, value, connection):\n    if isinstance(value, (tuple, list)):\n        params = ([connection.ops.Adapter(value[0])] + list(value)[1:])\n    else:\n        params = [connection.ops.Adapter(value)]\n    return ('%s', params)\n", "label": 0}
{"function": "\n\n@base.remotable_classmethod\ndef get_by_host(cls, context, host, expected_attrs=None, use_slave=False):\n    db_inst_list = cls._db_instance_get_all_by_host(context, host, columns_to_join=_expected_cols(expected_attrs), use_slave=use_slave)\n    return _make_instance_list(context, cls(), db_inst_list, expected_attrs)\n", "label": 0}
{"function": "\n\ndef addPackage(self, name, version='*'):\n    self.packageList.addPackage(name, version)\n", "label": 0}
{"function": "\n\n@dispatch(Expr)\ndef _print_python(expr, leaves=None):\n    raise NotImplementedError(('Do not know how to write expressions of type %s to Python code' % type(expr).__name__))\n", "label": 0}
{"function": "\n\n@transform(countLcaTaxa, suffix('.count'), '.count.load')\ndef loadCountLcaTaxa(infile, outfile):\n    '\\n    load taxa level counts\\n    '\n    P.load(infile, outfile)\n", "label": 0}
{"function": "\n\ndef quote_name_unless_alias(self, name):\n    \"\\n        A wrapper around connection.ops.quote_name that doesn't quote aliases\\n        for table names. This avoids problems with some SQL dialects that treat\\n        quoted strings specially (e.g. PostgreSQL).\\n        \"\n    if (name in self.quote_cache):\n        return self.quote_cache[name]\n    if (((name in self.query.alias_map) and (name not in self.query.table_map)) or (name in self.query.extra_select)):\n        self.quote_cache[name] = name\n        return name\n    r = self.connection.ops.quote_name(name)\n    self.quote_cache[name] = r\n    return r\n", "label": 0}
{"function": "\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\n    shutil.rmtree(self.dirpath)\n    return None\n", "label": 0}
{"function": "\n\ndef get_url_decorator(self, pattern):\n    'Fixes bug in Application.get_url_decorator wich would decorate None values.'\n    permissions = self.get_permissions(pattern.name)\n    if (permissions is not None):\n        return permissions_required(permissions)\n    return None\n", "label": 0}
{"function": "\n\ndef headers_received(self, start_line, headers):\n    self.set_request(httputil.HTTPServerRequest(connection=self.connection, start_line=start_line, headers=headers))\n    if self.stream_request_body:\n        self.request.body = Future()\n        return self.execute()\n", "label": 0}
{"function": "\n\n@base.resource(acc.Access)\ndef access(self, access_id):\n    '\\n        Return the resource corresponding to a single access.\\n        '\n    return acc.Access(self, access_id)\n", "label": 0}
{"function": "\n\ndef test_index_ints():\n    A = npr.randn(5, 6, 4)\n\n    def fun(x):\n        return to_scalar(x[(3, 0, 1)])\n    d_fun = (lambda x: to_scalar(grad(fun)(x)))\n    check_grads(fun, A)\n    check_grads(d_fun, A)\n", "label": 0}
{"function": "\n\ndef test_wait_timed_out_with_capacity(self):\n    inst = instance.Instance(self.request_data, 'name', self.proxy, max_concurrent_requests=1)\n    inst._started = True\n    self.mox.StubOutWithMock(inst._condition, 'wait')\n    self.mox.ReplayAll()\n    self.assertTrue(inst.wait(0))\n    self.mox.VerifyAll()\n", "label": 0}
{"function": "\n\ndef __setattr__(self, name, value):\n    setattr(self.value, name, value)\n", "label": 0}
{"function": "\n\ndef add_link(self, rel, href, **kwargs):\n    self.builder.add_link(rel, self.make_target(href), **kwargs)\n", "label": 0}
{"function": "\n\ndef test_without_support_without_session(self):\n    message = ConnectMessage(self.version)\n    self.assertEqual(message.version, self.version)\n    self.assertEqual(message.support, [self.version])\n    self.assertFalse(message.has_session())\n    self.assertIsNone(message.session)\n", "label": 0}
{"function": "\n\ndef test_post_hanlder(self):\n\n    def post_hanlder(emulator, instruction, parameter):\n        paramter.append(True)\n    asm = ['mov eax, ebx']\n    x86_instrs = map(self._asm_parser.parse, asm)\n    self.__set_address(3735928559, x86_instrs)\n    reil_instrs = map(self._translator.translate, x86_instrs)\n    paramter = []\n    self._emulator.set_instruction_post_handler(post_hanlder, paramter)\n    (reil_ctx_out, reil_mem_out) = self._emulator.execute_lite(reil_instrs[0])\n    self.assertTrue((len(paramter) > 0))\n", "label": 0}
{"function": "\n\ndef addGetSpeech(self, **kwargs):\n    return self.append(GetSpeech(**kwargs))\n", "label": 0}
{"function": "\n\ndef _post_update(self, existing_obj):\n    existing_obj.code = (self.code if existing_obj.approved else None)\n", "label": 0}
{"function": "\n\ndef _check_301(self, engine, data):\n    self.assertColumnExists(engine, 'compute_nodes', 'cpu_allocation_ratio')\n    self.assertColumnExists(engine, 'compute_nodes', 'ram_allocation_ratio')\n", "label": 0}
{"function": "\n\ndef test_basic_usage(self):\n    cmd = commands.Command()\n    self.assertEqual('usage: foo', cmd.format_usage('foo').strip())\n    cmd.add_argument('-h', '--help', action='store_true')\n    self.assertEqual('usage: foo [-h]', cmd.format_usage('foo').strip())\n    cmd.add_argument('bar')\n    self.assertEqual('usage: foo [-h] bar', cmd.format_usage('foo').strip())\n", "label": 0}
{"function": "\n\ndef test_inheritance(self):\n    self.assertTrue(issubclass(self.app.json_encoder, DummyEncoder))\n    json_encoder_name = self.app.json_encoder.__name__\n    self.assertEqual(json_encoder_name, 'MongoEngineJSONEncoder')\n", "label": 0}
{"function": "\n\ndef __init__(self, seeds=1, max_feval=50000.0, max_iter=100000.0):\n    self.seeds = seeds\n    self.max_feval = max_feval\n    self.max_iter = max_iter\n    self.model = None\n    self.n_dims = None\n    self.kernel = None\n    self._kerns = None\n    self._kernf = None\n    self._kernb = None\n", "label": 0}
{"function": "\n\ndef star_output_mode(cluster, logdir, cmdline, *args):\n    'Select output mode: [stream|ordered|off]'\n    modes = ('stream', 'ordered', 'off')\n    if (args[0] not in modes):\n        raise ValueError(('Output mode must be one of: %s' % repr(modes)))\n    cluster.output_mode = args[0]\n    print(('Output mode set to %s' % cluster.output_mode))\n", "label": 0}
{"function": "\n\ndef create_comment(self, revision, message, silent=False):\n    \"Make a comment on the specified 'revision'.\\n\\n        :revision: id of the revision to comment on\\n        :message: the string message to leave as a comment, may be empty\\n        :silent: mail notifications won't be sent if False\\n        :returns: None\\n\\n        \"\n    _ = silent\n    self._data.assert_is_revision(revision)\n    str(message)\n    self._data.set_changed()\n", "label": 0}
{"function": "\n\ndef match_type(self, event_type):\n    return (self._included_type(event_type) and (not self._excluded_type(event_type)))\n", "label": 0}
{"function": "\n\ndef test_leq():\n    calculated = leq(levels)\n    real = 79.806989166\n    assert_almost_equal(calculated, real)\n", "label": 0}
{"function": "\n\ndef test_set_get_name(self):\n    '\\n        Application name accessors behave properly\\n        '\n    application = Application()\n    application.set_name('foo')\n    self.assertEqual('foo', application.get_name(), msg='.set_name() sets the name of the application')\n", "label": 0}
{"function": "\n\ndef _prepare_toplevel_for_item(self, layout):\n    ' Returns a sized toplevel QDockWidget for a LayoutItem.\\n        '\n    if isinstance(layout, PaneItem):\n        dock_widget = self._get_dock_widget(layout)\n        if (dock_widget is None):\n            logger.warning(('Cannot retrieve dock widget for pane %r' % layout.id))\n        else:\n            if (layout.width > 0):\n                dock_widget.widget().setFixedWidth(layout.width)\n            if (layout.height > 0):\n                dock_widget.widget().setFixedHeight(layout.height)\n        return dock_widget\n    elif isinstance(layout, LayoutContainer):\n        return self._prepare_toplevel_for_item(layout.items[0])\n    else:\n        raise MainWindowLayoutError('Leaves of layout must be PaneItems')\n", "label": 0}
{"function": "\n\ndef clone(self, deep=True):\n    \"\\n        Return a clone of this tag. If deep is True, clone all of this tag's\\n        children. Otherwise, just shallow copy the children list without copying\\n        the children themselves.\\n        \"\n    if deep:\n        newchildren = [self._clone(x, True) for x in self.children]\n    else:\n        newchildren = self.children[:]\n    newattrs = self.attributes.copy()\n    for key in newattrs:\n        newattrs[key] = self._clone(newattrs[key], True)\n    newslotdata = None\n    if self.slotData:\n        newslotdata = self.slotData.copy()\n        for key in newslotdata:\n            newslotdata[key] = self._clone(newslotdata[key], True)\n    newtag = Tag(self.tagName, attributes=newattrs, children=newchildren, render=self.render, filename=self.filename, lineNumber=self.lineNumber, columnNumber=self.columnNumber)\n    newtag.slotData = newslotdata\n    return newtag\n", "label": 0}
{"function": "\n\ndef initialize_options(self, *args, **kwargs):\n    install.initialize_options(self, *args, **kwargs)\n    self.corpora_zip_url = None\n", "label": 0}
{"function": "\n\ndef test_servers_rebuild(self):\n    subs = {\n        'image_id': fake.get_valid_image_id(),\n        'host': self._get_host(),\n        'glance_host': self._get_glance_host(),\n        'access_ip_v4': '1.2.3.4',\n        'access_ip_v6': '80fe::',\n    }\n    uuid = self._post_server(use_common_server_api_samples=False)\n    response = self._do_post(('servers/%s/action' % uuid), 'server-action-rebuild-req', subs)\n    subs['hostid'] = '[a-f0-9]+'\n    subs['id'] = uuid\n    self._verify_response('server-action-rebuild-resp', subs, response, 202)\n", "label": 0}
{"function": "\n\ndef test_contributor_cohort_analysis(self):\n    c1 = Cohort.objects.get(kind__code=CONTRIBUTOR_COHORT_CODE, start=self.start_of_first_week)\n    eq_(c1.size, 10)\n    c1r1 = c1.retention_metrics.get(start=(self.start_of_first_week + timedelta(weeks=1)))\n    eq_(c1r1.size, 2)\n    c1r2 = c1.retention_metrics.get(start=(self.start_of_first_week + timedelta(weeks=2)))\n    eq_(c1r2.size, 5)\n    c2 = Cohort.objects.get(kind__code=CONTRIBUTOR_COHORT_CODE, start=(self.start_of_first_week + timedelta(weeks=1)))\n    eq_(c2.size, 8)\n    c2r1 = c2.retention_metrics.get(start=(self.start_of_first_week + timedelta(weeks=2)))\n    eq_(c2r1.size, 2)\n", "label": 0}
{"function": "\n\ndef line(self, source, x, y, agg=None):\n    'Compute a reduction by pixel, mapping data to pixels as a line.\\n\\n        For aggregates that take in extra fields, the interpolated bins will\\n        receive the fields from the previous point. In pseudocode:\\n\\n        >>> for i in range(len(rows) - 1):    # doctest: +SKIP\\n        ...     row0 = rows[i]\\n        ...     row1 = rows[i + 1]\\n        ...     for xi, yi in interpolate(row0.x, row0.y, row1.x, row1.y):\\n        ...         add_to_aggregate(xi, yi, row0)\\n\\n        Parameters\\n        ----------\\n        source : pandas.DataFrame, dask.DataFrame\\n            The input datasource.\\n        x, y : str\\n            Column names for the x and y coordinates of each vertex.\\n        agg : Reduction, optional\\n            Reduction to compute. Default is ``any()``.\\n        '\n    from .glyphs import Line\n    from .reductions import any\n    if (agg is None):\n        agg = any()\n    return bypixel(source, self, Line(x, y), agg)\n", "label": 0}
{"function": "\n\n@wrap_exceptions\ndef get_process_uids(self):\n    (real, effective, saved) = _psutil_osx.get_process_uids(self.pid)\n    return nt_uids(real, effective, saved)\n", "label": 0}
{"function": "\n\n@receiver(post_delete, sender=MediaFile)\ndef _mediafile_post_delete(sender, instance, **kwargs):\n    instance.delete_mediafile()\n    logger.info(('Deleted mediafile %d (%s)' % (instance.id, instance.file.name)))\n", "label": 0}
{"function": "\n\ndef items(self):\n    return [option['name'] for option in self.options]\n", "label": 0}
{"function": "\n\ndef __init__(self, att, docstring=''):\n    self.__doc__ = docstring\n    self._att = att.split('.')\n", "label": 0}
{"function": "\n\ndef _prepare_verified_images(self, verify_image_url):\n    self._verified_images = self._verify_images(self._find_images(), verify_image_url)\n    print(self._verified_images)\n", "label": 0}
{"function": "\n\ndef _flow_control_change_from_settings(self, old_value, new_value):\n    '\\n        Update flow control windows in response to a change in the value of\\n        SETTINGS_INITIAL_WINDOW_SIZE.\\n\\n        When this setting is changed, it automatically updates all flow control\\n        windows by the delta in the settings values. Note that it does not\\n        increment the *connection* flow control window, per section 6.9.2 of\\n        RFC 7540.\\n        '\n    delta = (new_value - old_value)\n    for stream in self.streams.values():\n        stream.outbound_flow_control_window = guard_increment_window(stream.outbound_flow_control_window, delta)\n", "label": 0}
{"function": "\n\n@dispatch(object)\ndef _print_python(expr, leaves=None):\n    return (repr(expr), {\n        \n    })\n", "label": 0}
{"function": "\n\ndef get_touched_payload_files(payload):\n    ' Return a set of files modified in payload commits.\\n    '\n    touched = set()\n    for commit in payload['commits']:\n        for filelist in (commit['added'], commit['modified']):\n            touched.update(filelist)\n        for filename in commit['removed']:\n            if (filename in touched):\n                touched.remove(filename)\n    current_app.logger.debug('Touched files {}'.format(', '.join(touched)))\n    return touched\n", "label": 0}
{"function": "\n\ndef _set_interfaces_if_not_set_in_meta(self, node_id, meta):\n    if ((not meta) or ('interfaces' not in meta)):\n        self._add_interfaces_to_node(node_id)\n", "label": 0}
{"function": "\n\ndef _validate(self, value):\n    if (not isinstance(value, (int, long))):\n        raise datastore_errors.BadValueError(('Expected integer, got %r' % (value,)))\n    return int(value)\n", "label": 0}
{"function": "\n\ndef add_input(self, string_input):\n    \"Adds the specified string to the pending input that will be returned by calls to 'recv' on this\\n        socket.\\n\\n        Testing code can use this to simulate bytes being received.\\n\\n        @param string_input: The input.\\n        \"\n    original_position = self.__stream.tell()\n    self.__stream.seek(0, 2)\n    self.__stream.write(string_input)\n    self.__stream.seek(original_position)\n", "label": 0}
{"function": "\n\ndef is_type_compatible(self, kind):\n    return (kind == self.inferred_type)\n", "label": 0}
{"function": "\n\n@returns_clone\ndef join(self, dest, join_type=None, on=None):\n    src = self._query_ctx\n    if (not on):\n        require_join_condition = (isinstance(dest, SelectQuery) or (isclass(dest) and (not src._meta.rel_exists(dest))))\n        if require_join_condition:\n            raise ValueError('A join condition must be specified.')\n    elif isinstance(on, basestring):\n        on = src._meta.fields[on]\n    self._joins.setdefault(src, [])\n    self._joins[src].append(Join(src, dest, join_type, on))\n    if (not isinstance(dest, SelectQuery)):\n        self._query_ctx = dest\n", "label": 0}
{"function": "\n\ndef _evaluate_nz(self, tb):\n    return tb._negate_reg(self._flags['zf'])\n", "label": 0}
{"function": "\n\ndef test_01_complex(self):\n    ab = array([[0.0, 0.0, 2.0, 2.0], [(- 99), (- 1j), (- 1j), (- 1j)], [4.0, 4.0, 4.0, 4.0]])\n    b = array([(2 - 1j), (4.0 - 1j), (4 + 1j), (2 + 1j)])\n    x = solveh_banded(ab, b)\n    assert_array_almost_equal(x, [0.0, 1.0, 1.0, 0.0])\n", "label": 0}
{"function": "\n\ndef __init__(self, next_hop_type, id=None, address_prefix=None, next_hop_ip_address=None, provisioning_state=None, name=None, etag=None):\n    super(Route, self).__init__(id=id)\n    self.address_prefix = address_prefix\n    self.next_hop_type = next_hop_type\n    self.next_hop_ip_address = next_hop_ip_address\n    self.provisioning_state = provisioning_state\n    self.name = name\n    self.etag = etag\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    '\\n        Starts websocket server\\n        '\n    autodiscover()\n    mease.run_websocket_server(host=options['host'], port=options['port'], debug=options['debug'])\n", "label": 0}
{"function": "\n\ndef cloneNode(self, deep=0, parent=None):\n    return Comment(self.nodeValue, parent)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef get_model_parameter_bounds():\n    '\\n        Returns a dict where each key-value pair is a model parameter and a\\n        tuple with the lower and upper bounds\\n        '\n    inf = float('inf')\n    params = dict(mu=(0.0, (2 * pi)), rho=(0.0, inf))\n    return params\n", "label": 0}
{"function": "\n\ndef test_util2d_external_free_nomodelws():\n    model_ws = os.path.join(out_dir)\n    if os.path.exists(model_ws):\n        shutil.rmtree(model_ws)\n    os.mkdir(model_ws)\n    base_dir = os.getcwd()\n    os.chdir(out_dir)\n    ml = flopy.modflow.Modflow()\n    stress_util2d_for_joe_the_file_king(ml, 1, 1, 1)\n    stress_util2d_for_joe_the_file_king(ml, 10, 1, 1)\n    stress_util2d_for_joe_the_file_king(ml, 1, 10, 1)\n    stress_util2d_for_joe_the_file_king(ml, 1, 1, 10)\n    stress_util2d_for_joe_the_file_king(ml, 10, 10, 1)\n    stress_util2d_for_joe_the_file_king(ml, 1, 10, 10)\n    stress_util2d_for_joe_the_file_king(ml, 10, 1, 10)\n    stress_util2d_for_joe_the_file_king(ml, 10, 10, 10)\n    os.chdir(base_dir)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Tangible()\n    result.template = 'object/tangible/deed/faction_perk/hq/shared_hq_s05.iff'\n    result.attribute_template_id = 2\n    result.stfName('deed', 'hq_s05')\n    return result\n", "label": 0}
{"function": "\n\ndef test_localtime_without_arg(self):\n    lt0 = time.localtime()\n    lt1 = time.localtime(None)\n    t0 = time.mktime(lt0)\n    t1 = time.mktime(lt1)\n    self.assertAlmostEqual(t1, t0, delta=0.2)\n", "label": 0}
{"function": "\n\ndef suggest(self, history, searchspace):\n    'Randomly suggest params from searchspace.\\n        '\n    return searchspace.rvs(self.seed)\n", "label": 0}
{"function": "\n\ndef _actor_migrate(self, to_rt_uuid, callback, actor_id, requirements, extend, move, status, peer_node_id=None, uri=None):\n    ' Got link? continue actor migrate '\n    if status:\n        msg = {\n            'cmd': 'ACTOR_MIGRATE',\n            'requirements': requirements,\n            'actor_id': actor_id,\n            'extend': extend,\n            'move': move,\n        }\n        self.network.links[to_rt_uuid].send_with_reply(callback, msg)\n    elif callback:\n        callback(status=status)\n", "label": 0}
{"function": "\n\n@mock.patch('pkg_resources.iter_entry_points')\ndef test_entry_points(self, iep):\n\n    def function():\n        pass\n\n    def entry_points(group, *args):\n        (yield function)\n    iep.side_effect = entry_points\n    collector = begin.subcommands.Collector()\n    collector.load_plugins('entry.point')\n    self.assertEqual(list(collector.commands()), [function])\n", "label": 0}
{"function": "\n\ndef queue(self, *args, **kwargs):\n    return gevent.queue.Queue(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _parse_minimum_links(self, config):\n    value = 0\n    match = re.search('port-channel min-links (\\\\d+)', config)\n    if match:\n        value = int(match.group(1))\n    return dict(minimum_links=value)\n", "label": 0}
{"function": "\n\ndef test_peer_review_modal_on_response_question(self):\n    client = self.get_logged_in_client()\n    kwargs = {\n        'HTTP_X_REQUESTED_WITH': 'XMLHttpRequest',\n    }\n    response = client.post('/teacher/course/1/peer_review/1/peer_review_modal', {\n        'question_id': 1,\n        'question_type': settings.RESPONSE_QUESTION_TYPE,\n        'submission_id': 4,\n    }, **kwargs)\n    self.assertEqual(response.status_code, 200)\n    self.assertIn(b'peer_review_modal', response.content)\n", "label": 0}
{"function": "\n\ndef test_named_property_setter(self):\n    d = self.d\n    d.CompareMode = 42\n    d.Item['foo'] = 1\n    d.Item['bar'] = 'spam foo'\n    d.Item['baz'] = 3.14\n    self.assertAccessInterface(d)\n", "label": 0}
{"function": "\n\ndef prefetch(sq, *subqueries):\n    if (not subqueries):\n        return sq\n    fixed_queries = prefetch_add_subquery(sq, subqueries)\n    deps = {\n        \n    }\n    rel_map = {\n        \n    }\n    for prefetch_result in reversed(fixed_queries):\n        query_model = prefetch_result.model\n        if prefetch_result.fields:\n            for rel_model in prefetch_result.rel_models:\n                rel_map.setdefault(rel_model, [])\n                rel_map[rel_model].append(prefetch_result)\n        deps[query_model] = {\n            \n        }\n        id_map = deps[query_model]\n        has_relations = bool(rel_map.get(query_model))\n        for instance in prefetch_result.query:\n            if prefetch_result.fields:\n                prefetch_result.store_instance(instance, id_map)\n            if has_relations:\n                for rel in rel_map[query_model]:\n                    rel.populate_instance(instance, deps[rel.model])\n    return prefetch_result.query\n", "label": 1}
{"function": "\n\ndef pushlines(self, lines):\n    self._lines[:0] = lines[::(- 1)]\n", "label": 0}
{"function": "\n\ndef apply(self, query, value):\n    flt = {\n        ('%s__nin' % self.column.name): value,\n    }\n    return query.filter(**flt)\n", "label": 0}
{"function": "\n\ndef _on_invalid_digest(self, message, **kwargs):\n    self._invalid_digests += 1\n    self._write_status(message, True)\n", "label": 0}
{"function": "\n\n@property\ndef headers(self):\n    h = [DataTablesColumn(mark_safe('\\n                    Select  <a class=\"select-visible btn btn-xs btn-info\">all</a>\\n                    <a class=\"select-none btn btn-xs btn-default\">none</a>\\n                    '), sortable=False, span=3), DataTablesColumn(_('View Form'), span=2), DataTablesColumn(_('Username'), prop_name='form.meta.username', span=3), DataTablesColumn((_('Submission Time') if self.by_submission_time else _('Completion Time')), prop_name=self.time_field, span=3), DataTablesColumn(_('Form'), prop_name='form.@name')]\n    h.extend([DataTablesColumn(field) for field in self.other_fields])\n    return DataTablesHeader(*h)\n", "label": 0}
{"function": "\n\ndef setup(self, node):\n    ' Setup the the layout saver.\\n\\n        '\n    self.stack = []\n", "label": 0}
{"function": "\n\ndef test_setup(self):\n    for (p, c) in self.edges:\n        child = self.target_map[c]\n        parent = self.target_map[p]\n        self.assertTrue((parent in child.deps))\n", "label": 0}
{"function": "\n\ndef bool_value(value, default=False):\n    '\\n    Convert strings like \\'yes\\', \\'y\\', \\'true\\', \\'1\\',\\n    \\'no\\', \\'n\\', \\'false\\', \\'0\\' or \\'\\' to boolean values.\\n    :param str value: \"Value to parse to bool\"\\n    :param bool default: \"Default value if unable to convert\"\\n    :rtype: bool\\n    '\n    if (not value):\n        return default\n    true_values = ('yes', 'y', 'true', '1')\n    false_values = ('no', 'n', 'false', '0', '')\n    normalized_value = value.strip().lower()\n    if (normalized_value in true_values):\n        return True\n    elif (normalized_value in false_values):\n        return False\n    else:\n        return default\n", "label": 0}
{"function": "\n\ndef _Cmd(self, command, mode=None):\n    'CiscoXR wrapper for ParamikoDevice._Cmd().'\n    result = super(CiscoXrDevice, self)._Cmd(command, mode)\n    if result.endswith(\"% Invalid input detected at '^' marker.\\r\\n\"):\n        raise exceptions.CmdError(('Invalid input: %s' % command))\n    if result.endswith('% Bad hostname or protocol not running\\r\\n'):\n        raise exceptions.CmdError(('Bad hostname or protocol not running: %s' % command))\n    if result.endswith('% Incomplete command.\\r\\n'):\n        raise exceptions.CmdError(('Incomplete command: %s' % command))\n    return result\n", "label": 0}
{"function": "\n\ndef test_named_command_class_should_set_command_name_if_not_defined(self):\n    description = 'some description'\n\n    class SomeCommand(NamedCommand):\n\n        def __init__(self, lizy):\n            super(SomeCommand, self).__init__(lizy, description)\n\n        def _process_command(self, lizy, args):\n            return (None, None)\n    subparser_mock = (lambda : 0)\n    subparser_mock.add_parser = MagicMock()\n    no_name = SomeCommand(Pylease(None, subparser_mock, None))\n    eq_(no_name.name, 'some')\n    subparser_mock.add_parser.assert_called_once_with('some', help=description)\n", "label": 0}
{"function": "\n\ndef __init__(self, custUnit=None, builtInUnit=None, dispUnitsLbl=None, extLst=None):\n    self.custUnit = custUnit\n    self.builtInUnit = builtInUnit\n    self.dispUnitsLbl = dispUnitsLbl\n", "label": 0}
{"function": "\n\ndef set_transition_sels(view, sels):\n    'Set the updated transition selections and marks.\\n    '\n    view.add_regions('transition_sels', sels, scope=_TRANSITION_CURSOR_SCOPE_TYPE, icon=_TRANSITION_CURSOR_ICON, flags=_TRANSITION_CURSOR_FLAGS)\n", "label": 0}
{"function": "\n\ndef remove(self, obj):\n    'Removes a child given its name or object\\n\\n        If the node was added with name, it is better to remove by name, else\\n        the name will be unavailable for further adds (and will raise an \\n        Exception if add with this same name is attempted)\\n\\n        If the node was part of the active scene, its :meth:`on_exit` method \\n        will be called.\\n\\n        Arguments:\\n            obj (str or object):\\n                Name of the reference to be removed or object to be removed.\\n        '\n    if isinstance(obj, string_types):\n        if (obj in self.children_names):\n            child = self.children_names.pop(obj)\n            self._remove(child)\n        else:\n            raise Exception(('Child not found: %s' % obj))\n    else:\n        self._remove(obj)\n", "label": 0}
{"function": "\n\ndef test_get_users(self):\n    users = list(get_users())\n    assert_equal(len(users), 1)\n    assert_not_in(self.unconfirmed, users)\n    assert_equal(users, [self.user])\n", "label": 0}
{"function": "\n\ndef test_margeff_dummy_overall(self):\n    me = self.res1.get_margeff(dummy=True)\n    assert_almost_equal(me.margeff, self.res2.margeff_dummy_overall, DECIMAL_4)\n    assert_almost_equal(me.margeff_se, self.res2.margeff_dummy_overall_se, DECIMAL_4)\n", "label": 0}
{"function": "\n\ndef getEdgeOppositeHorizontally(self, pp, symbol, x, onY2):\n    xMin = pp.getXMin()\n    xMax = pp.getXMax()\n    xMid = symbol.getBaseline()\n    if (Double.NaN == xMid):\n        xMid = ((xMin + xMax) / 2.0)\n    xMinPx = pp.xToPixel(xMin)\n    xMaxPx = pp.xToPixel(xMax)\n    xMidPx = pp.xToPixel(xMid)\n    xPx = pp.xToPixel(x)\n    prevXPx = Double.NaN\n    nextXPx = Double.NaN\n    width = symbol.getWidth(pp)\n    symWidth = self.getAdjustedWidth(width, xPx, prevXPx, nextXPx, xMinPx, xMaxPx, xMidPx)\n    if (Double.NaN == symWidth):\n        return Double.NaN\n    xLeft = self.getUpperLeftX(width, xPx, prevXPx, nextXPx, xMinPx, xMaxPx, xMidPx, pp.getXMousePlotArea())\n    if (Double.NaN == xLeft):\n        return Double.NaN\n    result = (xLeft + symWidth)\n    if (abs((xLeft - xPx)) > abs((result - xPx))):\n        result = xLeft\n    return result\n", "label": 0}
{"function": "\n\ndef test_plugin_context(self):\n    glossary = {\n        'link_content': 'Knopf',\n        'button-type': 'btn-default',\n    }\n    model_instance = add_plugin(self.placeholder, BootstrapButtonPlugin, 'en', glossary=glossary)\n    button_plugin = model_instance.get_plugin_class_instance()\n    context = button_plugin.render({\n        \n    }, model_instance, None)\n    self.assertIn('instance', context)\n    self.assertIsInstance(context['instance'], LinkElementMixin)\n    self.assertListEqual(button_plugin.get_css_classes(model_instance), ['btn', 'btn-default'])\n    self.assertEqual(button_plugin.get_identifier(model_instance), 'Knopf')\n", "label": 0}
{"function": "\n\ndef test_query_table_with_sqlite3_connection(self):\n    database_file = 'sometable.db'\n    db = Database(sqlite3.connect(database_file))\n    db.open()\n    db.execute('CREATE TABLE IF NOT EXISTS SomeTable (id VARCHAR(255))')\n    db.execute('INSERT INTO SomeTable VALUES (1)')\n    db.execute('INSERT INTO SomeTable VALUES (?)', (2,))\n    db.execute('SELECT * FROM SomeTable')\n    rows = db.fetchall()\n    self.assertEqual(True, isinstance(rows, types.GeneratorType))\n    self.assertEqual(['1', '2'], [row[0] for row in rows])\n    db.rollback()\n    db.close()\n    os.remove(database_file)\n", "label": 0}
{"function": "\n\ndef remote_flow_control_window(self, stream_id):\n    '\\n        Returns the maximum amount of data the remote peer can send on stream\\n        ``stream_id``.\\n\\n        This value will never be larger than the total data that can be sent on\\n        the connection: even if the given stream allows more data, the\\n        connection window provides a logical maximum to the amount of data that\\n        can be sent.\\n\\n        The maximum data that can be sent in a single data frame on a stream\\n        is either this value, or the maximum frame size, whichever is\\n        *smaller*.\\n\\n        :param stream_id: The ID of the stream whose flow control window is\\n            being queried.\\n        :type stream_id: ``int``\\n        :returns: The amount of data in bytes that can be received on the\\n            stream before the flow control window is exhausted.\\n        :rtype: ``int``\\n        '\n    stream = self._get_stream_by_id(stream_id)\n    return min(self.inbound_flow_control_window, stream.inbound_flow_control_window)\n", "label": 0}
{"function": "\n\ndef _remove_punct(self, inStr):\n    '\\n        Function to remove punctuation from Unicode string.\\n        :param input: the input string\\n        :return: Unicode string after remove all punctuation\\n        '\n    punc_cat = set(['Pc', 'Pd', 'Ps', 'Pe', 'Pi', 'Pf', 'Po'])\n    return ''.join((x for x in inStr if (unicodedata.category(x) not in punc_cat)))\n", "label": 0}
{"function": "\n\ndef test_download_object_as_stream_escaped(self):\n    container = Container(name='foo & bar_container', extra={\n        \n    }, driver=self.driver)\n    obj = Object(name='foo & bar_object', size=1000, hash=None, extra={\n        \n    }, container=container, meta_data=None, driver=self.driver)\n    stream = self.driver.download_object_as_stream(obj=obj, chunk_size=None)\n    self.assertTrue(hasattr(stream, '__iter__'))\n", "label": 0}
{"function": "\n\ndef min(x, axis=None, keepdims=False):\n    'Minimum value in a tensor.\\n    '\n    axis = _normalize_axis(axis, ndim(x))\n    return tf.reduce_min(x, reduction_indices=axis, keep_dims=keepdims)\n", "label": 0}
{"function": "\n\ndef parseJSON(self, json_data):\n    self.json = json.loads(json_data)\n", "label": 0}
{"function": "\n\ndef testInstanceMetadataSignals(self):\n    n = GafferTest.AddNode()\n    ncs = GafferTest.CapturingSlot(Gaffer.Metadata.nodeValueChangedSignal())\n    pcs = GafferTest.CapturingSlot(Gaffer.Metadata.plugValueChangedSignal())\n    Gaffer.Metadata.registerNodeValue(n, 'signalTest', 1)\n    Gaffer.Metadata.registerPlugValue(n['op1'], 'signalTest', 1)\n    self.assertEqual(len(ncs), 1)\n    self.assertEqual(len(pcs), 1)\n    self.assertEqual(ncs[0], (GafferTest.AddNode.staticTypeId(), 'signalTest', n))\n    self.assertEqual(pcs[0], (GafferTest.AddNode.staticTypeId(), 'op1', 'signalTest', n['op1']))\n    Gaffer.Metadata.registerNodeValue(n, 'signalTest', 1)\n    Gaffer.Metadata.registerPlugValue(n['op1'], 'signalTest', 1)\n    self.assertEqual(len(ncs), 1)\n    self.assertEqual(len(pcs), 1)\n    Gaffer.Metadata.registerNodeValue(n, 'signalTest', 2)\n    Gaffer.Metadata.registerPlugValue(n['op1'], 'signalTest', 2)\n    self.assertEqual(len(ncs), 2)\n    self.assertEqual(len(pcs), 2)\n    self.assertEqual(ncs[1], (GafferTest.AddNode.staticTypeId(), 'signalTest', n))\n    self.assertEqual(pcs[1], (GafferTest.AddNode.staticTypeId(), 'op1', 'signalTest', n['op1']))\n", "label": 0}
{"function": "\n\ndef setup_hist_calc_counter(self):\n    self.setup_subset()\n    m = MagicMock()\n    self.artist._calculate_histogram = m\n    return m\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    BaseTestCase.setUp(self)\n    self.rawData = []\n    self.dataByKey = {\n        \n    }\n    for i in range(1, 11):\n        timeTuple = (2002, 12, 9, 0, 0, 0, 0, 0, (- 1))\n        timeInTicks = ((time.mktime(timeTuple) + (i * 86400)) + (i * 8640))\n        dateCol = cx_Oracle.TimestampFromTicks(int(timeInTicks))\n        if (i % 2):\n            timeInTicks = ((time.mktime(timeTuple) + ((i * 86400) * 2)) + (i * 12960))\n            nullableCol = cx_Oracle.TimestampFromTicks(int(timeInTicks))\n        else:\n            nullableCol = None\n        tuple = (i, dateCol, nullableCol)\n        self.rawData.append(tuple)\n        self.dataByKey[i] = tuple\n", "label": 0}
{"function": "\n\ndef test_group_by_composed(self):\n    table = self.tables.some_table\n    expr = (table.c.x + table.c.y).label('lx')\n    stmt = select([func.count(table.c.id), expr]).group_by(expr).order_by(expr)\n    self._assert_result(stmt, [(1, 3), (1, 5), (1, 7)])\n", "label": 0}
{"function": "\n\ndef testLimitView(self):\n    cb = self.make_connection()\n    d = cb.queryAll('beer', 'brewery_beers', limit=10)\n\n    def _verify(o):\n        self.assertIsInstance(o, BatchedView)\n        rows = list(o)\n        self.assertEqual(len(rows), 10)\n    return d.addCallback(_verify)\n", "label": 0}
{"function": "\n\ndef get_unapplied_migrations(migrations, applied_migrations):\n    applied_migration_names = [('%s.%s' % (mi.app_name, mi.migration)) for mi in applied_migrations]\n    for migration in migrations:\n        is_applied = (('%s.%s' % (migration.app_label(), migration.name())) in applied_migration_names)\n        if (not is_applied):\n            (yield migration)\n", "label": 0}
{"function": "\n\ndef _get_content(item, base_url=None):\n    '\\n    Return a dictionary of content, for documents, objects and errors.\\n    '\n    return {_unescape_key(key): _primative_to_document(value, base_url) for (key, value) in item.items() if (key not in ('_type', '_meta'))}\n", "label": 0}
{"function": "\n\ndef get_quality_ratio(self, mimetype):\n    if (mimetype == 'text/x-textile'):\n        return 8\n    return 0\n", "label": 0}
{"function": "\n\ndef getCenterX_2(self, pp, symbol, prevX, x, nextX):\n    xMin = pp.getXMin()\n    xMax = pp.getXMax()\n    xMid = symbol.getBaseline()\n    if (Double.NaN == xMid):\n        xMid = ((xMin + xMax) / 2.0)\n    xMinPx = pp.xToPixel(xMin)\n    xMaxPx = pp.xToPixel(xMax)\n    xMidPx = pp.xToPixel(xMid)\n    xPx = pp.xToPixel(x)\n    prevXPx = pp.xToPixel(prevX)\n    nextXPx = pp.xToPixel(nextX)\n    width = symbol.getWidth(pp)\n    symWidth = self.getAdjustedWidth(width, xPx, prevXPx, nextXPx, xMinPx, xMaxPx, xMidPx)\n    if (Double.NaN == symWidth):\n        return Double.NaN\n    xLeft = self.getUpperLeftX(width, xPx, prevXPx, nextXPx, xMinPx, xMaxPx, xMidPx, pp.getXMousePlotArea())\n    if (Double.NaN == xLeft):\n        return Double.NaN\n    xCenter = (xLeft + (symWidth / 2.0))\n    return xCenter\n", "label": 0}
{"function": "\n\ndef generate_plot(filename, rplot='A50.rplot', rpdf='A50.pdf'):\n    from jcvi.apps.r import RTemplate\n    rplot_template = '\\n    library(ggplot2)\\n\\n    data <- read.table(\"$rplot\", header=T, sep=\"\\t\")\\n    g <- ggplot(data, aes(x=index, y=cumsize, group=fasta))\\n    g + geom_line(aes(colour=fasta)) +\\n    xlab(\"Contigs\") + ylab(\"Cumulative size (Mb)\") +\\n    opts(title=\"A50 plot\", legend.position=\"top\")\\n\\n    ggsave(file=\"$rpdf\")\\n    '\n    rtemplate = RTemplate(rplot_template, locals())\n    rtemplate.run()\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kw):\n    super(ActorsLikeDogs, self).__init__(*args, **kw)\n    self.bestFriend = None\n    self.waiter = None\n", "label": 0}
{"function": "\n\ndef start_historyserver(cluster):\n    hs = vu.get_historyserver(cluster)\n    if hs:\n        run.start_historyserver(hs)\n", "label": 0}
{"function": "\n\ndef receive(self, payload):\n    if (not isinstance(payload, bytes)):\n        payload = payload.encode('utf-8')\n    if (len(payload) > 0):\n        self.message_buffer += payload\n        self.bytes_received += len(payload)\n", "label": 0}
{"function": "\n\ndef cidr(self, cidr):\n    validator.validate_cidr(cidr, self.param_name)\n", "label": 0}
{"function": "\n\ndef list_sizes(self):\n    return [NodeSize(driver=self.connection.driver, **i) for i in self._instance_types.values()]\n", "label": 0}
{"function": "\n\n@property\ndef headers(self):\n    headers = DataTablesHeader(DataTablesColumn(mark_safe('Select  <a href=\"#\" class=\"select-all btn btn-xs btn-info\">all</a> <a href=\"#\" class=\"select-none btn btn-xs btn-default\">none</a>'), sortable=False, span=2), DataTablesColumn(_('Case Name'), span=3, prop_name='name.exact'), DataTablesColumn(_('Case Type'), span=2, prop_name='type.exact'), DataTablesColumn(_('Owner'), span=2, prop_name='owner_display', sortable=False), DataTablesColumn(_('Last Modified'), span=3, prop_name='modified_on'))\n    return headers\n", "label": 0}
{"function": "\n\n@classmethod\ndef _get_cache_key(cls, key):\n    key_hash = (hashlib.md5(key).hexdigest() if key else '')\n    return 'django-exp-backoff.{}'.format(key_hash)\n", "label": 0}
{"function": "\n\ndef get_berry_flavors(self, obj):\n    flavor_map_objects = BerryFlavorMap.objects.filter(berry=obj)\n    flavor_maps = BerryFlavorMapSerializer(flavor_map_objects, many=True, context=self.context).data\n    flavors = []\n    for map in flavor_maps:\n        del map['berry']\n        flavors.append(map)\n    return flavors\n", "label": 0}
{"function": "\n\ndef with_metaclass(meta, base=object):\n    return meta('NewBase', (base,), {\n        \n    })\n", "label": 0}
{"function": "\n\ndef computeExpression(self, constraint_collection):\n    (new_node, change_tags, change_desc) = ExpressionBuiltinTypeBase.computeExpression(self, constraint_collection)\n    if (new_node is self):\n        str_value = self.getValue().getStrValue()\n        if (str_value is not None):\n            new_node = wrapExpressionWithNodeSideEffects(new_node=str_value, old_node=self.getValue())\n            change_tags = 'new_expression'\n            change_desc = \"Predicted 'str' built-in result\"\n    return (new_node, change_tags, change_desc)\n", "label": 0}
{"function": "\n\ndef test_filling_image_file_field(self):\n    self.dummy = mommy.make(DummyImageFieldModel)\n    field = DummyImageFieldModel._meta.get_field('image_field')\n    self.assertIsInstance(field, ImageField)\n    import time\n    path = ('%s/%s/mock-img.jpeg' % (gettempdir(), time.strftime('%Y/%m/%d')))\n    from django import VERSION\n    if (VERSION[1] >= 4):\n        self.assertEqual(abspath(self.dummy.image_field.path), abspath(path))\n        self.assertTrue(self.dummy.image_field.width)\n        self.assertTrue(self.dummy.image_field.height)\n", "label": 0}
{"function": "\n\ndef mention(sourceURL, targetURL, vouchDomain=None):\n    'Process the Webmention of the targetURL from the sourceURL.\\n\\n    To verify that the sourceURL has indeed referenced our targetURL\\n    we run findMentions() at it and scan the resulting href list.\\n    '\n    app.logger.info(('discovering Webmention endpoint for %s' % sourceURL))\n    mentions = ronkyuu.findMentions(sourceURL)\n    result = False\n    app.logger.info(('mentions %s' % mentions))\n    for href in mentions['refs']:\n        if ((href != sourceURL) and (href == targetURL)):\n            app.logger.info(('post at %s was referenced by %s' % (targetURL, sourceURL)))\n            result = processWebmention(sourceURL, targetURL, vouchDomain)\n    app.logger.info(('mention() returning %s' % result))\n    return result\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    levelname = record.levelname\n    if (self.use_color and (levelname in COLORS)):\n        levelname_color = (((COLOR_SEQ % (30 + COLORS[levelname])) + levelname) + RESET_SEQ)\n        record.levelname = levelname_color\n    return logging.Formatter.format(self, record)\n", "label": 0}
{"function": "\n\ndef _volume_is_attached_to_vm(self, vol_obj):\n    match_vm_id = 0\n    for attached_dev in vol_obj.attachments:\n        for dst_vm in self.dst_vm_list:\n            if (dst_vm.id == attached_dev['server_id']):\n                match_vm_id += 1\n    return (len(vol_obj.attachments) == match_vm_id)\n", "label": 0}
{"function": "\n\ndef get_func(self, *args, **kwargs):\n    from django.contrib.gis.geos.prototypes.threadsafe import GEOSFunc\n    func = GEOSFunc(self.func_name)\n    func.argtypes = (self.argtypes or [])\n    func.restype = self.restype\n    if self.errcheck:\n        func.errcheck = self.errcheck\n    return func\n", "label": 0}
{"function": "\n\ndef _round_over(self):\n    message = ('left score: %d\\nright score: %d' % (self._left_score, self._right_score))\n    self._operations.show_text_on_feed(message)\n    if self._continue_protocol:\n        self._operations.pause_shot_detection(True)\n        self._new_round_thread = Thread(target=self._new_round, name='new_round_thread')\n        self._new_round_thread.start()\n", "label": 0}
{"function": "\n\ndef eventFilter(self, source, event):\n    typ = event.type()\n    if (typ == QtCore.QEvent.Drop):\n        self.dropEvent(event)\n    elif (typ == QtCore.QEvent.DragEnter):\n        self.dragEnterEvent(event)\n    return super(_DropEventFilter, self).eventFilter(source, event)\n", "label": 0}
{"function": "\n\n@property\ndef crop_size(self):\n    from cropduster.resizing import Size\n    size_json = (self.get('size', {\n        \n    }).get('json') or None)\n    if size_json:\n        return size_json\n    size_w = (self.get('size', {\n        \n    }).get('w') or None)\n    size_h = (self.get('size', {\n        \n    }).get('h') or None)\n    if ((not size_w) and (not size_h)):\n        return None\n    return Size('crop', w=size_w, h=size_h)\n", "label": 0}
{"function": "\n\ndef post(self):\n    username = self.get_argument('username', None)\n    password = self.get_argument('password', None)\n    if ((not username) or (not password)):\n        self.flash.error = 'You must enter a username and password to proceed. Please try again.'\n        self.redirect('/signup')\n        return\n    if (password != self.get_argument('password2', None)):\n        self.flash.error = 'Passwords do not match. Please try again.'\n        self.redirect('/signup')\n        return\n    user = User.instance(username, password)\n    Mongo.db.ui.users.insert(user)\n    self.flash.info = 'Successfully created your account, please log in.'\n    self.redirect('/login')\n", "label": 0}
{"function": "\n\ndef _translate_xor(self, tb, instruction):\n    oprnd0 = tb.read(instruction.operands[0])\n    oprnd1 = tb.read(instruction.operands[1])\n    tmp0 = tb.temporal((oprnd0.size * 2))\n    tb.add(self._builder.gen_xor(oprnd0, oprnd1, tmp0))\n    if (self._translation_mode == FULL_TRANSLATION):\n        self._clear_flag(tb, self._flags['of'])\n        self._clear_flag(tb, self._flags['cf'])\n        self._update_sf(tb, oprnd0, oprnd1, tmp0)\n        self._update_zf(tb, oprnd0, oprnd1, tmp0)\n        self._update_pf(tb, oprnd0, oprnd1, tmp0)\n        self._undefine_flag(tb, self._flags['af'])\n    tb.write(instruction.operands[0], tmp0)\n", "label": 0}
{"function": "\n\ndef list_processes(self):\n    return [mach.list_processes() for mach in self]\n", "label": 0}
{"function": "\n\ndef install_dependencies(self, bundle_store, parent_dict, dest_path, copy):\n    \"\\n        Symlink or copy this bundle's dependencies into the directory at dest_path.\\n        The caller is responsible for cleaning up this directory.\\n        \"\n    precondition(os.path.isabs(dest_path), ('%s is a relative path!' % (dest_path,)))\n    pairs = self.get_dependency_paths(bundle_store, parent_dict, dest_path, relative_symlinks=(not copy))\n    for (target, link_path) in pairs:\n        if os.path.exists(link_path):\n            path_util.remove(link_path)\n        if copy:\n            path_util.copy(target, link_path, follow_symlinks=False)\n        else:\n            os.symlink(target, link_path)\n", "label": 0}
{"function": "\n\ndef store(self, filename):\n    'Write the variables into a file'\n    file = open(filename, 'w')\n    merged_table = self.get_merged_dict()\n    keys = list(merged_table.keys())\n    keys.sort()\n    for k in keys:\n        file.write(('%s = %r\\n' % (k, merged_table[k])))\n    file.close()\n", "label": 0}
{"function": "\n\ndef _create_link(self):\n    ' Create a ComponentLink form the state of the GUI\\n\\n        Returns\\n        -------\\n        A new component link\\n        '\n    expression = str(self.ui.expression.toPlainText())\n    pattern = '[^\\\\s]*:[^\\\\s]*'\n\n    def add_curly(m):\n        return (('{' + m.group(0)) + '}')\n    expression = re.sub(pattern, add_curly, expression)\n    pc = parse.ParsedCommand(expression, self._labels)\n    label = (str(self.ui.new_label.text()) or 'new component')\n    new_id = core.data.ComponentID(label)\n    link = parse.ParsedComponentLink(new_id, pc)\n    return link\n", "label": 0}
{"function": "\n\n@staticmethod\ndef split_program_source_bc(lst):\n    (py_files, pyc_files) = ([], [])\n    for f in lst:\n        dest = (py_files if f.lower().endswith('.py') else pyc_files)\n        dest.append(f)\n    return (py_files, pyc_files)\n", "label": 0}
{"function": "\n\ndef console(self, channel, payload):\n    if self.config['IRC']['show-channel-server']:\n        self.rawConsole({\n            'text': ('[%s] ' % channel),\n            'color': 'gold',\n            'extra': payload,\n        })\n    else:\n        self.rawConsole({\n            'extra': payload,\n        })\n", "label": 0}
{"function": "\n\ndef GenerateCommand(self):\n    super_cmds = super(MsgBox, self).GenerateCommand()\n    cmds = []\n    cmds.extend(['--button1', self._button1])\n    if self._button2:\n        cmds.extend(['--button2', self._button2])\n    if self._button3:\n        cmds.extend(['--button3', self._button3])\n    if self._informative_text:\n        cmds.extend(['--informative-text', self._informative_text])\n    if self._float:\n        cmds.append('--float')\n    super_cmds.extend(cmds)\n    return super_cmds\n", "label": 0}
{"function": "\n\ndef _make_subnet_args(self, detail, subnet, subnetpool_id):\n    gateway_ip = (str(detail.gateway_ip) if detail.gateway_ip else None)\n    args = {\n        'tenant_id': detail.tenant_id,\n        'id': detail.subnet_id,\n        'name': subnet['name'],\n        'network_id': subnet['network_id'],\n        'ip_version': subnet['ip_version'],\n        'cidr': str(detail.subnet_cidr),\n        'subnetpool_id': subnetpool_id,\n        'enable_dhcp': subnet['enable_dhcp'],\n        'gateway_ip': gateway_ip,\n        'description': subnet.get('description'),\n    }\n    if ((subnet['ip_version'] == 6) and subnet['enable_dhcp']):\n        if validators.is_attr_set(subnet['ipv6_ra_mode']):\n            args['ipv6_ra_mode'] = subnet['ipv6_ra_mode']\n        if validators.is_attr_set(subnet['ipv6_address_mode']):\n            args['ipv6_address_mode'] = subnet['ipv6_address_mode']\n    return args\n", "label": 0}
{"function": "\n\ndef autonomous(self):\n    'Called when autonomous mode is enabled'\n    timer = wpilib.Timer()\n    timer.start()\n    while (self.isAutonomous() and self.isEnabled()):\n        if (timer.get() < 2.0):\n            self.robot_drive.arcadeDrive((- 1.0), (- 0.3))\n        else:\n            self.robot_drive.arcadeDrive(0, 0)\n        wpilib.Timer.delay(0.01)\n", "label": 0}
{"function": "\n\ndef get_dialect(self, dialect_source):\n    'Get a CSV dialect instance for this data.'\n    if dialect_source:\n        (_valid, rv) = csv_dialect.validate(dialect_source)\n        if (not _valid):\n            raise exceptions.InvalidSpec\n        return rv\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef process(args):\n    conduit = phlsys_makeconduit.make_conduit(args.uri, args.user, args.cert, args.act_as_user)\n    if args.text:\n        text = args.text\n    elif args.text_file:\n        text = args.text_file.read()\n    else:\n        print('error: you have not specified any content for the paste')\n        sys.exit(1)\n    result = phlcon_paste.create_paste(conduit, text, args.title, args.language)\n    print((result.uri if (not args.format_id) else result.id))\n", "label": 0}
{"function": "\n\ndef main():\n    parser = make_parser()\n    args = parser.parse_args()\n    logger = Logger(args.verbose)\n    storage = Storage(args.storage, logger)\n    if (args.command == 'list'):\n        for file in storage.query():\n            print(file)\n    elif (args.command == 'compare'):\n        results_table = TableResults(args.columns, args.sort, first_or_value(args.histogram, False), NAME_FORMATTERS[args.name], logger)\n        groups = load(storage, args.glob_or_file, args.group_by)\n        results_table.display(TerminalReporter(), groups, progress_reporter=report_noprogress)\n        if args.csv:\n            results_csv = CSVResults(args.columns, args.sort, logger)\n            (output_file,) = args.csv\n            results_csv.render(output_file, groups)\n    else:\n        parser.error('Unknown command {0!r}'.format(args.command))\n", "label": 0}
{"function": "\n\ndef _set_byteorder(self, order):\n    if (order not in (0, 1)):\n        raise ValueError('Byte order parameter must be 0 (Big Endian) or 1 (Little Endian).')\n    wkb_writer_set_byteorder(self.ptr, order)\n", "label": 0}
{"function": "\n\ndef _local_settings_acked(self):\n    '\\n        Handle the local settings being ACKed, update internal state.\\n        '\n    changes = self.local_settings.acknowledge()\n    if (INITIAL_WINDOW_SIZE in changes):\n        setting = changes[INITIAL_WINDOW_SIZE]\n        self._inbound_flow_control_change_from_settings(setting.original_value, setting.new_value)\n    return changes\n", "label": 0}
{"function": "\n\ndef __init__(self, rules, header=None, separator=''):\n    '\\n            Constructor.\\n\\n            :param rules:\\n                Child rules\\n            :param header:\\n                Header text\\n            :param separator:\\n                Child rule separator\\n        '\n    if header:\n        rule_set = ([Header(header)] + list(rules))\n    else:\n        rule_set = list(rules)\n    super(FieldSet, self).__init__(rule_set, separator=separator)\n", "label": 0}
{"function": "\n\ndef get_test_problem(task='regression'):\n    X = sp.csc_matrix(np.array([[6, 1], [2, 3], [3, 0], [6, 1], [4, 5]]), dtype=np.float64)\n    y = np.array([298, 266, 29, 298, 848], dtype=np.float64)\n    V = np.array([[6, 0], [5, 8]], dtype=np.float64)\n    w = np.array([9, 2], dtype=np.float64)\n    w0 = 2\n    if (task == 'classification'):\n        y_labels = np.ones_like(y)\n        y_labels[(y < np.median(y))] = (- 1)\n        y = y_labels\n    return (w0, w, V, y, X)\n", "label": 0}
{"function": "\n\ndef build_waiter_state_description(self):\n    description = self._waiter_config.description\n    if (not description):\n        description = 'Wait until '\n        for acceptor in self._waiter_config.acceptors:\n            if (acceptor.state == 'success'):\n                description += self._build_success_description(acceptor)\n                break\n        description += self._build_operation_description(self._waiter_config.operation)\n    description += self._build_polling_description(self._waiter_config.delay, self._waiter_config.max_attempts)\n    return description\n", "label": 0}
{"function": "\n\ndef _selected_location_entries(self, mobile_user_and_group_slugs):\n    location_ids = self.selected_location_ids(mobile_user_and_group_slugs)\n    if (not location_ids):\n        return []\n    return map(self.utils.location_tuple, SQLLocation.objects.filter(location_id__in=location_ids))\n", "label": 0}
{"function": "\n\ndef setup(self):\n    self.mi = MultiIndex.from_tuples([(x, y) for x in range(1000) for y in range(1000)])\n    self.s = Series(np.random.randn(1000000), index=self.mi)\n    self.df = DataFrame(self.s)\n", "label": 0}
{"function": "\n\ndef run_backfill(config_path, config_id, config, start_at, end_at):\n    collector_path = join(dirname(sys.executable), 'pp-collector')\n    query_path = get_query_path(config_path, config_id)\n    credentials_path = join(config_path, 'credentials', 'ga.json')\n    token_path = get_token_path(config_path, config['token'])\n    platform_path = join(config_path, 'performanceplatform.json')\n    command = [collector_path, '-q', query_path, '-c', credentials_path, '-t', token_path, '-b', platform_path, '-s', start_at.isoformat()]\n    if (end_at is not None):\n        command += ['-e', end_at.isoformat()]\n    status = subprocess.call(command, stderr=sys.stdout.fileno(), stdout=sys.stdout.fileno())\n    if (status != 0):\n        print('Failed!')\n        raise SystemExit(1)\n", "label": 0}
{"function": "\n\n@ensure_tag(['r'])\ndef is_bold(r):\n    '\\n    The function will return True if the r tag passed in is considered bold.\\n    '\n    w_namespace = get_namespace(r, 'w')\n    rpr = r.find(('%srPr' % w_namespace))\n    if (rpr is None):\n        return False\n    bold = rpr.find(('%sb' % w_namespace))\n    return style_is_false(bold)\n", "label": 0}
{"function": "\n\ndef get_db_prep_save(self, value):\n    if (value is not None):\n        value = smart_unicode(value)\n    return super(TimeZoneField, self).get_db_prep_save(value)\n", "label": 0}
{"function": "\n\ndef __init__(self, manager, user_state=None, project_state=None):\n    if (not user_state):\n        user_state = {\n            \n        }\n    if (not project_state):\n        project_state = {\n            \n        }\n    self.manager = manager\n    if ('name' not in user_state):\n        user_state['name'] = 'test1'\n    if ('name' not in project_state):\n        project_state['name'] = 'testproj'\n    if ('manager_user' not in project_state):\n        project_state['manager_user'] = 'test1'\n    self.user = manager.create_user(**user_state)\n    self.project = manager.create_project(**project_state)\n", "label": 0}
{"function": "\n\ndef _validate(self):\n    for (group, dbs) in self.partition_config['groups'].items():\n        for db in dbs:\n            if (db not in self.database_config):\n                raise PartitionValidationError('{} not in found in DATABASES'.format(db))\n    shards_seen = set()\n    previous_range = None\n    for (group, shard_range) in sorted(self.partition_config['shards'].items(), key=(lambda x: x[1])):\n        if (not previous_range):\n            if (shard_range[0] != 0):\n                raise NotZeroStartError('Shard numbering must start at 0')\n        elif ((previous_range[1] + 1) != shard_range[0]):\n            raise NonContinuousShardsError('Shards must be numbered consecutively: {} -> {}'.format(previous_range[1], shard_range[0]))\n        shards_seen |= set(range(shard_range[0], (shard_range[1] + 1)))\n        previous_range = shard_range\n    num_shards = len(shards_seen)\n    if (not _is_power_of_2(num_shards)):\n        raise NotPowerOf2Error('Total number of shards must be a power of 2')\n", "label": 1}
{"function": "\n\ndef project_documentation_file(request, snapshot):\n    project = Domain.get(snapshot)\n    if project.documentation_file_path:\n        documentation_file = project.fetch_attachment(project.documentation_file_path)\n        return HttpResponse(documentation_file, content_type=project.documentation_file_type)\n    else:\n        raise Http404()\n", "label": 0}
{"function": "\n\ndef bytes2human(use_si_units=False):\n    if use_si_units:\n        prefixes = ('TB', 'GB', 'MB', 'kB', 'B')\n        values = (1000000000000.0, 1000000000.0, 1000000.0, 1000.0, 1)\n    else:\n        prefixes = ('TiB', 'GiB', 'MiB', 'KiB', 'B')\n        values = ((2 ** 40), (2 ** 30), (2 ** 20), (2 ** 10), 1)\n\n    def b2h(nbytes):\n        for (prefix, value) in zip(prefixes, values):\n            scaled = (float(nbytes) / value)\n            if (scaled >= 1):\n                break\n        return ('%.1f%s' % (scaled, prefix))\n    return b2h\n", "label": 0}
{"function": "\n\ndef fetch_gerrit_hook_ssh(path, username, server, port=None):\n    ' Fetch the ``commit-msg`` hook from gerrit\\n\\n    '\n    if (port is None):\n        port = 22\n    git_hooks_dir = os.path.join(path, '.git', 'hooks')\n    if (not os.path.isdir(git_hooks_dir)):\n        qisys.sh.mkdir(git_hooks_dir)\n    if sys.platform.startswith('win'):\n        git_hooks_dir = qisys.sh.to_posix_path(git_hooks_dir, fix_drive=True)\n    scp = qisys.command.find_program('scp', raises=False)\n    if (not scp):\n        return (False, 'Could not find scp executable')\n    cmd = [scp, '-P', str(port), ('%s@%s:hooks/commit-msg' % (username, server)), git_hooks_dir]\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    (out, _) = process.communicate()\n    if (process.returncode == 0):\n        return (True, '')\n    else:\n        return (False, out)\n", "label": 0}
{"function": "\n\ndef __init__(self, verbose_name=None, name=None, timezone=None, **kwargs):\n    if isinstance(timezone, basestring):\n        timezone = smart_str(timezone)\n    if (timezone in pytz.all_timezones_set):\n        self.timezone = pytz.timezone(timezone)\n    else:\n        self.timezone = timezone\n    super(LocalizedDateTimeField, self).__init__(verbose_name, name, **kwargs)\n", "label": 0}
{"function": "\n\ndef visit_Call(self, node):\n    if isinstance(node.func, ast.Attribute):\n        if isinstance(node.func.value, ast.Name):\n            if (node.func.value.id == 'context'):\n                if (node.func.attr == 'getFrame'):\n                    self.contextReads.add('frame')\n                elif (node.func.attr == 'getTime'):\n                    self.contextReads.add('frame')\n                    self.contextReads.add('framesPerSecond')\n                elif (node.func.attr == 'getFramesPerSecond'):\n                    self.contextReads.add('framesPerSecond')\n                elif (node.func.attr == 'get'):\n                    if (not isinstance(node.args[0], ast.Str)):\n                        raise SyntaxError('Context name must be a string')\n                    self.contextReads.add(node.args[0].s)\n    ast.NodeVisitor.generic_visit(self, node)\n", "label": 1}
{"function": "\n\ndef __init__(self, names, propagate_map_exceptions=False):\n    'Initialise the resource handler by loading the plugins.\\n\\n        The ResourceHandler uses stevedore to load the resource plugins.\\n        The handler can handle and report exceptions raised in the plugins\\n        depending on the value of the propagate_map_exceptions parameter.\\n        It is useful in testing to propagate exceptions so they are exposed\\n        as part of the test. If exceptions are not propagated they are\\n        logged at error level.\\n\\n        Any named plugins that are not located are logged.\\n\\n        :param names: the list of plugins to load by name\\n        :param propagate_map_exceptions: True indicates exceptions in the\\n        plugins should be raised, False indicates they should be handled and\\n        logged.\\n        '\n    self._mgr = stevedore.NamedExtensionManager(namespace=RESOURCE_NAMESPACE, names=names, propagate_map_exceptions=propagate_map_exceptions, invoke_on_load=True)\n    if self._mgr.names():\n        LOG.warning(_LW('The Extensible Resource Tracker is deprecated and will be removed in the 14.0.0 release. If you use this functionality and have custom resources that are managed by the Extensible Resource Tracker, please contact the Nova development team by posting to the openstack-dev mailing list. There is no future planned support for the tracking of custom resources.'))\n    self._log_missing_plugins(names)\n", "label": 0}
{"function": "\n\ndef look_ahead(self, s):\n    if ((self.index + len(s)) >= len(self.input)):\n        return False\n    return (self.input[self.index:(self.index + len(s))] == s)\n", "label": 0}
{"function": "\n\ndef __init__(self, session_id=None):\n    self.session_id = (session_id or 1)\n    self.expiry = None\n    self.promoted = None\n    self._delete_called = False\n", "label": 0}
{"function": "\n\ndef valueChange(self, event):\n    f = event.getProperty().getValue()\n    v = self._tree.getValue()\n    if (((f is not None) and (f != v)) or ((f is None) and (v is not None))):\n        self._tree.setValue(f)\n    self._window.removeSubwindows()\n", "label": 0}
{"function": "\n\ndef flush(self, timestamp, interval):\n    if (self.count is None):\n        return []\n    try:\n        return [self.formatter(hostname=self.hostname, device_name=self.device_name, tags=self.tags, metric=self.name, value=self.count, timestamp=timestamp, metric_type=MetricTypes.COUNT, interval=interval)]\n    finally:\n        self.prev_counter = self.curr_counter\n        self.curr_counter = None\n        self.count = None\n", "label": 0}
{"function": "\n\ndef interpretkeyevent(keyEvent):\n    'Returns the character represented by the pygame.event.Event object in keyEvent. This makes adjustments for the shift key and capslock.'\n    key = keyEvent.key\n    if (((key >= 32) and (key < 127)) or (key in (ord('\\n'), ord('\\r'), ord('\\t')))):\n        caps = bool((keyEvent.mod & KMOD_CAPS))\n        shift = bool(((keyEvent.mod & KMOD_LSHIFT) or (keyEvent.mod & KMOD_RSHIFT)))\n        char = chr(key)\n        if (char.isalpha() and (caps ^ shift)):\n            char = char.upper()\n        elif (shift and (char in _shiftchars)):\n            char = _shiftchars[char]\n        return char\n    return None\n", "label": 1}
{"function": "\n\ndef apply_settings(self, view):\n    ' Applies the settings from the settings file '\n    if (self.pluginSettings is None):\n        self.pluginSettings = sublime.load_settings((__name__ + '.sublime-settings'))\n        self.pluginSettings.clear_on_change('glsv_validator')\n        self.pluginSettings.add_on_change('glsv_validator', self.clear_settings)\n    if (view.settings().get('glsv_configured') is None):\n        view.settings().set('glsv_configured', True)\n        for setting in self.DEFAULT_SETTINGS:\n            settingValue = self.DEFAULT_SETTINGS[setting]\n            if (self.pluginSettings.get(setting) is not None):\n                settingValue = self.pluginSettings.get(setting)\n            view.settings().set(setting, settingValue)\n", "label": 0}
{"function": "\n\ndef __init__(self, digest_provider, starting_bucket, starting_prefix, public_key_provider, digest_validator=None, on_invalid=None, on_gap=None, on_missing=None):\n    '\\n        :type digest_provider: DigestProvider\\n        :param digest_provider: DigestProvider object\\n        :param starting_bucket: S3 bucket where the digests are stored.\\n        :param starting_prefix: An optional prefix applied to each S3 key.\\n        :param public_key_provider: Provides public keys for a range.\\n        :param digest_validator: Validates digest using a validate method.\\n        :param on_invalid: Callback invoked when a digest is invalid.\\n        :param on_gap: Callback invoked when a digest has no parent, but\\n            there are still more digests to validate.\\n        :param on_missing: Callback invoked when a digest file is missing.\\n        '\n    self.starting_bucket = starting_bucket\n    self.starting_prefix = starting_prefix\n    self.digest_provider = digest_provider\n    self._public_key_provider = public_key_provider\n    self._on_gap = on_gap\n    self._on_invalid = on_invalid\n    self._on_missing = on_missing\n    if (digest_validator is None):\n        digest_validator = Sha256RSADigestValidator()\n    self._digest_validator = digest_validator\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return (isinstance(other, self.__class__) and (self.__dict__ == other.__dict__))\n", "label": 0}
{"function": "\n\ndef _security_group_rule_exists(self, security_group, values):\n    'Indicates whether the specified rule values are already\\n           defined in the given security group.\\n        '\n    for rule in security_group.rules:\n        if ('group_id' in values):\n            if (rule['group_id'] == values['group_id']):\n                return True\n        else:\n            is_duplicate = True\n            for key in ('cidr', 'from_port', 'to_port', 'protocol'):\n                if (rule[key] != values[key]):\n                    is_duplicate = False\n                    break\n            if is_duplicate:\n                return True\n    return False\n", "label": 0}
{"function": "\n\n@classmethod\ndef pull_users_and_groups(cls, domain, mobile_user_and_group_slugs, include_inactive=False):\n    user_ids = cls.selected_user_ids(mobile_user_and_group_slugs)\n    user_types = cls.selected_user_types(mobile_user_and_group_slugs)\n    group_ids = cls.selected_group_ids(mobile_user_and_group_slugs)\n    users = []\n    if (user_ids or (HQUserType.REGISTERED in user_types)):\n        users = util.get_all_users_by_domain(domain=domain, user_ids=user_ids, simplified=True, CommCareUser=CommCareUser)\n    user_filter = tuple([HQUserToggle(id, (id in user_types)) for id in range(4)])\n    other_users = util.get_all_users_by_domain(domain=domain, user_filter=user_filter, simplified=True, CommCareUser=CommCareUser, include_inactive=include_inactive)\n    groups = [Group.get(g) for g in group_ids]\n    all_users = (users + other_users)\n    user_dict = {\n        \n    }\n    for group in groups:\n        user_dict[('%s|%s' % (group.name, group._id))] = util.get_all_users_by_domain(group=group, simplified=True)\n    users_in_groups = [user for sublist in user_dict.values() for user in sublist]\n    users_by_group = user_dict\n    combined_users = remove_dups((all_users + users_in_groups), 'user_id')\n    return _UserData(users=all_users, admin_and_demo_users=other_users, groups=groups, users_by_group=users_by_group, combined_users=combined_users)\n", "label": 0}
{"function": "\n\ndef AddColumn(self, column, default='', col_index=(- 1)):\n    \"Appends a new column to the table.\\n\\n    Args:\\n      column: A string, name of the column to add.\\n      default: Default value for entries. Defaults to ''.\\n      col_index: Integer index for where to insert new column.\\n\\n    Raises:\\n      TableError: Column name already exists.\\n\\n    \"\n    if (column in self.table):\n        raise TableError(('Column %r already in table.' % column))\n    if (col_index == (- 1)):\n        self._table[0][column] = column\n        for i in xrange(1, len(self._table)):\n            self._table[i][column] = default\n    else:\n        self._table[0].Insert(column, column, col_index)\n        for i in xrange(1, len(self._table)):\n            self._table[i].Insert(column, default, col_index)\n", "label": 0}
{"function": "\n\ndef run(self):\n    repo = self.get_repo()\n    if (not repo):\n        return\n    status = self.get_porcelain_status(repo)\n    if (not status):\n        status = [GIT_WORKING_DIR_CLEAN]\n\n    def on_done(idx):\n        if ((idx == (- 1)) or (status[idx] == GIT_WORKING_DIR_CLEAN)):\n            return\n        (state, filename) = (status[idx][0:2], status[idx][3:])\n        (index, worktree) = state\n        if (state == '??'):\n            return sublime.error_message('Cannot show diff for untracked files.')\n        window = self.window\n        if (worktree != ' '):\n            window.run_command('git_diff', {\n                'repo': repo,\n                'path': filename,\n            })\n        if (index != ' '):\n            window.run_command('git_diff', {\n                'repo': repo,\n                'path': filename,\n                'cached': True,\n            })\n    self.window.show_quick_panel(status, on_done, sublime.MONOSPACE_FONT)\n", "label": 0}
{"function": "\n\ndef put_continue_creation(self, token, content, content_range, last=False):\n    'Continue object upload with PUTs.\\n\\n    This implements the resumable upload XML API.\\n\\n    Args:\\n      token: upload token returned by post_start_creation.\\n      content: object content.\\n      content_range: a (start, end) tuple specifying the content range of this\\n        chunk. Both are inclusive according to XML API.\\n      last: True if this is the last chunk of file content.\\n\\n    Raises:\\n      ValueError: if token is invalid.\\n    '\n    gcs_file = _AE_GCSFileInfo_.get_by_key_name(token)\n    if (not gcs_file):\n        raise ValueError('Invalid token')\n    if content:\n        (start, end) = content_range\n        if (len(content) != ((end - start) + 1)):\n            raise ValueError(('Invalid content range %d-%d' % content_range))\n        blobkey = ('%s-%d-%d' % (token, content_range[0], content_range[1]))\n        self.blob_storage.StoreBlob(blobkey, StringIO.StringIO(content))\n        new_content = _AE_GCSPartialFile_(parent=gcs_file, partial_content=blobkey, start=start, end=(end + 1))\n        new_content.put()\n    if last:\n        self._end_creation(token)\n", "label": 0}
{"function": "\n\ndef body(self, result):\n    body = 'OK'\n    if (self.response_type == 'json'):\n        body = json_dumps(result)\n    return body\n", "label": 0}
{"function": "\n\ndef process_input(self, *args, **kwargs):\n    '\\n        Convert input json data into a statement object.\\n        '\n    if (not args):\n        raise TypeError('process_input expects at least one positional argument')\n    input_json = args[0]\n    text = input_json['text']\n    del input_json['text']\n    return Statement(text, **input_json)\n", "label": 0}
{"function": "\n\ndef decompress(self, value):\n    '\\n        Receives an instance of `MultiLingualFile` and returns a list of\\n        broken-out-files corresponding in position to the current ordering\\n        of settings.LANGUAGES.\\n        '\n    text_dict = (dict(((code, getattr(value, code)) for (code, verbose) in LANGUAGES)) if value else {\n        \n    })\n    return [text_dict.get(code) for (code, verbose) in LANGUAGES]\n", "label": 0}
{"function": "\n\ndef test_displaying_more_than_one_blank_form(self):\n    ChoiceFormSet = formset_factory(Choice, extra=3)\n    formset = ChoiceFormSet(auto_id=False, prefix='choices')\n    form_output = []\n    for form in formset.forms:\n        form_output.append(form.as_ul())\n    self.assertEqual('\\n'.join(form_output), '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" /></li>\\n<li>Votes: <input type=\"text\" name=\"choices-0-votes\" /></li>\\n<li>Choice: <input type=\"text\" name=\"choices-1-choice\" /></li>\\n<li>Votes: <input type=\"text\" name=\"choices-1-votes\" /></li>\\n<li>Choice: <input type=\"text\" name=\"choices-2-choice\" /></li>\\n<li>Votes: <input type=\"text\" name=\"choices-2-votes\" /></li>')\n    data = {\n        'choices-TOTAL_FORMS': '3',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': '',\n        'choices-0-votes': '',\n        'choices-1-choice': '',\n        'choices-1-votes': '',\n        'choices-2-choice': '',\n        'choices-2-votes': '',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([form.cleaned_data for form in formset.forms], [{\n        \n    }, {\n        \n    }, {\n        \n    }])\n", "label": 0}
{"function": "\n\ndef _contains(self, other):\n    if (other.is_positive and other.is_integer):\n        return S.true\n    elif ((other.is_integer is False) or (other.is_positive is False)):\n        return S.false\n", "label": 0}
{"function": "\n\n@base.remotable_classmethod\ndef get_by_uuid(cls, context, uuid, expected_attrs=None, use_slave=False):\n    if (expected_attrs is None):\n        expected_attrs = ['info_cache', 'security_groups']\n    columns_to_join = _expected_cols(expected_attrs)\n    db_inst = cls._db_instance_get_by_uuid(context, uuid, columns_to_join, use_slave=use_slave)\n    return cls._from_db_object(context, cls(), db_inst, expected_attrs)\n", "label": 0}
{"function": "\n\ndef roleTypeName(modelXbrl, roleURI, *args, **kwargs):\n    modelManager = modelXbrl.modelManager\n    if hasattr(modelManager, 'efmFiling'):\n        modelRoles = modelXbrl.roleTypes.get(roleURI, ())\n        if (modelRoles and modelRoles[0].definition):\n            return re.sub('\\\\{\\\\s*(transposed|unlabeled|elements)\\\\s*\\\\}', '', modelRoles[0].definition.rpartition('-')[2], flags=re.I).strip()\n        return roleURI\n    return None\n", "label": 0}
{"function": "\n\ndef add_device(self, bind_data, cert, transports=0):\n    certificate = object_session(self).query(Certificate).filter((Certificate.fingerprint == b2a_hex(cert.fingerprint(hashes.SHA1())))).first()\n    if (certificate is None):\n        certificate = Certificate(cert)\n    return Device(self, bind_data, certificate, transports)\n", "label": 0}
{"function": "\n\ndef poll(self, timeout):\n    if (timeout < 0):\n        timeout = None\n    events = self._kqueue.control(None, KqueueLoop.MAX_EVENTS, timeout)\n    results = defaultdict((lambda : POLL_NULL))\n    for e in events:\n        fd = e.ident\n        if (e.filter == select.KQ_FILTER_READ):\n            results[fd] |= POLL_IN\n        elif (e.filter == select.KQ_FILTER_WRITE):\n            results[fd] |= POLL_OUT\n    return results.items()\n", "label": 0}
{"function": "\n\ndef get_tree(self, user, repo, ref='master', recursive=False, callback=None, **kwargs):\n    'Get a git tree'\n    path = 'repos/{user}/{repo}/git/trees/{ref}'.format(**locals())\n    if recursive:\n        params = kwargs.setdefault('params', {\n            \n        })\n        params['recursive'] = True\n    return self.github_api_request(path, callback, **kwargs)\n", "label": 0}
{"function": "\n\n@property\ndef elementAttributesTuple(self):\n    return tuple(((name, value) for (name, value) in self.items()))\n", "label": 0}
{"function": "\n\ndef _register_variable(self, objective_scope, variable_name, node, is_implicit=False, is_builtin=False, is_function=False):\n    variable = self._create_variable(is_implicit=is_implicit, is_builtin=is_builtin)\n    objective_variable_list = objective_scope[('functions' if is_function else 'variables')]\n    same_name_variables = objective_variable_list.setdefault(variable_name, [])\n    same_name_variables.append(variable)\n    self.link_registry.link_variable_to_declarative_identifier(variable, node)\n    current_scope = self.get_current_scope()\n    self.link_registry.link_identifier_to_context_scope(node, current_scope)\n", "label": 0}
{"function": "\n\ndef mode_init(self, request):\n    '\\n        This is called by render_POST when the client requests an init\\n        mode operation (at startup)\\n\\n        Args:\\n            request (Request): Incoming request.\\n\\n        '\n    suid = request.args.get('suid', ['0'])[0]\n    remote_addr = request.getClientIP()\n    host_string = ('%s (%s:%s)' % (SERVERNAME, request.getRequestHostname(), request.getHost().port))\n    if (suid == '0'):\n        suid = md5(str(time.time())).hexdigest()\n        self.databuffer[suid] = []\n        sess = WebClientSession()\n        sess.client = self\n        sess.init_session('webclient', remote_addr, self.sessionhandler)\n        sess.suid = suid\n        sess.sessionhandler.connect(sess)\n    return jsonify({\n        'msg': host_string,\n        'suid': suid,\n    })\n", "label": 0}
{"function": "\n\ndef __get__(self, obj, cls):\n    if (obj is None):\n        return self\n    value = self._deferred(obj)\n    setattr(obj, self._deferred.func_name, value)\n    return value\n", "label": 0}
{"function": "\n\ndef __init__(self, layers, loss):\n    self.layers = layers\n    self.loss = loss\n    self.bprop_until = next((idx for (idx, l) in enumerate(self.layers) if isinstance(l, ParamMixin)), 0)\n    self.layers[self.bprop_until].bprop_to_x = False\n    self.collection = self.layers\n    self._initialized = False\n", "label": 0}
{"function": "\n\ndef generate_random_string(string_length=88, chars=((string.ascii_lowercase + string.ascii_uppercase) + string.digits)):\n    '\\n    Generate a random string.\\n    :param string_length:\\n    :param chars:\\n    :return:\\n    '\n    return ''.join((random.SystemRandom().choice(chars) for _ in range(string_length)))\n", "label": 0}
{"function": "\n\ndef as_entity(self, with_table=False):\n    if with_table:\n        return Entity(self.model_class._meta.db_table, self.db_column)\n    return Entity(self.db_column)\n", "label": 0}
{"function": "\n\ndef validURL(targetURL):\n    'Validate the target URL exists.\\n\\n    In a real app you would need to do a database lookup or a HEAD request, here we just check the URL\\n    '\n    if ('/article' in targetURL):\n        result = 200\n    else:\n        result = 404\n    return result\n", "label": 0}
{"function": "\n\ndef perform(self, node, inputs, outputs):\n    (x, y, p) = inputs\n    (out,) = outputs\n    if _is_sparse(x):\n        raise TypeError(x)\n    if _is_sparse(y):\n        raise TypeError(y)\n    if (not _is_sparse(p)):\n        raise TypeError(p)\n    out[0] = p.__class__(p.multiply(numpy.dot(x, y.T)))\n", "label": 0}
{"function": "\n\ndef wait(self, timeout=900, check_interval_secs=5):\n    'Waits for a deployment to finish\\n\\n        If a deployment completes successfully, True is returned, if it fails\\n        for any reason a DeploymentFailed exception is raised. If the\\n        deployment does not complete within ``timeout`` seconds, a\\n        DeploymentFailed exception is raised.\\n\\n        ``check_interval_secs`` is used to determine the delay between\\n        subsequent checks. The default of 5 seconds is adequate for normal\\n        use.\\n        '\n    _started = datetime.datetime.utcnow()\n    while True:\n        time.sleep(check_interval_secs)\n        if self.check():\n            return True\n        delta = (datetime.datetime.utcnow() - _started)\n        if (delta.total_seconds() > timeout):\n            raise DeploymentFailed(('Timed out: %d seconds' % timeout))\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    return ('Binary Expression: %s %s' % (self.operator, [str(x) for x in self.args]))\n", "label": 0}
{"function": "\n\ndef render_errors(self, errors, request, response):\n    data = {\n        'errors': errors,\n    }\n    if getattr(self, 'on_invalid', False):\n        data = self.on_invalid(data, **self._arguments(self._params_for_on_invalid, request, response))\n    response.status = HTTP_BAD_REQUEST\n    if getattr(self, 'invalid_outputs', False):\n        response.content_type = self.invalid_content_type(request, response)\n        response.data = self.invalid_outputs(data, **self._arguments(self._params_for_invalid_outputs, request, response))\n    else:\n        response.data = self.outputs(data, **self._arguments(self._params_for_outputs, request, response))\n", "label": 0}
{"function": "\n\ndef AppendAdditionalCoefficientBounds(self, inModel):\n    if inModel.baseEquationHasGlobalMultiplierOrDivisor_UsedInExtendedVersions:\n        if (inModel.upperCoefficientBounds != []):\n            inModel.upperCoefficientBounds.append(None)\n        if (inModel.lowerCoefficientBounds != []):\n            inModel.lowerCoefficientBounds.append(None)\n    else:\n        if (inModel.upperCoefficientBounds != []):\n            inModel.upperCoefficientBounds.append(None)\n            inModel.upperCoefficientBounds.append(None)\n        if (inModel.lowerCoefficientBounds != []):\n            inModel.lowerCoefficientBounds.append(None)\n            inModel.lowerCoefficientBounds.append(None)\n", "label": 0}
{"function": "\n\ndef reclassify_java(code_element, scode_reference):\n    reclassified = False\n    if ((scode_reference.snippet is not None) or (code_element is not None)):\n        return reclassified\n    automatic_reclass = set(['method', 'field', 'annotation field', 'enumeration value', 'annotation', 'enumeration'])\n    unknown_kind = cu.get_value(PREFIX_UNKNOWN, UNKNOWN_KEY, gl.get_unknown_kind, None)\n    if (scode_reference.kind_hint.kind in automatic_reclass):\n        scode_reference.kind_hint = unknown_kind\n        reclassified = True\n    elif (scode_reference.child_references.count() == 0):\n        scode_reference.kind_hint = unknown_kind\n        reclassified = True\n    if reclassified:\n        scode_reference.save()\n    return reclassified\n", "label": 0}
{"function": "\n\ndef __call__(self, instance, dictionary, bulkload_state):\n    'Implementation of StatPropertyTypePropertyNameKindPostExport.\\n\\n    See class docstring for more info.\\n\\n    Args:\\n      instance: Input, current entity being exported.\\n      dictionary: Output, dictionary created by property_map transforms.\\n      bulkload_state: Passed bulkload_state.\\n\\n    Returns:\\n      Dictionary--same object as passed in dictionary.\\n    '\n    kind_name = dictionary['kind_name']\n    property_name = dictionary['property_name']\n    property_type = dictionary['property_type']\n    if kind_name.startswith('__'):\n        return None\n    if (property_type == 'NULL'):\n        return None\n    property_key = (kind_name, property_name)\n    if (kind_name != self.last_seen):\n        self.last_seen = kind_name\n        separator = (KIND_PREAMBLE % dictionary)\n    elif (property_key in self.seen_properties):\n        separator = (PROPERTY_DUPE_WARNING % dictionary)\n    else:\n        separator = ''\n    self.seen_properties[property_key] = (self.seen_properties.get(property_key, 0) + 1)\n    dictionary['separator'] = separator\n    return dictionary\n", "label": 0}
{"function": "\n\ndef unstage(self, repo, files):\n    if self.no_commits(repo):\n        return self.git((['rm', '--cached', '--'] + files), cwd=repo)\n    return self.git((['reset', '-q', 'HEAD', '--'] + files), cwd=repo)\n", "label": 0}
{"function": "\n\ndef getwithinrange(value, min=0, max=255):\n    '\\n    Returns value if it is between the min and max number arguments. If value is greater than max, then max is returned. If value is less than min, then min is returned. If min and/or max is not specified, then the value is not limited in that direction.\\n    '\n    if ((min is not None) and (value < min)):\n        return min\n    elif ((max is not None) and (value > max)):\n        return max\n    else:\n        return value\n", "label": 0}
{"function": "\n\ndef _compute_transitive_deps_by_target(self):\n    'Map from target to all the targets it depends on, transitively.'\n    sorted_targets = reversed(sort_targets(self.context.targets()))\n    transitive_deps_by_target = defaultdict(set)\n    for target in sorted_targets:\n        transitive_deps = set()\n        for dep in target.dependencies:\n            transitive_deps.update(transitive_deps_by_target.get(dep, []))\n            transitive_deps.add(dep)\n        if hasattr(target, 'java_sources'):\n            for java_source_target in target.java_sources:\n                for transitive_dep in java_source_target.dependencies:\n                    transitive_deps_by_target[java_source_target].add(transitive_dep)\n        transitive_deps_by_target[target] = transitive_deps\n    return transitive_deps_by_target\n", "label": 0}
{"function": "\n\n@classmethod\ndef to_path(cls, f):\n    if isinstance(f, SerializableFunction):\n        f.dumps_simple()\n    else:\n        return ('%s.%s' % (f.__module__, f.__name__))\n", "label": 0}
{"function": "\n\ndef spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n    'Pads the 2nd and 3rd dimensions of a 4D tensor\\n    with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\\n    '\n    if (dim_ordering == 'th'):\n        pattern = [[0, 0], [0, 0], [padding[0], padding[0]], [padding[1], padding[1]]]\n    else:\n        pattern = [[0, 0], [padding[0], padding[0]], [padding[1], padding[1]], [0, 0]]\n    return tf.pad(x, pattern)\n", "label": 0}
{"function": "\n\ndef decodeValue(self, value):\n    'Return True, False or -1 decoded from ``value``.'\n    if (value == '?'):\n        return (- 1)\n    if (value in 'NnFf '):\n        return False\n    if (value in 'YyTt'):\n        return True\n    raise ValueError(('[%s] Invalid logical value %r' % (self.name, value)))\n", "label": 0}
{"function": "\n\ndef action_get_by_request_id(self, ctxt, instance, request_id):\n    if (not instance['cell_name']):\n        raise exception.InstanceUnknownCell(instance_uuid=instance['uuid'])\n    cctxt = self.client.prepare(version='1.5')\n    return cctxt.call(ctxt, 'action_get_by_request_id', cell_name=instance['cell_name'], instance_uuid=instance['uuid'], request_id=request_id)\n", "label": 0}
{"function": "\n\ndef purge_content(self, account_id, urls):\n    'Purges one or more URLs from the CDN edge nodes.\\n\\n        :param int account_id: the CDN account ID from which content should\\n                               be purged.\\n        :param urls: a string or a list of strings representing the CDN URLs\\n                     that should be purged.\\n        :returns: true if all purge requests were successfully submitted;\\n                  otherwise, returns the first error encountered.\\n        '\n    if isinstance(urls, six.string_types):\n        urls = [urls]\n    for i in range(0, len(urls), MAX_URLS_PER_PURGE):\n        result = self.account.purgeCache(urls[i:(i + MAX_URLS_PER_PURGE)], id=account_id)\n        if (not result):\n            return result\n    return True\n", "label": 0}
{"function": "\n\ndef get_port_configuration(self, port):\n    conf = []\n    if port.shutdown:\n        conf.append('shutdown')\n    if port.description:\n        conf.append(\"description '{}'\".format(port.description))\n    if (port.mode and (port.mode != 'access')):\n        conf.append('switchport mode {}'.format(port.mode))\n    if port.access_vlan:\n        conf.append('switchport access vlan {}'.format(port.access_vlan))\n    if port.trunk_native_vlan:\n        conf.append('switchport general pvid {}'.format(port.trunk_native_vlan))\n    if port.trunk_vlans:\n        conf.append('switchport {} allowed vlan add {}'.format(port.mode, to_vlan_ranges(port.trunk_vlans)))\n    if (port.spanning_tree is False):\n        conf.append('spanning-tree disable')\n    if port.spanning_tree_portfast:\n        conf.append('spanning-tree portfast')\n    if (port.lldp_transmit is False):\n        conf.append('no lldp transmit')\n    if (port.lldp_receive is False):\n        conf.append('no lldp receive')\n    if (port.lldp_med_transmit_capabilities is False):\n        conf.append('no lldp med transmit-tlv capabilities')\n    if (port.lldp_med_transmit_network_policy is False):\n        conf.append('no lldp med transmit-tlv network-policy')\n    return conf\n", "label": 1}
{"function": "\n\ndef _format_lines(self, tokensource):\n    for (tag, line) in HtmlFormatter._format_lines(self, tokensource):\n        if (tag == 1):\n            line = ('<span class=line>%s</span>' % line)\n        (yield (tag, line))\n", "label": 0}
{"function": "\n\ndef directiveOrStatement(self, argstr, h):\n    i = self.skipSpace(argstr, h)\n    if (i < 0):\n        return i\n    j = self.graph(argstr, i)\n    if (j >= 0):\n        return j\n    j = self.sparqlDirective(argstr, i)\n    if (j >= 0):\n        return j\n    j = self.directive(argstr, i)\n    if (j >= 0):\n        return self.checkDot(argstr, j)\n    j = self.statement(argstr, i)\n    if (j >= 0):\n        return self.checkDot(argstr, j)\n    return j\n", "label": 0}
{"function": "\n\ndef activate(self, X):\n    if self.is_elementwise():\n        raise Exception('No identity nodes allowed.')\n    return X\n", "label": 0}
{"function": "\n\ndef encode_qs_params(self, request):\n    query = port.urlparse(request.uri).query\n    params = port.parse_qsl(query, True)\n    return tuple(((self.encode(key), self.encode(val)) for (key, val) in params))\n", "label": 0}
{"function": "\n\ndef _get_value(self, entity):\n    if (entity._projection and (self._name in entity._projection)):\n        return super(ComputedProperty, self)._get_value(entity)\n    value = self._func(entity)\n    self._store_value(entity, value)\n    return value\n", "label": 0}
{"function": "\n\ndef utcoffset(self, dt):\n    if self._isdst(dt):\n        return DSTOFFSET\n    else:\n        return STDOFFSET\n", "label": 0}
{"function": "\n\ndef InjectScript(content, content_type, script_to_inject):\n    \"Inject |script_to_inject| into |content| if |content_type| is 'text/html'.\\n\\n  Inject |script_to_inject| into |content| immediately after <head>, <html> or\\n  <!doctype html>, if one of them is found. Otherwise, inject at the beginning.\\n\\n  Returns:\\n    content, already_injected\\n    |content| is the new content if script is injected, otherwise the original.\\n    |already_injected| indicates if |script_to_inject| is already in |content|.\\n  \"\n    already_injected = False\n    if (content_type and (content_type == 'text/html')):\n        already_injected = ((not content) or (script_to_inject in content))\n        if (not already_injected):\n\n            def InsertScriptAfter(matchobj):\n                return ('%s<script>%s</script>' % (matchobj.group(0), script_to_inject))\n            (content, is_injected) = HEAD_RE.subn(InsertScriptAfter, content, 1)\n            if (not is_injected):\n                (content, is_injected) = HTML_RE.subn(InsertScriptAfter, content, 1)\n            if (not is_injected):\n                (content, is_injected) = DOCTYPE_RE.subn(InsertScriptAfter, content, 1)\n            if (not is_injected):\n                content = ('<script>%s</script>%s' % (script_to_inject, content))\n                logging.warning('Inject at the very beginning, because no tag of <head>, <html> or <!doctype html> is found.')\n    return (content, already_injected)\n", "label": 0}
{"function": "\n\ndef _run_main(self, args, parsed_globals):\n    self.handle_args(args)\n    self.setup_services(parsed_globals)\n    self._call()\n    if ((self._invalid_digests > 0) or (self._invalid_logs > 0)):\n        return 1\n    return 0\n", "label": 0}
{"function": "\n\ndef _propsetcursory(self, value):\n    \"\\n        Set the cursor's y coordinate.\\n\\n        value - The new y coordinate. A negative value can be used to specify\\n        the y coordinate in terms of its relative distance to the bottom border\\n        of the surface. No operation will be performed if value is greater than\\n        or equal to the height of the surface.\\n        \"\n    y = int(value)\n    if ((y >= self._height) or (y <= (- self._height))):\n        return\n    if (y < 0):\n        y = (self._height + y)\n    self._cursory = y\n", "label": 0}
{"function": "\n\ndef _Tuple(child, ctx):\n    if (len(child) > 1):\n        return Tuple(child, ctx)\n    else:\n        return child[0]\n", "label": 0}
{"function": "\n\ndef _load_aggregates(self, conn):\n    for (name, (klass, num_params)) in self._aggregates.items():\n\n        def make_aggregate():\n            instance = klass()\n            return (instance, instance.step, instance.finalize)\n        conn.createaggregatefunction(name, make_aggregate)\n", "label": 0}
{"function": "\n\n@classmethod\ndef validate(cls, mapper_spec):\n    'Validates mapper specification.\\n\\n    Args:\\n      mapper_spec: an instance of model.MapperSpec to validate.\\n    '\n    if (mapper_spec.output_writer_class() != cls):\n        raise errors.BadWriterParamsError('Output writer class mismatch')\n    output_sharding = cls._get_output_sharding(mapper_spec=mapper_spec)\n    if ((output_sharding != cls.OUTPUT_SHARDING_NONE) and (output_sharding != cls.OUTPUT_SHARDING_INPUT_SHARDS)):\n        raise errors.BadWriterParamsError(('Invalid output_sharding value: %s' % output_sharding))\n    params = _get_params(mapper_spec)\n    filesystem = cls._get_filesystem(mapper_spec)\n    if (filesystem not in files.FILESYSTEMS):\n        raise errors.BadWriterParamsError((\"Filesystem '%s' is not supported. Should be one of %s\" % (filesystem, files.FILESYSTEMS)))\n    if (filesystem == files.GS_FILESYSTEM):\n        if (not (cls.GS_BUCKET_NAME_PARAM in params)):\n            raise errors.BadWriterParamsError(('%s is required for Google store filesystem' % cls.GS_BUCKET_NAME_PARAM))\n    elif (params.get(cls.GS_BUCKET_NAME_PARAM) is not None):\n        raise errors.BadWriterParamsError(('%s can only be provided for Google store filesystem' % cls.GS_BUCKET_NAME_PARAM))\n", "label": 0}
{"function": "\n\ndef validate(self):\n    initial_validation = super(RegisterForm, self).validate()\n    if (not initial_validation):\n        return False\n    user = User.query.filter_by(email=self.email.data).first()\n    if user:\n        self.email.errors.append('Email already registered')\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef add_examples(list_of_examples, path, example_type=None, skip=None):\n    example_path = join(example_dir, path)\n    if (skip is not None):\n        skip = set(skip)\n    for f in os.listdir(example_path):\n        flags = 0\n        if f.startswith(('_', '.')):\n            continue\n        elif f.endswith('.py'):\n            if (example_type is not None):\n                flags |= example_type\n            elif (('server' in f) or ('animate' in f)):\n                flags |= Flags.server\n            else:\n                flags |= Flags.file\n        elif f.endswith('.ipynb'):\n            flags |= Flags.notebook\n        else:\n            continue\n        if ('animate' in f):\n            flags |= Flags.animated\n            if (flags & Flags.file):\n                raise ValueError(\"file examples can't be animated\")\n        if (skip and (f in skip)):\n            flags |= Flags.skip\n        list_of_examples.append((join(example_path, f), flags))\n    return list_of_examples\n", "label": 1}
{"function": "\n\ndef _send_request(payload=None, session=None):\n    '\\n    Using requests we send a SOAP envelope directly to the\\n    vCenter API to reset an alarm to the green state.\\n\\n    :param payload:\\n    :param session:\\n    :return:\\n    '\n    stub = session\n    host_port = stub.host\n    url = 'https://{0}/sdk'.format(host_port)\n    logging.debug('Sending {0} to {1}'.format(payload, url))\n    res = requests.post(url=url, data=payload, headers={\n        'Cookie': stub.cookie,\n        'SOAPAction': 'urn:vim25',\n        'Content-Type': 'application/xml',\n    }, verify=False)\n    if (res.status_code != 200):\n        logging.debug('Failed to reset alarm. HTTP Status: {0}'.format(res.status_code))\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef _parse_sync_args(self, args):\n    parent = super(ContainerReplicatorRpc, self)\n    remote_info = parent._parse_sync_args(args)\n    if (len(args) > 9):\n        remote_info['status_changed_at'] = args[7]\n        remote_info['count'] = args[8]\n        remote_info['storage_policy_index'] = args[9]\n    return remote_info\n", "label": 0}
{"function": "\n\ndef __init__(self, name, size, bold=False, italic=False, dpi=None):\n    super(FreeTypeFont, self).__init__()\n    if (dpi is None):\n        dpi = 96\n    lname = ((name and name.lower()) or '')\n    if ((lname, bold, italic) in self._memory_fonts):\n        font = self._memory_fonts[(lname, bold, italic)]\n        self._set_face(font.face, size, dpi)\n        return\n    ft_library = ft_get_library()\n    match = self.get_fontconfig_match(name, size, bold, italic)\n    if (not match):\n        raise base.FontException(('Could not match font \"%s\"' % name))\n    f = FT_Face()\n    if (fontconfig.FcPatternGetFTFace(match, FC_FT_FACE, 0, byref(f)) != 0):\n        value = FcValue()\n        result = fontconfig.FcPatternGet(match, FC_FILE, 0, byref(value))\n        if (result != 0):\n            raise base.FontException(('No filename or FT face for \"%s\"' % name))\n        result = FT_New_Face(ft_library, value.u.s, 0, byref(f))\n        if result:\n            raise base.FontException(('Could not load \"%s\": %d' % (name, result)))\n    fontconfig.FcPatternDestroy(match)\n    self._set_face(f, size, dpi)\n", "label": 1}
{"function": "\n\ndef connect(self, initialize=True):\n    if initialize:\n        self.initialize()\n        self.add_source_files_to_schema()\n    return sqlite3.connect(self.app.config['SHELF_SQLITE_FILEPATH'])\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    if (type(other) == ChronykDelta):\n        newtimest = (self.timestamp() + other.seconds)\n        return Chronyk(newtimest, timezone=self.timezone)\n    if (type(other) in [int, float]):\n        newtimest = (self.timestamp() + other)\n        return Chronyk(newtimest, timezone=self.timezone)\n    return NotImplemented\n", "label": 0}
{"function": "\n\ndef download(url, filename):\n    if (not os.path.exists('data')):\n        os.makedirs('data')\n    out_file = os.path.join('data', filename)\n    if (not os.path.isfile(out_file)):\n        urllib.urlretrieve(url, out_file)\n", "label": 0}
{"function": "\n\ndef process_payload_files(payload, github_auth):\n    ' Return a dictionary of file paths to raw JSON contents and file IDs.\\n    '\n    if (('action' in payload) and ('pull_request' in payload)):\n        return process_pullrequest_payload_files(payload, github_auth)\n    if (('commits' in payload) and ('head_commit' in payload)):\n        return process_pushevent_payload_files(payload, github_auth)\n    raise ValueError('Unintelligible webhook payload')\n", "label": 0}
{"function": "\n\ndef __get__(self, instance, instance_type=None):\n    if (instance is not None):\n        return self.rel_model.select().where((self.field == getattr(instance, self.field.to_field.name)))\n    return self\n", "label": 0}
{"function": "\n\ndef bulk_download_url(self, **options):\n    url = self._bulk_download_path()\n    url = ((ApiConfig.api_base + '/') + url)\n    if ('params' not in options):\n        options['params'] = {\n            \n        }\n    if ApiConfig.api_key:\n        options['params']['api_key'] = ApiConfig.api_key\n    if ApiConfig.api_version:\n        options['params']['api_version'] = ApiConfig.api_version\n    if list(options.keys()):\n        url += ('?' + urlencode(options['params']))\n    return url\n", "label": 0}
{"function": "\n\n@classmethod\ndef by_foreign_id(cls, foreign_id):\n    if (foreign_id is None):\n        return\n    return cls.all().filter_by(foreign_id=foreign_id).first()\n", "label": 0}
{"function": "\n\ndef __read_datasource(jboss_config, name, profile=None):\n    operation = '/subsystem=datasources/data-source=\"{name}\":read-resource'.format(name=name)\n    if (profile is not None):\n        operation = ('/profile=\"{profile}\"'.format(profile=profile) + operation)\n    operation_result = __salt__['jboss7_cli.run_operation'](jboss_config, operation)\n    return operation_result\n", "label": 0}
{"function": "\n\n@mock.patch('traceback.format_exception', wraps=fake_format_exception)\ndef test_frame_stripping(self, mock_format_exception):\n    'On assertion error, testify strips head and tail frame which originate from testify.'\n    test_result = TestResult((lambda : 'wat'), runner_id='foo!')\n    test_result.start()\n    root_tb = tb = mock.Mock()\n    testify_frames = [True, True, False, True, False, True, True]\n    for testify_frame in testify_frames:\n        tb.tb_next = mock.Mock()\n        tb = tb.tb_next\n        f_globals = ({\n            '__testify': True,\n        } if testify_frame else {\n            \n        })\n        tb.configure_mock(**{\n            'tb_frame.f_globals': f_globals,\n        })\n    tb.tb_next = None\n    tb = root_tb.tb_next\n    test_result.end_in_failure((AssertionError, 'wat', tb))\n    formatted = test_result.format_exception_info()\n    assert_equal(formatted, 'Traceback: AssertionError\\n')\n    mock_format_exception.assert_called_with(AssertionError, 'wat', tb.tb_next.tb_next, 3)\n", "label": 0}
{"function": "\n\ndef _debugdirtyFn(self, x, y):\n    if self._screendirty[x][y]:\n        return 'D'\n    else:\n        return '.'\n", "label": 0}
{"function": "\n\ndef sanitize_ohlc(open_, high, low, close):\n    if (low > open_):\n        low = open_\n    if (low > close):\n        low = close\n    if (high < open_):\n        high = open_\n    if (high < close):\n        high = close\n    return (open_, high, low, close)\n", "label": 0}
{"function": "\n\ndef order_modified_iter(cursor, trim, sentinel):\n    \"\\n    Yields blocks of rows from a cursor. We use this iterator in the special\\n    case when extra output columns have been added to support ordering\\n    requirements. We must trim those extra columns before anything else can use\\n    the results, since they're only needed to make the SQL valid.\\n    \"\n    for rows in iter((lambda : cursor.fetchmany(GET_ITERATOR_CHUNK_SIZE)), sentinel):\n        (yield [r[:(- trim)] for r in rows])\n", "label": 0}
{"function": "\n\ndef test_args(self, *args, **kwargs):\n    args = [(arg if isinstance(arg, str) else arg.encode()) for arg in args]\n    return ('%s\\n%s' % (repr(tuple(args)), repr(kwargs)))\n", "label": 0}
{"function": "\n\ndef groupsizes_to_partition(*gsizes):\n    '\\n    >>> from logpy.assoccomm import groupsizes_to_partition\\n    >>> groupsizes_to_partition(2, 3)\\n    [[0, 1], [2, 3, 4]]\\n    '\n    idx = 0\n    part = []\n    for gs in gsizes:\n        l = []\n        for i in range(gs):\n            l.append(idx)\n            idx += 1\n        part.append(l)\n    return part\n", "label": 0}
{"function": "\n\ndef WithArgs(self, **kwargs):\n    'Creates a new Measurement, see openhtf.PhaseInfo.WithArgs.'\n    new_meas = mutablerecords.CopyRecord(self)\n    if ('{' in new_meas.name):\n        formatter = (lambda x: (x.format(**kwargs) if x else x))\n    else:\n        formatter = (lambda x: ((x % kwargs) if x else x))\n    new_meas.name = formatter(self.name)\n    new_meas.docstring = formatter(self.docstring)\n    return new_meas\n", "label": 0}
{"function": "\n\ndef _replace_datalayout(llvmir):\n    '\\n    Find the line containing the datalayout and replace it\\n    '\n    lines = llvmir.splitlines()\n    for (i, ln) in enumerate(lines):\n        if ln.startswith('target datalayout'):\n            tmp = 'target datalayout = \"{0}\"'\n            lines[i] = tmp.format(default_data_layout)\n            break\n    return '\\n'.join(lines)\n", "label": 0}
{"function": "\n\ndef NamedTemporaryFile23(*args, **kwargs):\n    'Works exactly as a wrapper to tempfile.NamedTemporaryFile except that\\n       in python2.x, it excludes the \"encoding\" parameter when provided.'\n    if (sys.version_info[0] == 2):\n        kwargs.pop('encoding', None)\n    return NamedTemporaryFile(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef _fill_argtypes(self):\n    '\\n        Get dtypes\\n        '\n    for (i, ary) in enumerate(self.arrays):\n        if (ary is not None):\n            self.argtypes[i] = ary.dtype\n", "label": 0}
{"function": "\n\ndef to_event(self, notification_body, condenser=None):\n    if (condenser is None):\n        condenser = DictionaryCondenser()\n    event_type = notification_body['event_type']\n    message_id = notification_body['message_id']\n    edef = None\n    for d in self.definitions:\n        if d.match_type(event_type):\n            edef = d\n            break\n    if (edef is None):\n        msg = ('Dropping Notification %(type)s (uuid:%(msgid)s)' % dict(type=event_type, msgid=message_id))\n        if self.catchall:\n            logger.error(msg)\n        else:\n            logger.debug(msg)\n        return None\n    return edef.to_event(notification_body, condenser)\n", "label": 0}
{"function": "\n\ndef get_output_shape_for(self, input_shape):\n    if (self._output_shape is None):\n        if (K._BACKEND == 'tensorflow'):\n            if (type(input_shape) is list):\n                xs = [K.placeholder(shape=shape) for shape in input_shape]\n                x = self.call(xs)\n            else:\n                x = K.placeholder(shape=input_shape)\n                x = self.call(x)\n            if (type(x) is list):\n                return [K.int_shape(x_elem) for x_elem in x]\n            else:\n                return K.int_shape(x)\n        return input_shape\n    elif (type(self._output_shape) in {tuple, list}):\n        nb_samples = (input_shape[0] if input_shape else None)\n        return ((nb_samples,) + tuple(self._output_shape))\n    else:\n        shape = self._output_shape(input_shape)\n        if (type(shape) not in {list, tuple}):\n            raise Exception('output_shape function must return a tuple')\n        return tuple(shape)\n", "label": 1}
{"function": "\n\ndef iter_dist_files(dist):\n    if dist.has_metadata('RECORD'):\n        for line in dist.get_metadata_lines('RECORD'):\n            line = line.split(',')[0]\n            if line.endswith('.pyc'):\n                continue\n            (yield os.path.normpath(os.path.join(dist.location, line)))\n    elif dist.has_metadata('installed-files.txt'):\n        for line in dist.get_metadata_lines('installed-files.txt'):\n            if line.endswith('.pyc'):\n                continue\n            (yield os.path.normpath(os.path.join(dist.location, dist.egg_info, line)))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef to_model(notify_api_object):\n    if notify_api_object.get('on-success', None):\n        on_success = NotificationsHelper._to_model_sub_schema(notify_api_object['on-success'])\n    else:\n        on_success = None\n    if notify_api_object.get('on-complete', None):\n        on_complete = NotificationsHelper._to_model_sub_schema(notify_api_object['on-complete'])\n    else:\n        on_complete = None\n    if notify_api_object.get('on-failure', None):\n        on_failure = NotificationsHelper._to_model_sub_schema(notify_api_object['on-failure'])\n    else:\n        on_failure = None\n    model = NotificationSchema(on_success=on_success, on_failure=on_failure, on_complete=on_complete)\n    return model\n", "label": 0}
{"function": "\n\ndef delete_association(self, association=''):\n    '\\n        Delete an association between two IonObjects\\n        @param association  Association object, association id or 3-list of [subject, predicate, object]\\n        '\n    if ((type(association) in (list, tuple)) and (len(association) == 3)):\n        (subject, predicate, obj) = association\n        assoc_id_list = self.find_associations(subject=subject, predicate=predicate, object=obj, id_only=True)\n        success = True\n        for aid in assoc_id_list:\n            success = (success and self.rr_store.delete(aid, object_type='Association'))\n        return success\n    else:\n        return self.rr_store.delete(association, object_type='Association')\n", "label": 0}
{"function": "\n\ndef _clean_join_name(opposite_side_colnames, suffix, c):\n    if (c.name not in opposite_side_colnames):\n        return c\n    else:\n        return c.label((c.name + suffix))\n", "label": 0}
{"function": "\n\ndef results(cl):\n    for res in cl.result_list:\n        (yield list(items_for_result(cl, res)))\n", "label": 0}
{"function": "\n\ndef get_URL(self, obj, logfile=None, cookiefile=None, **kwargs):\n    'Simple interface for making requests with uniform headers.\\n\\n        If you supply a string or URL object, it is taken as the URL to\\n        fetch. If you supply a list, multiple requests are issued. Returns\\n        two lists, as the perform() method does.\\n\\n        '\n    obj_t = type(obj)\n    if issubclass(obj_t, (str, urlparse.UniversalResourceLocator)):\n        r = HTTPRequest(obj, **kwargs)\n        resp = r.perform(logfile, cookiefile)\n        if resp.error:\n            return ([], [resp])\n        else:\n            return ([resp], [])\n    elif issubclass(obj_t, HTTPRequest):\n        resp = obj.perform(logfile)\n        if resp.error:\n            return ([], [resp])\n        else:\n            return ([resp], [])\n    else:\n        for url in iter(obj):\n            r = HTTPRequest(url, **kwargs)\n            self._requests.append(r)\n            return self.perform(logfile)\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    super(S3ObjectStoreTestCase, self).setUp()\n    options.options.domain = 'goviewfinder.com'\n    secrets.InitSecretsForTest()\n    self.object_store = S3ObjectStore('test-goviewfinder-com')\n    self.key = ('test/hello%d' % random.randint(1, 1000000))\n    self.listkey = ('test/list%d' % random.randint(1, 1000000))\n    self.listkeyA = '/'.join((self.listkey, 'a'))\n    self.listkeyB = '/'.join((self.listkey, 'b'))\n    self.listitems = [('item%d' % i) for i in range(0, 5)]\n    self.meter = counters.Meter(counters.counters.viewfinder.s3)\n    self.meter_start = time.time()\n", "label": 0}
{"function": "\n\ndef is_verb(self, word):\n    for n in self.verbs:\n        if word.startswith(n):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef streamStarted(self, rootElement):\n    '\\n        Called by the stream when it has started.\\n\\n        This examines the default namespace of the incoming stream and whether\\n        there is a requested hostname for the component. Then it generates a\\n        stream identifier, sends a response header and adds an observer for\\n        the first incoming element, triggering L{onElement}.\\n        '\n    xmlstream.ListenAuthenticator.streamStarted(self, rootElement)\n    if (rootElement.defaultUri != self.namespace):\n        exc = error.StreamError('invalid-namespace')\n        self.xmlstream.sendStreamError(exc)\n        return\n    if (not self.xmlstream.thisEntity):\n        exc = error.StreamError('improper-addressing')\n        self.xmlstream.sendStreamError(exc)\n        return\n    self.xmlstream.sendHeader()\n    self.xmlstream.addOnetimeObserver('/*', self.onElement)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _build_initializer_body_with_properties(properties_to_set):\n    initializer_body = ''\n    initializer_body += '\\n'.join(['    project.set_property(\"{0}\", \"{1}\")'.format(k, v) for (k, v) in properties_to_set])\n    if (not initializer_body):\n        initializer_body += '    pass'\n    return initializer_body\n", "label": 0}
{"function": "\n\ndef setLineCap(self, lineCap):\n    if (lineCap.strip().lower == GWTCanvasConsts.BUTT):\n        self.context.lineCap = BUTT\n    else:\n        self.context.lineCap = lineCap\n", "label": 0}
{"function": "\n\ndef normalize(self):\n    if (not self.words):\n        return NullQuery\n    if (len(self.words) == 1):\n        return Term(self.fieldname, self.words[0])\n    return self.__class__(self.fieldname, [w for w in self.words if (w is not None)], slop=self.slop, boost=self.boost)\n", "label": 0}
{"function": "\n\ndef _do_layout_node(self, node, level, y):\n    if (self.hide_root and (node is self.root)):\n        level -= 1\n    else:\n        node.x = ((self.x + self.indent_start) + (level * self.indent_level))\n        node.top = y\n        if node.size_hint_x:\n            node.width = ((self.width - (node.x - self.x)) * node.size_hint_x)\n        y -= node.height\n        if (not node.is_open):\n            return y\n    for cnode in node.nodes:\n        y = self._do_layout_node(cnode, (level + 1), y)\n    return y\n", "label": 0}
{"function": "\n\ndef update(self, validate=False):\n    \"\\n        Update the DB instance's status information by making a call to fetch\\n        the current instance attributes from the service.\\n\\n        :type validate: bool\\n        :param validate: By default, if EC2 returns no data about the\\n                         instance the update method returns quietly.  If\\n                         the validate param is True, however, it will\\n                         raise a ValueError exception if no data is\\n                         returned from EC2.\\n        \"\n    rs = self.connection.get_all_dbinstances(self.id)\n    if (len(rs) > 0):\n        for i in rs:\n            if (i.id == self.id):\n                self.__dict__.update(i.__dict__)\n    elif validate:\n        raise ValueError(('%s is not a valid Instance ID' % self.id))\n    return self.status\n", "label": 0}
{"function": "\n\ndef __setitem__(self, column, value):\n    for i in xrange(len(self)):\n        if (self._keys[i] == column):\n            self._values[i] = value\n            return\n    self._keys.append(column)\n    self._values.append(value)\n    self._BuildIndex()\n", "label": 0}
{"function": "\n\ndef build_es_query(self, case_type=None, afilter=None, status=None):\n\n    def _domain_term():\n        return {\n            'term': {\n                'domain.exact': self.domain,\n            },\n        }\n    subterms = ([_domain_term(), afilter] if afilter else [_domain_term()])\n    if case_type:\n        subterms.append({\n            'term': {\n                'type.exact': case_type,\n            },\n        })\n    if status:\n        subterms.append({\n            'term': {\n                'closed': (status == 'closed'),\n            },\n        })\n    es_query = {\n        'query': {\n            'filtered': {\n                'query': {\n                    'match_all': {\n                        \n                    },\n                },\n                'filter': {\n                    'and': subterms,\n                },\n            },\n        },\n        'sort': self.get_sorting_block(),\n        'from': self.pagination.start,\n        'size': self.pagination.count,\n    }\n    return es_query\n", "label": 0}
{"function": "\n\ndef sneak(self, sneak=True):\n    self._entity_action((constants.ENTITY_ACTION_SNEAK if sneak else constants.ENTITY_ACTION_UNSNEAK))\n    self.sneaking = sneak\n", "label": 0}
{"function": "\n\ndef test_nils_20Feb04(self):\n    n = 2\n    A = (random([n, n]) + (random([n, n]) * 1j))\n    X = zeros((n, n), 'D')\n    Ainv = inv(A)\n    R = (identity(n) + (identity(n) * 0j))\n    for i in arange(0, n):\n        r = R[:, i]\n        X[:, i] = solve(A, r)\n    assert_array_almost_equal(X, Ainv)\n", "label": 0}
{"function": "\n\n@task\ndef sync_project(project_pk):\n    'Syncronize all dependencies of a project.\\n\\n    This syncronizes all package versions and creates proper\\n    log entries on updates as well as starts the notification\\n    routing.\\n    '\n    project = Project.objects.get(pk=project_pk)\n    log_entries = []\n    for dependency in project.dependencies.all():\n        package = dependency.package\n        package.sync_versions()\n        versions = list(package.versions.values_list('version', flat=True))\n        if versions:\n            versions.sort(key=LooseVersion)\n            if (LooseVersion(dependency.version) >= LooseVersion(versions[(- 1)])):\n                ProjectDependency.objects.filter(pk=dependency.pk).update(update=None)\n                continue\n            pv = PackageVersion.objects.get(package=package, version=versions[(- 1)])\n            if (pv.pk == dependency.update_id):\n                continue\n            dependency.update = pv\n            dependency.save()\n            tz = timezone.utc\n            since = str(timezone.make_naive(pv.release_date, tz))\n            log_entries.append(Log(type='project_dependency', action='update_available', project=project, package=package, data={\n                'version': versions[(- 1)],\n                'since': since,\n            }))\n    if log_entries:\n        Log.objects.bulk_create(log_entries)\n        send_notifications(project, log_entries)\n", "label": 0}
{"function": "\n\ndef summariseList(lst):\n    '\\n        Takes a sorted list of numbers, and returns a summary.\\n        Eg. [1, 2, 3, 4, 9] -> [(1, 4), 9]\\n            [1, 2, 3, 7, 8, 9] -> [(1, 3), (7, 9)]\\n    '\n    if (len(lst) < 2):\n        return lst\n    ranges = []\n    start = 0\n    for i in range(1, len(lst)):\n        if ((lst[i] - lst[(i - 1)]) > 1):\n            if ((i - 1) == start):\n                start = i\n                ranges.append(lst[(i - 1)])\n            else:\n                ranges.append((lst[start], lst[(i - 1)]))\n                start = i\n    if (lst[start] == lst[i]):\n        ranges.append(lst[i])\n    else:\n        ranges.append((lst[start], lst[i]))\n    return ranges\n", "label": 0}
{"function": "\n\ndef set_etag_header(self):\n    \"Sets the response's Etag header using ``self.compute_etag()``.\\n\\n        Note: no header will be set if ``compute_etag()`` returns ``None``.\\n\\n        This method is called automatically when the request is finished.\\n        \"\n    etag = self.compute_etag()\n    if (etag is not None):\n        self.set_header('Etag', etag)\n", "label": 0}
{"function": "\n\ndef getStND(self, x, mue=0.0, sig=1.0):\n    x = ((x - mue) / sig)\n    if (abs(x) >= 4.0):\n        return 1e-09\n    x = int((((x + 4.0) / 8.0) * 1000))\n    return (stND[x] / sig)\n", "label": 0}
{"function": "\n\ndef is_alpha_number(number):\n    'Checks if the number is a valid vanity (alpha) number such as 800\\n    MICROSOFT. A valid vanity number will start with at least 3 digits and\\n    will have three or more alpha characters. This does not do region-specific\\n    checks - to work out if this number is actually valid for a region, it\\n    should be parsed and methods such as is_possible_number_with_reason() and\\n    is_valid_number() should be used.\\n\\n    Arguments:\\n    number -- the number that needs to be checked\\n\\n    Returns True if the number is a valid vanity number\\n    '\n    if (not _is_viable_phone_number(number)):\n        return False\n    (extension, stripped_number) = _maybe_strip_extension(number)\n    return bool(fullmatch(_VALID_ALPHA_PHONE_PATTERN, stripped_number))\n", "label": 0}
{"function": "\n\ndef test_page_paths_keys_exist_in_static(self):\n    static = self.site.static()[0]\n    for old_key in ('full', 'full-build'):\n        self._paths_key_exists(static, old_key)\n", "label": 0}
{"function": "\n\ndef __setattr__(self, key, value):\n    if key.startswith('_'):\n        object.__setattr__(self, key, value)\n    else:\n        self[key] = value\n", "label": 0}
{"function": "\n\ndef _key(self, obj):\n    memo_key = id(obj)\n    if (memo_key in self._repr_memo):\n        return self._repr_memo[memo_key]\n    result = self._format(obj)\n    self._repr_memo[memo_key] = result\n    return result\n", "label": 0}
{"function": "\n\ndef stop(self):\n    ' Stop recording S3Log messages (and return the messages) '\n    handler = self.handler\n    if (handler is not None):\n        logger = logging.getLogger(__name__)\n        logger.removeHandler(handler)\n        handler.close()\n        self.handler = None\n    strbuf = self.strbuf\n    if (strbuf is not None):\n        return strbuf.getvalue()\n    else:\n        return ''\n", "label": 0}
{"function": "\n\ndef __init__(self, host, username='guest', password='guest', vhost='/', port=5672, heartbeat=0, io_loop=None):\n    self.host = host\n    self.port = port\n    self.username = username\n    self.password = password\n    self.vhost = vhost\n    self.heartbeat = heartbeat\n    self.last_received_frame = None\n    self.frame_max = 0\n    self.io_loop = (io_loop or IOLoop.instance())\n    self.stream = None\n    self.channels = {\n        0: self,\n    }\n    self.last_channel_id = 0\n    self.channel_id = 0\n    self.on_connect = None\n    self.on_disconnect = None\n    self.on_error = None\n    self._close_callback = None\n    self._frame_count = 0\n    super(Connection, self).__init__(connection=self)\n", "label": 0}
{"function": "\n\ndef add_field_def(self, field_def):\n    self._check()\n    if (self._index != field_def.index):\n        raise ValueError('Invalid field index')\n    self._index += 1\n    self._field_defs.append(field_def)\n    return self\n", "label": 0}
{"function": "\n\ndef _format_call(translator, func, *args):\n    formatted_args = []\n    for arg in args:\n        fmt_arg = translator.translate(arg)\n        formatted_args.append(fmt_arg)\n    return '{0!s}({1!s})'.format(func, ', '.join(formatted_args))\n", "label": 0}
{"function": "\n\ndef query(self, data=None):\n    if (len(self.job_queue) > 1000):\n        LOGGER.debug(('Skipping query. %s jobs already active.' % len(self.job_queue)))\n        return\n    if self.querying_for_jobs:\n        LOGGER.debug('Skipping query. Already querying for jobs.')\n        return\n    self.querying_for_jobs = True\n    if ((self.uuid_limits['start'] is None) and (self.uuid_limits['end'] is not None)):\n        uuid_limit_clause = (\"AND itemName() < '%s'\" % self.uuid_limits['end'])\n    elif ((self.uuid_limits['start'] is not None) and (self.uuid_limits['end'] is None)):\n        uuid_limit_clause = (\"AND itemName() > '%s'\" % self.uuid_limits['start'])\n    elif ((self.uuid_limits['start'] is None) and (self.uuid_limits['end'] is None)):\n        uuid_limit_clause = ''\n    else:\n        uuid_limit_clause = (\"AND itemName() BETWEEN '%s' AND '%s'\" % (self.uuid_limits['start'], self.uuid_limits['end']))\n    sql = (\"SELECT * \\n                FROM `%s` \\n                WHERE\\n                reservation_next_request < '%s' %s\\n                LIMIT 2500\" % (self.aws_sdb_reservation_domain, sdb_now(offset=self.time_offset), uuid_limit_clause))\n    sql = re.sub('\\\\s\\\\s*', ' ', sql)\n    self.current_sql = sql\n    LOGGER.debug(('Querying SimpleDB, \"%s\"' % sql))\n    d = self.sdb.select(sql, max_results=5000)\n    d.addCallback(self._queryCallback)\n    d.addErrback(self._queryErrback)\n", "label": 1}
{"function": "\n\ndef labelOrSubject(self, argstr, i, res):\n    j = self.skipSpace(argstr, i)\n    if (j < 0):\n        return j\n    i = j\n    j = self.uri_ref2(argstr, i, res)\n    if (j >= 0):\n        return j\n    if (argstr[i] == '['):\n        j = self.skipSpace(argstr, (i + 1))\n        if (j < 0):\n            self.BadSyntax(argstr, i, 'Expected ] got EOF')\n        if (argstr[j] == ']'):\n            res.append(self.blankNode())\n            return (j + 1)\n    return (- 1)\n", "label": 0}
{"function": "\n\ndef and_then(self, text):\n    index = self.testcase.asm[self.start:].find(text)\n    if (index > 0):\n        self.start += (index + len(text))\n        self.last = text\n    else:\n        print(self.testcase.asm)\n        raise AssertionError(('\"%s\" was not found after \"%s\" in code' % (text, self.last)))\n    return self\n", "label": 0}
{"function": "\n\ndef test_disabled_down_hosts_are_skipped(self):\n    active_host_names = ['active1', 'active2']\n    disabled_host_names = ['disabled1', 'disabled2', 'disabled3']\n    disabled_hosts = [self._host(name, enabled=False, up=False) for name in disabled_host_names]\n    active_hosts = [self._host(name) for name in active_host_names]\n    all_hosts = (active_hosts + disabled_hosts)\n    self.mock_client().services.list.return_value = all_hosts\n    self.mock_client().hosts.list.return_value = all_hosts\n    hosts = self.nova_client.get_compute_hosts()\n    self.assertIsNotNone(hosts)\n    self.assertTrue(isinstance(hosts, list))\n    for active in active_host_names:\n        self.assertIn(active, hosts)\n    for disabled in disabled_host_names:\n        self.assertNotIn(disabled, hosts)\n", "label": 0}
{"function": "\n\ndef find_kth(self, A, i, j, k):\n    p = self.pivot(A, i, j)\n    if (k == p):\n        return A[p]\n    elif (k > p):\n        return self.find_kth(A, (p + 1), j, k)\n    else:\n        return self.find_kth(A, i, p, k)\n", "label": 0}
{"function": "\n\ndef to_dot(self):\n    body = ''.join((c.to_dot() for c in self.columns))\n    return TABLE.format(self.name, self.header_dot, body)\n", "label": 0}
{"function": "\n\ndef crt(a_values, modulo_values):\n    'Chinese Remainder Theorem.\\n\\n    Calculates x such that x = a[i] (mod m[i]) for each i.\\n\\n    :param a_values: the a-values of the above equation\\n    :param modulo_values: the m-values of the above equation\\n    :returns: x such that x = a[i] (mod m[i]) for each i\\n    \\n\\n    >>> crt([2, 3], [3, 5])\\n    8\\n\\n    >>> crt([2, 3, 2], [3, 5, 7])\\n    23\\n\\n    >>> crt([2, 3, 0], [7, 11, 15])\\n    135\\n    '\n    m = 1\n    x = 0\n    for modulo in modulo_values:\n        m *= modulo\n    for (m_i, a_i) in zip(modulo_values, a_values):\n        M_i = (m // m_i)\n        inv = inverse(M_i, m_i)\n        x = ((x + ((a_i * M_i) * inv)) % m)\n    return x\n", "label": 0}
{"function": "\n\ndef beforeTest(self, test):\n    if (not hasattr(test.test, 'cls')):\n        return\n    plugin_base.before_test(test, test.test.cls.__module__, test.test.cls, test.test.method.__name__)\n", "label": 0}
{"function": "\n\n@presentation.render_for(Icon)\ndef render_Icon(self, h, comp, *args):\n    if (self.title is not None):\n        (h << h.i(class_=self.icon, title=self.title))\n        (h << self.title)\n    else:\n        (h << h.i(class_=self.icon, title=self.title))\n    return h.root\n", "label": 0}
{"function": "\n\ndef prepareShapeModel(datasetDirectory, shape):\n    import glob\n    datasetFiles_shape = glob.glob((((datasetDirectory + '/') + shape) + '.dat'))\n    if (len(datasetFiles_shape) < 1):\n        raise Exception(((('Dataset not available at ' + datasetDirectory) + ' for shape ') + shape))\n    shapeModeler = ShapeModeler(filename=datasetFiles_shape[0], num_principle_components=numParams)\n    return shapeModeler\n", "label": 0}
{"function": "\n\ndef remove_punctuation(self, content):\n    if isinstance(content, unicode):\n        content = content.encode('utf-8')\n    return content.translate(self.TRANS_TABLE, string.punctuation).decode('utf-8')\n", "label": 0}
{"function": "\n\ndef get_runners(self):\n    if self.runner_name:\n        return filter((lambda f: (f.__name__ == self.runner_name)), self.runners)\n    return self.runners\n", "label": 0}
{"function": "\n\ndef dup_primitive_prs(f, g, K):\n    '\\n    Primitive polynomial remainder sequence (PRS) in `K[x]`.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys import ring, ZZ\\n    >>> R, x = ring(\"x\", ZZ)\\n\\n    >>> f = x**8 + x**6 - 3*x**4 - 3*x**3 + 8*x**2 + 2*x - 5\\n    >>> g = 3*x**6 + 5*x**4 - 4*x**2 - 9*x + 21\\n\\n    >>> prs = R.dup_primitive_prs(f, g)\\n\\n    >>> prs[0]\\n    x**8 + x**6 - 3*x**4 - 3*x**3 + 8*x**2 + 2*x - 5\\n    >>> prs[1]\\n    3*x**6 + 5*x**4 - 4*x**2 - 9*x + 21\\n    >>> prs[2]\\n    -5*x**4 + x**2 - 3\\n    >>> prs[3]\\n    13*x**2 + 25*x - 49\\n    >>> prs[4]\\n    4663*x - 6150\\n    >>> prs[5]\\n    1\\n\\n    '\n    prs = [f, g]\n    (_, h) = dup_primitive(dup_prem(f, g, K), K)\n    while h:\n        prs.append(h)\n        (f, g) = (g, h)\n        (_, h) = dup_primitive(dup_prem(f, g, K), K)\n    return prs\n", "label": 0}
{"function": "\n\ndef find_data_files(self, package, src_dir):\n    \"Return filenames for package's data files in 'src_dir'\"\n    globs = (self.package_data.get('', []) + self.package_data.get(package, []))\n    files = self.manifest_files.get(package, [])[:]\n    for pattern in globs:\n        files.extend(glob(os.path.join(src_dir, convert_path(pattern))))\n    return self.exclude_data_files(package, src_dir, files)\n", "label": 0}
{"function": "\n\ndef export(self, lwrite, level, namespace_='DNSQueryObj:', name_='DNSResourceRecordsType', namespacedef_='', pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    showIndent(lwrite, level, pretty_print)\n    lwrite(('<%s%s%s' % (namespace_, name_, ((namespacedef_ and (' ' + namespacedef_)) or ''))))\n    already_processed = set()\n    self.exportAttributes(lwrite, level, already_processed, namespace_, name_='DNSResourceRecordsType')\n    if self.hasContent_():\n        lwrite(('>%s' % (eol_,)))\n        self.exportChildren(lwrite, (level + 1), namespace_, name_, pretty_print=pretty_print)\n        showIndent(lwrite, level, pretty_print)\n        lwrite(('</%s%s>%s' % (namespace_, name_, eol_)))\n    else:\n        lwrite(('/>%s' % (eol_,)))\n", "label": 0}
{"function": "\n\ndef __call__(self, env, start_response):\n    if ('swift.trans_id' not in env):\n        raise Exception('Trans id should always be in env')\n    if self.error:\n        if (self.error == 'strange'):\n            raise StrangeException('whoa')\n        raise Exception('An error occurred')\n    if (self.body_iter is None):\n        return ['FAKE APP']\n    else:\n        return self.body_iter\n", "label": 0}
{"function": "\n\ndef serialize(self):\n    if (self.length == 0):\n        self.length = len(self.value)\n    options_pack_str = ('!BB%ds' % self.length)\n    return struct.pack(options_pack_str, self.tag, self.length, self.value)\n", "label": 0}
{"function": "\n\ndef _make_embedded_from(self, doc):\n    'Creates embedded navigators from a HAL response doc'\n    ld = utils.CurieDict(self._core.default_curie, {\n        \n    })\n    for (rel, doc) in doc.get('_embedded', {\n        \n    }).items():\n        if isinstance(doc, list):\n            ld[rel] = [self._recursively_embed(d) for d in doc]\n        else:\n            ld[rel] = self._recursively_embed(doc)\n    return ld\n", "label": 0}
{"function": "\n\ndef write_video_frame(self, img):\n    if ((img.shape[0] != self._output_size[1]) or (img.shape[1] != self._output_size[0])):\n        warn('Frame data is wrong size! Video will be corrupted.')\n    self._ffmpeg_pipe.write(img.tostring())\n", "label": 0}
{"function": "\n\n@classmethod\ndef make_static_url(cls, settings, path, include_version=True):\n    'Constructs a versioned url for the given path.\\n\\n        This method may be overridden in subclasses (but note that it\\n        is a class method rather than an instance method).  Subclasses\\n        are only required to implement the signature\\n        ``make_static_url(cls, settings, path)``; other keyword\\n        arguments may be passed through `~RequestHandler.static_url`\\n        but are not standard.\\n\\n        ``settings`` is the `Application.settings` dictionary.  ``path``\\n        is the static path being requested.  The url returned should be\\n        relative to the current host.\\n\\n        ``include_version`` determines whether the generated URL should\\n        include the query string containing the version hash of the\\n        file corresponding to the given ``path``.\\n\\n        '\n    url = (settings.get('static_url_prefix', '/static/') + path)\n    if (not include_version):\n        return url\n    version_hash = cls.get_version(settings, path)\n    if (not version_hash):\n        return url\n    return ('%s?v=%s' % (url, version_hash))\n", "label": 0}
{"function": "\n\ndef step(self, x):\n    '\\n        This routine applies the convergence acceleration to the individual terms.\\n\\n        A = sum(a_k, k = 0..infinity)\\n\\n        v, e = ...step(a_k)\\n\\n        output:\\n          v      current estimate of the series A\\n          e      an error estimate which is simply the difference between the current\\n                 estimate and the last estimate.\\n        '\n    if (self.variant != 'v'):\n        if (self.n == 0):\n            self.s = x\n            self.run(self.s, x)\n        else:\n            self.s += x\n            self.run(self.s, x)\n    else:\n        if isinstance(self.last_s, bool):\n            self.last_s = x\n            self.s = 0\n            self.last = 0\n            return (x, abs(x))\n        self.s += self.last_s\n        self.run(self.s, self.last_s, x)\n        self.last_s = x\n    value = (self.A[0] / self.B[0])\n    err = abs((value - self.last))\n    self.last = value\n    return (value, err)\n", "label": 0}
{"function": "\n\ndef handleTimeout(self, watcher):\n    if (self._new_modify and (not self._refreshed)):\n        sublime.set_timeout(functools.partial(self.handleTimeout, watcher), 1000)\n        self._new_modify = False\n    else:\n        if self._refreshed:\n            return\n        self._refreshed = True\n        watcher.refresh()\n", "label": 0}
{"function": "\n\ndef dmp_gcdex(f, g, u, K):\n    '\\n    Extended Euclidean algorithm in `F[X]`.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.polys import ring, ZZ\\n    >>> R, x,y = ring(\"x,y\", ZZ)\\n\\n    '\n    if (not u):\n        return dup_gcdex(f, g, K)\n    else:\n        raise MultivariatePolynomialError(f, g)\n", "label": 0}
{"function": "\n\ndef _exec(self, globals_dict=None):\n    'exec compiled code'\n    globals_dict = (globals_dict or {\n        \n    })\n    globals_dict.setdefault('__builtins__', {\n        \n    })\n    exec(self._code, globals_dict)\n    return globals_dict\n", "label": 0}
{"function": "\n\ndef clean(self):\n    data = super(CreateImageForm, self).clean()\n    image_file = data.get('image_file', None)\n    if ((not data['copy_from']) and (not image_file)):\n        raise ValidationError(_('A image or external image location must be specified.'))\n    elif (data['copy_from'] and image_file):\n        raise ValidationError(_('Can not specify both image and external image location.'))\n    else:\n        return data\n", "label": 0}
{"function": "\n\ndef setup(self, api_version=2):\n    from ceilometerclient import client\n    (options, args) = self.ceilometer.parser.parse_known_args(self.base_argv)\n    if options.help:\n        options.command = None\n        self.do_help(options)\n        sys.exit(2)\n    client_kwargs = vars(options)\n    return (options, client.get_client(api_version, **client_kwargs))\n", "label": 0}
{"function": "\n\ndef gen_url(scheme=None, subdomain=None, tlds=None):\n    \"Generates a random URL address\\n\\n    :param str scheme: Either http, https or ftp.\\n    :param str subdomain: A valid subdmain\\n    :param str tlds: A qualified top level domain name (e.g. 'com', 'net')\\n    :raises: ``ValueError`` if arguments are not valid.\\n    :returns: A random URL address.\\n    :rtype: str\\n\\n    \"\n    subdomainator = re.compile('^[a-zA-Z0-9][-\\\\w.~]*$')\n    schemenator = re.compile('^(https?|ftp)$')\n    tldsnator = re.compile('^[a-zA-Z]{1,3}$')\n    if scheme:\n        if (schemenator.match(scheme) is None):\n            raise ValueError('Protocol {0} is not valid.'.format(scheme))\n    else:\n        scheme = gen_choice(SCHEMES)\n    if subdomain:\n        if (subdomainator.match(subdomain) is None):\n            raise ValueError('Subdomain {0} is invalid.'.format(subdomain))\n    else:\n        subdomain = gen_choice(SUBDOMAINS)\n    if tlds:\n        if (tldsnator.match(tlds) is None):\n            raise ValueError('TLDS name {0} is invalid.'.format(tlds))\n    else:\n        tlds = gen_choice(TLDS)\n    url = '{0}://{1}.{2}'.format(scheme, subdomain, tlds)\n    return _make_unicode(url)\n", "label": 0}
{"function": "\n\n@nottest\ndef test_receipt(self, command):\n    'Test if the server send RECEIPT for a command'\n    m = Main()\n    m.tcp_connect()\n    m.send_connect({\n        \n    })\n    m.get_frame()\n    tmpId = str(uuid.uuid4())\n    m.send_frame(command, [('receipt', tmpId)], '')\n    (result, headers, body) = m.get_frame()\n    eq_(result, 'RECEIPT')\n    for (name, value) in headers:\n        if (name == 'receipt-id'):\n            eq_(value, tmpId)\n            return\n    raise Exception('No receipt-id')\n", "label": 0}
{"function": "\n\ndef _pop_styles(args):\n    styles = args.pop(STYLES_PARAM_NAME, [])\n    if isinstance(styles, str):\n        styles = styles.split(',')\n    styles = [S for S in styles if S.lower().endswith('.css')]\n    return styles\n", "label": 0}
{"function": "\n\ndef post(self, post_data={\n    \n}, collection_id=None):\n    return self.client.post(reverse('wagtailadmin_collections:edit', args=((collection_id or self.collection.id),)), post_data)\n", "label": 0}
{"function": "\n\ndef __delitem__(self, key):\n    if self.__contains__(key):\n        self.execute('DELETE FROM {0} WHERE key = ?'.format(self.tablename), (key,))\n    else:\n        raise KeyError()\n", "label": 0}
{"function": "\n\ndef getNetworkKey(self, name):\n    for netkey in self.networks:\n        if (netkey.name == name):\n            return netkey\n    raise NodeError('Could not find network key with the supplied name.')\n", "label": 0}
{"function": "\n\n@value.setter\ndef value(self, value):\n    py_type = type(value)\n    (fieldname, discriminator) = self.type_map[py_type]\n    self.type = discriminator\n    if (fieldname is not None):\n        setattr(self, fieldname, value)\n", "label": 0}
{"function": "\n\ndef gplvm_simulation(optimize=True, verbose=1, plot=True, plot_sim=False, max_iters=20000.0):\n    from GPy import kern\n    from GPy.models import GPLVM\n    (D1, D2, D3, N, num_inducing, Q) = (13, 5, 8, 45, 3, 9)\n    (_, _, Ylist) = _simulate_matern(D1, D2, D3, N, num_inducing, plot_sim)\n    Y = Ylist[0]\n    k = kern.Linear(Q, ARD=True)\n    m = GPLVM(Y, Q, init='PCA', kernel=k)\n    m.likelihood.variance = 0.1\n    if optimize:\n        print('Optimizing model:')\n        m.optimize('bfgs', messages=verbose, max_iters=max_iters, gtol=0.05)\n    if plot:\n        m.X.plot('BGPLVM Latent Space 1D')\n        m.kern.plot_ARD('BGPLVM Simulation ARD Parameters')\n    return m\n", "label": 0}
{"function": "\n\ndef tearDownModule():\n    if (RUN_SEARCH and (not RUN_YZ)):\n        c = IntegrationTestBase.create_client()\n        b = c.bucket(testrun_search_bucket)\n        b.clear_properties()\n        c.close()\n", "label": 0}
{"function": "\n\ndef from_wire(cls, rdclass, rdtype, wire, current, rdlen, origin=None):\n    l = ord(wire[current])\n    current += 1\n    rdlen -= 1\n    if (l > rdlen):\n        raise dns.exception.FormError\n    latitude = wire[current:(current + l)]\n    current += l\n    rdlen -= l\n    l = ord(wire[current])\n    current += 1\n    rdlen -= 1\n    if (l > rdlen):\n        raise dns.exception.FormError\n    longitude = wire[current:(current + l)]\n    current += l\n    rdlen -= l\n    l = ord(wire[current])\n    current += 1\n    rdlen -= 1\n    if (l != rdlen):\n        raise dns.exception.FormError\n    altitude = wire[current:(current + l)]\n    return cls(rdclass, rdtype, latitude, longitude, altitude)\n", "label": 0}
{"function": "\n\ndef __init__(self, Dataset, order=['y', 'x']):\n    '\\n        Create and set up object\\n        '\n    self.Dataset = Dataset\n    self.lenX = int(Dataset.shape[1])\n    self.lenY = int(Dataset.shape[0])\n    a = [self.lenY, self.lenX]\n    self.order = order\n    self.shape = tuple([a[order.index(k)] for k in ['y', 'x']])\n    self.ndim = 2\n    self.dtype = Dataset.dtype\n", "label": 0}
{"function": "\n\ndef generate_generic_calls(base, ns):\n    methods = (m for m in METHODS if (m.split('.', 1)[0] == base))\n    for method in methods:\n        call_name = method.split('.', 1)[1]\n        if (call_name not in ns):\n            call_class = _get_call_class(method)\n            generic_call = GenericCall(call_class)\n            generic_call.__name__ = str(call_name)\n            generic_call.__doc__ = METHODS[method]\n            ns[call_name] = generic_call\n            if (('__all__' in ns) and (call_name not in ns['__all__'])):\n                ns['__all__'].append(call_name)\n        elif (not ns[call_name].__doc__):\n            ns[call_name].__doc__ = METHODS[method]\n", "label": 0}
{"function": "\n\ndef __init__(self, path, index_page='index.html', hide_index_with_redirect=False, **kw):\n    self.path = os.path.abspath(path)\n    if (not self.path.endswith(os.path.sep)):\n        self.path += os.path.sep\n    if (not os.path.isdir(self.path)):\n        raise IOError(('Path does not exist or is not directory: %r' % self.path))\n    self.index_page = index_page\n    self.hide_index_with_redirect = hide_index_with_redirect\n    self.fileapp_kw = kw\n", "label": 0}
{"function": "\n\ndef _chunks(items, chunk_size):\n    for i in range(0, len(items), chunk_size):\n        (yield items[i:(i + chunk_size)])\n", "label": 0}
{"function": "\n\ndef save(self, commit=True, *args, **kwargs):\n    model = super(SoftDeleteObjectAdminForm, self).save(*args, commit=False, **kwargs)\n    model.deleted = self.cleaned_data['deleted']\n    if commit:\n        model.save()\n    return model\n", "label": 0}
{"function": "\n\ndef _end_creation(self, token):\n    'End object upload.\\n\\n    Args:\\n      token: upload token returned by post_start_creation.\\n\\n    Raises:\\n      ValueError: if token is invalid. Or file is corrupted during upload.\\n\\n    Save file content to blobstore. Save blobinfo and _AE_GCSFileInfo.\\n    '\n    gcs_file = _AE_GCSFileInfo_.get_by_key_name(token)\n    if (not gcs_file):\n        raise ValueError('Invalid token')\n    (error_msg, content) = self._get_content(gcs_file)\n    if error_msg:\n        raise ValueError(error_msg)\n    gcs_file.etag = hashlib.md5(content).hexdigest()\n    gcs_file.creation = datetime.datetime.utcnow()\n    gcs_file.size = len(content)\n    self.blob_storage.StoreBlob(token, StringIO.StringIO(content))\n    gcs_file.finalized = True\n    gcs_file.put()\n", "label": 0}
{"function": "\n\ndef get(self, request, *args, **kwargs):\n    service = request.GET.get('service')\n    ticket = request.GET.get('ticket')\n    renew = to_bool(request.GET.get('renew'))\n    (st, pgt, error) = validate_service_ticket(service, ticket, None, renew)\n    if st:\n        content = ('yes\\n%s\\n' % st.user.get_username())\n    else:\n        content = 'no\\n\\n'\n    return HttpResponse(content=content, content_type='text/plain')\n", "label": 0}
{"function": "\n\ndef get_account(self, account_id, **kwargs):\n    'Retrieves a CDN account with the specified account ID.\\n\\n        :param account_id int: the numeric ID associated with the CDN account.\\n        :param dict \\\\*\\\\*kwargs: additional arguments to include in the object\\n                                mask.\\n        '\n    if ('mask' not in kwargs):\n        kwargs['mask'] = 'status'\n    return self.account.getObject(id=account_id, **kwargs)\n", "label": 0}
{"function": "\n\ndef items(self):\n    '\\n        Returns a list of (key, value) pairs, where value is the last item in\\n        the list associated with the key.\\n        '\n    return [(key, self[key]) for key in self.keys()]\n", "label": 0}
{"function": "\n\n@staticmethod\ndef get_pl_tags(xml, ignore=[]):\n    pl_tags = []\n    for elem in xml.iterchildren():\n        if (elem.tag not in ignore):\n            pl_tag = PLTag({\n                'tagname': elem.tag,\n                'value': elem.text,\n            })\n            pl_tags.append(pl_tag)\n    return pl_tags\n", "label": 0}
{"function": "\n\ndef connect_to_openstack(self, openstack_params, flavor_name, image_name, external_net):\n    LOG.debug('Connecting to OpenStack')\n    self.openstack_client = openstack.OpenStackClient(openstack_params)\n    self.flavor_name = flavor_name\n    self.image_name = image_name\n    self.external_net = (external_net or neutron.choose_external_net(self.openstack_client.neutron))\n    self.stack_name = ('shaker_%s' % utils.random_string())\n", "label": 0}
{"function": "\n\ndef get_site_path(self):\n    target = self\n    npath = [self.name]\n    while True:\n        if (target.parent is None):\n            break\n        else:\n            npath.append(target.parent.name)\n            target = target.parent\n    return '.'.join(npath)\n", "label": 0}
{"function": "\n\ndef __init__(self, srcDB, destDB=None, alignedIvalsAttrs=_default_ivals_attrs):\n    self.srcDB = srcDB\n    if destDB:\n        self.destDB = destDB\n    else:\n        self.destDB = srcDB\n    self.getAttr = classutil.make_attribute_interface(alignedIvalsAttrs)\n", "label": 0}
{"function": "\n\ndef get_metadata(error_code=0, error_message=None, status='OK'):\n    metadata = {\n        'status': status,\n        'error_code': error_code,\n    }\n    if (error_message is not None):\n        metadata['error_message'] = error_message\n    metadata.update(get_config().get('service_metadata', {\n        \n    }))\n    return metadata\n", "label": 0}
{"function": "\n\n@mock.patch('alembic.script.ScriptDirectory.walk_revisions')\ndef test__find_milestone_revisions_one_branch(self, walk_mock):\n    c_revs = [FakeRevision(labels={cli.CONTRACT_BRANCH}) for r in range(5)]\n    c_revs[1].module.neutron_milestone = [migration.LIBERTY]\n    walk_mock.return_value = c_revs\n    m = cli._find_milestone_revisions(self.configs[0], 'liberty', cli.CONTRACT_BRANCH)\n    self.assertEqual(1, len(m))\n    m = cli._find_milestone_revisions(self.configs[0], 'liberty', cli.EXPAND_BRANCH)\n    self.assertEqual(0, len(m))\n", "label": 0}
{"function": "\n\ndef test_queueSendIntentIsSentOnFailureCallbackOfPending(self):\n    self.resetCounters()\n    self.test_sendIntentToTransportUpToLimitAndThenQueueInternally()\n    self.testTrans.forTestingCompleteAPendingIntent(SendStatus.Failed)\n    self.assertEqual(self.successCBcalls, 0)\n    self.assertEqual(self.failureCBcalls, 1)\n    self.assertEqual((MAX_PENDING_TRANSMITS + 1), len(self.testTrans.intents))\n    self.assertEqual(1, len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n", "label": 0}
{"function": "\n\ndef process_pushevent_payload_files(payload, github_auth):\n    ' Return a dictionary of files paths from a push event payload.\\n    \\n        https://developer.github.com/v3/activity/events/types/#pushevent\\n    '\n    files = dict()\n    touched = get_touched_payload_files(payload)\n    touched |= get_touched_branch_files(payload, github_auth)\n    commit_sha = payload['head_commit']['id']\n    for filename in touched:\n        if relpath(filename, 'sources').startswith('..'):\n            continue\n        if (splitext(filename)[1] != '.json'):\n            continue\n        contents_url = (payload['repository']['contents_url'] + '{?ref}')\n        contents_url = expand_uri(contents_url, dict(path=filename, ref=commit_sha))\n        current_app.logger.debug('Contents URL {}'.format(contents_url))\n        got = get(contents_url, auth=github_auth)\n        contents = got.json()\n        if (got.status_code not in range(200, 299)):\n            current_app.logger.warning('Skipping {} - {}'.format(filename, got.status_code))\n            continue\n        if (contents['encoding'] != 'base64'):\n            raise ValueError('Unrecognized encoding \"{encoding}\"'.format(**contents))\n        current_app.logger.debug('Contents SHA {sha}'.format(**contents))\n        files[filename] = (contents['content'], contents['sha'])\n    return files\n", "label": 0}
{"function": "\n\ndef _fetch_users(self):\n    response = self.slacker.users.list()\n    for user in response.body['members']:\n        self.users[user['name']] = user['id']\n    self.testbot_userid = self.users[self.testbot_username]\n    self.driver_userid = self.users[self.driver_username]\n", "label": 0}
{"function": "\n\n@webapi_check_local_site\n@webapi_login_required\n@webapi_response_errors(INVALID_FORM_DATA, NOT_LOGGED_IN, PERMISSION_DENIED)\n@webapi_request_fields(required={\n    'name': {\n        'type': six.text_type,\n        'description': 'The name of the default reviewer entry.',\n    },\n    'file_regex': {\n        'type': six.text_type,\n        'description': 'The regular expression used to match file paths in newly uploaded diffs.',\n    },\n}, optional={\n    'repositories': {\n        'type': six.text_type,\n        'description': 'A comma-separated list of repository IDs.',\n    },\n    'groups': {\n        'type': six.text_type,\n        'description': 'A comma-separated list of group names.',\n    },\n    'users': {\n        'type': six.text_type,\n        'description': 'A comma-separated list of usernames.',\n    },\n})\ndef create(self, request, local_site=None, *args, **kwargs):\n    \"Creates a new default reviewer entry.\\n\\n        Note that by default, a default reviewer will apply to review\\n        requests on all repositories, unless one or more repositories are\\n        provided in the default reviewer's list.\\n        \"\n    if (not self.model.objects.can_create(request.user, local_site)):\n        return self.get_no_access_error(request)\n    (code, data) = self._create_or_update(local_site, **kwargs)\n    if (code == 200):\n        return (201, data)\n    else:\n        return (code, data)\n", "label": 0}
{"function": "\n\ndef to_event(self, notification_body, condenser):\n    event_type = notification_body['event_type']\n    message_id = notification_body['message_id']\n    when = self._extract_when(notification_body)\n    condenser.add_envelope_info(event_type, message_id, when)\n    for t in self.traits:\n        trait_info = self.traits[t].to_trait(notification_body)\n        if (trait_info is not None):\n            condenser.add_trait(*trait_info)\n    return condenser\n", "label": 0}
{"function": "\n\ndef apply_kwargs(func, **kwargs):\n    'Call *func* with kwargs, but only those kwargs that it accepts.\\n    '\n    new_kwargs = {\n        \n    }\n    params = signature(func).parameters\n    for param_name in params.keys():\n        if (param_name in kwargs):\n            new_kwargs[param_name] = kwargs[param_name]\n    return func(**new_kwargs)\n", "label": 0}
{"function": "\n\ndef testEntryTypeConversion(self):\n    for entry in self.feed.entry:\n        if (entry.id.text == 'http://picasaweb.google.com/data/feed/api/user/sample.user/albumid/'):\n            self.assert_(isinstance(entry, gdata.photos.PhotoEntry))\n", "label": 0}
{"function": "\n\ndef __init__(self, config):\n    if (not config['host']):\n        raise SystemExit('Error: IRC controller needs to be configured with a hostname')\n    if (not config['nick']):\n        raise SystemExit('Error: IRC controller needs to be configured with a nick')\n    print(('Connecting to %s:%s as %s' % (config['host'], config['port'], config['nick'])))\n    irc.bot.SingleServerIRCBot.__init__(self, [(config['host'], config['port'])], config['nick'], config['realname'])\n    channels = config['channels']\n    if channels:\n        self.channel = channels\n        print(('Joining Channels: %s' % channels))\n    self.config = config\n", "label": 0}
{"function": "\n\ndef test_list_float_complex(self):\n    x = ([np.random.rand() for i in range(5)] + [(np.random.rand() + (1j * np.random.rand())) for i in range(5)])\n    x_rec = self.encode_decode(x)\n    self.assertTrue(np.allclose(x, x_rec))\n", "label": 0}
{"function": "\n\ndef _try_retry(self, invocation):\n    if invocation.connection:\n        return False\n    if (invocation.timeout < time.time()):\n        return False\n    invoke_func = functools.partial(self.invoke, invocation)\n    self.logger.debug('Rescheduling request %s to be retried in %s seconds', invocation.request, RETRY_WAIT_TIME_IN_SECONDS)\n    self._client.reactor.add_timer(RETRY_WAIT_TIME_IN_SECONDS, invoke_func)\n    return True\n", "label": 0}
{"function": "\n\ndef __init__(self, session):\n    m = re.search('/(\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+):(\\\\d+)\\\\[(\\\\d+)\\\\]\\\\((.*)\\\\)', session)\n    if m:\n        self.host = m.group(1)\n        self.port = m.group(2)\n        self.interest_ops = m.group(3)\n        for d in m.group(4).split(','):\n            (k, v) = d.split('=')\n            self.__dict__[k] = v\n    else:\n        raise Session.BrokenLine()\n", "label": 0}
{"function": "\n\ndef rx_tx_dump(interface):\n    for line in open('/proc/net/dev'):\n        if (interface in line):\n            data = line.split(('%s:' % interface))[1].split()\n            (rx, tx) = ([int(x) for x in data[0:8]], [int(x) for x in data[8:]])\n    return (rx, tx)\n", "label": 0}
{"function": "\n\ndef _oec_0_9_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_server_e75ead52_692f_4314_8725_c8a4f4d13a87_backup_client(self, method, url, body, headers):\n    if (method == 'POST'):\n        body = self.fixtures.load('oec_0_9_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_server_e75ead52_692f_4314_8725_c8a4f4d13a87_backup_client_SUCCESS_PUT.xml')\n        return (httplib.OK, body, {\n            \n        }, httplib.responses[httplib.OK])\n    else:\n        raise ValueError('Unknown Method {0}'.format(method))\n", "label": 0}
{"function": "\n\ndef create_a(name):\n    a = A(name=name)\n    for name in ('auto', 'auto_nullable', 'setvalue', 'setnull', 'setdefault', 'setdefault_none', 'cascade', 'cascade_nullable', 'protect', 'donothing', 'o2o_setnull'):\n        r = R.objects.create()\n        setattr(a, name, r)\n    a.child = RChild.objects.create()\n    a.child_setnull = RChild.objects.create()\n    a.save()\n    return a\n", "label": 0}
{"function": "\n\ndef _get_min_depth_interaction(self, txn, room_id):\n    min_depth = self._simple_select_one_onecol_txn(txn, table='room_depth', keyvalues={\n        'room_id': room_id,\n    }, retcol='min_depth', allow_none=True)\n    return (int(min_depth) if (min_depth is not None) else None)\n", "label": 0}
{"function": "\n\ndef complete(self, text, state):\n    if (state == 0):\n        self.matches = filter((lambda choice: choice.startswith(text)), self.choices)\n    if ((self.matches is not None) and (state < len(self.matches))):\n        return self.matches[state]\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef __match_string(self, subject):\n    reasons = []\n    for item in self._expected:\n        if (not (item in subject)):\n            return (False, ['item {0!r} not found'.format(item)])\n        else:\n            reasons.append('item {0!r} found'.format(item))\n    if (len(subject) != len(''.join(self._expected))):\n        return (False, ['have a different length'])\n    return (True, reasons)\n", "label": 0}
{"function": "\n\ndef build_readme(base_path=None):\n    'Call the conversion routine on README.md to generate README.rst.\\n    Why do all this? Because pypi requires reStructuredText, but markdown\\n    is friendlier to work with and is nicer for GitHub.'\n    if base_path:\n        path = os.path.join(base_path, 'README.md')\n    else:\n        path = 'README.md'\n    convert_md_to_rst(path)\n    print('Successfully converted README.md to README.rst')\n", "label": 0}
{"function": "\n\ndef get_all_session_madlibs(self):\n    sessions = []\n    for session_template in SESSION_MADLIBS:\n        session = get_session_madlib(session_template, self.subdomain_as_string, self.year)\n        sessions.append(session)\n    return sessions\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(TokenController, self).__init__(*args, **kwargs)\n    if (cfg.CONF.auth.mode == 'standalone'):\n        self._auth_backend = get_backend_instance(name=cfg.CONF.auth.backend)\n    else:\n        self._auth_backend = None\n", "label": 0}
{"function": "\n\ndef __init__(self, file):\n    if isinstance(file, SeekableFile):\n        file = file.file\n    self.file = file\n", "label": 0}
{"function": "\n\ndef collect_request_parameters(self, request):\n    'Collect parameters in an object for convenient access'\n\n    class OAuthParameters(object):\n        \"Used as a parameter container since plain object()s can't\"\n        pass\n    query = urlparse(request.url.decode('utf-8')).query\n    content_type = request.headers.get('Content-Type', '')\n    if request.form:\n        body = request.form.to_dict()\n    elif (content_type == 'application/x-www-form-urlencoded'):\n        body = request.data.decode('utf-8')\n    else:\n        body = ''\n    headers = dict(encode_params_utf8(request.headers.items()))\n    params = dict(collect_parameters(uri_query=query, body=body, headers=headers))\n    oauth_params = OAuthParameters()\n    oauth_params.client_key = params.get('oauth_consumer_key')\n    oauth_params.resource_owner_key = params.get('oauth_token', None)\n    oauth_params.nonce = params.get('oauth_nonce')\n    oauth_params.timestamp = params.get('oauth_timestamp')\n    oauth_params.verifier = params.get('oauth_verifier', None)\n    oauth_params.callback_uri = params.get('oauth_callback', None)\n    oauth_params.realm = params.get('realm', None)\n    return oauth_params\n", "label": 0}
{"function": "\n\ndef to_op(self):\n    '\\n        Extracts the modification operation from the set.\\n\\n        :rtype: dict, None\\n        '\n    if ((not self._adds) and (not self._removes)):\n        return None\n    changes = {\n        \n    }\n    if self._adds:\n        changes['adds'] = list(self._adds)\n    if self._removes:\n        changes['removes'] = list(self._removes)\n    return changes\n", "label": 0}
{"function": "\n\ndef _compute_memory_address(self, mem_operand):\n    'Return operand memory access translation.\\n        '\n    size = self._arch_info.architecture_size\n    addr = None\n    if mem_operand.base:\n        addr = ReilRegisterOperand(mem_operand.base, size)\n    if (mem_operand.index and (mem_operand.scale != 0)):\n        index = ReilRegisterOperand(mem_operand.index, size)\n        scale = ReilImmediateOperand(mem_operand.scale, size)\n        scaled_index = self.temporal(size)\n        self.add(self._builder.gen_mul(index, scale, scaled_index))\n        if addr:\n            tmp = self.temporal(size)\n            self.add(self._builder.gen_add(addr, scaled_index, tmp))\n            addr = tmp\n        else:\n            addr = scaled_index\n    if (mem_operand.displacement != 0):\n        disp = ReilImmediateOperand(mem_operand.displacement, size)\n        if addr:\n            tmp = self.temporal(size)\n            self.add(self._builder.gen_add(addr, disp, tmp))\n            addr = tmp\n        else:\n            addr = disp\n    elif (not addr):\n        disp = ReilImmediateOperand(mem_operand.displacement, size)\n        addr = disp\n    return addr\n", "label": 0}
{"function": "\n\ndef test_geotransform_and_friends(self):\n    self.assertEqual(self.rs.geotransform, [511700.4680706557, 100.0, 0.0, 435103.3771231986, 0.0, (- 100.0)])\n    self.assertEqual(self.rs.origin, [511700.4680706557, 435103.3771231986])\n    self.assertEqual(self.rs.origin.x, 511700.4680706557)\n    self.assertEqual(self.rs.origin.y, 435103.3771231986)\n    self.assertEqual(self.rs.scale, [100.0, (- 100.0)])\n    self.assertEqual(self.rs.scale.x, 100.0)\n    self.assertEqual(self.rs.scale.y, (- 100.0))\n    self.assertEqual(self.rs.skew, [0, 0])\n    self.assertEqual(self.rs.skew.x, 0)\n    self.assertEqual(self.rs.skew.y, 0)\n    rsmem = GDALRaster(JSON_RASTER)\n    rsmem.geotransform = range(6)\n    self.assertEqual(rsmem.geotransform, [float(x) for x in range(6)])\n    self.assertEqual(rsmem.origin, [0, 3])\n    self.assertEqual(rsmem.origin.x, 0)\n    self.assertEqual(rsmem.origin.y, 3)\n    self.assertEqual(rsmem.scale, [1, 5])\n    self.assertEqual(rsmem.scale.x, 1)\n    self.assertEqual(rsmem.scale.y, 5)\n    self.assertEqual(rsmem.skew, [2, 4])\n    self.assertEqual(rsmem.skew.x, 2)\n    self.assertEqual(rsmem.skew.y, 4)\n    self.assertEqual(rsmem.width, 5)\n    self.assertEqual(rsmem.height, 5)\n", "label": 0}
{"function": "\n\ndef pom_check_if_child_exists(parent, prefix, values, comparison_tags):\n    for child in parent:\n        match = 0\n        for tag in comparison_tags:\n            key = child.find((prefix + tag))\n            if (key.text == values[tag]):\n                match += 1\n        if (match == len(comparison_tags)):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef fetch_file_names(commit_url, sha):\n    resp = requests.get(commit_url.format(sha))\n    jsonstr = resp.content.decode('utf-8')\n    jsonobj = json.loads(jsonstr)\n    files = [d['filename'] for d in jsonobj['files']]\n    return files\n", "label": 0}
{"function": "\n\ndef new_relation_input(node, modelclass, metadata, order_by=None):\n    choices = models.get_choices(dbsession, modelclass, metadata.colname, order_by)\n    if (not choices):\n        return node.new_para(('%s has no choices.' % metadata.colname))\n    if metadata.nullable:\n        choices.insert(0, (0, '----'))\n    elid = ('id_' + metadata.colname)\n    node.add_label(metadata.colname, elid)\n    return node.add_select(choices, name=metadata.colname, multiple=metadata.uselist, id=elid)\n", "label": 0}
{"function": "\n\ndef getServiceXML(self):\n    xml = None\n    if (self._capabilities is not None):\n        xml = etree.tostring(self._capabilities)\n    return xml\n", "label": 0}
{"function": "\n\ndef finalize(self):\n    string = '</table>'\n    self.handle.write(string)\n    if self.html_footer:\n        self.handle.write(self.html_footer)\n    if self.close_file:\n        self.handle.close()\n", "label": 0}
{"function": "\n\ndef register_aliases(self, aliases):\n    'Registers the given aliases to be exposed in parsed BUILD files.\\n\\n    :param aliases: The BuildFileAliases to register.\\n    :type aliases: :class:`pants.build_graph.build_file_aliases.BuildFileAliases`\\n    '\n    if (not isinstance(aliases, BuildFileAliases)):\n        raise TypeError('The aliases must be a BuildFileAliases, given {}'.format(aliases))\n    for (alias, target_type) in aliases.target_types.items():\n        self._register_target_alias(alias, target_type)\n    for (alias, target_macro_factory) in aliases.target_macro_factories.items():\n        self._register_target_macro_factory_alias(alias, target_macro_factory)\n    for (alias, obj) in aliases.objects.items():\n        self._register_exposed_object(alias, obj)\n    for (alias, context_aware_object_factory) in aliases.context_aware_object_factories.items():\n        self._register_exposed_context_aware_object_factory(alias, context_aware_object_factory)\n", "label": 0}
{"function": "\n\ndef batch_write_item(self):\n    table_batches = self.body['RequestItems']\n    for (table_name, table_requests) in table_batches.items():\n        for table_request in table_requests:\n            request_type = list(table_request)[0]\n            request = list(table_request.values())[0]\n            if (request_type == 'PutRequest'):\n                item = request['Item']\n                dynamodb_backend.put_item(table_name, item)\n            elif (request_type == 'DeleteRequest'):\n                key = request['Key']\n                hash_key = key['HashKeyElement']\n                range_key = key.get('RangeKeyElement')\n                item = dynamodb_backend.delete_item(table_name, hash_key, range_key)\n    response = {\n        'Responses': {\n            'Thread': {\n                'ConsumedCapacityUnits': 1.0,\n            },\n            'Reply': {\n                'ConsumedCapacityUnits': 1.0,\n            },\n        },\n        'UnprocessedItems': {\n            \n        },\n    }\n    return dynamo_json_dump(response)\n", "label": 0}
{"function": "\n\ndef getter(self, widget):\n    '\\n        Return the itemData stored in the currently-selected item\\n        '\n    if (widget.currentIndex() == (- 1)):\n        return None\n    else:\n        return widget.itemData(widget.currentIndex())\n", "label": 0}
{"function": "\n\ndef _create_context(self, request, *args, **kwargs):\n    request_id = kwargs.get('request_id')\n    if request_id:\n        silk_request = Request.objects.get(pk=request_id)\n    else:\n        silk_request = None\n    show = request.GET.get('show', self.default_show)\n    order_by = request.GET.get('order_by', self.defualt_order_by)\n    if show:\n        show = int(show)\n    func_name = request.GET.get('func_name', None)\n    name = request.GET.get('name', None)\n    filters = request.session.get(self.session_key_profile_filters, {\n        \n    })\n    context = {\n        'show': show,\n        'order_by': order_by,\n        'request': request,\n        'func_name': func_name,\n        'options_show': self.show,\n        'options_order_by': self.order_by,\n        'options_func_names': self._get_function_names(silk_request),\n        'options_names': self._get_names(silk_request),\n        'filters': filters,\n    }\n    context.update(csrf(request))\n    if silk_request:\n        context['silk_request'] = silk_request\n    if func_name:\n        context['func_name'] = func_name\n    if name:\n        context['name'] = name\n    objs = self._get_objects(show=show, order_by=order_by, func_name=func_name, silk_request=silk_request, name=name, filters=[BaseFilter.from_dict(x) for (_, x) in filters.items()])\n    context['results'] = objs\n    return context\n", "label": 0}
{"function": "\n\ndef set_flowcontrol(self, name, direction, value=None, default=False, disable=False):\n    \"Configures the interface flowcontrol value\\n\\n        Args:\\n            name (string): The interface identifier.  It must be a full\\n                interface name (ie Ethernet, not Et)\\n\\n            direction (string): one of either 'send' or 'receive'\\n\\n            value (boolean): True if the interface should enable flow control\\n                packet handling, otherwise False\\n\\n            default (boolean): Specifies to default the interface flow control\\n                send or receive value\\n\\n            disable (boolean): Specifies to disable the interface flow control\\n                send or receive value\\n\\n        Returns:\\n            True if the operation succeeds otherwise False is returned\\n        \"\n    if (value is not None):\n        if (value not in ['on', 'off']):\n            raise ValueError('invalid flowcontrol value')\n    if (direction not in ['send', 'receive']):\n        raise ValueError('invalid direction specified')\n    commands = [('interface %s' % name)]\n    commands.append(self.command_builder(('flowcontrol %s' % direction), value=value, default=default, disable=disable))\n    return self.configure(commands)\n", "label": 0}
{"function": "\n\ndef test_components_get_all(self):\n    (uuid, assembly_uuid, plan_uuid) = self._create_component()\n    (resp, body) = self.client.get('v1/components')\n    data = json.loads(body)\n    self.assertEqual(resp.status, 200)\n    filtered = [com for com in data if (com['uuid'] == uuid)]\n    self.assertEqual(len(filtered), 1)\n    self.assertEqual(filtered[0]['uuid'], uuid)\n    self._delete_component(uuid)\n", "label": 0}
{"function": "\n\ndef dictfetchall(cursor):\n    'Returns all rows from a cursor as a dict'\n    desc = cursor.description\n    return [dict(zip([col[0] for col in desc], row)) for row in cursor.fetchall()]\n", "label": 0}
{"function": "\n\n@classmethod\ndef parse(cls, api, json):\n    result = cls(api)\n    if (json is not None):\n        for (k, v) in json.items():\n            setattr(result, k, v)\n    return result\n", "label": 0}
{"function": "\n\ndef _get_m2m_db_table(self, opts):\n    'Function that can be curried to provide the m2m table name for this relation'\n    if (self.rel.through is not None):\n        return self.rel.through._meta.db_table\n    elif self.db_table:\n        return self.db_table\n    else:\n        return util.truncate_name(('%s_%s' % (opts.db_table, self.name)), connection.ops.max_name_length())\n", "label": 0}
{"function": "\n\ndef get(name, type=None):\n    'To get the value of a vim variable.'\n    rawval = vim.eval((prefix + name))\n    if (type is bool):\n        return (False if (rawval == '0') else True)\n    elif (type is int):\n        return int(rawval)\n    elif (type is float):\n        return float(rawval)\n    else:\n        return rawval\n", "label": 0}
{"function": "\n\ndef import_coordinator_root(coordinator, coordinator_definition_root, metadata=None):\n    xslt_definition_fh = open(('%(xslt_dir)s/coordinator.xslt' % {\n        'xslt_dir': os.path.join(conf.DEFINITION_XSLT_DIR.get(), 'coordinators'),\n    }))\n    tag = etree.QName(coordinator_definition_root.tag)\n    schema_version = tag.namespace\n    if (schema_version not in OOZIE_NAMESPACES):\n        raise RuntimeError((_('Tag with namespace %(namespace)s is not valid. Please use one of the following namespaces: %(namespaces)s') % {\n            'namespace': coordinator_definition_root.tag,\n            'namespaces': ', '.join(OOZIE_NAMESPACES),\n        }))\n    xslt = etree.parse(xslt_definition_fh)\n    xslt_definition_fh.close()\n    transform = etree.XSLT(xslt)\n    transformed_root = transform(coordinator_definition_root)\n    objects = serializers.deserialize('xml', etree.tostring(transformed_root))\n    _set_coordinator_properties(coordinator, coordinator_definition_root, schema_version)\n    _set_controls(coordinator, coordinator_definition_root, schema_version)\n    _reconcile_datasets(coordinator, objects, coordinator_definition_root, schema_version)\n    _set_properties(coordinator, coordinator_definition_root, schema_version)\n    if metadata:\n        _process_metadata(coordinator, metadata)\n    coordinator.schema_version = schema_version\n    coordinator.save()\n", "label": 0}
{"function": "\n\ndef get_level(self, parent, block):\n    ' Get level of indent based on list level. '\n    m = self.INDENT_RE.match(block)\n    if m:\n        indent_level = (len(m.group(1)) / self.tab_length)\n    else:\n        indent_level = 0\n    if self.parser.state.isstate('list'):\n        level = 1\n    else:\n        level = 0\n    while (indent_level > level):\n        child = self.lastChild(parent)\n        if (child and ((child.tag in self.LIST_TYPES) or (child.tag in self.ITEM_TYPES))):\n            if (child.tag in self.LIST_TYPES):\n                level += 1\n            parent = child\n        else:\n            break\n    return (level, parent)\n", "label": 0}
{"function": "\n\ndef __init__(self, station, start_year=None, end_year=None, cache_directory=None, cache_filename=None):\n    super(NOAAWeatherSourceBase, self).__init__(station, cache_directory, cache_filename)\n    self._year_fetches_attempted = set()\n    self.station_id = station\n    if ((start_year is not None) and (end_year is not None)):\n        self.add_year_range(start_year, end_year)\n    elif (start_year is not None):\n        self.add_year_range(start_year, date.today().year)\n    elif (end_year is not None):\n        self.add_year_range(date.today().year, end_year)\n    self._check_for_recent_data()\n", "label": 0}
{"function": "\n\ndef convert_value(self, value):\n    if (not isinstance(value, (list, tuple))):\n        return self._convert_value_to_string(value)\n    values = []\n    for item in value:\n        values.append(self._convert_value_to_string(item))\n    return values\n", "label": 0}
{"function": "\n\ndef splitLPOintervals(lpoList, ival, targetIval=None):\n    'return list of intervals split to different LPOs'\n    if (ival.start < 0):\n        start = (- ival.stop)\n        stop = (- ival.start)\n    else:\n        start = ival.start\n        stop = ival.stop\n    l = []\n    i = (len(lpoList) - 1)\n    while (i >= 0):\n        offset = lpoList[i].offset\n        if (offset < stop):\n            if (offset <= start):\n                if (ival.start < 0):\n                    myslice = slice((offset - stop), (offset - start))\n                else:\n                    myslice = slice((start - offset), (stop - offset))\n                if (targetIval is not None):\n                    l.append((lpoList[i], myslice, targetIval))\n                else:\n                    l.append((lpoList[i], myslice))\n                return l\n            else:\n                if (ival.start < 0):\n                    myslice = slice((offset - stop), 0)\n                else:\n                    myslice = slice(0, (stop - offset))\n                if (targetIval is not None):\n                    l.append((lpoList[i], myslice, targetIval[(offset - start):]))\n                    targetIval = targetIval[:(offset - start)]\n                else:\n                    l.append((lpoList[i], myslice))\n                stop = offset\n        i -= 1\n    raise ValueError('empty lpoList or offset not starting at 0?  Debug!')\n", "label": 1}
{"function": "\n\ndef raise_app(self, app):\n    app_zi = self.window_zindex[app.identifier]\n    for t in self.window_zindex.keys():\n        w = self.window[t]\n        zi = self.window_zindex[t]\n        if (zi > app_zi):\n            self.set_app_zindex(t, (zi - 1))\n    app_zi = (len(self.window) - 1)\n    self.set_app_zindex(app.identifier, app_zi)\n", "label": 0}
{"function": "\n\ndef _program_in_path(program):\n    path = os.environ.get('PATH', os.defpath).split(os.pathsep)\n    path = [os.path.join(dir, program) for dir in path]\n    path = [True for file in path if (os.path.isfile(file) or os.path.isfile((file + '.exe')))]\n    return bool(path)\n", "label": 0}
{"function": "\n\ndef get_batch_run_times(db, owner, repository):\n    \" Return dictionary of source paths to run time strings like '00:01:23'.\\n    \"\n    last_set = objects.read_latest_set(db, owner, repository)\n    return {run.source_path: run.state['process time'] for run in objects.read_completed_runs_to_date(db, last_set.id) if run.state}\n", "label": 0}
{"function": "\n\ndef ask_description(self, package, class_name):\n    status = ('Class \"%s\" will use \"%s\" package to organize import' % (class_name, package))\n    class_path = '.'.join([x for x in [package, class_name] if x])\n    if (not JavaUtils().is_class_path(class_path)):\n        status = 'Invalid package naming'\n    StatusManager().show_status(status, delay=(- 1), ref='ask_package_description')\n", "label": 0}
{"function": "\n\ndef get_metadata(self, container, prefix=None):\n    '\\n        Returns a dictionary containing the metadata for the container.\\n        '\n    headers = self.get_headers(container)\n    if (prefix is None):\n        prefix = CONTAINER_META_PREFIX\n    low_prefix = prefix.lower()\n    ret = {\n        \n    }\n    for (hkey, hval) in list(headers.items()):\n        if hkey.lower().startswith(low_prefix):\n            cleaned = hkey.replace(low_prefix, '').replace('-', '_')\n            ret[cleaned] = hval\n    return ret\n", "label": 0}
{"function": "\n\ndef GetEntityViaMemcache(entity_key):\n    'Get entity from memcache if available, from datastore if not.'\n    entity = memcache.get(entity_key)\n    if (entity is not None):\n        return entity\n    key = ndb.Key(urlsafe=entity_key)\n    entity = key.get()\n    if (entity is not None):\n        memcache.set(entity_key, entity)\n    return entity\n", "label": 0}
{"function": "\n\ndef _EntriesGenerator(self):\n    'Retrieves directory entries.\\n\\n    Since a directory can contain a vast number of entries using\\n    a generator is more memory efficient.\\n\\n    Yields:\\n      A path specification (instance of path.FakePathSpec).\\n    '\n    location = getattr(self.path_spec, 'location', None)\n    if (location is None):\n        return\n    paths = self._file_system.GetPaths()\n    for path in iter(paths.keys()):\n        if ((not path) or (not path.startswith(location))):\n            continue\n        (_, suffix) = self._file_system.GetPathSegmentAndSuffix(location, path)\n        if (suffix or (path == location)):\n            continue\n        path_spec_location = self._file_system.JoinPath([path])\n        (yield fake_path_spec.FakePathSpec(location=path_spec_location))\n", "label": 0}
{"function": "\n\ndef keypress(self, size, key):\n    if (not self.app.input_buffer):\n        key = super(SideDiffCommentEdit, self).keypress(size, key)\n    keys = (self.app.input_buffer + [key])\n    commands = self.app.config.keymap.getCommands(keys)\n    if ((keymap.NEXT_SELECTABLE in commands) or (keymap.PREV_SELECTABLE in commands)):\n        if (((self.context.old_ln is not None) and (self.context.new_ln is not None)) or self.context.header):\n            if (self.focus_position == 3):\n                self.focus_position = 1\n            else:\n                self.focus_position = 3\n        return None\n    return key\n", "label": 0}
{"function": "\n\ndef python_value(self, value):\n    if isinstance(value, uuid.UUID):\n        return value\n    return (None if (value is None) else uuid.UUID(value))\n", "label": 0}
{"function": "\n\ndef _fetch_period(self, period):\n    years = []\n    if ((period.start is not None) and (period.end is not None)):\n        self.add_year_range(period.start.year, period.end.year)\n    elif (period.start is not None):\n        self.add_year(period.start.year)\n    elif (period.end is not None):\n        self.add_year(period.end.year)\n", "label": 0}
{"function": "\n\ndef safe_makedirs(*files):\n    for filename in files:\n        dirname = os.path.dirname(filename)\n        if (not os.path.isdir(dirname)):\n            os.makedirs(dirname)\n", "label": 0}
{"function": "\n\ndef run_test_case(self, input_string, length_to_send, truncate_size=None):\n    input_buffer = cStringIO.StringIO()\n    input_buffer.write(struct.pack('!I', length_to_send))\n    input_buffer.write(input_string)\n    if (truncate_size is not None):\n        input_buffer.truncate(truncate_size)\n    num_bytes = input_buffer.tell()\n    input_buffer.seek(0)\n    result = Int32RequestParser(10).parse_request(input_buffer, num_bytes)\n    if (result is None):\n        self.assertEquals(0, input_buffer.tell())\n    else:\n        self.assertEquals((len(result) + struct.calcsize('!I')), input_buffer.tell())\n    return result\n", "label": 0}
{"function": "\n\ndef full_clean(self):\n    'Clean and validate all form fields.'\n    cache_type = (self['cache_type'].data or self['cache_type'].initial)\n    for (iter_cache_type, field) in six.iteritems(self.CACHE_LOCATION_FIELD_MAP):\n        self.fields[field].required = (cache_type == iter_cache_type)\n    return super(GeneralSettingsForm, self).full_clean()\n", "label": 0}
{"function": "\n\ndef choose_form_by_element(self, xpath):\n    elem = self.select(xpath).node()\n    while (elem is not None):\n        if (elem.tag == 'form'):\n            self._lxml_form = elem\n            return\n        else:\n            elem = elem.getparent()\n    self._lxml_form = None\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    ((a, b), (x, y)) = (self, other)\n    return ((a == x) and (b.dtype == y.dtype) and (type(b) == type(y)) and (b.shape == y.shape) and (abs((b - y)).sum() < (1e-06 * b.nnz)))\n", "label": 0}
{"function": "\n\ndef _safe_readinto(self, b):\n    'Same as _safe_read, but for reading into a buffer.'\n    total_bytes = 0\n    mvb = memoryview(b)\n    while (total_bytes < len(b)):\n        if (MAXAMOUNT < len(mvb)):\n            temp_mvb = mvb[0:MAXAMOUNT]\n            n = self.fp.readinto(temp_mvb)\n        else:\n            n = self.fp.readinto(mvb)\n        if (not n):\n            raise IncompleteRead(bytes(mvb[0:total_bytes]), len(b))\n        mvb = mvb[n:]\n        total_bytes += n\n    return total_bytes\n", "label": 0}
{"function": "\n\ndef call_function(self, **parameters):\n    if (not self.interface.takes_kwargs):\n        parameters = {key: value for (key, value) in parameters.items() if (key in self.parameters)}\n    return self.interface(**parameters)\n", "label": 0}
{"function": "\n\ndef test_recursive_simple_and_not(self):\n    for k in range(2, 10):\n        G = self.worst_case_graph(k)\n        cc = sorted(nx.simple_cycles(G))\n        rcc = sorted(nx.recursive_simple_cycles(G))\n        assert_equal(len(cc), len(rcc))\n        for c in cc:\n            assert_true(any((self.is_cyclic_permutation(c, rc) for rc in rcc)))\n", "label": 0}
{"function": "\n\ndef _expected_cols(expected_attrs):\n    'Return expected_attrs that are columns needing joining.\\n\\n    NB: This function may modify expected_attrs if one\\n    requested attribute requires another.\\n    '\n    if (not expected_attrs):\n        return expected_attrs\n    simple_cols = [attr for attr in expected_attrs if (attr in _INSTANCE_OPTIONAL_JOINED_FIELDS)]\n    complex_cols = [('extra.%s' % field) for field in _INSTANCE_EXTRA_FIELDS if (field in expected_attrs)]\n    if complex_cols:\n        simple_cols.append('extra')\n    simple_cols = [x for x in simple_cols if (x not in _INSTANCE_EXTRA_FIELDS)]\n    return (simple_cols + complex_cols)\n", "label": 1}
{"function": "\n\ndef get_footer(email_account, footer=None):\n    'append a footer (signature)'\n    footer = (footer or '')\n    if (email_account and email_account.footer):\n        footer += '<div style=\"margin: 15px auto;\">{0}</div>'.format(email_account.footer)\n    footer += '<!--unsubscribe link here-->'\n    company_address = frappe.db.get_default('email_footer_address')\n    if company_address:\n        footer += '<div style=\"margin: 15px auto; text-align: center; color: #8d99a6\">{0}</div>'.format(company_address.replace('\\n', '<br>'))\n    if (not cint(frappe.db.get_default('disable_standard_email_footer'))):\n        for default_mail_footer in frappe.get_hooks('default_mail_footer'):\n            footer += '<div style=\"margin: 15px auto;\">{0}</div>'.format(default_mail_footer)\n    return footer\n", "label": 0}
{"function": "\n\ndef ancestorOrSelf(domainMemberRelationshipSet, sourceConcept, result=None):\n    if (not result):\n        result = set()\n    if (not (sourceConcept in result)):\n        result.add(sourceConcept)\n        for rels in domainMemberRelationshipSet.toModelObject(sourceConcept):\n            ancestorOrSelf(domainMemberRelationshipSet, rels.fromModelObject, result)\n    return result\n", "label": 0}
{"function": "\n\ndef repo(args):\n    cd_conf = getattr(args, 'cd_conf', None)\n    for hostname in args.host:\n        LOG.debug('Detecting platform for host %s ...', hostname)\n        distro = hosts.get(hostname, username=args.username)\n        rlogger = logging.getLogger(hostname)\n        LOG.info('Distro info: %s %s %s', distro.name, distro.release, distro.codename)\n        if args.remove:\n            distro.packager.remove_repo(args.repo_name)\n        else:\n            install_repo(distro, args, cd_conf, rlogger)\n", "label": 0}
{"function": "\n\ndef get_actions(self, request):\n    actions = super(CommentsAdmin, self).get_actions(request)\n    if ((not request.user.is_superuser) and ('delete_selected' in actions)):\n        actions.pop('delete_selected')\n    if (not request.user.has_perm('comments.can_moderate')):\n        if ('approve_comments' in actions):\n            actions.pop('approve_comments')\n        if ('remove_comments' in actions):\n            actions.pop('remove_comments')\n    return actions\n", "label": 0}
{"function": "\n\ndef worst_case_graph(self, k):\n    G = nx.DiGraph()\n    for n in range(2, (k + 2)):\n        G.add_edge(1, n)\n        G.add_edge(n, (k + 2))\n    G.add_edge(((2 * k) + 1), 1)\n    for n in range((k + 2), ((2 * k) + 2)):\n        G.add_edge(n, ((2 * k) + 2))\n        G.add_edge(n, (n + 1))\n    G.add_edge(((2 * k) + 3), (k + 2))\n    for n in range(((2 * k) + 3), ((3 * k) + 3)):\n        G.add_edge(((2 * k) + 2), n)\n        G.add_edge(n, ((3 * k) + 3))\n    G.add_edge(((3 * k) + 3), ((2 * k) + 2))\n    return G\n", "label": 0}
{"function": "\n\ndef _send_fiff_command(self, command, data=None):\n    'Send a command through the data connection as a fiff tag\\n\\n        Parameters\\n        ----------\\n        command : int\\n            The command code.\\n\\n        data : str\\n            Additional data to send.\\n        '\n    kind = FIFF.FIFF_MNE_RT_COMMAND\n    type = FIFF.FIFFT_VOID\n    size = 4\n    if (data is not None):\n        size += len(data)\n    next = 0\n    msg = np.array(kind, dtype='>i4').tostring()\n    msg += np.array(type, dtype='>i4').tostring()\n    msg += np.array(size, dtype='>i4').tostring()\n    msg += np.array(next, dtype='>i4').tostring()\n    msg += np.array(command, dtype='>i4').tostring()\n    if (data is not None):\n        msg += np.array(data, dtype='>c').tostring()\n    self._data_sock.sendall(msg)\n", "label": 0}
{"function": "\n\ndef _get_pages(self):\n    if (self._pages is None):\n        hits = ((self.hits - 1) - self.orphans)\n        if (hits < 1):\n            hits = 0\n        self._pages = ((hits // self.num_per_page) + 1)\n    return self._pages\n", "label": 0}
{"function": "\n\ndef star_enable(cluster, logdir, cmdline, *args):\n    'Enable specific nodes by name, wildcard, network, or IP glob'\n    if (args and (args[0] != '*')):\n        cluster.enable(args)\n    else:\n        cluster.enable()\n", "label": 0}
{"function": "\n\ndef Bind(self, name, flags=0, lHashVal=0):\n    'Bind to a name'\n    bindptr = BINDPTR()\n    desckind = DESCKIND()\n    ti = POINTER(ITypeInfo)()\n    self.__com_Bind(name, lHashVal, flags, byref(ti), byref(desckind), byref(bindptr))\n    kind = desckind.value\n    if (kind == DESCKIND_FUNCDESC):\n        fd = bindptr.lpfuncdesc[0]\n        fd.__ref__ = weakref.ref(fd, (lambda dead: ti.ReleaseFuncDesc(bindptr.lpfuncdesc)))\n        return ('function', fd)\n    elif (kind == DESCKIND_VARDESC):\n        vd = bindptr.lpvardesc[0]\n        vd.__ref__ = weakref.ref(vd, (lambda dead: ti.ReleaseVarDesc(bindptr.lpvardesc)))\n        return ('variable', vd)\n    elif (kind == DESCKIND_TYPECOMP):\n        return ('type', bindptr.lptcomp)\n    elif (kind == DESCKIND_IMPLICITAPPOBJ):\n        raise NotImplementedError\n    elif (kind == DESCKIND_NONE):\n        raise NameError(('Name %s not found' % name))\n", "label": 0}
{"function": "\n\ndef __buildKey(self, combination):\n    \"Computes the permutations' key based on the given combination\"\n    result = []\n    for key in sorted(combination):\n        value = combination[key]\n        if (value == True):\n            value = 'true'\n        elif (value == False):\n            value = 'false'\n        elif (value == None):\n            value = 'null'\n        result.append(('%s:%s' % (key, value)))\n    return ';'.join(result)\n", "label": 0}
{"function": "\n\ndef linklist(url):\n    'Return a JSON list of all on-domain links on the page.'\n    apexdom = apex(url)\n    import lxml.html\n    ro = requests.get(url, timeout=5)\n    doc = lxml.html.fromstring(ro.text)\n    doc.make_links_absolute(url)\n    somelinks = doc.xpath('/html/body//a/@href')\n    plinks = set()\n    for alink in somelinks:\n        if (urlparse.urlparse(alink)[1][(- len(apexdom)):] == apexdom):\n            plinks.add(alink)\n    plinks = list(plinks)\n    return plinks\n", "label": 0}
{"function": "\n\ndef runcode(self, code):\n    (sys.stdout, sys.stderr) = (StringIO(), StringIO())\n    InteractiveConsole.runcode(self, code)\n    (output, errors) = (sys.stdout.getvalue(), sys.stderr.getvalue())\n    (sys.stdout, sys.stderr) = (sys.__stdout__, sys.__stderr__)\n    if (output or errors):\n        self.wait_for_user_input()\n    if output:\n        self.write(output)\n    if errors:\n        self.write(errors)\n", "label": 0}
{"function": "\n\ndef getmoduleinfo(path):\n    'Get the module name, suffix, mode, and module type for a given file.'\n    filename = os.path.basename(path)\n\n    def _f(suffix, mode, mtype):\n        return ((- len(suffix)), suffix, mode, mtype)\n    suffixes = map(_f, imp.get_suffixes())\n    suffixes.sort()\n    for (neglen, suffix, mode, mtype) in suffixes:\n        if (filename[neglen:] == suffix):\n            return (filename[:neglen], suffix, mode, mtype)\n", "label": 0}
{"function": "\n\ndef get_tvtk_name(vtk_name):\n    \"Converts a VTK class name to a TVTK class name.\\n\\n    This function additionally converts any leading digits into a\\n    suitable string.\\n\\n    For example:\\n\\n      >>> get_tvtk_name('vtk3DSImporter')\\n      'ThreeDSImporter'\\n      >>> get_tvtk_name('vtkXMLDataReader')\\n      'XMLDataReader'\\n\\n    \"\n    if (vtk_name[:3] == 'vtk'):\n        name = vtk_name[3:]\n        dig2name = {\n            '1': 'One',\n            '2': 'Two',\n            '3': 'Three',\n            '4': 'Four',\n            '5': 'Five',\n            '6': 'Six',\n            '7': 'Seven',\n            '8': 'Eight',\n            '9': 'Nine',\n            '0': 'Zero',\n        }\n        if (name[0] in string.digits):\n            return (dig2name[name[0]] + name[1:])\n        else:\n            return name\n    else:\n        return vtk_name\n", "label": 0}
{"function": "\n\n@base.remotable\ndef refresh(self, use_slave=False):\n    extra = [field for field in INSTANCE_OPTIONAL_ATTRS if self.obj_attr_is_set(field)]\n    current = self.__class__.get_by_uuid(self._context, uuid=self.uuid, expected_attrs=extra, use_slave=use_slave)\n    current._context = None\n    for field in self.fields:\n        if self.obj_attr_is_set(field):\n            if (field == 'info_cache'):\n                self.info_cache.refresh()\n            elif (self[field] != current[field]):\n                self[field] = current[field]\n    self.obj_reset_changes()\n", "label": 0}
{"function": "\n\ndef joinBank(self, owner):\n    bank = self.getKernel().serviceManager().equipmentService().getEquippedObject(owner, 'bank')\n    if bank:\n        if (bank.scene_id == owner.scene_id):\n            SystemMessage.sendSystemMessage(owner, 'system_msg', 'already_member_of_bank', False, False)\n        elif (bank.scene_id == 0):\n            bank.scene_id = owner.scene_id\n            SystemMessage.sendSystemMessage(owner, swgpy.OutOfBand('system_msg', 'succesfully_joined_bank'), False, False)\n        else:\n            SystemMessage.sendSystemMessage(owner, swgpy.OutOfBand('system_msg', 'member_of_different_bank'), False, False)\n", "label": 0}
{"function": "\n\ndef hostname_or_ip(self, address):\n    result = self._cache.get(address, None)\n    now = time.time()\n    if ((result is None) or (result[0] <= now)):\n        logging.debug('Cache miss for %s in hostname_or_ip', address)\n        result = ((now + self.timeout), self._hostname_or_ip(address))\n        self._cache[address] = result\n    return result[1]\n", "label": 0}
{"function": "\n\ndef read_model_data(self, row):\n    models = {\n        \n    }\n    for (model_class, column_data) in self.columns_to_compare.items():\n        models[model_class] = []\n        for (idx, col_name) in column_data:\n            models[model_class].append(row[idx])\n    return models\n", "label": 0}
{"function": "\n\ndef assertParser(self, _cli, parser_args, argv):\n    '\\n        Assertion method for subparsers, subcommands and positional arguments.\\n        Parameter argv is a list where the first item is the name of the subparser,\\n        followed by the subcommand and the remaining items are positional arguments\\n        '\n    number_of_parsing_levels = 2\n    cmd = 'cmd_{0}_{1}'.format(argv[0], argv[1]).replace('-', '_')\n    self.assertEqual(parser_args.func.__name__, cmd)\n    self.assertTrue((argv[1] in _cli.subcommands))\n    self.assertTrue((argv[0] in _cli.subparsers))\n    number_of_positional_arguments = (len(argv) - number_of_parsing_levels)\n    if number_of_positional_arguments:\n        positional_args = []\n        for item in parser_args.func.arguments:\n            arg = item[0][0]\n            if arg.startswith('--'):\n                break\n            positional_args.append(arg)\n        for (index, arg) in enumerate(positional_args, start=number_of_parsing_levels):\n            args_type = parser_args.func.arguments[(index - number_of_parsing_levels)][1].get('type')\n            if args_type:\n                self.assertEqual(getattr(parser_args, arg), args_type(argv[index]))\n            else:\n                self.assertEqual(getattr(parser_args, arg), argv[index])\n            self.assertRaises(SystemExit, self.cli, argv[:index])\n", "label": 0}
{"function": "\n\ndef start(self, job_id, job_tag):\n    \"\\n        Apply the 'start' action to the session object\\n        If the request can be accepted it modifies the relevant fields\\n        and sets the need_update member to notify that the stored\\n        document needs to be updated\\n        \"\n    now = int(time.time())\n    time_since_last_start = (now - self.doc.get('time_start', 0))\n    if (job_tag > self.session_tag):\n        raise freezer_api_exc.BadDataFormat('requested tag value too high')\n    if (time_since_last_start <= self.doc.get('hold_off', 60)):\n        if (job_tag < self.session_tag):\n            self.action_result = 'success'\n            self.set_job_start(job_id, now)\n            self.need_update = True\n        else:\n            self.action_result = 'hold-off'\n            self.need_update = False\n    elif (time_since_last_start > self.doc.get('hold_off', 60)):\n        if (self.session_tag == job_tag):\n            self.session_tag += 1\n            self.doc['time_start'] = now\n            self.doc['status'] = 'running'\n            self.doc['result'] = ''\n            self.action_result = 'success'\n            self.set_job_start(job_id, now)\n            self.need_update = True\n        else:\n            self.action_result = 'out-of-sync'\n            self.need_update = False\n", "label": 0}
{"function": "\n\ndef onexit():\n    global kafka, consumer, producer\n    if consumer:\n        consumer.commit()\n        consumer.stop()\n        consumer = None\n    if producer:\n        producer.stop()\n        producer = None\n    if kafka:\n        kafka.close()\n        kafka = None\n    print('remote_rainter {0} is shutting down'.format(os.getpid()))\n", "label": 0}
{"function": "\n\ndef getNgrams(query, corpus, startYear, endYear, smoothing, caseInsensitive):\n    params = dict(content=query, year_start=startYear, year_end=endYear, corpus=corpora[corpus], smoothing=smoothing, case_insensitive=caseInsensitive)\n    if (params['case_insensitive'] is False):\n        params.pop('case_insensitive')\n    if ('?' in params['content']):\n        params['content'] = params['content'].replace('?', '*')\n    if ('@' in params['content']):\n        params['content'] = params['content'].replace('@', '=>')\n    req = requests.get('http://books.google.com/ngrams/graph', params=params)\n    res = re.findall('var data = (.*?);\\\\n', req.text)\n    if res:\n        data = {qry['ngram']: qry['timeseries'] for qry in literal_eval(res[0])}\n        df = DataFrame(data)\n        df.insert(0, 'year', list(range(startYear, (endYear + 1))))\n    else:\n        df = DataFrame()\n    return (req.url, params['content'], df)\n", "label": 0}
{"function": "\n\ndef get_vexrc(options, environ):\n    'Get a representation of the contents of the config file.\\n\\n    :returns:\\n        a Vexrc instance.\\n    '\n    if (options.config and (not os.path.exists(options.config))):\n        raise exceptions.InvalidVexrc('nonexistent config: {0!r}'.format(options.config))\n    filename = (options.config or os.path.expanduser('~/.vexrc'))\n    vexrc = config.Vexrc.from_file(filename, environ)\n    return vexrc\n", "label": 0}
{"function": "\n\ndef random(pages=1):\n    '\\n  Get a list of random Wikipedia article titles.\\n\\n  .. note:: Random only gets articles from namespace 0, meaning no Category, User talk, or other meta-Wikipedia pages.\\n\\n  Keyword arguments:\\n\\n  * pages - the number of random pages returned (max of 10)\\n  '\n    query_params = {\n        'list': 'random',\n        'rnnamespace': 0,\n        'rnlimit': pages,\n    }\n    request = _wiki_request(query_params)\n    titles = [page['title'] for page in request['query']['random']]\n    if (len(titles) == 1):\n        return titles[0]\n    return titles\n", "label": 0}
{"function": "\n\ndef AddOperands(self, lhs, rhs):\n    if (isinstance(lhs, Expression) and isinstance(rhs, Expression)):\n        self.args.insert(0, lhs)\n        self.args.append(rhs)\n    else:\n        raise ParseError(('Expected expression, got %s %s %s' % (lhs, self.operator, rhs)))\n", "label": 0}
{"function": "\n\ndef outputSide(self):\n    if (not self.getIsSidebarMode()):\n        return\n    (maxy, maxx) = self.screenControl.getScreenDimensions()\n    borderX = (maxx - self.WIDTH)\n    if (self.mode == COMMAND_MODE):\n        borderX = (len(SHORT_COMMAND_PROMPT) + 20)\n    usageLines = usageStrings.USAGE_PAGE.split('\\n')\n    if (self.mode == COMMAND_MODE):\n        usageLines = usageStrings.USAGE_COMMAND.split('\\n')\n    for (index, usageLine) in enumerate(usageLines):\n        self.printer.addstr((self.getMinY() + index), (borderX + 2), usageLine)\n    for y in range(self.getMinY(), maxy):\n        self.printer.addstr(y, borderX, '|')\n", "label": 0}
{"function": "\n\ndef withinregion(x, y, region):\n    return ((x > region[0]) and (x < (region[0] + region[2])) and (y > region[1]) and (y < (region[1] + region[3])))\n", "label": 0}
{"function": "\n\ndef get_imm_property(ipmicmd, propname):\n    propname = propname.encode('utf-8')\n    proplen = (len(propname) | 128)\n    cmdlen = (len(propname) + 1)\n    cdata = (bytearray([0, 0, cmdlen, proplen]) + propname)\n    rsp = ipmicmd.xraw_command(netfn=58, command=196, data=cdata)\n    rsp['data'] = bytearray(rsp['data'])\n    if (rsp['data'][0] != 0):\n        return None\n    propdata = rsp['data'][3:]\n    if (propdata[0] & 128):\n        return str(propdata[1:]).rstrip(' \\x00')\n    else:\n        raise Exception(('Unknown format for property: ' + repr(propdata)))\n", "label": 0}
{"function": "\n\ndef clean(self, value):\n    return [v.strip() for v in value.split(',') if v.strip()]\n", "label": 0}
{"function": "\n\ndef clean_url(self):\n    '\\n        Validate the input for field *url* checking if the specified\\n        URL already exists. If it is an actual update and the URL has\\n        not been changed, validation will be skipped.\\n\\n        Returns cleaned URL or raises an exception.\\n        '\n    url = self.cleaned_data['url']\n    if ('url' in self.changed_data):\n        if (not url.endswith('/')):\n            url += '/'\n        URLDoesNotExistValidator()(url)\n    return url\n", "label": 0}
{"function": "\n\n@classmethod\ndef CreateWithAllComps(cls, oDict, obsPrior, compDictList):\n    ' Create MultObsModel, all K component Distr objects,\\n         and the prior Distr object in one call\\n    '\n    if (oDict['inferType'] == 'EM'):\n        raise NotImplementedError('TODO')\n    self = cls(oDict['inferType'], obsPrior=obsPrior)\n    self.K = len(compDictList)\n    self.comp = [None for k in range(self.K)]\n    for k in xrange(self.K):\n        self.comp[k] = BetaDistr(**compDictList[k])\n    return self\n", "label": 0}
{"function": "\n\ndef _find_arch(mycc):\n    for (i, cc) in enumerate(SUPPORTED_CC):\n        if (cc == mycc):\n            return cc\n        elif (cc > mycc):\n            if (i == 0):\n                raise NvvmSupportError(('GPU compute capability %d.%d is not supported (requires >=2.0)' % mycc))\n            else:\n                return SUPPORTED_CC[(i - 1)]\n    return SUPPORTED_CC[(- 1)]\n", "label": 0}
{"function": "\n\n@internationalizeDocstring\ndef levenshtein(self, irc, msg, args, s1, s2):\n    '<string1> <string2>\\n\\n        Returns the levenshtein distance (also known as the \"edit distance\"\\n        between <string1> and <string2>)\\n        '\n    max = self.registryValue('levenshtein.max')\n    if ((len(s1) > max) or (len(s2) > max)):\n        irc.error(_('Levenshtein distance is a complicated algorithm, try it with some smaller inputs.'))\n    else:\n        irc.reply(str(utils.str.distance(s1, s2)))\n", "label": 0}
{"function": "\n\ndef get_stack(limit=10):\n    if (not DEBUG):\n        return ()\n    frame = sys._getframe(1)\n    lines = []\n    while ((len(lines) < limit) and (frame is not None)):\n        f_locals = frame.f_locals\n        ndb_debug = f_locals.get('__ndb_debug__')\n        if (ndb_debug != 'SKIP'):\n            line = frame_info(frame)\n            if (ndb_debug is not None):\n                line += (' # ' + str(ndb_debug))\n            lines.append(line)\n        frame = frame.f_back\n    return lines\n", "label": 0}
{"function": "\n\ndef _validate_logical_condition_counts(fn, conditions):\n    if (len(conditions) < 2):\n        raise IntrinsicFuncInputError(fn._errmsg_min)\n    elif (len(conditions) > 10):\n        raise IntrinsicFuncInputError(fn._errmsg_max)\n", "label": 0}
{"function": "\n\ndef on_activated(self, view):\n    if ((view.settings().get('git_view') == 'status') and get_setting('git_update_status_on_focus', True)):\n        goto = None\n        if view.sel():\n            goto = ('point:%s' % view.sel()[0].begin())\n        view.run_command('git_status_refresh', {\n            'goto': goto,\n        })\n", "label": 0}
{"function": "\n\ndef uninstall(packages, purge=False, options=None):\n    '\\n    Remove one or more packages.\\n\\n    If *purge* is ``True``, the package configuration files will be\\n    removed from the system.\\n\\n    Extra *options* may be passed to ``apt-get`` if necessary.\\n    '\n    manager = MANAGER\n    command = ('purge' if purge else 'remove')\n    if (options is None):\n        options = []\n    if (not isinstance(packages, basestring)):\n        packages = ' '.join(packages)\n    options.append('--assume-yes')\n    options = ' '.join(options)\n    cmd = ('%(manager)s %(command)s %(options)s %(packages)s' % locals())\n    run_as_root(cmd, pty=False)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (type(other) != type(self)):\n        return False\n    return ((self.display_name == other.display_name) and (self.username == other.username) and (self.domain == other.domain))\n", "label": 0}
{"function": "\n\ndef formfield_for_foreignkey(self, db_field, request, **kwargs):\n    if ((db_field.name == 'lifestream') and (not request.user.is_superuser)):\n        kwargs['queryset'] = Lifestream.objects.filter(user=request.user)\n        return db_field.formfield(**kwargs)\n    return super(FeedAdmin, self).formfield_for_foreignkey(db_field, request, **kwargs)\n", "label": 0}
{"function": "\n\ndef _isgroup(obj, *args, **kws):\n    \"return whether argument is a group or the name of a group\\n\\n    With additional arguments (all must be strings), it also tests\\n    that the group has an an attribute named for each argument. This\\n    can be used to test not only if a object is a Group, but whether\\n    it a group with expected arguments.\\n\\n        > x = 10\\n        > g = group(x=x, y=2)\\n        > isgroup(g), isgroup(x)\\n        True, False\\n        > isgroup('g'), isgroup('x')\\n        True, False\\n        > isgroup(g, 'x', 'y')\\n        True\\n        > isgroup(g, 'x', 'y', 'z')\\n        False\\n\\n    \"\n    _larch = kws.get('_larch', None)\n    if (_larch is None):\n        raise Warning('cannot run isgroup() -- larch broken?')\n    stable = _larch.symtable\n    if (isinstance(obj, (str, unicode)) and stable.has_symbol(obj)):\n        obj = stable.get_symbol(obj)\n    return isgroup(obj, *args)\n", "label": 0}
{"function": "\n\ndef stripcomments(url):\n    'Return HTML with comments stripped out'\n    html = url\n    if checkurl(url):\n        html = gethtml(url)\n        if (not html):\n            return None\n    pattern = '<!--.*?-->'\n    pat = re.compile(pattern, (re.IGNORECASE | re.DOTALL))\n    byeblock = pat.sub('', html)\n    return byeblock\n", "label": 0}
{"function": "\n\ndef clean_enabled_services(self):\n    'Clean the enabled_services field.\\n\\n        Raises:\\n            django.core.exceptions.ValidationError:\\n                Raised if an unknown service is attempted to be enabled.\\n        '\n    for service_id in self.cleaned_data['enabled_services']:\n        if (not avatar_services.has_service(service_id)):\n            raise ValidationError(('Unknown service \"%s\"' % service_id))\n    return self.cleaned_data['enabled_services']\n", "label": 0}
{"function": "\n\n@attr(migrated_tenant=['admin', 'tenant1', 'tenant2'])\ndef test_tenants_volumes_on_dst(self):\n    \"Validate deleted tenant's volumes were migrated.\"\n    undeleted_volumes = []\n    for (tenant_name, tenant) in self.deleted_tenants:\n        tenant_undeleted_volumes = self._tenant_volumes_exist_on_dst(tenant['cinder_volumes'])\n        if tenant_undeleted_volumes:\n            undeleted_volumes.append({\n                tenant_name: tenant_undeleted_volumes,\n            })\n    if undeleted_volumes:\n        msg = \"Tenant's cinder volumes with ids {0} exist on destination, but should be deleted!\"\n        self.fail(msg.format(undeleted_volumes))\n", "label": 0}
{"function": "\n\ndef __call__(self, environ, start_response):\n    '\\n        A note about the zappa cookie: Only 1 cookie can be passed through API\\n        Gateway. Hence all cookies are packed into a special cookie, the\\n        zappa cookie. There are a number of problems with this:\\n\\n            * updates of single cookies, when there are multiple present results\\n              in deletion of the ones that are not being updated.\\n            * expiration of cookies. The client no longer knows when cookies\\n              expires.\\n\\n        The first is solved by unpacking the zappa cookie on each request and\\n        saving all incoming cookies. The response Set-Cookies are then used\\n        to update the saved cookies, which are packed and set as the zappa\\n        cookie.\\n\\n        The second is solved by filtering cookies on their expiration time,\\n        only passing cookies that are still valid to the WSGI app.\\n        '\n    self.start_response = start_response\n    parsed = parse_cookie(environ)\n    if ('zappa' in parsed):\n        self.decode_zappa_cookie(parsed['zappa'])\n        self.filter_expired_cookies()\n        environ['HTTP_COOKIE'] = self.cookie_environ_string()\n    else:\n        self.request_cookies = dict()\n    response = self.application(environ, self.encode_response)\n    if self.redirect_content:\n        response = [self.redirect_content for item in response]\n    return ClosingIterator(response)\n", "label": 0}
{"function": "\n\ndef getch(self, *args):\n    if (len(args) == 0):\n        val = lib.wgetch(self._win)\n    elif (len(args) == 2):\n        val = lib.mvwgetch(self._win, *args)\n    else:\n        raise error('getch requires 0 or 2 arguments')\n    return val\n", "label": 0}
{"function": "\n\ndef _render(self, request, formencode=False, realm=None):\n    'Render a signed request according to signature type\\n\\n        Returns a 3-tuple containing the request URI, headers, and body.\\n\\n        If the formencode argument is True and the body contains parameters, it\\n        is escaped and returned as a valid formencoded string.\\n        '\n    (uri, headers, body) = (request.uri, request.headers, request.body)\n    if (self.signature_type == SIGNATURE_TYPE_AUTH_HEADER):\n        headers = parameters.prepare_headers(request.oauth_params, request.headers, realm=realm)\n    elif ((self.signature_type == SIGNATURE_TYPE_BODY) and (request.decoded_body is not None)):\n        body = parameters.prepare_form_encoded_body(request.oauth_params, request.decoded_body)\n        if formencode:\n            body = urlencode(body)\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    elif (self.signature_type == SIGNATURE_TYPE_QUERY):\n        uri = parameters.prepare_request_uri_query(request.oauth_params, request.uri)\n    else:\n        raise ValueError('Unknown signature type specified.')\n    return (uri, headers, body)\n", "label": 0}
{"function": "\n\n@classmethod\ndef from_obj(cls, obj, return_obj=None):\n    if (not obj):\n        return None\n    if (not return_obj):\n        return_obj = cls()\n    super(KillChainPhaseReference, cls).from_obj(obj, return_obj=return_obj)\n    return_obj.kill_chain_id = obj.kill_chain_id\n    return_obj.kill_chain_name = obj.kill_chain_name\n    return return_obj\n", "label": 0}
{"function": "\n\ndef getElementById(self, id):\n    childNodes = self.childNodes[:]\n    while childNodes:\n        node = childNodes.pop(0)\n        if node.childNodes:\n            childNodes.extend(node.childNodes)\n        if (hasattr(node, 'getAttribute') and (node.getAttribute('id') == id)):\n            return node\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((type(other) is CSM) and (other.format == self.format) and _kmap_eq(self.kmap, other.kmap))\n", "label": 0}
{"function": "\n\ndef run(self, parent, blocks):\n    block = blocks.pop(0)\n    prelines = block[:self.match.start()].rstrip('\\n')\n    if prelines:\n        self.parser.parseBlocks(parent, [prelines])\n    hr = util.etree.SubElement(parent, 'hr')\n    postlines = block[self.match.end():].lstrip('\\n')\n    if postlines:\n        blocks.insert(0, postlines)\n", "label": 0}
{"function": "\n\n@permission_checker.require('delete')\ndef delete(request, image_id):\n    image = get_object_or_404(get_image_model(), id=image_id)\n    if (not permission_policy.user_has_permission_for_instance(request.user, 'delete', image)):\n        return permission_denied(request)\n    if (request.method == 'POST'):\n        image.delete()\n        messages.success(request, _(\"Image '{0}' deleted.\").format(image.title))\n        return redirect('wagtailimages:index')\n    return render(request, 'wagtailimages/images/confirm_delete.html', {\n        'image': image,\n    })\n", "label": 0}
{"function": "\n\ndef on_text_command(self, view, command_name, args):\n    isDragSelect = (command_name == 'drag_select')\n    if (args == None):\n        return\n    isSelectingWord = (('by' in args.keys()) and (args['by'] == 'words'))\n    if (isDragSelect and isSelectingWord and (view.name() == 'Fuse - Auto Reload Result')):\n        view.run_command('fuse_goto_location')\n", "label": 0}
{"function": "\n\ndef get_object_or_id(self, instance):\n    rel_id = instance._data.get(self.att_name)\n    if ((rel_id is not None) or (self.att_name in instance._obj_cache)):\n        if (self.att_name not in instance._obj_cache):\n            obj = self.rel_model.get((self.field.to_field == rel_id))\n            instance._obj_cache[self.att_name] = obj\n        return instance._obj_cache[self.att_name]\n    elif (not self.field.null):\n        raise self.rel_model.DoesNotExist\n    return rel_id\n", "label": 0}
{"function": "\n\ndef _safe_read(self, amt):\n    'Read the number of bytes requested, compensating for partial reads.\\n\\n        Normally, we have a blocking socket, but a read() can be interrupted\\n        by a signal (resulting in a partial read).\\n\\n        Note that we cannot distinguish between EOF and an interrupt when zero\\n        bytes have been read. IncompleteRead() will be raised in this\\n        situation.\\n\\n        This function should be used when <amt> bytes \"should\" be present for\\n        reading. If the bytes are truly not available (due to EOF), then the\\n        IncompleteRead exception can be used to detect the problem.\\n        '\n    s = []\n    while (amt > 0):\n        chunk = self.fp.read(min(amt, MAXAMOUNT))\n        if (not chunk):\n            raise IncompleteRead(bytes(b'').join(s), amt)\n        s.append(chunk)\n        amt -= len(chunk)\n    return bytes(b'').join(s)\n", "label": 0}
{"function": "\n\ndef match(self, node):\n    if ((node.type in (token.COLON, token.COMMA, token.SEMI)) and (node.get_suffix() != ' ')):\n        if ((node.get_suffix().find('\\n') == 0) or (node.next_sibling and node.next_sibling.children and (node.next_sibling.children[0] == Newline()))):\n            return False\n        if (node.parent.type in [symbols.subscript, symbols.sliceop]):\n            return False\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef process_chunk(self, start, end, chunk):\n    'Called when chunk of data should be processed.\\n        Removes existing records if such option was provided and re-populates\\n        the aggregates.\\n\\n        '\n    if (self.clear_existing and self.chunk_delete):\n        self.clear_past_results(start, end)\n    self.call_command_verbose('db_refresh_mviews', **{\n        'force': True,\n        'start': start,\n        'end': end,\n        'clear-existing': False,\n    })\n    self.call_command_verbose('db_update_agregates', **{\n        'start': start,\n        'end': end,\n        'clear-existing': False,\n    })\n    return True\n", "label": 0}
{"function": "\n\ndef _get_status(self, event):\n    state = event.get('state')\n    state_description = event.get('state_description')\n    status = 'UNKNOWN'\n    status_map = {\n        'building': 'BUILD',\n        'stopped': 'SHUTOFF',\n        'paused': 'PAUSED',\n        'suspended': 'SUSPENDED',\n        'rescued': 'RESCUE',\n        'error': 'ERROR',\n        'deleted': 'DELETED',\n        'soft-delete': 'SOFT_DELETED',\n        'shelved': 'SHELVED',\n        'shelved_offloaded': 'SHELVED_OFFLOADED',\n    }\n    if (state in status_map):\n        status = status_map[state]\n    if (state == 'resized'):\n        if (state_description == 'resize_reverting'):\n            status = 'REVERT_RESIZE'\n        else:\n            status = 'VERIFY_RESIZE'\n    if (state == 'active'):\n        active_map = {\n            'rebooting': 'REBOOT',\n            'rebooting_hard': 'HARD_REBOOT',\n            'updating_password': 'PASSWORD',\n            'rebuilding': 'REBUILD',\n            'rebuild_block_device_mapping': 'REBUILD',\n            'rebuild_spawning': 'REBUILD',\n            'migrating': 'MIGRATING',\n            'resize_prep': 'RESIZE',\n            'resize_migrating': 'RESIZE',\n            'resize_migrated': 'RESIZE',\n            'resize_finish': 'RESIZE',\n        }\n        status = active_map.get(state_description, 'ACTIVE')\n    if (status == 'UNKNOWN'):\n        logger.error(('Unknown status for event %s: state %s (%s)' % (event.get('message_id'), state, state_description)))\n    return status\n", "label": 0}
{"function": "\n\ndef setUp(self):\n    super(VerifyDstDeletedTenantResources, self).setUp()\n    self.dst_cloud.switch_user(user=self.dst_cloud.username, password=self.dst_cloud.password, tenant=self.dst_cloud.tenant)\n    self.deleted_tenants = [(tenant['name'], tenant) for tenant in config.tenants if (('deleted' in tenant) and (tenant['deleted'] is True))]\n    self.dst_tenants = {ten.name: ten.id for ten in self.dst_cloud.keystoneclient.tenants.list()}\n    dst_cinder_volumes = self.dst_cloud.cinderclient.volumes.list(search_opts={\n        'all_tenants': 0,\n    })\n    self.dst_volumes_admin = {vol.display_name: vol for vol in dst_cinder_volumes}\n    self.dst_vm_list = self.dst_cloud.novaclient.servers.list(search_opts={\n        'tenant_id': self.dst_tenants[self.dst_cloud.tenant],\n    })\n", "label": 0}
{"function": "\n\ndef func(self):\n    '\\n        Create a new message and send it to channel, using\\n        the already formatted input.\\n        '\n    caller = self.caller\n    (channelkey, msg) = self.args\n    if (not msg):\n        caller.msg('Say what?')\n        return\n    channel = ChannelDB.objects.get_channel(channelkey)\n    if (not channel):\n        caller.msg((\"Channel '%s' not found.\" % channelkey))\n        return\n    if (not channel.has_connection(caller)):\n        string = \"You are not connected to channel '%s'.\"\n        caller.msg((string % channelkey))\n        return\n    if (not channel.access(caller, 'send')):\n        string = \"You are not permitted to send to channel '%s'.\"\n        caller.msg((string % channelkey))\n        return\n    msg = ('[%s] %s: %s' % (channel.key, caller.name, msg))\n    msgobj = create.create_message(caller, msg, channels=[channel])\n    channel.msg(msgobj)\n", "label": 0}
{"function": "\n\ndef show_tip(self, tip):\n    ' Attempts to show the specified tip at the current cursor location.\\n        '\n    text_edit = self._text_edit\n    document = text_edit.document()\n    cursor = text_edit.textCursor()\n    search_pos = (cursor.position() - 1)\n    (self._start_position, _) = self._find_parenthesis(search_pos, forward=False)\n    if (self._start_position == (- 1)):\n        return False\n    self.setText(tip)\n    self.resize(self.sizeHint())\n    padding = 3\n    cursor_rect = text_edit.cursorRect(cursor)\n    screen_rect = QtGui.QApplication.desktop().screenGeometry(text_edit)\n    point = text_edit.mapToGlobal(cursor_rect.bottomRight())\n    point.setY((point.y() + padding))\n    tip_height = self.size().height()\n    if ((point.y() + tip_height) > screen_rect.height()):\n        point = text_edit.mapToGlobal(cursor_rect.topRight())\n        point.setY(((point.y() - tip_height) - padding))\n    self.move(point)\n    self.show()\n    return True\n", "label": 0}
{"function": "\n\n@property\ndef tier_names(self):\n    result = set()\n    for (_, ag) in self:\n        for tier_name in ag.structure_type_handler.flat_data_hierarchy:\n            result.add(tier_name)\n    return result\n", "label": 0}
{"function": "\n\ndef _urllib2_fetch(self, uri, params, method=None):\n    if (self.opener == None):\n        self.opener = urllib2.build_opener(HTTPErrorProcessor)\n        urllib2.install_opener(self.opener)\n    if (method and (method == 'GET')):\n        uri = self._build_get_uri(uri, params)\n        req = PlivoUrlRequest(uri)\n    else:\n        req = PlivoUrlRequest(uri, urllib.urlencode(params))\n        if (method and ((method == 'DELETE') or (method == 'PUT'))):\n            req.http_method = method\n    authstring = base64.encodestring(('%s:%s' % (self.auth_id, self.auth_token)))\n    authstring = authstring.replace('\\n', '')\n    req.add_header('Authorization', ('Basic %s' % authstring))\n    response = urllib2.urlopen(req)\n    return response.read()\n", "label": 0}
{"function": "\n\ndef _dump_item(self, item, indent=''):\n    if isBadItem(item):\n        return\n    v = item.value\n    if v.vsHasField('children'):\n        (yield '{indent:s}<{tag:s}>'.format(indent=indent, tag=getTagName(item.header)))\n        for (_, c) in v.children:\n            if isBadItem(c):\n                continue\n            for l in self._dump_item(c, (indent + '  ')):\n                (yield l)\n        (yield '{indent:s}</{tag:s}>'.format(indent=indent, tag=getTagName(item.header)))\n    else:\n        (yield \"{indent:s}<{tag:s} type='{type_:s}'>{data:s}</{tag:s}>\".format(indent=indent, type_=formatValueType(item), data=self._formatValue(item), tag=getTagName(item.header)))\n", "label": 0}
{"function": "\n\ndef read_subject(self, subject_type='', predicate='', object='', assoc='', id_only=False):\n    if assoc:\n        if (type(assoc) is str):\n            assoc = self.read_association(assoc)\n        return (assoc.s if id_only else self.read(assoc.s))\n    else:\n        (sub_list, assoc_list) = self.find_subjects(subject_type=subject_type, predicate=predicate, object=object, id_only=True)\n        if (not sub_list):\n            raise NotFound(('No subject found for subject_type=%s, predicate=%s, object=%s' % (subject_type, predicate, object)))\n        elif (len(sub_list) > 1):\n            raise Inconsistent(('More than one subject found for subject_type=%s, predicate=%s, object=%s: count=%s' % (subject_type, predicate, object, len(sub_list))))\n        return (sub_list[0] if id_only else self.read(sub_list[0]))\n", "label": 0}
{"function": "\n\ndef _frame_loop(self, frame):\n    if self.heartbeat:\n        self.last_received_frame = time.time()\n    self.channels[frame.channel].process_frame(frame)\n    self._frame_count += 1\n    if self.stream:\n        if (self._frame_count == 5):\n            self._frame_count = 0\n            cb = (lambda : FrameReader(self.stream, self._frame_loop))\n            self._add_ioloop_callback(cb)\n        else:\n            FrameReader(self.stream, self._frame_loop)\n", "label": 0}
{"function": "\n\ndef _remove_expired_items_from_cache(self):\n    'Remove expired items from temporary URL cache\\n\\n        This function removes entries that will expire before the expected\\n        usage time.\\n        '\n    max_valid_time = (int(time.time()) + CONF.glance.swift_temp_url_expected_download_start_delay)\n    keys_to_remove = [k for (k, v) in six.iteritems(self._cache) if (v.url_expires_at < max_valid_time)]\n    for k in keys_to_remove:\n        del self._cache[k]\n", "label": 0}
{"function": "\n\ndef __init__(self, user, skip=None, *args, **kwargs):\n    if (skip is None):\n        skip = []\n    super(FilterForm, self).__init__(*args, **kwargs)\n    if ('name' in skip):\n        del self.fields['name']\n    else:\n        self.fields['name'].required = False\n        self.fields['name'].label = _('Name')\n    if ('contact_type' in skip):\n        del self.fields['contact_type']\n    else:\n        self.fields['contact_type'].queryset = Object.filter_permitted(user, ContactType.objects)\n        self.fields['contact_type'].required = True\n        self.fields['contact_type'].label = _('Contact type')\n", "label": 0}
{"function": "\n\ndef get_default_exprs(self, col, col_type):\n    if (col in self.col_blacklist):\n        return {\n            \n        }\n    d = {\n        \n    }\n    d[(col, 'non_null')] = 'COUNT({col})'\n    if (col_type in ['double', 'int', 'bigint', 'float', 'double']):\n        d[(col, 'sum')] = 'SUM({col})'\n        d[(col, 'min')] = 'MIN({col})'\n        d[(col, 'max')] = 'MAX({col})'\n        d[(col, 'avg')] = 'AVG({col})'\n    elif (col_type == 'boolean'):\n        d[(col, 'true')] = 'SUM(CASE WHEN {col} THEN 1 ELSE 0 END)'\n        d[(col, 'false')] = 'SUM(CASE WHEN NOT {col} THEN 1 ELSE 0 END)'\n    elif (col_type in ['string']):\n        d[(col, 'len')] = 'SUM(CAST(LENGTH({col}) AS BIGINT))'\n        d[(col, 'approx_distinct')] = 'APPROX_DISTINCT({col})'\n    return {k: v.format(col=col) for (k, v) in d.items()}\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    return '\\n'.join([('%s %s\\n  %s' % (record.name, record.levelname, '\\n'.join([line for line in record.getMessage().split('\\n') if line.strip()]))) for record in self.records])\n", "label": 0}
{"function": "\n\ndef archive(self, ref, destination, *archive_args):\n    '\\n        Archive the specified GIT ref to the specified destination\\n\\n        Any extra args are passed to the sh command directly so you can\\n        add extra flags for `git archive` should you desire.\\n        '\n    tmp_dir = tempfile.mkdtemp()\n    tar_file = None\n    try:\n        git = sh.git.bake(_cwd=tmp_dir, _env={\n            'GIT_SSH': 'loose_ssh.sh',\n        })\n        self.log.debug(('Cloning %s to %s' % (self.url, tmp_dir)))\n        git('clone', '--recursive', self.url, tmp_dir)\n        self.log.debug(\"Archiving '{ref}' to {destination}\".format(ref=ref, destination=destination))\n        settings.mkdir_p(destination)\n        (fd, tar_file) = tempfile.mkstemp()\n        git.archive(('origin/%s' % ref), *archive_args, _out=tar_file)\n        sh.tar('xpf', tar_file, _cwd=destination)\n    finally:\n        if tar_file:\n            os.remove(tar_file)\n        if os.path.isdir(tmp_dir):\n            shutil.rmtree(tmp_dir)\n", "label": 0}
{"function": "\n\ndef use(self, player, game):\n    super().use(player, game)\n    targets = copy.copy(player.game.current_player.minions)\n    if ((self.target.card.minion_type is MINION_TYPE.DEMON) and (self.target in targets)):\n        self.target.change_attack(5)\n        self.target.increase_health(5)\n    else:\n        self.target.damage(player.effective_spell_damage(5), self)\n", "label": 0}
{"function": "\n\ndef _get_streams(self):\n    match = _url_re.match(self.url)\n    channel_name = match.group('channel')\n    form = {\n        'cid': channel_name,\n        'watchTime': 0,\n        'firstConnect': 1,\n        'ip': 'NaN',\n    }\n    res = http.post(API_URL, data=form, headers=HEADERS)\n    params = parse_query(res.text, schema=_schema)\n    if (params['status'] <= 0):\n        return\n    if (params['block_type'] != 0):\n        if (params['block_type'] == BLOCK_TYPE_VIEWING_LIMIT):\n            msg = BLOCKED_MSG_FORMAT.format(params.get('block_time', 'UNKNOWN'), params.get('reconnect_time', 'UNKNOWN'))\n            raise PluginError(msg)\n        elif (params['block_type'] == BLOCK_TYPE_NO_SLOTS):\n            raise PluginError('No free slots available')\n        else:\n            raise PluginError('Blocked for unknown reasons')\n    if ('token' not in params):\n        raise PluginError('Server seems busy, retry again later')\n    streams = {\n        \n    }\n    stream_names = ['sd']\n    if params['multibitrate']:\n        stream_names += ['hd']\n    for stream_name in stream_names:\n        playpath = params['playpath']\n        if (stream_name == 'hd'):\n            playpath += 'HI'\n        stream = RTMPStream(self.session, {\n            'rtmp': '{0}/{1}'.format(params['rtmp'], playpath),\n            'pageUrl': self.url,\n            'swfVfy': SWF_URL,\n            'weeb': params['token'],\n            'live': True,\n        })\n        streams[stream_name] = stream\n    return streams\n", "label": 1}
{"function": "\n\ndef _is_number_match_OO(numobj1_in, numobj2_in):\n    'Takes two phone number objects and compares them for equality.'\n    numobj1 = PhoneNumber()\n    numobj1.merge_from(numobj1_in)\n    numobj2 = PhoneNumber()\n    numobj2.merge_from(numobj2_in)\n    numobj1.raw_input = None\n    numobj1.country_code_source = None\n    numobj1.preferred_domestic_carrier_code = None\n    numobj2.raw_input = None\n    numobj2.country_code_source = None\n    numobj2.preferred_domestic_carrier_code = None\n    if ((numobj1.extension is not None) and (len(numobj1.extension) == 0)):\n        numobj1.extension = None\n    if ((numobj2.extension is not None) and (len(numobj2.extension) == 0)):\n        numobj2.extension = None\n    if ((numobj1.extension is not None) and (numobj2.extension is not None) and (numobj1.extension != numobj2.extension)):\n        return MatchType.NO_MATCH\n    country_code1 = numobj1.country_code\n    country_code2 = numobj2.country_code\n    if ((country_code1 != 0) and (country_code2 != 0)):\n        if (numobj1 == numobj2):\n            return MatchType.EXACT_MATCH\n        elif ((country_code1 == country_code2) and _is_national_number_suffix_of_other(numobj1, numobj2)):\n            return MatchType.SHORT_NSN_MATCH\n        return MatchType.NO_MATCH\n    numobj1.country_code = country_code2\n    if (numobj1 == numobj2):\n        return MatchType.NSN_MATCH\n    if _is_national_number_suffix_of_other(numobj1, numobj2):\n        return MatchType.SHORT_NSN_MATCH\n    return MatchType.NO_MATCH\n", "label": 1}
{"function": "\n\ndef storage_cb(self, key, value, handle, connection):\n    self.send_response(handle, connection, (None if (value is None) else json.dumps(value)), status=(calvinresponse.NOT_FOUND if None else calvinresponse.OK))\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    s = ''\n    if (self.start is not None):\n        s = (s + str(self.start))\n    s = (s + ':')\n    if (self.stop is not None):\n        s = (s + str(self.stop))\n    if (self.step is not None):\n        s = ((s + ':') + str(self.step))\n    return s\n", "label": 0}
{"function": "\n\ndef falseColor(self, tile, color):\n    'Recolor a tile from 8bit to CMYRGB'\n    data32 = np.uint32(tile)\n    if ((color == 'C') or (color == 'cyan')):\n        fcdata = ((4278190080 + np.left_shift(data32, 8)) + np.left_shift(data32, 16))\n    elif ((color == 'Y') or (color == 'yellow')):\n        fcdata = ((4278190080 + np.left_shift(data32, 8)) + data32)\n    elif ((color == 'M') or (color == 'magenta')):\n        fcdata = ((4278190080 + np.left_shift(data32, 16)) + data32)\n    if ((color == 'R') or (color == 'red')):\n        fcdata = (4278190080 + data32)\n    elif ((color == 'G') or (color == 'green')):\n        fcdata = (4278190080 + np.left_shift(data32, 8))\n    elif ((color == 'B') or (color == 'blue')):\n        fcdata = (4278190080 + np.left_shift(data32, 16))\n    return fcdata\n", "label": 1}
{"function": "\n\ndef remove_tag(self, key, value=None):\n    \"\\n        Remove a tag from this object.  Removing a tag involves a round-trip\\n        to the EC2 service.\\n\\n        :type key: str\\n        :param key: The key or name of the tag being stored.\\n\\n        :type value: str\\n        :param value: An optional value that can be stored with the tag.\\n                      If a value is provided, it must match the value\\n                      currently stored in EC2.  If not, the tag will not\\n                      be removed.  If a value of None is provided, all\\n                      tags with the specified name will be deleted.\\n                      NOTE: There is an important distinction between\\n                      a value of '' and a value of None.\\n        \"\n    if value:\n        tags = {\n            key: value,\n        }\n    else:\n        tags = [key]\n    status = self.connection.delete_tags([self.id], tags)\n    if (key in self.tags):\n        del self.tags[key]\n", "label": 0}
{"function": "\n\ndef _process(self, overlay, key=None):\n    if (len(overlay) != 2):\n        raise Exception('Overlay must contain at least to items.')\n    [target, kernel] = (overlay.get(0), overlay.get(1))\n    if (len(target.vdims) != 1):\n        raise Exception('Convolution requires inputs with single value dimensions.')\n    xslice = slice(self.p.kernel_roi[0], self.p.kernel_roi[2])\n    yslice = slice(self.p.kernel_roi[1], self.p.kernel_roi[3])\n    k = (kernel.data if (self.p.kernel_roi == (0, 0, 0, 0)) else kernel[(xslice, yslice)].data)\n    fft1 = np.fft.fft2(target.data)\n    fft2 = np.fft.fft2(k, s=target.data.shape)\n    convolved_raw = np.fft.ifft2((fft1 * fft2)).real\n    (k_rows, k_cols) = k.shape\n    rolled = np.roll(np.roll(convolved_raw, (- (k_cols // 2)), axis=(- 1)), (- (k_rows // 2)), axis=(- 2))\n    convolved = (rolled / float(k.sum()))\n    return Image(convolved, bounds=target.bounds, group=self.p.group)\n", "label": 0}
{"function": "\n\ndef initialize(self, description):\n    super(AggregateQueryResultWrapper, self).initialize(description)\n    self.all_models = set()\n    for (key, _, _, _) in self.column_map:\n        self.all_models.add(key)\n    self.models_with_aggregate = set()\n    self.back_references = {\n        \n    }\n    self.source_to_dest = {\n        \n    }\n    self.dest_to_source = {\n        \n    }\n    for metadata in self.join_list:\n        if metadata.is_backref:\n            att_name = metadata.foreign_key.related_name\n        else:\n            att_name = metadata.attr\n        is_backref = (metadata.is_backref or metadata.is_self_join)\n        if is_backref:\n            self.models_with_aggregate.add(metadata.src)\n        else:\n            self.dest_to_source.setdefault(metadata.dest, set())\n            self.dest_to_source[metadata.dest].add(metadata.src)\n        self.source_to_dest.setdefault(metadata.src, {\n            \n        })\n        self.source_to_dest[metadata.src][metadata.dest] = JoinCache(metadata=metadata, attr=(metadata.alias or att_name))\n    self.columns_to_compare = {\n        \n    }\n    key_to_columns = {\n        \n    }\n    for (idx, (key, model_class, col_name, _)) in enumerate(self.column_map):\n        if (key in self.models_with_aggregate):\n            self.columns_to_compare.setdefault(key, [])\n            self.columns_to_compare[key].append((idx, col_name))\n        key_to_columns.setdefault(key, [])\n        key_to_columns[key].append((idx, col_name))\n    for model_or_alias in self.models_with_aggregate:\n        if (model_or_alias not in self.columns_to_compare):\n            continue\n        sources = self.dest_to_source.get(model_or_alias, ())\n        for joined_model in sources:\n            self.columns_to_compare[model_or_alias].extend(key_to_columns[joined_model])\n", "label": 1}
{"function": "\n\ndef _get_tr_css(row, tr_style):\n    if tr_style:\n        if isinstance(tr_style, string_types):\n            return tr_style\n        elif callable(tr_style):\n            return tr_style(row)\n        else:\n            raise ArgumentError(('expected string or callable, got %r' % tr_style))\n    return ''\n", "label": 0}
{"function": "\n\ndef _compare_diffs(self, diffs, compare_to):\n    diffs = [(cmd, (fk._get_colspec() if isinstance(fk, sa.ForeignKey) else ('tone_id_fk' if (fk is None) else fk)), tname, fk_info) for (cmd, fk, tname, fk_info) in diffs]\n    self.assertEqual(diffs, compare_to)\n", "label": 0}
{"function": "\n\n@cronjobs.register\ndef update_l10n_contributor_metrics(day=None):\n    'Update the number of active contributors for each locale/product.\\n\\n    An active contributor is defined as a user that created or reviewed a\\n    revision in the previous calendar month.\\n    '\n    if (day is None):\n        day = date.today()\n    first_of_month = date(day.year, day.month, 1)\n    if (day.month == 1):\n        previous_first_of_month = date((day.year - 1), 12, 1)\n    else:\n        previous_first_of_month = date(day.year, (day.month - 1), 1)\n    for locale in settings.SUMO_LANGUAGES:\n        for product in ([None] + list(Product.objects.filter(visible=True))):\n            num = num_active_contributors(from_date=previous_first_of_month, to_date=first_of_month, locale=locale, product=product)\n            WikiMetric.objects.create(code=L10N_ACTIVE_CONTRIBUTORS_CODE, locale=locale, product=product, date=previous_first_of_month, value=num)\n", "label": 0}
{"function": "\n\ndef expand_tag(self, str):\n    _str = str.group(0)\n    s = re.findall('([\\\\w\\\\-]+(?:=(?:\"[^\"]*\"|\\\\\\'[^\\\\\\']*\\\\\\'))?)', _str)\n    if (len(s) <= self.minimum_attribute_count):\n        return _str\n    tagEnd = re.search('/?>$', _str)\n    if (not (tagEnd == None)):\n        s += [tagEnd.group(0)]\n    tag = ('<' + s[0])\n    indent = (len(tag) + 1)\n    s = s[1:]\n    if self.first_attribute_on_new_line:\n        if self.indent_with_tabs:\n            indent = 0\n            extra_tabs = 1\n        else:\n            indent = self.indent_size\n            extra_tabs = 0\n    else:\n        if self.indent_with_tabs:\n            extra_tabs = int((indent / self.tab_size))\n            indent = (indent % self.tab_size)\n        else:\n            extra_tabs = 0\n        tag += (' ' + s[0])\n        s = s[1:]\n    for l in s:\n        tag += ((('\\n' + (((self.indent_level * self.indent_size) + extra_tabs) * self.indent_char)) + (indent * ' ')) + l)\n    return tag\n", "label": 0}
{"function": "\n\ndef _set_color(self):\n    control = self.control\n    if (not self._in_window):\n        color = control.GetParent().GetBackgroundColour()\n    elif self._down:\n        color = DownColor\n    else:\n        color = HoverColor\n    control.SetBackgroundColour(color)\n    control.Refresh()\n", "label": 0}
{"function": "\n\ndef rewrite_config_items(self, name, packages):\n    if (name == 'packages'):\n        packages += ['openssh-server', 'unzip', 'tar', 'sudo', 'openjdk-6-jre']\n        packages += ['openmpi-bin']\n    for package in packages:\n        env.logger.info(((('Selected: ' + name) + ' ') + package))\n    return packages\n", "label": 0}
{"function": "\n\ndef _isdst(dt):\n    'Check if date is in dst.\\n    '\n    if (type(dt) == datetime.date):\n        dt = datetime.datetime.combine(dt, datetime.datetime.min.time())\n    dtc = dt.replace(year=datetime.datetime.now().year)\n    if (time.localtime(dtc.timestamp()).tm_isdst == 1):\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n    'Instantiates a placeholder.\\n\\n    # Arguments\\n        shape: shape of the placeholder\\n            (integer tuple, may include None entries).\\n        ndim: number of axes of the tensor.\\n            At least one of {`shape`, `ndim`} must be specified.\\n            If both are specified, `shape` is used.\\n        dtype: placeholder type.\\n        name: optional name string for the placeholder.\\n\\n    # Returns\\n        Placeholder tensor instance.\\n    '\n    if (not shape):\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n    x = tf.placeholder(dtype, shape=shape, name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    return x\n", "label": 0}
{"function": "\n\n@permission_checker.require('add')\ndef add(request):\n    ImageModel = get_image_model()\n    ImageForm = get_image_form(ImageModel)\n    if (request.method == 'POST'):\n        image = ImageModel(uploaded_by_user=request.user)\n        form = ImageForm(request.POST, request.FILES, instance=image, user=request.user)\n        if form.is_valid():\n            image.file_size = image.file.size\n            form.save()\n            for backend in get_search_backends():\n                backend.add(image)\n            messages.success(request, _(\"Image '{0}' added.\").format(image.title), buttons=[messages.button(reverse('wagtailimages:edit', args=(image.id,)), _('Edit'))])\n            return redirect('wagtailimages:index')\n        else:\n            messages.error(request, _('The image could not be created due to errors.'))\n    else:\n        form = ImageForm(user=request.user)\n    return render(request, 'wagtailimages/images/add.html', {\n        'form': form,\n    })\n", "label": 0}
{"function": "\n\ndef plot_predict(self, start=None, end=None, exog=None, dynamic=False, alpha=0.05, plot_insample=True, ax=None):\n    from statsmodels.graphics.utils import _import_mpl, create_mpl_ax\n    _ = _import_mpl()\n    (fig, ax) = create_mpl_ax(ax)\n    forecast = self.predict(start, end, exog, 'levels', dynamic)\n    start = self.model._get_predict_start(start, dynamic=dynamic)\n    (end, out_of_sample) = self.model._get_predict_end(end, dynamic=dynamic)\n    if out_of_sample:\n        steps = out_of_sample\n        fc_error = self._forecast_error(steps)\n        conf_int = self._forecast_conf_int(forecast[(- steps):], fc_error, alpha)\n    if hasattr(self.data, 'predict_dates'):\n        from pandas import Series\n        forecast = Series(forecast, index=self.data.predict_dates)\n        ax = forecast.plot(ax=ax, label='forecast')\n    else:\n        ax.plot(forecast)\n    x = ax.get_lines()[(- 1)].get_xdata()\n    if out_of_sample:\n        label = '{0:.0%} confidence interval'.format((1 - alpha))\n        ax.fill_between(x[(- out_of_sample):], conf_int[:, 0], conf_int[:, 1], color='gray', alpha=0.5, label=label)\n    if plot_insample:\n        import re\n        k_diff = self.k_diff\n        label = re.sub('D\\\\d*\\\\.', '', self.model.endog_names)\n        levels = unintegrate(self.model.endog, self.model._first_unintegrate)\n        ax.plot(x[:((end + 1) - start)], levels[(start + k_diff):((end + k_diff) + 1)], label=label)\n    ax.legend(loc='best')\n    return fig\n", "label": 0}
{"function": "\n\ndef _get_battle_target(self):\n    if (self in self.battle.teamA):\n        other_team = self.battle.teamB\n    else:\n        other_team = self.battle.teamA\n    if self._battle_target:\n        if (not (self._battle_target in other_team)):\n            self._battle_target = other_team[0]\n        self.world.log.debug(('%s attack target: %s' % (self.fancy_name(), self._battle_target.fancy_name())))\n        return self._battle_target\n", "label": 0}
{"function": "\n\ndef _create_port(self, **kwargs):\n    body = {\n        'port': {\n            'binding:vnic_type': model.VNIC_TYPE_NORMAL,\n        },\n    }\n    fields = ['security_groups', 'device_id', 'network_id', 'port_security_enabled']\n    for field in fields:\n        if (field in kwargs):\n            body['port'][field] = kwargs[field]\n    neutron = get_client()\n    return neutron.create_port(body)\n", "label": 0}
{"function": "\n\ndef format_select_set(self):\n    context = self.context\n    formatted = []\n    for expr in self.select_set:\n        if isinstance(expr, ir.ValueExpr):\n            expr_str = self._translate(expr, named=True)\n        elif isinstance(expr, ir.TableExpr):\n            if context.need_aliases():\n                alias = context.get_ref(expr)\n                expr_str = ('{0}.*'.format(alias) if alias else '*')\n            else:\n                expr_str = '*'\n        formatted.append(expr_str)\n    buf = StringIO()\n    line_length = 0\n    max_length = 70\n    tokens = 0\n    for (i, val) in enumerate(formatted):\n        if val.count('\\n'):\n            if i:\n                buf.write(',')\n            buf.write('\\n')\n            indented = util.indent(val, self.indent)\n            buf.write(indented)\n            line_length = len(indented.split('\\n')[(- 1)])\n            tokens = 1\n        elif ((tokens > 0) and line_length and ((len(val) + line_length) > max_length)):\n            (buf.write(',\\n       ') if i else buf.write('\\n'))\n            buf.write(val)\n            line_length = (len(val) + 7)\n            tokens = 1\n        else:\n            if i:\n                buf.write(',')\n            buf.write(' ')\n            buf.write(val)\n            tokens += 1\n            line_length += (len(val) + 2)\n    if self.distinct:\n        select_key = 'SELECT DISTINCT'\n    else:\n        select_key = 'SELECT'\n    return '{0}{1}'.format(select_key, buf.getvalue())\n", "label": 1}
{"function": "\n\ndef RequiresConfiguration(self, svcRec):\n    return (svcRec.Authorization['Full'] and (('SyncRoot' not in svcRec.Config) or (not len(svcRec.Config['SyncRoot']))))\n", "label": 0}
{"function": "\n\ndef cleanup_choice(rule, lst):\n    if (len(lst) == 0):\n        return Sequence(rule, [])\n    if (len(lst) == 1):\n        return lst[0]\n    return parsetree.Choice(rule, *tuple(lst))\n", "label": 0}
{"function": "\n\n@flow.StateHandler(next_state='ProcessListDirectory')\ndef ListDeviceDirectories(self, responses):\n    if (not responses.success):\n        raise flow.FlowError('Unable to query Volume Shadow Copy information.')\n    for response in responses:\n        device_object = response.GetItem('DeviceObject', '')\n        global_root = '\\\\\\\\?\\\\GLOBALROOT\\\\Device'\n        if device_object.startswith(global_root):\n            device_object = ('\\\\\\\\.' + device_object[len(global_root):])\n            path_spec = rdf_paths.PathSpec(path=device_object, pathtype=rdf_paths.PathSpec.PathType.OS)\n            path_spec.Append(path='/', pathtype=rdf_paths.PathSpec.PathType.TSK)\n            self.Log('Listing Volume Shadow Copy device: %s.', device_object)\n            self.CallClient('ListDirectory', pathspec=path_spec, next_state='ProcessListDirectory')\n            self.state.raw_device = aff4.AFF4Object.VFSGRRClient.PathspecToURN(path_spec, self.client_id).Dirname()\n            self.state.shadows.append(aff4.AFF4Object.VFSGRRClient.PathspecToURN(path_spec, self.client_id))\n", "label": 0}
{"function": "\n\ndef import_set(dset, in_stream, headers=True, **kwargs):\n    'Returns dataset from CSV stream.'\n    dset.wipe()\n    kwargs.setdefault('delimiter', DEFAULT_DELIMITER)\n    if (not is_py3):\n        kwargs.setdefault('encoding', DEFAULT_ENCODING)\n    rows = csv.reader(StringIO(in_stream), **kwargs)\n    for (i, row) in enumerate(rows):\n        if ((i == 0) and headers):\n            dset.headers = row\n        else:\n            dset.append(row)\n", "label": 0}
{"function": "\n\ndef settings_get(name, default=None):\n    plugin_settings = sublime.load_settings('TypeScript.sublime-settings')\n    if (sublime.active_window() and sublime.active_window().active_view()):\n        project_settings = sublime.active_window().active_view().settings().get('TypeScript')\n    else:\n        project_settings = {\n            \n        }\n    if (project_settings is None):\n        project_settings = {\n            \n        }\n    setting = project_settings.get(name, plugin_settings.get(name, default))\n    return setting\n", "label": 0}
{"function": "\n\ndef getFrontier(na_data):\n    'Function gets a 100 sample point frontier for given returns'\n    (na_avgrets, na_std, b_error) = tsu.OptPort(na_data, None)\n    na_lower = np.zeros(na_data.shape[1])\n    na_upper = np.ones(na_data.shape[1])\n    (f_min, f_max) = tsu.getRetRange(na_data, na_lower, na_upper, na_avgrets, s_type='long')\n    f_step = ((f_max - f_min) / 100.0)\n    lf_returns = [(f_min + (x * f_step)) for x in range(101)]\n    lf_std = []\n    lna_portfolios = []\n    for f_target in lf_returns:\n        (na_weights, f_std, b_error) = tsu.OptPort(na_data, f_target, na_lower, na_upper, s_type='long')\n        lf_std.append(f_std)\n        lna_portfolios.append(na_weights)\n    return (lf_returns, lf_std, lna_portfolios, na_avgrets, na_std)\n", "label": 0}
{"function": "\n\ndef _kmap_eq(a, b):\n    if ((a is None) and (b is None)):\n        return True\n    if ((a is None) or (b is None)):\n        return False\n    return numpy.all((a == b))\n", "label": 0}
{"function": "\n\ndef __init__(self, master=None, **kw):\n    master = _setup_master(master)\n    global _TKTABLE_LOADED\n    if (not _TKTABLE_LOADED):\n        tktable_lib = os.environ.get('TKTABLE_LIBRARY')\n        if tktable_lib:\n            master.tk.eval(('global auto_path; lappend auto_path {%s}' % tktable_lib))\n        master.tk.call('package', 'require', 'Tktable')\n        _TKTABLE_LOADED = True\n    if (not ('padx' in kw)):\n        kw['padx'] = 1\n    if (not ('pady' in kw)):\n        kw['pady'] = 1\n    tkinter.Widget.__init__(self, master, 'table', kw)\n    self.contextMenuClick = ('<Button-2>' if (sys.platform == 'darwin') else '<Button-3>')\n", "label": 0}
{"function": "\n\ndef enter_OperationDefinition(self, node):\n    definition_type = None\n    if (node.operation == 'query'):\n        definition_type = self._schema.get_query_type()\n    elif (node.operation == 'mutation'):\n        definition_type = self._schema.get_mutation_type()\n    self._type_stack.append(definition_type)\n", "label": 0}
{"function": "\n\n@login_required\ndef users(request):\n    context = {\n        \n    }\n    counts = {\n        \n    }\n    for questions in Question.objects.all():\n        email = questions.added_by.email\n        if (email not in counts):\n            counts[email] = 0\n        counts[email] += 1\n    for user in User.objects.all():\n        if (user.email not in counts):\n            counts[user.email] = 0\n    context['counts'] = sorted(counts.items(), key=operator.itemgetter(0))\n    context['counts'] = sorted(context['counts'], key=operator.itemgetter(1), reverse=True)\n    return r2r(request, 'users', context)\n", "label": 0}
{"function": "\n\ndef f_alarm(burglary, earthquake, alarm):\n    table = dict()\n    table['ttt'] = 0.95\n    table['ttf'] = 0.05\n    table['tft'] = 0.94\n    table['tff'] = 0.06\n    table['ftt'] = 0.29\n    table['ftf'] = 0.71\n    table['fft'] = 0.001\n    table['fff'] = 0.999\n    key = ''\n    key = ((key + 't') if burglary else (key + 'f'))\n    key = ((key + 't') if earthquake else (key + 'f'))\n    key = ((key + 't') if alarm else (key + 'f'))\n    return table[key]\n", "label": 0}
{"function": "\n\ndef generate_client_login_request_body(email, password, service, source, account_type='HOSTED_OR_GOOGLE', captcha_token=None, captcha_response=None):\n    \"Creates the body of the autentication request\\n\\n  See http://code.google.com/apis/accounts/AuthForInstalledApps.html#Request\\n  for more details.\\n\\n  Args:\\n    email: str\\n    password: str\\n    service: str\\n    source: str\\n    account_type: str (optional) Defaul is 'HOSTED_OR_GOOGLE', other valid\\n        values are 'GOOGLE' and 'HOSTED'\\n    captcha_token: str (optional)\\n    captcha_response: str (optional)\\n\\n  Returns:\\n    The HTTP body to send in a request for a client login token.\\n  \"\n    request_fields = {\n        'Email': email,\n        'Passwd': password,\n        'accountType': account_type,\n        'service': service,\n        'source': source,\n    }\n    if (captcha_token and captcha_response):\n        request_fields['logintoken'] = captcha_token\n        request_fields['logincaptcha'] = captcha_response\n    return urllib.urlencode(request_fields)\n", "label": 0}
{"function": "\n\ndef ensure_timedelta(delta):\n    '\\n    Attempts to convert an object to a time delta.\\n\\n    @rtype\\n      `datetime.timedelta`\\n    '\n    if isinstance(delta, datetime.timedelta):\n        return delta\n    if isinstance(delta, str):\n        match = re.match('(\\\\d+) *(d|h|m|s)', delta)\n        if (match is not None):\n            (num, unit) = match.groups()\n            if (unit == 'd'):\n                return datetime.timedelta(int(num), 0)\n            else:\n                secs = (int(num) * {\n                    'h': 3600,\n                    'm': 60,\n                    's': 1,\n                }.get(unit))\n                return datetime.timedelta(0, secs)\n    raise TypeError('not a timedelta: {!r}'.format(delta))\n", "label": 0}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef spawn_processes(self):\n    'Spawn processes.\\n        '\n    if self.pending_socket_event:\n        self._status = 'stopped'\n        return\n    for i in self._found_wids:\n        self.spawn_process(i)\n        (yield tornado_sleep(0))\n    self._found_wids = {\n        \n    }\n    for i in range((self.numprocesses - len(self.processes))):\n        res = self.spawn_process()\n        if (res is False):\n            (yield self._stop())\n            break\n        delay = self.warmup_delay\n        if isinstance(res, float):\n            delay -= (time.time() - res)\n            if (delay < 0):\n                delay = 0\n        (yield tornado_sleep(delay))\n", "label": 0}
{"function": "\n\ndef _watch_expression(self, check_watch_view):\n    if (check_watch_view and (not has_debug_view(TITLE_WINDOW_WATCH))):\n        return\n    show_content(DATA_WATCH)\n", "label": 0}
{"function": "\n\ndef get_loggers(self):\n    return [l for l in self.__dict__.values() if isinstance(l, AlertLogger)]\n", "label": 0}
{"function": "\n\ndef create(self, req, body):\n    'Creates a new security group.'\n    context = req.environ['nova.context']\n    if (not body):\n        return exc.HTTPUnprocessableEntity()\n    security_group = body.get('security_group', None)\n    if (security_group is None):\n        return exc.HTTPUnprocessableEntity()\n    group_name = security_group.get('name', None)\n    group_description = security_group.get('description', None)\n    self._validate_security_group_property(group_name, 'name')\n    self._validate_security_group_property(group_description, 'description')\n    group_name = group_name.strip()\n    group_description = group_description.strip()\n    LOG.audit(_('Create Security Group %s'), group_name, context=context)\n    self.compute_api.ensure_default_security_group(context)\n    if db.security_group_exists(context, context.project_id, group_name):\n        msg = (_('Security group %s already exists') % group_name)\n        raise exc.HTTPBadRequest(explanation=msg)\n    group = {\n        'user_id': context.user_id,\n        'project_id': context.project_id,\n        'name': group_name,\n        'description': group_description,\n    }\n    group_ref = db.security_group_create(context, group)\n    return {\n        'security_group': self._format_security_group(context, group_ref),\n    }\n", "label": 0}
{"function": "\n\ndef test_disabled_hosts_are_skipped(self):\n    active_host_names = ['active1', 'active2']\n    disabled_host_names = ['disabled1', 'disabled2']\n    disabled_hosts = [self._host(name, enabled=False) for name in disabled_host_names]\n    active_hosts = [self._host(name) for name in active_host_names]\n    all_hosts = (active_hosts + disabled_hosts)\n    self.mock_client().services.list.return_value = all_hosts\n    self.mock_client().hosts.list.return_value = all_hosts\n    hosts = self.nova_client.get_compute_hosts()\n    self.assertIsNotNone(hosts)\n    self.assertTrue(isinstance(hosts, list))\n    for active in active_host_names:\n        self.assertIn(active, hosts)\n    for disabled in disabled_host_names:\n        self.assertNotIn(disabled, hosts)\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    keys = set()\n    cur = self\n    while cur:\n        keys.update(cur.table.keys())\n        cur = getattr(cur, 'parent', None)\n    keys = list(keys)\n    keys.sort()\n    return '\\n'.join([('%r %r' % (x, self.__getitem__(x))) for x in keys])\n", "label": 0}
{"function": "\n\ndef snapshot(self):\n    \"\\n        Return a nested dictionary snapshot of all metrics and their\\n        values at this time. Example:\\n        {\\n            'category': {\\n                'metric1_name': 42.0,\\n                'metric2_name': 'foo'\\n            }\\n        }\\n        \"\n    return dict(((category, dict(((name, metric.value()) for (name, metric) in list(metrics.items())))) for (category, metrics) in list(self._store.items())))\n", "label": 0}
{"function": "\n\ndef log_request(self, handler):\n    'Writes a completed HTTP request to the logs.\\n\\n        By default writes to the python root logger.  To change\\n        this behavior either subclass Application and override this method,\\n        or pass a function in the application settings dictionary as\\n        ``log_function``.\\n        '\n    if ('log_function' in self.settings):\n        self.settings['log_function'](handler)\n        return\n    if (handler.get_status() < 400):\n        log_method = access_log.info\n    elif (handler.get_status() < 500):\n        log_method = access_log.warning\n    else:\n        log_method = access_log.error\n    request_time = (1000.0 * handler.request.request_time())\n    log_method('%d %s %.2fms', handler.get_status(), handler._request_summary(), request_time)\n", "label": 0}
{"function": "\n\ndef test__map_reduce_with_object_id(self):\n    obj1 = ObjectId()\n    obj2 = ObjectId()\n    data = [{\n        'x': 1,\n        'tags': [obj1, obj2],\n    }, {\n        'x': 2,\n        'tags': [obj1],\n    }]\n    for item in data:\n        self.db.things_with_obj.insert(item)\n    expected_results = [{\n        '_id': obj1,\n        'value': 2,\n    }, {\n        '_id': obj2,\n        'value': 1,\n    }]\n    result = self.db.things_with_obj.map_reduce(self.map_func, self.reduce_func, 'myresults')\n    self.assertTrue(isinstance(result, mongomock.Collection))\n    self.assertEqual(result.name, 'myresults')\n    self.assertEqual(result.count(), 2)\n    for doc in result.find():\n        self.assertIn(doc, expected_results)\n", "label": 0}
{"function": "\n\ndef documentation(self, add_to=None, version=None, base_url='', url=''):\n    'Returns the documentation specific to an HTTP interface'\n    doc = (OrderedDict() if (add_to is None) else add_to)\n    usage = self.interface.spec.__doc__\n    if usage:\n        doc['usage'] = usage\n    for example in self.examples:\n        example_text = '{0}{1}{2}'.format(base_url, ('/v{0}'.format(version) if version else ''), url)\n        if isinstance(example, str):\n            example_text += '?{0}'.format(example)\n        doc_examples = doc.setdefault('examples', [])\n        if (not (example_text in doc_examples)):\n            doc_examples.append(example_text)\n    doc = super().documentation(doc)\n    if getattr(self, 'output_doc', ''):\n        doc['outputs']['type'] = self.output_doc\n    return doc\n", "label": 0}
{"function": "\n\n@cache_readonly\ndef fittedvalues(self):\n    model = self.model\n    endog = model.endog.copy()\n    k_ar = self.k_ar\n    exog = model.exog\n    if (exog is not None):\n        if ((model.method == 'css') and (k_ar > 0)):\n            exog = exog[k_ar:]\n    if ((model.method == 'css') and (k_ar > 0)):\n        endog = endog[k_ar:]\n    fv = (endog - self.resid)\n    return fv\n", "label": 0}
{"function": "\n\ndef handle_charref(self, char):\n    if (char[:1] == 'x'):\n        char = int(char[1:], 16)\n    else:\n        char = int(char)\n    if (0 <= char < 128):\n        self.__builder.data(chr(char))\n    else:\n        self.__builder.data(unichr(char))\n", "label": 0}
{"function": "\n\ndef get_package_freq(local_ctx_id, source, prelease):\n    linksets = ReleaseLinkSet.objects.filter(code_reference__local_object_id=local_ctx_id).filter(code_reference__source=source).filter(project_release=prelease).all()\n    packages = defaultdict(int)\n    for linkset in linksets:\n        if (linkset.links.count() == 1):\n            package_name = find_package(linkset.first_link.code_element)\n            if (package_name is not None):\n                packages[package_name] += 1\n    package_freq = [(k, packages[k]) for k in packages]\n    package_freq.sort(key=(lambda v: v[1]), reverse=True)\n    return package_freq\n", "label": 0}
{"function": "\n\n@property\ndef formfields(self):\n    formfields = OrderedDict()\n    for field in self.fields:\n        options = self.get_field_options(field)\n        if (field.field_type in self.FIELD_TYPES):\n            formfields[field.clean_name] = self.FIELD_TYPES[field.field_type](self, field, options)\n        else:\n            raise Exception(('Unrecognised field type: ' + field.field_type))\n    return formfields\n", "label": 0}
{"function": "\n\ndef _dup_rr_trivial_gcd(f, g, K):\n    'Handle trivial cases in GCD algorithm over a ring. '\n    if (not (f or g)):\n        return ([], [], [])\n    elif (not f):\n        if K.is_nonnegative(dup_LC(g, K)):\n            return (g, [], [K.one])\n        else:\n            return (dup_neg(g, K), [], [(- K.one)])\n    elif (not g):\n        if K.is_nonnegative(dup_LC(f, K)):\n            return (f, [K.one], [])\n        else:\n            return (dup_neg(f, K), [(- K.one)], [])\n    return None\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.stdout.write('sendalerts is now running')\n    ticks = 0\n    while True:\n        if self.handle_many():\n            ticks = 1\n        else:\n            ticks += 1\n        time.sleep(1)\n        if ((ticks % 60) == 0):\n            formatted = timezone.now().isoformat()\n            self.stdout.write(('-- MARK %s --' % formatted))\n", "label": 0}
{"function": "\n\ndef get(self, *args):\n    if (not self.api_key.is_admin()):\n        raise HTTPError(403)\n    if (self.get_argument('include_user', 'false').lower() == 'true'):\n        include_user = True\n    else:\n        include_user = False\n    self.write({\n        'users': self.auth_mgr.all_users(include_user=include_user),\n    })\n", "label": 0}
{"function": "\n\n@classmethod\ndef sqlall(cls):\n    queries = []\n    compiler = cls._meta.database.compiler()\n    pk = cls._meta.primary_key\n    if (cls._meta.database.sequences and pk.sequence):\n        queries.append(compiler.create_sequence(pk.sequence))\n    queries.append(compiler.create_table(cls))\n    for field in cls._fields_to_index():\n        queries.append(compiler.create_index(cls, [field], field.unique))\n    if cls._meta.indexes:\n        for (field_names, unique) in cls._meta.indexes:\n            fields = [cls._meta.fields[f] for f in field_names]\n            queries.append(compiler.create_index(cls, fields, unique))\n    return [sql for (sql, _) in queries]\n", "label": 0}
{"function": "\n\ndef wait_for_erlang_prompt(self):\n    prompted = False\n    buffer = ''\n    while (not prompted):\n        line = self._server.stdout.readline()\n        if (len(line) > 0):\n            buffer += line\n        if re.search(('\\\\(%s\\\\)\\\\d+>' % self.vm_args['-name']), buffer):\n            prompted = True\n        if re.search('\"Kernel pid terminated\".*\\\\n', buffer):\n            raise Exception('Riak test server failed to start.')\n", "label": 0}
{"function": "\n\ndef generate_atom(self, event, event_type, content, content_type, categories=None, title=None):\n    template = '<atom:entry xmlns:atom=\"http://www.w3.org/2005/Atom\"><atom:id>urn:uuid:%(message_id)s</atom:id>%(categories)s<atom:title type=\"text\">%(title)s</atom:title><atom:content type=\"%(content_type)s\">%(content)s</atom:content></atom:entry>'\n    if (title is None):\n        title = event_type\n    if (categories is None):\n        cats = []\n    else:\n        cats = categories[:]\n    cats.append(event_type)\n    original_message_id = event.get('original_message_id')\n    if (original_message_id is not None):\n        cats.append(('original_message_id:%s' % original_message_id))\n    cattags = ''.join((('<atom:category term=\"%s\" />' % cat) for cat in cats))\n    info = dict(message_id=event.get('message_id'), original_message_id=original_message_id, event=event, event_type=event_type, content=content, categories=cattags, title=title, content_type=content_type)\n    return (template % info)\n", "label": 0}
{"function": "\n\ndef items(self, section, noreplace=False):\n    items = StrictConfigParser.items(self, section)\n    if noreplace:\n        return items\n    return [(key, replace_gnu_args(value, env=self._env)) for (key, value) in items]\n", "label": 0}
{"function": "\n\ndef skip_reset(self):\n    outf = util.StringIO()\n    cm = pycurl.CurlMulti()\n    cm.setopt(pycurl.M_PIPELINING, 1)\n    eh = pycurl.Curl()\n    for x in range(1, 20):\n        eh.setopt(pycurl.WRITEFUNCTION, outf.write)\n        eh.setopt(pycurl.URL, 'http://localhost:8380/success')\n        cm.add_handle(eh)\n        while 1:\n            (ret, active_handles) = cm.perform()\n            if (ret != pycurl.E_CALL_MULTI_PERFORM):\n                break\n        while active_handles:\n            ret = cm.select(1.0)\n            if (ret == (- 1)):\n                continue\n            while 1:\n                (ret, active_handles) = cm.perform()\n                if (ret != pycurl.E_CALL_MULTI_PERFORM):\n                    break\n        (count, good, bad) = cm.info_read()\n        for (h, en, em) in bad:\n            print(('Transfer to %s failed with %d, %s\\n' % (h.getinfo(pycurl.EFFECTIVE_URL), en, em)))\n            raise RuntimeError\n        for h in good:\n            httpcode = h.getinfo(pycurl.RESPONSE_CODE)\n            if (httpcode != 200):\n                print(('Transfer to %s failed with code %d\\n' % (h.getinfo(pycurl.EFFECTIVE_URL), httpcode)))\n                raise RuntimeError\n            else:\n                print(('Recd %d bytes from %s' % (h.getinfo(pycurl.SIZE_DOWNLOAD), h.getinfo(pycurl.EFFECTIVE_URL))))\n        cm.remove_handle(eh)\n        eh.reset()\n    eh.close()\n    cm.close()\n    outf.close()\n", "label": 1}
{"function": "\n\ndef _ensure_index(index_like, copy=False):\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, 'name'):\n        return Index(index_like, name=index_like.name, copy=copy)\n    if isinstance(index_like, list):\n        if (type(index_like) != list):\n            index_like = list(index_like)\n        (converted, all_arrays) = lib.clean_index_list(index_like)\n        if ((len(converted) > 0) and all_arrays):\n            from .multi import MultiIndex\n            return MultiIndex.from_arrays(converted)\n        else:\n            index_like = converted\n    elif copy:\n        from copy import copy\n        index_like = copy(index_like)\n    return Index(index_like)\n", "label": 1}
{"function": "\n\ndef create(self, name, metadata=None, prefix=None, *args, **kwargs):\n    '\\n        Creates a new container, and returns a Container object that represents\\n        that contianer. If a container by the same name already exists, no\\n        exception is raised; instead, a reference to that existing container is\\n        returned.\\n        '\n    uri = ('/%s' % name)\n    headers = {\n        \n    }\n    if (prefix is None):\n        prefix = CONTAINER_META_PREFIX\n    if metadata:\n        metadata = _massage_metakeys(metadata, prefix)\n        headers = metadata\n    (resp, resp_body) = self.api.method_put(uri, headers=headers)\n    if (resp.status_code in (201, 202)):\n        (hresp, hresp_body) = self.api.method_head(uri)\n        num_obj = int(hresp.headers.get('x-container-object-count', '0'))\n        num_bytes = int(hresp.headers.get('x-container-bytes-used', '0'))\n        cont_info = {\n            'name': name,\n            'object_count': num_obj,\n            'total_bytes': num_bytes,\n        }\n        return Container(self, cont_info)\n    elif (resp.status_code == 400):\n        raise exc.ClientException(('Container creation failed: %s' % resp_body))\n", "label": 0}
{"function": "\n\ndef deletetag(url, tag):\n    'Takes html and a tag and returns html with tags removed, but in-between html still there.'\n    html = url\n    if checkurl(url):\n        html = gethtml(url)\n        if (not html):\n            return None\n    pattern = ('(<\\\\s*%s\\\\s*.*?>)|(<\\\\s*/%s\\\\s*>)' % (tag, tag))\n    pat = re.compile(pattern, (re.IGNORECASE | re.DOTALL))\n    tagless = pat.sub('', html)\n    return tagless\n", "label": 0}
{"function": "\n\n@property\ndef addresses(self):\n    if (self._addresses is None):\n        self._addresses = tuple([address for group in self._groups for address in group.addresses])\n    return self._addresses\n", "label": 0}
{"function": "\n\ndef normalize(self, obj):\n    '\\n        Runs an object through the normalizer mechanism, with the goal of producing a value consisting only of \"native types\" (:obj:`unicode`, :obj:`int`, :obj:`long`, :obj:`float`, :obj:`dict`, :obj:`list`, etc).\\n\\n        The resolution order looks like this:\\n\\n        - Loop through :attr:`self.normalizer_overrides[type(obj)] <normalizer_overrides>` (taking parent classes into account), should be a callable taking (obj, pushrod), falls through on :obj:`NotImplemented`\\n        - :attr:`self.normalizers[type(obj)] <normalizers>` (taking parent classes into account), should be a callable taking (obj, pushrod), falls through on :obj:`NotImplemented`\\n\\n        See :ref:`bundled-normalizers` for all default normalizers.\\n\\n        :param obj: The object to normalize.\\n        '\n    for cls in type(obj).__mro__:\n        for override in self.normalizer_overrides[cls]:\n            attempt = override(obj, self)\n            if (attempt is not NotImplemented):\n                return attempt\n    attempt = normalizers.normalize_object(obj, self)\n    if (attempt is not NotImplemented):\n        return attempt\n    for cls in type(obj).__mro__:\n        if (cls in self.normalizers):\n            attempt = self.normalizers[cls](obj, self)\n            if (attempt is not NotImplemented):\n                return attempt\n    return NotImplemented\n", "label": 0}
{"function": "\n\ndef width(self):\n    'Return the total width of the operation.'\n    if (not self._tasks):\n        return 0\n    task = self._tasks[(- 1)]\n    if (len(self._tasks) > 1):\n        return (task[3] * (task[1] or 1))\n    else:\n        return task[1]\n", "label": 0}
{"function": "\n\ndef start_digging(self, pos, face=constants.FACE_TOP):\n    pos = Vector3(pos)\n    if self.auto_look:\n        self.look_at((pos.floor() + Vector3(0.5, 0.5, 0.5)))\n    self._send_dig_block(status=constants.DIG_START, pos=pos, face=face)\n    if self.auto_swing:\n        self.swing_arm()\n", "label": 0}
{"function": "\n\ndef GetOSXMajorVersion(os):\n    if os:\n        version = distutils.version.LooseVersion(os).version\n        if (len(version) < 2):\n            return None\n        return str(version[1])\n    return None\n", "label": 0}
{"function": "\n\ndef setup(self):\n    self.ER_G = zn.Graph()\n    for i in range(self.NUM_NODES):\n        self.ER_G.add_node(i)\n    for i in range(self.NUM_NODES):\n        for j in range((i + 1), self.NUM_NODES):\n            r = random.random()\n            w = random.random()\n            if (r < self.P):\n                w = math.ceil((w * 10.0))\n                self.ER_G.add_edge_(i, j, weight=w)\n                self.weights.append(w)\n", "label": 0}
{"function": "\n\ndef changed_files(self):\n    'Determines the files changed according to SCM/workspace and options.'\n    if self._diffspec:\n        return self._workspace.changes_in(self._diffspec)\n    else:\n        since = (self._changes_since or self._scm.current_rev_identifier())\n        return self._workspace.touched_files(since)\n", "label": 0}
{"function": "\n\ndef start_unite(state):\n    (state, old_state) = state_logic(state)\n    if (not state.buff):\n        state.buff = make_pyunite_buffer(state)\n    if (state.options['close_on_empty'] and (not len(state.source.candidates))):\n        return\n    saved = vim.current.window\n    window_logic(state, old_state)\n    set_buffer_syntax(state)\n    if (not state.options['focus_on_open']):\n        ui.change_window(saved, autocmd=True)\n    states.add(state)\n", "label": 0}
{"function": "\n\ndef _handle_failure(self, on_failure, error):\n    'Handles the situation where a resource failed to succeed.\\n\\n        :param callable on_failure: What should be done when the resource\\n                                    failed?\\n        :param str error: Error message.\\n\\n        If `on_failure` is ``None``, nothing is done when the resource failed.\\n        Otherwise, it is called with `error`. If the returned value is an\\n        exception, it is raised.\\n        '\n    if (on_failure is not None):\n        obj = on_failure(error)\n        if isinstance(obj, Exception):\n            raise obj\n", "label": 0}
{"function": "\n\ndef getSubtag(self, name=None):\n    'determine whether current tag contains other tags, and returns\\n        the tag with a matching name (if name is given) or None (if not)'\n    if ('Icontain' in self.tag):\n        for subtag in self.tag['Icontain']:\n            if (subtag.name == name):\n                return subtag\n    return None\n", "label": 0}
{"function": "\n\ndef gplvm_oil_100(optimize=True, verbose=1, plot=True):\n    import GPy\n    import pods\n    data = pods.datasets.oil_100()\n    Y = data['X']\n    kernel = (GPy.kern.RBF(6, ARD=True) + GPy.kern.Bias(6))\n    m = GPy.models.GPLVM(Y, 6, kernel=kernel)\n    m.data_labels = data['Y'].argmax(axis=1)\n    if optimize:\n        m.optimize('scg', messages=verbose)\n    if plot:\n        m.plot_latent(labels=m.data_labels)\n    return m\n", "label": 0}
{"function": "\n\ndef _base_notification(self, exists):\n    basen = exists.copy()\n    if ('bandwidth_in' not in basen):\n        basen['bandwidth_in'] = 0\n    if ('bandwidth_out' not in basen):\n        basen['bandwidth_out'] = 0\n    if ('rax_options' not in basen):\n        basen['rax_options'] = '0'\n    basen['original_message_id'] = exists.get('message_id', '')\n    return basen\n", "label": 0}
{"function": "\n\ndef _lookup_vhd_path(self, instance_name, vhd_path_func):\n    vhd_path = None\n    for format_ext in ['vhd', 'vhdx']:\n        test_path = vhd_path_func(instance_name, format_ext)\n        if self.exists(test_path):\n            vhd_path = test_path\n            break\n    return vhd_path\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    if isinstance(key, slice):\n        return self._lines[key.start:key.stop:key.step]\n    if (not isinstance(key, int)):\n        raise TypeError(('Index should be integer, not %s' % classname(key)))\n    return self._lines[key]\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    name = ('unknown, ' if (self.subject is None) else (self.subject + ', '))\n    name += (repr(self.name) if (self.name is not None) else 'unnamed')\n    n_vert = len(self)\n    return ('<Label  |  %s, %s : %i vertices>' % (name, self.hemi, n_vert))\n", "label": 0}
{"function": "\n\ndef get_output_fmt(output_param):\n    output_class = output_param['class']\n    output_fmt = ''\n    if output_class.startswith('array'):\n        item_class = output_class[6:]\n        if (item_class in class_to_dxclass):\n            output_fmt = (('[dxpy.dxlink(item) for item in ' + output_param['name']) + ']')\n        else:\n            output_fmt = output_param['name']\n    elif (output_class in class_to_dxclass):\n        output_fmt = (('dxpy.dxlink(' + output_param['name']) + ')')\n    else:\n        output_fmt = output_param['name']\n    return output_fmt\n", "label": 0}
{"function": "\n\ndef set_status(self, view):\n    kind = get_setting('git_status_bar', 'fancy')\n    if (kind not in ('fancy', 'simple')):\n        return\n    repo = self.get_repo_from_view(view)\n    if (not repo):\n        return\n    bin = get_executable('git', self.bin)\n    encoding = get_setting('encoding', 'utf-8')\n    fallback = get_setting('fallback_encodings', [])\n    updater = GitStatusBarUpdater(bin, encoding, fallback, repo, kind, view)\n    updater.start()\n", "label": 0}
{"function": "\n\ndef get_match_children(self):\n    if (self.match_children is None):\n        self.match_children = []\n        for route in self.get_children():\n            for r in route.get_match_routes():\n                self.match_children.append(r)\n    for rv in self.match_children:\n        (yield rv)\n", "label": 0}
{"function": "\n\ndef product(parameter):\n    keys = sorted(parameter)\n    values = [parameter[key] for key in keys]\n    values_product = itertools.product(*values)\n    return [dict(zip(keys, vals)) for vals in values_product]\n", "label": 0}
{"function": "\n\n@defun_wrapped\ndef legendre(ctx, n, x, **kwargs):\n    if ctx.isint(n):\n        n = int(n)\n        if ((n + (n < 0)) & 1):\n            if (not x):\n                return x\n            mag = ctx.mag(x)\n            if (mag < (((- 2) * ctx.prec) - 10)):\n                return x\n            if (mag < (- 5)):\n                ctx.prec += (- mag)\n    return ctx.hyp2f1((- n), (n + 1), 1, ((1 - x) / 2), **kwargs)\n", "label": 0}
{"function": "\n\ndef _replacement(self, node):\n    if (isinstance(node.left, ast.Constant) and (node.left.value == False)):\n        return node.right\n    if (isinstance(node.right, ast.Constant) and (node.right.value == False)):\n        return node.left\n    return None\n", "label": 0}
{"function": "\n\ndef use(self, player, game):\n    super().use(player, game)\n    if ((self.target.health <= player.effective_spell_damage(2)) and (isinstance(self.target, Minion) and (not self.target.divine_shield))):\n        self.target.damage(player.effective_spell_damage(2), self)\n        demons = CollectionSource([IsType(MINION_TYPE.DEMON)])\n        demons.get_card(player, player, self).summon(player, game, len(player.minions))\n    else:\n        self.target.damage(player.effective_spell_damage(2), self)\n", "label": 0}
{"function": "\n\ndef __init__(self, text='Enter object label', parent=None, listItem=None):\n    super(LabelDialog, self).__init__(parent)\n    self.edit = QLineEdit()\n    self.edit.setText(text)\n    self.edit.setValidator(labelValidator())\n    self.edit.editingFinished.connect(self.postProcess)\n    layout = QVBoxLayout()\n    layout.addWidget(self.edit)\n    self.buttonBox = bb = BB((BB.Ok | BB.Cancel), Qt.Horizontal, self)\n    bb.button(BB.Ok).setIcon(newIcon('done'))\n    bb.button(BB.Cancel).setIcon(newIcon('undo'))\n    bb.accepted.connect(self.validate)\n    bb.rejected.connect(self.reject)\n    layout.addWidget(bb)\n    if ((listItem is not None) and (len(listItem) > 0)):\n        self.listWidget = QListWidget(self)\n        for item in listItem:\n            self.listWidget.addItem(item)\n        self.listWidget.itemDoubleClicked.connect(self.listItemClick)\n        layout.addWidget(self.listWidget)\n    self.setLayout(layout)\n", "label": 0}
{"function": "\n\ndef getEdgeTop(self, pp, symbol, y, onY2):\n    if onY2:\n        yMin = pp.getY2Max()\n        yMax = pp.getY2Min()\n    else:\n        yMin = pp.getYMax()\n        yMax = pp.getYMin()\n    yMid = symbol.getBaseline()\n    if (Double.NaN == yMid):\n        yMid = ((yMin + yMax) / 2.0)\n    yMinPx = pp.yToPixel(yMin, onY2)\n    yMaxPx = pp.yToPixel(yMax, onY2)\n    yMidPx = pp.yToPixel(yMid, onY2)\n    yPx = pp.yToPixel(y, onY2)\n    prevYPx = Double.NaN\n    nextYPx = Double.NaN\n    height = symbol.getHeight(pp, onY2)\n    symHeight = self.getAdjustedHeight(height, yPx, prevYPx, nextYPx, yMinPx, yMaxPx, yMidPx)\n    if (Double.NaN == symHeight):\n        return Double.NaN\n    yTop = self.getUpperLeftY(height, yPx, prevYPx, nextYPx, yMinPx, yMaxPx, yMidPx, pp.getYMousePlotArea())\n    if (Double.NaN == yTop):\n        return Double.NaN\n    result = yTop\n    return result\n", "label": 0}
{"function": "\n\ndef testReadArray(self):\n    stream = StringIO('[0,1,2,3,4,[0,1,2,3,4,[0,1,2,3,4]],[0,1,2,3,4]]')\n    reader = pulljson.JSONPullParser(stream)\n    arr = reader.readArray()\n    self.assertEqual(len(arr), 7)\n    for i in range(0, 5):\n        self.assertEqual(arr[i], i)\n    for i in range(0, 5):\n        self.assertEqual(arr[5][i], i)\n    for i in range(0, 5):\n        self.assertEqual(arr[5][5][i], i)\n    for i in range(0, 5):\n        self.assertEqual(arr[6][i], i)\n", "label": 0}
{"function": "\n\ndef childConnectionLost(self, fd, reason):\n    if self.disconnected:\n        return\n    if (reason.value.__class__ == error.ConnectionDone):\n        if (fd == 'read'):\n            self._readConnectionLost(reason)\n        else:\n            self._writeConnectionLost(reason)\n    else:\n        self.connectionLost(reason)\n", "label": 0}
{"function": "\n\ndef score(self, X, Y):\n    'Compute score as 1 - loss over whole data set.\\n\\n        Returns the average accuracy (in terms of model.loss)\\n        over X and Y.\\n\\n        Parameters\\n        ----------\\n        X : iterable\\n            Evaluation data.\\n\\n        Y : iterable\\n            True labels.\\n\\n        Returns\\n        -------\\n        score : float\\n            Average of 1 - loss over training examples.\\n        '\n    if hasattr(self.model, 'batch_loss'):\n        losses = self.model.batch_loss(Y, self.model.batch_inference(X, self.w))\n    else:\n        losses = [self.model.loss(y, self.model.inference(y, self.w)) for (y, y_pred) in zip(Y, self.predict(X))]\n    max_losses = [self.model.max_loss(y) for y in Y]\n    return (1.0 - (np.sum(losses) / float(np.sum(max_losses))))\n", "label": 0}
{"function": "\n\ndef _flush_notifications(self):\n    'Flush notifications of engine registrations waiting\\n        in ZMQ queue.'\n    (idents, msg) = self.session.recv(self._notification_socket, mode=zmq.NOBLOCK)\n    while (msg is not None):\n        if self.debug:\n            pprint(msg)\n        msg_type = msg['msg_type']\n        handler = self._notification_handlers.get(msg_type, None)\n        if (handler is None):\n            raise Exception(('Unhandled message type: %s' % msg.msg_type))\n        else:\n            handler(msg)\n        (idents, msg) = self.session.recv(self._notification_socket, mode=zmq.NOBLOCK)\n", "label": 0}
{"function": "\n\ndef delete_instance(self, recursive=False, delete_nullable=False):\n    if recursive:\n        dependencies = self.dependencies(delete_nullable)\n        for (query, fk) in reversed(list(dependencies)):\n            model = fk.model_class\n            if (fk.null and (not delete_nullable)):\n                model.update(**{\n                    fk.name: None,\n                }).where(query).execute()\n            else:\n                model.delete().where(query).execute()\n    return self.delete().where(self._pk_expr()).execute()\n", "label": 0}
{"function": "\n\ndef _increment_sparse_diagonal_precision(X, mean_vector, covariances, n, graph, n_features, n_features_per_vertex, dtype=np.float32, n_components=None, bias=0, verbose=False):\n    all_blocks = np.zeros((graph.n_vertices, n_features_per_vertex, n_features_per_vertex), dtype=dtype)\n    columns = np.zeros(graph.n_vertices)\n    rows = np.zeros(graph.n_vertices)\n    if verbose:\n        vertices = print_progress(range(graph.n_vertices), n_items=graph.n_vertices, prefix='Precision per vertex', end_with_newline=False)\n    else:\n        vertices = range(graph.n_vertices)\n    for v in vertices:\n        i_from = (v * n_features_per_vertex)\n        i_to = ((v + 1) * n_features_per_vertex)\n        edge_data = X[:, i_from:i_to]\n        m = mean_vector[i_from:i_to]\n        (_, covariances[v]) = _increment_multivariate_gaussian_cov(edge_data, m, covariances[v], n, bias=bias)\n        all_blocks[v] = _covariance_matrix_inverse(covariances[v], n_components)\n        rows[v] = v\n        columns[v] = v\n    rows_arg_sort = rows.argsort()\n    columns = columns[rows_arg_sort]\n    all_blocks = all_blocks[rows_arg_sort]\n    rows = rows[rows_arg_sort]\n    n_rows = graph.n_vertices\n    indptr = np.zeros((n_rows + 1))\n    for i in range(n_rows):\n        (inds,) = np.where((rows == i))\n        if (inds.size == 0):\n            indptr[(i + 1)] = indptr[i]\n        else:\n            indptr[i] = inds[0]\n            indptr[(i + 1)] = (inds[(- 1)] + 1)\n    return (bsr_matrix((all_blocks, columns, indptr), shape=(n_features, n_features), dtype=dtype), covariances)\n", "label": 0}
{"function": "\n\ndef clean_history(self, current_project_only):\n    if current_project_only:\n        self.__clean_history(self.get_current_project_key())\n    else:\n        orphan_list = []\n        open_projects = [self.get_project_key(window) for window in sublime.windows()]\n        for project_key in self.history:\n            if (is_ST2 or (project_key == 'global') or os.path.exists(project_key) or (project_key in open_projects)):\n                self.__clean_history(project_key)\n            else:\n                orphan_list.append(project_key)\n        for project_key in orphan_list:\n            self.debug(('Removing orphaned project \"%s\" from the history' % project_key))\n            del self.history[project_key]\n    self.__save_history()\n", "label": 1}
{"function": "\n\ndef update(self, data):\n    'Set the value of user fields. The field types will be the same.\\n\\n        data ... dict, with field name as key, field value as value\\n\\n        Returns None\\n\\n        '\n    self.loaddoc()\n    all_fields = self.document.getElementsByType(UserFieldDecl)\n    for f in all_fields:\n        field_name = f.getAttribute('name')\n        if (field_name in data):\n            value_type = f.getAttribute('valuetype')\n            value = data.get(field_name)\n            if (value_type == 'string'):\n                f.setAttribute('stringvalue', value)\n            else:\n                f.setAttribute('value', value)\n    self.savedoc()\n", "label": 0}
{"function": "\n\ndef clean_choices(self):\n    choices = self.cleaned_data['choices']\n    if (not self.poll.is_multiple_choice):\n        return choices\n    if (len(choices) > self.poll.choice_max):\n        raise forms.ValidationError((_('Too many selected choices. Limit is %s') % self.poll.choice_max))\n    if (len(choices) < self.poll.choice_min):\n        raise forms.ValidationError((_('Too few selected choices. Minimum is %s') % self.poll.choice_min))\n    return choices\n", "label": 0}
{"function": "\n\ndef __init__(self, layers, name=None, alphas=None):\n    super(Tree, self).__init__(name=name)\n    self.layers = []\n    for l in layers:\n        if isinstance(l, Sequential):\n            self.layers.append(l)\n        elif isinstance(l, list):\n            self.layers.append(Sequential(l))\n        elif isinstance(l, Layer):\n            self.layers.append(Sequential([l]))\n        else:\n            ValueError('Incompatible element for Tree container')\n    self.alphas = ([1.0 for _ in self.layers] if (alphas is None) else alphas)\n    self.betas = []\n    next_root = None\n    for l in reversed(self.layers):\n        root = l.layers[0]\n        beta = (1.0 if ((root is next_root) or (type(root) is not BranchNode)) else 0.0)\n        next_root = root\n        self.betas.append(beta)\n    self.betas.reverse()\n", "label": 1}
{"function": "\n\ndef __set__(self, instance, value):\n    if isinstance(value, self.rel_model):\n        instance._data[self.att_name] = getattr(value, self.field.to_field.name)\n        instance._obj_cache[self.att_name] = value\n    else:\n        orig_value = instance._data.get(self.att_name)\n        instance._data[self.att_name] = value\n        if ((orig_value != value) and (self.att_name in instance._obj_cache)):\n            del instance._obj_cache[self.att_name]\n    instance._dirty.add(self.att_name)\n", "label": 0}
{"function": "\n\ndef type_cmp(a, b):\n    'Python 2 style comparison based on type'\n    (ta, tb) = (type(a).__name__, type(b).__name__)\n    if (ta == 'str'):\n        ta = 'unicode'\n    if (tb == 'str'):\n        tb = 'unicode'\n    if (ta > tb):\n        return 1\n    elif (ta < tb):\n        return (- 1)\n    else:\n        return 0\n", "label": 0}
{"function": "\n\ndef test_allQueueSendIntentsAreSentOnEnoughPendingCallbackCompletions(self):\n    numExtras = len(self.extraTransmitIds)\n    self.resetCounters()\n    self.test_sendIntentToTransportUpToLimitAndThenQueueInternally()\n    self.assertTrue((MAX_PENDING_TRANSMITS > numExtras))\n    for _extras in range(numExtras):\n        self.testTrans.forTestingCompleteAPendingIntent(SendStatus.Sent)\n    self.assertEqual(self.successCBcalls, numExtras)\n    self.assertEqual((MAX_PENDING_TRANSMITS + numExtras), len(self.testTrans.intents))\n    self.assertEqual(numExtras, len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n", "label": 0}
{"function": "\n\ndef __get__(self, obj, type=None):\n    '\\n        This accessor retrieves the geometry, initializing it using the geometry\\n        class specified during initialization and the HEXEWKB value of the field.  \\n        Currently, only GEOS or OGR geometries are supported.\\n        '\n    geom_value = obj.__dict__[self._field.attname]\n    if isinstance(geom_value, self._klass):\n        geom = geom_value\n    elif ((geom_value is None) or (geom_value == '')):\n        geom = None\n    else:\n        geom = self._klass(geom_value)\n        setattr(obj, self._field.attname, geom)\n    return geom\n", "label": 0}
{"function": "\n\ndef get_shards(self):\n    'Returns a list of ShardMeta objects sorted by shard ID'\n    shard_config = self.partition_config['shards']\n    host_map = self.partition_config.get('host_map', {\n        \n    })\n    db_shards = []\n    for (db, shard_range) in shard_config.items():\n        db_shards.extend([DbShard(shard_num, db) for shard_num in range(shard_range[0], (shard_range[1] + 1))])\n    db_shards = sorted(db_shards, key=(lambda shard: shard.shard_id))\n    return [shard.to_shard_meta(host_map) for shard in db_shards]\n", "label": 0}
{"function": "\n\ndef iter_dicts(self):\n    self._set_headers()\n    for row in self:\n        result = {\n            \n        }\n        for (key, value) in zip(self._headers, row):\n            if value:\n                result[key] = value\n        if result:\n            (yield result)\n    self.close()\n", "label": 0}
{"function": "\n\ndef detectFileType(inFile):\n    firstLine = inFile.readline()\n    secondLine = inFile.readline()\n    thirdLine = inFile.readline()\n    inFile.seek(0)\n    if ((firstLine.find('nmap') != (- 1)) and (thirdLine.find('Host:') != (- 1))):\n        if ((firstLine.find('-sV') != (- 1)) or (firstLine.find('-A') != (- 1)) or (firstLine.find('-sSV') != (- 1))):\n            return 'gnmap'\n        else:\n            utility.Msg('Nmap version detection not used! Discovery module may miss some hosts!', LOG.INFO)\n            return 'gnmap'\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef get_template_vars(self, ignorevars=[], sort=True, maxnestlevels=100):\n    'Return a list of all variables found in the template\\n\\n\\n        Arguments:\\n\\n            ignorevars  -- a list of variables that are removed from the output\\n            sort        -- True (default) or False if returned list should be sorted\\n            maxnestlevels -- a positve integer which defines how deep you can nest templates with includes\\n        '\n    tplvars = []\n    templates = []\n    templatesseen = []\n    nestlevels = 0\n    env = Environment(loader=FileSystemLoader(self.templatepath), undefined=StrictUndefined)\n    templates.append(self.templatename)\n    templatesseen.append(self.templatename)\n    while (len(templates) > 0):\n        tpl = templates.pop()\n        nested = False\n        tplsrc = env.loader.get_source(env, tpl)[0]\n        ast = env.parse(tplsrc)\n        for template in meta.find_referenced_templates(ast):\n            if (template in templatesseen):\n                raise Exception('Template loop detected: \"{}\" references \"{}\" which was seen earlier'.format(tpl, template))\n            else:\n                templates.append(template)\n                templatesseen.append(template)\n                nested = True\n        for e in meta.find_undeclared_variables(ast):\n            if (not (e in ignorevars)):\n                tplvars.append(e)\n        if (nested and (nestlevels >= maxnestlevels)):\n            raise Exception('Maximum template nesting depth of {} reached in template {}'.format(maxnestlevels, template))\n        else:\n            nestlevels += 1\n    if sort:\n        return sorted(tplvars)\n    else:\n        return tplvars\n", "label": 1}
{"function": "\n\ndef _literal(translator, expr):\n    if isinstance(expr, ir.BooleanValue):\n        typeclass = 'boolean'\n    elif isinstance(expr, ir.StringValue):\n        typeclass = 'string'\n    elif isinstance(expr, ir.NumericValue):\n        typeclass = 'number'\n    elif isinstance(expr, ir.TimestampValue):\n        typeclass = 'timestamp'\n    else:\n        raise NotImplementedError\n    return _literal_formatters[typeclass](expr)\n", "label": 0}
{"function": "\n\n@login_required\ndef upload(request):\n    '\\n    Upload images for embedding in articles.\\n    '\n    js = ('js' in request.GET)\n    if (request.method == 'POST'):\n        form = ImageUploadForm(data=request.POST, files=request.FILES)\n        if form.is_valid():\n            image = form.save()\n            if js:\n                return HttpResponse(('<script>window.opener.editor.insertImage(\"[image:%d]\");window.close();</script>' % image.id))\n            else:\n                return redirect(image)\n    else:\n        form = ImageUploadForm()\n    return render(request, 'images/upload.html', {\n        'title': 'Upload',\n        'form': form,\n        'description': 'Use this form to upload an image.',\n        'action': 'Upload',\n        'js': js,\n    })\n", "label": 0}
{"function": "\n\ndef get_virtualenv_name(options):\n    if options.path:\n        return os.path.dirname(options.path)\n    else:\n        ve_name = (options.rest.pop(0) if options.rest else '')\n    if (not ve_name):\n        raise exceptions.NoVirtualenvName('could not find a virtualenv name in the command line.')\n    return ve_name\n", "label": 0}
{"function": "\n\ndef describe_new_commits(self, max_commits=100, max_size=16000):\n    'Return a string description of the new commits on the branch.'\n    hashes = None\n    previous = None\n    latest = self._review_branch.remote_branch\n    if self.is_new():\n        previous = self._review_branch.remote_base\n    else:\n        previous = self._tracking_branch.remote_branch\n    hashes = self._repo.get_range_hashes(previous, latest)\n    hashes.reverse()\n    revisions = self._repo.make_revisions_from_hashes(hashes)\n    message = ''\n    count = 0\n    message_size = 0\n    for r in revisions:\n        new_message = (((r.abbrev_hash + ' ') + r.subject) + '\\n')\n        count += 1\n        message_size += len(new_message)\n        if ((count > max_commits) or (message_size > max_size)):\n            message += '...{num_commits} commits not shown.\\n'.format(num_commits=((len(revisions) - count) + 1))\n            break\n        else:\n            message += new_message\n    return phlsys_textconvert.ensure_ascii(message)\n", "label": 0}
{"function": "\n\ndef allocate(self, shared_outputs=None):\n    alloc_layers = [l for l in self.layers if l.owns_output]\n    alloc_layers[(- 1)].allocate(shared_outputs)\n    for l in self.layers:\n        l.allocate()\n", "label": 0}
{"function": "\n\n@classmethod\ndef from_string(cls, s):\n    \"Parse the string given by 's' into a structure object.\"\n    if ((len(s) < 1) or (s[0] != cls.SIGIL)):\n        raise SynapseError(400, (\"Expected %s string to start with '%s'\" % (cls.__name__, cls.SIGIL)))\n    parts = s[1:].split(':', 1)\n    if (len(parts) != 2):\n        raise SynapseError(400, (\"Expected %s of the form '%slocalname:domain'\" % (cls.__name__, cls.SIGIL)))\n    domain = parts[1]\n    return cls(localpart=parts[0], domain=domain)\n", "label": 0}
{"function": "\n\ndef test_registered_functions(self):\n    'Test the correct functions are registered with gearman'\n    self.start_server()\n    t0 = time.time()\n    failed = True\n    while ((time.time() - t0) < 10):\n        if (len(self.gearman_server.functions) == 4):\n            failed = False\n            break\n        time.sleep(0.01)\n    if failed:\n        self.log.debug(self.gearman_server.functions)\n        self.fail(\"The correct number of functions haven't registered with gearman\")\n    hostname = os.uname()[1]\n    self.assertIn(('stop:turbo-hipster-manager-%s' % hostname), self.gearman_server.functions)\n", "label": 0}
{"function": "\n\ndef recover_view(self, request, version_id, extra_context=None):\n    'Displays a form that can recover a deleted model.'\n    if (not self.has_add_permission(request)):\n        raise PermissionDenied\n    version = get_object_or_404(Version, pk=version_id)\n    context = {\n        'title': (_('Recover %(name)s') % {\n            'name': version.object_repr,\n        }),\n    }\n    context.update((extra_context or {\n        \n    }))\n    return self.revisionform_view(request, version, (self.recover_form_template or self._get_template_list('recover_form.html')), context)\n", "label": 0}
{"function": "\n\n@cli.command('transpose')\n@click.option('-r', '--rotate', callback=convert_rotation, help='Rotates the image (in degrees)')\n@click.option('-f', '--flip', callback=convert_flip, help='Flips the image  [LR / TB]')\n@processor\ndef transpose_cmd(images, rotate, flip):\n    'Transposes an image by either rotating or flipping it.'\n    for image in images:\n        if (rotate is not None):\n            (mode, degrees) = rotate\n            click.echo(('Rotate \"%s\" by %ddeg' % (image.filename, degrees)))\n            image = copy_filename(image.transpose(mode), image)\n        if (flip is not None):\n            (mode, direction) = flip\n            click.echo(('Flip \"%s\" %s' % (image.filename, direction)))\n            image = copy_filename(image.transpose(mode), image)\n        (yield image)\n", "label": 0}
{"function": "\n\ndef __init__(self, domain=None, dependencies=None, setup=None, requirements=None, config_schema=None, platform_schema=None):\n    'Initialize the mock module.'\n    self.DOMAIN = domain\n    self.DEPENDENCIES = (dependencies or [])\n    self.REQUIREMENTS = (requirements or [])\n    if (config_schema is not None):\n        self.CONFIG_SCHEMA = config_schema\n    if (platform_schema is not None):\n        self.PLATFORM_SCHEMA = platform_schema\n    if (setup is None):\n        self.setup = (lambda hass, config: True)\n    else:\n        self.setup = setup\n", "label": 0}
{"function": "\n\ndef exportChildren(self, lwrite, level, namespace_='DNSQueryObj:', name_='DNSQueryObjectType', fromsubclass_=False, pretty_print=True):\n    super(DNSQueryObjectType, self).exportChildren(lwrite, level, 'DNSQueryObj:', name_, True, pretty_print=pretty_print)\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    if (self.Transaction_ID is not None):\n        self.Transaction_ID.export(lwrite, level, 'DNSQueryObj:', name_='Transaction_ID', pretty_print=pretty_print)\n    if (self.Question is not None):\n        self.Question.export(lwrite, level, 'DNSQueryObj:', name_='Question', pretty_print=pretty_print)\n    if (self.Answer_Resource_Records is not None):\n        self.Answer_Resource_Records.export(lwrite, level, 'DNSQueryObj:', name_='Answer_Resource_Records', pretty_print=pretty_print)\n    if (self.Authority_Resource_Records is not None):\n        self.Authority_Resource_Records.export(lwrite, level, 'DNSQueryObj:', name_='Authority_Resource_Records', pretty_print=pretty_print)\n    if (self.Additional_Records is not None):\n        self.Additional_Records.export(lwrite, level, 'DNSQueryObj:', name_='Additional_Records', pretty_print=pretty_print)\n    if (self.Date_Ran is not None):\n        self.Date_Ran.export(lwrite, level, 'DNSQueryObj:', name_='Date_Ran', pretty_print=pretty_print)\n    if (self.Service_Used is not None):\n        self.Service_Used.export(lwrite, level, 'DNSQueryObj:', name_='Service_Used', pretty_print=pretty_print)\n", "label": 1}
{"function": "\n\ndef create(self, string):\n    'Deserialize an xml-formatted security group create request'\n    dom = minidom.parseString(string)\n    security_group = {\n        \n    }\n    sg_node = self.find_first_child_named(dom, 'security_group')\n    if (sg_node is not None):\n        if sg_node.hasAttribute('name'):\n            security_group['name'] = sg_node.getAttribute('name')\n        desc_node = self.find_first_child_named(sg_node, 'description')\n        if desc_node:\n            security_group['description'] = self.extract_text(desc_node)\n    return {\n        'body': {\n            'security_group': security_group,\n        },\n    }\n", "label": 0}
{"function": "\n\ndef create_security_group(self, body=None):\n    s = body.get('security_group')\n    if (not isinstance(s.get('name', ''), six.string_types)):\n        msg = 'BadRequest: Invalid input for name. Reason: None is not a valid string.'\n        raise n_exc.BadRequest(message=msg)\n    if (not isinstance(s.get('description.', ''), six.string_types)):\n        msg = 'BadRequest: Invalid input for description. Reason: None is not a valid string.'\n        raise n_exc.BadRequest(message=msg)\n    if ((len(s.get('name')) > 255) or (len(s.get('description')) > 255)):\n        msg = 'Security Group name great than 255'\n        raise n_exc.NeutronClientException(message=msg, status_code=401)\n    ret = {\n        'name': s.get('name'),\n        'description': s.get('description'),\n        'tenant_id': 'fake',\n        'security_group_rules': [],\n        'id': str(uuid.uuid4()),\n    }\n    self._fake_security_groups[ret['id']] = ret\n    return {\n        'security_group': ret,\n    }\n", "label": 0}
{"function": "\n\ndef _eval_imageset(self, f):\n    expr = f.expr\n    if (not isinstance(expr, Expr)):\n        return\n    if (len(f.variables) > 1):\n        return\n    n = f.variables[0]\n    c = f(0)\n    fx = (f(n) - c)\n    f_x = (f((- n)) - c)\n    neg_count = (lambda e: sum((_coeff_isneg(_) for _ in Add.make_args(e))))\n    if (neg_count(f_x) < neg_count(fx)):\n        expr = (f_x + c)\n    a = Wild('a', exclude=[n])\n    b = Wild('b', exclude=[n])\n    match = expr.match(((a * n) + b))\n    if (match and match[a]):\n        expr = ((match[a] * n) + (match[b] % match[a]))\n    if (expr != f.expr):\n        return ImageSet(Lambda(n, expr), S.Integers)\n", "label": 0}
{"function": "\n\ndef dump_header(iterable, allow_token=True):\n    'Dump an HTTP header again.  This is the reversal of\\n    :func:`parse_list_header`, :func:`parse_set_header` and\\n    :func:`parse_dict_header`.  This also quotes strings that include an\\n    equals sign unless you pass it as dict of key, value pairs.\\n\\n    >>> dump_header({\\'foo\\': \\'bar baz\\'})\\n    \\'foo=\"bar baz\"\\'\\n    >>> dump_header((\\'foo\\', \\'bar baz\\'))\\n    \\'foo, \"bar baz\"\\'\\n\\n    :param iterable: the iterable or dict of values to quote.\\n    :param allow_token: if set to `False` tokens as values are disallowed.\\n                        See :func:`quote_header_value` for more details.\\n    '\n    if isinstance(iterable, dict):\n        items = []\n        for (key, value) in iteritems(iterable):\n            if (value is None):\n                items.append(key)\n            else:\n                items.append(('%s=%s' % (key, quote_header_value(value, allow_token=allow_token))))\n    else:\n        items = [quote_header_value(x, allow_token=allow_token) for x in iterable]\n    return ', '.join(items)\n", "label": 0}
{"function": "\n\ndef wait_for_job(self, job, interval=5, timeout=60):\n    '\\n        Waits until the job indicated by job_resource is done or has failed\\n\\n        Parameters\\n        ----------\\n        job : Union[dict, str]\\n            ``dict`` representing a BigQuery job resource, or a ``str``\\n            representing the BigQuery job id\\n        interval : float, optional\\n            Polling interval in seconds, default = 5\\n        timeout : float, optional\\n            Timeout in seconds, default = 60\\n\\n        Returns\\n        -------\\n        dict\\n            Final state of the job resouce, as described here:\\n            https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.jobs.html#get\\n\\n        Raises\\n        ------\\n        Union[JobExecutingException, BigQueryTimeoutException]\\n            On http/auth failures or timeout\\n        '\n    complete = False\n    job_id = str((job if isinstance(job, (six.binary_type, six.text_type, int)) else job['jobReference']['jobId']))\n    job_resource = None\n    start_time = time()\n    elapsed_time = 0\n    while (not (complete or (elapsed_time > timeout))):\n        sleep(interval)\n        request = self.bigquery.jobs().get(projectId=self.project_id, jobId=job_id)\n        job_resource = request.execute()\n        self._raise_executing_exception_if_error(job_resource)\n        complete = (job_resource.get('status').get('state') == 'DONE')\n        elapsed_time = (time() - start_time)\n    if (not complete):\n        logger.error(('BigQuery job %s timeout' % job_id))\n        raise BigQueryTimeoutException()\n    return job_resource\n", "label": 0}
{"function": "\n\ndef grad(self, inputs, gout):\n    (x,) = inputs\n    (gz,) = gout\n    if self.sparse_grad:\n        left = sp_ones_like(x)\n        right = gz\n        if ((right.dtype == 'float64') and (left.dtype == 'float32')):\n            left = left.astype('float64')\n        if ((right.dtype == 'float32') and (left.dtype == 'float64')):\n            right = right.astype('float64')\n        return [(left * right)]\n    else:\n        return [SparseFromDense(x.type.format)(gz)]\n", "label": 0}
{"function": "\n\n@property\ndef images(self):\n    '\\n    List of URLs of images on the page.\\n    '\n    if (not getattr(self, '_images', False)):\n        self._images = [page['imageinfo'][0]['url'] for page in self.__continued_query({\n            'generator': 'images',\n            'gimlimit': 'max',\n            'prop': 'imageinfo',\n            'iiprop': 'url',\n        }) if ('imageinfo' in page)]\n    return self._images\n", "label": 0}
{"function": "\n\ndef avail(search=None, verbose=False):\n    \"\\n    Return a list of available images\\n\\n    search : string\\n        search keyword\\n    verbose : boolean (False)\\n        toggle verbose output\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' imgadm.avail [percona]\\n        salt '*' imgadm.avail verbose=True\\n    \"\n    ret = {\n        \n    }\n    imgadm = _check_imgadm()\n    cmd = '{0} avail -j'.format(imgadm)\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    result = {\n        \n    }\n    if (retcode != 0):\n        ret['Error'] = _exit_status(retcode)\n        return ret\n    for image in json.loads(res['stdout']):\n        if (image['manifest']['disabled'] or (not image['manifest']['public'])):\n            continue\n        if (search and (search not in image['manifest']['name'])):\n            continue\n        result[image['manifest']['uuid']] = _parse_image_meta(image, verbose)\n    return result\n", "label": 0}
{"function": "\n\ndef validate_args(fn, *args, **kwargs):\n    'Check that the supplied args are sufficient for calling a function.\\n\\n    >>> validate_args(lambda a: None)\\n    Traceback (most recent call last):\\n        ...\\n    MissingArgs: Missing argument(s): a\\n    >>> validate_args(lambda a, b, c, d: None, 0, c=1)\\n    Traceback (most recent call last):\\n        ...\\n    MissingArgs: Missing argument(s): b, d\\n\\n    :param fn: the function to check\\n    :param args: the positional arguments supplied\\n    :param kwargs: the keyword arguments supplied\\n    '\n    argspec = inspect.getargspec(fn)\n    num_defaults = len((argspec.defaults or []))\n    required_args = argspec.args[:(len(argspec.args) - num_defaults)]\n\n    def isbound(method):\n        return (getattr(method, '__self__', None) is not None)\n    if isbound(fn):\n        required_args.pop(0)\n    missing = [arg for arg in required_args if (arg not in kwargs)]\n    missing = missing[len(args):]\n    if missing:\n        raise MissingArgs(missing)\n", "label": 0}
{"function": "\n\ndef __init__(self, application):\n    from django.conf import settings\n    from django.core.exceptions import ImproperlyConfigured\n    from django import VERSION\n    for app in settings.INSTALLED_APPS:\n        if app.startswith('django.'):\n            raise ImproperlyConfigured(\"You must place 'djangae' before any 'django' apps in INSTALLED_APPS\")\n        elif (app == 'djangae'):\n            break\n    self.wrapped_app = application\n", "label": 0}
{"function": "\n\ndef trunc_decimal(val, places):\n    'Legacy compatibility, rounds the way the old satchmo 0.8.1 used to round.'\n    if ((val is None) or (val == '')):\n        return val\n    if (val < 0):\n        roundfactor = '-0.01'\n    else:\n        roundfactor = '0.01'\n    return round_decimal(val=val, places=places, roundfactor=roundfactor, normalize=True)\n", "label": 0}
{"function": "\n\ndef _existing_terms(self, ixreader, termset, reverse=False, phrases=True):\n    fieldname = self.fieldname\n    for word in self._words(ixreader):\n        t = (fieldname, word)\n        contains = (t in ixreader)\n        if reverse:\n            contains = (not contains)\n        if contains:\n            termset.add(t)\n", "label": 0}
{"function": "\n\ndef set_relay_option(self, code, value):\n    if ((code == LINK_SELECTION) or (code == SERVER_IDENTIFIER_OVERRIDE)):\n        value = socket.inet_aton(value)\n    elif (code == VSS):\n        value = binascii.unhexlify(('01' + value))\n    self.relay_options[code] = value\n", "label": 0}
{"function": "\n\ndef gen_info(gen):\n    if (not DEBUG):\n        return None\n    frame = gen.gi_frame\n    if gen.gi_running:\n        prefix = 'running generator '\n    elif frame:\n        if (frame.f_lasti < 0):\n            prefix = 'initial generator '\n        else:\n            prefix = 'suspended generator '\n    else:\n        prefix = 'terminated generator '\n    if frame:\n        return (prefix + frame_info(frame))\n    code = getattr(gen, 'gi_code', None)\n    if code:\n        return (prefix + code_info(code))\n    return (prefix + hex(id(gen)))\n", "label": 0}
{"function": "\n\ndef dump_options_header(header, options):\n    'The reverse function to :func:`parse_options_header`.\\n\\n    :param header: the header to dump\\n    :param options: a dict of options to append.\\n    '\n    segments = []\n    if (header is not None):\n        segments.append(header)\n    for (key, value) in iteritems(options):\n        if (value is None):\n            segments.append(key)\n        else:\n            segments.append(('%s=%s' % (key, quote_header_value(value))))\n    return '; '.join(segments)\n", "label": 0}
{"function": "\n\ndef _format_message(self, msg, tobot=True, toname=False, colon=True, space=True):\n    colon = (':' if colon else '')\n    space = (' ' if space else '')\n    if tobot:\n        msg = '<@{}>{}{}{}'.format(self.testbot_userid, colon, space, msg)\n    elif toname:\n        msg = '{}{}{}{}'.format(self.testbot_username, colon, space, msg)\n    return msg\n", "label": 0}
{"function": "\n\ndef init_empty_package(base_dir, name, scala, java, python, r):\n    repo_name = name.split('/')[1]\n    package_dir = os.path.join(base_dir, repo_name)\n    if os.path.exists(package_dir):\n        raise RuntimeError(('Directory %s already exists' % package_dir))\n    license_id = get_license_id()\n    os.makedirs(package_dir)\n    os.chdir(package_dir)\n    create_license_file(license_id)\n    create_static_file('README.md')\n    create_static_file('.gitignore')\n    if ((not scala) and (not java)):\n        if python:\n            init_python_directories()\n        if r:\n            init_r_directories(name, license_id)\n    else:\n        init_src_directories('resources')\n        if (java or scala):\n            init_sbt_directories(name, license_id)\n        if java:\n            init_src_directories('java')\n        if scala:\n            init_src_directories('scala')\n        if python:\n            init_python_directories()\n        if r:\n            init_r_directories(name, license_id)\n", "label": 1}
{"function": "\n\ndef __init__(self, app, conf, logger=None):\n    self.app = app\n    self.logger = (logger or get_logger(conf, log_route='ratelimit'))\n    self.memcache_client = None\n    self.account_ratelimit = float(conf.get('account_ratelimit', 0))\n    self.max_sleep_time_seconds = float(conf.get('max_sleep_time_seconds', 60))\n    self.log_sleep_time_seconds = float(conf.get('log_sleep_time_seconds', 0))\n    self.clock_accuracy = int(conf.get('clock_accuracy', 1000))\n    self.rate_buffer_seconds = int(conf.get('rate_buffer_seconds', 5))\n    self.ratelimit_whitelist = [acc.strip() for acc in conf.get('account_whitelist', '').split(',') if acc.strip()]\n    self.ratelimit_blacklist = [acc.strip() for acc in conf.get('account_blacklist', '').split(',') if acc.strip()]\n    self.container_ratelimits = interpret_conf_limits(conf, 'container_ratelimit_')\n    self.container_listing_ratelimits = interpret_conf_limits(conf, 'container_listing_ratelimit_')\n", "label": 0}
{"function": "\n\n@blueprint.route('/hint', methods=['GET'])\n@api_wrapper\n@require_login\n@block_before_competition(WebError('The competition has not begun yet!'))\ndef request_problem_hint_hook():\n\n    @log_action\n    def hint(pid, source):\n        return None\n    source = request.args.get('source')\n    pid = request.args.get('pid')\n    if (pid is None):\n        return WebError('Please supply a pid.')\n    if (source is None):\n        return WebError('You have to supply the source of the hint.')\n    tid = api.user.get_team()['tid']\n    if (pid not in api.problem.get_unlocked_pids(tid)):\n        return WebError(\"Your team hasn't unlocked this problem yet!\")\n    hint(pid, source)\n    return WebSuccess('Hint noted.')\n", "label": 0}
{"function": "\n\ndef actions(self, action, comp):\n    if (action == 'delete'):\n        self.emit_event(comp, events.ColumnDeleted, comp)\n    elif (action == 'set_limit'):\n        self.card_counter.call(model='edit')\n    elif (action == 'purge'):\n        self.purge_cards()\n    self.emit_event(comp, events.SearchIndexUpdated)\n", "label": 0}
{"function": "\n\ndef add_item(self, filepath, filetype):\n    annotation_graph = poioapi.annotationgraph.AnnotationGraph(None)\n    if (filetype == poioapi.data.EAF):\n        annotation_graph.from_elan(filepath)\n    if (filetype == poioapi.data.EAFFROMTOOLBOX):\n        annotation_graph.from_elan(filepath)\n    elif (filetype == poioapi.data.TYPECRAFT):\n        annotation_graph.from_typecraft(filepath)\n    else:\n        raise poioapi.data.UnknownFileFormatError()\n    annotation_graph.structure_type_handler = poioapi.data.DataStructureType(annotation_graph.tier_hierarchies[0])\n    self.append((filepath, annotation_graph))\n", "label": 0}
{"function": "\n\ndef reset(ip=None, username=None):\n    'Reset records that match ip or username, and\\n    return the count of removed attempts.\\n    '\n    count = 0\n    attempts = AccessAttempt.objects.all()\n    if ip:\n        attempts = attempts.filter(ip_address=ip)\n    if username:\n        attempts = attempts.filter(username=username)\n    if attempts:\n        count = attempts.count()\n        attempts.delete()\n    return count\n", "label": 0}
{"function": "\n\ndef parseRow(self, csvRowDict):\n    dateTime = datetime.datetime.strptime(csvRowDict[self.__dateTimeColumn], self.__dateTimeFormat)\n    if (self.__timezone is not None):\n        if (self.__timeDelta is not None):\n            dateTime += self.__timeDelta\n        dateTime = dt.localize(dateTime, self.__timezone)\n    values = {\n        \n    }\n    for (key, value) in csvRowDict.items():\n        if (key != self.__dateTimeColumn):\n            values[key] = self.__converter(key, value)\n    return (dateTime, values)\n", "label": 0}
{"function": "\n\ndef __init__(self, lookup_func, dictionary, project, identifier):\n    super(PBXFileReference, self).__init__(lookup_func, dictionary, project, identifier)\n    if (kPBX_REFERENCE_lastKnownFileType in dictionary.keys()):\n        self.ftype = dictionary[kPBX_REFERENCE_lastKnownFileType]\n    if (kPBX_REFERENCE_fileEncoding in dictionary.keys()):\n        self.fileEncoding = dictionary[kPBX_REFERENCE_fileEncoding]\n    if (kPBX_REFERENCE_explicitFileType in dictionary.keys()):\n        self.explicitFileType = dictionary[kPBX_REFERENCE_explicitFileType]\n    if (kPBX_REFERENCE_includeInIndex in dictionary.keys()):\n        self.includeInIndex = dictionary[kPBX_REFERENCE_includeInIndex]\n", "label": 0}
{"function": "\n\ndef get_frame(self):\n    'Get one stomp frame'\n    while (not ('\\x00' in self.incomingData)):\n        data = self.socket.recv(1024)\n        if (not data):\n            raise Exception('Socket seem closed.')\n        else:\n            self.incomingData += data\n    split_data = self.incomingData.split('\\x00', 1)\n    self.incomingData = split_data[1]\n    frame_split = split_data[0].split('\\n')\n    command = frame_split[0]\n    headers = []\n    body = ''\n    headerMode = True\n    for x in frame_split[1:]:\n        if ((x == '') and headerMode):\n            headerMode = False\n        elif headerMode:\n            (key, value) = x.split(':', 1)\n            headers.append((key, value))\n        else:\n            body += (x + '\\n')\n    body = body[:(- 1)]\n    return (command, headers, body)\n", "label": 0}
{"function": "\n\ndef product_simplify(s):\n    'Main function for Product simplification'\n    from sympy.concrete.products import Product\n    terms = Mul.make_args(s)\n    p_t = []\n    o_t = []\n    for term in terms:\n        if isinstance(term, Product):\n            p_t.append(term)\n        else:\n            o_t.append(term)\n    used = ([False] * len(p_t))\n    for method in range(2):\n        for (i, p_term1) in enumerate(p_t):\n            if (not used[i]):\n                for (j, p_term2) in enumerate(p_t):\n                    if ((not used[j]) and (i != j)):\n                        if isinstance(product_mul(p_term1, p_term2, method), Product):\n                            p_t[i] = product_mul(p_term1, p_term2, method)\n                            used[j] = True\n    result = Mul(*o_t)\n    for (i, p_term) in enumerate(p_t):\n        if (not used[i]):\n            result = Mul(result, p_term)\n    return result\n", "label": 1}
{"function": "\n\ndef _build_rooms(self, data):\n    'Get or Create Rooms based on schedule type and set of Tracks'\n    created_rooms = []\n    rooms = sorted(set([x[self.ROOM_KEY] for x in data]))\n    for (i, room) in enumerate(rooms):\n        (room, created) = Room.objects.get_or_create(schedule=self.schedule, name=room, order=i)\n        if created:\n            created_rooms.append(room)\n    return created_rooms\n", "label": 0}
{"function": "\n\ndef run(self, edit, ask=True, edit_pattern=False):\n    window = self.view.window()\n    repo = self.get_repo()\n    if (not repo):\n        return\n    files = self.get_selected_files()\n    to_ignore = [f for (_, f) in files]\n    tracked = [f for (s, f) in files if (s != UNTRACKED_FILES)]\n    if (tracked and (not self.confirm_tracked(tracked))):\n        return\n    if (not to_ignore):\n        sublime.error_message(self.IGNORE_NO_FILES)\n        return\n    if edit_pattern:\n        patterns = []\n        to_ignore.reverse()\n\n        def on_done(pattern=None):\n            if pattern:\n                patterns.append(pattern)\n            if to_ignore:\n                filename = to_ignore.pop()\n                window.show_input_panel(self.IGNORE_LABEL, filename, on_done, noop, on_done)\n            elif patterns:\n                if ask:\n                    if (not self.confirm_ignore(patterns)):\n                        return\n                self.add_to_gitignore(repo, patterns)\n                goto = self.logical_goto_next_file()\n                self.update_status(goto)\n        filename = to_ignore.pop()\n        window.show_input_panel(self.IGNORE_LABEL, filename, on_done, noop, on_done)\n    else:\n        if ask:\n            if (not self.confirm_ignore(to_ignore)):\n                return\n        self.add_to_gitignore(repo, to_ignore)\n        goto = self.logical_goto_next_file()\n        self.update_status(goto)\n", "label": 1}
{"function": "\n\ndef _get_cluster_field(cluster, field):\n    if cluster.get(field):\n        return cluster[field]\n    if cluster.get('cluster_template_id'):\n        cluster_template = api.get_cluster_template(id=cluster['cluster_template_id'])\n        if cluster_template.get(field):\n            return cluster_template[field]\n    return None\n", "label": 0}
{"function": "\n\ndef find_best_match(worktree, token):\n    ' Find the best match for a project in a worktree\\n\\n    '\n    candidates = list()\n    for project in worktree.projects:\n        to_match = os.path.basename(project.src)\n        if (token in to_match):\n            candidates.append(project)\n    max_score = 0\n    best_project = None\n    for candidate in candidates:\n        to_match = os.path.basename(candidate.src)\n        sequence_matcher = difflib.SequenceMatcher(a=token, b=to_match)\n        score = sequence_matcher.ratio()\n        if (score > max_score):\n            max_score = score\n            best_project = candidate\n    if best_project:\n        return best_project.path\n", "label": 0}
{"function": "\n\ndef set(self, section, option, value):\n    \"Functions similarly to PythonConfigParser's set method, except that\\n        the value is implicitly converted to a string.\\n        \"\n    e_value = value\n    if (not isinstance(value, string_types)):\n        e_value = str(value)\n    if PY2:\n        if isinstance(value, unicode):\n            e_value = value.encode('utf-8')\n    ret = PythonConfigParser.set(self, section, option, e_value)\n    self._do_callbacks(section, option, value)\n    return ret\n", "label": 0}
{"function": "\n\ndef find_configdir(self):\n    if (self.args.get('--config') is not None):\n        return self.args.get('--config')\n    if ('GAFFERD_CONFIG' in os.environ):\n        return os.environ.get('GAFFERD_CONFIG')\n    if is_admin():\n        default_paths = system_path()\n    else:\n        default_paths = user_path()\n    for path in default_paths:\n        if os.path.isdir(path):\n            return path\n    return default_path()\n", "label": 0}
{"function": "\n\n@staticmethod\ndef from_model(notify_model):\n    notify = {\n        \n    }\n    if getattr(notify_model, 'on_complete', None):\n        notify['on-complete'] = NotificationsHelper._from_model_sub_schema(notify_model.on_complete)\n    if getattr(notify_model, 'on_success', None):\n        notify['on-success'] = NotificationsHelper._from_model_sub_schema(notify_model.on_success)\n    if getattr(notify_model, 'on_failure', None):\n        notify['on-failure'] = NotificationsHelper._from_model_sub_schema(notify_model.on_failure)\n    return notify\n", "label": 0}
{"function": "\n\n@property\ndef filter_options(self):\n    if hasattr(self, '_filter_options'):\n        return self._filter_options\n    slugs = self.view.kwargs[self.view_kwarg].split('/')\n    (id_map, unresolved) = Identifier.objects.resolve(slugs, exclude_apps=self.exclude_apps)\n    options = {\n        \n    }\n    if unresolved:\n        options['extra'] = []\n        for (key, items) in unresolved.items():\n            if (len(items) > 0):\n                raise Exception(('Could not resolve %s to a single item!' % key))\n            options['extra'].append(key)\n    if id_map:\n        for (slug, ident) in id_map.items():\n            ctype = ident.content_type.model\n            if (ctype not in options):\n                options[ctype] = []\n            options[ctype].append(ident)\n    self._filter_options = options\n    return options\n", "label": 0}
{"function": "\n\ndef _help(*args, **kws):\n    'show help on topic or object'\n    helper.buffer = []\n    _larch = kws.get('_larch', None)\n    if ((helper._larch is None) and (_larch is not None)):\n        helper._larch = _larch\n    if (args == ('',)):\n        args = ('help',)\n    if (helper._larch is None):\n        helper.addtext('cannot start help system!')\n    else:\n        for a in args:\n            helper.help(a)\n    if (helper._larch is not None):\n        helper._larch.writer.write(('%s\\n' % helper.getbuffer()))\n    else:\n        return helper.getbuffer()\n", "label": 0}
{"function": "\n\ndef grad(self, inp, grads):\n    (x,) = inp\n    (gz,) = grads\n    if (x.type in complex_types):\n        raise NotImplementedError()\n    if (self(x).type in discrete_types):\n        if (x.type in discrete_types):\n            return [x.zeros_like(dtype=theano.config.floatX)]\n        else:\n            return [x.zeros_like()]\n    cst = numpy.asarray((numpy.sqrt(numpy.pi) / 2.0), dtype=upcast(x.type.dtype, gz.type.dtype))\n    return (((gz * cst) * exp((erfinv(x) ** 2))),)\n", "label": 0}
{"function": "\n\ndef processWebmention(sourceURL, targetURL, vouchDomain=None):\n    result = False\n    r = requests.get(sourceURL, verify=False)\n    if (r.status_code == requests.codes.ok):\n        mentionData = {\n            'sourceURL': sourceURL,\n            'targetURL': targetURL,\n            'vouchDomain': vouchDomain,\n            'vouched': False,\n            'received': datetime.date.today().strftime('%d %b %Y %H:%M'),\n            'postDate': datetime.date.today().strftime('%Y-%m-%dT%H:%M:%S'),\n        }\n        if ('charset' in r.headers.get('content-type', '')):\n            mentionData['content'] = r.text\n        else:\n            mentionData['content'] = r.content\n        if ((vouchDomain is not None) and cfg['require_vouch']):\n            mentionData['vouched'] = processVouch(sourceURL, targetURL, vouchDomain)\n            result = mentionData['vouched']\n            app.logger.info(('result of vouch? %s' % result))\n        else:\n            result = (not cfg['require_vouch'])\n            app.logger.info(('no vouch domain, result %s' % result))\n        mf2Data = Parser(doc=mentionData['content']).to_dict()\n        hcard = extractHCard(mf2Data)\n        mentionData['hcardName'] = hcard['name']\n        mentionData['hcardURL'] = hcard['url']\n        mentionData['mf2data'] = mf2Data\n    return result\n", "label": 0}
{"function": "\n\ndef save(self):\n    'Save the form.'\n    super(EMailSettingsForm, self).save()\n    load_site_config()\n    if self.cleaned_data['send_test_mail']:\n        site = Site.objects.get_current()\n        siteconfig = SiteConfiguration.objects.get_current()\n        site_url = ('%s://%s' % (siteconfig.get('site_domain_method'), site.domain))\n        if (self.request and self.request.user.is_authenticated()):\n            to_user = self.request.user.email\n        else:\n            to_user = siteconfig.get('site_admin_email')\n        send_mail(_('E-mail settings test'), (_('This is a test of the e-mail settings for the Review Board server at %s.') % site_url), siteconfig.get('mail_default_from'), [to_user], fail_silently=True)\n", "label": 0}
{"function": "\n\ndef cache_set(self, key, expiry, data):\n    '\\n        Add a result to the cache\\n\\n        :key: Cache key to use\\n        :expiry: The expiry timestamp after which the result is stale\\n        :data: The data to cache\\n        '\n    self.cache.set(key, (expiry, data), self.cache_ttl)\n    if getattr(settings, 'CACHEBACK_VERIFY_CACHE_WRITE', True):\n        (__, cached_data) = self.cache.get(key, (None, None))\n        if ((data is not None) and (cached_data is None)):\n            raise RuntimeError(('Unable to save data of type %s to cache' % type(data)))\n", "label": 0}
{"function": "\n\ndef _get_path(self, path):\n    if path.lower().endswith('.yaml'):\n        return path\n    elif path.lower().endswith(('.zip', '.csar')):\n        csar = CSAR(path, self.a_file)\n        if csar.validate():\n            csar.decompress()\n            self.a_file = True\n            return os.path.join(csar.temp_dir, csar.get_main_template())\n    else:\n        ExceptionCollector.appendException(ValueError((_('\"%(path)s\" is not a valid file.') % {\n            'path': path,\n        })))\n", "label": 0}
{"function": "\n\ndef get_foreign_key(self, source, dest, field=None):\n    if (isinstance(source, SelectQuery) or isinstance(dest, SelectQuery)):\n        return (None, None)\n    fk_field = source._meta.rel_for_model(dest, field)\n    if (fk_field is not None):\n        return (fk_field, False)\n    reverse_rel = source._meta.reverse_rel_for_model(dest, field)\n    if (reverse_rel is not None):\n        return (reverse_rel, True)\n    return (None, None)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.file_path == other.file_path) and (self.append_trailing_newlines == other.append_trailing_newlines) and (self.coder == other.coder) and (self.file_name_prefix == other.file_name_prefix) and (self.file_name_suffix == other.file_name_suffix) and (self.num_shards == other.num_shards) and (self.shard_name_template == other.shard_name_template) and (self.validate == other.validate))\n", "label": 0}
{"function": "\n\ndef __init__(self, event_dict, internal_metadata_dict={\n    \n}, rejected_reason=None):\n    event_dict = dict(event_dict)\n    signatures = {name: {sig_id: sig for (sig_id, sig) in sigs.items()} for (name, sigs) in event_dict.pop('signatures', {\n        \n    }).items()}\n    unsigned = dict(event_dict.pop('unsigned', {\n        \n    }))\n    event_dict = intern_dict(event_dict)\n    if USE_FROZEN_DICTS:\n        frozen_dict = freeze(event_dict)\n    else:\n        frozen_dict = event_dict\n    super(FrozenEvent, self).__init__(frozen_dict, signatures=signatures, unsigned=unsigned, internal_metadata_dict=internal_metadata_dict, rejected_reason=rejected_reason)\n", "label": 0}
{"function": "\n\ndef _join_monotonic(self, other, how='left', return_indexers=False):\n    if self.equals(other):\n        ret_index = (other if (how == 'right') else self)\n        if return_indexers:\n            return (ret_index, None, None)\n        else:\n            return ret_index\n    sv = self.values\n    ov = other._values\n    if (self.is_unique and other.is_unique):\n        if (how == 'left'):\n            join_index = self\n            lidx = None\n            ridx = self._left_indexer_unique(sv, ov)\n        elif (how == 'right'):\n            join_index = other\n            lidx = self._left_indexer_unique(ov, sv)\n            ridx = None\n        elif (how == 'inner'):\n            (join_index, lidx, ridx) = self._inner_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n        elif (how == 'outer'):\n            (join_index, lidx, ridx) = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n    else:\n        if (how == 'left'):\n            (join_index, lidx, ridx) = self._left_indexer(sv, ov)\n        elif (how == 'right'):\n            (join_index, ridx, lidx) = self._left_indexer(ov, sv)\n        elif (how == 'inner'):\n            (join_index, lidx, ridx) = self._inner_indexer(sv, ov)\n        elif (how == 'outer'):\n            (join_index, lidx, ridx) = self._outer_indexer(sv, ov)\n        join_index = self._wrap_joined_index(join_index, other)\n    if return_indexers:\n        return (join_index, lidx, ridx)\n    else:\n        return join_index\n", "label": 1}
{"function": "\n\ndef findall(dir=os.curdir):\n    \"Find all files under 'dir' and return the list of full filenames\\n    (relative to 'dir').\\n    \"\n    all_files = []\n    for (base, dirs, files) in os.walk(dir):\n        if ((base == os.curdir) or base.startswith((os.curdir + os.sep))):\n            base = base[2:]\n        if base:\n            files = [os.path.join(base, f) for f in files]\n        all_files.extend(filter(os.path.isfile, files))\n    return all_files\n", "label": 0}
{"function": "\n\ndef remove(self, **kwargs):\n    if kwargs:\n        pipe = self.redis.pipeline()\n        for (list_name, values) in six.iteritems(kwargs):\n            key = self.get_key(list_name)\n            for value in values:\n                pipe.lrem(key, 0, value)\n        pipe.execute()\n", "label": 0}
{"function": "\n\ndef get_config(self):\n    config = {\n        'output_dim': self.output_dim,\n        'init': self.init.__name__,\n        'activation': self.activation.__name__,\n        'W_regularizer': (self.W_regularizer.get_config() if self.W_regularizer else None),\n        'b_regularizer': (self.b_regularizer.get_config() if self.b_regularizer else None),\n        'activity_regularizer': (self.activity_regularizer.get_config() if self.activity_regularizer else None),\n        'W_constraint': (self.W_constraint.get_config() if self.W_constraint else None),\n        'b_constraint': (self.b_constraint.get_config() if self.b_constraint else None),\n        'bias': self.bias,\n        'input_dim': self.input_dim,\n        'input_length': self.input_length,\n    }\n    base_config = super(TimeDistributedDense, self).get_config()\n    return dict((list(base_config.items()) + list(config.items())))\n", "label": 0}
{"function": "\n\ndef call(self, x, mask=None):\n    input_shape = self.input_spec[0].shape\n    input_length = input_shape[1]\n    if (not input_length):\n        if hasattr(K, 'int_shape'):\n            input_length = K.int_shape(x)[1]\n            if (not input_length):\n                raise Exception((('Layer ' + self.name) + ' requires to know the length of its input, but it could not be inferred automatically. Specify it manually by passing an input_shape argument to the first layer in your model.'))\n        else:\n            input_length = K.shape(x)[1]\n    x = K.reshape(x, ((- 1), input_shape[(- 1)]))\n    y = K.dot(x, self.W)\n    if self.bias:\n        y += self.b\n    y = K.reshape(y, ((- 1), input_length, self.output_dim))\n    y = self.activation(y)\n    return y\n", "label": 0}
{"function": "\n\ndef ClauseD(self, rule, tokens, _parent=None):\n    _context = self.Context(_parent, self._scanner, 'ClauseD', [rule, tokens])\n    _token = self._peek('STR', 'ID', 'LP', 'LB', 'STMT', context=_context)\n    if (_token == 'STR'):\n        STR = self._scan('STR', context=_context)\n        t = (STR, eval(STR, {\n            \n        }, {\n            \n        }))\n        if (t not in tokens):\n            tokens.insert(0, t)\n        return parsetree.Terminal(rule, STR)\n    elif (_token == 'ID'):\n        ID = self._scan('ID', context=_context)\n        OptParam = self.OptParam(_context)\n        return resolve_name(rule, tokens, ID, OptParam)\n    elif (_token == 'LP'):\n        LP = self._scan('LP', context=_context)\n        ClauseA = self.ClauseA(rule, tokens, _context)\n        RP = self._scan('RP', context=_context)\n        return ClauseA\n    elif (_token == 'LB'):\n        LB = self._scan('LB', context=_context)\n        ClauseA = self.ClauseA(rule, tokens, _context)\n        RB = self._scan('RB', context=_context)\n        return parsetree.Option(rule, ClauseA)\n    else:\n        STMT = self._scan('STMT', context=_context)\n        return parsetree.Eval(rule, STMT[2:(- 2)])\n", "label": 0}
{"function": "\n\ndef include(elem, loader=None):\n    if (loader is None):\n        loader = default_loader\n    i = 0\n    while (i < len(elem)):\n        e = elem[i]\n        if (e.tag == XINCLUDE_INCLUDE):\n            href = e.get('href')\n            parse = e.get('parse', 'xml')\n            if (parse == 'xml'):\n                node = loader(href, parse)\n                if (node is None):\n                    raise FatalIncludeError(('cannot load %r as %r' % (href, parse)))\n                node = copy.copy(node)\n                if e.tail:\n                    node.tail = ((node.tail or '') + e.tail)\n                elem[i] = node\n            elif (parse == 'text'):\n                text = loader(href, parse, e.get('encoding'))\n                if (text is None):\n                    raise FatalIncludeError(('cannot load %r as %r' % (href, parse)))\n                if i:\n                    node = elem[(i - 1)]\n                    node.tail = (((node.tail or '') + text) + (e.tail or ''))\n                else:\n                    elem.text = (((elem.text or '') + text) + (e.tail or ''))\n                del elem[i]\n                continue\n            else:\n                raise FatalIncludeError(('unknown parse type in xi:include tag (%r)' % parse))\n        elif (e.tag == XINCLUDE_FALLBACK):\n            raise FatalIncludeError(('xi:fallback tag must be child of xi:include (%r)' % e.tag))\n        else:\n            include(e, loader)\n        i = (i + 1)\n", "label": 1}
{"function": "\n\ndef _iter_funcs(self, func_name):\n    hooks = []\n    for plugin in self.instance.master.ctrl.plugins.values():\n        if ('get_hooks' not in plugin):\n            continue\n        hooks.extend(plugin['get_hooks']())\n    if ('hooks' in self.instance.config):\n        hooks.extend(self.instance.config['hooks'].hooks)\n    for hook in hooks:\n        func = getattr(hook, func_name, None)\n        if (func is not None):\n            (yield func)\n", "label": 0}
{"function": "\n\ndef __init__(self, host, username, password, versioncheck=True, use_session=True, verify=True):\n    self._host = host\n    self._username = username\n    self._versioncheck = versioncheck\n    self._use_session = use_session\n    self._verify = verify\n    if use_session:\n        self._transport = bigsuds.BIGIP(host, username, password, verify).with_session_id()\n    else:\n        self._transport = bigsuds.BIGIP(host, username, password, verify)\n    version = self._transport.System.SystemInfo.get_version()\n    if (versioncheck and (not ('BIG-IP_v11' in version))):\n        raise UnsupportedF5Version('This class only supports BIG-IP v11', version)\n    self._active_folder = self.active_folder\n    self._recursive_query = self.recursive_query\n    self._transaction = self.transaction\n    self._transaction_timeout = self.transaction_timeout\n", "label": 0}
{"function": "\n\n@app.route('/direct/<path:url>')\ndef direct(url):\n    kwargs = {\n        \n    }\n    for key in ('width', 'height', 'mode', 'quality', 'transform'):\n        value = (request.args.get(key) or request.args.get(key[0]))\n        if (value is not None):\n            value = (int(value) if value.isdigit() else value)\n            kwargs[key] = value\n    return redirect(images.build_url(url, **kwargs))\n", "label": 0}
{"function": "\n\ndef validate_stats(self):\n    '\\n        Making sure all stats are within range.\\n        Nevermind that we have modernized_stats!\\n        If the modernized stat is bigger than the difference between max and base, we have to adjust.\\n        '\n    diff = self.ship.max_stats.diff(self.ship.base_stats)\n    mapper = inspect(diff)\n    for column in mapper.attrs:\n        if (column.key == 'id'):\n            continue\n        modern_value = getattr(self.modernized_stats, column.key)\n        if (modern_value is not None):\n            diff_value = getattr(diff, column.key)\n            if (modern_value > diff_value):\n                setattr(self.stats, column.key, getattr(self.ship.max_stats, column.key))\n                setattr(self.modernized_stats, column.key, diff_value)\n", "label": 0}
{"function": "\n\ndef _words(self, ixreader):\n    if self.prefix:\n        candidates = ixreader.expand_prefix(self.fieldname, self.prefix)\n    else:\n        candidates = ixreader.lexicon(self.fieldname)\n    exp = self.expression\n    for text in candidates:\n        if exp.match(text):\n            (yield text)\n", "label": 0}
{"function": "\n\ndef expand_prefix(self, fieldid, prefix):\n    'Yields terms in the given field that start with the given prefix.\\n        '\n    fieldid = self.schema.to_number(fieldid)\n    for (fn, t, _, _) in self.iter_from(fieldid, prefix):\n        if ((fn != fieldid) or (not t.startswith(prefix))):\n            return\n        (yield t)\n", "label": 0}
{"function": "\n\ndef __build_args(self, func, args, req, environ):\n    args = build_func_args(func, args, req.GET, self._app_args(args, req))\n    func_args = inspect.getargspec(func).args\n    for func_arg in func_args:\n        if (func_arg == 'ip'):\n            args['ip'] = self.__get_client_address(environ)\n    if ('body' in func_args):\n        args['body'] = req.body_file\n    return args\n", "label": 0}
{"function": "\n\ndef do(self, which_callback, *args):\n    if ((which_callback == 'before_training') or (which_callback == 'on_resumption')):\n        self.worker.init_shared_params(self.main_loop.model.parameters)\n    elif (which_callback == 'after_training'):\n        self.worker.send_req('done')\n    else:\n        self.worker.sync_params()\n", "label": 0}
{"function": "\n\ndef _verify_transaction_file_header(reader, required_header):\n    header = reader.next()\n    if (len(header) != len(required_header)):\n        raise Exception('Header mismatch for transaction file')\n    for i in range(len(required_header)):\n        if (header[i] != required_header[i]):\n            raise Exception(('Header mismatch at %d: %s <> %s' % (i, header[i], required_header[i])))\n", "label": 0}
{"function": "\n\ndef get_proc_family_model():\n    'Extracts the family and model based on vendorId\\n\\n        :rtype: (ComputedFamily, ComputedModel)\\n    '\n    cpuid_res = do_cpuid(1)\n    if is_intel_proc():\n        format = X86IntelCpuidFamilly\n    elif is_amd_proc():\n        format = X86AmdCpuidFamilly\n    else:\n        raise NotImplementedError('Cannot get familly information of proc <{0}>'.format(get_vendor_id()))\n    infos = format.from_buffer_copy(struct.pack('<I', cpuid_res.EAX))\n    if ((infos.FamilyID == 6) or (infos.FamilyID == 15)):\n        ComputedModel = (infos.ModelID + (infos.ExtendedModel << 4))\n    else:\n        ComputedModel = infos.ModelID\n    if (infos.FamilyID == 15):\n        ComputedFamily = (infos.FamilyID + infos.ExtendedFamily)\n    else:\n        ComputedFamily = infos.FamilyID\n    return (ComputedFamily, ComputedModel)\n", "label": 0}
{"function": "\n\ndef with_labels(self, labels=None):\n    'A new landmark group that contains only the certain labels\\n\\n        Parameters\\n        ----------\\n        labels : `str` or `list` of `str`, optional\\n            Labels that should be kept in the returned landmark group. If\\n            ``None`` is passed, and if there is only one label on this group,\\n            the label will be substituted automatically.\\n\\n        Returns\\n        -------\\n        landmark_group : :map:`LandmarkGroup`\\n            A new landmark group with the same group label but containing only\\n            the given label.\\n        '\n    if (labels is None):\n        if (self.n_labels == 1):\n            labels = self.labels\n        else:\n            raise ValueError('Cannot use None as there are {} labels'.format(self.n_labels))\n    if isinstance(labels, str):\n        labels = [labels]\n    return self._new_group_with_only_labels(labels)\n", "label": 0}
{"function": "\n\ndef buttonClick(self, event):\n    curr = self._window._currentFeature.getValue()\n    if (curr is None):\n        nextt = self._app._allFeatures.firstItemId()\n    else:\n        nextt = self._app._allFeatures.nextItemId(curr)\n    while ((nextt is not None) and isinstance(nextt, FeatureSet)):\n        nextt = self._app._allFeatures.nextItemId(nextt)\n    if (nextt is not None):\n        self._window._currentFeature.setValue(nextt)\n    else:\n        self.showNotification('Last sample')\n", "label": 0}
{"function": "\n\n@util.debuglog\ndef info(self, extended=False):\n    result = dict([(proc.pid, proc.info()) for proc in self.processes.values()])\n    if (extended and ('extended_stats' in self.hooks)):\n        for (pid, stats) in result.items():\n            self.hooks['extended_stats'](self, self.arbiter, 'extended_stats', pid=pid, stats=stats)\n    return result\n", "label": 0}
{"function": "\n\ndef createReservation(self, function_name, **kwargs):\n    LOGGER.debug(('%s Called' % function_name))\n    if (function_name == 'schedulerserver/remoteaddtoheap'):\n        LOGGER.debug('remoteaddtoheap has been called')\n        LOGGER.debug(('kwargs: %s' % repr(kwargs)))\n        if set(('uuid', 'type')).issubset(set(kwargs)):\n            LOGGER.debug(('\\tUUID: %s\\n\\tType: %s' % (kwargs['uuid'], kwargs['type'])))\n            if kwargs['uuid']:\n                self.addToHeap(kwargs['uuid'], kwargs['type'])\n            return {\n                \n            }\n        else:\n            return {\n                'error': 'Required parameters are uuid and type',\n            }\n    elif (function_name == 'schedulerserver/remoteremovefromheap'):\n        LOGGER.debug('remoteremovefromheap has been called')\n        LOGGER.debug(('kwargs: %s' % repr(kwargs)))\n        if ('uuid' in kwargs):\n            LOGGER.debug(('UUID: %s' % kwargs['uuid']))\n            if kwargs['uuid']:\n                self.removeFromHeap(kwargs['uuid'])\n            return {\n                \n            }\n        else:\n            return {\n                'error': 'Required parameters are uuid',\n            }\n    return\n", "label": 0}
{"function": "\n\ndef filingEnd(cntlr, options, filesource, entrypointFiles, sourceZipStream=None, responseZipStream=None, *args, **kwargs):\n    modelManager = cntlr.modelManager\n    if hasattr(modelManager, 'efmFiling'):\n        for pluginXbrlMethod in pluginClassMethods('EdgarRenderer.Filing.End'):\n            pluginXbrlMethod(cntlr, options, filesource, modelManager.efmFiling)\n        filingReferences = dict(((report.url, report) for report in modelManager.efmFiling.reports))\n        modelManager.efmFiling.close()\n        del modelManager.efmFiling\n", "label": 0}
{"function": "\n\ndef apply(self, data, sort=True):\n    group_by = self.group_bys_cols[0]\n    if sort:\n        data = sorted(data, key=self.attrgetter(group_by))\n    result = []\n    for item in data:\n        result_item = [self.format_columns(self.attrgetter(group_by)(item))]\n        for aggr_by_col in self.aggr_by_cols:\n            result_item.append(self.attrgetter(aggr_by_col)(item))\n        result.append(result_item)\n    return result\n", "label": 0}
{"function": "\n\ndef minor(self, fill=True):\n    self_list = self.as_list\n    v0 = (str(self_list[0]) if (len(self_list) > 0) else '0')\n    v1 = (str(self_list[1]) if (len(self_list) > 1) else '0')\n    if fill:\n        return Version('.'.join([v0, v1, 'Z']))\n    return Version('.'.join([v0, v1]))\n", "label": 0}
{"function": "\n\n@json_view(permission='upload_tender_documents', validators=(validate_file_update,))\ndef put(self):\n    'Tender Document Update'\n    if (((self.request.authenticated_role != 'auction') and (self.request.validated['tender_status'] != 'active.enquiries')) or ((self.request.authenticated_role == 'auction') and (self.request.validated['tender_status'] not in ['active.auction', 'active.qualification']))):\n        self.request.errors.add('body', 'data', \"Can't update document in current ({}) tender status\".format(self.request.validated['tender_status']))\n        self.request.errors.status = 403\n        return\n    document = upload_file(self.request)\n    self.request.validated['tender'].documents.append(document)\n    if save_tender(self.request):\n        self.LOGGER.info('Updated tender document {}'.format(self.request.context.id), extra=context_unpack(self.request, {\n            'MESSAGE_ID': 'tender_document_put',\n        }))\n        return {\n            'data': document.serialize('view'),\n        }\n", "label": 0}
{"function": "\n\ndef _detect_new_issues(self):\n    loop = True\n    while loop:\n        new_issues = self._jira_client.search_issues(self._jql_query, maxResults=50, startAt=0)\n        for issue in new_issues:\n            if (issue.key not in self._issues_in_project):\n                self._dispatch_issues_trigger(issue)\n                self._issues_in_project[issue.key] = issue\n            else:\n                loop = False\n                break\n", "label": 0}
{"function": "\n\n@expose('/lab/availability/local', methods=['POST'])\ndef change_accessibility(self):\n    lab_id = int(request.form['lab_id'])\n    activate = (request.form['activate'] == 'true')\n    lab = self.session.query(Laboratory).filter_by(id=lab_id).first()\n    if (lab is not None):\n        if activate:\n            lab.available = (not activate)\n            lab.default_local_identifier = ''\n        else:\n            local_id = request.form['default_local_identifier']\n            local_id = local_id.lstrip(' ')\n            local_id = local_id.strip(' ')\n            if ((not activate) and (len(local_id) == 0)):\n                flash(gettext('Invalid local identifier (empty)'))\n                return redirect(url_for('.index_view'))\n            existing_labs = self.session.query(Laboratory).filter_by(default_local_identifier=local_id).all()\n            if ((len(existing_labs) > 0) and (lab not in existing_labs)):\n                flash(gettext(\"Local identifier '%(localidentifier)s' already exists\", localidentifier=local_id))\n                return redirect(url_for('.index_view'))\n            lab.available = (not activate)\n            lab.default_local_identifier = local_id\n        self.session.add(lab)\n        self.session.commit()\n    return redirect(url_for('.index_view'))\n", "label": 0}
{"function": "\n\n@property\ndef average(self):\n    from .quantize import get_color_index\n    if (self._avg is None):\n        total = 0\n        mult = (1 << (8 - SIGBITS))\n        r_sum = 0\n        g_sum = 0\n        b_sum = 0\n        for i in range(self.r1, (self.r2 + 1)):\n            for j in range(self.g1, (self.g2 + 1)):\n                for k in range(self.b1, (self.b2 + 1)):\n                    index = get_color_index(i, j, k)\n                    hval = self.histo[index]\n                    total += hval\n                    r_sum += ((hval * (i + 0.5)) * mult)\n                    g_sum += ((hval * (j + 0.5)) * mult)\n                    b_sum += ((hval * (k + 0.5)) * mult)\n        if total:\n            r_avg = (~ (~ int((r_sum / total))))\n            g_avg = (~ (~ int((g_sum / total))))\n            b_avg = (~ (~ int((b_sum / total))))\n        else:\n            r_avg = (~ (~ int(((mult * ((self.r1 + self.r2) + 1)) / 2))))\n            g_avg = (~ (~ int(((mult * ((self.g1 + self.g2) + 1)) / 2))))\n            b_avg = (~ (~ int(((mult * ((self.b1 + self.b2) + 1)) / 2))))\n        self._avg = (r_avg, g_avg, b_avg)\n    return self._avg\n", "label": 0}
{"function": "\n\ndef __init__(self, root, nav_class, apiname=None, default_curie=None, session=None, id_map=None):\n    self.root = root\n    self.nav_class = nav_class\n    self.apiname = (utils.namify(root) if (apiname is None) else apiname)\n    self.default_curie = default_curie\n    self.session = (session or requests.Session())\n    self.id_map = (id_map if (id_map is not None) else WeakValueDictionary())\n", "label": 0}
{"function": "\n\ndef send_metric(self, name, type, metric, keys, snapshot_keys=None):\n    if (snapshot_keys is None):\n        snapshot_keys = []\n    base_name = re.sub('\\\\s+', '_', name)\n    if self.prefix:\n        base_name = '{0}.{1}'.format(self.prefix, base_name)\n    for name in keys:\n        value = True\n        value = getattr(metric, name)\n        self._buffered_send_metric(base_name, name, value, now())\n    if hasattr(metric, 'snapshot'):\n        snapshot = metric.snapshot\n        for name in snapshot_keys:\n            value = True\n            value = getattr(snapshot, name)\n            self._buffered_send_metric(base_name, name, value, now())\n", "label": 0}
{"function": "\n\ndef test_add_column(self):\n    some_dataset_map = {column.name: column for column in SomeDataSet.columns}\n    sub_dataset_new_col_map = {column.name: column for column in SubDataSetNewCol.columns}\n    sub_col_names = {column.name for column in SubDataSetNewCol.columns}\n    self.assertIn('qux', sub_col_names)\n    self.assertEqual(sub_dataset_new_col_map['qux'].dtype, float64_dtype)\n    self.assertEqual({column.name for column in SomeDataSet.columns}, (sub_col_names - {'qux'}))\n    for (k, some_dataset_column) in some_dataset_map.items():\n        sub_dataset_column = sub_dataset_new_col_map[k]\n        self.assertIsNot(some_dataset_column, sub_dataset_column, ('subclass column %r should not have the same identity as the parent' % k))\n        self.assertEqual(some_dataset_column.dtype, sub_dataset_column.dtype, ('subclass column %r should have the same dtype as the parent' % k))\n", "label": 0}
{"function": "\n\ndef __init__(self, address=None, Body=None):\n    self.address = address\n    if (not Body):\n        self.rawBody = '{}'\n    else:\n        self.rawBody = Body\n    data = json.loads(self.rawBody)\n    self.cmdid = None\n    self.timestamp = int(time.time())\n    self.params = dict()\n    if data.has_key('cmdid'):\n        self.cmdid = int(data['cmdid'])\n    if data.has_key('timestamp'):\n        self.timestamp = int(data['timestamp'])\n    if data.has_key('params'):\n        self.params = data['params']\n", "label": 0}
{"function": "\n\ndef git_root(directory):\n    global git_root_cache\n    retval = False\n    leaf_dir = directory\n    if ((leaf_dir in git_root_cache) and (git_root_cache[leaf_dir]['expires'] > time.time())):\n        return git_root_cache[leaf_dir]['retval']\n    while directory:\n        if os.path.exists(os.path.join(directory, '.git')):\n            retval = directory\n            break\n        parent = os.path.realpath(os.path.join(directory, os.path.pardir))\n        if (parent == directory):\n            retval = False\n            break\n        directory = parent\n    git_root_cache[leaf_dir] = {\n        'retval': retval,\n        'expires': (time.time() + 5),\n    }\n    return retval\n", "label": 0}
{"function": "\n\n@staticmethod\ndef from_full_message(commit_msg_str):\n    '  Parses a full git commit message by parsing a given string into the different parts of a commit message '\n    lines = [line for line in commit_msg_str.split('\\n') if (not line.startswith('#'))]\n    full = '\\n'.join(lines)\n    title = (lines[0] if (len(lines) > 0) else '')\n    body = (lines[1:] if (len(lines) > 1) else [])\n    return GitCommitMessage(original=commit_msg_str, full=full, title=title, body=body)\n", "label": 0}
{"function": "\n\ndef import_pgp(keyring, keydata):\n    '\\n    keydata should be public key data to be imported to the keyring.\\n    The return value will be the sha1 fingerprint of the public key added.\\n    '\n    (out, err, ret) = run_command(['gpg', '--batch', '--status-fd', '1--no-default-keyring', '--keyring', keyring, '--import'], input=keydata)\n    fingerprint = None\n    for line in out.split('\\n'):\n        data = line.split()\n        if ((not data) or (data[0] != '[GNUPG:]')):\n            continue\n        if (data[1] == 'IMPORT_OK'):\n            fingerprint = data[3]\n            break\n    else:\n        raise ValueError('GPG failed to import pgp public key')\n    return fingerprint\n", "label": 0}
{"function": "\n\ndef _parse_list_data(self, raw_data):\n    for entry in raw_data:\n        if ('type' not in entry):\n            raise BootstrapSourceError((self.ERROR_PREFIX + 'No type defined for entry'))\n        entry_type = entry['type']\n        if (entry_type == 'host'):\n            self._parse_host_entry(entry)\n        elif (entry_type == 'cdn'):\n            self._parse_cdn_entry(entry)\n        else:\n            raise BootstrapSourceError((self.ERROR_PREFIX + (\"Invalid type: '%s'\" % entry_type)))\n", "label": 0}
{"function": "\n\ndef _createReservationCallback2(self, data, function_name, uuid):\n    for row in data:\n        if (row[0] == False):\n            raise row[1]\n    if (len(data) == 1):\n        return {\n            data[0][1]: {\n                \n            },\n        }\n    else:\n        return {\n            data[0][1]: data[1][1],\n        }\n", "label": 0}
{"function": "\n\ndef output(self, key, obj):\n    value = get_value((key if (self.attribute is None) else self.attribute), obj)\n    if (value is None):\n        if self.allow_null:\n            return None\n        elif (self.default is not None):\n            return self.default\n    return marshal(value, self.nested)\n", "label": 0}
{"function": "\n\ndef should_return_304(self):\n    'Returns True if the headers indicate that we should return 304.\\n\\n        .. versionadded:: 3.1\\n        '\n    if self.check_etag_header():\n        return True\n    ims_value = self.request.headers.get('If-Modified-Since')\n    if (ims_value is not None):\n        date_tuple = email.utils.parsedate(ims_value)\n        if (date_tuple is not None):\n            if_since = datetime.datetime(*date_tuple[:6])\n            if (if_since >= self.modified):\n                return True\n    return False\n", "label": 0}
{"function": "\n\ndef get_tmy3_data(self, station, station_fallback=True):\n    if (self.stations is None):\n        self.stations = self._load_stations()\n    if (not (station in self.stations)):\n        warnings.warn('Station {} is not a TMY3 station. See self.stations for a complete list.'.format(station))\n        if station_fallback:\n            station = self._find_nearby_station(station)\n        else:\n            station = None\n    if (station is None):\n        return None\n    url = 'http://rredc.nrel.gov/solar/old_data/nsrdb/1991-2005/data/tmy3/{}TYA.CSV'.format(station)\n    r = requests.get(url)\n    if (r.status_code == 200):\n        hours = []\n        for line in r.text.splitlines()[2:]:\n            row = line.split(',')\n            year = row[0][6:10]\n            month = row[0][0:2]\n            day = row[0][3:5]\n            hour = (int(row[1][0:2]) - 1)\n            date_string = '{}{}{}{:02d}'.format(year, month, day, hour)\n            dt = datetime.strptime(date_string, '%Y%m%d%H')\n            temp_C = float(row[31])\n            hours.append({\n                'temp_C': temp_C,\n                'dt': dt,\n            })\n        return hours\n    else:\n        warnings.warn('Station {} was not found. Tried url {}.'.format(station, url))\n        return None\n", "label": 0}
{"function": "\n\ndef run(self, edit, forward=False):\n    view = self.view\n    current_sels = [s for s in view.sel()]\n    trans_sels = view.get_regions('transition_sels')\n    trans_sels.extend(current_sels)\n    view.add_regions('transition_sels', trans_sels)\n    trans_sels = view.get_regions('transition_sels')\n    if forward:\n        (index, sel) = find_next_sel(trans_sels, current_sels[(- 1)])\n    else:\n        (index, sel) = find_prev_sel(trans_sels, current_sels[0])\n    view.sel().clear()\n    view.sel().add(sel)\n    view.show(sel)\n    if (sel.a != sel.b):\n        view.add_regions('mark', [sublime.Region(sel.a, sel.a)], 'mark', '', (sublime.HIDDEN | sublime.PERSISTENT))\n    del trans_sels[index]\n    set_transition_sels(view, trans_sels)\n", "label": 0}
{"function": "\n\ndef _get_division_orientation(self, one, two, splitter=False):\n    ' Returns whether there is a division between two visible QWidgets.\\n\\n        Divided in context means that the widgets are adjacent and aligned along\\n        the direction of the adjaceny.\\n        '\n    united = one.united(two)\n    if splitter:\n        sep = self.control.style().pixelMetric(QtGui.QStyle.PM_DockWidgetSeparatorExtent, None, self.control)\n        united.adjust(0, 0, (- sep), (- sep))\n    if ((one.x() == two.x()) and (one.width() == two.width()) and (united.height() == (one.height() + two.height()))):\n        return QtCore.Qt.Horizontal\n    elif ((one.y() == two.y()) and (one.height() == two.height()) and (united.width() == (one.width() + two.width()))):\n        return QtCore.Qt.Vertical\n    return 0\n", "label": 0}
{"function": "\n\ndef is_filtering_value_found(self, filter_value, value):\n    if isinstance(filter_value, dict):\n        for tag_pair in value:\n            if ((not isinstance(tag_pair, dict)) or (filter_value.get('key') != tag_pair.get('key'))):\n                continue\n            for filter_dict_value in filter_value.get('value'):\n                if super(TaggableItemsDescriber, self).is_filtering_value_found(filter_dict_value, tag_pair.get('value')):\n                    return True\n        return False\n    return super(TaggableItemsDescriber, self).is_filtering_value_found(filter_value, value)\n", "label": 0}
{"function": "\n\ndef header_store_parse(self, name, value):\n    \"+\\n        The name is returned unchanged.  If the input value has a 'name'\\n        attribute and it matches the name ignoring case, the value is returned\\n        unchanged.  Otherwise the name and value are passed to header_factory\\n        method, and the resulting custom header object is returned as the\\n        value.  In this case a ValueError is raised if the input value contains\\n        CR or LF characters.\\n\\n        \"\n    if (hasattr(value, 'name') and (value.name.lower() == name.lower())):\n        return (name, value)\n    if (isinstance(value, str) and (len(value.splitlines()) > 1)):\n        raise ValueError('Header values may not contain linefeed or carriage return characters')\n    return (name, self.header_factory(name, value))\n", "label": 0}
{"function": "\n\n@contextmanager\ndef as_user(self, username=None):\n    'Run nested commands as the given user. For example::\\n\\n            head = local[\"head\"]\\n            head(\"-n1\", \"/dev/sda1\")    # this will fail...\\n            with local.as_user():\\n                head(\"-n1\", \"/dev/sda1\")\\n\\n        :param username: The user to run commands as. If not given, root (or Administrator) is assumed\\n        '\n    if IS_WIN32:\n        if (username is None):\n            username = 'Administrator'\n        self._as_user_stack.append((lambda argv: (['runas', '/savecred', ('/user:%s' % (username,)), (('\"' + ' '.join((str(a) for a in argv))) + '\"')], self.which('runas'))))\n    elif (username is None):\n        self._as_user_stack.append((lambda argv: ((['sudo'] + list(argv)), self.which('sudo'))))\n    else:\n        self._as_user_stack.append((lambda argv: ((['sudo', '-u', username] + list(argv)), self.which('sudo'))))\n    try:\n        (yield)\n    finally:\n        self._as_user_stack.pop((- 1))\n", "label": 0}
{"function": "\n\ndef store(self, name, timehashpairs):\n    ' Store a list of hashes in the hash table\\n            associated with a particular name (or integer ID) and time.\\n        '\n    id_ = self.name_to_id(name, add_if_missing=True)\n    hashmask = ((1 << self.hashbits) - 1)\n    maxtime = (1 << self.maxtimebits)\n    timemask = (maxtime - 1)\n    sortedpairs = timehashpairs\n    idval = (id_ << self.maxtimebits)\n    for (time_, hash_) in sortedpairs:\n        count = self.counts[hash_]\n        time_ &= timemask\n        hash_ &= hashmask\n        val = (idval + time_)\n        if (count < self.depth):\n            self.table[(hash_, count)] = val\n        else:\n            slot = random.randint(0, count)\n            if (slot < self.depth):\n                self.table[(hash_, slot)] = val\n        self.counts[hash_] = (count + 1)\n    self.hashesperid[id_] += len(timehashpairs)\n    self.dirty = True\n", "label": 0}
{"function": "\n\n@staticmethod\ndef getDescNames(arr, root, exclude):\n    for c in root.__subclasses__():\n        if (c.__name__ in exclude):\n            for b in TestSet.getBaseNames({\n                \n            }, c):\n                if (b.__name__ in arr):\n                    del arr[b.__name__]\n        else:\n            arr[c.__name__] = c.__name__\n            if (len(c.__subclasses__()) > 0):\n                TestSet.getDescNames(arr, c, exclude)\n    return arr\n", "label": 0}
{"function": "\n\ndef ex_list_network_domains(self, location=None, name=None, service_plan=None, state=None):\n    '\\n        List networks domains deployed across all data center locations\\n        for your organization.\\n        The response includes the location of each network domain.\\n\\n        :param      location: Only network domains in the location (optional)\\n        :type       location: :class:`NodeLocation` or ``str``\\n\\n        :param      name: Only network domains of this name (optional)\\n        :type       name: ``str``\\n\\n        :param      service_plan: Only network domains of this type (optional)\\n        :type       service_plan: ``str``\\n\\n        :param      state: Only network domains in this state (optional)\\n        :type       state: ``str``\\n\\n        :return: a list of `DimensionDataNetwork` objects\\n        :rtype: ``list`` of :class:`DimensionDataNetwork`\\n        '\n    params = {\n        \n    }\n    if (location is not None):\n        params['datacenterId'] = self._location_to_location_id(location)\n    if (name is not None):\n        params['name'] = name\n    if (service_plan is not None):\n        params['type'] = service_plan\n    if (state is not None):\n        params['state'] = state\n    response = self.connection.request_with_orgId_api_2('network/networkDomain', params=params).object\n    return self._to_network_domains(response)\n", "label": 0}
{"function": "\n\ndef _is_valid_dev_type(device_info, vg):\n    \"Returns bool value if we should use device based on different rules:\\n\\n    1. Should have approved MAJOR number\\n    2. Shouldn't be nbd/ram/loop device\\n    3. Should contain DEVNAME itself\\n    4. Should be compared with vg value\\n\\n    :param device_info: A dict of properties which we get from udevadm.\\n    :param vg: determine if we need LVM devices or not.\\n    :returns: bool if we should use this device.\\n    \"\n    if (('E: MAJOR' in device_info) and (int(device_info['E: MAJOR']) not in VALID_MAJORS)):\n        return False\n    if any((os.path.basename(device_info['E: DEVNAME']).startswith(n) for n in ('nbd', 'ram', 'loop'))):\n        return False\n    if ('E: DEVNAME' not in device_info):\n        return False\n    if ((vg and ('E: DM_VG_NAME' in device_info)) or ((not vg) and ('E: DM_VG_NAME' not in device_info))):\n        return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef close_app(self, app):\n    app_zi = self.window_zindex[app.identifier]\n    for t in self.window_zindex.keys():\n        w = self.window[t]\n        zi = self.window_zindex[t]\n        if (zi > app_zi):\n            self.set_app_zindex(t, (zi - 1))\n    t = self.window[app.identifier]\n    if (not self.remove(t)):\n        Window.alert(('%s not in app' % app.identifier))\n    t.hide()\n    del self.window[app.identifier]\n    del self.window_zindex[app.identifier]\n", "label": 0}
{"function": "\n\ndef datetime(obj, utc=False):\n    if (obj is None):\n        return None\n    elif isinstance(obj, builtin_datetime.date):\n        return obj\n    elif isinstance(obj, basestring):\n        return parse(obj, utc=utc)\n    else:\n        raise ValueError('Can only convert strings into dates, received {}'.format(obj.__class__))\n", "label": 0}
{"function": "\n\ndef __getitem__(self, key):\n    '\\n        Return null for keys beyond the range of the column. This allows for columns to be of uneven length and still be merged into rows cleanly.\\n        '\n    l = len(self)\n    if isinstance(key, slice):\n        indices = six.moves.range(*key.indices(l))\n        return [(list.__getitem__(self, i) if (i < l) else None) for i in indices]\n    if (key >= l):\n        return None\n    return list.__getitem__(self, key)\n", "label": 0}
{"function": "\n\ndef run_validator(self, view):\n    ' Runs a validation pass '\n    view.erase_status('glshadervalidator')\n    self.apply_settings(view)\n    if (view.settings().get('glsv_enabled') == 0):\n        self.clear_errors(view)\n        return\n    if (not self.is_glsl_or_essl(view)):\n        return\n    if self.is_valid_file_ending(view):\n        self.clear_errors\n        self.ANGLECLI.ensure_script_permissions()\n        self.errors = self.ANGLECLI.validate_contents(view)\n        self.show_errors(view)\n    else:\n        view.set_status('glshadervalidator', 'File name must end in .frag or .vert')\n", "label": 0}
{"function": "\n\ndef export(self, lwrite, level, namespace_='DNSQueryObj:', name_='DNSQueryObjectType', namespacedef_='', pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    showIndent(lwrite, level, pretty_print)\n    lwrite(('<%s%s%s' % (namespace_, name_, ((namespacedef_ and (' ' + namespacedef_)) or ''))))\n    already_processed = set()\n    self.exportAttributes(lwrite, level, already_processed, namespace_, name_='DNSQueryObjectType')\n    if self.hasContent_():\n        lwrite(('>%s' % (eol_,)))\n        self.exportChildren(lwrite, (level + 1), namespace_, name_, pretty_print=pretty_print)\n        showIndent(lwrite, level, pretty_print)\n        lwrite(('</%s%s>%s' % (namespace_, name_, eol_)))\n    else:\n        lwrite(('/>%s' % (eol_,)))\n", "label": 0}
{"function": "\n\ndef filingStart(cntlr, options, filesource, entrypointFiles, sourceZipStream=None, responseZipStream=None, *args, **kwargs):\n    modelManager = cntlr.modelManager\n    if (modelManager.validateDisclosureSystem and getattr(modelManager.disclosureSystem, 'EFMplugin', False)):\n        modelManager.efmFiling = Filing(cntlr, options, filesource, entrypointFiles, sourceZipStream, responseZipStream)\n        for pluginXbrlMethod in pluginClassMethods('EdgarRenderer.Filing.Start'):\n            pluginXbrlMethod(cntlr, options, entrypointFiles, modelManager.efmFiling)\n", "label": 0}
{"function": "\n\ndef __init__(self, mod, filename):\n    self.mod = mod\n    self.filename = filename\n    self.validators = []\n    self.runners = []\n    self.runner_name = None\n    for name in dir(self.mod):\n        obj = getattr(self.mod, name)\n        type = getattr(obj, 'scent_api_type', None)\n        if (type == 'runnable'):\n            self.runners.append(obj)\n        elif (type == 'file_validator'):\n            self.validators.append(obj)\n    self.runners = tuple(self.runners)\n    self.validators = tuple(self.validators)\n", "label": 0}
{"function": "\n\ndef best_models(self, nb_models, model, data, max_evals):\n    trials_list = self.compute_trials(model, data, max_evals)\n    num_trials = sum((len(trials) for trials in trials_list))\n    if (num_trials < nb_models):\n        nb_models = len(trials)\n    scores = []\n    for trials in trials_list:\n        scores = (scores + [trial.get('result').get('loss') for trial in trials])\n    cut_off = sorted(scores, reverse=True)[(nb_models - 1)]\n    model_list = []\n    for trials in trials_list:\n        for trial in trials:\n            if (trial.get('result').get('loss') >= cut_off):\n                model = model_from_yaml(trial.get('result').get('model'))\n                model.set_weights(pickle.loads(trial.get('result').get('weights')))\n                model_list.append(model)\n    return model_list\n", "label": 0}
{"function": "\n\ndef _dump_date(d, delim):\n    'Used for `http_date` and `cookie_date`.'\n    if (d is None):\n        d = gmtime()\n    elif isinstance(d, datetime):\n        d = d.utctimetuple()\n    elif isinstance(d, (int, float)):\n        d = gmtime(d)\n    return ('%s, %02d%s%s%s%s %02d:%02d:%02d GMT' % (('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')[d.tm_wday], d.tm_mday, delim, ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')[(d.tm_mon - 1)], delim, str(d.tm_year), d.tm_hour, d.tm_min, d.tm_sec))\n", "label": 0}
{"function": "\n\ndef find_up(name, path=None):\n    'Search upward from the starting path (or the current directory)\\n    until the given file or directory is found. The given name is\\n    assumed to be a basename, not a path.  Returns the absolute path\\n    of the file or directory if found, or None otherwise.\\n\\n    Args\\n    ----\\n    name : str\\n        Base name of the file or directory being searched for.\\n\\n    path : str, optional\\n        Starting directory.  If not supplied, current directory is used.\\n    '\n    if (not path):\n        path = os.getcwd()\n    if (not exists(path)):\n        return None\n    while path:\n        if exists(join(path, name)):\n            return abspath(join(path, name))\n        else:\n            pth = path\n            path = dirname(path)\n            if (path == pth):\n                return None\n    return None\n", "label": 0}
{"function": "\n\ndef asString(self):\n    if self.isSigned:\n        prefix = self.config.iprefix\n    else:\n        prefix = self.config.uprefix\n    if (self.width <= 8):\n        name = (prefix + '8')\n    elif (self.width <= 16):\n        name = (prefix + '16')\n    elif (self.width <= 32):\n        name = (prefix + '32')\n    else:\n        name = 'char*'\n    return name\n", "label": 0}
{"function": "\n\n@classmethod\ndef parse(cls, api, json):\n    status = cls(api)\n    for (k, v) in json.items():\n        if (k == 'user'):\n            user_model = (getattr(api.parser.model_factory, 'user') if api else User)\n            user = user_model.parse(api, v)\n            setattr(status, 'author', user)\n            setattr(status, 'user', user)\n        elif (k == 'created_at'):\n            setattr(status, k, parse_datetime(v))\n        elif (k == 'source'):\n            if ('<' in v):\n                setattr(status, k, parse_html_value(v))\n                setattr(status, 'source_url', parse_a_href(v))\n            else:\n                setattr(status, k, v)\n                setattr(status, 'source_url', None)\n        elif (k == 'retweeted_status'):\n            setattr(status, k, Status.parse(api, v))\n        elif (k == 'place'):\n            if (v is not None):\n                setattr(status, k, Place.parse(api, v))\n            else:\n                setattr(status, k, None)\n        else:\n            setattr(status, k, v)\n    return status\n", "label": 1}
{"function": "\n\ndef fake_db_secgroups(instance, names):\n    secgroups = []\n    for (i, name) in enumerate(names):\n        group_name = ('secgroup-%i' % i)\n        if (isinstance(name, dict) and name.get('name')):\n            group_name = name.get('name')\n        secgroups.append({\n            'id': i,\n            'instance_uuid': instance['uuid'],\n            'name': group_name,\n            'description': 'Fake secgroup',\n            'user_id': instance['user_id'],\n            'project_id': instance['project_id'],\n            'deleted': False,\n            'deleted_at': None,\n            'created_at': None,\n            'updated_at': None,\n        })\n    return secgroups\n", "label": 0}
{"function": "\n\ndef teardown(host=None, port=None):\n    'Remove an installed WSGI hook for ``host`` and ```port``.\\n\\n    If no host or port is passed, the default values will be assumed.\\n    If no hook is installed for the defaults, and both the host and\\n    port are missing, the last hook installed will be removed.\\n\\n    Returns True if a hook was removed, otherwise False.\\n    '\n    both_missing = ((not host) and (not port))\n    host = (host or DEFAULT_HOST)\n    port = (port or DEFAULT_PORT)\n    key = (host, port)\n    key_to_delete = None\n    if (key in INSTALLED):\n        key_to_delete = key\n    if ((not (key in INSTALLED)) and both_missing and (len(INSTALLED) > 0)):\n        (host, port) = key_to_delete = INSTALLED.keys()[(- 1)]\n    if key_to_delete:\n        (_, old_propagate) = INSTALLED[key_to_delete]\n        del INSTALLED[key_to_delete]\n        result = True\n        if (old_propagate is not None):\n            settings.DEBUG_PROPAGATE_EXCEPTIONS = old_propagate\n    else:\n        result = False\n    twill.remove_wsgi_intercept(host, port)\n    return result\n", "label": 1}
{"function": "\n\ndef delete(name, remove=False, force=False):\n    \"\\n    Remove a user from the minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' user.delete name remove=True force=True\\n    \"\n    if salt.utils.contains_whitespace(name):\n        raise SaltInvocationError('Username cannot contain whitespace')\n    if (not info(name)):\n        return True\n    if force:\n        log.warn('force option is unsupported on MacOS, ignoring')\n    if remove:\n        __salt__['file.remove'](info(name)['home'])\n    chgroups(name, ())\n    return (_dscl(['/Users/{0}'.format(name)], ctype='delete')['retcode'] == 0)\n", "label": 0}
{"function": "\n\ndef testOnClassData(self, dataset=None, verbose=False, return_targets=False):\n    'Return winner-takes-all classification output on a given dataset.\\n\\n        If no dataset is given, the dataset passed during Trainer\\n        initialization is used. If return_targets is set, also return\\n        corresponding target classes.\\n        '\n    if (dataset == None):\n        dataset = self.ds\n    dataset.reset()\n    out = []\n    targ = []\n    for seq in dataset._provideSequences():\n        self.module.reset()\n        for (input, target) in seq:\n            res = self.module.activate(input)\n            out.append(argmax(res))\n            targ.append(argmax(target))\n    if return_targets:\n        return (out, targ)\n    else:\n        return out\n", "label": 0}
{"function": "\n\ndef scan_table(scanInstances, scan=None):\n    table = Texttable(800)\n    table.set_cols_dtype(['t', 't', 't', 't'])\n    table.header(['Id', 'Name', 'Status', 'Distribution'])\n    if scan:\n        table.add_row([scan.dbId, ('\\t' + scan.name), scan_status(scan), ''])\n        return table\n    for myScannedInstance in scanInstances:\n        table.add_row([myScannedInstance.dbId, myScannedInstance.name, '', ((((myScannedInstance.distribution.name + ' ') + myScannedInstance.distribution.version) + ' ') + myScannedInstance.distribution.arch)])\n        scans = generics_utils.order_list_object_by(myScannedInstance.scans.scan, 'name')\n        for lscan in scans:\n            table.add_row([lscan.dbId, ('\\t' + lscan.name), scan_status(lscan), ''])\n    return table\n", "label": 0}
{"function": "\n\ndef url_for(self, fn, **kwargs):\n    if (not callable(fn)):\n        raise RouterException('router url_for method only accept callable object.')\n    for (rule, v) in self.rulesMap.items():\n        if (v == fn):\n            if (len(rule.variables) > 0):\n                return rule.build_url(kwargs)\n            return rule.build_url()\n    raise RouterException(\"callable object doesn't matched any routing rule.\")\n", "label": 0}
{"function": "\n\ndef readline(self):\n    if (not self._lines):\n        if self._closed:\n            return ''\n        return NeedMoreData\n    line = self._lines.pop()\n    for ateof in self._eofstack[::(- 1)]:\n        if ateof(line):\n            self._lines.append(line)\n            return ''\n    return line\n", "label": 0}
{"function": "\n\n@timed\ndef gen_predicates(num):\n    res = []\n    for x in range(num):\n        r = random.randint(0, 5)\n        if (r == 0):\n            p_str = (\"name is '%s' and not test\" % random.choice(SELECT_WORDS))\n        elif (r == 1):\n            gender = ('Male' if (random.random() > 0.5) else 'Female')\n            age = random.randint(1, 100)\n            p_str = (\"gender is '%s' and age > %d\" % (gender, age))\n        elif (r == 2):\n            city_letter = chr((97 + random.randint(0, 25)))\n            city_reg = ('^%s.*' % city_letter)\n            age = random.randint(1, 100)\n            p_str = (\"age > %d and city matches '%s'\" % (age, city_reg))\n        elif (r == 3):\n            interest = random.choice(SELECT_WORDS)\n            p_str = (\"interests contains '%s' and test\" % interest)\n        elif (r == 4):\n            gender = ('Male' if random.random() else 'Female')\n            p_str = (\"name is '%s' or gender is '%s'\" % (random.choice(SELECT_WORDS), gender))\n        elif (r == 5):\n            gender = ('Male' if random.random() else 'Female')\n            age = random.randint(1, 100)\n            p_str = (\"(age > %d and gender is '%s')\" % (age, gender))\n            gender = ('Male' if random.random() else 'Female')\n            age = random.randint(1, 100)\n            p_str += (\" or (age < %d and gender is '%s')\" % (age, gender))\n        p = Predicate(p_str)\n        res.append(p)\n    return res\n", "label": 1}
{"function": "\n\ndef scan(filename, installed, sentinel=None):\n    if (not sentinel):\n        sentinel = set()\n    if os.path.isfile(filename):\n        return scan_file(None, filename, sentinel, installed)\n    elif os.path.isdir(filename):\n        return scan_directory(None, filename, sentinel, installed)\n    else:\n        log.error('Could not scan: %s', filename)\n", "label": 0}
{"function": "\n\ndef _GetLibyalGoogleDriveLatestVersion(library_name):\n    'Retrieves the latest version number of a libyal library on Google Drive.\\n\\n  Args:\\n    library_name: the name of the libyal library.\\n\\n  Returns:\\n    The latest version for a given libyal library on Google Drive\\n    or 0 on error.\\n  '\n    download_url = 'https://code.google.com/p/{0:s}/'.format(library_name)\n    page_content = _DownloadPageContent(download_url)\n    if (not page_content):\n        return 0\n    page_content = page_content.decode('utf-8')\n    expression_string = b'<a href=\"(https://googledrive.com/host/[^/]*/)\"[^>]*>Downloads</a>'\n    matches = re.findall(expression_string, page_content)\n    if ((not matches) or (len(matches) != 1)):\n        return 0\n    page_content = _DownloadPageContent(matches[0])\n    if (not page_content):\n        return 0\n    page_content = page_content.decode('utf-8')\n    expression_string = b'/host/[^/]*/{0:s}-[a-z-]*([0-9]+)[.]tar[.]gz'.format(library_name)\n    matches = re.findall(expression_string, page_content)\n    if (not matches):\n        return 0\n    return int(max(matches))\n", "label": 0}
{"function": "\n\ndef ensure_http_prefix_contact(apps, schema_editor):\n    Contact = apps.get_model('membership', 'Contact')\n    for contact in Contact.objects.exclude(homepage='').exclude(homepage__startswith='http://').exclude(homepage__startswith='https://'):\n        if contact.homepage:\n            if (':/' not in contact.homepage):\n                contact.homepage = 'http://{uri}'.format(uri=contact.homepage)\n                contact.save()\n", "label": 0}
{"function": "\n\ndef insert(self, name, attr=None):\n    'Insert a new tag into the current one. The name can be either the\\n        new tag name or an XMLstruct object (in which case attr is ignored).\\n        Unless name is None, we descend into the new tag as a side effect.\\n        A dictionary is expected for attr.'\n    if (not self.current.hasSubtag()):\n        self.current.tag['Icontain'] = []\n    if (name == None):\n        return\n    elif (type(name) == str):\n        newtag = XMLstruct(name, attr)\n    else:\n        newtag = name\n    self.current.tag['Icontain'].append(newtag)\n    self.stack.append(self.current)\n    self.current = newtag\n", "label": 0}
{"function": "\n\ndef count(typename, objects=None):\n    \"Count objects tracked by the garbage collector with a given class name.\\n\\n    Example:\\n\\n        >>> count('dict')\\n        42\\n        >>> count('MyClass', get_leaking_objects())\\n        3\\n        >>> count('mymodule.MyClass')\\n        2\\n\\n    Note that the GC does not track simple objects like int or str.\\n\\n    .. versionchanged:: 1.7\\n       New parameter: ``objects``.\\n\\n    .. versionchanged:: 1.8\\n       Accepts fully-qualified type names (i.e. 'package.module.ClassName')\\n       as well as short type names (i.e. 'ClassName').\\n\\n    \"\n    if (objects is None):\n        objects = gc.get_objects()\n    try:\n        if ('.' in typename):\n            return sum((1 for o in objects if (_long_typename(o) == typename)))\n        else:\n            return sum((1 for o in objects if (_short_typename(o) == typename)))\n    finally:\n        del objects\n", "label": 0}
{"function": "\n\ndef _apply_filter_lookup_types(self):\n    if (not self.filter_lookup_types):\n        return\n    for f in self.filters:\n        f_type = self.filters[f].__class__\n        if (f_type in self.filter_lookup_types):\n            self.filters[f].lookup_type = self.filter_lookup_types[f_type]\n", "label": 0}
{"function": "\n\ndef __init__(self, store):\n    from lino.core.gfks import GenericForeignKey\n    SpecialStoreField.__init__(self, store)\n    self.always_disabled = set()\n    for f in self.store.all_fields:\n        if (f.field is not None):\n            if isinstance(f, VirtStoreField):\n                if (not f.vf.editable):\n                    if (not isinstance(f.vf.return_type, fields.DisplayField)):\n                        self.always_disabled.add(f.name)\n            elif (not isinstance(f.field, GenericForeignKey)):\n                if (not f.field.editable):\n                    self.always_disabled.add(f.name)\n", "label": 0}
{"function": "\n\ndef _validate(self, configuration):\n    for port in configuration.ports:\n        if (port_is_in_trunk_mode(port) and ((port.trunk_vlans is None) or (len(port.trunk_vlans) == 0))):\n            raise FailingCommitResults([TrunkShouldHaveVlanMembers(interface=port.name), ConfigurationCheckOutFailed()])\n    return super(JuniperQfxCopperNetconfDatastore, self)._validate(configuration)\n", "label": 0}
{"function": "\n\ndef inverse_transform(self, codes):\n    if self.character:\n        joiner = ''\n    else:\n        joiner = ' '\n    return [joiner.join([self.decoder[token] for token in code]) for code in codes]\n", "label": 0}
{"function": "\n\n@register_vcs_handler('git', 'keywords')\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    if (not keywords):\n        raise NotThisMethod('no keywords at all, weird')\n    refnames = keywords['refnames'].strip()\n    if refnames.startswith('$Format'):\n        if verbose:\n            print('keywords are unexpanded, not using')\n        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')\n    refs = set([r.strip() for r in refnames.strip('()').split(',')])\n    TAG = 'tag: '\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if (not tags):\n        tags = set([r for r in refs if re.search('\\\\d', r)])\n        if verbose:\n            print((\"discarding '%s', no digits\" % ','.join((refs - tags))))\n    if verbose:\n        print(('likely tags: %s' % ','.join(sorted(tags))))\n    for ref in sorted(tags):\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(('picking %s' % r))\n            return {\n                'version': r,\n                'full-revisionid': keywords['full'].strip(),\n                'dirty': False,\n                'error': None,\n            }\n    if verbose:\n        print('no suitable tags, using unknown + full revision id')\n    return {\n        'version': '0+unknown',\n        'full-revisionid': keywords['full'].strip(),\n        'dirty': False,\n        'error': 'no suitable tags',\n    }\n", "label": 1}
{"function": "\n\ndef last_insert_id(self, cursor, model):\n    sequence = self._get_pk_sequence(model)\n    if (not sequence):\n        return\n    meta = model._meta\n    if meta.schema:\n        schema = ('%s.' % meta.schema)\n    else:\n        schema = ''\n    cursor.execute(('SELECT CURRVAL(\\'%s\"%s\"\\')' % (schema, sequence)))\n    result = cursor.fetchone()[0]\n    if self.get_autocommit():\n        self.commit()\n    return result\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    'Create a string representation of the exception.'\n    sysid = self.getSystemId()\n    if (sysid is None):\n        sysid = '<unknown>'\n    linenum = self.getLineNumber()\n    if (linenum is None):\n        linenum = '?'\n    colnum = self.getColumnNumber()\n    if (colnum is None):\n        colnum = '?'\n    return ('%s:%s:%s: %s' % (sysid, linenum, colnum, self._msg))\n", "label": 0}
{"function": "\n\ndef launch_verify_networks(self, data=None, expect_errors=False, cluster_id=None):\n    if self.clusters:\n        cluster = self._get_cluster_by_id(cluster_id)\n        net_urls = {\n            'nova_network': {\n                'config': 'NovaNetworkConfigurationHandler',\n                'verify': 'NovaNetworkConfigurationVerifyHandler',\n            },\n            'neutron': {\n                'config': 'NeutronNetworkConfigurationHandler',\n                'verify': 'NeutronNetworkConfigurationVerifyHandler',\n            },\n        }\n        provider = cluster.net_provider\n        if data:\n            nets = jsonutils.dumps(data)\n        else:\n            resp = self.app.get(reverse(net_urls[provider]['config'], kwargs={\n                'cluster_id': cluster.id,\n            }), headers=self.default_headers)\n            self.tester.assertEqual(200, resp.status_code)\n            nets = resp.body\n        resp = self.app.put(reverse(net_urls[provider]['verify'], kwargs={\n            'cluster_id': cluster.id,\n        }), nets, headers=self.default_headers, expect_errors=expect_errors)\n        if expect_errors:\n            return resp\n        else:\n            task_uuid = resp.json_body['uuid']\n            return self.db.query(Task).filter_by(uuid=task_uuid).first()\n    else:\n        raise NotImplementedError('Nothing to verify - try creating cluster')\n", "label": 0}
{"function": "\n\ndef parse(readable, *args, **kwargs):\n    'Parse HTML or XML readable.'\n    if (not hasattr(readable, 'read')):\n        readable = open(readable, 'rb')\n    mdp = MicroDOMParser(*args, **kwargs)\n    mdp.filename = getattr(readable, 'name', '<xmlfile />')\n    mdp.makeConnection(None)\n    if hasattr(readable, 'getvalue'):\n        mdp.dataReceived(readable.getvalue())\n    else:\n        r = readable.read(1024)\n        while r:\n            mdp.dataReceived(r)\n            r = readable.read(1024)\n    mdp.connectionLost(None)\n    if (not mdp.documents):\n        raise ParseError(mdp.filename, 0, 0, 'No top-level Nodes in document')\n    if mdp.beExtremelyLenient:\n        if (len(mdp.documents) == 1):\n            d = mdp.documents[0]\n            if (not isinstance(d, Element)):\n                el = Element('html')\n                el.appendChild(d)\n                d = el\n        else:\n            d = Element('html')\n            for child in mdp.documents:\n                d.appendChild(child)\n    else:\n        d = mdp.documents[0]\n    doc = Document(d)\n    doc.doctype = mdp._mddoctype\n    return doc\n", "label": 1}
{"function": "\n\ndef __init__(self, is_root=False, alphabet=None):\n    if (is_root and (alphabet is not None)):\n        self.path = {token: NaiveTrie() for token in alphabet}\n    else:\n        self.path = {\n            \n        }\n    self.is_root = is_root\n    self.is_terminal = True\n    self.NULL_CHAR = 'NULL'\n", "label": 0}
{"function": "\n\ndef _fetch_arguments(handler, method):\n    'Get the arguments depending on the type of HTTP method.'\n    if (method.__name__ == 'get'):\n        arguments = {\n            \n        }\n        for (key, value) in six.iteritems(handler.request.arguments):\n            if isinstance(value, list):\n                arguments[key] = ','.join(value)\n            else:\n                arguments[key] = value\n    else:\n        arguments = handler.get_post_arguments()\n    return arguments\n", "label": 0}
{"function": "\n\ndef haskell_type(view, filename, module_name, line, column, cabal=None):\n    result = None\n    if hsdev.hsdev_enabled():\n\n        def to_file_pos(r):\n            return FilePosition((int(r['line']) - 1), (int(r['column']) - 1))\n\n        def to_region_type(r):\n            return RegionType(r['type'], to_file_pos(r['region']['from']), to_file_pos(r['region']['to']))\n        ts = autocomplete.hsdev_client.ghcmod_type(filename, (line + 1), (column + 1), ghc=get_ghc_opts(filename))\n        if ts:\n            return [to_region_type(r) for r in ts]\n        return None\n    column = sublime_column_to_ghc_column(view, line, column)\n    line = (line + 1)\n    if hdevtools_enabled():\n        result = hdevtools_type(filename, line, column, cabal=cabal)\n    if ((not result) and module_name and ghcmod_enabled()):\n        result = ghcmod_type(filename, module_name, line, column)\n    return (parse_type_output(view, result) if result else None)\n", "label": 1}
{"function": "\n\ndef Finalize(self, state):\n    'Perform all checks that need to occur after all lines are processed.'\n    super(JavaScriptLintRules, self).Finalize(state)\n    if error_check.ShouldCheck(Rule.UNUSED_PRIVATE_MEMBERS):\n        unused_private_members = (self._declared_private_members - self._used_private_members)\n        for variable in unused_private_members:\n            token = self._declared_private_member_tokens[variable]\n            self._HandleError(errors.UNUSED_PRIVATE_MEMBER, ('Unused private member: %s.' % token.string), token)\n        self._declared_private_member_tokens = {\n            \n        }\n        self._declared_private_members = set()\n        self._used_private_members = set()\n    namespaces_info = self._namespaces_info\n    if (namespaces_info is not None):\n        if ((not namespaces_info.GetProvidedNamespaces()) and (not namespaces_info.GetRequiredNamespaces())):\n            missing_provides = namespaces_info.GetMissingProvides()\n            if missing_provides:\n                self._ReportMissingProvides(missing_provides, state.GetFirstToken(), None)\n            (missing_requires, illegal_alias) = namespaces_info.GetMissingRequires()\n            if missing_requires:\n                self._ReportMissingRequires(missing_requires, state.GetFirstToken(), None)\n            if illegal_alias:\n                self._ReportIllegalAliasStatement(illegal_alias)\n    self._CheckSortedRequiresProvides(state.GetFirstToken())\n", "label": 1}
{"function": "\n\ndef __new__(cls, e, z, z0, dir='+'):\n    e = sympify(e)\n    z = sympify(z)\n    z0 = sympify(z0)\n    if (z0 is S.Infinity):\n        dir = '-'\n    elif (z0 is S.NegativeInfinity):\n        dir = '+'\n    if isinstance(dir, string_types):\n        dir = Symbol(dir)\n    elif (not isinstance(dir, Symbol)):\n        raise TypeError(('direction must be of type basestring or Symbol, not %s' % type(dir)))\n    if (str(dir) not in ('+', '-')):\n        raise ValueError((\"direction must be either '+' or '-', not %s\" % dir))\n    obj = Expr.__new__(cls)\n    obj._args = (e, z, z0, dir)\n    return obj\n", "label": 0}
{"function": "\n\n@staticmethod\ndef strategy(opponent):\n    'Looks at opponent history to see if they have defected.\\n\\n        If so, player defection is inversely proportional to when this occurred.\\n        '\n    index = next((index for (index, value) in enumerate(opponent.history, start=1) if (value == D)), None)\n    if (index is None):\n        return C\n    return random_choice((1 - (1 / float(abs(index)))))\n", "label": 0}
{"function": "\n\ndef attach_skins_and_materials(asset, definitions, material_name, default=True):\n    'Find all skins in the ``definitions`` asset which remap the material ``material_name``. Attach the found skin\\n    and references materials to the target ``asset``.'\n    skins = definitions.retrieve_skins()\n    for (skin, materials) in skins.iteritems():\n        attach_skin = False\n        for (v, k) in materials.iteritems():\n            if (v == material_name):\n                attach_skin = True\n                material = definitions.retrieve_material(k, default)\n                asset.attach_material(k, raw=material)\n        if attach_skin:\n            asset.attach_skin(skin, materials)\n", "label": 0}
{"function": "\n\ndef transform_data(self, data, request=None, response=None):\n    'Runs the transforms specified on this endpoint with the provided data, returning the data modified'\n    if (self.transform and (not (isinstance(self.transform, type) and isinstance(data, self.transform)))):\n        if self._params_for_transform:\n            return self.transform(data, **self._arguments(self._params_for_transform, request, response))\n        else:\n            return self.transform(data)\n    return data\n", "label": 0}
{"function": "\n\ndef es_deployments_query(params, facets=None, terms=None, sort_by='snapshot_time'):\n    if (terms is None):\n        terms = ['is_approved', 'sort_by', 'search']\n    if (facets is None):\n        facets = []\n    q = {\n        'query': {\n            'bool': {\n                'must': [{\n                    'match': {\n                        'doc_type': 'Domain',\n                    },\n                }, {\n                    'term': {\n                        'deployment.public': True,\n                    },\n                }],\n            },\n        },\n    }\n    search_query = params.get('search', '')\n    if search_query:\n        q['query']['bool']['must'].append({\n            'match': {\n                '_all': {\n                    'query': search_query,\n                    'operator': 'and',\n                },\n            },\n        })\n    return es_query(params, facets, terms, q)\n", "label": 0}
{"function": "\n\ndef chunkize(text, in_tag):\n    start = 0\n    idx = 0\n    chunks = []\n    for c in text:\n        if (c == '<'):\n            in_tag = True\n            if (start != idx):\n                chunks.append(('text', text[start:idx]))\n            start = idx\n        elif (c == '>'):\n            in_tag = False\n            if (start != (idx + 1)):\n                chunks.append(('tag', text[start:(idx + 1)]))\n            start = (idx + 1)\n        idx += 1\n    if (start != idx):\n        chunks.append((('tag' if in_tag else 'text'), text[start:idx]))\n    return (chunks, in_tag)\n", "label": 0}
{"function": "\n\ndef check_can_access(self, request, can_retry=True, **kwargs):\n    'Check can user access'\n    project = self.get_project(**kwargs)\n    if (not project.can_access(request.user)):\n        if (project.organization and can_retry and request.user.is_authenticated()):\n            Project.objects.update_user_projects(request.user)\n            return self.check_can_access(request, False)\n        elif (not request.user.is_authenticated()):\n            return redirect_to_login(request.get_full_path(), self.get_login_url(), self.get_redirect_field_name())\n        raise PermissionDenied\n", "label": 0}
{"function": "\n\n@flow.StateHandler(next_state='ParseFiles')\ndef Start(self):\n    'Determine the Firefox history directory.'\n    self.state.Register('hist_count', 0)\n    self.state.Register('history_paths', [])\n    if self.args.history_path:\n        self.state.history_paths.append(self.args.history_path)\n    else:\n        self.state.history_paths = self.GuessHistoryPaths(self.args.username)\n        if (not self.state.history_paths):\n            raise flow.FlowError('Could not find valid History paths.')\n    if (self.runner.output is not None):\n        self.runner.output = aff4.FACTORY.Create(self.runner.output.urn, 'VFSAnalysisFile', token=self.token)\n    filename = 'places.sqlite'\n    for path in self.state.history_paths:\n        self.CallFlow('FileFinder', paths=[os.path.join(path, '**2', filename)], pathtype=self.state.args.pathtype, action=file_finder.FileFinderAction(action_type=file_finder.FileFinderAction.Action.DOWNLOAD), next_state='ParseFiles')\n", "label": 0}
{"function": "\n\ndef _step(self, dt):\n    \"pumps all the actions in the node actions container\\n\\n            The actions scheduled to be removed are removed.\\n            Then a :meth:`.Action.step` is called for each action in the\\n            node actions container, and if the action doesn't need any more step\\n            calls, it will be scheduled to be removed. When scheduled to be\\n            removed, the :meth:`.Action.stop` method for the action is called.\\n\\n        Arguments:\\n            dt (float):\\n                The time in seconds that elapsed since that last time this \\n                function was called.\\n        \"\n    for x in self.to_remove:\n        if (x in self.actions):\n            self.actions.remove(x)\n    self.to_remove = []\n    if self.skip_frame:\n        self.skip_frame = False\n        return\n    if (len(self.actions) == 0):\n        self.scheduled = False\n        pyglet.clock.unschedule(self._step)\n    for action in self.actions:\n        if (not action.scheduled_to_remove):\n            action.step(dt)\n            if action.done():\n                self.remove_action(action)\n", "label": 0}
{"function": "\n\ndef defaultColor(self, s):\n    if (s == QsciLexerJavaScript.Keyword):\n        return QColor('#514CA6')\n    elif ((s == QsciLexerJavaScript.Comment) or (s == QsciLexerJavaScript.CommentLine)):\n        return QColor('#29A349')\n    elif (s == QsciLexerJavaScript.DoubleQuotedString):\n        return QColor('#DB0909')\n    elif (s == QsciLexerJavaScript.Number):\n        return QColor('#961212')\n    return QColor('black')\n", "label": 0}
{"function": "\n\ndef truncate_paths(paths, max_samples):\n    '\\n    Truncate the list of paths so that the total number of samples is exactly equal to max_samples. This is done by\\n    removing extra paths at the end of the list, and make the last path shorter if necessary\\n    :param paths: a list of paths\\n    :param max_samples: the absolute maximum number of samples\\n    :return: a list of paths, truncated so that the number of samples adds up to max-samples\\n    '\n    paths = list(paths)\n    total_n_samples = sum((len(path['rewards']) for path in paths))\n    while ((len(paths) > 0) and ((total_n_samples - len(paths[(- 1)]['rewards'])) >= max_samples)):\n        total_n_samples -= len(paths.pop((- 1))['rewards'])\n    if (len(paths) > 0):\n        last_path = paths.pop((- 1))\n        truncated_last_path = dict()\n        truncated_len = (len(last_path['rewards']) - (total_n_samples - max_samples))\n        for (k, v) in last_path.iteritems():\n            if (k in ['observations', 'actions', 'rewards']):\n                truncated_last_path[k] = tensor_utils.truncate_tensor_list(v, truncated_len)\n            elif (k in ['env_infos', 'agent_infos']):\n                truncated_last_path[k] = tensor_utils.truncate_tensor_dict(v, truncated_len)\n            else:\n                raise NotImplementedError\n        paths.append(truncated_last_path)\n    return paths\n", "label": 0}
{"function": "\n\ndef get_active_filters(self):\n    ' Returns a list of tuples with all active filters contained in\\n        this manager and their names.\\n        '\n    ret = []\n    for (i_name, i) in self.filters.iteritems():\n        if isinstance(i, self.FilterGroup):\n            for (f_name, f) in i.filters.iteritems():\n                if f.active:\n                    ret.append((f, f_name))\n        elif i.active:\n            ret.append((i, i_name))\n    return ret\n", "label": 0}
{"function": "\n\ndef stop_server(jboss_config, host=None):\n    '\\n    Stop running jboss instance\\n\\n    jboss_config\\n        Configuration dictionary with properties specified above.\\n    host\\n        The name of the host. JBoss domain mode only - and required if running in domain mode.\\n        The host name is the \"name\" attribute of the \"host\" element in host.xml\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt \\'*\\' jboss7.stop_server \\'{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}\\'\\n\\n       '\n    log.debug('======================== MODULE FUNCTION: jboss7.stop_server')\n    if (host is None):\n        operation = ':shutdown'\n    else:\n        operation = '/host=\"{host}\"/:shutdown'.format(host=host)\n    shutdown_result = __salt__['jboss7_cli.run_operation'](jboss_config, operation, fail_on_error=False)\n    if (shutdown_result['success'] or ((not shutdown_result['success']) and ('Operation failed: Channel closed' in shutdown_result['stdout']))):\n        return shutdown_result\n    else:\n        raise Exception(\"Cannot handle error, return code={retcode}, stdout='{stdout}', stderr='{stderr}' \".format(**shutdown_result))\n", "label": 0}
{"function": "\n\ndef record_metadata(self, root):\n    ' Record metadata for all variables of interest.\\n\\n        Args\\n        ----\\n        root : `System`\\n           System containing variables.\\n        '\n    for recorder in self._recorders:\n        if (recorder._parallel or (self.rank == 0)):\n            if recorder.options['record_metadata']:\n                recorder.record_metadata(root)\n", "label": 0}
{"function": "\n\ndef xmlToAddrList(xml_file):\n    tree = etree.parse(xml_file)\n    root = tree.getroot()\n    addr_list = []\n    for element in root:\n        if ((element.tag == 'node') or (element.tag == 'way')):\n            address = {\n                \n            }\n            for x in element.iter('tag'):\n                addr = ast.literal_eval(str(x.attrib))\n                address[addr['k']] = addr['v']\n            addr_list.append(address)\n    return addr_list\n", "label": 0}
{"function": "\n\ndef get_language(self, language_code, site_id=None):\n    '\\n        Return the language settings for the current site\\n\\n        This function can be used with other settings variables\\n        to support modules which create their own variation of the ``PARLER_LANGUAGES`` setting.\\n        For an example, see :func:`~parler.appsettings.add_default_language_settings`.\\n        '\n    if (language_code is None):\n        if (get_language() is None):\n            raise ValueError(\"language_code can't be null, use translation.activate(..) when accessing translated models outside the request/response loop.\")\n        else:\n            raise ValueError(\"language_code can't be null\")\n    if (site_id is None):\n        site_id = getattr(settings, 'SITE_ID', None)\n    for lang_dict in self.get(site_id, ()):\n        if (lang_dict['code'] == language_code):\n            return lang_dict\n    for lang_dict in self.get(site_id, ()):\n        if (lang_dict['code'].split('-')[0] == language_code.split('-')[0]):\n            return lang_dict\n    return self['default']\n", "label": 0}
{"function": "\n\ndef heartbeat_reply(self, stats):\n    if self.shut_down:\n        return\n    if (not self.online):\n        return\n    if (stats == None):\n        self.active_sessions = None\n        self.go_offline()\n    else:\n        sessions_created = active_sessions = active_streams = preceived = ptransmitted = 0\n        for line in stats.splitlines():\n            line_parts = line.split(':', 1)\n            if (line_parts[0] == 'sessions created'):\n                sessions_created = int(line_parts[1])\n            elif (line_parts[0] == 'active sessions'):\n                active_sessions = int(line_parts[1])\n            elif (line_parts[0] == 'active streams'):\n                active_streams = int(line_parts[1])\n            elif (line_parts[0] == 'packets received'):\n                preceived = int(line_parts[1])\n            elif (line_parts[0] == 'packets transmitted'):\n                ptransmitted = int(line_parts[1])\n            self.update_active(active_sessions, sessions_created, active_streams, preceived, ptransmitted)\n    Timeout(self.heartbeat, randomize(self.hrtb_ival, 0.1))\n", "label": 1}
{"function": "\n\n@team_required\n@login_required\ndef team_leave(request):\n    team = request.team\n    state = team.state_for(request.user)\n    if ((team.manager_access == Team.MEMBER_ACCESS_INVITATION) and (state is None) and (not request.user.is_staff)):\n        raise Http404()\n    if (team.can_leave(request.user) and (request.method == 'POST')):\n        membership = Membership.objects.get(team=team, user=request.user)\n        membership.delete()\n        messages.success(request, MESSAGE_STRINGS['left-team'])\n        return redirect('dashboard')\n    else:\n        return redirect('team_detail', slug=team.slug)\n", "label": 0}
{"function": "\n\n@display_hook\ndef map_display(vmap, max_frames, max_branches):\n    if (not isinstance(vmap, (HoloMap, DynamicMap))):\n        return None\n    if ((len(vmap) == 0) and ((not isinstance(vmap, DynamicMap)) or vmap.sampled)):\n        return sanitize_HTML(vmap)\n    elif (len(vmap) > max_frames):\n        max_frame_warning(max_frames)\n        return sanitize_HTML(vmap)\n    return render(vmap)\n", "label": 0}
{"function": "\n\ndef _do_one_outer_iteration(self, **kwargs):\n    '\\n        One iteration of an outer iteration loop for an algorithm\\n        (e.g. time or parametric study)\\n        '\n    nan_tol = sp.isnan(self['pore.source_tol'])\n    nan_max = sp.isnan(self['pore.source_maxiter'])\n    self._tol_for_all = sp.amin(self['pore.source_tol'][(~ nan_tol)])\n    self._maxiter_for_all = sp.amax(self['pore.source_maxiter'][(~ nan_max)])\n    if (self._guess is None):\n        self._guess = sp.zeros(self._coeff_dimension)\n    t = 1\n    step = 0\n    while ((t > self._tol_for_all) and (step <= self._maxiter_for_all)):\n        (X, t, A, b) = self._do_inner_iteration_stage(guess=self._guess, **kwargs)\n        logger.info(((('tol for Picard source_algorithm in step ' + str(step)) + ' : ') + str(t)))\n        self._guess = X\n        step += 1\n    self._steps = step\n    if ((t >= self._tol_for_all) and (step > self._maxiter_for_all)):\n        raise Exception(((('Iterative algorithm for the source term reached to the maxiter: ' + str(self._maxiter_for_all)) + ' without achieving tol: ') + str(self._tol_for_all)))\n    logger.info('Picard algorithm for source term converged!')\n    self.A = A\n    self.b = b\n    self._tol_reached = t\n    return X\n", "label": 0}
{"function": "\n\ndef star_result(cluster, logdir, cmdline, *args):\n    'Re-print stdout/stderr for last run job(s)'\n    if (not args):\n        args = sorted(cluster.connections.keys())\n    for x in args:\n        job = cluster.last_result.get(cluster.locate(x), None)\n        if job:\n            res = job.result\n            running_time = (job.end_time - job.start_time)\n            if isinstance(res, CommandResult):\n                if res.stdout:\n                    cluster.console.q.put(((x, False), res.stdout.decode(cluster.defaults['character_encoding'], 'replace')))\n                if res.stderr:\n                    cluster.console.q.put(((x, True), res.stderr.decode(cluster.defaults['character_encoding'], 'replace')))\n            else:\n                cluster.console.q.put(((x, True), repr(res)))\n            cluster.console.join()\n", "label": 0}
{"function": "\n\ndef _contains(self, other):\n    from sympy.functions import arg, Abs\n    from sympy.core.containers import Tuple\n    other = sympify(other)\n    isTuple = isinstance(other, Tuple)\n    if (isTuple and (len(other) != 2)):\n        raise ValueError('expecting Tuple of length 2')\n    if (not self.polar):\n        (re, im) = (other if isTuple else other.as_real_imag())\n        for element in self.psets:\n            if And(element.args[0]._contains(re), element.args[1]._contains(im)):\n                return True\n        return False\n    elif self.polar:\n        if isTuple:\n            (r, theta) = other\n        elif other.is_zero:\n            (r, theta) = (S.Zero, S.Zero)\n        else:\n            (r, theta) = (Abs(other), arg(other))\n        for element in self.psets:\n            if And(element.args[0]._contains(r), element.args[1]._contains(theta)):\n                return True\n            return False\n", "label": 1}
{"function": "\n\ndef doMode(self, irc, msg):\n    if self.disabled(irc):\n        return\n    chanserv = self.registryValue('ChanServ')\n    on = ('on %s' % irc.network)\n    if ircutils.strEqual(msg.nick, chanserv):\n        channel = msg.args[0]\n        if (len(msg.args) == 3):\n            if ircutils.strEqual(msg.args[2], irc.nick):\n                mode = msg.args[1]\n                info = self.log.info\n                if (mode == '+o'):\n                    info('Received op from ChanServ in %s %s.', channel, on)\n                elif (mode == '+h'):\n                    info('Received halfop from ChanServ in %s %s.', channel, on)\n                elif (mode == '+v'):\n                    info('Received voice from ChanServ in %s %s.', channel, on)\n", "label": 0}
{"function": "\n\ndef do(self, *args):\n    'Run Django with ImportD.'\n    for bp in self.blueprint_list:\n        self._apply_blueprint(bp)\n    if (not args):\n        args = sys.argv[1:]\n    if (len(args) == 0):\n        return self._handle_management_command('runserver', '8000')\n    if ('livereload' in sys.argv):\n        if (not hasattr(self, 'lr')):\n            print('Livereload setting, lr not configured.')\n            return\n        from livereload import Server\n        server = Server(self)\n        for (pat, cmd) in self.lr.items():\n            parts = pat.split(',')\n            for part in parts:\n                server.watch(part, cmd)\n        server.serve(port=8000)\n        return\n    return self._act_as_manage(*args)\n", "label": 0}
{"function": "\n\ndef normalize(self):\n    subqueries = [q for q in self.subqueries if (q is not NullQuery)]\n    if (not subqueries):\n        return NullQuery\n    subqs = []\n    seenterms = set()\n    for s in subqueries:\n        s = s.normalize()\n        if (s is NullQuery):\n            continue\n        if isinstance(s, Term):\n            term = (s.fieldname, s.text)\n            if (term in seenterms):\n                continue\n            seenterms.add(term)\n        if isinstance(s, self.__class__):\n            subqs += s.subqueries\n        else:\n            subqs.append(s)\n    if (not subqs):\n        return NullQuery\n    if (len(subqs) == 1):\n        return subqs[0]\n    return self.__class__(subqs, boost=self.boost)\n", "label": 1}
{"function": "\n\ndef getWindow(title, exact=False):\n    \"Return Window object if 'title' or its part found in visible windows titles, else return None\\n\\n    Return only 1 window found first\\n    Args:\\n        title: unicode string\\n        exact (bool): True if search only exact match\\n    \"\n    titles = getWindows()\n    hwnd = titles.get(title, None)\n    if ((not hwnd) and (not exact)):\n        for (k, v) in titles.items():\n            if (title in k):\n                hwnd = v\n                break\n    if hwnd:\n        return Window(hwnd)\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    val = self._getval()\n    if isNamedClass(val, Parameter):\n        val = val._getval()\n    w = []\n    if (self.name is not None):\n        w.append((\"name='%s'\" % self.name))\n    if ((self.decimals is not None) and (val is not None)):\n        fmtstr = (('value=%.' + str(self.decimals)) + 'f')\n        string = (fmtstr % float(repr(val)))\n        if (self.stderr is not None):\n            fmtstr = ((' +/- %.' + str(self.decimals)) + 'f')\n            string = (string + (fmtstr % self.stderr))\n        if (self.units is not None):\n            string = (string + (' %s' % self.units))\n        w.append(string)\n    else:\n        w.append(('value=%s' % repr(val)))\n    w.append(('vary=%s' % repr(self.vary)))\n    if (self._expr is not None):\n        w.append((\"expr='%s'\" % self._expr))\n    if (self.min not in (None, (- inf))):\n        w.append(('min=%s' % repr(self.min)))\n    if (self.max not in (None, inf)):\n        w.append(('max=%s' % repr(self.max)))\n    return ('param(%s)' % ', '.join(w))\n", "label": 1}
{"function": "\n\ndef __call__(self, *args, **kw):\n    'Call instance class.'\n    if args:\n        if (not hasattr(self, '_configured')):\n            self._configure_django(DEBUG=True)\n        if (isinstance(args[0], dict) and (len(args) == 2)):\n            for bp in self.blueprint_list:\n                self.apply_blueprint(bp)\n            return self.wsgi_application(*args)\n        if self._is_management_command(args[0]):\n            self._handle_management_command(*args, **kw)\n            return self\n        if isinstance(args[0], list):\n            self.update_urls(args[0])\n            return self\n        if isinstance(args[0], Callable):\n            self.add_view('/{}/'.format(args[0].__name__), args[0])\n            return args[0]\n\n        def ddecorator(candidate):\n            from django.forms import forms\n            if isinstance(candidate, forms.DeclarativeFieldsMetaclass):\n                self.add_form(args[0], candidate, *args[1:], **kw)\n                return candidate\n            self.add_view(args[0], candidate, *args[1:], **kw)\n            return candidate\n        return ddecorator\n    else:\n        self._configure_django(**kw)\n    return self\n", "label": 1}
{"function": "\n\ndef get_notification_data(self):\n    '\\n        Provides custom notification data that is used by the transport layer\\n        when the feed is updated.\\n        '\n    notification_data = dict()\n    if (self.track_unseen and self.track_unread):\n        (unseen_count, unread_count) = self.feed_markers.count('unseen', 'unread')\n        notification_data['unseen_count'] = unseen_count\n        notification_data['unread_count'] = unread_count\n    elif self.track_unseen:\n        unseen_count = self.feed_markers.count('unseen')\n        notification_data['unseen_count'] = unseen_count\n    elif self.track_unread:\n        unread_count = self.feed_markers.count('unread')\n        notification_data['unread_count'] = unread_count\n    return notification_data\n", "label": 0}
{"function": "\n\ndef testGetArtifacts(self):\n    self._LoadAllArtifacts()\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows')\n    for result in results:\n        self.assertTrue((('Windows' in result.supported_os) or (not result.supported_os)))\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', name_list=['TestAggregationArtifact', 'TestFileArtifact'])\n    self.assertItemsEqual([x.name for x in results], ['TestAggregationArtifact'])\n    for result in results:\n        self.assertTrue((('Windows' in result.supported_os) or (not result.supported_os)))\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', source_type=artifact_registry.ArtifactSource.SourceType.REGISTRY_VALUE, name_list=['DepsProvidesMultiple'])\n    self.assertEqual(results.pop().name, 'DepsProvidesMultiple')\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', name_list=['RekallPsList'])\n    self.assertEqual(len(results), 1)\n    self.assertEqual(results.pop().name, 'RekallPsList')\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', exclude_dependents=True)\n    for result in results:\n        self.assertFalse(result.GetArtifactPathDependencies())\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', provides=['users.homedir', 'domain'])\n    for result in results:\n        self.assertTrue((len(set(result.provides).union(set(['users.homedir', 'domain']))) >= 1))\n    results = artifact_registry.REGISTRY.GetArtifacts(os_name='Windows', provides=['nothingprovidesthis'])\n    self.assertEqual(len(results), 0)\n", "label": 0}
{"function": "\n\ndef _get_body_content_string(soup, comments=True):\n    if (not isinstance(soup, bs4.BeautifulSoup)):\n        soup = bs4.BeautifulSoup(soup, HTML_PARSER)\n    return ''.join(((('<!--{}-->'.format(C) if comments else '') if isinstance(C, bs4.Comment) else str(C)) for C in soup.body.contents))\n", "label": 0}
{"function": "\n\ndef dget(self, section, option, default=None, type=str):\n    if (not self.has_option(section, option)):\n        return default\n    value = self.get(section, option)\n    if (type is int):\n        value = int(value)\n    elif (type is bool):\n        value = to_bool(value)\n    elif (type is float):\n        value = float(value)\n    elif (type is not str):\n        raise NotImplementedError()\n    return value\n", "label": 0}
{"function": "\n\ndef __call__(self, state, inst, state_dict):\n    state.manager = manager = manager_of_class(self.class_)\n    if (manager is None):\n        raise exc.UnmappedInstanceError(inst, ('Cannot deserialize object of type %r - no mapper() has been configured for this class within the current Python process!' % self.class_))\n    elif (manager.is_mapped and (not manager.mapper.configured)):\n        manager.mapper._configure_all()\n    if (inst is not None):\n        manager.setup_instance(inst, state)\n    manager.dispatch.unpickle(state, state_dict)\n", "label": 0}
{"function": "\n\ndef _assert_fields_equal(self, a, b, exclude=None):\n    exclude = (exclude or [])\n    fields = {k: v for (k, v) in six.iteritems(self.db_type._fields) if (k not in exclude)}\n    assert_funcs = {\n        'mongoengine.fields.DictField': self.assertDictEqual,\n        'mongoengine.fields.ListField': self.assertListEqual,\n        'mongoengine.fields.SortedListField': self.assertListEqual,\n    }\n    for (k, v) in six.iteritems(fields):\n        assert_func = assert_funcs.get(str(v), self.assertEqual)\n        assert_func(getattr(a, k, None), getattr(b, k, None))\n", "label": 0}
{"function": "\n\ndef __init__(self, disk_spec, vm_name, machine_type):\n    super(AzureDisk, self).__init__(disk_spec)\n    self.host_caching = FLAGS.azure_host_caching\n    self.name = None\n    self.vm_name = vm_name\n    self.lun = None\n    if (self.disk_type == PREMIUM_STORAGE):\n        self.metadata = PREMIUM_STORAGE_METADATA\n    elif (self.disk_type == STANDARD_DISK):\n        self.metadata = {\n            disk.MEDIA: disk.HDD,\n            disk.REPLICATION: AZURE_REPLICATION_MAP[FLAGS.azure_storage_type],\n            disk.LEGACY_DISK_TYPE: disk.STANDARD,\n        }\n    elif (self.disk_type == disk.LOCAL):\n        media = (disk.SSD if LocalDiskIsSSD(machine_type) else disk.HDD)\n        self.metadata = {\n            disk.MEDIA: media,\n            disk.REPLICATION: disk.NONE,\n            disk.LEGACY_DISK_TYPE: disk.LOCAL,\n        }\n", "label": 0}
{"function": "\n\ndef _list_subfolders(project, path, cached_folder_lists, recurse=True):\n    if (project not in cached_folder_lists):\n        cached_folder_lists[project] = dxpy.get_handler(project).describe(input_params={\n            'folders': True,\n        })['folders']\n    if recurse:\n        return (f for f in cached_folder_lists[project] if f.startswith(path))\n    else:\n        return (f for f in cached_folder_lists[project] if (f.startswith(path) and ('/' not in f[(len(path) + 1):])))\n", "label": 0}
{"function": "\n\n@classdef.method('inspect')\ndef method_inspect(self, space):\n    string_format = ((not self.symbol) or (not self.symbol[0].isalpha()) or (not self.symbol.isalnum()))\n    if string_format:\n        result = [':', '\"']\n        for c in self.symbol:\n            if (c == '\"'):\n                result.append('\\\\')\n            result.append(c)\n        result.append('\"')\n        return space.newstr_fromchars(result)\n    else:\n        return space.newstr_fromstr((':%s' % self.symbol))\n", "label": 0}
{"function": "\n\n@property\ndef case_filter(self):\n    now = datetime.datetime.utcnow()\n    fromdate = (now - timedelta(days=42))\n    filters = BaseHNBCReport.base_filters(self)\n    filters.append({\n        'term': {\n            'pp_case_filter.#value': '1',\n        },\n    })\n    filters.append({\n        'range': {\n            'date_birth.#value': {\n                'gte': json_format_date(fromdate),\n            },\n        },\n    })\n    status = self.request_params.get('PNC_status', '')\n    or_stmt = []\n    if status:\n        if (status == 'On Time'):\n            for i in range(1, 8):\n                filters.append({\n                    'term': {\n                        ('case_pp_%s_done.#value' % i): 'yes',\n                    },\n                })\n        else:\n            for i in range(1, 8):\n                or_stmt.append({\n                    'not': {\n                        'term': {\n                            ('case_pp_%s_done.#value' % i): 'yes',\n                        },\n                    },\n                })\n            or_stmt = {\n                'or': or_stmt,\n            }\n            filters.append(or_stmt)\n    return ({\n        'and': filters,\n    } if filters else {\n        \n    })\n", "label": 0}
{"function": "\n\ndef tokenize(text):\n    tokenized = []\n    for c in punc:\n        text = text.replace(c, ((' ' + c) + ' '))\n    for c in contractions:\n        text = text.replace(c, (' |' + c))\n    text = text.replace(\" |'\", ' `')\n    text = text.replace(\"'\", \" ' \")\n    tokens = text.split(' ')\n    tokens = [token for token in tokens if token]\n    return tokens\n", "label": 0}
{"function": "\n\ndef get_ical(self, obj, request):\n    ' Returns a populated iCalendar instance. '\n    cal = vobject.iCalendar()\n    cal.add('method').value = 'PUBLISH'\n    items = self.__get_dynamic_attr('items', obj)\n    cal_name = self.__get_dynamic_attr('cal_name', obj)\n    cal_desc = self.__get_dynamic_attr('cal_desc', obj)\n    if cal_name:\n        cal.add('x-wr-calname').value = cal_name\n    if cal_desc:\n        cal.add('x-wr-caldesc').value = cal_desc\n    if get_current_site:\n        current_site = get_current_site(request)\n    else:\n        current_site = None\n    for item in items:\n        event = cal.add('vevent')\n        for (vkey, key) in EVENT_ITEMS:\n            value = self.__get_dynamic_attr(key, item)\n            if value:\n                if (vkey == 'rruleset'):\n                    event.rruleset = value\n                else:\n                    if ((vkey == 'url') and current_site):\n                        value = add_domain(current_site.domain, value, request.is_secure())\n                    event.add(vkey).value = value\n    return cal\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.I32):\n                self.success = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef configure_input(inp, op):\n    ' Configure the inp using op.'\n    if is_old_pipeline():\n        if op.is_a('vtkDataSet'):\n            inp.input = op\n        else:\n            inp.input = op.output\n    elif hasattr(op, 'output_port'):\n        if hasattr(inp, 'input_connection'):\n            inp.input_connection = op.output_port\n        elif hasattr(inp, 'set_input_connection'):\n            inp.set_input_connection(op.output_port)\n    elif op.is_a('vtkAlgorithmOutput'):\n        inp.input_connection = op\n    elif op.is_a('vtkDataSet'):\n        inp.set_input_data(op)\n    else:\n        raise ValueError(('Unknown input type for object %s' % op))\n", "label": 0}
{"function": "\n\ndef _regenerate_derived_data():\n    global SUPPORTED_REGIONS, COUNTRY_CODES_FOR_NON_GEO_REGIONS, _NANPA_REGIONS\n    SUPPORTED_REGIONS.clear()\n    COUNTRY_CODES_FOR_NON_GEO_REGIONS.clear()\n    for (cc, region_codes) in COUNTRY_CODE_TO_REGION_CODE.items():\n        if ((len(region_codes) == 1) and (region_codes[0] == REGION_CODE_FOR_NON_GEO_ENTITY)):\n            COUNTRY_CODES_FOR_NON_GEO_REGIONS.add(cc)\n        else:\n            SUPPORTED_REGIONS.update(region_codes)\n    if (REGION_CODE_FOR_NON_GEO_ENTITY in SUPPORTED_REGIONS):\n        SUPPORTED_REGIONS.remove(REGION_CODE_FOR_NON_GEO_ENTITY)\n    _NANPA_REGIONS.clear()\n    _NANPA_REGIONS.update(COUNTRY_CODE_TO_REGION_CODE[_NANPA_COUNTRY_CODE])\n", "label": 0}
{"function": "\n\ndef get_migrator(direction, db_dry_run, fake, load_initial_data):\n    if (not direction):\n        return direction\n    if db_dry_run:\n        direction = DryRunMigrator(migrator=direction, ignore_fail=False)\n    elif fake:\n        direction = FakeMigrator(migrator=direction)\n    elif load_initial_data:\n        direction = LoadInitialDataMigrator(migrator=direction)\n    return direction\n", "label": 0}
{"function": "\n\ndef generate_joins(self, joins, model_class, alias_map):\n    clauses = []\n    seen = set()\n    q = [model_class]\n    while q:\n        curr = q.pop()\n        if ((curr not in joins) or (curr in seen)):\n            continue\n        seen.add(curr)\n        for join in joins[curr]:\n            src = curr\n            dest = join.dest\n            if isinstance(join.on, (Expression, Func, Clause, Entity)):\n                constraint = join.on.clone().alias()\n            else:\n                metadata = join.metadata\n                if metadata.is_backref:\n                    fk_model = join.dest\n                    pk_model = join.src\n                else:\n                    fk_model = join.src\n                    pk_model = join.dest\n                fk = metadata.foreign_key\n                if fk:\n                    lhs = getattr(fk_model, fk.name)\n                    rhs = getattr(pk_model, fk.to_field.name)\n                    if metadata.is_backref:\n                        (lhs, rhs) = (rhs, lhs)\n                    constraint = (lhs == rhs)\n                else:\n                    raise ValueError('Missing required join predicate.')\n            if isinstance(dest, Node):\n                dest_n = dest\n            else:\n                q.append(dest)\n                dest_n = dest.as_entity().alias(alias_map[dest])\n            join_type = join.get_join_type()\n            if (join_type in self.join_map):\n                join_sql = SQL(self.join_map[join_type])\n            else:\n                join_sql = SQL(join_type)\n            clauses.append(Clause(join_sql, dest_n, SQL('ON'), constraint))\n    return clauses\n", "label": 1}
{"function": "\n\ndef read(config, args):\n    pp = pprint.PrettyPrinter()\n    url = (config['url'] + '/api/recording/')\n    if ((args['all'] is False) and (args['recording'] is None)):\n        print('No recording(s) specified to read. Use -h to see usage.')\n        return\n    params = {\n        \n    }\n    if ((args['recording'] is not None) and (args['all'] is False)):\n        url += (args['recording'] + '/')\n        if (args['traffic'] is True):\n            url += 'traffic/'\n        if (args['start'] is not None):\n            params['start'] = args['start']\n        if (args['offset'] is not None):\n            params['offset'] = args['offset']\n    token = auth.get_token(config)\n    r = requests.get(url, params=params, headers={\n        'X-Auth-Token': token,\n    })\n    if (r.status_code != 200):\n        print(('API returned status code %s' % r.status_code))\n        sys.exit(1)\n    else:\n        pp.pprint(json.loads(r.content))\n", "label": 1}
{"function": "\n\ndef build(self):\n    self.opt.log('Checking Root CA...')\n    self.checkRootCA()\n    self.opt.log('Checking Test Directory...')\n    self.initDirectory()\n    if self.opt.compFunc:\n        self.opt.log('Building Functionality Test Cases...')\n        cases = TestFunctionality(self.fqdn, self.info).build().getTestCases()\n        for test in cases:\n            self.addTestCase(test, self.opt.replace)\n    if self.opt.compCert:\n        self.opt.log('Building X509 Test Cases...')\n        cases = self.getAllTestCases(TestCase)\n        for test in cases:\n            self.addTestCase(test, self.opt.replace)\n    if self.opt.compOverflow:\n        self.opt.log('Building Overflow Test Cases...')\n        cases = TestOverflow(self.fqdn, self.info, self.opt.overflowLen).build().getTestCases()\n        for test in cases:\n            self.addTestCase(test, self.opt.replace)\n    self.baseCase = ValidCert(self.fqdn, self.info)\n    self.baseCase.testBuild(False)\n    saveSerial()\n    return self\n", "label": 0}
{"function": "\n\ndef clean(self):\n    external_segment_dict = {\n        \n    }\n    cleaned_data = super(UpdateL3PolicyForm, self).clean()\n    if self.is_valid():\n        ipversion = int(cleaned_data['ip_version'])\n        subnet_prefix_length = int(cleaned_data['subnet_prefix_length'])\n        msg = _('Subnet prefix out of range.')\n        if ((ipversion == 4) and (subnet_prefix_length not in range(2, 31))):\n            raise forms.ValidationError(msg)\n        if ((ipversion == 6) and (subnet_prefix_length not in range(2, 128))):\n            raise forms.ValidationError(msg)\n        if cleaned_data['external_segments']:\n            dic = {\n                \n            }\n            for external_segment in cleaned_data['external_segments']:\n                values = [i.split(':')[1] for i in external_segment.split(',')]\n                dic[values[0]] = [values[1]]\n                external_segment_dict.update(dic)\n            cleaned_data['external_segments'] = external_segment_dict\n        else:\n            cleaned_data['external_segments'] = {\n                \n            }\n        updated_data = {d: cleaned_data[d] for d in cleaned_data if (d in self.changed_data)}\n        cleaned_data = updated_data\n    return cleaned_data\n", "label": 1}
{"function": "\n\ndef update_subclass(self, target):\n    if (target not in self._clslevel):\n        self._clslevel[target] = collections.deque()\n    clslevel = self._clslevel[target]\n    for cls in target.__mro__[1:]:\n        if (cls in self._clslevel):\n            clslevel.extend([fn for fn in self._clslevel[cls] if (fn not in clslevel)])\n", "label": 0}
{"function": "\n\ndef authorize(self, cli, ev_msg):\n    if (('key' in self.props) and ((len(ev_msg['params']) < 2) or (self.props['key'] != ev_msg['params'][1]))):\n        cli.dump_numeric('475', [self.name, 'Cannot join channel (+k) - bad key'])\n        return False\n    if ('exempt' in self.props):\n        for e in self.props['exempt']:\n            if match(0, e, cli.hostmask):\n                return True\n    if ('ban' in self.props):\n        for b in self.props['ban']:\n            if match(0, b, cli.hostmask):\n                cli.dump_numeric('474', [self.name, 'You are banned.'])\n                return False\n    if (('invite' in self.props) and ('invite-exemption' in self.props)):\n        for i in self.props['invite-exemption']:\n            if match(0, i, cli.hostmask):\n                return True\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, offset, name=None):\n    if isinstance(offset, numbers.Integral):\n        offset = datetime.timedelta(minutes=offset)\n    elif (not isinstance(offset, datetime.timedelta)):\n        raise TypeError('offset must be an integer or datetime.timedelta')\n    if ((offset < self.MIN_PRECISION) and (total_seconds(offset) > 0)):\n        raise ValueError('the minimum precision of offset is minute')\n    elif (offset > self.MAX_PRECISION):\n        raise ValueError('the maximum precision of offset is hour')\n    if (name is None):\n        seconds = int(total_seconds(offset))\n        if (seconds != 0):\n            name = '{0:+03d}:{1:02d}'.format((seconds // 3600), ((abs(seconds) % 3600) // 60))\n        else:\n            name = 'UTC'\n    elif (not isinstance(name, basestring)):\n        raise TypeError(('name must be a string, not ' + repr(name)))\n    self.offset = offset\n    self.name = name\n", "label": 1}
{"function": "\n\n@staticmethod\ndef Mul(expr, assumptions):\n    '\\n        As long as there is at most only one noncommutative term:\\n        Hermitian*Hermitian         -> !Antihermitian\\n        Hermitian*Antihermitian     -> Antihermitian\\n        Antihermitian*Antihermitian -> !Antihermitian\\n        '\n    if expr.is_number:\n        return AskImaginaryHandler._number(expr, assumptions)\n    nccount = 0\n    result = False\n    for arg in expr.args:\n        if ask(Q.antihermitian(arg), assumptions):\n            result = (result ^ True)\n        elif (not ask(Q.hermitian(arg), assumptions)):\n            break\n        if ask((~ Q.commutative(arg)), assumptions):\n            nccount += 1\n            if (nccount > 1):\n                break\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef retrieve_and_update(self, control, attention=1.0):\n    value = None\n    if (self.left and self.right):\n        move_right = self.tree.search(self.h, control)\n        value = self.right.retrieve_and_update(control, attention=(attention * move_right))\n        value += self.left.retrieve_and_update(control, attention=(attention * (1 - move_right)))\n    else:\n        value = (attention * self.value)\n    if (self.left and self.right):\n        self.h = self.tree.join(self.left.h, self.right.h)\n    else:\n        self.h = ((attention * self.tree.write(self.h, control)) + ((1 - attention) * self.h))\n    return value\n", "label": 0}
{"function": "\n\ndef install(packages, update=False, options=None, version=None):\n    \"\\n    Install one or more packages.\\n\\n    If *update* is ``True``, the package definitions will be updated\\n    first, using :py:func:`~fabtools.deb.update_index`.\\n\\n    Extra *options* may be passed to ``apt-get`` if necessary.\\n\\n    Example::\\n\\n        import fabtools\\n\\n        # Update index, then install a single package\\n        fabtools.deb.install('build-essential', update=True)\\n\\n        # Install multiple packages\\n        fabtools.deb.install([\\n            'python-dev',\\n            'libxml2-dev',\\n        ])\\n\\n        # Install a specific version\\n        fabtools.deb.install('emacs', version='23.3+1-1ubuntu9')\\n\\n    \"\n    manager = MANAGER\n    if update:\n        update_index()\n    if (options is None):\n        options = []\n    if (version is None):\n        version = ''\n    if (version and (not isinstance(packages, list))):\n        version = ('=' + version)\n    if (not isinstance(packages, basestring)):\n        packages = ' '.join(packages)\n    options.append('--quiet')\n    options.append('--assume-yes')\n    options = ' '.join(options)\n    cmd = ('%(manager)s install %(options)s %(packages)s%(version)s' % locals())\n    run_as_root(cmd, pty=False)\n", "label": 0}
{"function": "\n\ndef format_uris_pubmedcentral(*args):\n    identifiers = helpers.gather_identifiers(args)\n    (provider_uris, object_uris) = helpers.seperate_provider_object_uris(identifiers)\n    for arg in args:\n        if (arg and ('oai:pubmedcentral.nih.gov:' in arg[0])):\n            PMC_ID = arg[0].replace('oai:pubmedcentral.nih.gov:', '')\n            canonical_uri = ('http://www.ncbi.nlm.nih.gov/pmc/articles/PMC' + PMC_ID)\n    if (not canonical_uri):\n        raise ValueError('No Canonical URI was returned for this record.')\n    return {\n        'canonicalUri': canonical_uri,\n        'objectUris': object_uris,\n        'providerUris': provider_uris,\n    }\n", "label": 0}
{"function": "\n\n@staticmethod\ndef log(expr, assumptions):\n    if ask(Q.real(expr.args[0]), assumptions):\n        if ask(Q.positive(expr.args[0]), assumptions):\n            return False\n        return\n    if (expr.args[0].func == exp):\n        if (expr.args[0].args[0] in [I, (- I)]):\n            return True\n    im = ask(Q.imaginary(expr.args[0]), assumptions)\n    if (im is False):\n        return False\n", "label": 0}
{"function": "\n\ndef _validate(self, value):\n    if (not isinstance(value, str)):\n        raise datastore_errors.BadValueError(('Expected str, got %r' % (value,)))\n    if (self._indexed and (not isinstance(self, TextProperty)) and (len(value) > _MAX_STRING_LENGTH)):\n        raise datastore_errors.BadValueError(('Indexed value %s must be at most %d bytes' % (self._name, _MAX_STRING_LENGTH)))\n", "label": 0}
{"function": "\n\n@cache\ndef search(query, results=10, suggestion=False):\n    '\\n  Do a Wikipedia search for `query`.\\n\\n  Keyword arguments:\\n\\n  * results - the maxmimum number of results returned\\n  * suggestion - if True, return results and suggestion (if any) in a tuple\\n  '\n    search_params = {\n        'list': 'search',\n        'srprop': '',\n        'srlimit': results,\n        'limit': results,\n        'srsearch': query,\n    }\n    if suggestion:\n        search_params['srinfo'] = 'suggestion'\n    raw_results = _wiki_request(search_params)\n    if ('error' in raw_results):\n        if (raw_results['error']['info'] in ('HTTP request timed out.', 'Pool queue is full')):\n            raise HTTPTimeoutError(query)\n        else:\n            raise WikipediaException(raw_results['error']['info'])\n    search_results = (d['title'] for d in raw_results['query']['search'])\n    if suggestion:\n        if raw_results['query'].get('searchinfo'):\n            return (list(search_results), raw_results['query']['searchinfo']['suggestion'])\n        else:\n            return (list(search_results), None)\n    return list(search_results)\n", "label": 0}
{"function": "\n\n@click.command()\n@click.option('--ip-version', help='Display only IPv4', type=click.Choice(['v4', 'v6']))\n@environment.pass_env\ndef cli(env, ip_version):\n    'List all global IPs.'\n    mgr = SoftLayer.NetworkManager(env.client)\n    table = formatting.Table(['id', 'ip', 'assigned', 'target'])\n    version = None\n    if (ip_version == 'v4'):\n        version = 4\n    elif (ip_version == 'v6'):\n        version = 6\n    ips = mgr.list_global_ips(version=version)\n    for ip_address in ips:\n        assigned = 'No'\n        target = 'None'\n        if ip_address.get('destinationIpAddress'):\n            dest = ip_address['destinationIpAddress']\n            assigned = 'Yes'\n            target = dest['ipAddress']\n            virtual_guest = dest.get('virtualGuest')\n            if virtual_guest:\n                target += (' (%s)' % virtual_guest['fullyQualifiedDomainName'])\n            elif ip_address['destinationIpAddress'].get('hardware'):\n                target += (' (%s)' % dest['hardware']['fullyQualifiedDomainName'])\n        table.add_row([ip_address['id'], ip_address['ipAddress']['ipAddress'], assigned, target])\n    env.fout(table)\n", "label": 0}
{"function": "\n\n@method_decorator(catch_and_raise_exceptions)\ndef get_total_row(self):\n\n    def _clean_total_row(val, col):\n        if isinstance(val, numbers.Number):\n            return val\n        elif col.calculate_total:\n            return 0\n        return ''\n\n    def _get_relevant_column_ids(col, column_id_to_expanded_column_ids):\n        return column_id_to_expanded_column_ids.get(col.column_id, [col.column_id])\n    expanded_columns = get_expanded_columns(self.column_configs, self.config)\n    qc = self.query_context()\n    for c in self.columns:\n        qc.append_column(c.view)\n    session = connection_manager.get_scoped_session(self.engine_id)\n    totals = qc.totals(session.connection(), [column_id for col in self.column_configs for column_id in _get_relevant_column_ids(col, expanded_columns) if col.calculate_total], self.filter_values)\n    total_row = [_clean_total_row(totals.get(column_id), col) for col in self.column_configs for column_id in _get_relevant_column_ids(col, expanded_columns)]\n    if (total_row and (total_row[0] is '')):\n        total_row[0] = ugettext('Total')\n    return total_row\n", "label": 1}
{"function": "\n\ndef _configure(self, as_defaults, kw):\n    if self._started:\n        raise TypeError('this TransactionFactory is already started')\n    not_supported = []\n    for (k, v) in kw.items():\n        for dict_ in (self._url_cfg, self._engine_cfg, self._maker_cfg, self._ignored_cfg, self._facade_cfg, self._transaction_ctx_cfg):\n            if (k in dict_):\n                dict_[k] = (_Default(v) if as_defaults else v)\n                break\n        else:\n            not_supported.append(k)\n    if not_supported:\n        warnings.warn(('Configuration option(s) %r not supported' % sorted(not_supported)), exception.NotSupportedWarning)\n", "label": 0}
{"function": "\n\ndef add_group(self, name, exclusive, group_filters=None, overwrite=False):\n    ' Adds a filter group.\\n\\n        :param str name: The name of the new filter group.\\n        :param bool exclusive: Determines if the new group is exclusive, i.e.\\n            only one of its items can be active at the same time.\\n        :param bool overwrite: If ``True``, an existing group with the same\\n            name will be overwritten. If ``False`` and a group with the same\\n            name exists, a value error is raised.\\n            Default: False\\n        '\n    if (name in self.filters):\n        prev = self.filters[name]\n        if isinstance(prev, FilterManager.Filter):\n            raise ValueError('A filter with this name already exists!')\n        if (not overwrite):\n            raise ValueError('A filter group with this name already exists!')\n    g = self.FilterGroup(exclusive)\n    if group_filters:\n        if exclusive:\n            for f in group_filters.itervalues():\n                f.active = False\n            group_filters.values()[0].active = True\n        g.filters = group_filters\n    else:\n        g.filters = OrderedDict()\n    self.filters[name] = g\n", "label": 0}
{"function": "\n\ndef validate(self, value):\n    if self.validator:\n        value = self.validator.validate(value)\n    if (self.min is not None):\n        if ((value < self.min) or ((value == self.min) and (self.include_min is False))):\n            self.error(value)\n    if (self.max is not None):\n        if ((value > self.max) or ((value == self.max) and (self.include_max is False))):\n            self.error(value)\n    return value\n", "label": 1}
{"function": "\n\ndef get_ports_for_vlan(self, vlan):\n    ports = []\n    for port in self.switch_configuration.ports:\n        if (not isinstance(port, VlanPort)):\n            if ((port.trunk_vlans and (vlan.number in port.trunk_vlans)) or (port.access_vlan == vlan.number)):\n                ports.append(port)\n    return ports\n", "label": 0}
{"function": "\n\ndef slice_locs(self, start=None, end=None, step=None, kind=None):\n    \"\\n        Compute slice locations for input labels.\\n\\n        Parameters\\n        ----------\\n        start : label, default None\\n            If None, defaults to the beginning\\n        end : label, default None\\n            If None, defaults to the end\\n        step : int, defaults None\\n            If None, defaults to 1\\n        kind : {'ix', 'loc', 'getitem'} or None\\n\\n        Returns\\n        -------\\n        start, end : int\\n\\n        \"\n    inc = ((step is None) or (step >= 0))\n    if (not inc):\n        (start, end) = (end, start)\n    start_slice = None\n    if (start is not None):\n        start_slice = self.get_slice_bound(start, 'left', kind)\n    if (start_slice is None):\n        start_slice = 0\n    end_slice = None\n    if (end is not None):\n        end_slice = self.get_slice_bound(end, 'right', kind)\n    if (end_slice is None):\n        end_slice = len(self)\n    if (not inc):\n        (end_slice, start_slice) = ((start_slice - 1), (end_slice - 1))\n        if (end_slice == (- 1)):\n            end_slice -= len(self)\n        if (start_slice == (- 1)):\n            start_slice -= len(self)\n    return (start_slice, end_slice)\n", "label": 1}
{"function": "\n\n@app.route('/shred/<string:cluster_id>', methods=['GET', 'POST'])\n@login.login_required\ndef shred(cluster_id):\n    cluster = Cluster.objects.get(id=cluster_id)\n    if (not cluster):\n        abort(404)\n    if (request.method == 'POST'):\n        tags = set(map(unicode.lower, request.form.getlist('tags')))\n        user_tags = cluster.get_user_tags(g.user)\n        if (not user_tags):\n            abort(404)\n        user_tags.tags = list(tags)\n        user_tags.recognizable_chars = request.form.get('recognizable_chars', '')\n        user_tags.angle = int(request.form.get('angle', 0))\n        cluster.save()\n        User.objects(pk=g.user.id).update_one(add_to_set__tags=list(tags))\n        for tag in tags:\n            Tags.objects(pk=tag).update_one(set_on_insert__is_base=False, set_on_insert__created_by=g.user.id, set_on_insert__created_at=Tags().created_at, inc__usages=1, add_to_set__shreds=request.form['_id'], upsert=True)\n        return render_template('_shred_snippet.html', cluster=cluster)\n    else:\n        return render_template('_shred.html', cluster=cluster, auto_tags=cluster.auto_tags, all_tags=get_tags(), user_data=cluster.get_user_tags(g.user), edit=True)\n", "label": 0}
{"function": "\n\n@classmethod\ndef parse(cls, buf):\n    'Parse DHCP Packet.\\n\\n        1. To get client IP Address(ciaddr).\\n        2. To get relaying gateway IP Address(giaddr).\\n        3. To get DHCP Relay Agent Information Option Suboption\\n        such as Link Selection, VSS, Server Identifier override.\\n        '\n    pkt = DhcpPacket()\n    (pkt.ciaddr,) = cls.struct('4s').unpack_from(buf, 12)\n    (pkt.giaddr,) = cls.struct('4s').unpack_from(buf, 24)\n    cls.struct('4s').pack_into(buf, 24, b'')\n    pos = 240\n    while (pos < len(buf)):\n        (opttag,) = cls.struct('B').unpack_from(buf, pos)\n        if (opttag == 0):\n            pos += 1\n            continue\n        if (opttag == END):\n            pkt.end = pos\n            break\n        (optlen,) = cls.struct('B').unpack_from(buf, (pos + 1))\n        startpos = pos\n        pos += 2\n        if (opttag != RELAY_AGENT_INFO):\n            pos += optlen\n            continue\n        optend = (pos + optlen)\n        while (pos < optend):\n            (subopttag, suboptlen) = cls.struct('BB').unpack_from(buf, pos)\n            fmt = ('%is' % (suboptlen,))\n            (val,) = cls.struct(fmt).unpack_from(buf, (pos + 2))\n            pkt.relay_options[subopttag] = val\n            pos += (suboptlen + 2)\n        cls.struct(('%is' % (optlen + 2))).pack_into(buf, startpos, b'')\n    pkt.buf = buf\n    return pkt\n", "label": 0}
{"function": "\n\ndef handle_starttag(self, tag, attributes):\n    name = [v for (k, v) in attributes if (k == 'id')]\n    if name:\n        self.names.append(name[0])\n    if (tag == 'a'):\n        name = [v for (k, v) in attributes if (k == 'name')]\n        if name:\n            self.names.append(name[0])\n", "label": 0}
{"function": "\n\ndef build_message_from_gitlog(user_profile, name, ref, commits, before, after, url, pusher, forced=None, created=None):\n    short_ref = re.sub('^refs/heads/', '', ref)\n    subject = name\n    if re.match('^0+$', after):\n        content = ('%s deleted branch %s' % (pusher, short_ref))\n    elif ((forced and (not created)) or ((forced is None) and (len(commits) == 0))):\n        content = ('%s [force pushed](%s) to branch %s.  Head is now %s' % (pusher, url, short_ref, after[:7]))\n    else:\n        content = build_commit_list_content(commits, short_ref, url, pusher)\n    return (subject, content)\n", "label": 0}
{"function": "\n\ndef __new__(mcs, *args, **kwargs):\n    \"\\n        Handles the url_map generation based on the rules from the base classes\\n        Added to the class' own rules\\n        \"\n    name = args[0]\n    bases = args[1]\n    members = args[2].copy()\n    rules = members.get('rules', [])\n    for base in (base for base in bases if hasattr(base, 'rules')):\n        for base_rule in base.rules:\n            if (not any((((base_rule.rule == rule.rule) and set(base_rule.methods).intersection(set(rule.methods))) for rule in rules))):\n                rules.append(base_rule)\n    url_map = members.get('url_map', Map())\n    for rule in rules:\n        url_map.add(Rule(rule.rule, methods=rule.methods, endpoint=rule.endpoint))\n    members['rules'] = list(url_map.iter_rules())\n    members['url_map'] = url_map\n    return super(mcs, mcs).__new__(mcs, *(name, bases, members), **kwargs)\n", "label": 1}
{"function": "\n\ndef _validate_field(self):\n    version = self._tpl_version()\n    if (not version):\n        ExceptionCollector.appendException(MissingRequiredFieldError(what='Template', required=DEFINITION_VERSION))\n    else:\n        self._validate_version(version)\n        self.version = version\n    for name in self.tpl:\n        if ((name not in SECTIONS) and (name not in self.ADDITIONAL_SECTIONS.get(version, ()))):\n            ExceptionCollector.appendException(UnknownFieldError(what='Template', field=name))\n", "label": 0}
{"function": "\n\ndef _split_path(self, path):\n    \"\\n        Hack to try to support both types of file system paths:\\n          - forward slash, /etc\\n          - backslash, C:\\\\windows\\\\system32\\n\\n        Linux uses forward slashes, so we'd like that when working with FUSE.\\n        The original file system used backslashes, so we'd also like that.\\n\\n        This is a poor attempt at doing both:\\n          - detect which slash type is in use\\n          - don't support both at the same time\\n\\n        This works like string.partition(PATH_SEPARATOR)\\n        \"\n    if ('\\\\' in path):\n        if ('/' in path):\n            raise UnsupportedPathError(path)\n        return path.partition('\\\\')\n    elif ('/' in path):\n        if ('\\\\' in path):\n            raise UnsupportedPathError(path)\n        return path.partition('/')\n    else:\n        return (path, '', '')\n", "label": 0}
{"function": "\n\ndef match_hostname(cert, hostname):\n    'Verify that *cert* (in decoded format as returned by\\n    SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 rules\\n    are mostly followed, but IP addresses are not accepted for *hostname*.\\n\\n    CertificateError is raised on failure. On success, the function\\n    returns nothing.\\n    '\n    if (not cert):\n        raise ValueError('empty or no certificate')\n    dnsnames = []\n    san = cert.get('subjectAltName', ())\n    for (key, value) in san:\n        if (key == 'DNS'):\n            if _dnsname_to_pat(value).match(hostname):\n                return\n            dnsnames.append(value)\n    if (not san):\n        for sub in cert.get('subject', ()):\n            for (key, value) in sub:\n                if (key == 'commonName'):\n                    if _dnsname_to_pat(value).match(hostname):\n                        return\n                    dnsnames.append(value)\n    if (len(dnsnames) > 1):\n        raise CertificateError((\"hostname %r doesn't match either of %s\" % (hostname, ', '.join(map(repr, dnsnames)))))\n    elif (len(dnsnames) == 1):\n        raise CertificateError((\"hostname %r doesn't match %r\" % (hostname, dnsnames[0])))\n    else:\n        raise CertificateError('no appropriate commonName or subjectAltName fields were found')\n", "label": 1}
{"function": "\n\n@classmethod\ndef generate_vlan_ids_list(cls, data, cluster, ng):\n    if (ng['name'] == 'fixed'):\n        netw_params = data.get('networking_parameters', {\n            \n        })\n        start = netw_params.get('fixed_networks_vlan_start')\n        amount = netw_params.get('fixed_networks_amount')\n        if (start and amount):\n            return range(int(start), (int(start) + int(amount)))\n    if (ng.get('vlan_start') is None):\n        return []\n    return [int(ng.get('vlan_start'))]\n", "label": 0}
{"function": "\n\ndef __init__(self, host=None, port=None, auth_mech=None):\n    if (host is not None):\n        self.host = host\n    elif ('IMPYLA_TEST_HOST' in os.environ):\n        self.host = os.environ['IMPYLA_TEST_HOST']\n    else:\n        sys.stderr.write(\"IMPYLA_TEST_HOST not set; using 'localhost'\")\n        self.host = 'localhost'\n    if (port is not None):\n        self.port = port\n    elif ('IMPYLA_TEST_PORT' in os.environ):\n        self.port = int(os.environ['IMPYLA_TEST_PORT'])\n    else:\n        sys.stderr.write('IMPYLA_TEST_PORT not set; using 21050')\n        self.port = 21050\n    if (auth_mech is not None):\n        self.auth_mech = auth_mech\n    elif ('IMPYLA_TEST_AUTH_MECH' in os.environ):\n        self.auth_mech = os.environ['IMPYLA_TEST_AUTH_MECH']\n    else:\n        sys.stderr.write(\"IMPYLA_TEST_AUTH_MECH not set; using 'NOSASL'\")\n        self.auth_mech = 'NOSASL'\n", "label": 0}
{"function": "\n\ndef off(self, event=None, *handlers):\n    'unregister an event or handler from an event'\n    if (not event):\n        self.events.clear()\n        return True\n    if (not (event in self.events)):\n        raise EventNotFound(event)\n    if (not handlers):\n        self.events.pop(event, None)\n        return True\n    for callback in handlers:\n        if (not (callback in self.events[event])):\n            raise HandlerNotFound(event, callback)\n        while (callback in self.events[event]):\n            self.events[event].remove(callback)\n    return True\n", "label": 0}
{"function": "\n\ndef standardize_Y(shape, Y):\n    if (not numpy_array(Y)):\n        Y = np.asarray(Y)\n    if (len(Y.shape) == 1):\n        Y = Y.reshape((- 1), 1)\n    if ((len(Y.shape) == 2) and (len(shape) == 2)):\n        if (shape[(- 1)] != Y.shape[(- 1)]):\n            return one_hot(Y, n=shape[(- 1)])\n        else:\n            return Y\n    else:\n        return Y\n", "label": 0}
{"function": "\n\ndef reindex(self, target, method=None, level=None, limit=None, tolerance=None):\n    \"\\n        Create index with target's values (move/add/delete values as necessary)\\n\\n        Parameters\\n        ----------\\n        target : an iterable\\n\\n        Returns\\n        -------\\n        new_index : pd.Index\\n            Resulting index\\n        indexer : np.ndarray or None\\n            Indices of output values in original index\\n\\n        \"\n    preserve_names = (not hasattr(target, 'name'))\n    target = _ensure_has_len(target)\n    if ((not isinstance(target, Index)) and (len(target) == 0)):\n        attrs = self._get_attributes_dict()\n        attrs.pop('freq', None)\n        target = self._simple_new(None, dtype=self.dtype, **attrs)\n    else:\n        target = _ensure_index(target)\n    if (level is not None):\n        if (method is not None):\n            raise TypeError('Fill method not supported if level passed')\n        (_, indexer, _) = self._join_level(target, level, how='right', return_indexers=True)\n    elif self.equals(target):\n        indexer = None\n    elif self.is_unique:\n        indexer = self.get_indexer(target, method=method, limit=limit, tolerance=tolerance)\n    else:\n        if ((method is not None) or (limit is not None)):\n            raise ValueError('cannot reindex a non-unique index with a method or limit')\n        (indexer, missing) = self.get_indexer_non_unique(target)\n    if (preserve_names and (target.nlevels == 1) and (target.name != self.name)):\n        target = target.copy()\n        target.name = self.name\n    return (target, indexer)\n", "label": 1}
{"function": "\n\ndef _introspect_inline_admin(self, inline):\n    'Introspects the given inline admin, returning a tuple of (inline_model, follow_field).'\n    inline_model = None\n    follow_field = None\n    fk_name = None\n    if issubclass(inline, GenericInlineModelAdmin):\n        inline_model = inline.model\n        ct_field = inline.ct_field\n        fk_name = inline.ct_fk_field\n        for field in self.model._meta.virtual_fields:\n            if (isinstance(field, GenericRelation) and (remote_model(field) == inline_model) and (field.object_id_field_name == fk_name) and (field.content_type_field_name == ct_field)):\n                follow_field = field.name\n                break\n    elif issubclass(inline, options.InlineModelAdmin):\n        inline_model = inline.model\n        fk_name = inline.fk_name\n        if (not fk_name):\n            for field in inline_model._meta.fields:\n                if (isinstance(field, (models.ForeignKey, models.OneToOneField)) and issubclass(self.model, remote_model(field))):\n                    fk_name = field.name\n                    break\n        if (fk_name and (not remote_field(inline_model._meta.get_field(fk_name)).is_hidden())):\n            field = inline_model._meta.get_field(fk_name)\n            accessor = remote_field(field).get_accessor_name()\n            follow_field = accessor\n    return (inline_model, follow_field, fk_name)\n", "label": 1}
{"function": "\n\ndef _build_rate_limits(self, rate_limits):\n    limits = []\n    for rate_limit in rate_limits:\n        _rate_limit_key = None\n        _rate_limit = self._build_rate_limit(rate_limit)\n        for limit in limits:\n            if ((limit['uri'] == rate_limit['URI']) and (limit['regex'] == rate_limit['regex'])):\n                _rate_limit_key = limit\n                break\n        if (not _rate_limit_key):\n            _rate_limit_key = {\n                'uri': rate_limit['URI'],\n                'regex': rate_limit['regex'],\n                'limit': [],\n            }\n            limits.append(_rate_limit_key)\n        _rate_limit_key['limit'].append(_rate_limit)\n    return limits\n", "label": 0}
{"function": "\n\ndef _simplify_permissions(self, queryset):\n    '\\n        Exclude some permissions from widget.\\n\\n        Permissions to exclude are defined in PERMISSIONS_EXCLUDE.\\n        '\n    queryset = queryset.exclude(content_type__app_label__in=[ct for ct in PERMISSIONS_EXCLUDE if (not isinstance(ct, (tuple, list)))])\n    for value in PERMISSIONS_EXCLUDE:\n        if isinstance(value, (tuple, list)):\n            (app_label, model_name) = value\n            queryset = queryset.exclude(content_type__app_label=app_label, content_type__model=model_name)\n    queryset = queryset.select_related('content_type').order_by('content_type__model')\n    return queryset\n", "label": 0}
{"function": "\n\ndef xml_to_dict(node):\n    'Recursively convert minidom XML node to dictionary'\n    node_data = {\n        \n    }\n    if (node.nodeType == node.TEXT_NODE):\n        node_data = node.data\n    elif (node.nodeType not in (node.DOCUMENT_NODE, node.DOCUMENT_TYPE_NODE)):\n        node_data.update(node.attributes.items())\n    if (node.nodeType not in (node.TEXT_NODE, node.DOCUMENT_TYPE_NODE)):\n        for child in node.childNodes:\n            (child_name, child_data) = xml_to_dict(child)\n            if (not child_data):\n                child_data = ''\n            if (child_name not in node_data):\n                node_data[child_name] = child_data\n            else:\n                if (not isinstance(node_data[child_name], list)):\n                    node_data[child_name] = [node_data[child_name]]\n                node_data[child_name].append(child_data)\n        if (node_data.keys() == ['#text']):\n            node_data = node_data['#text']\n    if (node.nodeType == node.DOCUMENT_NODE):\n        return node_data\n    else:\n        return (node.nodeName, node_data)\n", "label": 1}
{"function": "\n\ndef stem(word, cached=True, history=10000, **kwargs):\n    ' Returns the stem of the given word: ponies => poni.\\n        Note: it is often taken to be a crude error \\n        that a stemming algorithm does not leave a real word after removing the stem. \\n        But the purpose of stemming is to bring variant forms of a word together, \\n        not to map a word onto its \"paradigm\" form. \\n    '\n    stem = word.lower()\n    if (cached and (stem in cache)):\n        return case_sensitive(cache[stem], word)\n    if (cached and (len(cache) > history)):\n        cache.clear()\n    if (len(stem) <= 2):\n        return case_sensitive(stem, word)\n    if (stem in exceptions):\n        return case_sensitive(exceptions[stem], word)\n    if (stem in uninflected):\n        return case_sensitive(stem, word)\n    stem = upper_consonant_y(stem)\n    for f in (step_1a, step_1b, step_1c, step_2, step_3, step_4, step_5a, step_5b):\n        stem = f(stem)\n    stem = stem.lower()\n    stem = case_sensitive(stem, word)\n    if cached:\n        cache[word.lower()] = stem.lower()\n    return stem\n", "label": 1}
{"function": "\n\ndef files_changed(files):\n    global _mtimes, _win\n    for filename in files:\n        if (filename.endswith('.pyc') or filename.endswith('.pyo')):\n            filename = filename[:(- 1)]\n        if (not os.path.exists(filename)):\n            continue\n        stat = os.stat(filename)\n        mtime = stat.st_mtime\n        if _win:\n            mtime -= stat.st_ctime\n        if (filename not in _mtimes):\n            _mtimes[filename] = mtime\n            continue\n        if (mtime != _mtimes[filename]):\n            _mtimes = {\n                \n            }\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef __call__(self, connection):\n    \"Check a TLSConnection.\\n\\n        When a Checker is passed to a handshake function, this will\\n        be called at the end of the function.\\n\\n        @type connection: L{tlslite.tlsconnection.TLSConnection}\\n        @param connection: The TLSConnection to examine.\\n\\n        @raise tlslite.errors.TLSAuthenticationError: If the other\\n        party's certificate chain is missing or bad.\\n        \"\n    if ((not self.checkResumedSession) and connection.resumed):\n        return\n    if self.x509Fingerprint:\n        if connection._client:\n            chain = connection.session.serverCertChain\n        else:\n            chain = connection.session.clientCertChain\n        if self.x509Fingerprint:\n            if isinstance(chain, X509CertChain):\n                if self.x509Fingerprint:\n                    if (chain.getFingerprint() != self.x509Fingerprint):\n                        raise TLSFingerprintError(('X.509 fingerprint mismatch: %s, %s' % (chain.getFingerprint(), self.x509Fingerprint)))\n            elif chain:\n                raise TLSAuthenticationTypeError()\n            else:\n                raise TLSNoAuthenticationError()\n", "label": 1}
{"function": "\n\ndef is_superset(self, child):\n    for child_right in child.rights:\n        allowed = False\n        for my_right in self.rights:\n            if my_right.is_superset(child_right):\n                allowed = True\n                break\n        if (not allowed):\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef get_operator(self):\n    (token, negate) = self.get_token('Reached end of statement, still expecting an operator.')\n    if ((not isinstance(token, basestring)) or (token not in OPERATORS)):\n        raise self.error_class(('%s is not a valid operator.' % token))\n    if self.at_end():\n        raise self.error_class(('No variable provided after \"%s\".' % token))\n    (op, true) = OPERATORS[token]\n    if (not true):\n        negate = (not negate)\n    return (op, negate)\n", "label": 0}
{"function": "\n\ndef LabelValueTable(self, label_list=None):\n    'Returns whole table as rows of name/value pairs.\\n\\n    One (or more) column entries are used for the row prefix label.\\n    The remaining columns are each displayed as a row entry with the\\n    prefix labels appended.\\n\\n    Use the first column as the label if label_list is None.\\n\\n    Args:\\n      label_list: A list of prefix labels to use.\\n\\n    Returns:\\n      Label/Value formatted table.\\n\\n    Raises:\\n      TableError: If specified label is not a column header of the table.\\n    '\n    label_list = (label_list or self._Header()[0])\n    for label in label_list:\n        if (label not in self._Header()):\n            raise TableError(('Invalid label prefix: %s.' % label))\n    sorted_list = []\n    for header in self._Header():\n        if (header in label_list):\n            sorted_list.append(header)\n    label_str = ('# LABEL %s\\n' % '.'.join(sorted_list))\n    body = []\n    for row in self:\n        label_prefix = []\n        value_list = []\n        for (key, value) in row.items():\n            if (key in sorted_list):\n                label_prefix.append(value)\n            else:\n                value_list.append(('%s %s' % (key, value)))\n        body.append(''.join([('%s.%s\\n' % ('.'.join(label_prefix), v)) for v in value_list]))\n    return ('%s%s' % (label_str, ''.join(body)))\n", "label": 1}
{"function": "\n\ndef get_system_users():\n    retlist = []\n    rawlist = _psutil_osx.get_system_users()\n    for item in rawlist:\n        (user, tty, hostname, tstamp) = item\n        if (tty == '~'):\n            continue\n        if (not tstamp):\n            continue\n        nt = nt_user(user, (tty or None), (hostname or None), tstamp)\n        retlist.append(nt)\n    return retlist\n", "label": 0}
{"function": "\n\n@mock_ec2\ndef test_eip_describe():\n    'Listing of allocated Elastic IP Addresses.'\n    conn = boto.connect_ec2('the_key', 'the_secret')\n    eips = []\n    number_of_classic_ips = 2\n    number_of_vpc_ips = 2\n    for _ in range(number_of_classic_ips):\n        eips.append(conn.allocate_address())\n    for _ in range(number_of_vpc_ips):\n        eips.append(conn.allocate_address(domain='vpc'))\n    len(eips).should.be.equal((number_of_classic_ips + number_of_vpc_ips))\n    for eip in eips:\n        if eip.allocation_id:\n            lookup_addresses = conn.get_all_addresses(allocation_ids=[eip.allocation_id])\n        else:\n            lookup_addresses = conn.get_all_addresses(addresses=[eip.public_ip])\n        len(lookup_addresses).should.be.equal(1)\n        lookup_addresses[0].public_ip.should.be.equal(eip.public_ip)\n    lookup_addresses = conn.get_all_addresses(addresses=[eips[0].public_ip, eips[1].public_ip])\n    len(lookup_addresses).should.be.equal(2)\n    lookup_addresses[0].public_ip.should.be.equal(eips[0].public_ip)\n    lookup_addresses[1].public_ip.should.be.equal(eips[1].public_ip)\n    for eip in eips:\n        eip.release()\n    len(conn.get_all_addresses()).should.be.equal(0)\n", "label": 0}
{"function": "\n\n@display_hook\ndef element_png_display(element, max_frames, max_branches):\n    '\\n    Used to render elements to PNG if requested in the display formats.\\n    '\n    if ('png' not in Store.display_formats):\n        return None\n    info = process_object(element)\n    if info:\n        return info\n    backend = Store.current_backend\n    if (type(element) not in Store.registry[backend]):\n        return None\n    renderer = Store.renderers[backend]\n    if ('png' not in renderer.params('fig').objects):\n        return None\n    (data, info) = renderer(element, fmt='png')\n    return data\n", "label": 0}
{"function": "\n\ndef _parse_response(self, file, sock, encoding):\n    (p, u) = self.getparser(encoding)\n    while 1:\n        if sock:\n            response = sock.recv(1024)\n        else:\n            response = file.read(1024)\n        if (not response):\n            break\n        if self.verbose:\n            print('body:', repr(response))\n        p.feed(response)\n    file.close()\n    p.close()\n    return u.close()\n", "label": 0}
{"function": "\n\ndef do(args):\n    doc_worktree = qidoc.parsers.get_doc_worktree(args)\n    doc_project = qidoc.parsers.get_one_doc_project(doc_worktree, args)\n    if doc_project.translated:\n        doc_project.html_dir = os.path.join(doc_project.html_dir, args.language)\n    index_html = doc_project.index_html\n    if (not os.path.exists(doc_project.index_html)):\n        mess = ' The doc project in {path} does no appear to have been built yet.\\n({index_html} does not exist.\\nTry running  `qidoc build`\\n'\n        mess = mess.format(path=doc_project.path, index_html=index_html)\n        ui.error(mess)\n        sys.exit(1)\n    if (sys.platform == 'darwin'):\n        index_html = ('file://' + index_html)\n    if args.browser:\n        cmd = [args.browser, index_html]\n        qisys.command.call(cmd)\n    else:\n        webbrowser.open(index_html)\n", "label": 0}
{"function": "\n\ndef append_synopsis(self, paragraphs):\n    if ((len(self.lines) == 1) and self.lines[0].startswith('=') and paragraphs and hasattr(paragraphs[(- 1)], 'set_synopsis')):\n        paragraphs[(- 1)].set_synopsis(self.lines[0][1:].lstrip())\n        return True\n    else:\n        return False\n", "label": 0}
{"function": "\n\ndef __init__(self, **options):\n    self.funcnamehighlighting = get_bool_opt(options, 'funcnamehighlighting', True)\n    self.disabledmodules = get_list_opt(options, 'disabledmodules', ['unknown'])\n    self.startinline = get_bool_opt(options, 'startinline', False)\n    if ('_startinline' in options):\n        self.startinline = options.pop('_startinline')\n    self._functions = set()\n    if self.funcnamehighlighting:\n        from pygments.lexers._phpbuiltins import MODULES\n        for (key, value) in MODULES.iteritems():\n            if (key not in self.disabledmodules):\n                self._functions.update(value)\n    RegexLexer.__init__(self, **options)\n", "label": 0}
{"function": "\n\ndef waitforkeypress(fps=None):\n    if (fps is not None):\n        clock = pygame.time.Clock()\n    while True:\n        for event in pygame.event.get():\n            if (event.type == KEYDOWN):\n                continue\n            elif (event.type == QUIT):\n                pygame.quit()\n                sys.exit()\n            elif (event.type == KEYUP):\n                return interpretkeyevent(event)\n        pygame.display.update()\n        if (fps is not None):\n            clock.tick(fps)\n", "label": 0}
{"function": "\n\ndef load_config(domain):\n    '\\n    Parses and loads up the Galah configuration file and extracts the\\n    configuration data for the given domain (ex: \"web\" or \"sheep\").\\n\\n    Global configuration options will always be returned, no matter the domain.\\n\\n    The configuration file will only be loaded once per instance of the\\n    interpreter, so feel free to call this function multiple times in many files\\n    to avoid ugly dependency issues.\\n\\n    '\n    global loaded\n    first_load = (loaded is None)\n    config_file = os.environ.get('GALAH_CONFIG_PATH', '/etc/galah/galah.config')\n    if (not loaded):\n        if os.path.isfile(config_file):\n            loaded = imp.load_source('user_config_file', config_file)\n    local_config = {\n        \n    }\n    user_config = dict(defaults)\n    if loaded:\n        user_config.update(loaded.config)\n    prefix = ('%s/' % domain)\n    global_prefix = 'global/'\n    for (k, v) in user_config.items():\n        if (k.startswith(prefix) and (len(k) != len(prefix))):\n            local_key = k[len(prefix):]\n            local_config[local_key] = v\n        elif (k.startswith(global_prefix) and (len(k) != len(global_prefix))):\n            global_key = k[len(global_prefix):]\n            local_config[global_key] = v\n        elif ('/' in k):\n            local_config[k] = v\n    return local_config\n", "label": 1}
{"function": "\n\ndef ask_gerrit_username(server, ssh_port=29418):\n    ' Run a wizard to try to configure gerrit access\\n\\n    If that fails, ask the user for its username\\n    If that fails, give up and suggest upload the public key\\n\\n    '\n    ui.info(ui.green, 'Configuring gerrit ssh access ...')\n    username = qisys.sh.username()\n    if (not username):\n        username = qisys.interact.ask_string('Please enter your username')\n        if (not username):\n            return\n    ui.info(('Checking gerrit connection with %s@%s:%i' % (username, server, ssh_port)))\n    if check_gerrit_connection(username, server, ssh_port=ssh_port):\n        ui.info('Success')\n        return username\n    ui.warning('Could not connect to ssh using username', username)\n    try_other = qisys.interact.ask_yes_no('Do you want to try with another username?')\n    if (not try_other):\n        return\n    username = qisys.interact.ask_string('Please enter your username')\n    if (not username):\n        return\n    if check_gerrit_connection(username, server, ssh_port=ssh_port):\n        return username\n", "label": 0}
{"function": "\n\ndef collections(action):\n    if (not hasattr(request, 'auth_collections')):\n        request.auth_collections = {\n            READ: set(),\n            WRITE: set(),\n        }\n        if is_admin():\n            q = Collection.all_ids().filter((Collection.deleted_at == None))\n            for (col_id,) in q:\n                request.auth_collections[READ].add(col_id)\n                request.auth_collections[WRITE].add(col_id)\n        else:\n            q = Permission.all()\n            q = q.filter(Permission.role_id.in_(request.auth_roles))\n            q = q.filter((Permission.resource_type == Permission.COLLECTION))\n            for perm in q:\n                if perm.read:\n                    request.auth_collections[READ].add(perm.resource_id)\n                if (perm.write and request.logged_in):\n                    request.auth_collections[WRITE].add(perm.resource_id)\n    return list(request.auth_collections.get(action, []))\n", "label": 0}
{"function": "\n\ndef get_items(self, block):\n    ' Break a block into list items. '\n    items = []\n    for line in block.split('\\n'):\n        m = self.CHILD_RE.match(line)\n        if m:\n            if ((not items) and (self.TAG == 'ol')):\n                INTEGER_RE = re.compile('(\\\\d+)')\n                self.STARTSWITH = INTEGER_RE.match(m.group(1)).group()\n            items.append(m.group(3))\n        elif self.INDENT_RE.match(line):\n            if items[(- 1)].startswith((' ' * self.tab_length)):\n                items[(- 1)] = ('%s\\n%s' % (items[(- 1)], line))\n            else:\n                items.append(line)\n        else:\n            items[(- 1)] = ('%s\\n%s' % (items[(- 1)], line))\n    return items\n", "label": 0}
{"function": "\n\ndef process_errors(self, errors):\n    lp = [re.compile('.*?0:(\\\\d+): (.*)'), re.compile('0\\\\((\\\\d+)\\\\) :[^:]*: (.*)')]\n    linesOut = []\n    for line in errors.split('\\n'):\n        result = None\n        for p in lp:\n            result = p.match(line)\n            if result:\n                linesOut.append(('%s(%d): error: %s' % (self._filename, (int(result.group(1)) - preamble_lines), result.group(2))))\n                break\n        if (not result):\n            linesOut.append(line)\n    return '\\n'.join(linesOut)\n", "label": 0}
{"function": "\n\ndef dot(x, y):\n    '\\n    Operation for efficiently calculating the dot product when\\n    one or all operands is sparse. Supported format are CSC and CSR.\\n    The output of the operation is dense.\\n\\n    Parameters\\n    ----------\\n    x\\n        Sparse or dense matrix variable.\\n    y\\n        Sparse or dense matrix variable.\\n\\n    Returns\\n    -------\\n    The dot product `x`.`y` in a dense format.\\n\\n    Notes\\n    -----\\n    The grad implemented is regular, i.e. not structured.\\n\\n    At least one of `x` or `y` must be a sparse matrix.\\n\\n    When the operation has the form dot(csr_matrix, dense)\\n    the gradient of this operation can be performed inplace\\n    by UsmmCscDense. This leads to significant speed-ups.\\n\\n    '\n    if hasattr(x, 'getnnz'):\n        x = as_sparse_variable(x)\n    if hasattr(y, 'getnnz'):\n        y = as_sparse_variable(y)\n    x_is_sparse_variable = _is_sparse_variable(x)\n    y_is_sparse_variable = _is_sparse_variable(y)\n    if ((not x_is_sparse_variable) and (not y_is_sparse_variable)):\n        raise TypeError()\n    return _dot(x, y)\n", "label": 0}
{"function": "\n\ndef write_error(self, status_code, exc_info=None, error_text=None):\n    '\\n        Render a custom error page. This is invoked if a handler throws\\n        an exception. We also call it manually, in some places.\\n        '\n    if (status_code == 404):\n        self.render('404.html')\n        return\n    if (status_code == 403):\n        if (not error_text):\n            error_text = 'Not permitted'\n        if exc_info:\n            error_text = str(exc_info[1])\n        self.render('error.html', status_code=403, exctitle=None, exception=error_text)\n        return\n    exception = ''\n    exctitle = None\n    if error_text:\n        exception = error_text\n    if exc_info:\n        exctitle = str(exc_info[1])\n        ls = [ln for ln in traceback.format_exception(*exc_info)]\n        if exception:\n            exception = (exception + '\\n')\n        exception = (exception + ''.join(ls))\n    self.render('error.html', status_code=status_code, exctitle=exctitle, exception=exception)\n", "label": 1}
{"function": "\n\ndef related_models(self, backrefs=False):\n    models = []\n    stack = [self.model_class]\n    while stack:\n        model = stack.pop()\n        if (model in models):\n            continue\n        models.append(model)\n        for fk in model._meta.rel.values():\n            stack.append(fk.rel_model)\n        if backrefs:\n            for fk in model._meta.reverse_rel.values():\n                stack.append(fk.model_class)\n    return models\n", "label": 0}
{"function": "\n\ndef send_frame(self, frame):\n    \"\\n        Send a single frame. If there is no transport or we're not connected\\n        yet, append to the output buffer, else send immediately to the socket.\\n        This is called from within the MethodFrames.\\n        \"\n    if self._closed:\n        if (self._close_info and (len(self._close_info['reply_text']) > 0)):\n            raise ConnectionClosed(('connection is closed: %s : %s' % (self._close_info['reply_code'], self._close_info['reply_text'])))\n        raise ConnectionClosed('connection is closed')\n    if ((self._transport is None) or ((not self._connected) and (frame.channel_id != 0))):\n        self._output_frame_buffer.append(frame)\n        return\n    if (self._debug > 1):\n        self.logger.debug('WRITE: %s', frame)\n    buf = bytearray()\n    frame.write_frame(buf)\n    if (len(buf) > self._frame_max):\n        self.close(reply_code=501, reply_text=('attempted to send frame of %d bytes, frame max %d' % (len(buf), self._frame_max)), class_id=0, method_id=0, disconnect=True)\n        raise ConnectionClosed(('connection is closed: %s : %s' % (self._close_info['reply_code'], self._close_info['reply_text'])))\n    self._transport.write(buf)\n    self._frames_written += 1\n", "label": 1}
{"function": "\n\ndef _make_criterion_from_lists(includes, excludes):\n    '\\n    Give a list of items to include and exclude,\\n    construct the corresponding TargetingCriterion\\n\\n    @param includes: list\\n    @param excludes: list\\n    @return: TargetingCriterion|None\\n    '\n    incl = TargetingCriterion(includes, TargetingCriterion.OPERATOR.OR)\n    excl = TargetingCriterion(excludes, TargetingCriterion.OPERATOR.OR)\n    if (includes and excludes):\n        return (incl & (~ excl))\n    elif includes:\n        return incl\n    elif excludes:\n        return (~ excl)\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef _policy_get(request, policy_id, expand_rule):\n    policy = neutronclient(request).show_firewall_policy(policy_id).get('firewall_policy')\n    if expand_rule:\n        policy_rules = policy['firewall_rules']\n        if policy_rules:\n            rules = _rule_list(request, expand_policy=False, firewall_policy_id=policy_id)\n            rule_dict = OrderedDict(((rule.id, rule) for rule in rules))\n            policy['rules'] = [rule_dict.get(rule) for rule in policy_rules]\n        else:\n            policy['rules'] = []\n    return Policy(policy)\n", "label": 0}
{"function": "\n\ndef test_security_group_rule_create(self):\n    sg_rule = [r for r in self.api_q_secgroup_rules.list() if ((r['protocol'] == 'tcp') and r['remote_ip_prefix'])][0]\n    sg_id = sg_rule['security_group_id']\n    secgroup = [sg for sg in self.api_q_secgroups.list() if (sg['id'] == sg_id)][0]\n    post_rule = copy.deepcopy(sg_rule)\n    del post_rule['id']\n    del post_rule['tenant_id']\n    post_body = {\n        'security_group_rule': post_rule,\n    }\n    self.qclient.create_security_group_rule(post_body).AndReturn({\n        'security_group_rule': copy.deepcopy(sg_rule),\n    })\n    self.qclient.list_security_groups(id=set([sg_id]), fields=['id', 'name']).AndReturn({\n        'security_groups': [copy.deepcopy(secgroup)],\n    })\n    self.mox.ReplayAll()\n    ret = api.network.security_group_rule_create(self.request, sg_rule['security_group_id'], sg_rule['direction'], sg_rule['ethertype'], sg_rule['protocol'], sg_rule['port_range_min'], sg_rule['port_range_max'], sg_rule['remote_ip_prefix'], sg_rule['remote_group_id'])\n    self._cmp_sg_rule(sg_rule, ret)\n", "label": 0}
{"function": "\n\ndef load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):\n    'Load translations from the given directory.\\n\\n        :param dirname: the directory containing the ``MO`` files\\n        :param locales: the list of locales in order of preference (items in\\n                        this list can be either `Locale` objects or locale\\n                        strings)\\n        :param domain: the message domain\\n        :return: the loaded catalog, or a ``NullTranslations`` instance if no\\n                 matching translations were found\\n        :rtype: `Translations`\\n        '\n    if (locales is not None):\n        if (not isinstance(locales, (list, tuple))):\n            locales = [locales]\n        locales = [str(locale) for locale in locales]\n    if (not domain):\n        domain = cls.DEFAULT_DOMAIN\n    filename = gettext.find(domain, dirname, locales)\n    if (not filename):\n        return gettext.NullTranslations()\n    return cls(fileobj=open(filename, 'rb'), domain=domain)\n", "label": 0}
{"function": "\n\ndef analyze_manifest(self):\n    self.manifest_files = mf = {\n        \n    }\n    if (not self.distribution.include_package_data):\n        return\n    src_dirs = {\n        \n    }\n    for package in (self.packages or ()):\n        src_dirs[assert_relative(self.get_package_dir(package))] = package\n    self.run_command('egg_info')\n    ei_cmd = self.get_finalized_command('egg_info')\n    for path in ei_cmd.filelist.files:\n        (d, f) = os.path.split(assert_relative(path))\n        prev = None\n        oldf = f\n        while (d and (d != prev) and (d not in src_dirs)):\n            prev = d\n            (d, df) = os.path.split(d)\n            f = os.path.join(df, f)\n        if (d in src_dirs):\n            if (path.endswith('.py') and (f == oldf)):\n                continue\n            mf.setdefault(src_dirs[d], []).append(path)\n", "label": 1}
{"function": "\n\ndef export(self, lwrite, level, namespace_='NetworkConnectionObj:', name_='Layer7ConnectionsType', namespacedef_='', pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    showIndent(lwrite, level, pretty_print)\n    lwrite(('<%s%s%s' % (namespace_, name_, ((namespacedef_ and (' ' + namespacedef_)) or ''))))\n    already_processed = set()\n    self.exportAttributes(lwrite, level, already_processed, namespace_, name_='Layer7ConnectionsType')\n    if self.hasContent_():\n        lwrite(('>%s' % (eol_,)))\n        self.exportChildren(lwrite, (level + 1), namespace_, name_, pretty_print=pretty_print)\n        showIndent(lwrite, level, pretty_print)\n        lwrite(('</%s%s>%s' % (namespace_, name_, eol_)))\n    else:\n        lwrite(('/>%s' % (eol_,)))\n", "label": 0}
{"function": "\n\ndef eval(self):\n    '\\n        Return the Labeled Attachment Score (LAS) and Unlabeled Attachment Score (UAS)\\n\\n        :return : tuple(float,float)\\n        '\n    if (len(self._parsed_sents) != len(self._gold_sents)):\n        raise ValueError(' Number of parsed sentence is different with number of gold sentence.')\n    corr = 0\n    corrL = 0\n    total = 0\n    for i in range(len(self._parsed_sents)):\n        parsed_sent_nodes = self._parsed_sents[i].nodes\n        gold_sent_nodes = self._gold_sents[i].nodes\n        if (len(parsed_sent_nodes) != len(gold_sent_nodes)):\n            raise ValueError('Sentences must have equal length.')\n        for (parsed_node_address, parsed_node) in parsed_sent_nodes.items():\n            gold_node = gold_sent_nodes[parsed_node_address]\n            if (parsed_node['word'] is None):\n                continue\n            if (parsed_node['word'] != gold_node['word']):\n                raise ValueError('Sentence sequence is not matched.')\n            if (self._remove_punct(parsed_node['word']) == ''):\n                continue\n            total += 1\n            if (parsed_node['head'] == gold_node['head']):\n                corr += 1\n                if (parsed_node['rel'] == gold_node['rel']):\n                    corrL += 1\n    return ((corr / total), (corrL / total))\n", "label": 1}
{"function": "\n\ndef click_block(self, pos, look_at_block=True, swing=True, **kwargs):\n    '\\n        Click on a block.\\n        Examples: push button, open window, make redstone ore glow\\n\\n        Args:\\n            face (int): side of the block on which the block is placed on\\n            cursor_pos (Vector3): where to click inside the block,\\n                each dimension 0-15\\n        '\n    pos = Vector3(pos)\n    if (look_at_block and self.auto_look):\n        self.look_at((pos.floor() + Vector3(0.5, 0.5, 0.5)))\n    self._send_click_block(pos, **kwargs)\n    if (swing and self.auto_swing):\n        self.swing_arm()\n", "label": 0}
{"function": "\n\ndef pip(self, *args, **kwargs):\n    if ((pyversion_tuple < (2, 7, 9)) and args and (args[0] in ('search', 'install', 'download'))):\n        kwargs['expect_stderr'] = True\n    if (pyversion_tuple[:2] == (2, 6)):\n        kwargs['expect_stderr'] = True\n    return self.run('pip', *args, **kwargs)\n", "label": 0}
{"function": "\n\ndef close(self):\n    if self._closed:\n        return\n    snames = [n for n in dir(self) if n.endswith('socket')]\n    for socket in [getattr(self, name) for name in snames]:\n        if (isinstance(socket, zmq.Socket) and (not socket.closed)):\n            socket.close()\n    self._closed = True\n", "label": 0}
{"function": "\n\ndef _maybe_append_formatted_extension(numobj, metadata, num_format, number):\n    'Appends the formatted extension of a phone number to formatted number,\\n    if the phone number had an extension specified.\\n    '\n    if ((numobj.extension is not None) and (len(numobj.extension) > 0)):\n        if (num_format == PhoneNumberFormat.RFC3966):\n            return ((number + _RFC3966_EXTN_PREFIX) + numobj.extension)\n        elif (metadata.preferred_extn_prefix is not None):\n            return ((number + metadata.preferred_extn_prefix) + numobj.extension)\n        else:\n            return ((number + _DEFAULT_EXTN_PREFIX) + numobj.extension)\n    return number\n", "label": 0}
{"function": "\n\ndef scan(top, verbose=False):\n    packages = []\n    for (root, subdirs, files) in os.walk(top, followlinks=True):\n        if ('source.properties' in files):\n            del subdirs[:]\n            package = parse(top, root)\n            if package:\n                packages.append(package)\n                if verbose:\n                    print('Found package \"{:s}\" in {:s}'.format(str(package), root))\n    return packages\n", "label": 0}
{"function": "\n\ndef open_file_at_symbol(view, file_path, symbol):\n    index_locations = view.window().lookup_symbol_in_index(symbol)\n    if (file_path[1] == ':'):\n        file_path = (('/' + file_path[0]) + file_path[2:])\n    for (full_path, project_path, rowcol) in index_locations:\n        if (full_path == file_path):\n            (row, col) = rowcol\n            view.window().open_file(((((full_path + ':') + str(row)) + ':') + str(col)), (sublime.ENCODED_POSITION | sublime.FORCE_GROUP))\n            break\n    else:\n        view.window().open_file(file_path)\n", "label": 0}
{"function": "\n\ndef _setup_joins(self, pieces, opts, alias):\n    '\\n        A helper method for get_ordering and get_distinct. This method will\\n        call query.setup_joins, handle refcounts and then promote the joins.\\n\\n        Note that get_ordering and get_distinct must produce same target\\n        columns on same input, as the prefixes of get_ordering and get_distinct\\n        must match. Executing SQL where this is not true is an error.\\n        '\n    if (not alias):\n        alias = self.query.get_initial_alias()\n    (field, targets, opts, joins, _) = self.query.setup_joins(pieces, opts, alias)\n    joins_to_promote = [j for j in joins if (self.query.alias_refcount[j] < 2)]\n    alias = joins[(- 1)]\n    cols = [target.column for target in targets]\n    if (not field.rel):\n        self.query.ref_alias(alias)\n    self.query.promote_joins(joins_to_promote)\n    return (field, cols, alias, joins, opts)\n", "label": 0}
{"function": "\n\ndef _verify_fields(self, this, that, fields):\n    for field in fields:\n        if ((field not in this) and (field not in that)):\n            continue\n        if (this[field] != that[field]):\n            raise UsageException('U2', (\"Conflicting '%s' values ('%s' != '%s')\" % (field, this[field], that[field])))\n", "label": 0}
{"function": "\n\ndef _build_targets(self, targets):\n    \"Turn valid target IDs or 'all' into two lists:\\n        (int_ids, uuids).\\n        \"\n    if (not self._ids):\n        if (not self.ids):\n            raise error.NoEnginesRegistered(\"Can't build targets without any engines\")\n    if (targets is None):\n        targets = self._ids\n    elif isinstance(targets, str):\n        if (targets.lower() == 'all'):\n            targets = self._ids\n        else:\n            raise TypeError((\"%r not valid str target, must be 'all'\" % targets))\n    elif isinstance(targets, int):\n        if (targets < 0):\n            targets = self.ids[targets]\n        if (targets not in self._ids):\n            raise IndexError(('No such engine: %i' % targets))\n        targets = [targets]\n    if isinstance(targets, slice):\n        indices = list(range(len(self._ids)))[targets]\n        ids = self.ids\n        targets = [ids[i] for i in indices]\n    if (not isinstance(targets, (tuple, list, xrange))):\n        raise TypeError(('targets by int/slice/collection of ints only, not %s' % type(targets)))\n    return ([util.asbytes(self._engines[t]) for t in targets], list(targets))\n", "label": 1}
{"function": "\n\n@app.route('/success', methods=['GET'])\ndef handleLoginSuccess():\n    app.logger.info(('handleLoginSuccess [%s]' % request.method))\n    me = request.args.get('me')\n    code = request.args.get('code')\n    app.logger.info(('me [%s] code [%s]' % (me, code)))\n    if (db is not None):\n        app.logger.info('getting data to validate auth code')\n        key = ('login-%s' % me)\n        data = db.hgetall(key)\n        if data:\n            r = ninka.indieauth.validateAuthCode(code=code, client_id=me, redirect_uri=data['redirect_uri'])\n            if (r['status'] == requests.codes.ok):\n                app.logger.info('login code verified')\n                scope = r['response']['scope']\n                from_uri = data['from_uri']\n                token = str(uuid.uuid4())\n                db.hset(key, 'code', code)\n                db.hset(key, 'token', token)\n                db.expire(key, cfg['auth_timeout'])\n                db.set(('token-%s' % token), key)\n                db.expire(('token-%s' % code), cfg['auth_timeout'])\n                session['indieauth_token'] = token\n                session['indieauth_scope'] = scope\n                session['indieauth_id'] = me\n            else:\n                app.logger.info('login invalid')\n                clearAuth()\n        else:\n            app.logger.info(('nothing found for [%s]' % me))\n    if scope:\n        if from_uri:\n            return redirect(from_uri)\n        else:\n            return redirect('/')\n    else:\n        return ('authentication failed', 403)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(AddL3PolicyForm, self).clean()\n    if self.is_valid():\n        ipversion = int(cleaned_data['ip_version'])\n        subnet_prefix_length = int(cleaned_data['subnet_prefix_length'])\n        msg = _('Subnet prefix out of range.')\n        if ((ipversion == 4) and (subnet_prefix_length not in range(2, 31))):\n            raise forms.ValidationError(msg)\n        if ((ipversion == 6) and (subnet_prefix_length not in range(2, 128))):\n            raise forms.ValidationError(msg)\n    return cleaned_data\n", "label": 0}
{"function": "\n\n@pytest.hookimpl(tryfirst=True, hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = (yield)\n    rep = outcome.get_result()\n    if ((rep.when == 'call') and rep.failed):\n        if ('browser' in item.fixturenames):\n            browser = item.funcargs['browser']\n            for log_type in (set(browser.log_types) - {'har'}):\n                data = '\\n\\n'.join(filter(None, (l.get('message') for l in browser.get_log(log_type))))\n                if data:\n                    rep.sections.append(('Captured {} log'.format(log_type), data))\n", "label": 0}
{"function": "\n\n@require_instance_manager\ndef similar_objects(self):\n    lookup_kwargs = self._lookup_kwargs()\n    lookup_keys = sorted(lookup_kwargs)\n    qs = self.through.objects.values(*lookup_kwargs.keys())\n    qs = qs.annotate(n=models.Count('pk'))\n    qs = qs.exclude(**lookup_kwargs)\n    qs = qs.filter(tag__in=self.all())\n    qs = qs.order_by('-n')\n    items = {\n        \n    }\n    if (len(lookup_keys) == 1):\n        f = self.through._meta.get_field_by_name(lookup_keys[0])[0]\n        objs = f.rel.to._default_manager.filter(**{\n            ('%s__in' % f.rel.field_name): [r['content_object'] for r in qs],\n        })\n        for obj in objs:\n            items[(getattr(obj, f.rel.field_name),)] = obj\n    else:\n        preload = {\n            \n        }\n        for result in qs:\n            preload.setdefault(result['content_type'], set())\n            preload[result['content_type']].add(result['object_id'])\n        for (ct, obj_ids) in preload.iteritems():\n            ct = ContentType.objects.get_for_id(ct)\n            for obj in ct.model_class()._default_manager.filter(pk__in=obj_ids):\n                items[(ct.pk, obj.pk)] = obj\n    results = []\n    for result in qs:\n        obj = items[tuple((result[k] for k in lookup_keys))]\n        obj.similar_tags = result['n']\n        results.append(obj)\n    return results\n", "label": 1}
{"function": "\n\ndef _search_matching_signature(self, idtypes):\n    '\\n        Given the input types in `idtypes`, return a compatible sequence of\\n        types that is defined in `kernelmap`.\\n\\n        Note: Ordering is guaranteed by `kernelmap` being a OrderedDict\\n        '\n    for sig in self.kernelmap.keys():\n        if all((np.can_cast(actual, desired) for (actual, desired) in zip(sig, idtypes))):\n            return sig\n    else:\n        raise TypeError('no matching signature')\n", "label": 0}
{"function": "\n\ndef parse_accept_header(value, cls=None):\n    'Parses an HTTP Accept-* header.  This does not implement a complete\\n    valid algorithm but one that supports at least value and quality\\n    extraction.\\n\\n    Returns a new :class:`Accept` object (basically a list of ``(value, quality)``\\n    tuples sorted by the quality with some additional accessor methods).\\n\\n    The second parameter can be a subclass of :class:`Accept` that is created\\n    with the parsed values and returned.\\n\\n    :param value: the accept header string to be parsed.\\n    :param cls: the wrapper class for the return value (can be\\n                         :class:`Accept` or a subclass thereof)\\n    :return: an instance of `cls`.\\n    '\n    if (cls is None):\n        cls = Accept\n    if (not value):\n        return cls(None)\n    result = []\n    for match in _accept_re.finditer(value):\n        quality = match.group(2)\n        if (not quality):\n            quality = 1\n        else:\n            quality = max(min(float(quality), 1), 0)\n        result.append((match.group(1), quality))\n    return cls(result)\n", "label": 0}
{"function": "\n\ndef signup(request, template='accounts/account_signup.html', extra_context=None):\n    '\\n    Signup form.\\n    '\n    profile_form = get_profile_form()\n    form = profile_form((request.POST or None), (request.FILES or None))\n    if ((request.method == 'POST') and form.is_valid()):\n        new_user = form.save()\n        if (not new_user.is_active):\n            if settings.ACCOUNTS_APPROVAL_REQUIRED:\n                send_approve_mail(request, new_user)\n                info(request, _(\"Thanks for signing up! You'll receive an email when your account is activated.\"))\n            else:\n                send_verification_mail(request, new_user, 'signup_verify')\n                info(request, _('A verification email has been sent with a link for activating your account.'))\n            return redirect((next_url(request) or '/'))\n        else:\n            info(request, _('Successfully signed up'))\n            auth_login(request, new_user)\n            return login_redirect(request)\n    context = {\n        'form': form,\n        'title': _('Sign up'),\n    }\n    context.update((extra_context or {\n        \n    }))\n    return TemplateResponse(request, template, context)\n", "label": 1}
{"function": "\n\ndef do_GET(self):\n    self.send_response(200, 'OK')\n    acs = self.headers['Accept'].split(',')\n    acq = [x.split(';') for x in acs if (';' in x)]\n    acn = [(x, 'q=1') for x in acs if (';' not in x)]\n    acs = [(x[0], float(x[1].strip()[2:])) for x in (acq + acn)]\n    ac = sorted(acs, key=(lambda x: x[1]))\n    ct = ac[(- 1)]\n    if ('application/rdf+xml' in ct):\n        rct = 'application/rdf+xml'\n        content = xmltestdoc\n    elif ('text/n3' in ct):\n        rct = 'text/n3'\n        content = n3testdoc\n    elif ('text/plain' in ct):\n        rct = 'text/plain'\n        content = nttestdoc\n    self.send_header('Content-type', rct)\n    self.end_headers()\n    self.wfile.write(content.encode('utf-8'))\n", "label": 1}
{"function": "\n\ndef __ddl__(self, column_type):\n    'Return a list of Node instances that defines the column.'\n    ddl = [self.as_entity(), self.__ddl_column__(column_type)]\n    if (not self.null):\n        ddl.append(SQL('NOT NULL'))\n    if self.primary_key:\n        ddl.append(SQL('PRIMARY KEY'))\n    if self.sequence:\n        ddl.append(SQL((\"DEFAULT NEXTVAL('%s')\" % self.sequence)))\n    if self.constraints:\n        ddl.extend(self.constraints)\n    return ddl\n", "label": 0}
{"function": "\n\ndef placeholders(self, fill_missing=None):\n    '\\n        Generate placeholder dict.\\n        :param fill_missing: A function taking a missing placeholder name & returning a temporary fill value.\\n                May be `None` indicating not to return such replacements.\\n        :return: Dict of placeholder names to values.\\n        '\n    result = {K: self._format_placeholder(K, V) for (K, V) in self.values.items() if self._should_make_placeholder(K)}\n    print('Making result!')\n    if (fill_missing is not None):\n        for key in self.required:\n            print('Checking out {}'.format(key))\n            if (self._should_make_placeholder(key) and (not result.get(key))):\n                result[key] = self._format_placeholder(key, fill_missing(key))\n                print('Added!', key, result[key])\n    return result\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.input = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef enable(app_id, enabled=True):\n    \"\\n    Enable or disable an existing assistive access application.\\n\\n    app_id\\n        The bundle ID or command to set assistive access status.\\n\\n    enabled\\n        Sets enabled or disabled status. Default is ``True``.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' assistive.enable /usr/bin/osascript\\n        salt '*' assistive.enable com.smileonmymac.textexpander enabled=False\\n    \"\n    enable_str = ('1' if enabled else '0')\n    for a in _get_assistive_access():\n        if (app_id == a[0]):\n            cmd = 'sqlite3 \"/Library/Application Support/com.apple.TCC/TCC.db\" \"UPDATE access SET allowed=\\'{0}\\' WHERE client=\\'{1}\\'\"'.format(enable_str, app_id)\n            call = __salt__['cmd.run_all'](cmd, output_loglevel='debug', python_shell=False)\n            if (call['retcode'] != 0):\n                comment = ''\n                if ('stderr' in call):\n                    comment += call['stderr']\n                if ('stdout' in call):\n                    comment += call['stdout']\n                raise CommandExecutionError('Error enabling app: {0}'.format(comment))\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef find_or_create_ar(gplus_activity, activity_post):\n    if (gplus_activity['verb'] == 'post'):\n        original_post_id = gplus_activity['id']\n    elif (gplus_activity['verb'] == 'share'):\n        original_post_id = gplus_activity['object']['id']\n    if (original_post_id is not None):\n        records = ActivityRecord.query((ActivityRecord.gplus_posts == original_post_id)).fetch(20)\n        if (len(records) == 0):\n            return ar.create_activity_record(activity_post)\n        else:\n            return records[0]\n    return ar.create_activity_record(activity_post)\n", "label": 0}
{"function": "\n\n@discover.register(Collection)\ndef discover_pymongo_collection(coll, n=50):\n    items = list(take(n, coll.find()))\n    if (not items):\n        return (var * Record([]))\n    oid_cols = [k for (k, v) in items[0].items() if isinstance(v, ObjectId)]\n    for item in items:\n        for col in oid_cols:\n            del item[col]\n    ds = discover(items)\n    if isdimension(ds[0]):\n        return (coll.count() * ds.subshape[0])\n    else:\n        raise ValueError('Consistent datashape not found')\n", "label": 0}
{"function": "\n\ndef isExcluded(self, case):\n    c = case\n    if (not hasattr(c, '__name__')):\n        c = c.__class__\n    while True:\n        name = c.__name__\n        if (name == TestCase.__name__):\n            break\n        if (name in self.opt.exclude):\n            return True\n        c = c.__bases__[0]\n    return False\n", "label": 0}
{"function": "\n\ndef create(self, **kwargs):\n    release_data = kwargs.pop('release_kwargs', {\n        'api': False,\n    })\n    cluster_data = kwargs.pop('cluster_kwargs', {\n        \n    })\n    if ('release_id' not in cluster_data):\n        cluster_data['release_id'] = self.create_release(**release_data).id\n    cluster = self.create_cluster(**cluster_data)\n    for node_kwargs in kwargs.pop('nodes_kwargs', []):\n        if ('cluster_id' not in node_kwargs):\n            node_kwargs['cluster_id'] = cluster.id\n        node_kwargs.setdefault('api', False)\n        if ('pending_roles' not in node_kwargs):\n            node_kwargs.setdefault('roles', ['controller'])\n        self.create_node(**node_kwargs)\n    return cluster\n", "label": 0}
{"function": "\n\ndef configure(self, in_obj):\n    '\\n        Must receive a list of shapes for configuration (one for each pathway)\\n        the shapes correspond to the layer_container attribute\\n\\n        Arguments:\\n            in_obj: any object that has an out_shape (Layer) or shape (Tensor, dataset)\\n        '\n    config_layers = (self.layers if in_obj else self._layers)\n    in_obj = (in_obj if in_obj else self.layers[0])\n    super(Sequential, self).configure(in_obj)\n    prev_layer = None\n    for l in config_layers:\n        in_obj = l.configure(in_obj)\n        if (prev_layer is not None):\n            prev_layer.set_next(l)\n        prev_layer = l\n    self.parallelism = in_obj.parallelism\n    self.out_shape = in_obj.out_shape\n    return self\n", "label": 0}
{"function": "\n\ndef is_last_li(li, meta_data, current_numId):\n    '\\n    Determine if ``li`` is the last list item for a given list\\n    '\n    if (not is_li(li, meta_data)):\n        return False\n    w_namespace = get_namespace(li, 'w')\n    next_el = li\n    while True:\n        if (next_el is None):\n            return True\n        next_el = next_el.getnext()\n        if (not is_li(next_el, meta_data)):\n            continue\n        new_numId = get_numId(next_el, w_namespace)\n        if (current_numId != new_numId):\n            return True\n        return False\n", "label": 0}
{"function": "\n\ndef _handle_apply_reply(self, msg):\n    'Save the reply to an apply_request into our results.'\n    parent = msg['parent_header']\n    msg_id = parent['msg_id']\n    if (msg_id not in self.outstanding):\n        if (msg_id in self.history):\n            print(('got stale result: %s' % msg_id))\n            print(self.results[msg_id])\n            print(msg)\n        else:\n            print(('got unknown result: %s' % msg_id))\n    else:\n        self.outstanding.remove(msg_id)\n    content = msg['content']\n    header = msg['header']\n    md = self.metadata[msg_id]\n    md.update(self._extract_metadata(header, parent, content))\n    self.metadata[msg_id] = md\n    e_outstanding = self._outstanding_dict[md['engine_uuid']]\n    if (msg_id in e_outstanding):\n        e_outstanding.remove(msg_id)\n    if (content['status'] == 'ok'):\n        self.results[msg_id] = util.unserialize_object(msg['buffers'])[0]\n    elif (content['status'] == 'aborted'):\n        self.results[msg_id] = error.TaskAborted(msg_id)\n    elif (content['status'] == 'resubmitted'):\n        pass\n    else:\n        self.results[msg_id] = self._unwrap_exception(content)\n", "label": 0}
{"function": "\n\ndef get_output_for(self, input, deterministic=False, **kwargs):\n    input_mean = input.mean(self.axes)\n    input_std = TT.sqrt((input.var(self.axes) + self.epsilon))\n    use_averages = kwargs.get('batch_norm_use_averages', deterministic)\n    if use_averages:\n        mean = self.mean\n        std = self.std\n    else:\n        mean = input_mean\n        std = input_std\n    update_averages = kwargs.get('batch_norm_update_averages', (not deterministic))\n    if update_averages:\n        running_mean = theano.clone(self.mean, share_inputs=False)\n        running_std = theano.clone(self.std, share_inputs=False)\n        running_mean.default_update = (((1 - self.alpha) * running_mean) + (self.alpha * input_mean))\n        running_std.default_update = (((1 - self.alpha) * running_std) + (self.alpha * input_std))\n        mean += (0 * running_mean)\n        std += (0 * running_std)\n    param_axes = iter(range((input.ndim - len(self.axes))))\n    pattern = [('x' if (input_axis in self.axes) else next(param_axes)) for input_axis in range(input.ndim)]\n    beta = (0 if (self.beta is None) else self.beta.dimshuffle(pattern))\n    gamma = (1 if (self.gamma is None) else self.gamma.dimshuffle(pattern))\n    mean = mean.dimshuffle(pattern)\n    std = std.dimshuffle(pattern)\n    normalized = (((input - mean) * (gamma * TT.inv(std))) + beta)\n    return normalized\n", "label": 0}
{"function": "\n\ndef is_disk(dev, bspec=None, uspec=None):\n    'Checks if given device is a disk.\\n\\n    :param dev: A device file, e.g. /dev/sda.\\n    :param bspec: A dict of properties which we get from blockdev.\\n    :param uspec: A dict of properties which we get from udevadm.\\n\\n    :returns: True if device is disk else False.\\n    '\n    if (uspec is None):\n        uspec = udevreport(dev)\n    if (uspec.get('ID_CDROM') == '1'):\n        return False\n    if (uspec.get('DEVTYPE') == 'partition'):\n        return False\n    if (('MAJOR' in uspec) and (int(uspec['MAJOR']) not in VALID_MAJORS)):\n        return False\n    if (bspec is None):\n        bspec = blockdevreport(dev)\n    if (bspec.get('ro') == '1'):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef _move_ordered_dict_item(o_dict, item_key, new_position):\n    ' Move an item in an ordered dictionary to a new position\\n\\n    :param OrderedDict o_dict: The dictionary on which the move takes place\\n    :param key item_key: The key of the item to move. If the key does not\\n        exist in the dictionary, this function will do nothing.\\n    :param key new_position: The item to move will be inserted after the\\n        item with this key. If this key does not exist in the dictionary,\\n        the item will be moved to the first position.\\n    '\n    if ((not o_dict) or (not (item_key in o_dict))):\n        return\n    item = o_dict[item_key]\n    items = o_dict.items()\n    in_there = (new_position in o_dict)\n    o_dict.clear()\n    if (not in_there):\n        o_dict[item_key] = item\n    for i in items:\n        if (i[0] != item_key):\n            o_dict[i[0]] = i[1]\n        if (i[0] == new_position):\n            o_dict[item_key] = item\n", "label": 0}
{"function": "\n\ndef to_javascript(self):\n    for (k, vv) in self.keymap.items():\n        for (pos, v) in enumerate(vv.split('|')):\n            if ((len(v) >= 4) and (v == 'PIPE')):\n                v = '|'\n            print(('<input type=\"button\" id=\"btn_kw_%s_%d\" value=\"%s\" onClick=\"appendText(\\'%s\\');\" />' % (k, pos, v, v)))\n    print('/* -------------------------------- */')\n    return\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.prefix == other.prefix) and (self.mnemonic == other.mnemonic) and (self.operands == other.operands) and (self.bytes == other.bytes) and (self.size == other.size) and (self.address == other.address))\n", "label": 0}
{"function": "\n\ndef wait_for_status_change(self, parameters, conn, state_requested, max_wait_time=60, poll_interval=10):\n    ' After we have sent a signal to the cloud infrastructure to change the state\\n      of the instances (unsually from runnning to either stoppped or \\n      terminated), wait for the status to change.  If all the instances change\\n      successfully, return True, if not return False.\\n\\n    Args:\\n      parameters: A dictionary of parameters.\\n      conn: A connection object returned from self.open_connection().\\n      state_requrested: String of the requested final state of the instances.\\n      max_wait_time: int of maximum amount of time (in seconds)  to wait for the\\n        state change.\\n      poll_interval: int of the number of seconds to wait between checking of\\n        the state.\\n    '\n    time_start = time.time()\n    instance_ids = parameters[self.PARAM_INSTANCE_IDS]\n    instances_in_state = {\n        \n    }\n    while True:\n        time.sleep(poll_interval)\n        reservations = conn.get_all_instances(instance_ids)\n        instances = [i for r in reservations for i in r.instances]\n        for i in instances:\n            if ((i.state == state_requested) and (i.key_name == parameters[self.PARAM_KEYNAME])):\n                if (i.id not in instances_in_state.keys()):\n                    instances_in_state[i.id] = 1\n        if (len(instances_in_state.keys()) >= len(instance_ids)):\n            return True\n        if ((time.time() - time_start) > max_wait_time):\n            return False\n", "label": 1}
{"function": "\n\ndef _confirm_launched_at(self, block, exists):\n    if (exists.get('state') != 'active'):\n        return\n    (apb, ape) = self._get_audit_period(exists)\n    launched_at = self._extract_launched_at(exists)\n    if (apb and ape and (apb <= launched_at <= ape) and (len(block) == 0)):\n        raise UsageException('U8', '.exists launched_at in audit period, but no related events found.')\n", "label": 0}
{"function": "\n\ndef _recursive_custom_to_dfp(targeting):\n    '\\n    Convert custom targeting model to dfp dictionary\\n\\n    @param targeting: TargetingCriterion\\n    @return: dict\\n    '\n    (operator, targets) = targeting.get_data()\n    if (operator == TargetingCriterion.OPERATOR.NOT):\n        (operator, targets) = targets[0].get_data()\n        is_operator = 'IS_NOT'\n    else:\n        is_operator = 'IS'\n    if isinstance(targets[0], Custom):\n        children = _custom_target_list_to_child_list(targets, is_operator)\n    elif (len(targets) == 1):\n        return _recursive_custom_to_dfp(targets[0])\n    else:\n        children = [_recursive_custom_to_dfp(c) for c in targets]\n    return {\n        'xsi_type': 'CustomCriteriaSet',\n        'children': children,\n        'logicalOperator': operator,\n    }\n", "label": 0}
{"function": "\n\ndef _keystone_identity(self, environ):\n    'Extract the identity from the Keystone auth component.'\n    if ((environ.get('HTTP_X_IDENTITY_STATUS') != 'Confirmed') or (environ.get('HTTP_X_SERVICE_IDENTITY_STATUS') not in (None, 'Confirmed'))):\n        return\n    roles = list_from_csv(environ.get('HTTP_X_ROLES', ''))\n    service_roles = list_from_csv(environ.get('HTTP_X_SERVICE_ROLES', ''))\n    identity = {\n        'user': (environ.get('HTTP_X_USER_ID'), environ.get('HTTP_X_USER_NAME')),\n        'tenant': (environ.get('HTTP_X_TENANT_ID'), environ.get('HTTP_X_TENANT_NAME')),\n        'roles': roles,\n        'service_roles': service_roles,\n    }\n    token_info = environ.get('keystone.token_info', {\n        \n    })\n    auth_version = 0\n    user_domain = project_domain = (None, None)\n    if ('access' in token_info):\n        auth_version = 2\n    elif ('token' in token_info):\n        auth_version = 3\n        user_domain = (environ.get('HTTP_X_USER_DOMAIN_ID'), environ.get('HTTP_X_USER_DOMAIN_NAME'))\n        project_domain = (environ.get('HTTP_X_PROJECT_DOMAIN_ID'), environ.get('HTTP_X_PROJECT_DOMAIN_NAME'))\n    identity['user_domain'] = user_domain\n    identity['project_domain'] = project_domain\n    identity['auth_version'] = auth_version\n    return identity\n", "label": 0}
{"function": "\n\ndef _check_string(data):\n    val = data.lower()\n    return ((val.startswith('select ') and (' from ' in val)) or val.startswith('insert into') or (val.startswith('update ') and (' set ' in val)) or val.startswith('delete from '))\n", "label": 0}
{"function": "\n\ndef transform(self):\n    '\\n        Apply ModelView transformations.\\n\\n        You will most likely want to wrap calls to this function with\\n        ``glPushMatrix()``/``glPopMatrix()``\\n        '\n    (x, y) = director.get_window_size()\n    if (not (self.grid and self.grid.active)):\n        self.camera.locate()\n    gl.glTranslatef(self.position[0], self.position[1], 0)\n    gl.glTranslatef(self.transform_anchor_x, self.transform_anchor_y, 0)\n    if (self.rotation != 0.0):\n        gl.glRotatef((- self._rotation), 0, 0, 1)\n    if ((self.scale != 1.0) or (self.scale_x != 1.0) or (self.scale_y != 1.0)):\n        gl.glScalef((self._scale * self._scale_x), (self._scale * self._scale_y), 1)\n    if (self.transform_anchor != (0, 0)):\n        gl.glTranslatef((- self.transform_anchor_x), (- self.transform_anchor_y), 0)\n", "label": 0}
{"function": "\n\ndef inner(items):\n    '\\n    Sorted items to display as compatable js objects (eg bool,regex)\\n    '\n    for (k, v) in sorted(items):\n        if (k == 'is_'):\n            k = 'is'\n        if isinstance(v, bool):\n            (yield ('%s: %s' % (k, repr(v).lower())))\n        elif (k == 'pattern'):\n            (yield ('%s: new RegExp(/%s/)' % (k, v)))\n        else:\n            (yield ('%s: %r' % (k, v)))\n", "label": 0}
{"function": "\n\ndef setPath(self, path):\n    self._layout.removeAllComponents()\n    link = ActiveLink('Home', ExternalResource('#'))\n    link.addListener(self, ILinkActivatedListener)\n    self._layout.addComponent(link)\n    if ((path is not None) and (not ('' == path))):\n        parts = path.split('/')\n        link = None\n        for part in parts:\n            separator = Label('&raquo;', Label.CONTENT_XHTML)\n            separator.setSizeUndefined()\n            self._layout.addComponent(separator)\n            f = FeatureSet.FEATURES.getFeature(part)\n            link = ActiveLink(f.getName(), ExternalResource(('#' + f.getFragmentName())))\n            link.setData(f)\n            link.addListener(self, ILinkActivatedListener)\n            self._layout.addComponent(link)\n        if (link is not None):\n            link.setStyleName('bold')\n", "label": 0}
{"function": "\n\ndef _make_instance_list(context, inst_list, db_inst_list, expected_attrs):\n    get_fault = (expected_attrs and ('fault' in expected_attrs))\n    inst_faults = {\n        \n    }\n    if get_fault:\n        expected_attrs.remove('fault')\n        instance_uuids = [inst['uuid'] for inst in db_inst_list]\n        faults = objects.InstanceFaultList.get_by_instance_uuids(context, instance_uuids)\n        for fault in faults:\n            if (fault.instance_uuid not in inst_faults):\n                inst_faults[fault.instance_uuid] = fault\n    inst_cls = objects.Instance\n    inst_list.objects = []\n    for db_inst in db_inst_list:\n        inst_obj = inst_cls._from_db_object(context, inst_cls(context), db_inst, expected_attrs=expected_attrs)\n        if get_fault:\n            inst_obj.fault = inst_faults.get(inst_obj.uuid, None)\n        inst_list.objects.append(inst_obj)\n    inst_list.obj_reset_changes()\n    return inst_list\n", "label": 0}
{"function": "\n\ndef login(self, username=None, password=None, section='default'):\n    '\\n        Created the passport with ``username`` and ``password`` and log in.\\n        If either ``username`` or ``password`` is None or omitted, the\\n        credentials file will be parsed.\\n\\n        :param str username: username to login (email, phone number or user ID)\\n        :param str password: password\\n        :param str section: section name in the credential file\\n        :raise: raises :class:`.AuthenticationError` if failed to login\\n        '\n    if self.has_logged_in:\n        return True\n    if ((username is None) or (password is None)):\n        credential = conf.get_credential(section)\n        username = credential['username']\n        password = credential['password']\n    passport = Passport(username, password)\n    r = self.http.post(LOGIN_URL, passport.form)\n    if (r.state is True):\n        self.passport = passport\n        passport.data = r.content['data']\n        self._user_id = r.content['data']['USER_ID']\n        return True\n    else:\n        msg = None\n        if ('err_name' in r.content):\n            if (r.content['err_name'] == 'account'):\n                msg = 'Account does not exist.'\n            elif (r.content['err_name'] == 'passwd'):\n                msg = 'Password is incorrect.'\n        raise AuthenticationError(msg)\n", "label": 0}
{"function": "\n\ndef get_version():\n    '\\n    Returns the version from the generated version file without actually\\n    loading it (and thus trying to load the extension module).\\n    '\n    if (not os.path.exists(VERSION_FILE)):\n        raise VersionNotFound((VERSION_FILE + ' does not exist'))\n    fp = open(VERSION_FILE, 'r')\n    vline = None\n    for x in fp.readlines():\n        x = x.rstrip()\n        if (not x):\n            continue\n        if (not x.startswith('__version__')):\n            continue\n        vline = x.split('=')[1]\n        break\n    if (not vline):\n        raise VersionNotFound('version file present but has no contents')\n    return vline.strip().rstrip().replace(\"'\", '')\n", "label": 0}
{"function": "\n\ndef setup(self):\n    self._jira_url = self._config['url']\n    rsa_cert_file = self._config['rsa_cert_file']\n    if (not os.path.exists(rsa_cert_file)):\n        raise Exception(('Cert file for JIRA OAuth not found at %s.' % rsa_cert_file))\n    self._rsa_key = self._read_cert(rsa_cert_file)\n    self._poll_interval = self._config.get('poll_interval', self._poll_interval)\n    oauth_creds = {\n        'access_token': self._config['oauth_token'],\n        'access_token_secret': self._config['oauth_secret'],\n        'consumer_key': self._config['consumer_key'],\n        'key_cert': self._rsa_key,\n    }\n    self._jira_client = JIRA(options={\n        'server': self._jira_url,\n    }, oauth=oauth_creds)\n    if (self._projects_available is None):\n        self._projects_available = set()\n        for proj in self._jira_client.projects():\n            self._projects_available.add(proj.key)\n    self._project = self._config.get('project', None)\n    if ((not self._project) or (self._project not in self._projects_available)):\n        raise Exception(('Invalid project (%s) to track.' % self._project))\n    self._jql_query = ('project=%s' % self._project)\n    all_issues = self._jira_client.search_issues(self._jql_query, maxResults=None)\n    self._issues_in_project = {issue.key: issue for issue in all_issues}\n", "label": 0}
{"function": "\n\ndef get_videos(self):\n    self.candidates = self.parser.getElementsByTags(self.top_node, VIDEOS_TAGS)\n    for candidate in self.candidates:\n        tag = self.parser.getTag(candidate)\n        attr = ('get_%s_tag' % tag)\n        if hasattr(self, attr):\n            movie = getattr(self, attr)(candidate)\n            if ((movie is not None) and (movie.provider is not None)):\n                self.movies.append(movie)\n    return list(self.movies)\n", "label": 0}
{"function": "\n\ndef main():\n    INFO = 'Verilog code parser'\n    VERSION = pyverilog.utils.version.VERSION\n    USAGE = 'Usage: python example_parser.py file ...'\n\n    def showVersion():\n        print(INFO)\n        print(VERSION)\n        print(USAGE)\n        sys.exit()\n    optparser = OptionParser()\n    optparser.add_option('-v', '--version', action='store_true', dest='showversion', default=False, help='Show the version')\n    optparser.add_option('-I', '--include', dest='include', action='append', default=[], help='Include path')\n    optparser.add_option('-D', dest='define', action='append', default=[], help='Macro Definition')\n    (options, args) = optparser.parse_args()\n    filelist = args\n    if options.showversion:\n        showVersion()\n    for f in filelist:\n        if (not os.path.exists(f)):\n            raise IOError(('file not found: ' + f))\n    if (len(filelist) == 0):\n        showVersion()\n    (ast, directives) = parse(filelist, preprocess_include=options.include, preprocess_define=options.define)\n    ast.show()\n    for (lineno, directive) in directives:\n        print(('Line %d : %s' % (lineno, directive)))\n", "label": 0}
{"function": "\n\ndef win32GetLinkLocalIPv6Addresses():\n    '\\n    Return a list of strings in colon-hex format representing all the link local\\n    IPv6 addresses available on the system, as reported by\\n    I{WSAIoctl}/C{SIO_ADDRESS_LIST_QUERY}.\\n    '\n    s = socket(AF_INET6, SOCK_STREAM)\n    size = 4096\n    retBytes = c_int()\n    for i in range(2):\n        buf = create_string_buffer(size)\n        ret = WSAIoctl(s.fileno(), SIO_ADDRESS_LIST_QUERY, 0, 0, buf, size, byref(retBytes), 0, 0)\n        if (ret and retBytes.value):\n            size = retBytes.value\n        else:\n            break\n    if ret:\n        raise RuntimeError('WSAIoctl failure')\n    addrList = cast(buf, POINTER(make_SAL(0)))\n    addrCount = addrList[0].iAddressCount\n    addrList = cast(buf, POINTER(make_SAL(addrCount)))\n    addressStringBufLength = 1024\n    addressStringBuf = create_string_buffer(addressStringBufLength)\n    retList = []\n    for i in range(addrList[0].iAddressCount):\n        retBytes.value = addressStringBufLength\n        address = addrList[0].Address[i]\n        ret = WSAAddressToString(address.lpSockaddr, address.iSockaddrLength, 0, addressStringBuf, byref(retBytes))\n        if ret:\n            raise RuntimeError('WSAAddressToString failure')\n        retList.append(string_at(addressStringBuf))\n    return [addr for addr in retList if ('%' in addr)]\n", "label": 1}
{"function": "\n\n@flow.StateHandler()\ndef Start(self):\n    hunts_ttl = config_lib.CONFIG['DataRetention.hunts_ttl']\n    if (not hunts_ttl):\n        self.Log('TTL not set - nothing to do...')\n        return\n    exception_label = config_lib.CONFIG['DataRetention.hunts_ttl_exception_label']\n    hunts_root = aff4.FACTORY.Open('aff4:/hunts', token=self.token)\n    hunts_urns = list(hunts_root.ListChildren())\n    deadline = (rdfvalue.RDFDatetime().Now() - hunts_ttl)\n    hunts = aff4.FACTORY.MultiOpen(hunts_urns, aff4_type='GRRHunt', token=self.token)\n    for hunt in hunts:\n        if (exception_label in hunt.GetLabelsNames()):\n            continue\n        runner = hunt.GetRunner()\n        if (runner.context.expires < deadline):\n            aff4.FACTORY.Delete(hunt.urn, token=self.token)\n            self.HeartBeat()\n", "label": 0}
{"function": "\n\ndef get_view_from_another_group(self, window, filename):\n    if self.calling_view_index:\n        calling_group = self.calling_view_index[0]\n    else:\n        calling_group = window.get_view_index(window.active_view())[0]\n    for group in range(window.num_groups()):\n        if (group == calling_group):\n            continue\n        for view in window.views_in_group(group):\n            if (view.file_name() == filename):\n                return view\n", "label": 0}
{"function": "\n\ndef uncomment_json(lines):\n    new_lines = []\n    for line in lines:\n        if ('//' in line):\n            if (('\"' in line) or (\"'\" in line)):\n                single_quote_open = False\n                double_quote_open = False\n                previous_slash = False\n                counter = 0\n                comment_found = False\n                last_c = ''\n                for c in line:\n                    if (c == '/'):\n                        if (previous_slash and (not single_quote_open) and (not double_quote_open)):\n                            comment_found = True\n                            break\n                        previous_slash = True\n                    else:\n                        previous_slash = False\n                    if ((c == '\"') and (last_c != '\\\\')):\n                        double_quote_open = (not double_quote_open)\n                    if ((c == \"'\") and (last_c != '\\\\')):\n                        single_quote_open = (not single_quote_open)\n                    last_c = c\n                    counter += 1\n                if comment_found:\n                    new_lines.append((line[:(counter - 1)] + '\\n'))\n                else:\n                    new_lines.append(line)\n            else:\n                new_lines.append(line.split('//')[0])\n        else:\n            new_lines.append(line)\n    return new_lines\n", "label": 1}
{"function": "\n\n@classmethod\ndef fromapi(cls, attrib):\n    '\\n        Constructs a photo object from the Flickr API XML specification.\\n        '\n    if ('url_o' in attrib):\n        url = attrib['url_o']\n        size = (attrib['width_o'], attrib['height_o'])\n        format = 'original'\n    elif ('url_l' in attrib):\n        url = attrib['url_l']\n        size = (attrib['width_l'], attrib['height_l'])\n        format = 'large'\n    elif ('url_m' in attrib):\n        url = attrib['url_m']\n        size = (attrib['width_m'], attrib['height_m'])\n        format = 'medium'\n    elif ('url_s' in attrib):\n        url = attrib['url_s']\n        size = (attrib['width_s'], attrib['height_s'])\n        format = 'small'\n    else:\n        raise RuntimeError('Photo does not have URL')\n    return Photo(None, url, size, format, attrib['id'])\n", "label": 0}
{"function": "\n\ndef get_query_widget_class(descriptor, widget_type=None):\n    cls = get_widget_class(descriptor, widget_type, 'query', False)\n    if (cls is None):\n        if ((descriptor.module is not None) and hasattr(descriptor.module, 'get_query_widget_class')):\n            cls = descriptor.module.get_query_widget_class()\n        if (cls is None):\n\n            class DefaultQueryWidget(BaseQueryWidget):\n\n                def __init__(self, param, parent=None):\n                    BaseQueryWidget.__init__(self, get_widget_class(descriptor), ['==', '!='], param, parent)\n            return DefaultQueryWidget\n        reg = get_module_registry()\n        prefix = get_prefix(reg, descriptor)\n        return load_cls(cls, prefix)\n    return cls\n", "label": 0}
{"function": "\n\ndef _asdict(self):\n    'Convert this measurement to a dict of basic types.'\n    retval = {\n        'name': self.name,\n        'outcome': self.outcome,\n    }\n    if len(self.validators):\n        retval['validators'] = [str(v) for v in self.validators]\n    for attr in ('units', 'dimensions', 'docstring'):\n        if (getattr(self, attr) is not None):\n            retval[attr] = getattr(self, attr)\n    return retval\n", "label": 0}
{"function": "\n\ndef __init__(self):\n    for (name, url) in URL_MAP.items():\n        local_path = PATH_MAP[name]\n        if (not os.path.exists(local_path)):\n            logging.info('downloading %s dataset of binarized MNIST')\n            np.save(local_path, np.loadtxt(urllib.urlretrieve(url)[0]))\n    train_set = [(x,) for x in np.load(PATH_MAP['train'])]\n    valid_set = [(x,) for x in np.load(PATH_MAP['valid'])]\n    test_set = [(x,) for x in np.load(PATH_MAP['test'])]\n    super(BinarizedMnistDataset, self).__init__(train_set, valid=valid_set, test=test_set)\n", "label": 0}
{"function": "\n\ndef _shrink(self):\n    order = sorted((x for x in self.items() if (x[1][1] is not None)), (lambda a, b: cmp(b[1][1], a[1][1])))\n    toRemove = order[0:(self.limit / 10)]\n    if ((len(self) - len(toRemove)) > (self.limit * 0.95)):\n        toRemove += [x for x in self.items()][0:(self.limit / 10)]\n    for x in toRemove:\n        del self[x[0]]\n", "label": 0}
{"function": "\n\ndef add_option(self, mask):\n    'Set arbitrary query flags using a bitmask.\\n\\n        To set the tailable flag:\\n        cursor.add_option(2)\\n        '\n    if (not isinstance(mask, int)):\n        raise TypeError('mask must be an int')\n    self.__check_okay_to_chain()\n    if (mask & _QUERY_OPTIONS['slave_okay']):\n        self.__slave_okay = True\n    if (mask & _QUERY_OPTIONS['exhaust']):\n        if self.__limit:\n            raise InvalidOperation(\"Can't use limit and exhaust together.\")\n        if self.__collection.database.connection.is_mongos:\n            raise InvalidOperation('Exhaust cursors are not supported by mongos')\n        self.__exhaust = True\n    self.__query_flags |= mask\n    return self\n", "label": 0}
{"function": "\n\ndef _create_table(self, model_class, safe=False):\n    statement = ('CREATE TABLE IF NOT EXISTS' if safe else 'CREATE TABLE')\n    meta = model_class._meta\n    (columns, constraints) = ([], [])\n    if meta.composite_key:\n        pk_cols = [meta.fields[f].as_entity() for f in meta.primary_key.field_names]\n        constraints.append(Clause(SQL('PRIMARY KEY'), EnclosedClause(*pk_cols)))\n    for field in meta.sorted_fields:\n        columns.append(self.field_definition(field))\n        if (isinstance(field, ForeignKeyField) and (not field.deferred)):\n            constraints.append(self.foreign_key_constraint(field))\n    if model_class._meta.constraints:\n        for constraint in model_class._meta.constraints:\n            if (not isinstance(constraint, Node)):\n                constraint = SQL(constraint)\n            constraints.append(constraint)\n    return Clause(SQL(statement), model_class.as_entity(), EnclosedClause(*(columns + constraints)))\n", "label": 1}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    (xshp, yshp) = shapes\n    (x, y) = node.inputs\n    if ((x.ndim == 2) and (y.ndim == 2)):\n        return [(xshp[0], yshp[1])]\n    if ((x.ndim == 1) and (y.ndim == 2)):\n        return [(yshp[1],)]\n    if ((x.ndim == 2) and (y.ndim == 1)):\n        return [(xshp[0],)]\n    if ((x.ndim == 1) and (y.ndim == 1)):\n        return [()]\n    raise NotImplementedError()\n", "label": 1}
{"function": "\n\ndef GetInstallRequires():\n    'Returns the install_requires for setup.py'\n    install_requires = []\n    for values in PYTHON_DEPENDENCIES:\n        module_name = values[0]\n        module_version = values[2]\n        if (module_name == 'yaml'):\n            module_name = 'PyYAML'\n        elif (module_name == 'sqlite3'):\n            module_name = 'pysqlite'\n            module_version = None\n        if (not module_version):\n            install_requires.append(module_name)\n        else:\n            install_requires.append('{0:s} >= {1:s}'.format(module_name, module_version))\n    install_requires.append('pytsk3 >= 4.1.2')\n    for (module_name, module_version) in sorted(LIBYAL_DEPENDENCIES.items()):\n        if (not module_version):\n            install_requires.append(module_name)\n        else:\n            install_requires.append('{0:s} >= {1:d}'.format(module_name, module_version))\n    return sorted(install_requires)\n", "label": 0}
{"function": "\n\ndef test_none_entity(self):\n    (s, (u1, u2, u3, u4)) = self._fixture()\n    User = self.classes.User\n    ua = aliased(User)\n    q = s.query(User, ua)\n    kt = (lambda *x: KeyedTuple(x, ['User', 'useralias']))\n    collection = [kt(u1, u2), kt(u1, None), kt(u2, u3)]\n    it = loading.merge_result(q, collection)\n    eq_([(((x and x.id) or None), ((y and y.id) or None)) for (x, y) in it], [(u1.id, u2.id), (u1.id, None), (u2.id, u3.id)])\n", "label": 0}
{"function": "\n\ndef _nthroot_solve(p, n, prec):\n    '\\n     helper function for ``nthroot``\\n     It denests ``p**Rational(1, n)`` using its minimal polynomial\\n    '\n    from sympy.polys.numberfields import _minimal_polynomial_sq\n    from sympy.solvers import solve\n    while ((n % 2) == 0):\n        p = sqrtdenest(sqrt(p))\n        n = (n // 2)\n    if (n == 1):\n        return p\n    pn = (p ** Rational(1, n))\n    x = Symbol('x')\n    f = _minimal_polynomial_sq(p, n, x)\n    if (f is None):\n        return None\n    sols = solve(f, x)\n    for sol in sols:\n        if (abs((sol - pn)).n() < (1.0 / (10 ** prec))):\n            sol = sqrtdenest(sol)\n            if (_mexpand((sol ** n)) == p):\n                return sol\n", "label": 0}
{"function": "\n\ndef visit_literal(self, node):\n    classes = node.get('classes', [])\n    if ('code' in classes):\n        node['classes'] = [cls for cls in classes if (cls != 'code')]\n        self.body.append(self.starttag(node, 'code', ''))\n        return\n    self.body.append(self.starttag(node, 'code', '', CLASS='docutils literal'))\n    text = node.astext()\n    for token in self.words_and_spaces.findall(text):\n        if token.strip():\n            if self.sollbruchstelle.search(token):\n                self.body.append(('<span class=\"pre\">%s</span>' % self.encode(token)))\n            else:\n                self.body.append(self.encode(token))\n        elif (token in ('\\n', ' ')):\n            self.body.append(token)\n        else:\n            self.body.append((('&nbsp;' * (len(token) - 1)) + ' '))\n    self.body.append('</code>')\n    raise nodes.SkipNode\n", "label": 0}
{"function": "\n\ndef xbrlLoaded(cntlr, options, modelXbrl, entryPoint, *args, **kwargs):\n    modelManager = cntlr.modelManager\n    if hasattr(modelManager, 'efmFiling'):\n        if ((modelXbrl.modelDocument.type == ModelDocument.Type.INSTANCE) or (modelXbrl.modelDocument.type == ModelDocument.Type.INLINEXBRL)):\n            efmFiling = modelManager.efmFiling\n            efmFiling.addReport(modelXbrl)\n            _report = efmFiling.reports[(- 1)]\n            _report.entryPoint = entryPoint\n            if (('accessionNumber' in entryPoint) and (not hasattr(efmFiling, 'accessionNumber'))):\n                efmFiling.accessionNumber = entryPoint['accessionNumber']\n            if (('exhibitType' in entryPoint) and (not hasattr(_report, 'exhibitType'))):\n                _report.exhibitType = entryPoint['exhibitType']\n            efmFiling.arelleUnitTests = modelXbrl.arelleUnitTests.copy()\n        elif (modelXbrl.modelDocument.type == ModelDocument.Type.RSSFEED):\n            testcasesStart(cntlr, options, modelXbrl)\n", "label": 1}
{"function": "\n\ndef circle_data1(offset):\n    r1 = 1.5\n    center = (eu.Vector2(0.0, 0.0) + offset)\n    center_circle = create_obj_with_circle('center', center, r1)\n    r2 = 0.3\n    d1 = ((r1 + r2) - 0.1)\n    angles = [a for a in range(360, 1, ((- 360) // 12))]\n    ring_touching = set()\n    for a in angles:\n        center = ((d1 * eu.Vector2(cos(radians(a)), sin(radians(a)))) + offset)\n        circle = create_obj_with_circle(('ring_touching, distance 0.0, angle %3s' % a), center, r2)\n        ring_touching.add(circle)\n    near_distance = 0.1\n    d2 = ((r1 + r2) + near_distance)\n    angles = [a for a in range(360, 1, ((- 360) // 12))]\n    ring_near = set()\n    for a in angles:\n        center = ((d2 * eu.Vector2(cos(radians(a)), sin(radians(a)))) + offset)\n        circle = create_obj_with_circle(('ring_near, distance 0.1, angle %3s' % a), center, r2)\n        ring_near.add(circle)\n    far_distance = 0.2\n    d3 = ((r1 + r2) + far_distance)\n    angles = [a for a in range(360, 1, ((- 360) // 12))]\n    ring_far = set()\n    for a in angles:\n        center = ((d3 * eu.Vector2(cos(radians(a)), sin(radians(a)))) + offset)\n        circle = create_obj_with_circle(('ring_far, distance 0.2, angle %3s' % a), center, r2)\n        ring_far.add(circle)\n    return (center_circle, ring_touching, ring_near, ring_far, near_distance, far_distance, angles)\n", "label": 0}
{"function": "\n\ndef parse_etags(value):\n    'Parse an etag header.\\n\\n    :param value: the tag header to parse\\n    :return: an :class:`~werkzeug.datastructures.ETags` object.\\n    '\n    if (not value):\n        return ETags()\n    strong = []\n    weak = []\n    end = len(value)\n    pos = 0\n    while (pos < end):\n        match = _etag_re.match(value, pos)\n        if (match is None):\n            break\n        (is_weak, quoted, raw) = match.groups()\n        if (raw == '*'):\n            return ETags(star_tag=True)\n        elif quoted:\n            raw = quoted\n        if is_weak:\n            weak.append(raw)\n        else:\n            strong.append(raw)\n        pos = match.end()\n    return ETags(strong, weak)\n", "label": 0}
{"function": "\n\ndef render_pep440_old(pieces):\n    if pieces['closest-tag']:\n        rendered = pieces['closest-tag']\n        if (pieces['distance'] or pieces['dirty']):\n            rendered += ('.post%d' % pieces['distance'])\n            if pieces['dirty']:\n                rendered += '.dev0'\n    else:\n        rendered = ('0.post%d' % pieces['distance'])\n        if pieces['dirty']:\n            rendered += '.dev0'\n    return rendered\n", "label": 0}
{"function": "\n\ndef __init__(self, app, context, old_key=None, new_key=None, old='', new=''):\n    super(SideDiffCommentEdit, self).__init__([])\n    self.app = app\n    self.context = context\n    self.old_key = old_key\n    self.new_key = new_key\n    self.old = mywid.MyEdit(edit_text=old, multiline=True, ring=app.ring)\n    self.new = mywid.MyEdit(edit_text=new, multiline=True, ring=app.ring)\n    self.contents.append((urwid.Text(''), ('given', LN_COL_WIDTH, False)))\n    if (context.old_file_key and ((context.old_ln is not None) or context.header)):\n        self.contents.append((urwid.AttrMap(self.old, 'draft-comment'), ('weight', 1, False)))\n    else:\n        self.contents.append((urwid.Text(''), ('weight', 1, False)))\n    self.contents.append((urwid.Text(''), ('given', LN_COL_WIDTH, False)))\n    if (context.new_file_key and ((context.new_ln is not None) or context.header)):\n        self.contents.append((urwid.AttrMap(self.new, 'draft-comment'), ('weight', 1, False)))\n        new_editable = True\n    else:\n        self.contents.append((urwid.Text(''), ('weight', 1, False)))\n        new_editable = False\n    if new_editable:\n        self.focus_position = 3\n    else:\n        self.focus_position = 1\n", "label": 0}
{"function": "\n\ndef python_value(self, value):\n    if value:\n        if isinstance(value, basestring):\n            pp = (lambda x: x.time())\n            return format_date_time(value, self.formats, pp)\n        elif isinstance(value, datetime.datetime):\n            return value.time()\n    if ((value is not None) and isinstance(value, datetime.timedelta)):\n        return (datetime.datetime.min + value).time()\n    return value\n", "label": 0}
{"function": "\n\ndef attach_iso(self, name, iso):\n    (dom, host) = self.get_active_domain(name)\n    if (not dom):\n        dom = self.get_inactive_domain(name)\n        host = dom['host']\n    for x in self.hosts:\n        if (host == x['name']):\n            conn = x['conn']\n    dom = conn.lookupByName(name)\n    cdrom = None\n    desc = fromstring(dom.XMLDesc(libvirt.VIR_DOMAIN_XML_SECURE))\n    for disk in desc.findall('.//disk'):\n        if (disk.get('device') == 'cdrom'):\n            cdrom = disk\n            if cdrom.find('.//source'):\n                cdrom.find('.//source').set('file', iso)\n            else:\n                desc.find('.//devices').remove(cdrom)\n                cdrom = None\n    if (not cdrom):\n        xml = '\\n<disk type=\"file\" device=\"cdrom\">\\n  <driver name=\"qemu\"/>\\n  <source file=\"%s\" />\\n  <target dev=\"hdc\" bus=\"ide\"/>\\n  <readonly/>\\n</disk>\\n   '\n    xml = (xml % iso)\n    desc.find('.//devices').insert((- 1), fromstring(xml))\n    conn.defineXML(tostring(desc))\n    return True\n", "label": 0}
{"function": "\n\ndef read(self, filename):\n    'Read only one filename. In contrast to the original ConfigParser of\\n        Python, this one is able to read only one file at a time. The last\\n        read file will be used for the :meth:`write` method.\\n\\n        .. versionchanged:: 1.9.0\\n            :meth:`read` now calls the callbacks if read changed any values.\\n\\n        '\n    if (not isinstance(filename, string_types)):\n        raise Exception('Only one filename is accepted ({})'.format(string_types.__name__))\n    self.filename = filename\n    old_vals = {sect: {k: v for (k, v) in self.items(sect)} for sect in self.sections()}\n    PythonConfigParser.read(self, filename)\n    f = self._do_callbacks\n    for section in self.sections():\n        if (section not in old_vals):\n            for (k, v) in self.items(section):\n                f(section, k, v)\n            continue\n        old_keys = old_vals[section]\n        for (k, v) in self.items(section):\n            if ((k not in old_keys) or (v != old_keys[k])):\n                f(section, k, v)\n", "label": 1}
{"function": "\n\ndef _number_type_helper(national_number, metadata):\n    'Return the type of the given number against the metadata'\n    if (not _is_number_matching_desc(national_number, metadata.general_desc)):\n        return PhoneNumberType.UNKNOWN\n    if _is_number_matching_desc(national_number, metadata.premium_rate):\n        return PhoneNumberType.PREMIUM_RATE\n    if _is_number_matching_desc(national_number, metadata.toll_free):\n        return PhoneNumberType.TOLL_FREE\n    if _is_number_matching_desc(national_number, metadata.shared_cost):\n        return PhoneNumberType.SHARED_COST\n    if _is_number_matching_desc(national_number, metadata.voip):\n        return PhoneNumberType.VOIP\n    if _is_number_matching_desc(national_number, metadata.personal_number):\n        return PhoneNumberType.PERSONAL_NUMBER\n    if _is_number_matching_desc(national_number, metadata.pager):\n        return PhoneNumberType.PAGER\n    if _is_number_matching_desc(national_number, metadata.uan):\n        return PhoneNumberType.UAN\n    if _is_number_matching_desc(national_number, metadata.voicemail):\n        return PhoneNumberType.VOICEMAIL\n    if _is_number_matching_desc(national_number, metadata.fixed_line):\n        if metadata.same_mobile_and_fixed_line_pattern:\n            return PhoneNumberType.FIXED_LINE_OR_MOBILE\n        elif _is_number_matching_desc(national_number, metadata.mobile):\n            return PhoneNumberType.FIXED_LINE_OR_MOBILE\n        return PhoneNumberType.FIXED_LINE\n    if ((not metadata.same_mobile_and_fixed_line_pattern) and _is_number_matching_desc(national_number, metadata.mobile)):\n        return PhoneNumberType.MOBILE\n    return PhoneNumberType.UNKNOWN\n", "label": 1}
{"function": "\n\n@contextfunction\ndef permission_block(context, object):\n    'Block with objects permissions'\n    request = context['request']\n    response_format = 'html'\n    if ('response_format' in context):\n        response_format = context['response_format']\n    response_format_tags = response_format\n    if ('response_format_tags' in context):\n        response_format_tags = context['response_format_tags']\n    if ('permission' in request.GET):\n        if request.user.profile.has_permission(object, mode='w'):\n            if request.POST:\n                if ('cancel' in request.POST):\n                    request.redirect = request.path\n                    return Markup(render_to_string('core/tags/permission_block', {\n                        'object': object,\n                        'path': request.path,\n                    }, context_instance=RequestContext(request), response_format=response_format))\n                form = PermissionForm(request.POST, instance=object)\n                if form.is_valid():\n                    form.save()\n                    request.redirect = request.path\n                    return Markup(render_to_string('core/tags/permission_block', {\n                        'object': object,\n                        'path': request.path,\n                    }, context_instance=RequestContext(request), response_format=response_format))\n            else:\n                form = PermissionForm(instance=object)\n            context = {\n                'object': object,\n                'path': request.path,\n                'form': form,\n            }\n            if ('ajax' in response_format_tags):\n                context = converter.preprocess_context(context)\n            return Markup(render_to_string('core/tags/permission_block_edit', context, context_instance=RequestContext(request), response_format=response_format))\n    return Markup(render_to_string('core/tags/permission_block', {\n        'object': object,\n        'path': request.path,\n    }, context_instance=RequestContext(request), response_format=response_format))\n", "label": 1}
{"function": "\n\ndef insert_filename(self, dirname, filename, ext='.py'):\n    'Ensure that a specific filename exists in the breakpoint tree'\n    full_filename = os.path.join(dirname, filename)\n    if (not self.exists(nodify(full_filename))):\n        if (full_filename == self.normalizer(full_filename)):\n            if self.root:\n                return\n            self.insert_dirname(dirname)\n        elif (self.root is None):\n            return\n        files = sorted(self.get_children(nodify(dirname)), reverse=False)\n        index = len([item for item in files if (item < nodify(full_filename))])\n        self.insert(nodify(dirname), index, nodify(os.path.join(dirname, filename)), text=filename, open=True, tags=((['file'] + ['code']) if (ext == '.py') else ['non_code']))\n", "label": 0}
{"function": "\n\ndef _plot_topomap_multi_cbar(data, pos, ax, title=None, unit=None, vmin=None, vmax=None, cmap='RdBu_r', colorbar=False, cbar_fmt='%3.3f'):\n    'Aux Function'\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n    _hide_frame(ax)\n    vmin = (np.min(data) if (vmin is None) else vmin)\n    vmax = (np.max(data) if (vmax is None) else vmax)\n    if (title is not None):\n        ax.set_title(title, fontsize=10)\n    (im, _) = plot_topomap(data, pos, vmin=vmin, vmax=vmax, axes=ax, cmap=cmap, image_interp='bilinear', contours=False, show=False)\n    if (colorbar is True):\n        divider = make_axes_locatable(ax)\n        cax = divider.append_axes('right', size='10%', pad=0.25)\n        cbar = plt.colorbar(im, cax=cax, format=cbar_fmt)\n        cbar.set_ticks((vmin, vmax))\n        if (unit is not None):\n            cbar.ax.set_title(unit, fontsize=8)\n        cbar.ax.tick_params(labelsize=8)\n", "label": 0}
{"function": "\n\ndef unregister(self, vote_model, content_models, Provider):\n    provider_instance = Provider()\n    for signal in provider_instance.rate_signals:\n        if isinstance(signal, str):\n            sig_class_name = signal.split('.')[(- 1)]\n            sig_instance = import_from_classname(signal)\n            listener = getattr(provider_instance, sig_class_name, False)\n            if listener:\n                for model in content_models:\n                    sig_instance.disconnect(listener, sender=model)\n    new_set = [i for i in self.providers if (not isinstance(i, Provider))]\n    self.providers = set(new_set)\n    for model in content_models:\n        del self._content_providers[model_path(model)]\n    del self._vote_providers[model_path(vote_model)]\n", "label": 0}
{"function": "\n\ndef run_job(name, force=False):\n    \"\\n    Run a scheduled job on the minion immediately\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' schedule.run_job job1\\n\\n        salt '*' schedule.run_job job1 force=True\\n        Force the job to run even if it is disabled.\\n    \"\n    ret = {\n        'comment': [],\n        'result': True,\n    }\n    if (not name):\n        ret['comment'] = 'Job name is required.'\n        ret['result'] = False\n    schedule = list_(show_all=True, return_yaml=False)\n    if (name in schedule):\n        data = schedule[name]\n        if (('enabled' in data) and (not data['enabled']) and (not force)):\n            ret['comment'] = 'Job {0} is disabled.'.format(name)\n        else:\n            out = __salt__['event.fire']({\n                'name': name,\n                'func': 'run_job',\n            }, 'manage_schedule')\n            if out:\n                ret['comment'] = 'Scheduling Job {0} on minion.'.format(name)\n            else:\n                ret['comment'] = 'Failed to run job {0} on minion.'.format(name)\n                ret['result'] = False\n    else:\n        ret['comment'] = 'Job {0} does not exist.'.format(name)\n        ret['result'] = False\n    return ret\n", "label": 0}
{"function": "\n\ndef to_pairs(dictlike):\n    'Yield (key, value) pairs from any dict-like object.\\n\\n    Implements an optimized version of the dict.update() definition of\\n    \"dictlike\".\\n\\n    '\n    if hasattr(dictlike, 'iteritems'):\n        return dictlike.iteritems()\n    elif hasattr(dictlike, 'keys'):\n        return ((key, dictlike[key]) for key in dictlike.keys())\n    elif hasattr(dictlike, '_asdict'):\n        return dictlike._asdict().iteritems()\n    else:\n        return ((key, value) for (key, value) in dictlike)\n", "label": 0}
{"function": "\n\ndef _gen_reference(self):\n\n    def get_scope_data(scope):\n        ret = []\n        for si in ScopeInfoIterator(self.context.options.known_scope_to_info).iterate([scope]):\n            help_info = HelpInfoExtracter(si.scope).get_option_scope_help_info_from_parser(self.context.options.get_parser(si.scope))\n            ret.append({\n                'scope_info': si,\n                'help_info': help_info._asdict(),\n            })\n        return ret\n    all_global_data = get_scope_data(GLOBAL_SCOPE)\n    global_scope_data = all_global_data[0:1]\n    global_subsystem_data = all_global_data[1:]\n    goal_scopes = sorted([si.scope for si in self.context.options.known_scope_to_info.values() if (si.scope and ('.' not in si.scope) and (si.category != ScopeInfo.SUBSYSTEM))])\n    goal_data = []\n    for scope in goal_scopes:\n        goal_data.append({\n            'goal': scope,\n            'goal_description': Goal.by_name(scope).description,\n            'task_data': get_scope_data(scope)[1:],\n        })\n    self._do_render(self.get_options().pants_reference_template, {\n        'global_scope_data': global_scope_data,\n        'global_subsystem_data': global_subsystem_data,\n        'goal_data': goal_data,\n    })\n", "label": 0}
{"function": "\n\ndef min_max(obj, val, is_max):\n    ' min/max validator for float and integer\\n    '\n    n = getattr(obj, ('maximum' if is_max else 'minimum'), None)\n    if (n == None):\n        return\n    _eq = getattr(obj, ('exclusiveMaximum' if is_max else 'exclusiveMinimum'), False)\n    if is_max:\n        to_raise = ((val >= n) if _eq else (val > n))\n    else:\n        to_raise = ((val <= n) if _eq else (val < n))\n    if to_raise:\n        raise ValidationError('condition failed: {0}, v:{1} compared to o:{2}'.format(('maximum' if is_max else 'minimum'), val, n))\n", "label": 1}
{"function": "\n\ndef build_instances(self, ctxt, **kwargs):\n    'Build instances.'\n    build_inst_kwargs = kwargs\n    instances = build_inst_kwargs['instances']\n    build_inst_kwargs['image'] = jsonutils.to_primitive(build_inst_kwargs['image'])\n    version = '1.34'\n    if self.client.can_send_version('1.34'):\n        build_inst_kwargs.pop('legacy_bdm', None)\n    else:\n        bdm_p = objects_base.obj_to_primitive(build_inst_kwargs['block_device_mapping'])\n        build_inst_kwargs['block_device_mapping'] = bdm_p\n        version = '1.32'\n    if (not self.client.can_send_version('1.32')):\n        instances_p = [jsonutils.to_primitive(inst) for inst in instances]\n        build_inst_kwargs['instances'] = instances_p\n        version = '1.30'\n    if (not self.client.can_send_version('1.30')):\n        if ('filter_properties' in build_inst_kwargs):\n            filter_properties = build_inst_kwargs['filter_properties']\n            flavor = filter_properties['instance_type']\n            flavor_p = objects_base.obj_to_primitive(flavor)\n            filter_properties['instance_type'] = flavor_p\n        version = '1.8'\n    cctxt = self.client.prepare(version=version)\n    cctxt.cast(ctxt, 'build_instances', build_inst_kwargs=build_inst_kwargs)\n", "label": 0}
{"function": "\n\ndef W(self):\n    'ISO-8601 week number of year, weeks starting on Monday'\n    week_number = None\n    jan1_weekday = (self.data.replace(month=1, day=1).weekday() + 1)\n    weekday = (self.data.weekday() + 1)\n    day_of_year = self.z()\n    if ((day_of_year <= (8 - jan1_weekday)) and (jan1_weekday > 4)):\n        if ((jan1_weekday == 5) or ((jan1_weekday == 6) and calendar.isleap((self.data.year - 1)))):\n            week_number = 53\n        else:\n            week_number = 52\n    else:\n        if calendar.isleap(self.data.year):\n            i = 366\n        else:\n            i = 365\n        if ((i - day_of_year) < (4 - weekday)):\n            week_number = 1\n        else:\n            j = ((day_of_year + (7 - weekday)) + (jan1_weekday - 1))\n            week_number = (j // 7)\n            if (jan1_weekday > 4):\n                week_number -= 1\n    return week_number\n", "label": 1}
{"function": "\n\ndef topurl(url, positions=''):\n    'Return the top performing URL for a site given a positions object.'\n    if (not url):\n        return 'lite value required in column or Config tab.'\n    apexdom = apex(url)\n    if positions:\n        urldict = json.loads(positions)\n        if urldict:\n            for (thepos, aurl) in urldict.iteritems():\n                if (apexdom in aurl):\n                    return aurl\n        else:\n            return None\n", "label": 0}
{"function": "\n\ndef chfullname(name, fullname):\n    \"\\n    Change the user's Full Name\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' user.chfullname foo 'Foo Bar'\\n    \"\n    if isinstance(fullname, string_types):\n        fullname = _sdecode(fullname)\n    pre_info = info(name)\n    if (not pre_info):\n        raise CommandExecutionError(\"User '{0}' does not exist\".format(name))\n    if isinstance(pre_info['fullname'], string_types):\n        pre_info['fullname'] = _sdecode(pre_info['fullname'])\n    if (fullname == pre_info['fullname']):\n        return True\n    _dscl(['/Users/{0}'.format(name), 'RealName', fullname], ctype='create')\n    time.sleep(1)\n    current = info(name).get('fullname')\n    if isinstance(current, string_types):\n        current = _sdecode(current)\n    return (current == fullname)\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('InvalidOperation')\n    if (self.what is not None):\n        oprot.writeFieldBegin('what', TType.I32, 1)\n        oprot.writeI32(self.what)\n        oprot.writeFieldEnd()\n    if (self.why is not None):\n        oprot.writeFieldBegin('why', TType.STRING, 2)\n        oprot.writeString(self.why)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\n@login_notrequired\ndef show_login_page(request, login_errors=False):\n    'Used by the non-jframe login'\n    redirect_to = request.REQUEST.get('next', '/')\n    is_first_login_ever = OAuthBackend.is_first_login_ever()\n    request.session.set_test_cookie()\n    return render('oauth-login.mako', request, {\n        'action': urlresolvers.reverse('oauth_login'),\n        'next': redirect_to,\n        'first_login_ever': is_first_login_ever,\n        'login_errors': ((request.method == 'POST') or login_errors),\n        'socialGoogle': ((liboauth.conf.CONSUMER_KEY_GOOGLE.get() != '') and (liboauth.conf.CONSUMER_SECRET_GOOGLE.get() != '')),\n        'socialFacebook': ((liboauth.conf.CONSUMER_KEY_FACEBOOK.get() != '') and (liboauth.conf.CONSUMER_SECRET_FACEBOOK.get() != '')),\n        'socialLinkedin': ((liboauth.conf.CONSUMER_KEY_LINKEDIN.get() != '') and (liboauth.conf.CONSUMER_SECRET_LINKEDIN.get() != '')),\n        'socialTwitter': ((liboauth.conf.CONSUMER_KEY_TWITTER.get() != '') and (liboauth.conf.CONSUMER_SECRET_TWITTER.get() != '')),\n    })\n", "label": 0}
{"function": "\n\ndef inject(context, injection_list):\n    'Helper function to inject the payload and to collect the results\\n\\n    :param context: The Behave context\\n    :param injection_list: An anomaly dictionary, see dictwalker.py\\n    '\n    if hasattr(context, 'injection_methods'):\n        methods = context.injection_methods\n    else:\n        methods = ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS', 'HEAD', 'PATCH']\n    responses = []\n    for injection in injection_list:\n        for injected_submission in dictwalk(context.submission[0], injection):\n            for method in methods:\n                if (context.type == 'urlencode'):\n                    form_string = serialise_to_url(injected_submission, encode=True)\n                    if (method == 'GET'):\n                        form_string = ('?' + form_string)\n                if (context.type == 'url-parameters'):\n                    form_string = dict_to_urlparams(injected_submission)\n                if (context.type == 'json'):\n                    form_string = serialise_to_json(injected_submission, encode=True)\n                if (hasattr(context, 'proxy_address') is False):\n                    context.proxy_address = None\n                responses += send_http(context, form_string, timeout=context.timeout, proxy=context.proxy_address, method=method, content_type=context.content_type, scenario_id=context.scenario_id, auth=authenticate(context, context.authentication_id))\n                if hasattr(context, 'valid_case_instrumentation'):\n                    test_valid_submission(context, injected_submission)\n    return responses\n", "label": 1}
{"function": "\n\ndef validate_url(self, url):\n    'Validate the :class:`~urllib.parse.ParseResult` object.\\n\\n        This method will make sure the :meth:`~brownant.app.BrownAnt.parse_url`\\n        could work as expected even meet a unexpected URL string.\\n\\n        :param url: the parsed url.\\n        :type url: :class:`~urllib.parse.ParseResult`\\n        '\n    url_path = to_bytes_safe(url.path)\n    url_path = urllib.parse.quote(url_path, safe=b'/%')\n    url_query = to_bytes_safe(url.query)\n    url_query = urllib.parse.quote(url_query, safe=b'?=&')\n    url = urllib.parse.ParseResult(url.scheme, url.netloc, url_path, url.params, url_query, url.fragment)\n    has_hostname = ((url.hostname is not None) and (len(url.hostname) > 0))\n    has_http_scheme = (url.scheme in ('http', 'https'))\n    has_path = ((not len(url.path)) or url.path.startswith('/'))\n    if (not (has_hostname and has_http_scheme and has_path)):\n        raise NotSupported(('invalid url: %s' % repr(url)))\n    return url\n", "label": 0}
{"function": "\n\ndef circular_max_sum(self, A):\n    '\\n        dp:\\n        left: max sum for index 0..i\\n        right: max sum for index i..(n-1)\\n\\n        :param A:\\n        :return:\\n        '\n    n = len(A)\n    left = [None for _ in A]\n    right = [None for _ in A]\n    (cur, max_sum, idx) = (0, A[0], 0)\n    for i in xrange(n):\n        cur += A[i]\n        if (cur > max_sum):\n            idx = i\n            max_sum = cur\n        left[i] = (max_sum, idx)\n    (cur, max_sum, idx) = (0, A[(n - 1)], (n - 1))\n    for i in xrange((n - 1), (- 1), (- 1)):\n        cur += A[i]\n        if (cur > max_sum):\n            idx = i\n            max_sum = cur\n        right[i] = (max_sum, idx)\n    ret = Sum(A[0], 0, 0)\n    for i in xrange(1, n):\n        r = right[i]\n        l = left[(i - 1)]\n        if (ret.sum < (r[0] + l[0])):\n            ret = Sum((r[0] + l[0]), r[1], l[1])\n    return ret\n", "label": 1}
{"function": "\n\ndef write(self, x86_operand, value):\n    if isinstance(x86_operand, barf.arch.x86.x86base.X86RegisterOperand):\n        reil_operand = ReilRegisterOperand(x86_operand.name, x86_operand.size)\n        if ((self._arch_info.architecture_mode == ARCH_X86_MODE_64) and (x86_operand.size == 32)):\n            if (x86_operand.name in self._regs_mapper):\n                (base_reg, offset) = self._regs_mapper[x86_operand.name]\n                reil_operand_base = ReilRegisterOperand(base_reg, self._regs_size[base_reg])\n                reil_immediate = ReilImmediateOperand(0, self._regs_size[base_reg])\n                self.add(self._builder.gen_str(reil_immediate, reil_operand_base))\n        self.add(self._builder.gen_str(value, reil_operand))\n    elif isinstance(x86_operand, barf.arch.x86.x86base.X86MemoryOperand):\n        addr = self._compute_memory_address(x86_operand)\n        if (value.size != x86_operand.size):\n            tmp = self.temporal(x86_operand.size)\n            self.add(self._builder.gen_str(value, tmp))\n            self.add(self._builder.gen_stm(tmp, addr))\n        else:\n            self.add(self._builder.gen_stm(value, addr))\n    else:\n        raise Exception()\n", "label": 0}
{"function": "\n\n@classmethod\ndef parse(cls, api, json):\n    user = cls(api)\n    for (k, v) in json.items():\n        if (k == 'created_at'):\n            setattr(user, k, parse_datetime(v))\n        elif (k == 'status'):\n            setattr(user, k, Status.parse(api, v))\n        elif (k == 'following'):\n            if (v is True):\n                setattr(user, k, True)\n            else:\n                setattr(user, k, False)\n        else:\n            setattr(user, k, v)\n    return user\n", "label": 0}
{"function": "\n\ndef run(self, parent, blocks):\n    items = self.get_items(blocks.pop(0))\n    sibling = self.lastChild(parent)\n    if (sibling and (sibling.tag in ['ol', 'ul'])):\n        lst = sibling\n        if lst[(- 1)].text:\n            p = util.etree.Element('p')\n            p.text = lst[(- 1)].text\n            lst[(- 1)].text = ''\n            lst[(- 1)].insert(0, p)\n        lch = self.lastChild(lst[(- 1)])\n        if ((lch is not None) and lch.tail):\n            p = util.etree.SubElement(lst[(- 1)], 'p')\n            p.text = lch.tail.lstrip()\n            lch.tail = ''\n        li = util.etree.SubElement(lst, 'li')\n        self.parser.state.set('looselist')\n        firstitem = items.pop(0)\n        self.parser.parseBlocks(li, [firstitem])\n        self.parser.state.reset()\n    elif (parent.tag in ['ol', 'ul']):\n        lst = parent\n    else:\n        lst = util.etree.SubElement(parent, self.TAG)\n        if ((not self.parser.markdown.lazy_ol) and (self.STARTSWITH != '1')):\n            lst.attrib['start'] = self.STARTSWITH\n    self.parser.state.set('list')\n    for item in items:\n        if item.startswith((' ' * self.tab_length)):\n            self.parser.parseBlocks(lst[(- 1)], [item])\n        else:\n            li = util.etree.SubElement(lst, 'li')\n            self.parser.parseBlocks(li, [item])\n    self.parser.state.reset()\n", "label": 1}
{"function": "\n\ndef get_stories(self, story_type='', limit=30):\n    \"\\n        Yields a list of stories from the passed page\\n        of HN.\\n        'story_type' can be:\\n        \\t'' = top stories (homepage) (default)\\n        \\t'news2' = page 2 of top stories\\n        \\t'newest' = most recent stories\\n        \\t'best' = best stories\\n\\n        'limit' is the number of stories required from the given page.\\n        Defaults to 30. Cannot be more than 30.\\n        \"\n    if ((limit is None) or (limit < 1) or (limit > 30)):\n        limit = 30\n    stories_found = 0\n    while (stories_found < limit):\n        soup = get_soup(page=story_type)\n        all_rows = self._get_zipped_rows(soup)\n        stories = self._build_story(all_rows)\n        for story in stories:\n            (yield story)\n            stories_found += 1\n            if (stories_found == limit):\n                return\n", "label": 0}
{"function": "\n\ndef __call__(self, environ, start_response):\n    cleaned_path = environ.get('PATH_INFO', '').strip('/')\n    for sep in (os.sep, os.altsep):\n        if (sep and (sep != '/')):\n            cleaned_path = cleaned_path.replace(sep, '/')\n    path = '/'.join(([''] + [x for x in cleaned_path.split('/') if (x and (x != '..'))]))\n    (real_filename, file_loader) = self.loader(path[1:])\n    if (file_loader is None):\n        return self.app(environ, start_response)\n    guessed_type = mimetypes.guess_type(real_filename)\n    mime_type = (guessed_type[0] or self.fallback_mimetype)\n    (f, mtime, file_size) = file_loader()\n    headers = [('Date', http_date())]\n    headers.append(('Cache-Control', 'public'))\n    headers.extend((('Content-Type', mime_type), ('Content-Length', str(file_size)), ('Last-Modified', http_date(mtime))))\n    start_response('200 OK', headers)\n    return wrap_file(environ, f)\n", "label": 1}
{"function": "\n\n@options([make_option('-l', '--list', action='store_true', help='List MT interceptors'), make_option('-a', '--add', action='store_true', help='Add a new MT interceptor'), make_option('-r', '--remove', type='string', metavar='ORDER', help=\"Remove MT interceptor using it's ORDER\"), make_option('-s', '--show', type='string', metavar='ORDER', help=\"Show MT interceptor using it's ORDER\"), make_option('-f', '--flush', action='store_true', help='Flush MT interception table')], '')\ndef do_mtinterceptor(self, arg, opts=None):\n    'MT Interceptor management'\n    if opts.list:\n        self.managers['mtinterceptor'].list(arg, opts)\n    elif opts.add:\n        self.managers['mtinterceptor'].add(arg, opts)\n    elif opts.remove:\n        self.managers['mtinterceptor'].remove(arg, opts)\n    elif opts.show:\n        self.managers['mtinterceptor'].show(arg, opts)\n    elif opts.flush:\n        self.managers['mtinterceptor'].flush(arg, opts)\n    else:\n        return self.sendData('Missing required option')\n", "label": 0}
{"function": "\n\ndef _args_for_conf(self, default_cfg, conf):\n    if (conf is None):\n        return dict(((key, _Default.resolve(value)) for (key, value) in default_cfg.items() if _Default.is_set(value)))\n    else:\n        return dict(((key, _Default.resolve_w_conf(value, conf.database, key)) for (key, value) in default_cfg.items() if _Default.is_set_w_conf(value, conf.database, key)))\n", "label": 0}
{"function": "\n\ndef _check_close(self):\n    conn = self.headers.get('connection')\n    if (self.version == 11):\n        conn = self.headers.get('connection')\n        if (conn and ('close' in conn.lower())):\n            return True\n        return False\n    if self.headers.get('keep-alive'):\n        return False\n    if (conn and ('keep-alive' in conn.lower())):\n        return False\n    pconn = self.headers.get('proxy-connection')\n    if (pconn and ('keep-alive' in pconn.lower())):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef _matches_any(self, expected, subject):\n    if (len(subject) == 0):\n        return (False, 'is empty')\n    if isinstance(subject, _compat.string_types):\n        if (expected in subject):\n            return (True, 'item {0!r} found'.format(expected))\n        return (False, 'item {0!r} not found'.format(expected))\n    expected = default_matcher(expected)\n    for item in subject:\n        (matches, _) = expected._match(item)\n        if matches:\n            return (True, 'item {0!r} found'.format(expected))\n    return (False, 'item {0!r} not found'.format(expected))\n", "label": 0}
{"function": "\n\ndef flush(self, ts, interval):\n    if (not self.count):\n        return []\n    self.samples.sort()\n    length = len(self.samples)\n    min_ = self.samples[0]\n    max_ = self.samples[(- 1)]\n    med = self.samples[int(round(((length / 2) - 1)))]\n    avg = (sum(self.samples) / float(length))\n    aggregators = [('min', min_, MetricTypes.GAUGE), ('max', max_, MetricTypes.GAUGE), ('median', med, MetricTypes.GAUGE), ('avg', avg, MetricTypes.GAUGE), ('count', (self.count / interval), MetricTypes.RATE)]\n    metric_aggrs = [(agg_name, agg_func, m_type) for (agg_name, agg_func, m_type) in aggregators if (agg_name in self.aggregates)]\n    metrics = [self.formatter(hostname=self.hostname, device_name=self.device_name, tags=self.tags, metric=('%s.%s' % (self.name, suffix)), value=value, timestamp=ts, metric_type=metric_type, interval=interval) for (suffix, value, metric_type) in metric_aggrs]\n    for p in self.percentiles:\n        val = self.samples[int(round(((p * length) - 1)))]\n        name = ('%s.%spercentile' % (self.name, int((p * 100))))\n        metrics.append(self.formatter(hostname=self.hostname, tags=self.tags, metric=name, value=val, timestamp=ts, metric_type=MetricTypes.GAUGE, interval=interval))\n    self.samples = []\n    self.count = 0\n    return metrics\n", "label": 0}
{"function": "\n\ndef _deepcopy(self, x, memo=None):\n    \"Deepcopy helper for the data dictionary or list.\\n\\n        Regular expressions cannot be deep copied but as they are immutable we\\n        don't have to copy them when cloning.\\n        \"\n    if (not hasattr(x, 'items')):\n        (y, is_list, iterator) = ([], True, enumerate(x))\n    else:\n        (y, is_list, iterator) = ({\n            \n        }, False, x.iteritems())\n    if (memo is None):\n        memo = {\n            \n        }\n    val_id = id(x)\n    if (val_id in memo):\n        return memo.get(val_id)\n    memo[val_id] = y\n    for (key, value) in iterator:\n        if (isinstance(value, (dict, list)) and (not isinstance(value, SON))):\n            value = self._deepcopy(value, memo)\n        elif (not isinstance(value, RE_TYPE)):\n            value = copy.deepcopy(value, memo)\n        if is_list:\n            y.append(value)\n        else:\n            if (not isinstance(key, RE_TYPE)):\n                key = copy.deepcopy(key, memo)\n            y[key] = value\n    return y\n", "label": 1}
{"function": "\n\ndef run(self, parent, blocks):\n    block = blocks.pop(0)\n    if block.strip():\n        if self.parser.state.isstate('list'):\n            sibling = self.lastChild(parent)\n            if (sibling is not None):\n                if sibling.tail:\n                    sibling.tail = ('%s\\n%s' % (sibling.tail, block))\n                else:\n                    sibling.tail = ('\\n%s' % block)\n            elif parent.text:\n                parent.text = ('%s\\n%s' % (parent.text, block))\n            else:\n                parent.text = block.lstrip()\n        else:\n            p = util.etree.SubElement(parent, 'p')\n            p.text = block.lstrip()\n", "label": 0}
{"function": "\n\ndef submit_metric(self, name, value, mtype, tags=None, hostname=None, device_name=None, timestamp=None, sample_rate=1):\n    hostname = (hostname if (hostname is not None) else self.hostname)\n    if (tags is None):\n        context = (name, tuple(), hostname, device_name)\n    else:\n        context = (name, tuple(sorted(set(tags))), hostname, device_name)\n    cur_time = time()\n    if ((timestamp is not None) and ((cur_time - int(timestamp)) > self.recent_point_threshold)):\n        log.debug(('Discarding %s - ts = %s , current ts = %s ' % (name, timestamp, cur_time)))\n        self.num_discarded_old_points += 1\n    else:\n        timestamp = (timestamp or cur_time)\n        bucket_start_timestamp = self.calculate_bucket_start(timestamp)\n        if (bucket_start_timestamp == self.current_bucket):\n            metric_by_context = self.current_mbc\n        else:\n            if (bucket_start_timestamp not in self.metric_by_bucket):\n                self.metric_by_bucket[bucket_start_timestamp] = {\n                    \n                }\n            metric_by_context = self.metric_by_bucket[bucket_start_timestamp]\n            self.current_bucket = bucket_start_timestamp\n            self.current_mbc = metric_by_context\n        if (context not in metric_by_context):\n            metric_class = self.metric_type_to_class[mtype]\n            metric_by_context[context] = metric_class(self.formatter, name, tags, hostname, device_name, self.metric_config.get(metric_class))\n        metric_by_context[context].sample(value, sample_rate, timestamp)\n", "label": 1}
{"function": "\n\ndef write(self, out, obj):\n    the_big_int = (((- obj) - 1) if (obj < 0) else obj)\n    end_index = ((- 1) if (type(obj) == long) else None)\n    hex_str = hex(the_big_int)[2:end_index]\n    if ((len(hex_str) % 2) == 1):\n        prefix = '0'\n        hex_str = (prefix + hex_str)\n    num_array = bytearray(binascii.unhexlify(bytearray(hex_str)))\n    if (obj < 0):\n        neg = bytearray()\n        for c in num_array:\n            neg.append((c ^ 255))\n        num_array = neg\n    out.write_byte_array(num_array)\n", "label": 0}
{"function": "\n\ndef generate_update(self, query):\n    model = query.model_class\n    alias_map = self.alias_map_class()\n    alias_map.add(model, model._meta.db_table)\n    if query._on_conflict:\n        statement = ('UPDATE OR %s' % query._on_conflict)\n    else:\n        statement = 'UPDATE'\n    clauses = [SQL(statement), model.as_entity(), SQL('SET')]\n    update = []\n    for (field, value) in self._sorted_fields(query._update):\n        if (not isinstance(value, (Node, Model))):\n            value = Param(value, conv=field.db_value)\n        update.append(Expression(field.as_entity(with_table=False), OP.EQ, value, flat=True))\n    clauses.append(CommaClause(*update))\n    if query._where:\n        clauses.extend([SQL('WHERE'), query._where])\n    if (query._returning is not None):\n        returning_clause = Clause(*query._returning)\n        returning_clause.glue = ', '\n        clauses.extend([SQL('RETURNING'), returning_clause])\n    return self.build_query(clauses, alias_map)\n", "label": 0}
{"function": "\n\ndef checkAuth():\n    'Check if a valid Session cookie is found and the auth token within is valid\\n    '\n    authed = False\n    indieauth_id = None\n    if (('indieauth_id' in session) and ('indieauth_token' in session)):\n        indieauth_id = session['indieauth_id']\n        indieauth_token = session['indieauth_token']\n        app.logger.info('session cookie found')\n        if (db is not None):\n            key = db.get(('token-%s' % indieauth_token))\n            if key:\n                data = db.hgetall(key)\n                if (data and (data['token'] == indieauth_token)):\n                    authed = True\n    return (authed, indieauth_id)\n", "label": 0}
{"function": "\n\ndef _register_model(admin, model):\n    if (not hasattr(admin, 'revision_manager')):\n        admin.revision_manager = default_revision_manager\n    if (not hasattr(admin, 'reversion_format')):\n        admin.reversion_format = 'json'\n    if (not admin.revision_manager.is_registered(model)):\n        inline_fields = []\n        for inline in getattr(admin, 'inlines', []):\n            inline_model = inline.model\n            if getattr(inline, 'generic_inline', False):\n                ct_field = getattr(inline, 'ct_field', 'content_type')\n                ct_fk_field = getattr(inline, 'ct_fk_field', 'object_id')\n                for field in model._meta.many_to_many:\n                    if (isinstance(field, GenericRelation) and (field.rel.to == inline_model) and (field.object_id_field_name == ct_fk_field) and (field.content_type_field_name == ct_field)):\n                        inline_fields.append(field.name)\n                _autoregister(admin, inline_model)\n            else:\n                fk_name = getattr(inline, 'fk_name', None)\n                if (not fk_name):\n                    for field in inline_model._meta.fields:\n                        if (isinstance(field, (models.ForeignKey, models.OneToOneField)) and issubclass(model, field.rel.to)):\n                            fk_name = field.name\n                _autoregister(admin, inline_model, follow=[fk_name])\n                if (not inline_model._meta.get_field(fk_name).rel.is_hidden()):\n                    accessor = inline_model._meta.get_field(fk_name).remote_field.get_accessor_name()\n                    inline_fields.append(accessor)\n        _autoregister(admin, model, inline_fields)\n", "label": 1}
{"function": "\n\ndef is_git_dir(d):\n    ' This is taken from the git setup.c:is_git_directory. '\n    isdir = os.path.isdir\n    join = os.path.join\n    isfile = os.path.isfile\n    islink = os.path.islink\n    if (isdir(d) and isdir(join(d, 'objects')) and isdir(join(d, 'refs'))):\n        headref = join(d, 'HEAD')\n        return (isfile(headref) or (islink(headref) and os.readlink(headref).startswith('refs')))\n    return False\n", "label": 0}
{"function": "\n\ndef report_failure(self, item):\n    item_dict = model_to_dict(item)\n    url = settings.MY_EQUIPMENT_REPORT_FAILURE_URL\n    if url:\n        placeholders = [k[1] for k in Formatter().parse(url) if (k[1] is not None)]\n        item_dict.update({k: getattr_dunder(item, k) for k in placeholders})\n        if (self.request and ('username' not in item_dict)):\n            item_dict['username'] = self.request.user.username\n\n        def escape_param(p):\n            '\\n                Escape URL param and replace quotation by unicode inches sign\\n                '\n            return quote(str(p).replace('\"', '\u2033'))\n        return '<a href=\"{}\" target=\"_blank\">{}</a><br />'.format(url.format(**{k: escape_param(v) for (k, v) in item_dict.items()}), _('Report failure'))\n    return ''\n", "label": 0}
{"function": "\n\ndef ndd_prefix_for_region(region_code, strip_non_digits):\n    'Returns the national dialling prefix for a specific region.\\n\\n    For example, this would be 1 for the United States, and 0 for New\\n    Zealand. Set strip_non_digits to True to strip symbols like \"~\" (which\\n    indicates a wait for a dialling tone) from the prefix returned. If no\\n    national prefix is present, we return None.\\n\\n    Warning: Do not use this method for do-your-own formatting - for some\\n    regions, the national dialling prefix is used only for certain types of\\n    numbers. Use the library\\'s formatting functions to prefix the national\\n    prefix when required.\\n\\n    Arguments:\\n    region_code -- The region that we want to get the dialling prefix for.\\n    strip_non_digits -- whether to strip non-digits from the national\\n               dialling prefix.\\n\\n    Returns the dialling prefix for the region denoted by region_code.\\n    '\n    if (region_code is None):\n        return None\n    metadata = PhoneMetadata.metadata_for_region(region_code.upper(), None)\n    if (metadata is None):\n        return None\n    national_prefix = metadata.national_prefix\n    if ((national_prefix is None) or (len(national_prefix) == 0)):\n        return None\n    if strip_non_digits:\n        national_prefix = re.sub(U_TILDE, U_EMPTY_STRING, national_prefix)\n    return national_prefix\n", "label": 0}
{"function": "\n\ndef getUpperLeftX(self, width, x, xPrev, xNext, xMin, xMax, xMid, xMouse):\n    if (AnnotationLocation.AT_THE_MOUSE == self.location):\n        if (GChartConsts.NAI == xMouse):\n            result = Double.NaN\n        else:\n            result = xMouse\n    elif (AnnotationLocation.AT_THE_MOUSE_SNAP_TO_X == self.location):\n        if (GChartConsts.NAI == xMouse):\n            result = Double.NaN\n        else:\n            result = x\n    elif (AnnotationLocation.AT_THE_MOUSE_SNAP_TO_Y == self.location):\n        if (GChartConsts.NAI == xMouse):\n            result = Double.NaN\n        else:\n            result = xMouse\n    elif ((AnnotationLocation.NORTHWEST == self.location) or (AnnotationLocation.WEST == self.location) or (AnnotationLocation.SOUTHWEST == self.location)):\n        result = xMin\n    elif ((AnnotationLocation.NORTHEAST == self.location) or (AnnotationLocation.EAST == self.location) or (AnnotationLocation.SOUTHEAST == self.location)):\n        result = xMax\n    else:\n        result = ((xMin + xMax) / 2)\n    return result\n", "label": 1}
{"function": "\n\ndef get_language_from_request(request, current_page=None):\n    '\\n    Return the most obvious language according the request\\n    '\n    language = None\n    if hasattr(request, 'POST'):\n        language = request.POST.get('language', None)\n    if (hasattr(request, 'GET') and (not language)):\n        language = request.GET.get('language', None)\n    site_id = (current_page.site_id if current_page else None)\n    if language:\n        language = get_language_code(language)\n        if (not (language in get_language_list(site_id))):\n            language = None\n    if (language is None):\n        language = get_language_code(getattr(request, 'LANGUAGE_CODE', None))\n    if language:\n        if (not (language in get_language_list(site_id))):\n            language = None\n    if ((language is None) and current_page):\n        languages = current_page.get_languages()\n        if (len(languages) > 0):\n            language = languages[0]\n    if (language is None):\n        language = get_default_language(site_id=site_id)\n    return language\n", "label": 1}
{"function": "\n\ndef step_3(w):\n    ' Step 3 replaces -ic, -ful, -ness etc. suffixes.\\n        This only happens if there is at least one vowel-consonant pair before the suffix.\\n    '\n    for (suffix, rules) in suffixes3:\n        if w.endswith(suffix):\n            for (A, B) in rules:\n                if w.endswith(A):\n                    return ((R1(w).endswith(A) and (w[:(- len(A))] + B)) or w)\n    return w\n", "label": 0}
{"function": "\n\ndef run(self, refresh_only=False):\n    repo = self.get_repo(silent=(True if refresh_only else False))\n    if (not repo):\n        return\n    title = (GIT_STATUS_VIEW_TITLE_PREFIX + os.path.basename(repo))\n    view = find_view_by_settings(self.window, git_view='status', git_repo=repo)\n    if ((not view) and (not refresh_only)):\n        view = self.window.new_file()\n        view.set_name(title)\n        view.set_syntax_file(GIT_STATUS_VIEW_SYNTAX)\n        view.set_scratch(True)\n        view.set_read_only(True)\n        view.settings().set('git_view', 'status')\n        view.settings().set('git_repo', repo)\n        view.settings().set('__vi_external_disable', (get_setting('git_status_disable_vintageous') is True))\n        for (key, val) in list(GIT_STATUS_VIEW_SETTINGS.items()):\n            view.settings().set(key, val)\n    if (view is not None):\n        self.window.focus_view(view)\n        view.run_command('git_status_refresh')\n", "label": 0}
{"function": "\n\n@ensure_tag(['p'])\ndef get_single_list_nodes_data(li, meta_data):\n    '\\n    Find consecutive li tags that have content that have the same list id.\\n    '\n    (yield li)\n    w_namespace = get_namespace(li, 'w')\n    current_numId = get_numId(li, w_namespace)\n    starting_ilvl = get_ilvl(li, w_namespace)\n    el = li\n    while True:\n        el = el.getnext()\n        if (el is None):\n            break\n        if (not has_text(el)):\n            continue\n        if _is_top_level_upper_roman(el, meta_data):\n            break\n        if (is_li(el, meta_data) and (starting_ilvl > get_ilvl(el, w_namespace))):\n            break\n        new_numId = get_numId(el, w_namespace)\n        if ((new_numId is None) or (new_numId == (- 1))):\n            (yield el)\n            continue\n        if (current_numId != new_numId):\n            break\n        if is_last_li(el, meta_data, current_numId):\n            (yield el)\n            break\n        (yield el)\n", "label": 1}
{"function": "\n\ndef func(self):\n    '\\n        Creates the object.\\n        '\n    caller = self.caller\n    if (not self.args):\n        string = 'Usage: @create[/drop] <newname>[;alias;alias...] [:typeclass_path]'\n        caller.msg(string)\n        return\n    for objdef in self.lhs_objs:\n        string = ''\n        name = objdef['name']\n        aliases = objdef['aliases']\n        typeclass = objdef['option']\n        lockstring = ('control:id(%s);delete:id(%s) or perm(Wizards)' % (caller.id, caller.id))\n        obj = create.create_object(typeclass, name, caller, home=caller, aliases=aliases, locks=lockstring, report_to=caller)\n        if (not obj):\n            continue\n        if aliases:\n            string = 'You create a new %s: %s (aliases: %s).'\n            string = (string % (obj.typename, obj.name, ', '.join(aliases)))\n        else:\n            string = 'You create a new %s: %s.'\n            string = (string % (obj.typename, obj.name))\n        if (not obj.db.desc):\n            obj.db.desc = 'You see nothing special.'\n        if ('drop' in self.switches):\n            if caller.location:\n                obj.home = caller.location\n                obj.move_to(caller.location, quiet=True)\n    if string:\n        caller.msg(string)\n", "label": 1}
{"function": "\n\ndef _match_pattern(self):\n    buf = []\n    while ((self.c != EOF) and (self.c != self.delimiter)):\n        if (self.c == '\\\\'):\n            buf.append(self.c)\n            self.consume()\n            if (self.c == self.delimiter):\n                buf[(- 1)] = self.delimiter\n                self.consume()\n            if (self.c in '\\\\'):\n                buf.append(self.c)\n                self.consume()\n            if (self.c == EOF):\n                break\n        else:\n            buf.append(self.c)\n            self.consume()\n    return ''.join(buf)\n", "label": 0}
{"function": "\n\ndef star_help(cluster=None, logdir=None, cmdline=None, *args):\n    'Help on *commands'\n    if args:\n        for cmd in args:\n            if (not (cmd[0] == '*')):\n                cmd = ('*' + cmd)\n            if (cmd in commands):\n                print('Help for', cmd)\n                print(commands[cmd].help_text)\n            else:\n                print('Unknown command:', cmd)\n        return\n    for cmd in sorted(commands.keys()):\n        func = commands[cmd]\n        if (not isinstance(func, StarCommand)):\n            print(('%s (old-style) - %s' % (cmd, func.__doc__)))\n            continue\n        if func.version:\n            print(('%s (%s) - %s' % (cmd, func.version, func.synopsis)))\n        else:\n            print(('%s - %s' % (cmd, func.synopsis)))\n", "label": 0}
{"function": "\n\ndef hrn_to_urn(hrn, type=None):\n    '\\n    convert an hrn and type to a urn string\\n    '\n    if ((not hrn) or hrn.startswith(URN_PREFIX)):\n        return hrn\n    authority = get_authority(hrn)\n    name = get_leaf(hrn)\n    if (type == 'authority'):\n        authority = hrn\n        name = 'sa'\n    if authority.startswith('plc'):\n        if (type == None):\n            urn = '+'.join(['', authority.replace('.', ':'), name])\n        else:\n            urn = '+'.join(['', authority.replace('.', ':'), type, name])\n    else:\n        urn = '+'.join(['', authority, type, name])\n    return (URN_PREFIX + urn)\n", "label": 0}
{"function": "\n\ndef test_TypeInfo(self):\n    tlib = LoadTypeLibEx('shdocvw.dll')\n    for index in range(tlib.GetTypeInfoCount()):\n        ti = tlib.GetTypeInfo(index)\n        ta = ti.GetTypeAttr()\n        ti.GetDocumentation((- 1))\n        if (ta.typekind in (TKIND_INTERFACE, TKIND_DISPATCH)):\n            if ta.cImplTypes:\n                href = ti.GetRefTypeOfImplType(0)\n                base = ti.GetRefTypeInfo(href)\n                base.GetDocumentation((- 1))\n                ti.GetImplTypeFlags(0)\n        for f in range(ta.cFuncs):\n            fd = ti.GetFuncDesc(f)\n            names = ti.GetNames(fd.memid, 32)\n            ti.GetIDsOfNames(*names)\n            ti.GetMops(fd.memid)\n        for v in range(ta.cVars):\n            ti.GetVarDesc(v)\n", "label": 0}
{"function": "\n\ndef _send_request(self, method, url, body, headers):\n    header_names = dict.fromkeys([k.lower() for k in headers])\n    skips = {\n        \n    }\n    if ('host' in header_names):\n        skips['skip_host'] = 1\n    if ('accept-encoding' in header_names):\n        skips['skip_accept_encoding'] = 1\n    self.putrequest(method, url, **skips)\n    if ((body is not None) and ('content-length' not in header_names)):\n        self._set_content_length(body)\n    for (hdr, value) in headers.items():\n        self.putheader(hdr, value)\n    if isinstance(body, str):\n        body = body.encode('iso-8859-1')\n    self.endheaders(body)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef create_component(collection):\n    'Present user with a dialog to define and add new components.\\n\\n        Parameters\\n        ----------\\n        collection : A `DataCollection` to edit\\n        '\n    widget = CustomComponentWidget(collection)\n    while True:\n        widget.ui.show()\n        if (widget.ui.exec_() == QtGui.QDialog.Accepted):\n            if (len(str(widget.ui.expression.toPlainText())) == 0):\n                QtGui.QMessageBox.critical(widget.ui, 'Error', 'No expression set', buttons=QtGui.QMessageBox.Ok)\n            elif (widget._number_targets == 0):\n                QtGui.QMessageBox.critical(widget.ui, 'Error', 'Please specify the target dataset(s)', buttons=QtGui.QMessageBox.Ok)\n            elif (len(widget.ui.new_label.text()) == 0):\n                QtGui.QMessageBox.critical(widget.ui, 'Error', 'Please specify the new component name', buttons=QtGui.QMessageBox.Ok)\n            else:\n                link = widget._create_link()\n                if link:\n                    widget._add_link_to_targets(link)\n                break\n        else:\n            break\n", "label": 0}
{"function": "\n\ndef world_gen(name, width, height, seed, temps=[0.874, 0.765, 0.594, 0.439, 0.366, 0.124], humids=[0.941, 0.778, 0.507, 0.236, 0.073, 0.014, 0.002], num_plates=10, ocean_level=1.0, step=Step.full(), gamma_curve=1.25, curve_offset=0.2, fade_borders=True, verbose=get_verbose()):\n    if verbose:\n        start_time = time.time()\n    world = _plates_simulation(name, width, height, seed, temps, humids, gamma_curve, curve_offset, num_plates, ocean_level, step, verbose)\n    center_land(world)\n    if verbose:\n        elapsed_time = (time.time() - start_time)\n        print(((('...plates.world_gen: set_elevation, set_plates, center_land ' + 'complete. Elapsed time ') + str(elapsed_time)) + ' seconds.'))\n    if verbose:\n        start_time = time.time()\n    add_noise_to_elevation(world, numpy.random.randint(0, 4096))\n    if verbose:\n        elapsed_time = (time.time() - start_time)\n        print((('...plates.world_gen: elevation noise added. Elapsed time ' + str(elapsed_time)) + ' seconds.'))\n    if verbose:\n        start_time = time.time()\n    if fade_borders:\n        place_oceans_at_map_borders(world)\n    initialize_ocean_and_thresholds(world)\n    if verbose:\n        elapsed_time = (time.time() - start_time)\n        print((('...plates.world_gen: oceans initialized. Elapsed time ' + str(elapsed_time)) + ' seconds.'))\n    return generate_world(world, step)\n", "label": 0}
{"function": "\n\ndef list_security_groups(self, **_params):\n    ret = []\n    for security_group in self._fake_security_groups.values():\n        names = _params.get('name')\n        if names:\n            if (not isinstance(names, list)):\n                names = [names]\n            for name in names:\n                if (security_group.get('name') == name):\n                    ret.append(security_group)\n        ids = _params.get('id')\n        if ids:\n            if (not isinstance(ids, list)):\n                ids = [ids]\n            for id in ids:\n                if (security_group.get('id') == id):\n                    ret.append(security_group)\n        elif (not (names or ids)):\n            ret.append(security_group)\n    return {\n        'security_groups': ret,\n    }\n", "label": 1}
{"function": "\n\ndef _handle_sync_response(self, node, response, info, broker, http, different_region):\n    parent = super(ContainerReplicator, self)\n    if is_success(response.status):\n        remote_info = json.loads(response.data)\n        if incorrect_policy_index(info, remote_info):\n            status_changed_at = Timestamp(time.time())\n            broker.set_storage_policy_index(remote_info['storage_policy_index'], timestamp=status_changed_at.internal)\n        sync_timestamps = ('created_at', 'put_timestamp', 'delete_timestamp')\n        if any(((info[key] != remote_info[key]) for key in sync_timestamps)):\n            broker.merge_timestamps(*(remote_info[key] for key in sync_timestamps))\n    rv = parent._handle_sync_response(node, response, info, broker, http, different_region)\n    return rv\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.what = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.why = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef func(self):\n    '\\n        inp is the dict produced in ObjManipCommand.parse()\\n        '\n    caller = self.caller\n    if (not self.args):\n        caller.msg('Usage: @wipe <object>[/attribute/attribute...]')\n        return\n    objname = self.lhs_objattr[0]['name']\n    attrs = self.lhs_objattr[0]['attrs']\n    obj = caller.search(objname)\n    if (not obj):\n        return\n    if (not obj.access(caller, 'edit')):\n        caller.msg('You are not allowed to do that.')\n        return\n    if (not attrs):\n        obj.attributes.clear()\n        string = ('Wiped all attributes on %s.' % obj.name)\n    else:\n        for attrname in attrs:\n            obj.attributes.remove(attrname)\n        string = 'Wiped attributes %s on %s.'\n        string = (string % (','.join(attrs), obj.name))\n    caller.msg(string)\n", "label": 0}
{"function": "\n\ndef __menuDefinition(self):\n    result = IECore.MenuDefinition()\n    if (self.getPlug() is None):\n        return result\n    formats = [GafferImage.Format.format(n) for n in GafferImage.Format.registeredFormats()]\n    if (not self.getPlug().ancestor(Gaffer.ScriptNode).isSame(self.getPlug().node())):\n        formats.insert(0, GafferImage.Format())\n    currentFormat = self.getPlug().getValue()\n    modeIsCustom = (Gaffer.Metadata.plugValue(self.getPlug(), 'formatPlugValueWidget:mode') == 'custom')\n    for fmt in formats:\n        result.append(('/' + self.__formatLabel(fmt)), {\n            'command': functools.partial(Gaffer.WeakMethod(self.__applyFormat), fmt=fmt),\n            'checkBox': ((fmt == currentFormat) and (not modeIsCustom)),\n        })\n    result.append('/CustomDivider', {\n        'divider': True,\n    })\n    result.append('/Custom', {\n        'command': Gaffer.WeakMethod(self.__applyCustomFormat),\n        'checkBox': (modeIsCustom or (currentFormat not in formats)),\n    })\n    return result\n", "label": 0}
{"function": "\n\ndef holidays(self, start=None, end=None, return_name=False):\n    '\\n        Returns a curve with holidays between start_date and end_date\\n\\n        Parameters\\n        ----------\\n        start : starting date, datetime-like, optional\\n        end : ending date, datetime-like, optional\\n        return_names : bool, optional\\n            If True, return a series that has dates and holiday names.\\n            False will only return a DatetimeIndex of dates.\\n\\n        Returns\\n        -------\\n            DatetimeIndex of holidays\\n        '\n    if (self.rules is None):\n        raise Exception(('Holiday Calendar %s does not have any rules specified' % self.name))\n    if (start is None):\n        start = AbstractHolidayCalendar.start_date\n    if (end is None):\n        end = AbstractHolidayCalendar.end_date\n    start = Timestamp(start)\n    end = Timestamp(end)\n    holidays = None\n    if ((self._cache is None) or (start < self._cache[0]) or (end > self._cache[1])):\n        for rule in self.rules:\n            rule_holidays = rule.dates(start, end, return_name=True)\n            if (holidays is None):\n                holidays = rule_holidays\n            else:\n                holidays = holidays.append(rule_holidays)\n        self._cache = (start, end, holidays.sort_index())\n    holidays = self._cache[2]\n    holidays = holidays[start:end]\n    if return_name:\n        return holidays\n    else:\n        return holidays.index\n", "label": 1}
{"function": "\n\ndef __init__(self, authority=None, type=None, name=None, urn=None):\n    if (not (urn is None)):\n        if (not is_valid_urn(urn)):\n            raise ValueError(('Invalid URN %s' % urn))\n        spl = urn.split('+')\n        self.authority = urn_to_string_format(spl[1])\n        self.type = urn_to_string_format(spl[2])\n        self.name = urn_to_string_format('+'.join(spl[3:]))\n        self.urn = urn\n    else:\n        if ((not authority) or (not type) or (not name)):\n            raise ValueError('Must provide either all of authority, type, and name, or a urn must be provided')\n        for i in [authority, type, name]:\n            if (i.strip() == ''):\n                raise ValueError('Parameter to create_urn was empty string')\n        self.authority = authority\n        self.type = type\n        self.name = name\n        if (not is_valid_urn_string(authority)):\n            authority = string_to_urn_format(authority)\n        if (not is_valid_urn_string(type)):\n            type = string_to_urn_format(type)\n        if (not is_valid_urn_string(name)):\n            name = string_to_urn_format(name)\n        self.urn = ('%s+%s+%s+%s' % (Xrn.URN_PREFIX, authority, type, name))\n        if (not is_valid_urn(self.urn)):\n            raise ValueError(('Failed to create valid URN from args %s, %s, %s' % (self.authority, self.type, self.name)))\n", "label": 1}
{"function": "\n\ndef nits(self):\n    with_contexts = set(self.iter_ast_types(ast.With))\n    with_context_calls = set((node.context_expr for node in with_contexts if isinstance(node.context_expr, ast.Call)))\n    for call in self.iter_ast_types(ast.Call):\n        if (isinstance(call.func, ast.Name) and (call.func.id == 'open') and (call not in with_context_calls)):\n            (yield self.warning('T802', 'open() calls should be made within a contextmanager.', call))\n", "label": 0}
{"function": "\n\ndef __getitem__(self, args):\n    if (not isinstance(args, tuple)):\n        args = (args,)\n    if (len(args) == 2):\n        scalar_arg_1 = (numpy.isscalar(args[0]) or (getattr(args[0], 'type', None) == tensor.iscalar))\n        scalar_arg_2 = (numpy.isscalar(args[1]) or (getattr(args[1], 'type', None) == tensor.iscalar))\n        if (scalar_arg_1 and scalar_arg_2):\n            ret = get_item_scalar(self, args)\n        elif isinstance(args[0], list):\n            ret = get_item_2lists(self, args[0], args[1])\n        else:\n            ret = get_item_2d(self, args)\n    elif isinstance(args[0], list):\n        ret = get_item_list(self, args[0])\n    else:\n        ret = get_item_2d(self, args)\n    return ret\n", "label": 1}
{"function": "\n\ndef parse_args(self, env, args=None, namespace=None):\n    self.env = env\n    (self.args, no_options) = super(Parser, self).parse_known_args(args, namespace)\n    if self.args.debug:\n        self.args.traceback = True\n    self._apply_no_options(no_options)\n    self._apply_config()\n    self._validate_download_options()\n    self._setup_standard_streams()\n    self._process_output_options()\n    self._process_pretty_options()\n    self._guess_method()\n    self._parse_items()\n    if ((not self.args.ignore_stdin) and (not env.stdin_isatty)):\n        self._body_from_file(self.env.stdin)\n    if (not self.args.url.startswith((HTTP, HTTPS))):\n        scheme = (HTTPS if (self.env.progname == 'https') else HTTP)\n        self.args.url = (scheme + self.args.url)\n    self._process_auth()\n    return self.args\n", "label": 0}
{"function": "\n\ndef scan_directory(pym, directory, sentinel, installed, depth=0):\n    'Entry point scan that creates a PyModule instance if needed.\\n    '\n    if (not pym):\n        d = os.path.abspath(directory)\n        basename = os.path.basename(d)\n        pym = utils.find_package(basename, installed)\n        if (not pym):\n            version = 'DIRECTORY'\n            if os.path.isfile(os.path.join(d, '__init__.py')):\n                version = 'MODULE'\n            pym = PyModule(basename, version, d)\n            installed.insert(0, pym)\n        else:\n            pym.is_scan = True\n    bad_scans = 0\n    for item in _scan_directory(directory, sentinel, depth):\n        if os.path.isfile(item):\n            if (bad_scans > 100):\n                log.debug('Stopping scan of directory since it looks like a data dump: %s', directory)\n                break\n            if (not scan_file(pym, item, sentinel, installed)):\n                bad_scans += 1\n            else:\n                bad_scans = 0\n        elif os.path.isdir(item):\n            scan_directory(pym, item, sentinel, installed, (depth + 1))\n    return pym\n", "label": 1}
{"function": "\n\ndef locked_get(self):\n    'Retrieve Credential from datastore.\\n\\n    Returns:\\n      oauth2client.Credentials\\n    '\n    if self._cache:\n        json = self._cache.get(self._key_name)\n        if json:\n            return Credentials.new_from_json(json)\n    credential = None\n    entity = self._model.get_by_key_name(self._key_name)\n    if (entity is not None):\n        credential = getattr(entity, self._property_name)\n        if (credential and hasattr(credential, 'set_store')):\n            credential.set_store(self)\n            if self._cache:\n                self._cache.set(self._key_name, credentials.to_json())\n    return credential\n", "label": 0}
{"function": "\n\ndef _textNodes(self, recurse=False):\n    if (self.text and (getattr(self, 'xValid', 0) != VALID_NO_CONTENT)):\n        (yield self.text)\n    for c in self.iterchildren():\n        if (recurse and isinstance(c, ModelObject)):\n            for nestedText in c._textNodes(recurse):\n                (yield nestedText)\n        if (c.tail and (getattr(self, 'xValid', 0) != VALID_NO_CONTENT)):\n            (yield c.tail)\n", "label": 1}
{"function": "\n\ndef _has_changed(self, initial, data):\n    if (initial is None):\n        initial = []\n    if (data is None):\n        data = []\n    if (initial != data):\n        return True\n    initial_set = set([force_text(value) for value in initial])\n    data_set = set([force_text(value) for value in data])\n    return (data_set != initial_set)\n", "label": 0}
{"function": "\n\ndef _logm_force_nonsingular_triangular_matrix(T, inplace=False):\n    tri_eps = 1e-20\n    abs_diag = np.absolute(np.diag(T))\n    if np.any((abs_diag == 0)):\n        exact_singularity_msg = 'The logm input matrix is exactly singular.'\n        warnings.warn(exact_singularity_msg, LogmExactlySingularWarning)\n        if (not inplace):\n            T = T.copy()\n        n = T.shape[0]\n        for i in range(n):\n            if (not T[(i, i)]):\n                T[(i, i)] = tri_eps\n    elif np.any((abs_diag < tri_eps)):\n        near_singularity_msg = 'The logm input matrix may be nearly singular.'\n        warnings.warn(near_singularity_msg, LogmNearlySingularWarning)\n    return T\n", "label": 0}
{"function": "\n\ndef _validate_counts_matrix(counts, ids=None, suppress_cast=False):\n    results = []\n    if ((len(counts) == 0) or (not isinstance(counts[0], collections.Iterable))):\n        counts = [counts]\n    counts = np.asarray(counts)\n    if (counts.ndim > 2):\n        raise ValueError(('Only 1-D and 2-D array-like objects can be provided as input. Provided object has %d dimensions.' % counts.ndim))\n    if ((ids is not None) and (len(counts) != len(ids))):\n        raise ValueError('Number of rows in ``counts`` must be equal to number of provided ``ids``.')\n    lens = []\n    for v in counts:\n        results.append(_validate_counts_vector(v, suppress_cast))\n        lens.append(len(v))\n    if (len(set(lens)) > 1):\n        raise ValueError('All rows in ``counts`` must be of equal length.')\n    return np.asarray(results)\n", "label": 0}
{"function": "\n\n@classmethod\ndef parse(cls, value, kwds):\n    if isinstance(value, str):\n        kwds['parse_tree'] = address_list = cls.value_parser(value)\n        groups = []\n        for addr in address_list.addresses:\n            groups.append(Group(addr.display_name, [Address((mb.display_name or ''), (mb.local_part or ''), (mb.domain or '')) for mb in addr.all_mailboxes]))\n        defects = list(address_list.all_defects)\n    else:\n        if (not hasattr(value, '__iter__')):\n            value = [value]\n        groups = [(Group(None, [item]) if (not hasattr(item, 'addresses')) else item) for item in value]\n        defects = []\n    kwds['groups'] = groups\n    kwds['defects'] = defects\n    kwds['decoded'] = ', '.join([str(item) for item in groups])\n    if ('parse_tree' not in kwds):\n        kwds['parse_tree'] = cls.value_parser(kwds['decoded'])\n", "label": 1}
{"function": "\n\ndef parse_args(argv, defaults=None, expected_argc=0):\n    opts = {\n        \n    }\n    if defaults:\n        opts.update(defaults)\n    args = []\n    i = 0\n    while (i < len(argv)):\n        if (argv[i][0] == '-'):\n            arg = argv[i].lstrip('-')\n            if ('=' in arg):\n                (k, v) = arg.split('=', 2)\n                if (k in defaults):\n                    if (type(defaults[k]) == float):\n                        opts[k] = float(v)\n                    elif (type(defaults[k]) == int):\n                        opts[k] = int(v)\n                    else:\n                        opts[k] = v\n            else:\n                opts[arg] = True\n        else:\n            args.append(argv[i])\n        i += 1\n    while (len(args) < expected_argc):\n        args.append(None)\n    return (opts, args)\n", "label": 1}
{"function": "\n\ndef assocunify(u, v, s, eq=core.eq, n=None):\n    ' Associative Unification\\n\\n    See Also:\\n        eq_assoccomm\\n    '\n    (uop, uargs) = op_args(u)\n    (vop, vargs) = op_args(v)\n    if ((not uop) and (not vop)):\n        res = unify(u, v, s)\n        if (res is not False):\n            return (res,)\n    if (uop and vop):\n        s = unify(uop, vop, s)\n        if (s is False):\n            raise StopIteration()\n        op = walk(uop, s)\n        (sm, lg) = ((uargs, vargs) if (len(uargs) <= len(vargs)) else (vargs, uargs))\n        ops = assocsized(op, lg, len(sm))\n        goal = condeseq(([(eq, a, b) for (a, b) in zip(sm, lg2)] for lg2 in ops))\n        return goaleval(goal)(s)\n    if uop:\n        (op, tail) = (uop, uargs)\n        b = v\n    if vop:\n        (op, tail) = (vop, vargs)\n        b = u\n    ns = ([n] if n else range(2, (len(tail) + 1)))\n    knowns = (build(op, x) for n in ns for x in assocsized(op, tail, n))\n    goal = condeseq(([(core.eq, b, k)] for k in knowns))\n    return goaleval(goal)(s)\n", "label": 1}
{"function": "\n\ndef _get_call_class(method):\n    'Find the call class for method if it exists else create one.'\n    (call_base, call_name) = method.split('.', 1)\n    mod = __import__('ubersmith.calls.{0}'.format(call_base), fromlist=[''])\n    gen = (getattr(mod, x) for x in dir(mod) if (not x.startswith('_')))\n    gen = (x for x in gen if ((type(x) is type) and issubclass(x, BaseCall)))\n    for call_class in gen:\n        if (call_class.method == method):\n            return call_class\n    else:\n\n        class GenericCall(BaseCall):\n            method = '.'.join((call_base, call_name))\n        return GenericCall\n", "label": 1}
{"function": "\n\ndef _get_params(mapper_spec, allowed_keys=None):\n    'Obtain output writer parameters.\\n\\n  Utility function for output writer implementation. Fetches parameters\\n  from mapreduce specification giving appropriate usage warnings.\\n\\n  Args:\\n    mapper_spec: The MapperSpec for the job\\n    allowed_keys: set of all allowed keys in parameters as strings. If it is not\\n      None, then parameters are expected to be in a separate \"output_writer\"\\n      subdictionary of mapper_spec parameters.\\n\\n  Returns:\\n    mapper parameters as dict\\n\\n  Raises:\\n    BadWriterParamsError: if parameters are invalid/missing or not allowed.\\n  '\n    if ('output_writer' not in mapper_spec.params):\n        message = \"Output writer's parameters should be specified in output_writer subdictionary.\"\n        if allowed_keys:\n            raise errors.BadWriterParamsError(message)\n        params = mapper_spec.params\n        params = dict(((str(n), v) for (n, v) in params.iteritems()))\n    else:\n        if (not isinstance(mapper_spec.params.get('output_writer'), dict)):\n            raise errors.BadWriterParamsError('Output writer parameters should be a dictionary')\n        params = mapper_spec.params.get('output_writer')\n        params = dict(((str(n), v) for (n, v) in params.iteritems()))\n        if allowed_keys:\n            params_diff = (set(params.keys()) - allowed_keys)\n            if params_diff:\n                raise errors.BadWriterParamsError(('Invalid output_writer parameters: %s' % ','.join(params_diff)))\n    return params\n", "label": 0}
{"function": "\n\ndef post(self):\n    if (not may(CREATE)):\n        raise Forbidden()\n    f = request.files.get('file')\n    t = request.form.get('text')\n    if (f and f.filename):\n        if ContentRange.from_request():\n            abort(416)\n        content_type = (f.headers.get('Content-Type') or request.headers.get('Content-Type'))\n        content_type_hint = 'application/octet-stream'\n        filename = f.filename\n        f.seek(0, os.SEEK_END)\n        size = f.tell()\n        f.seek(0)\n    elif (t is not None):\n        t = t.encode('utf-8')\n        content_type = request.form.get('contenttype')\n        content_type_hint = 'text/plain'\n        size = len(t)\n        f = BytesIO(t)\n        filename = request.form.get('filename')\n    else:\n        raise NotImplementedError\n    maxlife_unit = request.form.get('maxlife-unit', 'forever').upper()\n    maxlife_value = int(request.form.get('maxlife-value', 1))\n    maxtime = time_unit_to_sec(maxlife_value, maxlife_unit)\n    maxlife_timestamp = ((int(time.time()) + maxtime) if (maxtime > 0) else maxtime)\n    name = create_item(f, filename, size, content_type, content_type_hint, maxlife_stamp=maxlife_timestamp)\n    return redirect_next('bepasty.display', name=name, _anchor=url_quote(filename))\n", "label": 0}
{"function": "\n\ndef __virtual__():\n    short_name = __name__.rsplit('.')[(- 1)]\n    mod_opts = __opts__.get(short_name, {\n        \n    })\n    if mod_opts:\n        if ((not cpy_error) and ('port' in mod_opts)):\n            return __virtualname__\n        if cpy_error:\n            from distutils.version import LooseVersion as V\n            if (('cherrypy' in globals()) and (V(cherrypy.__version__) < V(cpy_min))):\n                error_msg = 'Required version of CherryPy is {0} or greater.'.format(cpy_min)\n            else:\n                error_msg = cpy_error\n            logger.error(\"Not loading '%s'. Error loading CherryPy: %s\", __name__, error_msg)\n        if ('port' not in mod_opts):\n            logger.error(\"Not loading '%s'. 'port' not specified in config\", __name__)\n    return False\n", "label": 0}
{"function": "\n\ndef build(self, input_shape):\n    input_dim = input_shape[1]\n    self.input_spec = [InputSpec(dtype=K.floatx(), shape=(None, input_dim))]\n    self.W = self.init((input_dim, input_dim), name='{}_W'.format(self.name))\n    self.W_carry = self.init((input_dim, input_dim), name='{}_W_carry'.format(self.name))\n    if self.bias:\n        self.b = K.zeros((input_dim,), name='{}_b'.format(self.name))\n        self.b_carry = K.variable((np.ones((input_dim,)) * self.transform_bias), name='{}_b_carry'.format(self.name))\n        self.trainable_weights = [self.W, self.b, self.W_carry, self.b_carry]\n    else:\n        self.trainable_weights = [self.W, self.W_carry]\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if (self.bias and self.b_regularizer):\n        self.b_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    if self.activity_regularizer:\n        self.activity_regularizer.set_layer(self)\n        self.regularizers.append(self.activity_regularizer)\n    self.constraints = {\n        \n    }\n    if self.W_constraint:\n        self.constraints[self.W] = self.W_constraint\n    if (self.bias and self.b_constraint):\n        self.constraints[self.b] = self.b_constraint\n    if (self.initial_weights is not None):\n        self.set_weights(self.initial_weights)\n        del self.initial_weights\n", "label": 1}
{"function": "\n\n@property\ndef layers_to_optimize(self):\n    lto = []\n    for l in self.layers:\n        if isinstance(l, LayerContainer):\n            lto += l.layers_to_optimize\n        elif l.has_params:\n            if (hasattr(l, 'init') and (l.init.name == 'Identity')):\n                continue\n            lto.append(l)\n    return lto\n", "label": 0}
{"function": "\n\n@classmethod\ndef _validate(cls, name, config_dict):\n    '\\n        Raises a ConfigValidationError in case of an invalid configuration.\\n\\n        :type name: str\\n        :type config_dict: dict\\n        :rtype: None\\n        '\n    required_fields = {COMMANDS, ATOMIZERS}\n    allowed_fields_expected_types = {\n        SETUP_BUILD: [(list, str)],\n        TEARDOWN_BUILD: [(list, str)],\n        COMMANDS: [(list, str)],\n        ATOMIZERS: [(list, dict)],\n        MAX_EXECUTORS: [int],\n        MAX_EXECUTORS_PER_SLAVE: [int],\n    }\n    if (not isinstance(config_dict, dict)):\n        raise ConfigValidationError('Passed in configuration is not a dictionary for job: \"{}\".'.format(name))\n    missing_required_fields = (required_fields - config_dict.keys())\n    if missing_required_fields:\n        raise ConfigValidationError('Definition for job \"{}\" is missing required config sections: {}'.format(name, missing_required_fields))\n    for (config_section_name, config_section_value) in config_dict.items():\n        if (config_section_name not in allowed_fields_expected_types):\n            raise ConfigValidationError('Definition for job \"{}\" contains an invalid config section \"{}\".'.format(name, config_section_name))\n        expected_section_types = allowed_fields_expected_types[config_section_name]\n        actual_section_type = type(config_section_value)\n        if (actual_section_type is list):\n            actual_section_type = (list, type(config_section_value[0]))\n        if (actual_section_type not in expected_section_types):\n            raise ConfigValidationError('Definition for job \"{}\" contains an invalid value for config section \"{}\". Parser expected one of {} but found {}.'.format(name, config_section_name, expected_section_types, actual_section_type))\n", "label": 0}
{"function": "\n\ndef _handle_request_exception(self, e):\n    if isinstance(e, Finish):\n        if (not self._finished):\n            self.finish()\n        return\n    self.log_exception(*sys.exc_info())\n    if self._finished:\n        return\n    if isinstance(e, HTTPError):\n        if ((e.status_code not in httputil.responses) and (not e.reason)):\n            gen_log.error('Bad HTTP status code: %d', e.status_code)\n            self.send_error(500, exc_info=sys.exc_info())\n        else:\n            self.send_error(e.status_code, exc_info=sys.exc_info())\n    else:\n        self.send_error(500, exc_info=sys.exc_info())\n", "label": 0}
{"function": "\n\ndef wrap_salts_in_raw(self, host, hostvars):\n    if ('vault_wordpress_sites' in hostvars):\n        for (name, site) in hostvars['vault_wordpress_sites'].iteritems():\n            for (key, value) in site['env'].iteritems():\n                if (key.endswith(('_key', '_salt')) and (not value.startswith(('{% raw', '{%raw')))):\n                    hostvars['vault_wordpress_sites'][name]['env'][key] = ''.join(['{% raw %}', value, '{% endraw %}'])\n        host.vars['vault_wordpress_sites'] = hostvars['vault_wordpress_sites']\n", "label": 0}
{"function": "\n\ndef read(s, cc):\n    f = cc(s)\n    fd = f.params[0]\n    buf = f.params[1]\n    size = f.params[2]\n    s.log.function_call(f, 'read(fd={}, ptr={}, size={})', fd, buf, size)\n    output = OutputBuffer(s, buf)\n    if fd.symbolic:\n        raise ValueError('wtf')\n    if (fd.value > len(s.files)):\n        return f.ret(value=0)\n    else:\n        file = s.files[fd.value]\n        offset = file['offset']\n        output = OutputBuffer(s, buf)\n        real_fd = None\n        if (file['path'] not in ['stdin', 'stdout', 'stderr']):\n            real_fd = open(file['path'], 'rb')\n        if size.symbolic:\n            raise NotImplementedError()\n        elif (real_fd is None):\n            for i in xrange(0, size.value):\n                b = bv.Symbol(8, 'file_{}_{:x}'.format(fd.value, offset))\n                output.append(b)\n                file['bytes'][offset] = b\n                offset += 1\n        else:\n            real_fd.seek(offset, 0)\n            for i in range(0, size.value):\n                byte = real_fd.read(1)\n                if (len(byte) == 1):\n                    if (byte == '#'):\n                        b = bv.Symbol(8, 'file_{}_{:x}'.format(fd.value, offset))\n                    else:\n                        b = bv.Constant(8, ord(byte))\n                    output.append(b)\n                    file['bytes'][offset] = b\n                    offset += 1\n                else:\n                    break\n        file['offset'] = offset\n        if (real_fd is not None):\n            real_fd.close()\n    return f.ret(value=size)\n", "label": 1}
{"function": "\n\ndef findall(root, xpath, attribute_name=None, attribute_value=None):\n    'Find elements recursively from given root element based on\\n    xpath and possibly given attribute\\n\\n    :param root: Element root element where to start search\\n    :param xpath: xpath defintion, like {http://foo/bar/namespace}ElementName\\n    :param attribute_name: name of possible attribute of given element\\n    :param attribute_value: value of the attribute\\n    :return: list of elements or None\\n    '\n    found_elements = []\n    if (((2, 6) == sys.version_info[0:2]) and (which_etree() != 'lxml.etree')):\n        elements = root.getiterator(xpath)\n        if ((attribute_name is not None) and (attribute_value is not None)):\n            for element in elements:\n                if (element.attrib.get(attribute_name) == attribute_value):\n                    found_elements.append(element)\n        else:\n            found_elements = elements\n    else:\n        if ((attribute_name is not None) and (attribute_value is not None)):\n            xpath = ('%s[@%s=\"%s\"]' % (xpath, attribute_name, attribute_value))\n        found_elements = root.findall(('.//' + xpath))\n    if (found_elements == []):\n        found_elements = None\n    return found_elements\n", "label": 1}
{"function": "\n\n@team_required\n@login_required\ndef team_apply(request):\n    team = request.team\n    state = team.state_for(request.user)\n    if ((team.manager_access == Team.MEMBER_ACCESS_INVITATION) and (state is None) and (not request.user.is_staff)):\n        raise Http404()\n    if (team.can_apply(request.user) and (request.method == 'POST')):\n        (membership, created) = Membership.objects.get_or_create(team=team, user=request.user)\n        membership.state = Membership.STATE_APPLIED\n        membership.save()\n        messages.success(request, MESSAGE_STRINGS['applied-to-join'])\n    return redirect('team_detail', slug=team.slug)\n", "label": 0}
{"function": "\n\ndef GenerateCommand(self):\n    'Generate the actual commands to execute.\\n\\n    Some special casing is needed here per-class type because of the original\\n    design of CocoaDialog itself being inconsistent unfortunately.\\n\\n    SubClasses will normally call super and extend the array with their own\\n    parameters.\\n\\n    Returns:\\n      an array of commands.\\n    '\n    cmds = [self._cocoadialog]\n    runmode = self.__class__.__name__.lower().replace('_', '-')\n    cmds.append(runmode)\n    cmds.append('--string-output')\n    if self.title:\n        cmds.extend(['--title', self._title])\n    if self.debug:\n        cmds.append('--debug')\n    if (not self._timeout):\n        if (runmode == 'bubble'):\n            cmds.append('--no-timeout')\n    else:\n        cmds.extend(['--timeout', self._timeout])\n    if self._width:\n        cmds.extend(['--width', self._width])\n    if self._height:\n        cmds.extend(['--height', self._height])\n    return cmds\n", "label": 0}
{"function": "\n\ndef collect(self):\n    hosts = self.config.get('hosts')\n    if isinstance(hosts, basestring):\n        hosts = [hosts]\n    for host in hosts:\n        matches = re.search('((.+)\\\\@)?([^:]+)(:(\\\\d+))?', host)\n        alias = matches.group(2)\n        hostname = matches.group(3)\n        port = matches.group(5)\n        if (alias is None):\n            alias = hostname\n        stats = self.get_stats(hostname, port)\n        desired = self.config.get('publish', stats.keys())\n        for stat in desired:\n            if (stat in stats):\n                if (stat in self.GAUGES):\n                    self.publish_gauge(((alias + '.') + stat), stats[stat])\n                else:\n                    self.publish_counter(((alias + '.') + stat), stats[stat])\n            else:\n                self.log.error(\"No such key '%s' available, issue 'stats' for a full list\", stat)\n", "label": 0}
{"function": "\n\ndef _get_m2m_attr(self, related, attr):\n    'Function that can be curried to provide the source accessor or DB column name for the m2m table'\n    cache_attr = ('_m2m_%s_cache' % attr)\n    if hasattr(self, cache_attr):\n        return getattr(self, cache_attr)\n    for f in self.rel.through._meta.fields:\n        if (hasattr(f, 'rel') and f.rel and (f.rel.to == related.model)):\n            setattr(self, cache_attr, getattr(f, attr))\n            return getattr(self, cache_attr)\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    'Process form'\n    if self.instance:\n        if self.is_valid():\n            if self.cleaned_data['delete']:\n                if (self.cleaned_data['delete'] == 'delete'):\n                    self.instance.delete()\n                if (self.cleaned_data['delete'] == 'trash'):\n                    self.instance.trash = True\n                    self.instance.save()\n", "label": 0}
{"function": "\n\ndef SeriesFactory(values, xvalues=None, zvalues=None, title=None, title_from_data=False):\n    '\\n    Convenience Factory for creating chart data series.\\n    '\n    if (not isinstance(values, Reference)):\n        values = Reference(range_string=values)\n    if title_from_data:\n        cell = values.pop()\n        title = '{0}!{1}'.format(values.sheetname, cell)\n        title = SeriesLabel(strRef=StrRef(title))\n    elif (title is not None):\n        title = SeriesLabel(v=title)\n    source = NumDataSource(numRef=NumRef(f=values))\n    if (xvalues is not None):\n        if (not isinstance(xvalues, Reference)):\n            xvalues = Reference(range_string=xvalues)\n        series = XYSeries()\n        series.yVal = source\n        series.xVal = AxDataSource(numRef=NumRef(f=xvalues))\n        if (zvalues is not None):\n            if (not isinstance(zvalues, Reference)):\n                zvalues = Reference(range_string=zvalues)\n            series.zVal = NumDataSource(NumRef(f=zvalues))\n    else:\n        series = Series()\n        series.val = source\n    if (title is not None):\n        series.title = title\n    return series\n", "label": 1}
{"function": "\n\ndef test_random_complex_exact(self):\n    for dtype in COMPLEX_DTYPES:\n        for n in (20, 200):\n            for lapack_driver in TestLstsq.lapack_drivers:\n                for overwrite in (True, False):\n                    a = np.asarray((random([n, n]) + (1j * random([n, n]))), dtype=dtype)\n                    for i in range(n):\n                        a[(i, i)] = (20 * (0.1 + a[(i, i)]))\n                    for i in range(2):\n                        b = np.asarray(random([n, 3]), dtype=dtype)\n                        a1 = a.copy()\n                        b1 = b.copy()\n                        out = lstsq(a1, b1, lapack_driver=lapack_driver, overwrite_a=overwrite, overwrite_b=overwrite)\n                        x = out[0]\n                        r = out[2]\n                        assert_((r == n), ('expected efficient rank %s, got %s' % (n, r)))\n                        if (dtype is np.complex64):\n                            assert_allclose(dot(a, x), b, rtol=(400 * np.finfo(a1.dtype).eps), atol=(400 * np.finfo(a1.dtype).eps), err_msg=('driver: %s' % lapack_driver))\n                        else:\n                            assert_allclose(dot(a, x), b, rtol=(1000 * np.finfo(a1.dtype).eps), atol=(1000 * np.finfo(a1.dtype).eps), err_msg=('driver: %s' % lapack_driver))\n", "label": 0}
{"function": "\n\ndef attach_stream(self, source, shape, name, semantic, stride, offset):\n    'Missing'\n    if (source is not None):\n        if (isinstance(source[0], tuple) is False):\n            self.__attach_v1(shape, source, name, semantic, offset)\n        elif (len(source[0]) == 2):\n            self.__attach_v2(shape, source, name, semantic, offset)\n        elif (len(source[0]) == 3):\n            self.__attach_v3(shape, source, name, semantic, offset)\n        elif (len(source[0]) == 4):\n            self.__attach_v4(shape, source, name, semantic, offset)\n    else:\n        self.__set_source(shape, name, stride)\n        self.__set_input(shape, semantic, {\n            'source': name,\n            'offset': offset,\n        })\n", "label": 0}
{"function": "\n\ndef __init__(self, width, height, color='black', emphasis=None, highlight=0):\n    if (((width == 0) and (height == 0) and (color == 'red') and (emphasis == 'strong')) or (highlight > 100)):\n        raise ValueError('sorry, you lose')\n    if ((width == 0) and (height == 0) and ((color == 'red') or (emphasis is None))):\n        raise ValueError((\"I don't think so -- values are %s, %s\" % (width, height)))\n    Blob.__init__(self, width, height, color, emphasis, highlight)\n", "label": 1}
{"function": "\n\ndef add_entity(self, entity, platform=None):\n    'Add entity to component.'\n    if ((entity is None) or (entity in self.entities.values())):\n        return False\n    entity.hass = self.hass\n    if (getattr(entity, 'entity_id', None) is None):\n        object_id = (entity.name or DEVICE_DEFAULT_NAME)\n        if ((platform is not None) and (platform.entity_namespace is not None)):\n            object_id = '{} {}'.format(platform.entity_namespace, object_id)\n        entity.entity_id = generate_entity_id(self.entity_id_format, object_id, self.entities.keys())\n    self.entities[entity.entity_id] = entity\n    entity.update_ha_state()\n    return True\n", "label": 0}
{"function": "\n\ndef disk_partitions(all=False):\n    retlist = []\n    partitions = _psutil_osx.get_disk_partitions()\n    for partition in partitions:\n        (device, mountpoint, fstype, opts) = partition\n        if (device == 'none'):\n            device = ''\n        if (not all):\n            if ((not os.path.isabs(device)) or (not os.path.exists(device))):\n                continue\n        ntuple = nt_partition(device, mountpoint, fstype, opts)\n        retlist.append(ntuple)\n    return retlist\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _read_depgraph(node, depgraph, label_counter=None, parent=None):\n    if (not label_counter):\n        label_counter = Counter()\n    if (node['rel'].lower() in ['spec', 'punct']):\n        return (node['word'], node['tag'])\n    else:\n        fstruct = FStructure()\n        fstruct.pred = None\n        fstruct.label = FStructure._make_label(label_counter.get())\n        fstruct.parent = parent\n        (word, tag) = (node['word'], node['tag'])\n        if (tag[:2] == 'VB'):\n            if (tag[2:3] == 'D'):\n                fstruct.safeappend('tense', ('PAST', 'tense'))\n            fstruct.pred = (word, tag[:2])\n        if (not fstruct.pred):\n            fstruct.pred = (word, tag)\n        children = [depgraph.nodes[idx] for idx in sum(list(node['deps'].values()), [])]\n        for child in children:\n            fstruct.safeappend(child['rel'], FStructure._read_depgraph(child, depgraph, label_counter, fstruct))\n        return fstruct\n", "label": 0}
{"function": "\n\ndef show_vlan_page(self, vlans):\n    lines_per_pages = 18\n    self.write_line('')\n    self.write_line('VLAN       Name                         Ports          Type      Authorization')\n    self.write_line('-----  ---------------                  -------------  -----     -------------')\n    line_count = 0\n    while ((len(vlans) > 0) and (line_count < lines_per_pages)):\n        vlan = vlans.pop(0)\n        ports_strings = self._build_port_strings(self.get_ports_for_vlan(vlan))\n        self.write_line('{number: <5}  {name: <32} {ports: <13}  {type: <8}  {auth: <13}'.format(number=vlan.number, name=vlan_name(vlan), ports=ports_strings[0], type=('Default' if (vlan.number == 1) else 'Static'), auth='Required'))\n        line_count += 1\n        for port_string in ports_strings[1:]:\n            self.write_line('{number: <5}  {name: <32} {ports: <13}  {type: <8}  {auth: <13}'.format(number='', name='', ports=port_string, type='', auth=''))\n            line_count += 1\n    self.write_line('')\n    if (len(vlans) > 0):\n        self.write('--More-- or (q)uit')\n        self.on_keystroke(self.continue_vlan_pages, vlans)\n", "label": 0}
{"function": "\n\ndef transform_inventory_targeting_to_dfp(targeting):\n    '\\n    Convert inventory targeting model to dfp dictionary\\n\\n    @param adunit: TargetingCriterion\\n    @return: dict\\n    '\n    targeted_placements = []\n    targeted_adunits = []\n    excluded_adunits = []\n    (includes, excludes) = targeting.get_includes_and_excludes()\n    for i in includes:\n        if isinstance(i, AdUnit):\n            targeted_adunits.append(_adunit_to_dfp(i))\n        elif isinstance(i, Placement):\n            targeted_placements.append(i.id)\n    for e in excludes:\n        if isinstance(e, AdUnit):\n            excluded_adunits.append(_adunit_to_dfp(e))\n    target_dict = {\n        \n    }\n    if targeted_placements:\n        target_dict['targetedPlacementIds'] = targeted_placements\n    if targeted_adunits:\n        target_dict['targetedAdUnits'] = targeted_adunits\n    if excluded_adunits:\n        target_dict['excludedAdUnits'] = excluded_adunits\n    return target_dict\n", "label": 1}
{"function": "\n\ndef _filter(self, include_private, filter):\n    childnodes = {\n        \n    }\n    for name in dir(self.node.__class__):\n        if ((not name.startswith('__')) and (name != 'parent')):\n            if (include_private or (not name.startswith('_'))):\n                attr = getattr(self.node, name)\n                if filter(attr):\n                    childnodes[name] = attr\n    return childnodes\n", "label": 0}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    completions = []\n    if (not view.match_selector(locations[0], 'meta.scope.between-output-tags.cfml - meta.tag - comment - string,                 text.html.cfm - meta - source - comment - string,                 text.html.cfm.embedded.cfml - meta - source.cfscript.embedded.cfml - comment - string,                 punctuation.definition.tag.cf.begin,                 source.sql.embedded.cfml - string - comment - meta.name.interpolated.hash')):\n        return\n    if SETTINGS.get('verbose_tag_completions'):\n        return\n    sel = view.sel()[0]\n    if (view.substr((sel.begin() - 1)) == '.'):\n        return []\n    pt = ((locations[0] - len(prefix)) - 1)\n    if any(((s in view.scope_name(pt)) for s in ['meta.tag.block.cf', 'meta.tag.inline.cf', 'string', 'comment'])):\n        return\n    for s in self.cflib.completions.keys():\n        completions.extend([((s + '\\tTag (cmfl)'), s)])\n    if (view.substr(pt) != '<'):\n        completions = [(list(item)[(- 2)], ('<' + list(item)[1])) for item in completions]\n    return sorted(completions)\n", "label": 1}
{"function": "\n\n@no_mysql\ndef test_point_on_surface(self):\n    'Testing the `point_on_surface` GeoQuerySet method.'\n    if oracle:\n        ref = {\n            'New Zealand': fromstr('POINT (174.616364 -36.100861)', srid=4326),\n            'Texas': fromstr('POINT (-103.002434 36.500397)', srid=4326),\n        }\n    elif (postgis or spatialite):\n        ref = {\n            'New Zealand': Country.objects.get(name='New Zealand').mpoly.point_on_surface,\n            'Texas': Country.objects.get(name='Texas').mpoly.point_on_surface,\n        }\n    for c in Country.objects.point_on_surface():\n        if spatialite:\n            tol = 1e-05\n        else:\n            tol = 1e-09\n        self.assertEqual(True, ref[c.name].equals_exact(c.point_on_surface, tol))\n", "label": 0}
{"function": "\n\ndef _get_td_css(h, v, td_styles):\n    if td_styles:\n        if isinstance(td_styles, string_types):\n            return td_styles\n        elif callable(td_styles):\n            return td_styles(v)\n        elif isinstance(td_styles, dict):\n            if (h in td_styles):\n                s = td_styles[h]\n                if isinstance(s, string_types):\n                    return s\n                elif callable(s):\n                    return s(v)\n                else:\n                    raise ArgumentError(('expected string or callable, got %r' % s))\n        else:\n            raise ArgumentError(('expected string, callable or dict, got %r' % td_styles))\n    if (isinstance(v, numeric_types) and (not isinstance(v, bool))):\n        return 'text-align: right'\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef submit_packets(self, packets):\n    if self.utf8_decoding:\n        packets = unicode(packets, 'utf-8', errors='replace')\n    for packet in packets.splitlines():\n        if (not packet.strip()):\n            continue\n        if packet.startswith('_e'):\n            self.event_count += 1\n            event = self.parse_event_packet(packet)\n            self.event(**event)\n        elif packet.startswith('_sc'):\n            self.service_check_count += 1\n            service_check = self.parse_sc_packet(packet)\n            self.service_check(**service_check)\n        else:\n            self.count += 1\n            parsed_packets = self.parse_metric_packet(packet)\n            for (name, value, mtype, tags, sample_rate) in parsed_packets:\n                (hostname, device_name, tags) = self._extract_magic_tags(tags)\n                self.submit_metric(name, value, mtype, tags=tags, hostname=hostname, device_name=device_name, sample_rate=sample_rate)\n", "label": 0}
{"function": "\n\ndef documentation(self, add_to=None):\n    'Produces general documentation for the interface'\n    doc = (OrderedDict if (add_to is None) else add_to)\n    usage = self.interface.spec.__doc__\n    if usage:\n        doc['usage'] = usage\n    if getattr(self, 'requires', None):\n        doc['requires'] = [getattr(requirement, '__doc__', requirement.__name__) for requirement in self.requires]\n    doc['outputs'] = OrderedDict()\n    doc['outputs']['format'] = self.outputs.__doc__\n    doc['outputs']['content_type'] = self.outputs.content_type\n    parameters = [param for param in self.parameters if ((not (param in ('request', 'response', 'self'))) and (not param.startswith('hug_')) and (not hasattr(param, 'directive')))]\n    if parameters:\n        inputs = doc.setdefault('inputs', OrderedDict())\n        types = self.interface.spec.__annotations__\n        for argument in parameters:\n            kind = types.get(argument, text)\n            if (getattr(kind, 'directive', None) is True):\n                continue\n            input_definition = inputs.setdefault(argument, OrderedDict())\n            input_definition['type'] = (kind if isinstance(kind, str) else kind.__doc__)\n            default = self.defaults.get(argument, None)\n            if (default is not None):\n                input_definition['default'] = default\n    return doc\n", "label": 1}
{"function": "\n\ndef get_indexes(self, table, schema=None):\n    query = 'SELECT name, sql FROM sqlite_master WHERE tbl_name = ? AND type = ? ORDER BY name'\n    cursor = self.execute_sql(query, (table, 'index'))\n    index_to_sql = dict(cursor.fetchall())\n    unique_indexes = set()\n    cursor = self.execute_sql(('PRAGMA index_list(\"%s\")' % table))\n    for row in cursor.fetchall():\n        name = row[1]\n        is_unique = (int(row[2]) == 1)\n        if is_unique:\n            unique_indexes.add(name)\n    index_columns = {\n        \n    }\n    for index_name in sorted(index_to_sql):\n        cursor = self.execute_sql(('PRAGMA index_info(\"%s\")' % index_name))\n        index_columns[index_name] = [row[2] for row in cursor.fetchall()]\n    return [IndexMetadata(name, index_to_sql[name], index_columns[name], (name in unique_indexes), table) for name in sorted(index_to_sql)]\n", "label": 0}
{"function": "\n\ndef env(key, default='', factory=None):\n    if ((default is RaiseException) and (key not in os.environ)):\n        raise KeyError(key)\n    val = os.environ.get(key, default)\n    if isinstance(val, basestring):\n        val = val.strip()\n    if (val == default == NotSet):\n        return NotSet\n    if (factory != NotSet):\n        if ((not factory) and (default != NotSet)):\n            factory = type(default)\n        if factory:\n            if (factory == bool):\n                if (isinstance(val, basestring) and (val.lower() in ['no', 'false', 'off', '0'])):\n                    val = False\n            val = factory(val)\n    return val\n", "label": 1}
{"function": "\n\ndef _handle_player_events(self):\n    self.dx = 0.0\n    for event in pygame.event.get():\n        if (event.type == pygame.QUIT):\n            pygame.quit()\n            sys.exit()\n        if (event.type == pygame.KEYDOWN):\n            key = event.key\n            if (key == self.actions['left']):\n                self.dx -= self.player_speed\n            if (key == self.actions['right']):\n                self.dx += self.player_speed\n", "label": 0}
{"function": "\n\ndef getDrsRels(val, fromELR, rels, drsELR, drsRelsFrom, drsRelsTo, fromConcepts=None):\n    if (not fromConcepts):\n        fromConcepts = set()\n    for rel in rels:\n        relTo = rel.toModelObject\n        if isinstance(relTo, ModelConcept):\n            drsRelsFrom[rel.fromModelObject].append(rel)\n            drsRelsTo[relTo].append(rel)\n            toELR = rel.targetRole\n            if (not toELR):\n                toELR = fromELR\n            if (relTo not in fromConcepts):\n                fromConcepts.add(relTo)\n                domMbrRels = val.modelXbrl.relationshipSet(XbrlConst.domainMember, toELR).fromModelObject(relTo)\n                getDrsRels(val, toELR, domMbrRels, drsELR, drsRelsFrom, drsRelsTo, fromConcepts)\n                fromConcepts.discard(relTo)\n    return False\n", "label": 0}
{"function": "\n\ndef log_actor_migrate(self, actor_id, dest_node_id):\n    ' Trace actor migrate\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_ACTOR_MIGRATE in logger.events)):\n            if ((not logger.actors) or (actor_id in logger.actors)):\n                data = {\n                    \n                }\n                data['timestamp'] = time.time()\n                data['node_id'] = self.node.id\n                data['type'] = 'actor_migrate'\n                data['actor_id'] = actor_id\n                data['dest_node_id'] = dest_node_id\n                if (logger.connection is not None):\n                    if (not logger.connection.connection_lost):\n                        logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                    else:\n                        disconnected.append(user_id)\n                elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                    msg = {\n                        'cmd': 'logevent',\n                        'msgid': logger.handle,\n                        'header': None,\n                        'data': ('data: %s\\n\\n' % json.dumps(data)),\n                    }\n                    self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef _format_nsn_using_pattern(national_number, formatting_pattern, number_format, carrier_code=None):\n    number_format_rule = formatting_pattern.format\n    m_re = re.compile(formatting_pattern.pattern)\n    formatted_national_number = U_EMPTY_STRING\n    if ((number_format == PhoneNumberFormat.NATIONAL) and (carrier_code is not None) and (len(carrier_code) > 0) and (formatting_pattern.domestic_carrier_code_formatting_rule is not None) and (len(formatting_pattern.domestic_carrier_code_formatting_rule) > 0)):\n        cc_format_rule = formatting_pattern.domestic_carrier_code_formatting_rule\n        cc_format_rule = re.sub(_CC_PATTERN, carrier_code, cc_format_rule, count=1)\n        number_format_rule = re.sub(_FIRST_GROUP_PATTERN, cc_format_rule, number_format_rule, count=1)\n        formatted_national_number = re.sub(m_re, number_format_rule, national_number)\n    else:\n        national_prefix_formatting_rule = formatting_pattern.national_prefix_formatting_rule\n        if ((number_format == PhoneNumberFormat.NATIONAL) and (national_prefix_formatting_rule is not None) and (len(national_prefix_formatting_rule) > 0)):\n            first_group_rule = re.sub(_FIRST_GROUP_PATTERN, national_prefix_formatting_rule, number_format_rule, count=1)\n            formatted_national_number = re.sub(m_re, first_group_rule, national_number)\n        else:\n            formatted_national_number = re.sub(m_re, number_format_rule, national_number)\n    if (number_format == PhoneNumberFormat.RFC3966):\n        m = _SEPARATOR_PATTERN.match(formatted_national_number)\n        if m:\n            formatted_national_number = re.sub(_SEPARATOR_PATTERN, U_EMPTY_STRING, formatted_national_number, count=1)\n        formatted_national_number = re.sub(_SEPARATOR_PATTERN, U_DASH, formatted_national_number)\n    return formatted_national_number\n", "label": 1}
{"function": "\n\ndef __translate(self, asm_instrs):\n    instr_container = ReilContainer()\n    asm_instr_last = None\n    instr_seq_prev = None\n    for asm_instr in asm_instrs:\n        instr_seq = ReilSequence()\n        for reil_instr in self._translator.translate(asm_instr):\n            instr_seq.append(reil_instr)\n        if instr_seq_prev:\n            instr_seq_prev.next_sequence_address = instr_seq.address\n        instr_container.add(instr_seq)\n        instr_seq_prev = instr_seq\n    if instr_seq_prev:\n        if asm_instr_last:\n            instr_seq_prev.next_sequence_address = ((asm_instr_last.address + asm_instr_last.size) << 8)\n    return instr_container\n", "label": 0}
{"function": "\n\ndef perform(self, node, inputs, outputs):\n    (alpha, x, y, z) = inputs\n    (out,) = outputs\n    x_is_sparse = _is_sparse(x)\n    y_is_sparse = _is_sparse(y)\n    if ((not x_is_sparse) and (not y_is_sparse)):\n        raise TypeError(x)\n    rval = (x * y)\n    if isinstance(rval, scipy.sparse.spmatrix):\n        rval = rval.toarray()\n    if (rval.dtype == alpha.dtype):\n        rval *= alpha\n    else:\n        rval = (rval * alpha)\n    if (rval.dtype == z.dtype):\n        rval += z\n    else:\n        rval = (rval + z)\n    out[0] = rval\n", "label": 0}
{"function": "\n\ndef s3_menu_postp():\n    menu_selected = []\n    body_id = s3base.s3_get_last_record_id('dvi_body')\n    if body_id:\n        body = s3db.dvi_body\n        query = (body.id == body_id)\n        record = db(query).select(body.id, body.pe_label, limitby=(0, 1)).first()\n        if record:\n            label = record.pe_label\n            response.menu_options[(- 3)][(- 1)].append([(T('Candidate Matches for Body %(label)s') % dict(label=label)), False, URL(f='person', vars=dict(match=record.id))])\n            menu_selected.append([('%s: %s' % (T('Body'), label)), False, URL(f='body', args=[record.id])])\n    person_id = s3base.s3_get_last_record_id('pr_person')\n    if person_id:\n        person = s3db.pr_person\n        query = (person.id == person_id)\n        record = db(query).select(person.id, limitby=(0, 1)).first()\n        if record:\n            name = s3db.pr_person_id().represent(record.id)\n            menu_selected.append([('%s: %s' % (T('Person'), name)), False, URL(f='person', args=[record.id])])\n    if menu_selected:\n        menu_selected = [T('Open recent'), True, None, menu_selected]\n        response.menu_options.append(menu_selected)\n", "label": 0}
{"function": "\n\ndef get_subj_alt_name(peer_cert):\n    dns_name = []\n    if (not SUBJ_ALT_NAME_SUPPORT):\n        return dns_name\n    general_names = SubjectAltName()\n    for i in range(peer_cert.get_extension_count()):\n        ext = peer_cert.get_extension(i)\n        ext_name = ext.get_short_name()\n        if (ext_name != 'subjectAltName'):\n            continue\n        ext_dat = ext.get_data()\n        decoded_dat = der_decoder.decode(ext_dat, asn1Spec=general_names)\n        for name in decoded_dat:\n            if (not isinstance(name, SubjectAltName)):\n                continue\n            for entry in range(len(name)):\n                component = name.getComponentByPosition(entry)\n                if (component.getName() != 'dNSName'):\n                    continue\n                dns_name.append(str(component.getComponent()))\n    return dns_name\n", "label": 0}
{"function": "\n\ndef move_to_stash(self, which, where=None):\n    if ((which is not None) and where):\n        which = str(which)\n        stash_regions = self.get_all_stash_regions()\n        if stash_regions:\n            prev_regions = [r for r in stash_regions if (self.view.substr(r) < which)]\n            next_regions = [r for r in stash_regions if (self.view.substr(r) >= which)]\n            if next_regions:\n                next = next_regions[0]\n            else:\n                next = prev_regions[(- 1)]\n            self.move_to_region(next)\n        else:\n            self.move_to_file(1)\n    elif isinstance(which, int):\n        stashes = self.get_all_stash_regions()\n        if stashes:\n            if (len(stashes) >= which):\n                self.move_to_region(self.view.line(stashes[(which - 1)]))\n            else:\n                self.move_to_region(self.view.line(stashes[(- 1)]))\n", "label": 1}
{"function": "\n\ndef evaluate_srl_1step(find_preds_automatically=False, gold_file=None):\n    '\\n    Evaluates the network on the SRL task performed with one step for\\n    id + class.\\n    '\n    md = Metadata.load_from_file('srl')\n    nn = taggers.load_network(md)\n    r = taggers.create_reader(md, gold_file=gold_file)\n    itd = r.get_inverse_tag_dictionary()\n    if find_preds_automatically:\n        tagger = taggers.SRLTagger()\n    else:\n        iter_predicates = iter(r.predicates)\n    for sent in iter(r.sentences):\n        actual_sent = sent[0]\n        if find_preds_automatically:\n            pred_positions = tagger.find_predicates(sent)\n        else:\n            pred_positions = iter_predicates.next()\n        verbs = [(position, actual_sent[position].word) for position in pred_positions]\n        sent_codified = np.array([r.converter.convert(token) for token in actual_sent])\n        answers = nn.tag_sentence(sent_codified, pred_positions)\n        tags = [convert_iob_to_iobes([itd[x] for x in pred_answer]) for pred_answer in answers]\n        print(prop_conll(verbs, tags, len(actual_sent)))\n", "label": 0}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    sel = view.sel()[0].a\n    completions = []\n    pt = ((locations[0] - len(prefix)) - 1)\n    if any(((s in view.scope_name(pt)) for s in ['string', 'comment'])):\n        return\n    if any(((s in view.scope_name(sel)) for s in self.valid_scopes_tags)):\n        for region in view.sel():\n            pos = region.begin()\n            tagdata = view.substr(sublime.Region(0, pos)).split('<')\n            tagdata.reverse()\n            tagdata = tagdata.pop(0).split(' ')\n            tagname = tagdata[0]\n        if (tagname in self.cflib.completions.keys()):\n            completions = self.cflib.completions[tagname]['completions']\n    if (completions == []):\n        return\n    return (completions, (sublime.INHIBIT_WORD_COMPLETIONS | sublime.INHIBIT_EXPLICIT_COMPLETIONS))\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.num1 = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.num2 = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.op = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.comment = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef go(self, room, tell_new=None, tell_old=None):\n    'Go to a specific room.'\n    if (self.position[0] == 'standing'):\n        if room:\n            if self.location:\n                prev = ('%s_%s' % (self.location.id, self.location.area.name))\n                if tell_old:\n                    self.location.tell_room(tell_old, [self.name])\n                self.location.remove_char(self)\n            else:\n                prev = 'void'\n            if (self.location and (self.location == room)):\n                self.update_output(\"You're already there.\\n\")\n            else:\n                self.location = room\n                self.update_output(self.look_at_room())\n                self.location.add_char(self, prev)\n                if tell_new:\n                    self.location.tell_room(tell_new, [self.name])\n        else:\n            self.world.log.debug(('We gave %s a nonexistant room.' % self.name))\n    else:\n        self.update_output('You better stand up first.')\n", "label": 0}
{"function": "\n\ndef memmap(docompute, dowrite, verbose):\n    afilename = os.path.join(OUT_DIR, 'memmap-a.bin')\n    bfilename = os.path.join(OUT_DIR, 'memmap-b.bin')\n    rfilename = os.path.join(OUT_DIR, 'memmap-output.bin')\n    if dowrite:\n        t0 = time()\n        a = np.memmap(afilename, dtype='float32', mode='w+', shape=shape)\n        b = np.memmap(bfilename, dtype='float32', mode='w+', shape=shape)\n        row = np.arange(0, ncols, dtype='float32')\n        for i in range(nrows):\n            a[i] = (row * (i + 1))\n            b[i] = ((row * (i + 1)) * 2)\n        del a, b\n        print('[numpy.memmap] Time for creating inputs:', round((time() - t0), 3))\n    if docompute:\n        t0 = time()\n        a = np.memmap(afilename, dtype='float32', mode='r', shape=shape)\n        b = np.memmap(bfilename, dtype='float32', mode='r', shape=shape)\n        r = np.memmap(rfilename, dtype='float32', mode='w+', shape=shape)\n        for i in range(nrows):\n            r[i] = eval(expr, {\n                'a': a[i],\n                'b': b[i],\n            })\n        if verbose:\n            print('First ten values:', r[0, :10])\n        del a, b\n        del r\n        print('[numpy.memmap] Time for compute & save:', round((time() - t0), 3))\n", "label": 0}
{"function": "\n\n@logging.log_deploy_wrapper(LOG.info, _('Create containers on host'))\ndef create_servers(self):\n    host_provider = self.get_host_provider()\n    name_prefix = self.config['container_name_prefix']\n    hosts = []\n    if ('start_lxc_network' in self.config):\n        network = netaddr.IPNetwork(self.config['start_lxc_network'])\n    else:\n        network = None\n    distribution = self.config.get('distribution', 'ubuntu')\n    release = self.config.get('release')\n    for server in host_provider.create_servers():\n        config = {\n            'tunnel_to': self.config.get('tunnel_to', []),\n            'forward_ssh': self.config.get('forward_ssh', False),\n        }\n        if network:\n            config['network'] = str(network)\n        host = LxcHost(server, config)\n        host.prepare()\n        ip = (str(network.ip).replace('.', '-') if network else '0')\n        first_name = ('%s-000-%s' % (name_prefix, ip))\n        host.create_container(first_name, distribution, release)\n        for i in range(1, self.config.get('containers_per_host', 1)):\n            name = ('%s-%03d-%s' % (name_prefix, i, ip))\n            host.create_clone(name, first_name)\n        host.start_containers()\n        hosts.append(host)\n        if network:\n            network += 1\n    servers = []\n    for host in hosts:\n        for server in host.get_server_objects():\n            servers.append(server)\n        info = {\n            'host': host.server.get_credentials(),\n            'config': host.config,\n            'forwarded_ports': host._port_cache.items(),\n            'container_names': host.containers,\n        }\n        self.resources.create(info)\n    return servers\n", "label": 1}
{"function": "\n\ndef __init__(self, out='stdout'):\n    super(DumpRecorder, self).__init__()\n    self._parallel = True\n    if isinstance(out, string_types):\n        if (out == 'stdout'):\n            out = sys.stdout\n        elif (out == 'stderr'):\n            out = sys.stderr\n        else:\n            if MPI:\n                if ('.' in out):\n                    parts = out.split('.')\n                    parts[(- 2)] += ('_' + str(MPI.COMM_WORLD.rank))\n                    out = '.'.join(parts)\n                else:\n                    out += str(MPI.COMM_WORLD.rank)\n            out = open(out, 'w')\n    self.out = out\n", "label": 0}
{"function": "\n\ndef compute_tensor(self, input_var, mask=None, additional_inputs=None, steps=None, backward=False):\n    backward = (backward if backward else self._go_backwards)\n    steps = (steps if steps else self._steps)\n    mask = (mask if mask else self._mask)\n    if (mask and (self._input_type == 'one')):\n        raise Exception('Mask only works with sequence input')\n    init_state_map = self.get_initial_states(input_var)\n    if (self._input_type == 'sequence'):\n        input_var = input_var.dimshuffle((1, 0, 2))\n        seq_map = self.get_step_inputs(input_var, mask=mask, additional_inputs=additional_inputs)\n    else:\n        init_state_map[self.main_state] = input_var\n        seq_map = self.get_step_inputs(None, mask=mask, additional_inputs=additional_inputs)\n    (retval_map, _) = Scanner(self.step, sequences=seq_map, outputs_info=init_state_map, n_steps=steps, go_backwards=backward).compute()\n    main_states = retval_map[self.main_state]\n    if (self._output_type == 'one'):\n        return main_states[(- 1)]\n    elif (self._output_type == 'sequence'):\n        main_states = main_states.dimshuffle((1, 0, 2))\n        if mask:\n            main_states *= mask.dimshuffle((0, 1, 'x'))\n        return main_states\n", "label": 1}
{"function": "\n\ndef handle_bit(self, bit):\n    '\\n        Handle the current bit\\n        '\n    breakpoint = False\n    if (self.forced_next is not None):\n        if (bit != self.forced_next):\n            raise BreakpointExpected(self.tagname, [self.forced_next], bit)\n    elif (bit in self.options.reversed_combined_breakpoints):\n        expected = self.options.reversed_combined_breakpoints[bit]\n        raise BreakpointExpected(self.tagname, [expected], bit)\n    if (bit == self.options.next_breakpoint):\n        self.handle_next_breakpoint(bit)\n        breakpoint = bit\n    elif (bit in self.options.breakpoints):\n        self.handle_breakpoints(bit)\n        breakpoint = bit\n    else:\n        self.handle_argument(bit)\n    if (bit in self.options.combined_breakpoints):\n        self.forced_next = self.options.combined_breakpoints[bit]\n    else:\n        self.forced_next = None\n    del self.todo[0]\n    return breakpoint\n", "label": 0}
{"function": "\n\ndef update_track(track_dict):\n    for (key, value) in DEFAULT_TEMPLATE.items():\n        if (key in ['actions']):\n            if (track_dict[key] != DEFAULT_TEMPLATE[key]):\n                warning(\"Your track's '{0}' configuration is not the same as the default.\".format(key))\n                default = 'n'\n                if (key == 'actions'):\n                    default = 'y'\n                    warning(\"Unless you have manually modified your 'actions' (the commands which get run for a release), you should update to the new default.\")\n                warning('Should it be updated to the default setting?')\n                if maybe_continue(default):\n                    track_dict[key] = DEFAULT_TEMPLATE[key]\n        elif (key not in track_dict):\n            value = (value.default if isinstance(value, PromptEntry) else value)\n            track_dict[key] = value\n    return track_dict\n", "label": 0}
{"function": "\n\ndef getpygamecolor(value):\n    \"Returns a pygame.Color object of the argument passed in. The argument can be a RGB/RGBA tuple, pygame.Color object, or string in the colornames dict (such as 'blue' or 'gray').\"\n    if (type(value) in (tuple, list)):\n        alpha = (((len(value) > 3) and value[3]) or 255)\n        return pygame.Color(value[0], value[1], value[2], alpha)\n    elif (str(type(value)) in (\"<class 'pygame.Color'>\", \"<type 'pygame.Color'>\")):\n        return value\n    elif (value in colornames):\n        return colornames[value]\n    else:\n        raise Exception(('Color set to invalid value: %s' % repr(value)))\n    if (type(color) in (tuple, list)):\n        return pygame.Color(*color)\n    return color\n", "label": 0}
{"function": "\n\ndef initialize(self):\n    (self.handle, self.close_file) = base.open_resource(self.resource, 'w')\n    if self.html_header:\n        self.handle.write(self.html_header)\n    attr_string = ''\n    if self.table_attributes:\n        for attr_value in self.table_attributes.items():\n            attr_string += (' %s=\"%s\"\\n' % attr_value)\n    string = ('<table%s>\\n' % attr_string)\n    if self.write_headers:\n        string += '<tr>'\n        for field in self.fields:\n            if field.label:\n                header = field.label\n            else:\n                header = field.name\n            string += ('  <th>%s</th>\\n' % header)\n        string += '</tr>\\n'\n    self.handle.write(string)\n", "label": 0}
{"function": "\n\ndef build_files_status(self, repo):\n    status = ''\n    (untracked, unstaged, staged) = self.get_files_status(repo)\n    if ((not untracked) and (not unstaged) and (not staged)):\n        status += (GIT_WORKING_DIR_CLEAN + '\\n')\n    if untracked:\n        status += SECTIONS[UNTRACKED_FILES]\n        for (s, f) in untracked:\n            status += ('\\t%s\\n' % f.strip())\n        status += '\\n'\n    if unstaged:\n        status += (SECTIONS[UNSTAGED_CHANGES] if staged else SECTIONS[CHANGES])\n        for (s, f) in unstaged:\n            status += ('\\t%s %s\\n' % (STATUS_LABELS[s], f))\n        status += '\\n'\n    if staged:\n        status += SECTIONS[STAGED_CHANGES]\n        for (s, f) in staged:\n            status += ('\\t%s %s\\n' % (STATUS_LABELS[s], f))\n        status += '\\n'\n    return status\n", "label": 1}
{"function": "\n\n@spin_first\ndef shutdown(self, targets=None, restart=False, hub=False, block=None):\n    'Terminates one or more engine processes, optionally including the hub.'\n    block = (self.block if (block is None) else block)\n    if hub:\n        targets = 'all'\n    targets = self._build_targets(targets)[0]\n    for t in targets:\n        self.session.send(self._control_socket, 'shutdown_request', content={\n            'restart': restart,\n        }, ident=t)\n    error = False\n    if (block or hub):\n        self._flush_ignored_control()\n        for i in range(len(targets)):\n            (idents, msg) = self.session.recv(self._control_socket, 0)\n            if self.debug:\n                pprint(msg)\n            if (msg['content']['status'] != 'ok'):\n                error = self._unwrap_exception(msg['content'])\n    else:\n        self._ignored_control_replies += len(targets)\n    if hub:\n        time.sleep(0.25)\n        self.session.send(self._query_socket, 'shutdown_request')\n        (idents, msg) = self.session.recv(self._query_socket, 0)\n        if self.debug:\n            pprint(msg)\n        if (msg['content']['status'] != 'ok'):\n            error = self._unwrap_exception(msg['content'])\n    if error:\n        raise error\n", "label": 1}
{"function": "\n\ndef search(self):\n    sqs = super(AdvancedSearchForm, self).search()\n    if (not self.is_valid()):\n        return sqs\n    if self.cleaned_data['language']:\n        sqs = sqs.filter(language=self.cleaned_data['language'].name)\n    if self.cleaned_data['version']:\n        sqs = sqs.filter(version__in=self.cleaned_data['version'])\n    if self.cleaned_data['minimum_pub_date']:\n        sqs = sqs.filter(pub_date__gte=self.cleaned_data['minimum_pub_date'])\n    if self.cleaned_data['minimum_bookmark_count']:\n        sqs = sqs.filter(bookmark_count__gte=self.cleaned_data['minimum_bookmark_count'])\n    if self.cleaned_data['minimum_rating_score']:\n        sqs = sqs.filter(rating_score__gte=self.cleaned_data['minimum_rating_score'])\n    return sqs\n", "label": 0}
{"function": "\n\ndef __init__(self, sender='', recipients=(), subject='', alternative=0, reply_to=None, cc=(), email_account=None):\n    from email.mime.multipart import MIMEMultipart\n    from email import Charset\n    Charset.add_charset('utf-8', Charset.QP, Charset.QP, 'utf-8')\n    if isinstance(recipients, basestring):\n        recipients = recipients.replace(';', ',').replace('\\n', '')\n        recipients = split_emails(recipients)\n    recipients = filter(None, (strip(r) for r in recipients))\n    self.sender = sender\n    self.reply_to = (reply_to or sender)\n    self.recipients = recipients\n    self.subject = subject\n    self.msg_root = MIMEMultipart('mixed')\n    self.msg_multipart = MIMEMultipart('alternative')\n    self.msg_root.attach(self.msg_multipart)\n    self.cc = (cc or [])\n    self.html_set = False\n    self.email_account = (email_account or get_outgoing_email_account())\n", "label": 0}
{"function": "\n\ndef initialize_legacy_extensions(legacy_extensions):\n    leftovers = []\n    for ext in legacy_extensions:\n        ext_resources = ext.get_resources()\n        for ext_resource in ext_resources:\n            controller = ext_resource.controller.controller\n            collection = ext_resource.collection\n            resource = _handle_plurals(collection)\n            if manager.NeutronManager.get_plugin_for_resource(resource):\n                continue\n            if manager.NeutronManager.get_plugin_for_resource(collection):\n                continue\n            leftovers.append((collection, resource, controller))\n    for leftover in leftovers:\n        shim_controller = utils.ShimCollectionsController(*leftover)\n        manager.NeutronManager.set_controller_for_resource(shim_controller.collection, shim_controller)\n", "label": 0}
{"function": "\n\ndef visit_QDockArea(self, area):\n    ' Visit a QDockArea node.\\n\\n        This visitor generates an AreaLayout for the area and pushes\\n        it onto the stack.\\n\\n        '\n    central = area.centralWidget()\n    if (central is None):\n        layout = AreaLayout()\n    else:\n        self.visit(central)\n        layout = AreaLayout(self.stack.pop())\n    bar_data = defaultdict(list)\n    for (container, position) in area.dockBarContainers():\n        bar_data[position].append(container.objectName())\n    for (bar_pos, names) in bar_data.iteritems():\n        bar = DockBarLayout(*names, position=self.BAR_POSITIONS[bar_pos])\n        layout.dock_bars.append(bar)\n    maxed = area.maximizedWidget()\n    if (maxed is not None):\n        name = maxed.objectName()\n        for item in layout.find_all(ItemLayout):\n            if (item.name == name):\n                item.maximized = True\n                break\n    self.stack.append(layout)\n", "label": 0}
{"function": "\n\ndef generate_mount_url(self, regex, v_or_f, mod):\n    \"The self.mounts can be None, which means no url generation.\\n\\n        url is being managed by urlpatterns.\\n        else self.mounts is a dict, containing app name and where to mount\\n        if where it mount is None then again don't mount this fellow.\\n        \"\n    if (getattr(self, 'mounts', None) is None):\n        return\n    if (not regex.startswith('/')):\n        return regex\n    if (not mod):\n        if isinstance(v_or_f, basestring):\n            mod = v_or_f\n        else:\n            mod = v_or_f.__module__\n    (best_k, best_v) = ('', None)\n    for (k, v) in tuple(self.mounts.items()):\n        if (mod.startswith(k) and (len(k) > len(best_k))):\n            (best_k, best_v) = (k, v)\n    if best_k:\n        if (not best_v):\n            return\n        if (not best_v.endswith('/')):\n            best_k += '/'\n        if (best_v != '/'):\n            regex = (best_v[:(- 1)] + regex)\n    return regex\n", "label": 1}
{"function": "\n\ndef _split_mul(f, x):\n    '\\n    Split expression ``f`` into fac, po, g, where fac is a constant factor,\\n    po = x**s for some s independent of s, and g is \"the rest\".\\n\\n    >>> from sympy.integrals.meijerint import _split_mul\\n    >>> from sympy import sin\\n    >>> from sympy.abc import s, x\\n    >>> _split_mul((3*x)**s*sin(x**2)*x, x)\\n    (3**s, x*x**s, sin(x**2))\\n    '\n    from sympy import polarify, unpolarify\n    fac = S(1)\n    po = S(1)\n    g = S(1)\n    f = expand_power_base(f)\n    args = Mul.make_args(f)\n    for a in args:\n        if (a == x):\n            po *= x\n        elif (x not in a.free_symbols):\n            fac *= a\n        else:\n            if (a.is_Pow and (x not in a.exp.free_symbols)):\n                (c, t) = a.base.as_coeff_mul(x)\n                if (t != (x,)):\n                    (c, t) = expand_mul(a.base).as_coeff_mul(x)\n                if (t == (x,)):\n                    po *= (x ** a.exp)\n                    fac *= unpolarify(polarify((c ** a.exp), subs=False))\n                    continue\n            g *= a\n    return (fac, po, g)\n", "label": 0}
{"function": "\n\ndef generate_column_map(self):\n    column_map = []\n    models = set([self.model])\n    for (i, node) in enumerate(self.column_meta):\n        attr = conv = None\n        if isinstance(node, Field):\n            if isinstance(node, FieldProxy):\n                key = node._model_alias\n                constructor = node.model\n                conv = node.field_instance.python_value\n            else:\n                key = constructor = node.model_class\n                conv = node.python_value\n            attr = (node._alias or node.name)\n        else:\n            if (node._bind_to is None):\n                key = constructor = self.model\n            else:\n                key = constructor = node._bind_to\n            if (isinstance(node, Node) and node._alias):\n                attr = node._alias\n            elif isinstance(node, Entity):\n                attr = node.path[(- 1)]\n        column_map.append((key, constructor, attr, conv))\n        models.add(key)\n    return (column_map, models)\n", "label": 1}
{"function": "\n\ndef CreateInstance(self, punkouter=None, interface=None, dynamic=False):\n    if dynamic:\n        if (interface is not None):\n            raise ValueError('interface and dynamic are mutually exclusive')\n        realInterface = comtypes.automation.IDispatch\n    elif (interface is None):\n        realInterface = comtypes.IUnknown\n    else:\n        realInterface = interface\n    obj = ctypes.POINTER(realInterface)()\n    self.__com_CreateInstance(punkouter, realInterface._iid_, ctypes.byref(obj))\n    if dynamic:\n        return comtypes.client.dynamic.Dispatch(obj)\n    elif (interface is None):\n        return comtypes.client.GetBestInterface(obj)\n    return obj\n", "label": 0}
{"function": "\n\ndef makeLines(self, diff, lines_to_add, comment_lists):\n    lines = []\n    for (old, new) in lines_to_add:\n        context = self.makeContext(diff, old[0], new[0])\n        lines.append(SideDiffLine(self.app, context, old, new, callback=self.onSelect))\n        key = ('old-%s-%s' % (old[0], diff.oldname))\n        old_list = comment_lists.pop(key, [])\n        key = ('new-%s-%s' % (new[0], diff.newname))\n        new_list = comment_lists.pop(key, [])\n        while (old_list or new_list):\n            old_comment_key = new_comment_key = None\n            old_comment = new_comment = ''\n            if old_list:\n                (old_comment_key, old_comment) = old_list.pop(0)\n            if new_list:\n                (new_comment_key, new_comment) = new_list.pop(0)\n            lines.append(SideDiffComment(context, old_comment, new_comment))\n        key = ('olddraft-%s-%s' % (old[0], diff.oldname))\n        old_list = comment_lists.pop(key, [])\n        key = ('newdraft-%s-%s' % (new[0], diff.newname))\n        new_list = comment_lists.pop(key, [])\n        while (old_list or new_list):\n            old_comment_key = new_comment_key = None\n            old_comment = new_comment = ''\n            if old_list:\n                (old_comment_key, old_comment) = old_list.pop(0)\n            if new_list:\n                (new_comment_key, new_comment) = new_list.pop(0)\n            lines.append(SideDiffCommentEdit(self.app, context, old_comment_key, new_comment_key, old_comment, new_comment))\n    return lines\n", "label": 1}
{"function": "\n\ndef _recursively_embed(self, doc, update_state=True):\n    'Crafts a navigator from a hal-json embedded document'\n    self_link = None\n    self_uri = utils.getpath(doc, '_links.self.href')\n    if (self_uri is not None):\n        uri = urlparse.urljoin(self.uri, self_uri)\n        self_link = Link(uri=uri, properties=utils.getpath(doc, '_links.self'))\n    curies = utils.getpath(doc, '_links.curies')\n    state = utils.getstate(doc)\n    if (self_link is None):\n        nav = OrphanHALNavigator(link=None, response=None, parent=self, core=self._core, curies=curies, state=state)\n    else:\n        nav = HALNavigator(link=self_link, response=None, core=self._core, curies=curies, state=state)\n    if update_state:\n        nav.state = state\n    links = self._make_links_from(doc)\n    if (links is not None):\n        nav._links = links\n    embedded = self._make_embedded_from(doc)\n    if (embedded is not None):\n        nav._embedded = embedded\n    return nav\n", "label": 0}
{"function": "\n\ndef write_test_serialisation(self, tests, datafile):\n    arr = []\n    for t in tests:\n        exe = t.get_exe()\n        if isinstance(exe, dependencies.ExternalProgram):\n            fname = exe.fullpath\n        else:\n            fname = [os.path.join(self.environment.get_build_dir(), self.get_target_filename(t.get_exe()))]\n        is_cross = (self.environment.is_cross_build() and self.environment.cross_info.need_cross_compiler())\n        if is_cross:\n            exe_wrapper = self.environment.cross_info.config['binaries'].get('exe_wrapper', None)\n        else:\n            exe_wrapper = None\n        if mesonlib.is_windows():\n            extra_paths = self.determine_windows_extra_paths(exe)\n        else:\n            extra_paths = []\n        cmd_args = []\n        for a in t.cmd_args:\n            if isinstance(a, mesonlib.File):\n                a = os.path.join(self.environment.get_build_dir(), a.rel_to_builddir(self.build_to_src))\n            cmd_args.append(a)\n        ts = TestSerialisation(t.get_name(), t.suite, fname, is_cross, exe_wrapper, t.is_parallel, cmd_args, t.env, t.should_fail, t.valgrind_args, t.timeout, t.workdir, extra_paths)\n        arr.append(ts)\n    pickle.dump(arr, datafile)\n", "label": 0}
{"function": "\n\ndef _fetch_cdn_data(self):\n    \"Fetches the container's CDN data from the CDN service\"\n    if (self._cdn_enabled is FAULT):\n        headers = self.manager.fetch_cdn_data(self)\n    else:\n        headers = {\n            \n        }\n    self._set_cdn_defaults()\n    if (not headers):\n        return\n    else:\n        self._cdn_enabled = True\n    for (key, value) in headers.items():\n        low_key = key.lower()\n        if (low_key == 'x-cdn-uri'):\n            self._cdn_uri = value\n        elif (low_key == 'x-ttl'):\n            self._cdn_ttl = int(value)\n        elif (low_key == 'x-cdn-ssl-uri'):\n            self._cdn_ssl_uri = value\n        elif (low_key == 'x-cdn-streaming-uri'):\n            self._cdn_streaming_uri = value\n        elif (low_key == 'x-cdn-ios-uri'):\n            self._cdn_ios_uri = value\n        elif (low_key == 'x-log-retention'):\n            self._cdn_log_retention = (value == 'True')\n", "label": 1}
{"function": "\n\ndef _load_translation(directory, domain):\n    for lang in os.listdir(directory):\n        if lang.startswith('.'):\n            continue\n        if os.path.isfile(os.path.join(directory, lang)):\n            continue\n        if (not os.path.exists(os.path.join(directory, lang, 'LC_MESSAGES', (domain + '.mo')))):\n            continue\n        gettext_translations = gettext.translation(domain, directory, languages=[lang], class_=WebtoolsTranslation)\n        if (lang in t_locale._translations):\n            t_locale._translations[lang].merge(gettext_translations)\n        else:\n            t_locale._translations[lang] = gettext_translations\n", "label": 0}
{"function": "\n\ndef handleEvents(self, event, fpsClock):\n    LcarsScreen.handleEvents(self, event, fpsClock)\n    if (event.type == pygame.MOUSEBUTTONDOWN):\n        if (self.attempts > 1):\n            self.granted = True\n            self.sound_beep1.play()\n        else:\n            if (self.attempts == 0):\n                self.sound_deny1.play()\n            else:\n                self.sound_deny2.play()\n            self.granted = False\n            self.attempts += 1\n    if (event.type == pygame.MOUSEBUTTONUP):\n        if self.granted:\n            self.sound_granted.play()\n            from screens.main import ScreenMain\n            self.loadScreen(ScreenMain())\n        else:\n            self.sound_denied.play()\n    return False\n", "label": 0}
{"function": "\n\ndef service_check(self, check_name, status, tags=None, timestamp=None, hostname=None, message=None):\n    \"\\n        Send a service check run.\\n\\n        >>> statsd.service_check('my_service.check_name', DogStatsd.WARNING)\\n        \"\n    message = (self._escape_service_check_message(message) if (message is not None) else '')\n    string = '_sc|{0}|{1}'.format(check_name, status)\n    if timestamp:\n        string = '{0}|d:{1}'.format(string, timestamp)\n    if hostname:\n        string = '{0}|h:{1}'.format(string, hostname)\n    if tags:\n        string = '{0}|#{1}'.format(string, ','.join(tags))\n    if message:\n        string = '{0}|m:{1}'.format(string, message)\n    self._send(string)\n", "label": 0}
{"function": "\n\ndef append_dialog(self, paragraphs):\n    if (len(self.lines) < 2):\n        return False\n    character = self.lines[0]\n    if character.endswith(TWOSPACE):\n        return False\n    if (character.startswith('@') and (len(character) >= 2)):\n        character = character[1:]\n    elif (not character.isupper()):\n        return False\n    if (paragraphs and isinstance(paragraphs[(- 1)], Dialog)):\n        dual_match = dual_dialog_re.match(character)\n        if dual_match:\n            previous = paragraphs.pop()\n            dialog = self._create_dialog(dual_match.group(1))\n            paragraphs.append(DualDialog(previous, dialog))\n            return True\n    paragraphs.append(self._create_dialog(character))\n    return True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_dict(connector_dict):\n    connector_dict.setdefault('link_config', [])\n    connector_dict['link_config'] = [Config.from_dict(link_config_dict) for link_config_dict in connector_dict['link-config']]\n    connector_dict.setdefault('job_config', {\n        \n    })\n    connector_dict['job_config'] = {\n        \n    }\n    if ('FROM' in connector_dict['job-config']):\n        connector_dict['job_config']['FROM'] = [Config.from_dict(from_config_dict) for from_config_dict in connector_dict['job-config']['FROM']]\n    if ('TO' in connector_dict['job-config']):\n        connector_dict['job_config']['TO'] = [Config.from_dict(to_config_dict) for to_config_dict in connector_dict['job-config']['TO']]\n    connector_dict['config_resources'] = connector_dict['all-config-resources']\n    return Connector(**force_dict_to_strings(connector_dict))\n", "label": 0}
{"function": "\n\ndef stop(self):\n    if (not (self.inputView.id() in watchers)):\n        return\n    print(('Stop watching: ' + self.inputView.file_name()))\n    del watchers[self.inputView.id()]\n    window = (self.outputView.window() or self.inputView.window())\n    if self.outputView.window():\n        window.focus_view(self.outputView)\n        window.run_command('close')\n    if ((len(watchers) == 0) and (len(window.views_in_group(1)) == 0)):\n        window.run_command('set_layout', {\n            'cols': [0.0, 1.0],\n            'rows': [0.0, 1.0],\n            'cells': [[0, 0, 1, 1]],\n        })\n", "label": 0}
{"function": "\n\ndef fix_requirements(f):\n    requirements = []\n    before = []\n    after = []\n    for line in f:\n        before.append(line)\n        if ((not len(requirements)) or (requirements[(- 1)].value is not None)):\n            requirements.append(Requirement())\n        requirement = requirements[(- 1)]\n        if ((len(requirements) == 1) and (line.strip() == b'')):\n            if (len(requirement.comments) and requirement.comments[0].startswith(b'#')):\n                requirement.value = b'\\n'\n            else:\n                requirement.comments.append(line)\n        elif (line.startswith(b'#') or (line.strip() == b'')):\n            requirement.comments.append(line)\n        else:\n            requirement.value = line\n    for requirement in sorted(requirements):\n        for comment in requirement.comments:\n            after.append(comment)\n        after.append(requirement.value)\n    before_string = b''.join(before)\n    after_string = b''.join(after)\n    if (before_string == after_string):\n        return 0\n    else:\n        f.seek(0)\n        f.write(after_string)\n        f.truncate()\n        return 1\n", "label": 1}
{"function": "\n\ndef parse_json(self, page, exactly_one=True):\n    'Parse display name, latitude, and longitude from an JSON response.'\n    if (not isinstance(page, basestring)):\n        page = decode_page(page)\n    resources = json.loads(page)\n    if (exactly_one and (len(resources) != 1)):\n        from warnings import warn\n        warn((\"Didn't find exactly one resource!\" + ('(Found %d.), use exactly_one=False\\n' % len(resources))))\n\n    def parse_resource(resource):\n        location = resource['display_name']\n        latitude = (resource['lat'] or None)\n        longitude = (resource['lon'] or None)\n        if (latitude and longitude):\n            latitude = float(latitude)\n            longitude = float(longitude)\n        return (location, (latitude, longitude))\n    if exactly_one:\n        return parse_resource(resources[0])\n    else:\n        return [parse_resource(resource) for resource in resources]\n", "label": 0}
{"function": "\n\n@classdef.method('[]=')\n@check_frozen()\ndef method_subscript_assign(self, space, w_idx, w_count_or_obj, w_obj=None):\n    w_count = None\n    if w_obj:\n        w_count = w_count_or_obj\n    else:\n        w_obj = w_count_or_obj\n    (start, end, as_range, _) = space.subscript_access(self.length(), w_idx, w_count=w_count)\n    if (w_count and (end < start)):\n        raise space.error(space.w_IndexError, ('negative length (%d)' % (end - start)))\n    elif (start < 0):\n        raise space.error(space.w_IndexError, ('index %d too small for array; minimum: %d' % ((start - self.length()), (- self.length()))))\n    elif as_range:\n        w_converted = space.convert_type(w_obj, space.w_array, 'to_ary', raise_error=False)\n        if (w_converted is space.w_nil):\n            rep_w = [w_obj]\n        else:\n            rep_w = space.listview(w_converted)\n        self._subscript_assign_range(space, start, end, rep_w)\n    elif (start >= self.length()):\n        self.items_w += ([space.w_nil] * ((start - self.length()) + 1))\n        self.items_w[start] = w_obj\n    else:\n        self.items_w[start] = w_obj\n    return w_obj\n", "label": 0}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef manage_processes(self):\n    'Manage processes.'\n    if self.is_stopped():\n        return\n    for process in list(self.processes.values()):\n        if (process.status in (DEAD_OR_ZOMBIE, UNEXISTING)):\n            self.processes.pop(process.pid)\n    if self.max_age:\n        (yield self.remove_expired_processes())\n    if ((len(self.processes) < self.numprocesses) and (not self.is_stopping())):\n        if self.respawn:\n            (yield self.spawn_processes())\n        elif ((not len(self.processes)) and (not self.on_demand)):\n            (yield self._stop())\n    if (len(self.processes) > self.numprocesses):\n        processes_to_kill = []\n        for process in sorted(self.processes.values(), key=(lambda process: process.started), reverse=True)[self.numprocesses:]:\n            if (process.status in (DEAD_OR_ZOMBIE, UNEXISTING)):\n                self.processes.pop(process.pid)\n            else:\n                processes_to_kill.append(process)\n        removes = (yield [self.kill_process(process) for process in processes_to_kill])\n        for (i, process) in enumerate(processes_to_kill):\n            if removes[i]:\n                self.processes.pop(process.pid)\n", "label": 1}
{"function": "\n\ndef stripEmptyGeometry(mesh):\n    to_delete = []\n    geoms_to_delete = []\n    for (i, geom) in enumerate(mesh.geometries):\n        if (len(geom.primitives) == 0):\n            to_delete.append(i)\n            geoms_to_delete.append(geom)\n    for scene in mesh.scenes:\n        nodes_to_check = []\n        nodes_to_check.extend(scene.nodes)\n        while (len(nodes_to_check) > 0):\n            curnode = nodes_to_check.pop()\n            scene_nodes_to_delete = []\n            for (i, node) in enumerate(curnode.children):\n                if isinstance(node, collada.scene.Node):\n                    nodes_to_check.append(node)\n                elif isinstance(node, collada.scene.GeometryNode):\n                    if (node.geometry in geoms_to_delete):\n                        scene_nodes_to_delete.append(i)\n            scene_nodes_to_delete.sort(reverse=True)\n            for i in scene_nodes_to_delete:\n                del curnode.children[i]\n    to_delete.sort(reverse=True)\n    for i in to_delete:\n        del mesh.geometries[i]\n", "label": 1}
{"function": "\n\ndef selfloop_edges(self, data=False):\n    'Return a list of selfloop edges.\\n\\n        A selfloop edge has the same node at both ends.\\n\\n        Parameters\\n        -----------\\n        data : bool, optional (default=False)\\n            Return selfloop edges as two tuples (u,v) (data=False)\\n            or three-tuples (u,v,data) (data=True)\\n\\n        Returns\\n        -------\\n        edgelist : list of edge tuples\\n            A list of all selfloop edges.\\n\\n        See Also\\n        --------\\n        nodes_with_selfloops, number_of_selfloops\\n\\n        Examples\\n        --------\\n        >>> G = nx.Graph()   # or DiGraph, MultiGraph, MultiDiGraph, etc\\n        >>> G.add_edge(1,1)\\n        >>> G.add_edge(1,2)\\n        >>> G.selfloop_edges()\\n        [(1, 1)]\\n        >>> G.selfloop_edges(data=True)\\n        [(1, 1, {})]\\n        '\n    if data:\n        return [(n, n, nbrs[n]) for (n, nbrs) in self.adj.items() if (n in nbrs)]\n    else:\n        return [(n, n) for (n, nbrs) in self.adj.items() if (n in nbrs)]\n", "label": 0}
{"function": "\n\ndef execute(self, service, shared_data):\n    osutils = osutils_factory.get_os_utils()\n    network_details = service.get_network_details()\n    if (not network_details):\n        return (plugin_base.PLUGIN_EXECUTION_DONE, False)\n    network_adapters = osutils.get_network_adapters()\n    network_details = _preprocess_nics(network_details, network_adapters)\n    macnics = {\n        \n    }\n    for nic in network_details:\n        macnics[nic.mac] = nic\n    adapter_macs = [pair[1] for pair in network_adapters]\n    reboot_required = False\n    configured = False\n    for mac in adapter_macs:\n        nic = macnics.pop(mac, None)\n        if (not nic):\n            LOG.warn('Missing details for adapter %s', mac)\n            continue\n        LOG.info('Configuring network adapter %s', mac)\n        reboot = osutils.set_static_network_config(mac, nic.address, nic.netmask, nic.broadcast, nic.gateway, nic.dnsnameservers)\n        reboot_required = (reboot or reboot_required)\n        if (nic.address6 and nic.netmask6):\n            osutils.set_static_network_config_v6(mac, nic.address6, nic.netmask6, nic.gateway6)\n        configured = True\n    for mac in macnics:\n        LOG.warn('Details not used for adapter %s', mac)\n    if (not configured):\n        LOG.error('No adapters were configured')\n    return (plugin_base.PLUGIN_EXECUTION_DONE, reboot_required)\n", "label": 1}
{"function": "\n\ndef main():\n    cwd = os.getcwd()\n    if ((not (cwd in sys.path)) or (not (cwd.strip(os.sep) in sys.path))):\n        sys.path.append(cwd)\n    (found, kwargs) = (False, {\n        \n    })\n    manager = EntryPointPluginManager()\n    manager.loadPlugins()\n    for plugin in manager.plugins:\n        if isinstance(plugin, DjangoPlugin):\n            found = True\n            break\n    if (not found):\n        kwargs = {\n            'addplugins': [DjangoPlugin()],\n        }\n    os.environ['NOSE_WITH_DJANGO'] = '1'\n    nose.main(**kwargs)\n", "label": 0}
{"function": "\n\ndef __init__(self, name_or_email, email=None, encoding='utf-8'):\n    self.encoding = encoding\n    if (email is None):\n        if isinstance(name_or_email, AddressList):\n            if (not (0 < len(name_or_email) < 2)):\n                raise ValueError('AddressList to convert must only contain a single Address.')\n            name_or_email = unicode(name_or_email[0])\n        if isinstance(name_or_email, (tuple, list)):\n            self.name = unicodestr(name_or_email[0], encoding)\n            self.address = unicodestr(name_or_email[1], encoding)\n        elif isinstance(name_or_email, bytes):\n            (self.name, self.address) = parseaddr(unicodestr(name_or_email, encoding))\n        elif isinstance(name_or_email, unicode):\n            (self.name, self.address) = parseaddr(name_or_email)\n        else:\n            raise TypeError('Expected string, tuple or list, got {0} instead'.format(repr(type(name_or_email))))\n    else:\n        self.name = unicodestr(name_or_email, encoding)\n        self.address = unicodestr(email, encoding)\n    (email, err) = EmailValidator().validate_email(self.address)\n    if err:\n        raise ValueError('\"{0}\" is not a valid e-mail address: {1}'.format(email, err))\n", "label": 0}
{"function": "\n\n@wrap_exceptions\ndef set_process_ionice(self, ioclass, value):\n    if (ioclass in (IOPRIO_CLASS_NONE, None)):\n        if value:\n            raise ValueError(\"can't specify value with IOPRIO_CLASS_NONE\")\n        ioclass = IOPRIO_CLASS_NONE\n        value = 0\n    if (ioclass in (IOPRIO_CLASS_RT, IOPRIO_CLASS_BE)):\n        if (value is None):\n            value = 4\n    elif (ioclass == IOPRIO_CLASS_IDLE):\n        if value:\n            raise ValueError(\"can't specify value with IOPRIO_CLASS_IDLE\")\n        value = 0\n    else:\n        value = 0\n    if (not (0 <= value <= 8)):\n        raise ValueError('value argument range expected is between 0 and 8')\n    return _psutil_linux.ioprio_set(self.pid, ioclass, value)\n", "label": 0}
{"function": "\n\ndef process_request(self, request):\n    '\\n        Checks whether the page is already cached and returns the cached\\n        version if available.\\n        '\n    if (request.method not in ('GET', 'HEAD')):\n        request._cache_update_cache = False\n        return None\n    cache_key = get_cache_key(request, self.key_prefix, 'GET', cache=self.cache)\n    if (cache_key is None):\n        request._cache_update_cache = True\n        return None\n    response = self.cache.get(cache_key)\n    if ((response is None) and (request.method == 'HEAD')):\n        cache_key = get_cache_key(request, self.key_prefix, 'HEAD', cache=self.cache)\n        response = self.cache.get(cache_key)\n    if (response is None):\n        request._cache_update_cache = True\n        return None\n    request._cache_update_cache = False\n    return response\n", "label": 0}
{"function": "\n\ndef _translate_lods_suffix(self, tb, instruction, suffix):\n    if (self._arch_mode == ARCH_X86_MODE_32):\n        src = ReilRegisterOperand('esi', 32)\n    elif (self._arch_mode == ARCH_X86_MODE_64):\n        src = ReilRegisterOperand('rsi', 64)\n    else:\n        raise Exception('Invalid architecture mode: %d', self._arch_mode)\n    if (suffix == 'b'):\n        dst = ReilRegisterOperand('al', 8)\n    elif (suffix == 'w'):\n        dst = ReilRegisterOperand('ax', 16)\n    elif (suffix == 'd'):\n        dst = ReilRegisterOperand('eax', 32)\n    elif (suffix == 'q'):\n        dst = ReilRegisterOperand('rax', 64)\n    else:\n        raise Exception(('Invalid instruction suffix: %s' % suffix))\n    if instruction.prefix:\n        (counter, loop_start_lbl) = self._rep_prefix_begin(tb, instruction)\n    tb.add(self._builder.gen_ldm(src, dst))\n    self._update_strings_src(tb, src, dst.size, instruction)\n    if instruction.prefix:\n        self._rep_prefix_end(tb, instruction, counter, loop_start_lbl)\n", "label": 1}
{"function": "\n\ndef clean(self, value):\n    super(CommaSeparatedUserField, self).clean(value)\n    names = set(value.split(','))\n    names_set = set([name.strip() for name in names])\n    users = list(get_user_model().objects.filter(username__in=names_set))\n    unknown_names = (names_set ^ set([user.username for user in users]))\n    recipient_filter = self._recipient_filter\n    invalid_users = []\n    if (recipient_filter is not None):\n        for r in users:\n            if (recipient_filter(r) is False):\n                users.remove(r)\n                invalid_users.append(r.username)\n    if (unknown_names or invalid_users):\n        humanized_usernames = ', '.join((list(unknown_names) + invalid_users))\n        raise forms.ValidationError((_('The following usernames are incorrect: %(users)s.') % {\n            'users': humanized_usernames,\n        }))\n    return users\n", "label": 0}
{"function": "\n\ndef process_sequences(self, playlist, sequences):\n    (first_sequence, last_sequence) = (sequences[0], sequences[(- 1)])\n    if (first_sequence.segment.key and (first_sequence.segment.key.method != 'NONE')):\n        self.logger.debug('Segments in this playlist are encrypted')\n        if (not CAN_DECRYPT):\n            raise StreamError('Need pyCrypto installed to decrypt this stream')\n    self.playlist_changed = ([s.num for s in self.playlist_sequences] != [s.num for s in sequences])\n    self.playlist_reload_time = (playlist.target_duration or last_sequence.segment.duration)\n    self.playlist_sequences = sequences\n    if (not self.playlist_changed):\n        self.playlist_reload_time = max((self.playlist_reload_time / 2), 1)\n    if playlist.is_endlist:\n        self.playlist_end = last_sequence.num\n    if (self.playlist_sequence < 0):\n        if (self.playlist_end is None):\n            edge_index = (- min(len(sequences), max(int(self.live_edge), 1)))\n            edge_sequence = sequences[edge_index]\n            self.playlist_sequence = edge_sequence.num\n        else:\n            self.playlist_sequence = first_sequence.num\n", "label": 1}
{"function": "\n\ndef _custom_target_list_to_child_list(custom_targets, is_operator):\n    \"\\n    Convert a list of custom targets to a list of dictionaries\\n    formatted for use in DFP\\n\\n    @param custom_targets: list(Custom)\\n    @param is_operator: str, 'IS' or 'IS_NOT'\\n    @return: list(dict)\\n    \"\n    _children = defaultdict((lambda : defaultdict(list)))\n    for value in custom_targets:\n        _children[value.parent_id][is_operator].append(value)\n    dfp_children = []\n    for (_parent_id, _operations) in _children.iteritems():\n        for (_operator, _values) in _operations.iteritems():\n            _value_target_ids = []\n            _node_type = None\n            _node_value_type = None\n            for _custom_value in _values:\n                _this_node_type = _custom_value.node_key\n                _this_node_value_type = _custom_value.id_key\n                if ((not _node_type) and (not _node_value_type)):\n                    _node_type = _this_node_type\n                    _node_value_type = _this_node_value_type\n                if ((_this_node_type != _custom_value.node_key) or (_this_node_value_type != _custom_value.id_key)):\n                    raise ParselmouthException(('Could not serialize Custom Target: %s' % _custom_value))\n                _value_target_ids.append(_custom_value.id)\n            _doc = {\n                'operator': _operator,\n                'xsi_type': _node_type,\n                _node_value_type: _value_target_ids,\n            }\n            if _parent_id:\n                _doc['keyId'] = _parent_id\n            dfp_children.append(_doc)\n    return dfp_children\n", "label": 1}
{"function": "\n\n@attr(migrated_tenant=['admin', 'tenant1', 'tenant2'])\ndef test_tenant_key_exists_on_dst(self):\n    \"Validate deleted tenant's keypairs were migrated.\"\n    unused_keypairs = []\n    for (tenant_name, tenant) in self.deleted_tenants:\n        migrated_keys = [key['name'] for key in config.keypairs if (key['user'] in self._get_tenant_users_with_keypair(tenant_name))]\n        keys_list = []\n        for dst_vm in self.dst_vm_list:\n            for src_vm in tenant['vms']:\n                if (dst_vm.name == src_vm['name']):\n                    if ('key_name' not in src_vm):\n                        continue\n                    if (dst_vm.key_name not in migrated_keys):\n                        keys_list.append(dst_vm.key_name)\n        if keys_list:\n            unused_keypairs.append({\n                tenant_name: keys_list,\n            })\n    if unused_keypairs:\n        msg = \"Tenant's key_pairs {0} exist on destination, but should be deleted!\"\n        self.fail(msg.format(unused_keypairs))\n", "label": 1}
{"function": "\n\ndef unescape(item):\n    if (not isinstance(item, basestring)):\n        return item\n    result = []\n    unprocessed = item\n    while True:\n        res = _ESCAPE_RE.search(unprocessed)\n        if (res is None):\n            result.append(unprocessed)\n            break\n        result.append(unprocessed[:res.start()])\n        escapes = res.group(1)\n        nextchars = res.group(2)\n        unprocessed = unprocessed[res.end():]\n        result.append(('\\\\' * (len(escapes) / 2)))\n        if (((len(escapes) % 2) == 0) or (len(nextchars) == 0) or (nextchars[0] not in ['n', 'r', 't'])):\n            result.append(nextchars)\n        elif (nextchars[0] == 'n'):\n            if ((len(nextchars) == 1) or (nextchars[1] == ' ')):\n                result.append('\\n')\n            else:\n                result.append(('\\n' + nextchars[1]))\n        elif (nextchars[0] == 'r'):\n            result.append(('\\r' + nextchars[1:]))\n        else:\n            result.append(('\\t' + nextchars[1:]))\n    return ''.join(result)\n", "label": 1}
{"function": "\n\ndef _human_st_mode(self, mode):\n    'Convert the st_mode returned by stat in human readable (ls-like) format'\n    hr = ''\n    if os.path.stat.S_ISREG(mode):\n        hr = '-'\n    elif os.path.stat.S_ISLNK(mode):\n        hr = 'l'\n    elif os.path.stat.S_ISSOCK(mode):\n        hr = 's'\n    elif os.path.stat.S_ISDIR(mode):\n        hr = 'd'\n    elif os.path.stat.S_ISBLK(mode):\n        hr = 'b'\n    elif os.path.stat.S_ISFIFO(mode):\n        hr = 'p'\n    elif os.path.stat.S_ISCHR(mode):\n        hr = 'c'\n    else:\n        hr = '-'\n    for who in ('USR', 'GRP', 'OTH'):\n        for perm in ('R', 'W', 'X'):\n            if (mode & getattr(os.path.stat, (('S_I' + perm) + who))):\n                hr += perm.lower()\n            else:\n                hr += '-'\n    return hr\n", "label": 1}
{"function": "\n\n@ensure_tag(['tbl'])\ndef get_rowspan_data(table):\n    w_namespace = get_namespace(table, 'w')\n    tr_index = 0\n    td_index = 0\n    tr_rows = list(table.xpath('.//w:tr', namespaces=table.nsmap))\n    for tr in table.xpath('.//w:tr', namespaces=table.nsmap):\n        for td in tr.xpath('.//w:tc', namespaces=tr.nsmap):\n            v_merge = get_v_merge(td)\n            if (v_merge is None):\n                td_index += get_grid_span(td)\n                continue\n            if (v_merge.get(('%sval' % w_namespace)) == 'restart'):\n                row_span = 1\n                for tr_el in tr_rows[(tr_index + 1):]:\n                    td_el = get_td_at_index(tr_el, td_index)\n                    td_el_v_merge = get_v_merge(td_el)\n                    if (td_el_v_merge is None):\n                        break\n                    val = td_el_v_merge.get(('%sval' % w_namespace))\n                    if (val == 'restart'):\n                        break\n                    row_span += 1\n                (yield row_span)\n            td_index += get_grid_span(td)\n        tr_index += 1\n        td_index = 0\n", "label": 0}
{"function": "\n\n@eventmgr_rfc1459.message('NAMES', min_params=1)\ndef m_NAMES(cli, ev_msg):\n    chanlist = ev_msg['params'][0].split(',')\n    for chan in chanlist:\n        if (not validate_chan(chan)):\n            cli.dump_numeric('479', [chan, 'Illegal channel name'])\n            return\n        ch = cli.ctx.chmgr.get(ev_msg['params'][0], create=False)\n        if (not ch):\n            cli.dump_numeric('403', [chan, 'No such channel'])\n            return\n        names_f = (lambda x: True)\n        if (not ch.has_member(cli)):\n            names_f = (lambda x: ('user:invisible' not in x.client.props))\n        if ('userhost-in-names' in cli.caps):\n            cli.dump_numeric('353', [ch.classification, ch.name, ' '.join([m.hostmask for m in filter(names_f, ch.members)])])\n        else:\n            cli.dump_numeric('353', [ch.classification, ch.name, ' '.join([m.name for m in filter(names_f, ch.members)])])\n        cli.dump_numeric('366', [ch.name, 'End of /NAMES list.'])\n", "label": 0}
{"function": "\n\ndef yesno(question, default=None, all=False):\n    if (default is True):\n        question = ('%s [Yes/no' % question)\n        answers = {\n            False: ('n', 'no'),\n            True: ('', 'y', 'yes'),\n        }\n    elif (default is False):\n        question = ('%s [yes/No' % question)\n        answers = {\n            False: ('', 'n', 'no'),\n            True: ('y', 'yes'),\n        }\n    else:\n        question = ('%s [yes/no' % question)\n        answers = {\n            False: ('n', 'no'),\n            True: ('y', 'yes'),\n        }\n    if all:\n        if (default is 'all'):\n            answers['all'] = ('', 'a', 'all')\n            question = ('%s/All' % question)\n        else:\n            answers['all'] = ('a', 'all')\n            question = ('%s/all' % question)\n    question = ('%s] ' % question)\n    while 1:\n        answer = get_input(question).lower()\n        for option in answers:\n            if (answer in answers[option]):\n                return option\n        if all:\n            print('You have to answer with y, yes, n, no, a or all.', file=sys.stderr)\n        else:\n            print('You have to answer with y, yes, n or no.', file=sys.stderr)\n", "label": 1}
{"function": "\n\ndef filter_queryset(self, request, queryset, view):\n    self.request = request\n    self.view = view\n    for (name, idents) in self.filter_options.items():\n        if (name == 'extra'):\n            continue\n        ids = [ident.object_id for ident in idents]\n        fn = getattr(self, ('filter_by_%s' % name), None)\n        if (not fn):\n            fn = getattr(self.view, ('filter_by_%s' % name), None)\n        if fn:\n            queryset = fn(queryset, ids)\n        else:\n            raise Exception((\"Don't know how to filter by %s\" % name))\n    if ('extra' in self.filter_options):\n        fn = getattr(self.view, 'filter_by_extra', self.filter_by_extra)\n        queryset = fn(queryset, self.filter_options['extra'])\n    return queryset\n", "label": 0}
{"function": "\n\ndef get_rank_score(variant_line=None, variant_dict=None, family_id=0):\n    '\\n    Return the rank score priority for a certain family.\\n    \\n    If no family is given the first family found is used\\n    \\n    Arguments:\\n        variant_line (str): A vcf variant line\\n        variant_dict (dict): A variant dictionary\\n        family_id (str): A family id\\n    \\n    Return:\\n        rank_score (str): The rank score for this variant\\n    '\n    rank_score = (- 100)\n    raw_entry = None\n    if variant_line:\n        variant_line = variant_line.split('\\t')\n        for info_annotation in variant_line[7].split(';'):\n            info_annotation = info_annotation.split('=')\n            if (len(info_annotation) == 2):\n                key = info_annotation[0]\n                value = info_annotation[1]\n            if (key == 'RankScore'):\n                raw_entry = value\n                break\n    elif variant_dict:\n        raw_entry = variant_dict['info_dict'].get('RankScore')\n    if raw_entry:\n        for family_annotation in raw_entry.split(','):\n            family_annotation = family_annotation.split(':')\n            if family_id:\n                if (family_id == family_annotation[0]):\n                    rank_score = float(family_annotation[1])\n            else:\n                rank_score = float(family_annotation[1])\n    return str(rank_score)\n", "label": 1}
{"function": "\n\n@classmethod\ndef setUpClass(cls):\n    TestShowLatency.rc = controller.RootController()\n    actual_out = util.capture_stdout(TestShowLatency.rc.execute, ['show', 'latency'])\n    TestShowLatency.output_list = test_util.get_separate_output(actual_out, 'Latency')\n    for item in TestShowLatency.output_list:\n        if ('~~~proxy Latency~~' in item):\n            TestShowLatency.proxy_latency = item\n        elif ('~~query Latency~~' in item):\n            TestShowLatency.query_latency = item\n        elif ('~~reads Latency~~' in item):\n            TestShowLatency.reads_latency = item\n        elif ('~~udf Latency~~' in item):\n            TestShowLatency.udf_latency = item\n        elif ('~~writes_master Latency~~' in item):\n            TestShowLatency.writes_master_latency = item\n        elif ('~~writes_reply Latency~~' in item):\n            TestShowLatency.writes_reply_latency = item\n", "label": 0}
{"function": "\n\ndef iterate_open_nodes(self, node=None):\n    'Generator to iterate over all the expended nodes starting from\\n        `node` and down. If `node` is `None`, the generator start with\\n        :attr:`root`.\\n\\n        To get all the open nodes::\\n\\n            treeview = TreeView()\\n            # ... add nodes ...\\n            for node in treeview.iterate_open_nodes():\\n                print(node)\\n\\n        '\n    if (not node):\n        node = self.root\n    if (self.hide_root and (node is self.root)):\n        pass\n    else:\n        (yield node)\n    if (not node.is_open):\n        return\n    f = self.iterate_open_nodes\n    for cnode in node.nodes:\n        for ynode in f(cnode):\n            (yield ynode)\n", "label": 0}
{"function": "\n\ndef get_page(self, user, max_page_size, is_above, score, score_time):\n    self._read_leaderboard()\n    scores = self.scores\n    leaderboard = []\n    player = None\n    query_complete = False\n    if (not is_above):\n        scores = reversed(scores)\n    for s in scores:\n        if is_above:\n            if (((self.sort_by * s.score) < (self.sort_by * score)) or ((s.score == score) and (s.score_time >= score_time))):\n                query_complete = True\n        elif (((self.sort_by * s.score) > (self.sort_by * score)) or ((s.score == score) and (s.score_time <= score_time))):\n            query_complete = True\n        if (query_complete and (len(leaderboard) >= max_page_size)):\n            break\n        username = s.user\n        row = self._get_row(username, s)\n        if (username == user.username):\n            player = row\n        leaderboard.append(row)\n    leaderboard = leaderboard[(- max_page_size):]\n    if (not is_above):\n        leaderboard = list(reversed(leaderboard))\n    if (player is None):\n        player = self._get_user_row(user)\n    if (len(leaderboard) > 0):\n        self._rank_leaderboard(leaderboard, self._get_rank(leaderboard[0]['score']))\n        top = (self.scores[0].user == leaderboard[0]['user']['username'])\n        bottom = (self.scores[(- 1)].user == leaderboard[(- 1)]['user']['username'])\n    else:\n        top = True\n        bottom = True\n    return self.create_response(top, bottom, leaderboard, player)\n", "label": 1}
{"function": "\n\ndef in_edges_iter(self, nbunch=None, data=False):\n    'Return an iterator over the incoming edges.\\n\\n        Parameters\\n        ----------\\n        nbunch : iterable container, optional (default= all nodes)\\n            A container of nodes.  The container will be iterated\\n            through once.\\n        data : bool, optional (default=False)\\n            If True, return edge attribute dict in 3-tuple (u,v,data).\\n\\n        Returns\\n        -------\\n        in_edge_iter : iterator\\n            An iterator of (u,v) or (u,v,d) tuples of incoming edges.\\n\\n        See Also\\n        --------\\n        edges_iter : return an iterator of edges\\n        '\n    if (nbunch is None):\n        nodes_nbrs = self.pred.items()\n    else:\n        nodes_nbrs = ((n, self.pred[n]) for n in self.nbunch_iter(nbunch))\n    if data:\n        for (n, nbrs) in nodes_nbrs:\n            for (nbr, data) in nbrs.items():\n                (yield (nbr, n, data))\n    else:\n        for (n, nbrs) in nodes_nbrs:\n            for nbr in nbrs:\n                (yield (nbr, n))\n", "label": 0}
{"function": "\n\ndef attrs(attrs=[], terse=False, undefined=None):\n    buf = []\n    if bool(attrs):\n        buf.append('')\n        for (k, v) in attrs:\n            if ((undefined is not None) and isinstance(v, undefined)):\n                continue\n            if ((v != None) and ((v != False) or (type(v) != bool))):\n                if ((k == 'class') and isinstance(v, (list, tuple))):\n                    v = ' '.join(map(str, flatten(v)))\n                t = ((v == True) and (type(v) == bool))\n                if (t and (not terse)):\n                    v = k\n                buf.append((('%s' % k) if (terse and t) else ('%s=\"%s\"' % (k, escape(v)))))\n    return ' '.join(buf)\n", "label": 1}
{"function": "\n\ndef shutdown(self):\n    LOGGER.critical('Shutting down.')\n    self.job_queue = []\n    self.queued_jobs = {\n        \n    }\n    deferreds = []\n    LOGGER.debug(('%s stopping on main HTTP interface.' % self.name))\n    d = self.site_port.stopListening()\n    if isinstance(d, Deferred):\n        deferreds.append(d)\n    if (self.reportjobspeedloop is not None):\n        LOGGER.debug('Stopping report job speed loop.')\n        d = self.reportjobspeedloop.stop()\n        if isinstance(d, Deferred):\n            deferreds.append(d)\n    if (self.jobsloop is not None):\n        LOGGER.debug('Stopping jobs loop.')\n        d = self.jobsloop.stop()\n        if isinstance(d, Deferred):\n            deferreds.append(d)\n    if (self.queryloop is not None):\n        LOGGER.debug('Stopping query loop.')\n        d = self.queryloop.stop()\n        if isinstance(d, Deferred):\n            deferreds.append(d)\n    if (self.coordinateloop is not None):\n        LOGGER.debug('Stopping coordinating loop.')\n        d = self.coordinateloop.stop()\n        if isinstance(d, Deferred):\n            deferreds.append(d)\n        LOGGER.debug('Removing data from SDB coordination domain.')\n        d = self.sdb.delete(self.aws_sdb_coordination_domain, self.uuid)\n        d.addCallback(self.peerCheckRequest)\n        deferreds.append(d)\n    if (len(deferreds) > 0):\n        d = DeferredList(deferreds)\n        d.addCallback(self._shutdownCallback)\n        return d\n    else:\n        return self._shutdownCallback(None)\n", "label": 1}
{"function": "\n\ndef get_metadata(self, engine, noisy=False, force=False, do_reflection=True):\n    'Fetch metadata for an sqlalchemy engine'\n    (db_key, ipydb_engine) = get_metadata_engine(engine)\n    create_schema(ipydb_engine)\n    db = self.databases[db_key]\n    if db.reflecting:\n        log.debug('Is already reflecting')\n        return db\n    if (not do_reflection):\n        return db\n    if force:\n        log.debug('was foreced to re-reflect')\n        db = self.read_expunge(ipydb_engine)\n        self.databases[db_key] = db\n        if noisy:\n            print('ipydb is fetching database metadata')\n        self.spawn_reflection_thread(db_key, db, engine.url)\n        return db\n    if (db.age > MAX_CACHE_AGE):\n        log.debug('Cache expired age:%s reading from sqlite', db.age)\n        db = self.read_expunge(ipydb_engine)\n        self.databases[db_key] = db\n        if (db.age > MAX_CACHE_AGE):\n            log.debug('Sqlite data too old: %s, re-reflecting', db.age)\n            if noisy:\n                print('ipydb is fetching database metadata')\n            self.spawn_reflection_thread(db_key, db, engine.url)\n    return db\n", "label": 0}
{"function": "\n\n@log_debug\ndef get_target_in_direction(level, location, direction, attack_range=100):\n    '\\n    Get target of the attack\\n\\n    :param level: level to operate\\n    :type level: Level\\n    :param location: start location\\n    :type location: (int, int)\\n    :param direction: direction to follow\\n    :type direction: int\\n    :returns: target character if found, otherwise None\\n    :rtype: Character\\n    '\n    target = None\n    target_location = location\n    target_data = None\n    if (direction == 9):\n        return TargetData('void', None, None, None)\n    off_sets = [(0, 0), (0, (- 1)), (1, (- 1)), (1, 0), (1, 1), (0, 1), ((- 1), 1), ((- 1), 0), ((- 1), (- 1))]\n    while ((target is None) and (distance_between(location, target_location) <= attack_range)):\n        target_location = tuple([x for x in map(sum, zip(target_location, off_sets[direction]))])\n        if blocks_movement(level, target_location):\n            target_data = TargetData('wall', target_location, None, target_data)\n            target = target_data\n        else:\n            target = get_character(level, target_location)\n            if target:\n                target_data = TargetData('character', target_location, target, target_data)\n                target = target_data\n            else:\n                target_data = TargetData('void', target_location, None, target_data)\n    return target_data\n", "label": 0}
{"function": "\n\ndef is_resource_modified(environ, etag=None, data=None, last_modified=None):\n    'Convenience method for conditional requests.\\n\\n    :param environ: the WSGI environment of the request to be checked.\\n    :param etag: the etag for the response for comparison.\\n    :param data: or alternatively the data of the response to automatically\\n                 generate an etag using :func:`generate_etag`.\\n    :param last_modified: an optional date of the last modification.\\n    :return: `True` if the resource was modified, otherwise `False`.\\n    '\n    if ((etag is None) and (data is not None)):\n        etag = generate_etag(data)\n    elif (data is not None):\n        raise TypeError('both data and etag given')\n    if (environ['REQUEST_METHOD'] not in ('GET', 'HEAD')):\n        return False\n    unmodified = False\n    if isinstance(last_modified, string_types):\n        last_modified = parse_date(last_modified)\n    if (last_modified is not None):\n        last_modified = last_modified.replace(microsecond=0)\n    modified_since = parse_date(environ.get('HTTP_IF_MODIFIED_SINCE'))\n    if (modified_since and last_modified and (last_modified <= modified_since)):\n        unmodified = True\n    if etag:\n        if_none_match = parse_etags(environ.get('HTTP_IF_NONE_MATCH'))\n        if if_none_match:\n            unmodified = if_none_match.contains_raw(etag)\n    return (not unmodified)\n", "label": 1}
{"function": "\n\ndef _validate_temp_url_config(self):\n    'Validate the required settings for a temporary URL.'\n    if (not CONF.glance.swift_temp_url_key):\n        raise exc.MissingParameterValue(_('Swift temporary URLs require a shared secret to be created. You must provide \"swift_temp_url_key\" as a config option.'))\n    if (not CONF.glance.swift_endpoint_url):\n        raise exc.MissingParameterValue(_('Swift temporary URLs require a Swift endpoint URL. You must provide \"swift_endpoint_url\" as a config option.'))\n    if ((not CONF.glance.swift_account) and (CONF.glance.temp_url_endpoint_type == 'swift')):\n        raise exc.MissingParameterValue(_('Swift temporary URLs require a Swift account string. You must provide \"swift_account\" as a config option.'))\n    if (CONF.glance.swift_temp_url_duration < CONF.glance.swift_temp_url_expected_download_start_delay):\n        raise exc.InvalidParameterValue(_('\"swift_temp_url_duration\" must be greater than or equal to \"[glance]swift_temp_url_expected_download_start_delay\" option, otherwise the Swift temporary URL may expire before the download starts.'))\n    seed_num_chars = CONF.glance.swift_store_multiple_containers_seed\n    if ((seed_num_chars is None) or (seed_num_chars < 0) or (seed_num_chars > 32)):\n        raise exc.InvalidParameterValue(_('An integer value between 0 and 32 is required for swift_store_multiple_containers_seed.'))\n", "label": 1}
{"function": "\n\ndef __init__(self, author=None, category=None, content=None, atom_id=None, link=None, published=None, title=None, updated=None, organization=None, phone_number=None, nickname=None, occupation=None, gender=None, birthday=None, postal_address=None, structured_postal_address=None, email=None, im=None, relation=None, user_defined_field=None, website=None, external_id=None, event=None, batch_operation=None, batch_id=None, batch_status=None, text=None, extension_elements=None, extension_attributes=None, etag=None):\n    gdata.BatchEntry.__init__(self, author=author, category=category, content=content, atom_id=atom_id, link=link, published=published, batch_operation=batch_operation, batch_id=batch_id, batch_status=batch_status, title=title, updated=updated)\n    self.organization = (organization or [])\n    self.phone_number = (phone_number or [])\n    self.nickname = nickname\n    self.occupation = occupation\n    self.gender = gender\n    self.birthday = birthday\n    self.postal_address = (postal_address or [])\n    self.structured_postal_address = (structured_postal_address or [])\n    self.email = (email or [])\n    self.im = (im or [])\n    self.relation = (relation or [])\n    self.user_defined_field = (user_defined_field or [])\n    self.website = (website or [])\n    self.external_id = (external_id or [])\n    self.event = (event or [])\n    self.text = text\n    self.extension_elements = (extension_elements or [])\n    self.extension_attributes = (extension_attributes or {\n        \n    })\n    self.etag = etag\n", "label": 1}
{"function": "\n\ndef match(self, daytime):\n    match = self.pattern.match(daytime)\n    if (not match):\n        raise ValueError('format mismatch')\n    get = match.groupdict().get\n    tm = ([0] * 9)\n    year = get('year')\n    if year:\n        year = int(year)\n        if (year < 68):\n            year = (2000 + year)\n        elif (year < 100):\n            year = (1900 + year)\n        tm[0] = year\n    month = get('month')\n    if month:\n        if (month in MONTHS):\n            month = (MONTHS.index(month) + 1)\n        tm[1] = int(month)\n    day = get('day')\n    if day:\n        tm[2] = int(day)\n    hour = get('hour')\n    if hour:\n        tm[3] = int(hour)\n    else:\n        hour = get('hour12')\n        if hour:\n            hour = int(hour)\n            if (get('ampm12', '').lower() == 'pm'):\n                hour = (hour + 12)\n            tm[3] = hour\n    minute = get('minute')\n    if minute:\n        tm[4] = int(minute)\n    second = get('second')\n    if second:\n        tm[5] = int(second)\n    return tuple(tm)\n", "label": 1}
{"function": "\n\ndef update_models(new_obj, current_table, tables, relations):\n    ' Update the state of the parsing. '\n    _update_check_inputs(current_table, tables, relations)\n    _check_no_current_table(new_obj, current_table)\n    if isinstance(new_obj, Table):\n        tables_names = [t.name for t in tables]\n        _check_not_creating_duplicates(new_obj.name, tables_names, 'table', DuplicateTableException)\n        return (new_obj, (tables + [new_obj]), relations)\n    if isinstance(new_obj, Relation):\n        tables_names = [t.name for t in tables]\n        _check_colname_in_lst(new_obj.right_col, tables_names)\n        _check_colname_in_lst(new_obj.left_col, tables_names)\n        return (current_table, tables, (relations + [new_obj]))\n    if isinstance(new_obj, Column):\n        columns_names = [c.name for c in current_table.columns]\n        _check_not_creating_duplicates(new_obj.name, columns_names, 'column', DuplicateColumnException)\n        current_table.columns.append(new_obj)\n        return (current_table, tables, relations)\n    msg = 'new_obj cannot be of type {}'\n    raise ValueError(msg.format(new_obj.__class__.__name__))\n", "label": 0}
{"function": "\n\ndef is_gde_post(activity):\n    'Identify gde post.'\n    content = activity['object']['content']\n    if ('annotation' in activity):\n        content += (' ' + activity['annotation'])\n    if (not is_valid_expert_tag(content)):\n        return False\n    if ('verb' in activity):\n        if (activity['verb'] == 'checkin'):\n            return False\n        if (activity['verb'] == 'post'):\n            return True\n        elif (activity['object']['actor']['id'] == activity['actor']['id']):\n            return True\n        else:\n            return False\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef results_iter(self):\n    '\\n        Returns an iterator over the results from executing this query.\\n        '\n    resolve_columns = hasattr(self, 'resolve_columns')\n    if resolve_columns:\n        from django.db.models.fields import DateField\n        fields = [DateField()]\n    else:\n        from django.db.backends.util import typecast_date\n        needs_string_cast = self.connection.features.needs_datetime_string_cast\n    offset = len(self.query.extra_select)\n    for rows in self.execute_sql(MULTI):\n        for row in rows:\n            date = row[offset]\n            if resolve_columns:\n                date = self.resolve_columns(row, fields)[offset]\n            elif needs_string_cast:\n                date = typecast_date(str(date))\n            if isinstance(date, datetime.datetime):\n                date = date.date()\n            (yield date)\n", "label": 0}
{"function": "\n\ndef _case_changed(case_id, handler_ids):\n    subcases = None\n    case = CommCareCase.get(case_id)\n    for handler in CaseReminderHandler.get_handlers_from_ids(handler_ids):\n        if (handler.start_condition_type == CASE_CRITERIA):\n            kwargs = {\n                \n            }\n            if handler.uses_time_case_property:\n                kwargs = {\n                    'schedule_changed': True,\n                    'prev_definition': handler,\n                }\n            handler.case_changed(case, **kwargs)\n            if handler.uses_parent_case_property:\n                if (subcases is None):\n                    subcases = get_subcases(case)\n                for subcase in subcases:\n                    handler.case_changed(subcase, **kwargs)\n", "label": 0}
{"function": "\n\ndef json_api_exception_handler(exc, context):\n    '\\n    Custom exception handler that returns errors object as an array\\n    '\n    from rest_framework.views import exception_handler\n    response = exception_handler(exc, context)\n    errors = []\n    if response:\n        message = response.data\n        if isinstance(exc, TwoFactorRequiredError):\n            response['X-OSF-OTP'] = 'required; app'\n        if isinstance(exc, JSONAPIException):\n            errors.extend([{\n                'source': (exc.source or {\n                    \n                }),\n                'detail': exc.detail,\n                'meta': (exc.meta or {\n                    \n                }),\n            }])\n        elif isinstance(message, dict):\n            errors.extend(dict_error_formatting(message, None))\n        else:\n            if isinstance(message, basestring):\n                message = [message]\n            for (index, error) in enumerate(message):\n                if isinstance(error, dict):\n                    errors.extend(dict_error_formatting(error, index))\n                else:\n                    errors.append({\n                        'detail': error,\n                    })\n        response.data = {\n            'errors': errors,\n        }\n    return response\n", "label": 1}
{"function": "\n\ndef validate(self):\n    if ('direction' in self.resource):\n        direction = self.resource['direction']\n        if (direction not in ['ingress', 'egress']):\n            raise ValueError('The direction attribute must be either ingress or egress')\n    if ('ethertype' in self.resource):\n        ethertype = self.resource['ethertype']\n        if (ethertype not in ['IPv4', 'IPv6']):\n            raise ValueError('The ethertype attribute must be either IPv4 or IPv6')\n    if ('protocol' in self.resource):\n        protocol = self.resource['protocol']\n        if (protocol not in ['tcp', 'udp', 'icmp']):\n            raise ValueError('The protocol attribute must be either tcp, udp or icmp')\n    if ('remote_mode' in self.resource):\n        remote_mode = self.resource['remote_mode']\n        if (remote_mode not in ['remote_ip_prefix', 'remote_group_id']):\n            raise ValueError('The remote_mode attribute must be either remote_ip_prefix or remote_group_id')\n    return True\n", "label": 1}
{"function": "\n\ndef testcaseVariationXbrlLoaded(testcaseModelXbrl, instanceModelXbrl, modelTestcaseVariation, *args, **kwargs):\n    modelManager = instanceModelXbrl.modelManager\n    if (hasattr(testcaseModelXbrl, 'efmOptions') and modelManager.validateDisclosureSystem and getattr(modelManager.disclosureSystem, 'EFMplugin', False) and ((instanceModelXbrl.modelDocument.type == ModelDocument.Type.INSTANCE) or (instanceModelXbrl.modelDocument.type == ModelDocument.Type.INLINEXBRL))):\n        cntlr = modelManager.cntlr\n        options = testcaseModelXbrl.efmOptions\n        entrypointFiles = [{\n            'file': instanceModelXbrl.modelDocument.uri,\n        }]\n        if (not hasattr(modelManager, 'efmFiling')):\n            modelManager.efmFiling = Filing(cntlr, options, instanceModelXbrl.fileSource, entrypointFiles, None, None, instanceModelXbrl.errorCaptureLevel)\n            for pluginXbrlMethod in pluginClassMethods('EdgarRenderer.Filing.Start'):\n                pluginXbrlMethod(cntlr, options, entrypointFiles, modelManager.efmFiling)\n        modelManager.efmFiling.addReport(instanceModelXbrl)\n        _report = modelManager.efmFiling.reports[(- 1)]\n        _report.entryPoint = entrypointFiles[0]\n        modelManager.efmFiling.arelleUnitTests = instanceModelXbrl.arelleUnitTests.copy()\n        for _instanceElt in XmlUtil.descendants(modelTestcaseVariation, '*', 'instance', 'readMeFirst', 'true', False):\n            if instanceModelXbrl.modelDocument.uri.endswith(_instanceElt.text):\n                if _instanceElt.get('exhibitType'):\n                    _report.entryPoint['exhibitType'] = _report.exhibitType = _instanceElt.get('exhibitType')\n                break\n", "label": 1}
{"function": "\n\ndef __init__(self, timestr=None, timezone=LOCALTZ, allowpast=True, allowfuture=True):\n    ' Converts input to UTC timestamp. '\n    if (timestr is None):\n        timestr = time.time()\n    self.timezone = timezone\n    if (type(timestr) == str):\n        self.__timestamp__ = self.__fromstring__(timestr)\n    elif (type(timestr) in [int, float]):\n        self.__timestamp__ = (timestr + self.timezone)\n    elif (type(timestr) in [datetime.datetime, datetime.date, datetime.time]):\n        self.__timestamp__ = (_mktime(timestr.timetuple()) + self.timezone)\n    elif (type(timestr) == time.struct_time):\n        self.__timestamp__ = (_mktime(timestr) + self.timezone)\n    else:\n        raise TypeError('Failed to recognize given type.')\n    if ((not allowpast) and (self.__timestamp__ < currentutc())):\n        raise DateRangeError('Values from the past are not allowed.')\n    if ((not allowfuture) and (self.__timestamp__ > currentutc())):\n        raise DateRangeError('Values from the future are not allowed.')\n", "label": 1}
{"function": "\n\ndef dict_flat_generator(value, attname=None, splitter=JSPLITTER, dumps=None, prefix=None, error=ValueError, recursive=True):\n    'Convert a nested dictionary into a flat dictionary representation'\n    if ((not isinstance(value, dict)) or (not recursive)):\n        if (not prefix):\n            raise error('Cannot assign a non dictionary to a JSON field')\n        else:\n            name = (('%s%s%s' % (attname, splitter, prefix)) if attname else prefix)\n            (yield (name, (dumps(value) if dumps else value)))\n    else:\n        for field in value:\n            val = value[field]\n            key = prefix\n            if field:\n                key = (('%s%s%s' % (prefix, splitter, field)) if prefix else field)\n            for (k, v2) in dict_flat_generator(val, attname, splitter, dumps, key, error, field):\n                (yield (k, v2))\n", "label": 1}
{"function": "\n\ndef _print_phantomjs_output(result):\n    errors = result['errors']\n    messages = result['messages']\n    resources = result['resources']\n    for message in messages:\n        msg = message['msg']\n        line = message.get('line')\n        source = message.get('source')\n        if (source is None):\n            write(msg)\n        elif (line is None):\n            write(('%s: %s' % (source, msg)))\n        else:\n            write(('%s:%s: %s' % (source, line, msg)))\n    for resource in resources:\n        url = resource['url']\n        if url.endswith('.png'):\n            ok(('%s: %s (%s)' % (url, yellow(resource['status']), resource['statusText'])))\n        else:\n            warn(('Resource error:: %s: %s (%s)' % (url, red(resource['status']), resource['statusText'])))\n    for error in errors:\n        warn(('%s: %s' % (red('PhatomJS Error: '), error['msg'])))\n        for item in error['trace']:\n            write(('    %s: %d' % (item['file'], item['line'])))\n", "label": 0}
{"function": "\n\ndef getEditFileCommand(filePath, lineNum):\n    (editor, _editor_path) = getEditorAndPath()\n    if ((editor in ['vim', 'vim -p']) and (lineNum != 0)):\n        return (\"'%s' +%d\" % (filePath, lineNum))\n    elif ((editor in ['vi', 'nvim', 'nano', 'joe', 'emacs', 'emacsclient']) and (lineNum != 0)):\n        return (\"+%d '%s'\" % (lineNum, filePath))\n    elif ((editor in ['subl', 'sublime', 'atom']) and (lineNum != 0)):\n        return (\"'%s:%d'\" % (filePath, lineNum))\n    else:\n        return (\"'%s'\" % filePath)\n", "label": 0}
{"function": "\n\ndef add_many_rows(self, user, data):\n    '\\n        Shortcut for adding rows in bulk. \\n\\n        ``data`` must be an array of tuples in the format (data_array, external_id)\\n        '\n    self.lock()\n    try:\n        data_typer = DataTyper(self.column_schema)\n        solr_rows = [utils.solr.make_data_row(self, d[0], external_id=d[1]) for d in data]\n        solr_rows = [data_typer(s, d[0]) for (s, d) in zip(solr_rows, data)]\n        solr.add(settings.SOLR_DATA_CORE, solr_rows, commit=True)\n        self.schema = data_typer.schema\n        if (not self.sample_data):\n            self.sample_data = []\n        if (len(self.sample_data) < 5):\n            needed = (5 - len(self.sample_data))\n            self.sample_data.extend([d[0] for d in data[:needed]])\n        old_row_count = self.row_count\n        self.row_count = self._count_rows()\n        added = (self.row_count - (old_row_count or 0))\n        updated = (len(data) - added)\n        self.last_modified = now()\n        self.last_modified_by = user\n        if (added and updated):\n            self.last_modification = (_('%(added)i rows added and %(updated)i updated') % {\n                'added': added,\n                'updated': updated,\n            })\n        elif added:\n            self.last_modification = (_('%i rows added') % added)\n        else:\n            self.last_modification = (_('%i rows updated') % updated)\n        self.save()\n        return solr_rows\n    finally:\n        self.unlock()\n", "label": 1}
{"function": "\n\ndef generate_data_center(self):\n    self.stdout.write('Generating Data Center assets')\n    data_center_status = DataCenterAssetStatus()\n    parent_category = DataCenterCategoryFactory(name='DATA CENTER', imei_required=False)\n    for i in range(2):\n        server_room = ServerRoomFactory()\n        visualization_col = 1\n        visualization_row = 1\n        for j in range(10):\n            rack = RackFactory(server_room=server_room, visualization_row=visualization_row, visualization_col=visualization_col)\n            visualization_row += 1\n            if (visualization_row > server_room.data_center.visualization_rows_num):\n                visualization_row = 1\n                visualization_col += 1\n            accessory = AccessoryFactory()\n            RackAccessoryFactory(rack=rack, accessory=accessory)\n            position = 1\n            for (status_id, name) in data_center_status:\n                for i in range(2):\n                    asset_model = DataCenterAssetModelFactory(category=DataCenterCategoryFactory(parent=parent_category))\n                    DataCenterAssetFactory(rack=rack, status=status_id, position=position, slot_no='', service_env=ServiceEnvironmentFactory(), model=asset_model)\n                    position += asset_model.height_of_device\n                    if (position > rack.max_u_height):\n                        position = 1\n        chassis = DataCenterAssetFactory(rack=rack, status=DataCenterAssetStatus.used.id, position=38, slot_no=None, service_env=ServiceEnvironmentFactory(), model=DataCenterAssetModelFactory(name='Chassis', category=DataCenterCategoryFactory(parent=parent_category), height_of_device=5))\n        for i in range(5):\n            DataCenterAssetFactory(rack=rack, status=DataCenterAssetStatus.used.id, position=None, service_env=ServiceEnvironmentFactory(), slot_no=i, parent=chassis, model=DataCenterAssetModelFactory(name='Blade', has_parent=True, category=DataCenterCategoryFactory(parent=parent_category)))\n", "label": 0}
{"function": "\n\ndef _fold(self, name, value, refold_binary=False):\n    if hasattr(value, 'name'):\n        return value.fold(policy=self)\n    maxlen = (self.max_line_length if self.max_line_length else float('inf'))\n    lines = value.splitlines()\n    refold = ((self.refold_source == 'all') or ((self.refold_source == 'long') and ((lines and (((len(lines[0]) + len(name)) + 2) > maxlen)) or any(((len(x) > maxlen) for x in lines[1:])))))\n    if (refold or (refold_binary and _has_surrogates(value))):\n        return self.header_factory(name, ''.join(lines)).fold(policy=self)\n    return (((name + ': ') + self.linesep.join(lines)) + self.linesep)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef Add(expr, assumptions):\n    '\\n        Imaginary + Imaginary -> Imaginary\\n        Imaginary + Complex   -> ?\\n        Imaginary + Real      -> !Imaginary\\n        '\n    if expr.is_number:\n        return AskImaginaryHandler._number(expr, assumptions)\n    reals = 0\n    for arg in expr.args:\n        if ask(Q.imaginary(arg), assumptions):\n            pass\n        elif ask(Q.real(arg), assumptions):\n            reals += 1\n        else:\n            break\n    else:\n        if (reals == 0):\n            return True\n        if ((reals == 1) or (len(expr.args) == reals)):\n            return False\n", "label": 1}
{"function": "\n\ndef wait(self, jobs=None, timeout=(- 1)):\n    'waits on one or more `jobs`, for up to `timeout` seconds.\\n        \\n        Parameters\\n        ----------\\n        \\n        jobs : int, str, or list of ints and/or strs, or one or more AsyncResult objects\\n                ints are indices to self.history\\n                strs are msg_ids\\n                default: wait on all outstanding messages\\n        timeout : float\\n                a time in seconds, after which to give up.\\n                default is -1, which means no timeout\\n        \\n        Returns\\n        -------\\n        \\n        True : when all msg_ids are done\\n        False : timeout reached, some msg_ids still outstanding\\n        '\n    tic = time.time()\n    if (jobs is None):\n        theids = self.outstanding\n    else:\n        if isinstance(jobs, (int, str, AsyncResult)):\n            jobs = [jobs]\n        theids = set()\n        for job in jobs:\n            if isinstance(job, int):\n                job = self.history[job]\n            elif isinstance(job, AsyncResult):\n                list(map(theids.add, job.msg_ids))\n                continue\n            theids.add(job)\n    if (not theids.intersection(self.outstanding)):\n        return True\n    self.spin()\n    while theids.intersection(self.outstanding):\n        if ((timeout >= 0) and ((time.time() - tic) > timeout)):\n            break\n        time.sleep(0.001)\n        self.spin()\n    return (len(theids.intersection(self.outstanding)) == 0)\n", "label": 1}
{"function": "\n\ndef get_expr_list_by_hackathon_id(self, hackathon, context):\n    user_name = (context.user_name if ('user_name' in context) else None)\n    status = (context.status if ('status' in context) else None)\n    page = (int(context.page) if ('page' in context) else 1)\n    per_page = (int(context.per_page) if ('per_page' in context) else 10)\n    users = (User.objects(name=user_name).all() if user_name else [])\n    if (user_name and status):\n        experiments_pagi = Experiment.objects(hackathon=hackathon, status=status, user__in=users).paginate(page, per_page)\n    elif (user_name and (not status)):\n        experiments_pagi = Experiment.objects(hackathon=hackathon, user__in=users).paginate(page, per_page)\n    elif ((not user_name) and status):\n        experiments_pagi = Experiment.objects(hackathon=hackathon, status=status).paginate(page, per_page)\n    else:\n        experiments_pagi = Experiment.objects(hackathon=hackathon).paginate(page, per_page)\n    return self.util.paginate(experiments_pagi, self.__get_expr_with_detail)\n", "label": 1}
{"function": "\n\ndef log_application_new(self, application_id, application_name):\n    ' Trace application new\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_APPLICATION_NEW in logger.events)):\n            data = {\n                \n            }\n            data['timestamp'] = time.time()\n            data['node_id'] = self.node.id\n            data['type'] = 'application_new'\n            data['application_id'] = application_id\n            data['application_name'] = application_name\n            if (logger.connection is not None):\n                if (not logger.connection.connection_lost):\n                    logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                else:\n                    disconnected.append(user_id)\n            elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                msg = {\n                    'cmd': 'logevent',\n                    'msgid': logger.handle,\n                    'header': None,\n                    'data': ('data: %s\\n\\n' % json.dumps(data)),\n                }\n                self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef iterate(self, prod_, rule_):\n    newProduction = ''\n    for i in range(len(prod_)):\n        step = self.production[i]\n        if (step == 'W'):\n            newProduction = (newProduction + self.ruleW)\n        elif (step == 'X'):\n            newProduction = (newProduction + self.ruleX)\n        elif (step == 'Y'):\n            newProduction = (newProduction + self.ruleY)\n        elif (step == 'Z'):\n            newProduction = (newProduction + self.ruleZ)\n        elif (step != 'F'):\n            newProduction = (newProduction + step)\n    self.drawLength = (self.drawLength * 0.5)\n    self.generations += 1\n    return newProduction\n", "label": 0}
{"function": "\n\ndef describe_diff(a, b):\n    \"\\n    Takes two strings and calculates the difference between them.\\n\\n    Output format is a list of::\\n\\n        (line_number, diff_type, line)\\n\\n    ``diff_type`` can be:\\n        * ``' '``\\n        * ``'-'``\\n        * ``'+'``\\n\\n    Example::\\n\\n        >>> list(describe_diff('a\\\\nb\\\\nc', 'a\\\\nc\\\\nd'))\\n        [(1, ' ', 'a'),\\n        (2, '-', 'b'),\\n        (2, ' ', 'c'),\\n        (3, '+', 'd')]\\n    \"\n    a = a.splitlines()\n    b = b.splitlines()\n    for (tag, i1, i2, j1, j2) in SequenceMatcher(None, a, b).get_opcodes():\n        if (tag == 'equal'):\n            for (idx, line) in enumerate(b[j1:j2]):\n                (yield (((idx + j1) + 1), ' ', _make_unicode(line)))\n            continue\n        if ((tag == 'replace') or (tag == 'delete')):\n            for (idx, line) in enumerate(a[i1:i2]):\n                (yield (((idx + i1) + 1), '-', _make_unicode(line)))\n        if ((tag == 'replace') or (tag == 'insert')):\n            for (idx, line) in enumerate(b[j1:j2]):\n                (yield (((idx + j1) + 1), '+', _make_unicode(line)))\n", "label": 1}
{"function": "\n\ndef before(base=_datetime, diff=None):\n    '\\n    count datetime before `base` time\\n    :param base:  minuend -> str/datetime/date\\n    :param diff:  str\\n    :return: datetime\\n    '\n    _base = parse(base)\n    if isinstance(_base, datetime.date):\n        _base = midnight(_base)\n    if (not diff):\n        return _base\n    result_dict = dp(diff)\n    for unit in result_dict:\n        _val = result_dict[unit]\n        if (not _val):\n            continue\n        if (unit == 'years'):\n            _base = _base.replace(year=(_base.year - _val))\n        elif (unit == 'months'):\n            if (_base.month <= _val):\n                _month_diff = (12 - (_val - _base.month))\n                _base = _base.replace(year=(_base.year - 1)).replace(month=_month_diff)\n            else:\n                _base = _base.replace(month=(_base.month - _val))\n        elif (unit in ['days', 'hours', 'minutes', 'seconds']):\n            _base = (_base - datetime.timedelta(**{\n                unit: _val,\n            }))\n    return _base\n", "label": 1}
{"function": "\n\ndef add_namespaces(root, ns_keys):\n    if isinstance(ns_keys, six.string_types):\n        ns_keys = [ns_keys]\n    namespaces = Namespaces()\n    ns_keys = [(x, namespaces.get_namespace(x)) for x in ns_keys]\n    if (etree.__name__ != 'lxml.etree'):\n        existing_namespaces = set()\n        for elem in root.getiterator():\n            if (elem.tag[0] == '{'):\n                (uri, tag) = elem.tag[1:].split('}')\n                existing_namespaces.add(namespaces.get_namespace_from_url(uri))\n        for (key, link) in ns_keys:\n            if ((link is not None) and (key not in existing_namespaces)):\n                root.set(('xmlns:%s' % key), link)\n        return root\n    else:\n        new_map = root.nsmap\n        for (key, link) in ns_keys:\n            if (link is not None):\n                new_map[key] = link\n        new_root = etree.Element(root.tag, nsmap=new_map)\n        for (a, v) in list(root.items()):\n            new_root.set(a, v)\n        for child in root:\n            new_root.append(deepcopy(child))\n        return new_root\n", "label": 1}
{"function": "\n\ndef _flush_iopub(self, sock):\n    'Flush replies from the iopub channel waiting\\n        in the ZMQ queue.\\n        '\n    (idents, msg) = self.session.recv(sock, mode=zmq.NOBLOCK)\n    while (msg is not None):\n        if self.debug:\n            pprint(msg)\n        parent = msg['parent_header']\n        msg_id = parent['msg_id']\n        content = msg['content']\n        header = msg['header']\n        msg_type = msg['msg_type']\n        md = self.metadata[msg_id]\n        if (msg_type == 'stream'):\n            name = content['name']\n            s = (md[name] or '')\n            md[name] = (s + content['data'])\n        elif (msg_type == 'pyerr'):\n            md.update({\n                'pyerr': self._unwrap_exception(content),\n            })\n        elif (msg_type == 'pyin'):\n            md.update({\n                'pyin': content['code'],\n            })\n        else:\n            md.update({\n                msg_type: content.get('data', ''),\n            })\n        self.metadata[msg_id] = md\n        (idents, msg) = self.session.recv(sock, mode=zmq.NOBLOCK)\n", "label": 0}
{"function": "\n\ndef file_containing_import(import_path, import_root):\n    'Finds the file that might contain the import_path.\\n    '\n    if (not _import_paths):\n        load_stdlib()\n    if os.path.isfile(import_root):\n        import_root = os.path.dirname(import_root)\n    search_paths = ([import_root] + _import_paths)\n    module_parts = import_path.split('.')\n    for i in range(len(module_parts), 0, (- 1)):\n        module_path = os.path.join(*module_parts[:i])\n        for sp in search_paths:\n            p = os.path.join(sp, module_path)\n            if os.path.isdir(p):\n                return os.path.join(p, '__init__.py')\n            elif os.path.isfile((p + '.py')):\n                return (p + '.py')\n    return None\n", "label": 0}
{"function": "\n\ndef run(self, edit):\n    sel = self.view.sel()[0]\n    for region in self.view.sel():\n        self.view.insert(edit, region.end(), '>')\n    self.view.run_command('hide_auto_complete')\n    if (not SETTINGS.get('auto_close_cfml')):\n        return\n    if (self.view.match_selector(sel.end(), 'string') or self.view.match_selector(sel.end(), 'source.cfscript.embedded.cfml') or (not self.view.match_selector(sel.end(), 'meta.tag.block.cf'))):\n        return\n    for region in self.view.sel():\n        pos = region.begin()\n        tagdata = self.view.substr(sublime.Region(0, pos)).split('<')\n        tagdata.reverse()\n        tagdata = tagdata.pop(0).split(' ')\n        tagname = tagdata[0]\n    if (self.view.match_selector(sel.end(), 'meta.tag.block.cf') and (not (self.view.substr((sel.end() - 1)) == '/')) and (not (tagname[0] == '/'))):\n        if (not (tagname[(- 1)] == '>')):\n            tagname = (tagname + '>')\n        if ((not SETTINGS.get('auto_indent_on_close')) or (tagname == 'cfoutput>')):\n            self.view.run_command('insert_snippet', {\n                'contents': ('$0</' + tagname),\n            })\n        else:\n            self.view.run_command('insert_snippet', {\n                'contents': ('\\n\\t$0\\n</' + tagname),\n            })\n", "label": 1}
{"function": "\n\ndef _confirm_delete(self, exists, delete_events, fields):\n    deleted_at = exists.get('deleted_at')\n    state = exists.get('state')\n    (apb, ape) = self._get_audit_period(exists)\n    if ((not deleted_at) and delete_events):\n        raise UsageException('U6', '.deleted events found but .exists has no deleted_at value.')\n    if (deleted_at and (state != 'deleted')):\n        raise UsageException('U3', \".exists state not 'deleted' but .exists deleted_at is set.\")\n    if (deleted_at and (not delete_events)):\n        launched_at = exists.get('launched_at')\n        if (deleted_at < launched_at):\n            raise UsageException('U4', '.exists deleted_at < .exists launched_at.')\n        if (apb and ape and (deleted_at >= apb) and (deleted_at <= ape)):\n            raise UsageException('U5', '.exists deleted_at in audit period, but no matching .delete event found.')\n    if (len(delete_events) > 1):\n        raise UsageException('U7', 'Multiple .delete.end events')\n    if delete_events:\n        self._verify_fields(exists, delete_events[(- 1)], fields)\n", "label": 1}
{"function": "\n\ndef do_mask(img, mask_path, mask_source, mask_mode=None):\n    if (not mask_path):\n        return img\n    if (mask_source == 'media'):\n        mask = pil.open(MEDIA_STORAGE.open(mask_path)).convert('RGBA')\n    else:\n        mask = pil.open(STATIC_STORAGE.open(mask_path)).convert('RGBA')\n    if (mask_mode == 'distort'):\n        (iw, ih) = img.size\n        (mw, mh) = mask.size\n        if ((mw != iw) or (mh != ih)):\n            mask = mask.resize((iw, ih), pil.ANTIALIAS)\n    else:\n        (iw, ih) = img.size\n        (ow, oh) = mask.size\n        overlay_ratio = (float(ow) / float(oh))\n        have_to_scale = False\n        if (ow > iw):\n            ow = iw\n            oh = int((float(iw) / overlay_ratio))\n        if (oh > ih):\n            ow = int((float(ih) * overlay_ratio))\n            oh = ih\n        if ((ow != iw) or (oh != ih)):\n            have_to_scale = True\n        if have_to_scale:\n            nmask = mask.resize((ow, oh), pil.ANTIALIAS)\n            mask = pil.new('RGBA', (iw, ih))\n            do_paste(mask, nmask, (int(((iw - ow) / 2)), int(((ih - oh) / 2))))\n            (ow, oh) = mask.size\n    (r, g, b, a) = mask.split()\n    img.putalpha(a)\n", "label": 1}
{"function": "\n\ndef _link_all_references(self, unknown_refs, ucount, progress_monitor):\n    skipped = 0\n    class_tuples = []\n    progress_monitor.start('Parsing all unknown refs', ucount)\n    for reference in unknown_refs:\n        if self._reject_reference(reference):\n            progress_monitor.work('Rejected reference.', 1)\n            continue\n        content = su.safe_strip(reference.content)\n        if ((content is None) or (content == '')):\n            progress_monitor.work('Empty {0}'.format(reference.pk), 1)\n            skipped += 1\n            continue\n        (simple, fqn) = je.clean_java_name(je.get_clean_name(content))\n        prefix = '{0}{1}'.format(PREFIX_GENERIC_LINKER, cu.get_codebase_key(self.codebase))\n        if ((reference.source == 'd') or (reference.snippet is not None)):\n            exact = True\n            prefix += EXACT\n        else:\n            exact = False\n            prefix += IEXACT\n        code_elements = cu.get_value(prefix, simple, gl.get_any_code_element, [simple, self.codebase, exact])\n        classified_elements = self._classify_code_elements(code_elements)\n        class_tuples.append(((reference, simple, fqn) + classified_elements))\n        progress_monitor.work('Classified reference', 1)\n    progress_monitor.done()\n    progress_monitor.info('Classified all refs. Processing now')\n    count = self._process_tuples(class_tuples, progress_monitor)\n    progress_monitor.info('Associated {0} elements, Skipped {1} elements'.format(count, skipped))\n    progress_monitor.done()\n", "label": 0}
{"function": "\n\ndef eq_comm(u, v, eq=None):\n    \" Goal for commutative equality\\n\\n    >>> from logpy import run, var, fact\\n    >>> from logpy.assoccomm import eq_comm as eq\\n    >>> from logpy.assoccomm import commutative, associative\\n\\n    >>> fact(commutative, 'add')    # declare that 'add' is commutative\\n    >>> fact(associative, 'add')    # declare that 'add' is associative\\n\\n    >>> x = var()\\n    >>> run(0, x, eq(('add', 1, 2, 3), ('add', 2, x, 1)))\\n    (3,)\\n    \"\n    eq = (eq or eq_comm)\n    op = var()\n    utail = var()\n    vtail = var()\n    if (isvar(u) and isvar(v)):\n        return (core.eq, u, v)\n        raise EarlyGoalError()\n    (uop, uargs) = op_args(u)\n    (vop, vargs) = op_args(v)\n    if ((not uop) and (not vop)):\n        return (core.eq, u, v)\n    if (vop and (not uop)):\n        (uop, uargs) = (vop, vargs)\n        (v, u) = (u, v)\n    return (conde, ((core.eq, u, v),), ((commutative, uop), (buildo, uop, vtail, v), (permuteq, uargs, vtail, eq)))\n", "label": 0}
{"function": "\n\ndef select_class_paths(self, remains, class_name, class_paths, index=None):\n    class_paths = (class_paths or [])\n    manual_enter_text = 'Enter Package Manually'\n    if (index is None):\n        if (len(class_paths) > 1):\n            class_paths.append(manual_enter_text)\n            sublime.set_timeout(self.view.window().show_quick_panel(class_paths, (lambda i: self.select_class_paths(remains, class_name, class_paths, i))), 10)\n        else:\n            self.organize_step_three_callback(remains, class_name, (class_paths[0] if class_paths else None))\n    elif (index < 0):\n        self.organize_step_three_callback(remains, class_name, None)\n    elif (class_paths[index] == manual_enter_text):\n        self.organize_step_three_callback(remains, class_name, (- 1))\n    else:\n        self.organize_step_three_callback(remains, class_name, class_paths[index])\n", "label": 0}
{"function": "\n\ndef sources(action):\n    if (not hasattr(request, 'auth_sources')):\n        request.auth_sources = {\n            READ: set(),\n            WRITE: set(),\n        }\n        if is_admin():\n            for (source_id,) in Source.all_ids():\n                request.auth_sources[READ].add(source_id)\n                request.auth_sources[WRITE].add(source_id)\n        else:\n            q = Permission.all()\n            q = q.filter(Permission.role_id.in_(request.auth_roles))\n            q = q.filter((Permission.resource_type == Permission.SOURCE))\n            for perm in q:\n                if perm.read:\n                    request.auth_sources[READ].add(perm.resource_id)\n                if (perm.write and request.logged_in):\n                    request.auth_sources[WRITE].add(perm.resource_id)\n    return list(request.auth_sources.get(action, []))\n", "label": 0}
{"function": "\n\ndef _number_desc_for_type(metadata, num_type):\n    'Return the PhoneNumberDesc of the metadata for the given number type'\n    if (num_type == PhoneNumberType.PREMIUM_RATE):\n        return metadata.premium_rate\n    elif (num_type == PhoneNumberType.TOLL_FREE):\n        return metadata.toll_free\n    elif (num_type == PhoneNumberType.MOBILE):\n        return metadata.mobile\n    elif ((num_type == PhoneNumberType.FIXED_LINE) or (num_type == PhoneNumberType.FIXED_LINE_OR_MOBILE)):\n        return metadata.fixed_line\n    elif (num_type == PhoneNumberType.SHARED_COST):\n        return metadata.shared_cost\n    elif (num_type == PhoneNumberType.VOIP):\n        return metadata.voip\n    elif (num_type == PhoneNumberType.PERSONAL_NUMBER):\n        return metadata.personal_number\n    elif (num_type == PhoneNumberType.PAGER):\n        return metadata.pager\n    elif (num_type == PhoneNumberType.UAN):\n        return metadata.uan\n    elif (num_type == PhoneNumberType.VOICEMAIL):\n        return metadata.voicemail\n    else:\n        return metadata.general_desc\n", "label": 1}
{"function": "\n\ndef expand(self):\n    if (not self.is_valid()):\n        return None\n    if self.is_top():\n        return self.typed_region()\n    if (self.expanded_index is None):\n        for (i, r) in enumerate(self.regions):\n            if (r.contains_region(self.selection) and (r.region != self.selection)):\n                self.expanded_index = i\n                break\n    else:\n        self.expanded_index = (self.expanded_index + 1)\n    self.selection = self.typed_region().region\n    return self.typed_region()\n", "label": 0}
{"function": "\n\ndef get_oauth_params(self, request):\n    'Get the basic OAuth parameters to be used in generating a signature.\\n        '\n    nonce = (generate_nonce() if (self.nonce is None) else self.nonce)\n    timestamp = (generate_timestamp() if (self.timestamp is None) else self.timestamp)\n    params = [('oauth_nonce', nonce), ('oauth_timestamp', timestamp), ('oauth_version', '1.0'), ('oauth_signature_method', self.signature_method), ('oauth_consumer_key', self.client_key)]\n    if self.resource_owner_key:\n        params.append(('oauth_token', self.resource_owner_key))\n    if self.callback_uri:\n        params.append(('oauth_callback', self.callback_uri))\n    if self.verifier:\n        params.append(('oauth_verifier', self.verifier))\n    content_type = request.headers.get('Content-Type', None)\n    content_type_eligible = (content_type and (content_type.find('application/x-www-form-urlencoded') < 0))\n    if ((request.body is not None) and content_type_eligible):\n        params.append(('oauth_body_hash', base64.b64encode(hashlib.sha1(request.body.encode('utf-8')).digest()).decode('utf-8')))\n    return params\n", "label": 1}
{"function": "\n\ndef convertFromStr(self, value, type):\n    if (value is not None):\n        if (type == 'str'):\n            return str(value)\n        elif (value.strip() != ''):\n            if (type == 'long'):\n                return long(value)\n            elif (type == 'float'):\n                return float(value)\n            elif (type == 'int'):\n                return int(value)\n            elif (type == 'date'):\n                return date(*time_strptime(value, '%Y-%m-%d')[0:3])\n            elif (type == 'datetime'):\n                return datetime(*time_strptime(value, '%Y-%m-%d %H:%M:%S')[0:6])\n    return None\n", "label": 1}
{"function": "\n\ndef __init__(self, source, target, import_config, source_comments=None, source_attachments=None):\n    super(CsvYouTrackImporter, self).__init__(source, target, import_config)\n    self._after = 0\n    self._comments = dict()\n    self._attachments = dict()\n    if source_comments:\n        for c in source_comments.get_rows():\n            issue_id = ('%s-%s' % (c[0], c[1]))\n            if (issue_id not in self._comments):\n                self._comments[issue_id] = []\n            self._comments[issue_id].append(c[2:])\n    if source_attachments:\n        for a in source_attachments.get_rows():\n            issue_id = ('%s-%s' % (a[0], a[1]))\n            if (issue_id not in self._attachments):\n                self._attachments[issue_id] = []\n            self._attachments[issue_id].append(a[2:])\n", "label": 0}
{"function": "\n\ndef minCostII(self, costs):\n    '\\n        Lef F[i][j] be the total min costs when the houses BEFORE i are painted, with (i-1)-th house pained as color j\\n        F[i][j] = \\\\min(F[i-1][k] + cost[i-1][j] \\x0corall k, k != j\\n\\n        edge case handling for i\\n        :type costs: List[List[int]]\\n        :rtype: int\\n        '\n    if (not costs):\n        return 0\n    n = len(costs)\n    m = len(costs[0])\n    F = [[0 for _ in xrange(m)] for _ in xrange((n + 1))]\n    for i in xrange(1, (n + 1)):\n        for k1 in xrange(m):\n            F[i][k1] = min(((F[(i - 1)][k0] + costs[(i - 1)][k1]) for k0 in xrange(m) if ((i == 1) or (k1 != k0))))\n    return min((F[n][i] for i in xrange(m)))\n", "label": 1}
{"function": "\n\ndef _format_table(self, expr):\n    ctx = self.context\n    ref_expr = expr\n    op = ref_op = expr.op()\n    if isinstance(op, ops.SelfReference):\n        ref_expr = op.table\n        ref_op = ref_expr.op()\n    if isinstance(ref_op, ops.PhysicalTable):\n        name = ref_op.name\n        if (name is None):\n            raise com.RelationError('Table did not have a name: {0!r}'.format(expr))\n        result = quote_identifier(name)\n        is_subquery = False\n    else:\n        if ctx.is_extracted(ref_expr):\n            alias = ctx.get_ref(expr)\n            if isinstance(op, ops.SelfReference):\n                return '{0} {1}'.format(ctx.get_ref(ref_expr), alias)\n            else:\n                return alias\n        subquery = ctx.get_compiled_expr(expr)\n        result = '(\\n{0}\\n)'.format(util.indent(subquery, self.indent))\n        is_subquery = True\n    if (is_subquery or ctx.need_aliases()):\n        result += ' {0}'.format(ctx.get_ref(expr))\n    return result\n", "label": 0}
{"function": "\n\n@classmethod\ndef setUpClass(cls):\n    rc = controller.RootController()\n    actual_out = util.capture_stdout(rc.execute, ['show', 'config'])\n    TestShowConfig.output_list = test_util.get_separate_output(actual_out, 'Configuration')\n    TestShowConfig.is_bar_present = False\n    for item in TestShowConfig.output_list:\n        if ('~~Service Configuration~~' in item):\n            TestShowConfig.service_config = item\n        elif ('~~Network Configuration~~' in item):\n            TestShowConfig.network_config = item\n        elif ('~~test Namespace Configuration~~' in item):\n            TestShowConfig.test_namespace_config = item\n        elif ('~~bar Namespace Configuration~~' in item):\n            TestShowConfig.bar_namespace_config = item\n            TestShowConfig.is_bar_present = True\n        elif ('~~XDR Configuration~~' in item):\n            TestShowConfig.xdr_config = item\n", "label": 0}
{"function": "\n\ndef _make_kwargs(self, **kwargs):\n    'Creates the keyword arguments for the single run handling'\n    result_dict = {\n        'traj': self._traj,\n        'logging_manager': self._logging_manager,\n        'runfunc': self._runfunc,\n        'runargs': self._args,\n        'runkwargs': self._kwargs,\n        'clean_up_runs': self._clean_up_runs,\n        'automatic_storing': self._automatic_storing,\n        'wrap_mode': self._wrap_mode,\n        'niceness': self._niceness,\n        'graceful_exit': self._graceful_exit,\n    }\n    result_dict.update(kwargs)\n    if self._multiproc:\n        if (self._use_pool or self._use_scoop):\n            if self._use_scoop:\n                del result_dict['graceful_exit']\n            if self._freeze_input:\n                result_dict['full_copy'] = self.traj.v_full_copy\n                if self._map_arguments:\n                    del result_dict['runargs']\n                    del result_dict['runkwargs']\n            else:\n                result_dict['clean_up_runs'] = False\n                if self._use_pool:\n                    del result_dict['logging_manager']\n                    del result_dict['niceness']\n        else:\n            result_dict['clean_up_runs'] = False\n    return result_dict\n", "label": 0}
{"function": "\n\ndef check_hyperparams_dict(hypers):\n    if (type(hypers) is not dict):\n        raise TypeError('hypers should be a dict')\n    keys = ['a', 'b', 'kappa']\n    for key in keys:\n        if (key not in hypers):\n            raise KeyError(('missing key in hypers: %r' % (key,)))\n    for (key, value) in six.iteritems(hypers):\n        if (key not in keys):\n            raise KeyError(('invalid hypers key: %r' % (key,)))\n        if (not isinstance(value, (float, numpy.float64))):\n            raise TypeError(('%r should be float' % (key,)))\n        if (key in ['a', 'kappa']):\n            if (value <= 0.0):\n                raise ValueError(('hypers[%r] should be greater than 0' % (key,)))\n        if (key == 'b'):\n            if ((value <= 0.0) or (value >= (2 * pi))):\n                raise ValueError(('hypers[%r] should be in [0,2*pi]' % (key,)))\n", "label": 1}
{"function": "\n\n@contextlib.contextmanager\ndef hosting_device(self, template_id, management_port_id=None, fmt=None, admin_state_up=True, no_delete=False, set_port_device_id=True, **kwargs):\n    if (not fmt):\n        fmt = self.fmt\n    res = self._create_hosting_device(fmt, template_id, management_port_id, admin_state_up, **kwargs)\n    if (res.status_int >= 400):\n        raise webob.exc.HTTPClientError(code=res.status_int)\n    hosting_device = self.deserialize((fmt or self.fmt), res)\n    if ((set_port_device_id is True) and (management_port_id is not None)):\n        data = {\n            'port': {\n                'device_id': hosting_device['hosting_device']['id'],\n                'device_owner': 'Nova',\n            },\n        }\n        req = self.new_update_request('ports', data, management_port_id)\n        res = self.deserialize(self.fmt, req.get_response(self.api))\n    (yield hosting_device)\n    if (not no_delete):\n        self._delete('hosting_devices', hosting_device['hosting_device']['id'])\n", "label": 0}
{"function": "\n\ndef init(self, parent):\n    ' Finishes initializing the editor by creating the underlying toolkit\\n            widget.\\n        '\n    factory = self.factory\n    style = self.base_style\n    self.evaluate = factory.evaluate\n    self.sync_value(factory.evaluate_name, 'evaluate', 'from')\n    if ((not factory.multi_line) or factory.password):\n        style &= (~ wx.TE_MULTILINE)\n    if factory.password:\n        style |= wx.TE_PASSWORD\n    multi_line = ((style & wx.TE_MULTILINE) != 0)\n    if multi_line:\n        self.scrollable = True\n    if (factory.enter_set and (not multi_line)):\n        control = wx.TextCtrl(parent, (- 1), self.str_value, style=(style | wx.TE_PROCESS_ENTER))\n        wx.EVT_TEXT_ENTER(parent, control.GetId(), self.update_object)\n    else:\n        control = wx.TextCtrl(parent, (- 1), self.str_value, style=style)\n    wx.EVT_KILL_FOCUS(control, self.update_object)\n    if factory.auto_set:\n        wx.EVT_TEXT(parent, control.GetId(), self.update_object)\n    self.control = control\n    self.set_error_state(False)\n    self.set_tooltip()\n", "label": 0}
{"function": "\n\ndef _lookup_param_1_3(self):\n    lookup_params = self.params.copy()\n    for i in (ALL_VAR, ORDER_VAR, ORDER_TYPE_VAR, SEARCH_VAR, IS_POPUP_VAR, TO_FIELD_VAR):\n        if (i in lookup_params):\n            del lookup_params[i]\n    for (key, value) in lookup_params.items():\n        if (not isinstance(key, str)):\n            del lookup_params[key]\n            lookup_params[smart_str(key)] = value\n        if key.endswith('__in'):\n            value = value.split(',')\n            lookup_params[key] = value\n        if key.endswith('__isnull'):\n            if (value.lower() in ('', 'false')):\n                value = False\n            else:\n                value = True\n            lookup_params[key] = value\n        if (not self.model_admin.lookup_allowed(key, value)):\n            raise SuspiciousOperation(('Filtering by %s not allowed' % key))\n    return lookup_params\n", "label": 1}
{"function": "\n\ndef _is_name_allowed_in_acl(self, req, path_parts, identity):\n    if (not self.allow_names_in_acls):\n        return False\n    user_domain_id = identity['user_domain'][0]\n    if (user_domain_id and (user_domain_id != self.default_domain_id)):\n        return False\n    proj_domain_id = identity['project_domain'][0]\n    if (proj_domain_id and (proj_domain_id != self.default_domain_id)):\n        return False\n    (tenant_id, tenant_name) = identity['tenant']\n    (version, account, container, obj) = path_parts\n    if self._account_matches_tenant(account, tenant_id):\n        allow = True\n    else:\n        (exists, acc_domain_id) = self._get_project_domain_id(req.environ)\n        allow = (exists and (acc_domain_id in [self.default_domain_id, None]))\n    if allow:\n        self.logger.debug('Names allowed in acls.')\n    return allow\n", "label": 1}
{"function": "\n\n@staticmethod\ndef Mul(expr, assumptions):\n    '\\n        As long as there is at most only one noncommutative term:\\n        Hermitian*Hermitian         -> Hermitian\\n        Hermitian*Antihermitian     -> !Hermitian\\n        Antihermitian*Antihermitian -> Hermitian\\n        '\n    if expr.is_number:\n        return AskRealHandler._number(expr, assumptions)\n    nccount = 0\n    result = True\n    for arg in expr.args:\n        if ask(Q.antihermitian(arg), assumptions):\n            result = (result ^ True)\n        elif (not ask(Q.hermitian(arg), assumptions)):\n            break\n        if ask((~ Q.commutative(arg)), assumptions):\n            nccount += 1\n            if (nccount > 1):\n                break\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef _find_targets(self, targets):\n    found_target = False\n    for target in targets:\n        if found_target:\n            break\n        for region in target['regions']:\n            if ('subtarget' in region):\n                if region['subtarget'].startswith('left_plate'):\n                    self._targets_on_left.append(region['subtarget'])\n                    found_target = True\n                elif region['subtarget'].startswith('right_plate'):\n                    self._targets_on_right.append(region['subtarget'])\n                    found_target = True\n    if (not found_target):\n        self._operations.say('This training protocol requires a dueling tree target')\n    else:\n        self._operations.show_text_on_feed('left score: 0\\nright score: 0')\n    return found_target\n", "label": 0}
{"function": "\n\ndef _read(self, name, start_range=None, end_range=None):\n    name = self._path(name)\n    (headers, range_) = ({\n        \n    }, None)\n    if ((start_range is not None) and (end_range is not None)):\n        range_ = ('%s-%s' % (start_range, end_range))\n    elif (start_range is not None):\n        range_ = ('%s' % start_range)\n    if (range_ is not None):\n        headers = {\n            'Range': ('bytes=%s' % range_),\n        }\n    response = self.connection.get(self.bucket, name, headers)\n    valid_responses = [200]\n    if ((start_range is not None) or (end_range is not None)):\n        valid_responses.append(206)\n    if (response.http_response.status not in valid_responses):\n        raise S3Error(response.message)\n    headers = response.http_response.msg\n    data = response.object.data\n    if (headers.get('Content-Encoding') == 'gzip'):\n        gzf = GzipFile(mode='rb', fileobj=StringIO(data))\n        data = gzf.read()\n        gzf.close()\n    return (data, headers.get('etag', None), headers.get('content-range', None))\n", "label": 1}
{"function": "\n\ndef excludeStr(self, longname, buildShort=False):\n    '\\n        Generate an \"exclusion string\" for the given option\\n\\n        @type longname: C{str}\\n        @param longname: The long option name (e.g. \"verbose\" instead of \"v\")\\n\\n        @type buildShort: C{bool}\\n        @param buildShort: May be True to indicate we\\'re building an excludes\\n            string for the short option that correspondes to the given long opt.\\n\\n        @return: The generated C{str}\\n        '\n    if (longname in self.excludes):\n        exclusions = self.excludes[longname].copy()\n    else:\n        exclusions = set()\n    if (longname not in self.multiUse):\n        if (buildShort is False):\n            short = self.getShortOption(longname)\n            if (short is not None):\n                exclusions.add(short)\n        else:\n            exclusions.add(longname)\n    if (not exclusions):\n        return ''\n    strings = []\n    for optName in exclusions:\n        if (len(optName) == 1):\n            strings.append(('-' + optName))\n        else:\n            strings.append(('--' + optName))\n    strings.sort()\n    return ('(%s)' % ' '.join(strings))\n", "label": 0}
{"function": "\n\ndef on_post_save(self, view):\n    if self.IsValidScope(view):\n        settings = SublimePapyrus.GetSettings()\n        if (settings and settings.get('linter_on_save', True)):\n            filePath = view.file_name()\n            if filePath:\n                (folderPath, fileName) = os.path.split(filePath)\n                scriptName = fileName[:fileName.rfind('.')].upper()\n                if self.linterRunning:\n                    return\n                self.ClearSemanticAnalysisCache(scriptName)\n                self.ClearCompletionCache(scriptName)\n                self.bufferID = view.buffer_id()\n                if self.bufferID:\n                    self.linterQueue += 1\n                    (lineNumber, columnNumber) = view.rowcol(view.sel()[0].begin())\n                    lineNumber += 1\n                    self.Linter(view, lineNumber)\n", "label": 0}
{"function": "\n\ndef parse(self):\n    '\\n        We need to expand the default parsing to get all\\n        the cases, see the module doc.\\n        '\n    super(ObjManipCommand, self).parse()\n    obj_defs = ([], [])\n    obj_attrs = ([], [])\n    for (iside, arglist) in enumerate((self.lhslist, self.rhslist)):\n        for objdef in arglist:\n            (aliases, option, attrs) = ([], None, [])\n            if (':' in objdef):\n                (objdef, option) = [part.strip() for part in objdef.rsplit(':', 1)]\n            if (';' in objdef):\n                (objdef, aliases) = [part.strip() for part in objdef.split(';', 1)]\n                aliases = [alias.strip() for alias in aliases.split(';') if alias.strip()]\n            if ('/' in objdef):\n                (objdef, attrs) = [part.strip() for part in objdef.split('/', 1)]\n                attrs = [part.strip().lower() for part in attrs.split('/') if part.strip()]\n            obj_defs[iside].append({\n                'name': objdef,\n                'option': option,\n                'aliases': aliases,\n            })\n            obj_attrs[iside].append({\n                'name': objdef,\n                'attrs': attrs,\n            })\n    self.lhs_objs = obj_defs[0]\n    self.rhs_objs = obj_defs[1]\n    self.lhs_objattr = obj_attrs[0]\n    self.rhs_objattr = obj_attrs[1]\n", "label": 1}
{"function": "\n\ndef handleresponses(self):\n    self.rpc.fetch_responses()\n    ans = self.rpc.first_response()\n    if (not ans):\n        return False\n    (req_id, msg_id, msg_body) = ans\n    handled = False\n    for i in range(self.nolevels):\n        for responder in self.resp_idx[i]:\n            if (responder.isReqIdMatch(req_id) and responder.isMsgIdMatch(msg_id)):\n                handled = responder.handlemsg(req_id, msg_id, msg_body, self.rpc, self.parser)\n            if handled:\n                break\n        if handled:\n            break\n    if (not handled):\n        for responder in self.gen_responders:\n            handled = responder.handlemsg(req_id, msg_id, msg_body, self.rpc, self.parser)\n            if handled:\n                break\n    self.other_msgs.append((req_id, msg_id, msg_body))\n    return True\n", "label": 1}
{"function": "\n\ndef rewritepath(os, path):\n    components = parts(path)\n    newpath = ''\n    if (os == 'nt'):\n        for item in components:\n            newpath = ((newpath + '\\\\') + item)\n    elif (os == 'posix'):\n        for item in components:\n            newpath = ((newpath + '/') + item)\n    elif (os == 'url'):\n        for item in components:\n            newpath = ((newpath + '/') + item)\n        newpath = url_fix(newpath)\n    newpath = newpath[1:]\n    return newpath\n", "label": 0}
{"function": "\n\ndef filename_from_url(url, content_type):\n    fn = urlsplit(url).path.rstrip('/')\n    fn = (os.path.basename(fn) if fn else 'index')\n    if (('.' not in fn) and content_type):\n        content_type = content_type.split(';')[0]\n        if (content_type == 'text/plain'):\n            ext = '.txt'\n        else:\n            ext = mimetypes.guess_extension(content_type)\n        if (ext == '.htm'):\n            ext = '.html'\n        if ext:\n            fn += ext\n    return fn\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.make_database_faster()\n    create_images = (not options['withoutimages'])\n    for msg in create_shipping_methods():\n        self.stdout.write(msg)\n    for msg in create_items(self.placeholders_dir, 40, create_images):\n        self.stdout.write(msg)\n    for msg in create_users(20):\n        self.stdout.write(msg)\n    for msg in create_orders(20):\n        self.stdout.write(msg)\n    if options['createsuperuser']:\n        credentials = {\n            'email': 'admin@example.com',\n            'password': 'admin',\n        }\n        (user, created) = User.objects.get_or_create(email=credentials['email'], defaults={\n            'is_active': True,\n            'is_staff': True,\n            'is_superuser': True,\n        })\n        if created:\n            user.set_password(credentials['password'])\n            user.save()\n            self.stdout.write(('Superuser - %(email)s/%(password)s' % credentials))\n        else:\n            self.stdout.write(('Superuser already exists - %(email)s' % credentials))\n", "label": 0}
{"function": "\n\n@state(True)\ndef turn_on(self, transition_time, pipeline, **kwargs):\n    'Turn on (or adjust property of) a group.'\n    from limitlessled.presets import COLORLOOP\n    if (ATTR_BRIGHTNESS in kwargs):\n        self._brightness = kwargs[ATTR_BRIGHTNESS]\n    if (ATTR_RGB_COLOR in kwargs):\n        self._color = kwargs[ATTR_RGB_COLOR]\n    if (min(self._color) > (256 - RGB_BOUNDARY)):\n        pipeline.white()\n        self._color = WHITE\n    pipeline.transition(transition_time, brightness=_from_hass_brightness(self._brightness), color=_from_hass_color(self._color))\n    if (ATTR_FLASH in kwargs):\n        duration = 0\n        if (kwargs[ATTR_FLASH] == FLASH_LONG):\n            duration = 1\n        pipeline.flash(duration=duration)\n    if (ATTR_EFFECT in kwargs):\n        if (kwargs[ATTR_EFFECT] == EFFECT_COLORLOOP):\n            self.repeating = True\n            pipeline.append(COLORLOOP)\n        if (kwargs[ATTR_EFFECT] == EFFECT_WHITE):\n            pipeline.white()\n            self._color = WHITE\n", "label": 1}
{"function": "\n\ndef split_first(s, delims):\n    \"\\n    Given a string and an iterable of delimiters, split on the first found\\n    delimiter. Return two split parts and the matched delimiter.\\n\\n    If not found, then the first part is the full input string.\\n\\n    Example: ::\\n\\n        >>> split_first('foo/bar?baz', '?/=')\\n        ('foo', 'bar?baz', '/')\\n        >>> split_first('foo/bar?baz', '123')\\n        ('foo/bar?baz', '', None)\\n\\n    Scales linearly with number of delims. Not ideal for large number of delims.\\n    \"\n    min_idx = None\n    min_delim = None\n    for d in delims:\n        idx = s.find(d)\n        if (idx < 0):\n            continue\n        if ((min_idx is None) or (idx < min_idx)):\n            min_idx = idx\n            min_delim = d\n    if ((min_idx is None) or (min_idx < 0)):\n        return (s, '', None)\n    return (s[:min_idx], s[(min_idx + 1):], min_delim)\n", "label": 0}
{"function": "\n\ndef as_json(self):\n    ' Serializes to JSON by inspecting `elementProperties()` and creating\\n        a JSON dictionary of all registered properties.\\n        '\n    js = {\n        \n    }\n    found = set()\n    nonoptionals = set()\n    for (name, jsname, typ, is_list, of_many, not_optional) in self.elementProperties():\n        if not_optional:\n            nonoptionals.add((of_many or jsname))\n        val = getattr(self, name)\n        if (val is None):\n            continue\n        if is_list:\n            if (len(val) > 0):\n                found.add((of_many or jsname))\n            js[jsname] = [(v.as_json() if hasattr(v, 'as_json') else v) for v in val]\n        else:\n            found.add((of_many or jsname))\n            js[jsname] = (val.as_json() if hasattr(val, 'as_json') else val)\n    if (len((nonoptionals - found)) > 0):\n        for nonop in (nonoptionals - found):\n            logging.warning(\"Element '{}' is not optional, you should provide a value for it on {}\".format(nonop, self))\n    return js\n", "label": 1}
{"function": "\n\ndef __sub__(self, other):\n    if isinstance(other, BiHemiLabel):\n        if (self.hemi == 'lh'):\n            return (self - other.lh)\n        else:\n            return (self - other.rh)\n    elif isinstance(other, Label):\n        if (self.subject != other.subject):\n            raise ValueError(('Label subject parameters must match, got \"%s\" and \"%s\". Consider setting the subject parameter on initialization, or setting label.subject manually before combining labels.' % (self.subject, other.subject)))\n    else:\n        raise TypeError(('Need: Label or BiHemiLabel. Got: %r' % other))\n    if (self.hemi == other.hemi):\n        keep = in1d(self.vertices, other.vertices, True, invert=True)\n    else:\n        keep = np.arange(len(self.vertices))\n    name = ('%s - %s' % ((self.name or 'unnamed'), (other.name or 'unnamed')))\n    return Label(self.vertices[keep], self.pos[keep], self.values[keep], self.hemi, self.comment, name, None, self.subject, self.color, self.verbose)\n", "label": 0}
{"function": "\n\ndef write(self, file, depth=0):\n    'parse XML structure recursively and append to the output fileID,\\n        increasing the offset (tabs) while descending into the tree'\n    if ('myName' not in self.tag):\n        print('Error parsing XML structure: Tag name missing!')\n        sys.exit(1)\n    nAttr = self.nbAttributes()\n    endmark = '/>'\n    if self.hasSubtag():\n        endmark = '>'\n    if (nAttr > 0):\n        file.write((((((((self._tab * depth) + '<') + self.tag['myName']) + ' ') + ' '.join([(((name + '=\"') + str(val)) + '\"') for (name, val) in self.tag.items() if ((name != 'myName') and (name != 'Icontain'))])) + endmark) + '\\n'))\n    else:\n        file.write(((((self._tab * depth) + '<') + self.tag['myName']) + '>\\n'))\n    if self.hasSubtag():\n        for subtag in self.tag['Icontain']:\n            subtag.write(file, depth=(depth + 1))\n        file.write(((((self._tab * depth) + '</') + self.tag['myName']) + '>\\n'))\n", "label": 1}
{"function": "\n\ndef setup(self, context):\n    self.cyclictest_on_device = 'cyclictest'\n    self.cyclictest_result = os.path.join(self.device.working_directory, TXT_RESULT_NAME)\n    self.cyclictest_command = '{} --clock={} --duration={}s --thread={} --latency={} {} {} > {}'\n    self.device_binary = None\n    if (not self.device.is_rooted):\n        raise WorkloadError('This workload requires a device with root premissions to run')\n    host_binary = context.resolver.get(Executable(self, self.device.abi, 'cyclictest'))\n    self.device_binary = self.device.install(host_binary)\n    self.cyclictest_command = self.cyclictest_command.format(self.device_binary, (0 if (self.clock == 'monotonic') else 1), self.duration, self.thread, self.latency, ('--quiet' if self.quiet else ''), self.extra_parameters, self.cyclictest_result)\n    if self.clear_file_cache:\n        self.device.execute('sync')\n        self.device.set_sysfile_value('/proc/sys/vm/drop_caches', 3)\n    if (self.device.platform == 'android'):\n        if (self.screen_off and self.device.is_screen_on):\n            self.device.execute('input keyevent 26')\n", "label": 0}
{"function": "\n\ndef _configure_ipv6_ra_on_ext_gw_port_if_necessary(self, ri, state):\n    ex_gw_port_id = (ri.ex_gw_port and ri.ex_gw_port['id'])\n    if ((state == 'master') and ex_gw_port_id and ri.use_ipv6):\n        gateway_ips = ri._get_external_gw_ips(ri.ex_gw_port)\n        if (not ri.is_v6_gateway_set(gateway_ips)):\n            interface_name = ri.get_external_device_name(ex_gw_port_id)\n            if ri.router.get('distributed', False):\n                namespace = ri.ha_namespace\n            else:\n                namespace = ri.ns_name\n            ri.driver.configure_ipv6_ra(namespace, interface_name)\n", "label": 0}
{"function": "\n\n@app.route('/micropub', methods=['GET', 'POST', 'PATCH', 'PUT', 'DELETE'])\ndef handleMicroPub():\n    app.logger.info(('handleMicroPub [%s]' % request.method))\n    access_token = request.headers.get('Authorization')\n    if access_token:\n        access_token = access_token.replace('Bearer ', '')\n    (me, client_id, scope) = checkAccessToken(access_token)\n    app.logger.info(('micropub %s [%s] [%s, %s, %s]' % (request.method, access_token, me, client_id, scope)))\n    if ((me is None) or (client_id is None)):\n        return ('Invalid access_token', 400, {\n            \n        })\n    elif (request.method == 'POST'):\n        domain = baseDomain(me, includeScheme=False)\n        if (domain == cfg.our_domain):\n            data = {\n                \n            }\n            for key in ('h', 'name', 'summary', 'content', 'published', 'updated', 'category', 'slug', 'location', 'in-reply-to', 'repost-of', 'syndication', 'syndicate-to'):\n                data[key] = request.form.get(key)\n            return processMicropub(me, client_id, scope, data)\n        else:\n            return ('unauthorized', 401)\n    elif (request.method == 'GET'):\n        return ('not implemented', 501)\n", "label": 0}
{"function": "\n\ndef pretty_date(time=None):\n    \"\\n    Get a datetime object or a int() Epoch timestamp and return a\\n    pretty string like 'an hour ago', 'Yesterday', '3 months ago',\\n    'just now', etc\\n    :param time:\\n    \"\n    from datetime import datetime\n    now = datetime.now()\n    if (type(time) is int):\n        diff = (now - datetime.fromtimestamp(time))\n    elif isinstance(time, datetime):\n        diff = (now - time)\n    else:\n        diff = (now - now)\n    second_diff = diff.seconds\n    day_diff = diff.days\n    if (day_diff < 0):\n        return ''\n    if (day_diff == 0):\n        if (second_diff < 10):\n            return 'just now'\n        if (second_diff < 60):\n            return (str(second_diff) + ' seconds ago')\n        if (second_diff < 120):\n            return 'a minute ago'\n        if (second_diff < 3600):\n            return (str((second_diff / 60)) + ' minutes ago')\n        if (second_diff < 7200):\n            return 'an hour ago'\n        if (second_diff < 86400):\n            return (str((second_diff / 3600)) + ' hours ago')\n    if (day_diff == 1):\n        return 'Yesterday'\n    if (day_diff < 7):\n        return (str(day_diff) + ' days ago')\n    if (day_diff < 31):\n        return (str((day_diff / 7)) + ' weeks ago')\n    if (day_diff < 365):\n        return (str((day_diff / 30)) + ' months ago')\n    return (str((day_diff / 365)) + ' years ago')\n", "label": 1}
{"function": "\n\ndef lineReceived(self, line):\n    'a line has been received.'\n    parts = line.split(None, 1)\n    if ((len(parts) == 2) and line.startswith(parts[0])):\n        (cmd, rest) = parts\n        offset = (len(cmd) + 1)\n        cmd = cmd.rstrip(self._colon_sym)\n        if (cmd in self._test_sym):\n            self.startTest(offset, line)\n        elif (cmd in self._error_sym):\n            self.addError(offset, line)\n        elif (cmd in self._failure_sym):\n            self.addFailure(offset, line)\n        elif (cmd in self._progress_sym):\n            self.parser._handleProgress(offset, line)\n        elif (cmd in self._skip_sym):\n            self.addSkip(offset, line)\n        elif (cmd in self._success_sym):\n            self.addSuccess(offset, line)\n        elif (cmd in self._tags_sym):\n            self.parser._handleTags(offset, line)\n            self.parser.subunitLineReceived(line)\n        elif (cmd in self._time_sym):\n            self.parser._handleTime(offset, line)\n            self.parser.subunitLineReceived(line)\n        elif (cmd in self._xfail_sym):\n            self.addExpectedFail(offset, line)\n        elif (cmd in self._uxsuccess_sym):\n            self.addUnexpectedSuccess(offset, line)\n        else:\n            self.parser.stdOutLineReceived(line)\n    else:\n        self.parser.stdOutLineReceived(line)\n", "label": 1}
{"function": "\n\ndef step_2(w):\n    ' Step 2 replaces double suffixes (singularization => singularize).\\n        This only happens if there is at least one vowel-consonant pair before the suffix.\\n    '\n    for (suffix, rules) in suffixes2:\n        if w.endswith(suffix):\n            for (A, B) in rules:\n                if w.endswith(A):\n                    return ((R1(w).endswith(A) and (w[:(- len(A))] + B)) or w)\n    if (w.endswith('li') and (R1(w)[(- 3):(- 2)] in VALID_LI)):\n        return w[:(- 2)]\n    return w\n", "label": 1}
{"function": "\n\ndef matching_files(patterns):\n    for pattern in patterns:\n        if os.path.isfile(pattern):\n            (yield pattern)\n        elif os.path.isdir(pattern):\n            for (dirpath, dirnames, filenames) in os.walk(pattern):\n                for filename in filenames:\n                    (yield os.path.join(dirpath, filename))\n        else:\n            for filename in glob.glob(pattern):\n                (yield filename)\n", "label": 0}
{"function": "\n\n@property\ndef children(self):\n    (yield self.body)\n    for i in self.excepts:\n        for k in i:\n            if (k is not None):\n                (yield k)\n    if self.except_:\n        (yield self.except_)\n    if self.else_:\n        (yield self.else_)\n    if self.finally_:\n        (yield self.finally_)\n", "label": 0}
{"function": "\n\n@Appender(_index_shared_docs['copy'])\ndef copy(self, name=None, deep=False, dtype=None, **kwargs):\n    names = kwargs.get('names')\n    if ((names is not None) and (name is not None)):\n        raise TypeError('Can only provide one of `names` and `name`')\n    if deep:\n        from copy import deepcopy\n        new_index = self._shallow_copy(self._data.copy())\n        name = (name or deepcopy(self.name))\n    else:\n        new_index = self._shallow_copy()\n        name = self.name\n    if (name is not None):\n        names = [name]\n    if names:\n        new_index = new_index.set_names(names)\n    if dtype:\n        new_index = new_index.astype(dtype)\n    return new_index\n", "label": 0}
{"function": "\n\ndef step_4(w):\n    ' Step 4 strips -ant, -ent etc. suffixes.\\n        This only happens if there is more than one vowel-consonant pair before the suffix.\\n    '\n    for (suffix, rules) in suffixes4:\n        if w.endswith(suffix):\n            for A in rules:\n                if w.endswith(A):\n                    return ((R2(w).endswith(A) and w[:(- len(A))]) or w)\n    if (R2(w).endswith('ion') and w[:(- 3)].endswith(('s', 't'))):\n        return w[:(- 3)]\n    return w\n", "label": 1}
{"function": "\n\ndef _vpn_dict(self, context, project_id, instance):\n    elevated = context.elevated()\n    rv = {\n        'project_id': project_id,\n    }\n    if (not instance):\n        rv['state'] = 'pending'\n        return rv\n    rv['instance_id'] = instance.uuid\n    rv['created_at'] = utils.isotime(instance.created_at)\n    nw_info = compute_utils.get_nw_info_for_instance(instance)\n    if (not nw_info):\n        return rv\n    vif = nw_info[0]\n    ips = [ip for ip in vif.fixed_ips() if (ip['version'] == 4)]\n    if ips:\n        rv['internal_ip'] = ips[0]['address']\n    elevated.project_id = project_id\n    network = self.network_api.get(elevated, vif['network']['id'])\n    if network:\n        vpn_ip = network['vpn_public_address']\n        vpn_port = network['vpn_public_port']\n        rv['public_ip'] = vpn_ip\n        rv['public_port'] = vpn_port\n        if (vpn_ip and vpn_port):\n            if utils.vpn_ping(vpn_ip, vpn_port):\n                rv['state'] = 'running'\n            else:\n                rv['state'] = 'down'\n        else:\n            rv['state'] = 'invalid'\n    return rv\n", "label": 1}
{"function": "\n\ndef __cmp__(self, other):\n    if (other is None):\n        return 1\n    if (not isinstance(other, Version)):\n        other = Version(other)\n    for (ind, el) in enumerate(self.as_list):\n        if ((ind + 1) > len(other.as_list)):\n            return 1\n        if ((not isinstance(el, int)) and isinstance(other.as_list[ind], int)):\n            return (- 1)\n        elif ((not isinstance(other.as_list[ind], int)) and isinstance(el, int)):\n            return 1\n        elif (el == other.as_list[ind]):\n            continue\n        elif (el > other.as_list[ind]):\n            return 1\n        else:\n            return (- 1)\n    if (len(other.as_list) > len(self.as_list)):\n        return (- 1)\n    else:\n        return 0\n", "label": 1}
{"function": "\n\ndef processDirty(self):\n    if self.dirty:\n        self.printAll()\n        return\n    (minx, miny, maxx, maxy) = self.getChromeBoundaries()\n    didClearLine = False\n    for index in self.dirtyIndexes:\n        y = ((miny + index) + self.getScrollOffset())\n        if ((y >= miny) and (y < maxy)):\n            didClearLine = True\n            self.clearLine(y)\n            self.lineObjs[index].output(self.colorPrinter)\n    if (didClearLine and self.helperChrome.getIsSidebarMode()):\n        self.helperChrome.output(self.mode)\n", "label": 0}
{"function": "\n\ndef fetch():\n    retval = {\n        \n    }\n    content = retrieve_content(__url__)\n    if (__check__ in content):\n        for line in content.split('\\n'):\n            line = line.strip()\n            if ((not line) or line.startswith('#') or ('.' not in line) or ('Shunlist' in line)):\n                continue\n            line = line.lower()\n            if ('malware distribution' in line):\n                info = 'malware distribution'\n            elif (' ek ' in line):\n                info = 'malicious'\n            elif any(((_ in line) for _ in ('c&c', 'malware', 'botnet', 'virus'))):\n                info = 'malware'\n            else:\n                info = 'known attacker'\n            retval[line.split(',')[0]] = (info, __reference__)\n    return retval\n", "label": 1}
{"function": "\n\ndef dvi_match_query(body_id):\n    '\\n        Get a query for candidate matches between the missing\\n        persons registry and a dead body\\n\\n        @param body_id: the dvi_body record ID\\n    '\n    ptable = s3db.pr_person\n    ntable = s3db.pr_note\n    btable = s3db.dvi_body\n    query = ((((ptable.deleted == False) & (ptable.missing == True)) & (ntable.pe_id == ptable.pe_id)) & (ntable.status == 1))\n    body = btable[body_id]\n    if (not body):\n        return query\n    if body.date_of_recovery:\n        q = ((ntable.timestmp <= body.date_of_recovery) | (ntable.timestmp == None))\n        query = (query & q)\n    if (body.age_group and (body.age_group != 1)):\n        q = (((ptable.age_group == None) | (ptable.age_group == 1)) | (ptable.age_group == body.age_group))\n        query = (query & q)\n    if (body.gender and (body.gender != 1)):\n        q = (((ptable.gender == None) | (ptable.gender == 1)) | (ptable.gender == body.gender))\n    return query\n", "label": 0}
{"function": "\n\ndef _truncate_timestamps(self, tags):\n    lengths = [10, 16, 19]\n    for length in lengths:\n        considered_tags = [t for t in tags if (len(t.timestamp) > length)]\n        grouped_by_timestamp = {\n            \n        }\n        for t in considered_tags:\n            truncated_timestamp = t.timestamp[:length]\n            if (truncated_timestamp not in grouped_by_timestamp):\n                grouped_by_timestamp[truncated_timestamp] = []\n            grouped_by_timestamp[truncated_timestamp].append(t)\n        for (truncated_timestamp, similar_tags) in grouped_by_timestamp.items():\n            if (len(similar_tags) == 1):\n                similar_tags[0].timestamp = truncated_timestamp\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    kwargs = parse(self.queryset)\n    pargs = parse(self.params)\n    for i in kwargs:\n        if (kwargs[i] == 'False'):\n            kwargs[i] = False\n        elif (kwargs[i] == 'True'):\n            kwargs[i] = True\n        else:\n            kwargs[i] = resolve_variable(kwargs[i], context)\n    kwargs = dict([(str(x), kwargs[x]) for x in kwargs.keys()])\n    new_queryset = self.object.objects.filter(**kwargs)\n    if ('order_by' in pargs):\n        new_queryset = new_queryset.order_by(pargs['order_by'])\n    if ('limit' in pargs):\n        l = pargs['limit']\n        i = l.split(':')[0]\n        if i:\n            i = int(i)\n        else:\n            i = 0\n        j = l.split(':')[1]\n        if j:\n            j = int(j)\n        else:\n            j = 0\n        new_queryset = new_queryset[i:j]\n    if ('template' in pargs):\n        template_name = pargs['template']\n    else:\n        model = new_queryset.model\n        template_name = ('%s/%s_list.html' % (model._meta.app_label, model._meta.object_name.lower()))\n    context['object_list'] = new_queryset\n    context['today'] = datetime.date.today()\n    return template.loader.get_template(template_name).render(context)\n", "label": 1}
{"function": "\n\ndef execute(self):\n    insert_with_loop = (self._is_multi_row_insert and (self._query is None) and (self._returning is None) and (not self.database.insert_many))\n    if insert_with_loop:\n        return self._insert_with_loop()\n    if ((self._returning is not None) and (self._qr is None)):\n        return self._execute_with_result_wrapper()\n    elif (self._qr is not None):\n        return self._qr\n    else:\n        cursor = self._execute()\n        if (not self._is_multi_row_insert):\n            if self.database.insert_returning:\n                pk_row = cursor.fetchone()\n                meta = self.model_class._meta\n                clean_data = [field.python_value(column) for (field, column) in zip(meta.get_primary_key_fields(), pk_row)]\n                if self.model_class._meta.composite_key:\n                    return clean_data\n                return clean_data[0]\n            return self.database.last_insert_id(cursor, self.model_class)\n        elif self._return_id_list:\n            return map(operator.itemgetter(0), cursor.fetchall())\n        else:\n            return True\n", "label": 1}
{"function": "\n\ndef __init__(self, data):\n    self.buffer = (ctypes.c_byte * len(data))()\n    ctypes.memmove(self.buffer, data, len(data))\n    ft_library = ft_get_library()\n    self.face = FT_Face()\n    r = FT_New_Memory_Face(ft_library, self.buffer, len(self.buffer), 0, self.face)\n    if (r != 0):\n        raise base.FontException('Could not load font data')\n    self.name = self.face.contents.family_name\n    self.bold = ((self.face.contents.style_flags & FT_STYLE_FLAG_BOLD) != 0)\n    self.italic = ((self.face.contents.style_flags & FT_STYLE_FLAG_ITALIC) != 0)\n    if (self.face.contents.face_flags & FT_FACE_FLAG_SFNT):\n        name = FT_SfntName()\n        for i in range(FT_Get_Sfnt_Name_Count(self.face)):\n            result = FT_Get_Sfnt_Name(self.face, i, name)\n            if (result != 0):\n                continue\n            if (not ((name.platform_id == TT_PLATFORM_MICROSOFT) and (name.encoding_id == TT_MS_ID_UNICODE_CS))):\n                continue\n            if (name.name_id == TT_NAME_ID_FONT_FAMILY):\n                string = string_at(name.string, name.string_len)\n                self.name = string.decode('utf-16be', 'ignore')\n", "label": 0}
{"function": "\n\ndef _indent(elem, level=0):\n    i = ('\\n' + (level * '  '))\n    if len(elem):\n        if ((not elem.text) or (not elem.text.strip())):\n            elem.text = (i + '  ')\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n        for elem in elem:\n            _indent(elem, (level + 1))\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n    elif (level and ((not elem.tail) or (not elem.tail.strip()))):\n        elem.tail = i\n", "label": 1}
{"function": "\n\ndef dump_to_reconciler(self, broker, point):\n    '\\n        Look for object rows for objects updates in the wrong storage policy\\n        in broker with a ``ROWID`` greater than the rowid given as point.\\n\\n        :param broker: the container broker with misplaced objects\\n        :param point: the last verified ``reconciler_sync_point``\\n\\n        :returns: the last successful enqueued rowid\\n        '\n    max_sync = broker.get_max_row()\n    misplaced = broker.get_misplaced_since(point, self.per_diff)\n    if (not misplaced):\n        return max_sync\n    translator = get_row_to_q_entry_translator(broker)\n    errors = False\n    low_sync = point\n    while misplaced:\n        batches = defaultdict(list)\n        for item in misplaced:\n            container = get_reconciler_container_name(item['created_at'])\n            batches[container].append(translator(item))\n        for (container, item_list) in batches.items():\n            success = self.feed_reconciler(container, item_list)\n            if (not success):\n                errors = True\n        point = misplaced[(- 1)]['ROWID']\n        if (not errors):\n            low_sync = point\n        misplaced = broker.get_misplaced_since(point, self.per_diff)\n    return low_sync\n", "label": 0}
{"function": "\n\ndef get_log_entries(self, from_tag, to_tag, skip_merges=False):\n    cmd = [self._executable, 'log']\n    if (from_tag or to_tag):\n        cmd.append(('%s%s' % ((('%s..' % to_tag) if to_tag else ''), (from_tag if from_tag else ''))))\n    cmd.append('--format=format:%H')\n    if skip_merges:\n        cmd.append('--no-merges')\n    result = self._run_command(cmd)\n    if result['returncode']:\n        raise RuntimeError(('Could not fetch commit hashes:\\n%s' % result['output']))\n    log_entries = []\n    if result['output']:\n        hashes = result['output'].splitlines()\n        for hash_ in hashes:\n            cmd = [self._executable, 'log', hash_, '-n', '1', '--format=format:%B']\n            result = self._run_command(cmd)\n            if result['returncode']:\n                raise RuntimeError(('Could not fetch commit message:\\n%s' % result['output']))\n            if (result['output'] == from_tag):\n                continue\n            msg = result['output']\n            cmd = [self._executable, 'show', hash_, '--name-only', '--format=format:\"\"']\n            result = self._run_command(cmd)\n            if result['returncode']:\n                raise RuntimeError(('Could not fetch affected paths:\\n%s' % result['output']))\n            affected_paths = result['output'].splitlines()\n            log_entries.append(LogEntry(msg, affected_paths, self._get_author(hash_)))\n    return log_entries\n", "label": 1}
{"function": "\n\ndef eventFilter(self, obj, event):\n    ' Reimplemented to hide on certain key presses and on text edit focus\\n            changes.\\n        '\n    if (obj == self._text_edit):\n        etype = event.type()\n        if (etype == QtCore.QEvent.KeyPress):\n            key = event.key()\n            if (key in (QtCore.Qt.Key_Enter, QtCore.Qt.Key_Return)):\n                self.hide()\n            elif (key == QtCore.Qt.Key_Escape):\n                self.hide()\n                return True\n        elif (etype == QtCore.QEvent.FocusOut):\n            self.hide()\n        elif (etype == QtCore.QEvent.Enter):\n            self._hide_timer.stop()\n        elif (etype == QtCore.QEvent.Leave):\n            self._leave_event_hide()\n    return super(CallTipWidget, self).eventFilter(obj, event)\n", "label": 0}
{"function": "\n\ndef get_log_args(conf, log_file_name, **kwargs):\n    cmd_args = []\n    if conf.debug:\n        cmd_args.append('--debug')\n    if conf.verbose:\n        cmd_args.append('--verbose')\n    if (conf.log_dir or conf.log_file):\n        cmd_args.append(('--log-file=%s' % log_file_name))\n        log_dir = None\n        if (conf.log_dir and conf.log_file):\n            log_dir = os.path.dirname(os.path.join(conf.log_dir, conf.log_file))\n        elif conf.log_dir:\n            log_dir = conf.log_dir\n        elif conf.log_file:\n            log_dir = os.path.dirname(conf.log_file)\n        if log_dir:\n            cmd_args.append(('--log-dir=%s' % log_dir))\n        if (kwargs.get('metadata_proxy_watch_log') is False):\n            cmd_args.append('--nometadata_proxy_watch_log')\n    elif conf.use_syslog:\n        cmd_args.append('--use-syslog')\n        if conf.syslog_log_facility:\n            cmd_args.append(('--syslog-log-facility=%s' % conf.syslog_log_facility))\n    return cmd_args\n", "label": 1}
{"function": "\n\ndef parse_node(self, node, alias_map=None, conv=None):\n    (sql, params, unknown) = self._parse(node, alias_map, conv)\n    if (unknown and conv and params):\n        params = [conv.db_value(i) for i in params]\n    if isinstance(node, Node):\n        if node._negated:\n            sql = ('NOT %s' % sql)\n        if node._alias:\n            sql = ' '.join((sql, 'AS', node._alias))\n        if node._ordering:\n            sql = ' '.join((sql, node._ordering))\n    return (sql, params)\n", "label": 1}
{"function": "\n\ndef unify_users(request):\n    user = request.user\n    conf = app_by_application_name('molly.auth')\n    users = set()\n    for identifier in user.useridentifier_set.all():\n        if (not (identifier.namespace in conf.unify_identifiers)):\n            continue\n        identifiers = UserIdentifier.objects.filter(namespace=identifier.namespace, value=identifier.value)\n        users |= set((i.user for i in identifiers))\n    token_namespaces = set((t.namespace for t in user.externalservicetoken_set.all()))\n    identifier_namespaces = set((i.namespace for i in user.useridentifier_set.all()))\n    root_user = min(users, key=(lambda u: u.date_joined))\n    users_without_root = users.copy()\n    users_without_root = users_without_root.remove(root_user)\n    unifying_users.send_robust(sender=request, users=users_without_root, into=root_user)\n    for u in sorted(users, cmp=(lambda x, y: ((- 1) if (x == root_user) else (1 if (y == root_user) else 0)))):\n        u.usersession_set.all().update(user=root_user)\n        for token in u.externalservicetoken_set.all():\n            if ((u != user) and (token.namespace in token_namespaces)):\n                token.delete()\n            else:\n                token.user = root_user\n                token.save()\n        for identifier in u.useridentifier_set.all():\n            if ((u != user) and (identifier.namespace in identifier_namespaces)):\n                identifier.delete()\n            else:\n                identifier.user = root_user\n                identifier.save()\n        if (u != root_user):\n            u.delete()\n", "label": 1}
{"function": "\n\ndef getAllTestCases(self, root):\n    cases = []\n    counter = 0\n    q = queue.PriorityQueue()\n    q.put((0, counter, root))\n    while (not q.empty()):\n        c = q.get()[2]\n        if self.isExcluded(c):\n            continue\n        if (not hasattr(c, '__subclasses__')):\n            cases.append(c)\n        else:\n            for r in c.__subclasses__():\n                expansions = []\n                s = 0\n                counter += 1\n                if self.exp.isExpansion(r):\n                    continue\n                if (len(r.__subclasses__()) != 0):\n                    s += 100000\n                else:\n                    rc = r\n                    r = rc(self.fqdn, self.info)\n                    if (not r.getCritical()):\n                        if (r.getSeverity() == SEV_HIGH):\n                            s += 100\n                        elif (r.getSeverity() == SEV_MED):\n                            s += 200\n                        else:\n                            s += 300\n                    s += len(rc.__bases__)\n                    expansions = self.exp.getExpansions(r)\n                q.put((s, counter, r))\n                for e in expansions:\n                    s += 1\n                    q.put((s, counter, e))\n    counter = 0\n    for c in cases:\n        s = 1\n        counter += 1\n        if c.getCritical():\n            s = 0\n        q.put((s, counter, c))\n    cases = []\n    while (not q.empty()):\n        cases.append(q.get()[2])\n    return cases\n", "label": 1}
{"function": "\n\n@staticmethod\ndef requirements(fname):\n    '\\n        Create a list of requirements from the output of the pip freeze command\\n        saved in a text file.\\n        '\n    packages = Setup.read(fname, fail_silently=True).split('\\n')\n    packages = (p.strip() for p in packages)\n    packages = (p for p in packages if (p and (not p.startswith('#'))))\n    packages = (p for p in packages if (p and (not p.startswith('https://'))))\n    packages = (p for p in packages if (p and (not p.startswith('-r '))))\n    return list(packages)\n", "label": 1}
{"function": "\n\n@display_hook\ndef layout_display(layout, max_frames, max_branches):\n    if isinstance(layout, AdjointLayout):\n        layout = Layout.from_values(layout)\n    if (not isinstance(layout, (Layout, NdLayout))):\n        return None\n    nframes = len(unique_dimkeys(layout)[1])\n    if isinstance(layout, Layout):\n        if (layout._display == 'auto'):\n            branches = len(set([path[0] for path in list(layout.data.keys())]))\n            if (branches > max_branches):\n                return (('<tt>' + sanitize_HTML(layout)) + '</tt>')\n            elif ((len(layout.data) * nframes) > max_frames):\n                max_frame_warning(max_frames)\n                return (('<tt>' + sanitize_HTML(layout)) + '</tt>')\n    return render(layout)\n", "label": 0}
{"function": "\n\ndef grad(self, inputs, gout):\n    (gz,) = gout\n    is_continuous = [(inputs[i].dtype in tensor.continuous_dtypes) for i in range(len(inputs))]\n    if _is_sparse_variable(gz):\n        gz = dense_from_sparse(gz)\n    split = tensor.Split(len(inputs))(gz, 0, tensor.stack([x.shape[0] for x in inputs]))\n    if (not isinstance(split, list)):\n        split = [split]\n    derivative = [SparseFromDense(self.format)(s) for s in split]\n\n    def choose(continuous, derivative):\n        if continuous:\n            return derivative\n        else:\n            return None\n    return [choose(c, d) for (c, d) in zip(is_continuous, derivative)]\n", "label": 0}
{"function": "\n\ndef setup_fields_processors(config, model_cls, schema):\n    \" Set up model fields' processors.\\n\\n    :param config: Pyramid Configurator instance.\\n    :param model_cls: Model class for field of which processors should be\\n        set up.\\n    :param schema: Dict of model JSON schema.\\n    \"\n    properties = schema.get('properties', {\n        \n    })\n    for (field_name, props) in properties.items():\n        if (not props):\n            continue\n        processors = props.get('_processors')\n        backref_processors = props.get('_backref_processors')\n        if processors:\n            processors = [resolve_to_callable(val) for val in processors]\n            setup_kwargs = {\n                'model': model_cls,\n                'field': field_name,\n            }\n            config.add_field_processors(processors, **setup_kwargs)\n        if backref_processors:\n            db_settings = props.get('_db_settings', {\n                \n            })\n            is_relationship = (db_settings.get('type') == 'relationship')\n            document = db_settings.get('document')\n            backref_name = db_settings.get('backref_name')\n            if (not (is_relationship and document and backref_name)):\n                continue\n            backref_processors = [resolve_to_callable(val) for val in backref_processors]\n            setup_kwargs = {\n                'model': engine.get_document_cls(document),\n                'field': backref_name,\n            }\n            config.add_field_processors(backref_processors, **setup_kwargs)\n", "label": 1}
{"function": "\n\ndef spawn_independent(command, shell=False):\n    \" Given a command suitable for 'Popen', open the process such that if this\\n        process is killed, the spawned process survives.\\n\\n        `command` is either a list of strings, with the first item in the list being\\n        the executable and the rest being its arguments, or a single string containing\\n        the executable and its arguments.  In the latter case, any argument that\\n        contains spaces must be delimited with double-quotes.\\n    \"\n    if (sys.platform == 'win32'):\n        if isinstance(command, STRING_BASE_CLASS):\n            command = ('start /b ' + command)\n        else:\n            command.insert(0, 'start')\n            command.insert(1, '/b')\n        Popen(command, shell=True)\n    elif (sys.platform == 'darwin'):\n        pid = os.fork()\n        if pid:\n            return\n        else:\n            os.setpgrp()\n            if isinstance(command, STRING_BASE_CLASS):\n                tmp = quoted_split(command)\n            else:\n                tmp = command\n            os.execv(tmp[0], tmp)\n    else:\n        pid = os.fork()\n        if pid:\n            return\n        else:\n            os.setpgrp()\n            Popen(command, shell=shell)\n            sys.exit(0)\n", "label": 0}
{"function": "\n\ndef process_response(self, request, response):\n    'Sets the cache, if needed.'\n    if (not self._should_update_cache(request, response)):\n        return response\n    if (response.streaming or (response.status_code != 200)):\n        return response\n    if ((not request.COOKIES) and response.cookies and has_vary_header(response, 'Cookie')):\n        return response\n    timeout = get_max_age(response)\n    if (timeout is None):\n        timeout = self.cache_timeout\n    elif (timeout == 0):\n        return response\n    patch_response_headers(response, timeout)\n    if timeout:\n        cache_key = learn_cache_key(request, response, timeout, self.key_prefix, cache=self.cache)\n        if (hasattr(response, 'render') and callable(response.render)):\n            response.add_post_render_callback((lambda r: self.cache.set(cache_key, r, timeout)))\n        else:\n            self.cache.set(cache_key, response, timeout)\n    return response\n", "label": 1}
{"function": "\n\n@cuda.jit(argtypes=[f4[:, :], f4[:, :], f4[:, :]])\ndef cu_square_matrix_mul(A, B, C):\n    sA = cuda.shared.array(shape=(tpb, tpb), dtype=f4)\n    sB = cuda.shared.array(shape=(tpb, tpb), dtype=f4)\n    tx = cuda.threadIdx.x\n    ty = cuda.threadIdx.y\n    bx = cuda.blockIdx.x\n    by = cuda.blockIdx.y\n    bw = cuda.blockDim.x\n    bh = cuda.blockDim.y\n    x = (tx + (bx * bw))\n    y = (ty + (by * bh))\n    acc = 0.0\n    for i in range(bpg):\n        if ((x < n) and (y < n)):\n            sA[(ty, tx)] = A[(y, (tx + (i * tpb)))]\n            sB[(ty, tx)] = B[((ty + (i * tpb)), x)]\n        cuda.syncthreads()\n        if ((x < n) and (y < n)):\n            for j in range(tpb):\n                acc += (sA[(ty, j)] * sB[(j, tx)])\n        cuda.syncthreads()\n    if ((x < n) and (y < n)):\n        C[(y, x)] = acc\n", "label": 1}
{"function": "\n\ndef get_selected_file_regions(self):\n    files = []\n    lines = self.get_selected_lines()\n    if (not lines):\n        return files\n    for f in self.get_all_file_regions():\n        for l in lines:\n            if l.contains(f):\n                linestr = self.view.substr(l).strip()\n                if (linestr.startswith(STATUS_LABELS['R']) and (' -> ' in linestr)):\n                    names = self.view.substr(f)\n                    e = names.find(' -> ')\n                    s = (e + 4)\n                    f1 = sublime.Region(f.begin(), (f.begin() + e))\n                    f2 = sublime.Region((f.begin() + s), f.end())\n                    files.append((self.section_at_region(f), f1))\n                    files.append((self.section_at_region(f), f2))\n                else:\n                    files.append((self.section_at_region(f), f))\n    return files\n", "label": 0}
{"function": "\n\ndef GetApplicationPath(file=None):\n    import re, os, platform\n    if (not hasattr(GetApplicationPath, 'dir')):\n        if hasattr(sys, 'frozen'):\n            dir = os.path.dirname(sys.executable)\n        elif ('__file__' in globals()):\n            dir = os.path.dirname(os.path.realpath(__file__))\n        else:\n            dir = os.getcwd()\n        GetApplicationPath.dir = dir\n    if (file is None):\n        file = ''\n    if ((not file.startswith('/')) and (not file.startswith('\\\\')) and (not re.search('^[\\\\w-]+:', file))):\n        path = ((GetApplicationPath.dir + os.sep) + file)\n        if (platform.system() == 'Windows'):\n            path = re.sub('[/\\\\\\\\]+', re.escape(os.sep), path)\n        path = re.sub('[/\\\\\\\\]+$', '', path)\n        return path\n    return str(file)\n", "label": 1}
{"function": "\n\ndef queue(self):\n    while self.socket:\n        if (not self.ready):\n            time.sleep(0.1)\n            continue\n        for (i, message) in enumerate(self.msgQueue):\n            for channel in self.channels:\n                if (len(message) > 400):\n                    for l in range(int(math.ceil((len(message) / 400.0)))):\n                        chunk = message[(l * 400):((l + 1) * 400)]\n                        self.send(('PRIVMSG %s :%s' % (channel, chunk)))\n                else:\n                    self.send(('PRIVMSG %s :%s' % (channel, message)))\n            del self.msgQueue[i]\n        self.msgQueue = []\n        time.sleep(0.1)\n", "label": 0}
{"function": "\n\ndef readinto(self, b):\n    if (self.fp is None):\n        return 0\n    if (self._method == 'HEAD'):\n        self._close_conn()\n        return 0\n    if self.chunked:\n        return self._readinto_chunked(b)\n    if (self.length is not None):\n        if (len(b) > self.length):\n            b = memoryview(b)[0:self.length]\n    if PY2:\n        data = self.fp.read(len(b))\n        n = len(data)\n        b[:n] = data\n    else:\n        n = self.fp.readinto(b)\n    if ((not n) and b):\n        self._close_conn()\n    elif (self.length is not None):\n        self.length -= n\n        if (not self.length):\n            self._close_conn()\n    return n\n", "label": 1}
{"function": "\n\ndef __init__(self, code, type, value):\n    self.code = code\n    self.type = type\n    if (self.type == 'Message'):\n        if isinstance(value, django.core.exceptions.ValidationError):\n            self.value = value.message_dict\n            if [item[1][0] for item in self.value.items() if ('already exists' in item[1][0])]:\n                self.code = self.DUPLICATE_ENTRY\n        elif isinstance(value, list):\n            self.value = value[0].message_dict\n            self.value['message'] = str(value[1])\n        else:\n            self.value = {\n                'message': str(value),\n            }\n    else:\n        self.value = value\n", "label": 0}
{"function": "\n\ndef _union(self, other):\n    if other.is_ComplexRegion:\n        if ((not self.polar) and (not other.polar)):\n            return ComplexRegion(Union(self.sets, other.sets))\n        elif (self.polar and other.polar):\n            return ComplexRegion(Union(self.sets, other.sets), polar=True)\n    if (self == S.Complexes):\n        return self\n    return None\n", "label": 0}
{"function": "\n\ndef pickaddr(self, proto):\n    if (proto == socket.AF_INET):\n        return (HOST, 0)\n    else:\n        dir = None\n        if (os.name == 'os2'):\n            dir = '\\\\socket'\n        fn = tempfile.mktemp(prefix='unix_socket.', dir=dir)\n        if (os.name == 'os2'):\n            if (fn[1] == ':'):\n                fn = fn[2:]\n            if (fn[0] in (os.sep, os.altsep)):\n                fn = fn[1:]\n            if (os.sep == '/'):\n                fn = fn.replace(os.sep, os.altsep)\n            else:\n                fn = fn.replace(os.altsep, os.sep)\n        self.test_files.append(fn)\n        return fn\n", "label": 0}
{"function": "\n\ndef log_actor_new(self, actor_id, actor_name, actor_type, is_shadow):\n    ' Trace actor new\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_ACTOR_NEW in logger.events)):\n            if ((not logger.actors) or (actor_id in logger.actors)):\n                data = {\n                    \n                }\n                data['timestamp'] = time.time()\n                data['node_id'] = self.node.id\n                data['type'] = 'actor_new'\n                data['actor_id'] = actor_id\n                data['actor_name'] = actor_name\n                data['actor_type'] = actor_type\n                data['is_shadow'] = is_shadow\n                if (logger.connection is not None):\n                    if (not logger.connection.connection_lost):\n                        logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                    else:\n                        disconnected.append(user_id)\n                elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                    msg = {\n                        'cmd': 'logevent',\n                        'msgid': logger.handle,\n                        'header': None,\n                        'data': ('data: %s\\n\\n' % json.dumps(data)),\n                    }\n                    self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef handle_task(self, task, **kwargs):\n    log_prefix = ('%s - ' % (task.id,))\n    task_expiration = kwargs.get('task_expiration', None)\n    if self.expire_task(task, task_expiration):\n        return None\n    monitor = Monitor.registry[task.context['monitor']['name']]\n    command = monitor.command\n    task.context = command.build_context(task.context)\n    previous_state = self.get_state(task.id)\n    timeout = task.context.get('monitor_timeout', kwargs.get('monitor_timeout'))\n    max_retries = task.context.get('max_retries', kwargs.get('max_retries'))\n    last_attempt = int(task.attempt)\n    current_attempt = (last_attempt + 1)\n    result = self.execute_task(task, timeout, **kwargs)\n    if (result.state == types.STATE_OK):\n        if (previous_state and (not (previous_state.state == types.STATE_OK)) and (previous_state.state_type == types.STATE_TYPE_SOFT)):\n            result.state_type = types.STATE_TYPE_SOFT\n    else:\n        logger.debug((log_prefix + 'current_attempt: %d, max_retries: %d'), current_attempt, max_retries)\n        if (current_attempt <= max_retries):\n            if ((not previous_state) or (previous_state.state_type == types.STATE_TYPE_SOFT) or (previous_state.state == types.STATE_OK)):\n                result.state_type = types.STATE_TYPE_SOFT\n                delay = task.context.get('retry_delay', kwargs.get('retry_delay'))\n                delay = max(delay, 0)\n                logger.debug('Resubmitting task with %ds delay.', delay)\n                self.resubmit_task(task, delay, **kwargs)\n        else:\n            logger.debug('Retry limit hit, not resubmitting.')\n    result.validate()\n    return result\n", "label": 1}
{"function": "\n\ndef change_privileges(self, enable_privs=[], disable_privs=[]):\n    privs = []\n    privs.extend(((_privileges.privilege(p).pyobject(), _privileges.PRIVILEGE_ATTRIBUTE.ENABLED) for p in enable_privs))\n    privs.extend(((_privileges.privilege(p).pyobject(), 0) for p in disable_privs))\n    old_privs = wrapped(win32security.AdjustTokenPrivileges, self.hToken, 0, privs)\n    return ([_privileges.privilege(priv) for (priv, status) in old_privs if (status == _privileges.PRIVILEGE_ATTRIBUTE.ENABLED)], [_privileges.privilege(priv) for (priv, status) in old_privs if (status == 0)])\n", "label": 0}
{"function": "\n\ndef RunCommand(self):\n    'Command entry point for the cat command.'\n    show_header = False\n    request_range = None\n    start_byte = 0\n    end_byte = None\n    if self.sub_opts:\n        for (o, a) in self.sub_opts:\n            if (o == '-h'):\n                show_header = True\n            elif (o == '-r'):\n                request_range = a.strip()\n                range_matcher = re.compile('^(?P<start>[0-9]+)-(?P<end>[0-9]*)$|^(?P<endslice>-[0-9]+)$')\n                range_match = range_matcher.match(request_range)\n                if (not range_match):\n                    raise CommandException(('Invalid range (%s)' % request_range))\n                if range_match.group('start'):\n                    start_byte = long(range_match.group('start'))\n                if range_match.group('end'):\n                    end_byte = long(range_match.group('end'))\n                if range_match.group('endslice'):\n                    start_byte = long(range_match.group('endslice'))\n            else:\n                self.RaiseInvalidArgumentException()\n    return CatHelper(self).CatUrlStrings(self.args, show_header=show_header, start_byte=start_byte, end_byte=end_byte)\n", "label": 1}
{"function": "\n\ndef update_editor(self):\n    ' Updates the editor when the object trait changes externally to the\\n            editor.\\n        '\n    control = self.control\n    new_value = self.str_value\n    if (hasattr(self.factory, 'password') and self.factory.password):\n        new_value = ('*' * len(new_value))\n    if ((self.item.resizable is True) or (self.item.height != (- 1.0))):\n        if (control.GetValue() != new_value):\n            control.SetValue(new_value)\n            control.SetInsertionPointEnd()\n    elif (control.GetLabel() != new_value):\n        control.SetLabel(new_value)\n", "label": 0}
{"function": "\n\ndef move(self, entries, directory):\n    '\\n        Move one or more entries (file or directory) to the destination\\n        directory\\n\\n        :param list entries: a list of source entries (:class:`.BaseFile`\\n            object)\\n        :param directory: destination directory\\n        :return: whether the action is successful\\n        :raise: :class:`.APIError` if something bad happened\\n        '\n    fcids = []\n    for entry in entries:\n        if isinstance(entry, File):\n            fcid = entry.fid\n        elif isinstance(entry, Directory):\n            fcid = entry.cid\n        else:\n            raise APIError('Invalid BaseFile instance for an entry.')\n        fcids.append(fcid)\n    if (not isinstance(directory, Directory)):\n        raise APIError('Invalid destination directory.')\n    if self._req_files_move(directory.cid, fcids):\n        for entry in entries:\n            if isinstance(entry, File):\n                entry.cid = directory.cid\n            entry.reload()\n        return True\n    else:\n        raise APIError('Error moving entries.')\n", "label": 0}
{"function": "\n\ndef short_contests(platforms):\n    'Gets all the short contests(less than or equal to 4 hours of duration)'\n    contests_data = get_contests_data()\n    active_contests = contests_data['active']\n    upcoming_contests = contests_data['pending']\n    platform_filter = get_platform_filter(platforms)\n    get_challenge_duration = (lambda x: (int(x.split(':')[0]) if ('days' not in x) else float('inf')))\n    short_contests = [contest for contest in active_contests if ((get_challenge_duration(contest['duration']) <= 4) and (contest['host_name'] in platform_filter))]\n    short_contests += [contest for contest in upcoming_contests if ((get_challenge_duration(contest['duration']) <= 4) and (contest['host_name'] in platform_filter))]\n    return short_contests\n", "label": 0}
{"function": "\n\ndef start(self, sockets=None, **kwargs):\n    \"\\n        Present the PTY of the container inside the current process.\\n\\n        This will take over the current process' TTY until the container's PTY\\n        is closed.\\n        \"\n    (pty_stdin, pty_stdout, pty_stderr) = (sockets or self.sockets())\n    pumps = []\n    if (pty_stdin and self.interactive):\n        pumps.append(io.Pump(io.Stream(self.stdin), pty_stdin, wait_for_output=False))\n    if pty_stdout:\n        pumps.append(io.Pump(pty_stdout, io.Stream(self.stdout), propagate_close=False))\n    if pty_stderr:\n        pumps.append(io.Pump(pty_stderr, io.Stream(self.stderr), propagate_close=False))\n    if (not self._container_info()['State']['Running']):\n        self.client.start(self.container, **kwargs)\n    return pumps\n", "label": 0}
{"function": "\n\ndef write(self):\n    for (name, metric) in self.registry:\n        if isinstance(metric, Meter):\n            self.send_metric(name, 'meter', metric, ['count', 'one_minute_rate', 'five_minute_rate', 'fifteen_minute_rate', 'mean_rate'])\n        if isinstance(metric, Gauge):\n            self.send_metric(name, 'gauge', metric, ['value'])\n        if isinstance(metric, UtilizationTimer):\n            self.send_metric(name, 'timer', metric, ['count', 'one_minute_rate', 'five_minute_rate', 'fifteen_minute_rate', 'mean_rate', 'min', 'max', 'mean', 'stddev', 'one_minute_utilization', 'five_minute_utilization', 'fifteen_minute_utilization', 'mean_utilization'], ['median', 'percentile_95th'])\n        if isinstance(metric, Timer):\n            self.send_metric(name, 'timer', metric, ['count', 'one_minute_rate', 'five_minute_rate', 'fifteen_minute_rate', 'mean_rate', 'min', 'max', 'mean', 'stddev'], ['median', 'percentile_95th'])\n        if isinstance(metric, Counter):\n            self.send_metric(name, 'counter', metric, ['count'])\n        if isinstance(metric, Histogram):\n            self.send_metric(name, 'histogram', metric, ['count', 'min', 'max', 'mean', 'stddev'], ['median', 'percentile_95th'])\n    self._send()\n", "label": 0}
{"function": "\n\ndef update_state(bug):\n    log.debug('Starting bug \"{}\" update'.format(bug.externalId))\n    now = datetime.utcnow()\n    td = (now - datetime.replace(bug.updated, tzinfo=None))\n    diff = ((td.microseconds + ((td.seconds + ((td.days * 24) * 3600)) * (10 ** 6))) / (10 ** 6))\n    if (bug.state in settings.BUG_STATE_EXPIRED):\n        old_state = bug.state\n        new_state = get_issue_fields_from_bts(bug.externalId)['status']['name']\n        log.debug('Comparing bug state,\"{0}\" and \"{1}\"'.format(old_state, new_state))\n        if ((old_state == new_state) and (diff > float(settings.BUG_TIME_EXPIRED))):\n            log.debug('Bug \"{}\" expired, deleting it from DB'.format(bug.externalId))\n            bug.delete()\n        elif ((old_state == new_state) and (diff < float(settings.BUG_TIME_EXPIRED))):\n            log.debug('Bug \"{}\" not updated, because {} seconds not expired'.format(bug.externalId, settings.BUG_TIME_EXPIRED))\n        else:\n            bug.state = new_state\n            bug.updated = now\n            log.debug('Saving bug \"{}\"'.format(bug.externalId))\n            bug.save()\n    if ((bug.state not in settings.BUG_STATE_EXPIRED) and (diff > float(settings.TIME_BEFORE_UPDATE_BUG_INFO))):\n        log.debug('%s > %s time to update bug state.', diff, settings.TIME_BEFORE_UPDATE_BUG_INFO)\n        bug.updated = now\n        bug.state = get_issue_fields_from_bts(bug.externalId)['status']['name']\n        log.debug('Saving bug \"{}\"'.format(bug.externalId))\n        bug.save()\n", "label": 0}
{"function": "\n\n@staticmethod\ndef Mul(expr, assumptions):\n    '\\n        Integer*Integer      -> Integer\\n        Integer*Irrational   -> !Integer\\n        Odd/Even             -> !Integer\\n        Integer*Rational     -> ?\\n        '\n    if expr.is_number:\n        return AskIntegerHandler._number(expr, assumptions)\n    _output = True\n    for arg in expr.args:\n        if (not ask(Q.integer(arg), assumptions)):\n            if arg.is_Rational:\n                if (arg.q == 2):\n                    return ask(Q.even((2 * expr)), assumptions)\n                if (~ (arg.q & 1)):\n                    return None\n            elif ask(Q.irrational(arg), assumptions):\n                if _output:\n                    _output = False\n                else:\n                    return\n            else:\n                return\n    else:\n        return _output\n", "label": 1}
{"function": "\n\ndef compute_whdr(self, r, delta=0.1):\n    ' Compute the Weighted Human Disagreement for a reflectance image\\n        ``r``.  '\n    error_sum = 0.0\n    weight_sum = 0.0\n    for c in self.comparisons:\n        point1 = self.id_to_points[c['point1']]\n        point2 = self.id_to_points[c['point2']]\n        darker = c['darker']\n        weight = c['darker_score']\n        if ((not point1['opaque']) or (not point2['opaque'])):\n            continue\n        if ((weight < 0) or (weight is None)):\n            raise ValueError(('Invalid darker_score: %s' % weight))\n        if (darker not in ('1', '2', 'E')):\n            raise ValueError(('Invalid darker: %s' % darker))\n        l1 = np.mean(r[(int((point1['y'] * r.shape[0])), int((point1['x'] * r.shape[1])), ...)])\n        l2 = np.mean(r[(int((point2['y'] * r.shape[0])), int((point2['x'] * r.shape[1])), ...)])\n        l1 = max(l1, 1e-10)\n        l2 = max(l2, 1e-10)\n        if ((l2 / l1) > (1.0 + delta)):\n            alg_darker = '1'\n        elif ((l1 / l2) > (1.0 + delta)):\n            alg_darker = '2'\n        else:\n            alg_darker = 'E'\n        if (darker != alg_darker):\n            error_sum += weight\n        weight_sum += weight\n    if weight_sum:\n        return (error_sum / weight_sum)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef detect_nonsilent(audio_segment, min_silence_len=1000, silence_thresh=(- 16)):\n    silent_ranges = detect_silence(audio_segment, min_silence_len, silence_thresh)\n    len_seg = len(audio_segment)\n    if (not silent_ranges):\n        return [[0, len_seg]]\n    if ((silent_ranges[0][0] == 0) and (silent_ranges[0][1] == len_seg)):\n        return []\n    prev_end_i = 0\n    nonsilent_ranges = []\n    for (start_i, end_i) in silent_ranges:\n        nonsilent_ranges.append([prev_end_i, start_i])\n        prev_end_i = end_i\n    if (end_i != len_seg):\n        nonsilent_ranges.append([prev_end_i, len_seg])\n    if (nonsilent_ranges[0] == [0, 0]):\n        nonsilent_ranges.pop(0)\n    return nonsilent_ranges\n", "label": 0}
{"function": "\n\ndef _find_changed_targets(self):\n    changed = self._directly_changed_targets()\n    if (not changed):\n        return changed\n    if (self._include_dependees == 'none'):\n        return changed\n    for address in self._address_mapper.scan_addresses():\n        self._build_graph.inject_address_closure(address)\n    if (self._include_dependees == 'direct'):\n        return changed.union(*[self._build_graph.dependents_of(addr) for addr in changed])\n    if (self._include_dependees == 'transitive'):\n        return set((t.address for t in self._build_graph.transitive_dependees_of_addresses(changed)))\n    raise ValueError('Unknown dependee inclusion: \"{}\"'.format(self._include_dependees))\n", "label": 0}
{"function": "\n\ndef by_type(typename, objects=None):\n    \"Return objects tracked by the garbage collector with a given class name.\\n\\n    Example:\\n\\n        >>> by_type('MyClass')\\n        [<mymodule.MyClass object at 0x...>]\\n\\n    Note that the GC does not track simple objects like int or str.\\n\\n    .. versionchanged:: 1.7\\n       New parameter: ``objects``.\\n\\n    .. versionchanged:: 1.8\\n       Accepts fully-qualified type names (i.e. 'package.module.ClassName')\\n       as well as short type names (i.e. 'ClassName').\\n\\n    \"\n    if (objects is None):\n        objects = gc.get_objects()\n    try:\n        if ('.' in typename):\n            return [o for o in objects if (_long_typename(o) == typename)]\n        else:\n            return [o for o in objects if (_short_typename(o) == typename)]\n    finally:\n        del objects\n", "label": 0}
{"function": "\n\ndef parseGnmap(inFile):\n    targets = {\n        \n    }\n    for hostLine in inFile:\n        currentTarget = []\n        fields = hostLine.split(' ')\n        ip = fields[1]\n        for item in fields:\n            if ((item.find('http') != (- 1)) and re.findall('\\\\d+/open', item)):\n                port = None\n                https = False\n                \"\\n\\t\\t\\t\\tnmap has a bunch of ways to list HTTP like services, for example:\\n\\t\\t\\t\\t8089/open/tcp//ssl|http\\n\\t\\t\\t\\t8000/closed/tcp//http-alt///\\n\\t\\t\\t\\t8008/closed/tcp//http///\\n\\t\\t\\t\\t8080/closed/tcp//http-proxy//\\n\\t\\t\\t\\t443/open/tcp//ssl|https?///\\n\\t\\t\\t\\t8089/open/tcp//ssl|http\\n\\t\\t\\t\\tSince we want to detect them all, let's just match on the word http\\n\\t\\t\\t\\tand make special cases for things containing https and ssl when we\\n\\t\\t\\t\\tconstruct the URLs.\\n\\t\\t\\t\\t\"\n                port = item.split('/')[0]\n                if ((item.find('https') != (- 1)) or (item.find('ssl') != (- 1))):\n                    https = True\n                currentTarget.append([port, https])\n        if (len(currentTarget) > 0):\n            targets[ip] = currentTarget\n    return targets\n", "label": 0}
{"function": "\n\ndef get_virtualenv_path(ve_base, ve_name):\n    'Check a virtualenv path, raising exceptions to explain problems.\\n    '\n    if (not ve_base):\n        raise exceptions.NoVirtualenvsDirectory('could not figure out a virtualenvs directory. make sure $HOME is set, or $WORKON_HOME, or set virtualenvs=something in your .vexrc')\n    if (not os.path.exists(ve_base)):\n        message = 'virtualenvs directory {0!r} not found. Create it or use vex --make to get started.'.format(ve_base)\n        raise exceptions.NoVirtualenvsDirectory(message)\n    if (not ve_name):\n        raise exceptions.InvalidVirtualenv('no virtualenv name')\n    ve_path = os.path.join(ve_base, ve_name)\n    if ((ve_path == ve_name) and (os.path.basename(ve_name) != ve_name)):\n        raise exceptions.InvalidVirtualenv('To run in a virtualenv by its path, use \"vex --path {0}\"'.format(ve_path))\n    ve_path = os.path.abspath(ve_path)\n    if (not os.path.exists(ve_path)):\n        raise exceptions.InvalidVirtualenv('no virtualenv found at {0!r}.'.format(ve_path))\n    return ve_path\n", "label": 0}
{"function": "\n\ndef dates(self, start_date, end_date, return_name=False):\n    '\\n        Calculate holidays observed between start date and end date\\n\\n        Parameters\\n        ----------\\n        start_date : starting date, datetime-like, optional\\n        end_date : ending date, datetime-like, optional\\n        return_name : bool, optional, default=False\\n            If True, return a series that has dates and holiday names.\\n            False will only return dates.\\n        '\n    start_date = Timestamp(start_date)\n    end_date = Timestamp(end_date)\n    filter_start_date = start_date\n    filter_end_date = end_date\n    if (self.year is not None):\n        dt = Timestamp(datetime(self.year, self.month, self.day))\n        if return_name:\n            return Series(self.name, index=[dt])\n        else:\n            return [dt]\n    dates = self._reference_dates(start_date, end_date)\n    holiday_dates = self._apply_rule(dates)\n    if (self.days_of_week is not None):\n        holiday_dates = holiday_dates[np.in1d(holiday_dates.dayofweek, self.days_of_week)]\n    if (self.start_date is not None):\n        filter_start_date = max(self.start_date.tz_localize(filter_start_date.tz), filter_start_date)\n    if (self.end_date is not None):\n        filter_end_date = min(self.end_date.tz_localize(filter_end_date.tz), filter_end_date)\n    holiday_dates = holiday_dates[((holiday_dates >= filter_start_date) & (holiday_dates <= filter_end_date))]\n    if return_name:\n        return Series(self.name, index=holiday_dates)\n    return holiday_dates\n", "label": 0}
{"function": "\n\ndef get_message(data):\n    '\\n    Extract status and message from the given dictionary.\\n    The message is translated according to the hash map.\\n\\n    :param data: status and message hash/key pair\\n    :type data: dict\\n    :returns: (success, message) tuple\\n    '\n    success = data['success']\n    message = ''\n    if success:\n        if (('message' in data) and (len(data['message']) == 2)):\n            hash_map = data['message'][0]\n            message_code = data['message'][1]\n            message = getattr(g, hash_map)(message_code)\n    elif (('errors' in data) and (len(data['errors']) > 0)):\n        error_tuple = data['errors'][0].get('code')\n        if (error_tuple and (len(error_tuple) == 2)):\n            hash_map = error_tuple[0]\n            error_code = error_tuple[1]\n            message = getattr(g, hash_map)(error_code)\n    return (success, message)\n", "label": 0}
{"function": "\n\ndef get_constraint_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances')\n    constraint_matrix = [[True for j in xrange(num_instances)] for i in xrange(num_hosts)]\n    instance_type = (filter_properties.get('instance_type') or {\n        \n    })\n    requested_ram = instance_type.get('memory_mb', 0)\n    if ('memory_mb' not in instance_type):\n        LOG.warn(_LW(\"No information about requested instances' RAM size was found, default value (0) is used.\"))\n    if (requested_ram <= 0):\n        LOG.warn(_LW('RamConstraint is skipped because requested instance RAM size is 0 or invalid.'))\n        return constraint_matrix\n    for i in xrange(num_hosts):\n        ram_allocation_ratio = self._get_ram_allocation_ratio(hosts[i], filter_properties)\n        free_ram_mb = hosts[i].free_ram_mb\n        total_usable_ram_mb = hosts[i].total_usable_ram_mb\n        memory_mb_limit = (total_usable_ram_mb * ram_allocation_ratio)\n        used_ram_mb = (total_usable_ram_mb - free_ram_mb)\n        usable_ram = (memory_mb_limit - used_ram_mb)\n        acceptable_num_instances = int((usable_ram / requested_ram))\n        if (acceptable_num_instances < num_instances):\n            inacceptable_num = (num_instances - acceptable_num_instances)\n            constraint_matrix[i] = ([True for j in xrange(acceptable_num_instances)] + [False for j in xrange(inacceptable_num)])\n        LOG.debug('%(host)s can accept %(num)s requested instances according to RamConstraint.', {\n            'host': hosts[i],\n            'num': acceptable_num_instances,\n        })\n        hosts[i].limits['memory_mb'] = memory_mb_limit\n    return constraint_matrix\n", "label": 1}
{"function": "\n\ndef deploy_ftp(deploy_configs):\n    'for ftp'\n    conn_kwargs = {\n        'host': deploy_configs['host'],\n    }\n    login_kwargs = {\n        \n    }\n    if ('port' in deploy_configs):\n        conn_kwargs.update({\n            'port': deploy_configs['port'],\n        })\n    if ('user' in deploy_configs):\n        login_kwargs.update({\n            'user': deploy_configs['user'],\n        })\n    if ('password' in deploy_configs):\n        passwd = deploy_configs['password']\n        if (passwd is None):\n            passwd = getpass.getpass('Input your ftp password: ')\n        login_kwargs.update({\n            'passwd': passwd,\n        })\n    ftp_dir = deploy_configs.get('dir', '/')\n    output_dir = configs['destination']\n    ftp = ftplib.FTP()\n    ftp.connect(**conn_kwargs)\n    ftp.login(**login_kwargs)\n    for (root, dirs, files) in os.walk(output_dir):\n        rel_root = os.path.relpath(root, output_dir)\n        for fn in files:\n            store_fn = os.path.join(ftp_dir, rel_root, fn)\n            ftp.storbinary(('STOR %s' % store_fn), open(os.path.join(root, fn), 'rb'))\n    ftp.close()\n", "label": 0}
{"function": "\n\ndef find_gopaths(self):\n    'search for potential GOPATHs.'\n    goroot = set((os.path.normpath(s) for s in os.environ.get('GOROOT', '').split(os.pathsep)))\n    gopath = set((os.path.normpath(s) for s in os.environ.get('GOPATH', '').split(os.pathsep)))\n    if ('.' in gopath):\n        gopath.remove('.')\n    gopath = list(gopath)\n    dirparts = os.path.dirname(self.filename).split(os.sep)\n    for i in range((len(dirparts) - 1), 1, (- 1)):\n        if (dirparts[i].lower() != 'src'):\n            continue\n        p = os.path.normpath(os.sep.join(dirparts[:i]))\n        if ((p not in goroot) and (p not in gopath)):\n            gopath.append(p)\n    if persist.debug_mode():\n        persist.printf('{}: {} {}'.format(self.name, os.path.basename((self.filename or '<unsaved>')), ('guessed GOPATH=' + os.pathsep.join(gopath))))\n    return os.pathsep.join(gopath)\n", "label": 1}
{"function": "\n\ndef star_status(cluster, logdir, cmdline, *args):\n    'Print result itemization'\n    if (not args):\n        args = sorted(cluster.connections.keys())\n    if (not cluster.last_result):\n        print('Cluster status not availble / no command has been run')\n        return\n    missing_results = []\n    for x in args:\n        job = cluster.last_result.get(cluster.locate(x), None)\n        if job:\n            res = job.result\n            running_time = (job.end_time - job.start_time)\n            if isinstance(res, CommandResult):\n                print(('%s: %s - Return Code [%s] took %0.4g seconds' % (x, res.status, res.return_code, running_time)))\n            else:\n                print(('%s: *** Error *** [%s] took %0.4g seconds' % (x, repr(res), running_time)))\n        else:\n            missing_results.append(x)\n    if missing_results:\n        if (len(missing_results) < 10):\n            print('Missing results from:', ', '.join(missing_results))\n        else:\n            print(('Missing results from %d hosts' % len(missing_results)))\n", "label": 0}
{"function": "\n\ndef __call__(self, irc, msg):\n    self.__parent.__call__(irc, msg)\n    if self.disabled(irc):\n        return\n    nick = self._getNick(irc.network)\n    if (nick not in self.registryValue('nicks')):\n        return\n    nickserv = self.registryValue('NickServ')\n    password = self._getNickServPassword(nick)\n    ghostDelay = self.registryValue('ghostDelay')\n    if (not ghostDelay):\n        return\n    if (nick and nickserv and password and (not ircutils.strEqual(nick, irc.nick))):\n        if (irc.afterConnect and ((self.sentGhost is None) or ((self.sentGhost + ghostDelay) < time.time()))):\n            if (nick in irc.state.nicksToHostmasks):\n                self._doGhost(irc)\n            else:\n                irc.sendMsg(ircmsgs.nick(nick))\n", "label": 1}
{"function": "\n\ndef numIslands2(self, n, m, operators):\n    '\\n        :type n: int\\n        :type m: int\\n        :type operators: list[Point]\\n        :rtype: list[int]\\n        '\n    rows = n\n    cols = m\n    unroll = (lambda x, y: ((x * cols) + y))\n    mat = [[0 for _ in xrange(cols)] for _ in xrange(rows)]\n    uf = UnionFind(rows, cols)\n    ret = []\n    for op in operators:\n        uf.add(unroll(op.x, op.y))\n        mat[op.x][op.y] = 1\n        for dir in self.dirs:\n            x1 = (op.x + dir[0])\n            y1 = (op.y + dir[1])\n            if ((0 <= x1 < rows) and (0 <= y1 < cols) and (mat[x1][y1] == 1)):\n                uf.union(unroll(op.x, op.y), unroll(x1, y1))\n        ret.append(uf.count)\n    return ret\n", "label": 0}
{"function": "\n\n@ensure_tag(['p'])\ndef is_header(el, meta_data):\n    if _is_top_level_upper_roman(el, meta_data):\n        return 'h2'\n    el_is_natural_header = is_natural_header(el, meta_data.styles_dict)\n    if el_is_natural_header:\n        return el_is_natural_header\n    if _is_li(el):\n        return False\n    w_namespace = get_namespace(el, 'w')\n    if (el.tag == ('%stbl' % w_namespace)):\n        return False\n    if DETECT_FONT_SIZE:\n        font_size = get_font_size(el, meta_data.styles_dict)\n        if (font_size is not None):\n            if meta_data.font_sizes_dict[font_size]:\n                return meta_data.font_sizes_dict[font_size]\n    num_words = len(etree.tostring(el, encoding=unicode, method='text').split(' '))\n    if (num_words > 8):\n        return False\n    (whole_line_bold, whole_line_italics) = whole_line_styled(el)\n    if (whole_line_bold or whole_line_italics):\n        return 'h2'\n    return False\n", "label": 1}
{"function": "\n\ndef diff_config(base, ref, exclude=None):\n    diff = ConfigParser()\n    if (exclude is None):\n        exclude = set()\n    for section in ref.sections():\n        for option in ref.options(section):\n            if ((section, option) in exclude):\n                continue\n            value = ref.get(section, option)\n            if base.has_option(section, option):\n                if (base.get(section, option) == value):\n                    continue\n            if (not diff.has_section(section)):\n                diff.add_section(section)\n            diff.set(section, option, value)\n    return diff\n", "label": 0}
{"function": "\n\ndef main():\n    import sys\n    from Naked.commandline import Command\n    from Naked.toolshed.state import StateObject\n    c = Command(sys.argv[0], sys.argv[1:])\n    state = StateObject()\n    if (not c.command_suite_validates()):\n        from commit_entropy.settings import usage as commit_entropy_usage\n        print(commit_entropy_usage)\n        sys.exit(1)\n    if c.help():\n        from commit_entropy.settings import help as entropy_help\n        print(commit_entropy_help)\n        sys.exit(0)\n    elif c.usage():\n        from commit_entropy.settings import usage as commit_entropy_usage\n        print(commit_entropy_usage)\n        sys.exit(0)\n    elif c.version():\n        from commit_entropy.settings import app_name, major_version, minor_version, patch_version\n        version_display_string = ((((((app_name + ' ') + major_version) + '.') + minor_version) + '.') + patch_version)\n        print(version_display_string)\n        sys.exit(0)\n    elif (c.cmd == 'csv'):\n        from commit_entropy.commands.csv_printer import CsvPrinter\n        if c.flag('--ignore'):\n            ignore = c.flag_arg('--ignore').split(',')\n        else:\n            ignore = []\n        printer = CsvPrinter()\n        printer.run(ignore=ignore)\n    else:\n        print('Could not complete the command that you entered.  Please try again.')\n        sys.exit(1)\n", "label": 0}
{"function": "\n\n@classmethod\ndef build_security_headers(cls, context):\n    '\\n        Examining context, builds a set of headers containing necessary forwarded items.\\n\\n        @return     A new dictionary containing headers from the context that are important.\\n        '\n    header = {\n        \n    }\n    actor_id = context.get('ion-actor-id', None)\n    actor_roles = context.get('ion-actor-roles', None)\n    actor_tokens = context.get('ion-actor-tokens', None)\n    expiry = context.get('expiry', None)\n    container_id = context.get('origin-container-id', None)\n    original_conv_id = context.get('original-conv-id', None)\n    conv_id = context.get('conv-id', None)\n    if actor_id:\n        header['ion-actor-id'] = actor_id\n        if actor_roles:\n            header['ion-actor-roles'] = actor_roles\n    if actor_tokens:\n        header['ion-actor-tokens'] = actor_tokens\n    if expiry:\n        header['expiry'] = expiry\n    if container_id:\n        header['origin-container-id'] = container_id\n    if original_conv_id:\n        header['original-conv-id'] = original_conv_id\n    elif conv_id:\n        header['original-conv-id'] = conv_id\n    return header\n", "label": 0}
{"function": "\n\n@recursivereader\ndef pools_get_vs(self, pools=None, minimal=False):\n    'Returns VirtualServers associated with a list of Pools'\n    if (pools is None):\n        pools = f5.Pool._get_list(self)\n    elif isinstance(pools[0], f5.Pool):\n        pools = [pool.name for pool in pools]\n    result = {pool: [] for pool in pools}\n    vss = f5.VirtualServer._get(self, minimal=minimal)\n    if (minimal is True):\n        vss = f5.VirtualServer._refresh_default_pool(self, vss)\n    for pool in pools:\n        for vs in vss:\n            print(vs._default_pool)\n            if (pool == vs._default_pool.name):\n                result[pool].append(vs)\n    return result\n", "label": 1}
{"function": "\n\ndef snapshot(self, conf, no_metadata=False, disk_only=False, reuse_ext=False, quiesce=False):\n    'Creates a guest snapshot.\\n\\n        :param conf: libvirt.LibvirtConfigGuestSnapshotDisk\\n        :param no_metadata: Make snapshot without remembering it\\n        :param disk_only: Disk snapshot, no system checkpoint\\n        :param reuse_ext: Reuse any existing external files\\n        :param quiesce: Use QGA to quiece all mounted file systems\\n        '\n    flags = (no_metadata and (libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_NO_METADATA or 0))\n    flags |= (disk_only and (libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_DISK_ONLY or 0))\n    flags |= (reuse_ext and (libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_REUSE_EXT or 0))\n    flags |= ((quiesce and libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_QUIESCE) or 0)\n    self._domain.snapshotCreateXML(conf.to_xml(), flags=flags)\n", "label": 1}
{"function": "\n\ndef buildChildren(self, child_, node, nodeName_, fromsubclass_=False):\n    if (nodeName_ == 'Transaction_ID'):\n        obj_ = cybox_common.HexBinaryObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Transaction_ID(obj_)\n    elif (nodeName_ == 'Question'):\n        obj_ = DNSQuestionType.factory()\n        obj_.build(child_)\n        self.set_Question(obj_)\n    elif (nodeName_ == 'Answer_Resource_Records'):\n        obj_ = DNSResourceRecordsType.factory()\n        obj_.build(child_)\n        self.set_Answer_Resource_Records(obj_)\n    elif (nodeName_ == 'Authority_Resource_Records'):\n        obj_ = DNSResourceRecordsType.factory()\n        obj_.build(child_)\n        self.set_Authority_Resource_Records(obj_)\n    elif (nodeName_ == 'Additional_Records'):\n        obj_ = DNSResourceRecordsType.factory()\n        obj_.build(child_)\n        self.set_Additional_Records(obj_)\n    elif (nodeName_ == 'Date_Ran'):\n        obj_ = cybox_common.DateTimeObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Date_Ran(obj_)\n    elif (nodeName_ == 'Service_Used'):\n        obj_ = cybox_common.StringObjectPropertyType.factory()\n        obj_.build(child_)\n        self.set_Service_Used(obj_)\n    super(DNSQueryObjectType, self).buildChildren(child_, node, nodeName_, True)\n", "label": 0}
{"function": "\n\ndef sum_add(self, other, method=0):\n    'Helper function for Sum simplification'\n    from sympy.concrete.summations import Sum\n    if (type(self) == type(other)):\n        if (method == 0):\n            if (self.limits == other.limits):\n                return Sum((self.function + other.function), *self.limits)\n        elif (method == 1):\n            if (simplify((self.function - other.function)) == 0):\n                if (len(self.limits) == len(other.limits) == 1):\n                    i = self.limits[0][0]\n                    x1 = self.limits[0][1]\n                    y1 = self.limits[0][2]\n                    j = other.limits[0][0]\n                    x2 = other.limits[0][1]\n                    y2 = other.limits[0][2]\n                    if (i == j):\n                        if (x2 == (y1 + 1)):\n                            return Sum(self.function, (i, x1, y2))\n                        elif (x1 == (y2 + 1)):\n                            return Sum(self.function, (i, x2, y1))\n    return Add(self, other)\n", "label": 1}
{"function": "\n\ndef build_policy(config=None, update=None, replace=None):\n    'Builds the policy as a string from the settings.'\n    if (config is None):\n        config = from_settings()\n    if (update is not None):\n        for (key, value) in update.items():\n            if (not isinstance(value, (list, tuple))):\n                value = (value,)\n            if (config[key] is not None):\n                config[key] += value\n            else:\n                config[key] = value\n    if (replace is not None):\n        for (key, value) in replace.items():\n            if ((value is not None) and (not isinstance(value, (list, tuple)))):\n                value = [value]\n            config[key] = value\n    report_uri = config.pop('report-uri', None)\n    policy = [('%s %s' % (k, ' '.join(v))) for (k, v) in sorted(config.items()) if (v is not None)]\n    if report_uri:\n        policy.append(('report-uri %s' % report_uri))\n    return '; '.join(policy)\n", "label": 1}
{"function": "\n\ndef _translate_stos_suffix(self, tb, instruction, suffix):\n    if (suffix == 'b'):\n        src = ReilRegisterOperand('al', 8)\n    elif (suffix == 'w'):\n        src = ReilRegisterOperand('ax', 16)\n    elif (suffix == 'd'):\n        src = ReilRegisterOperand('eax', 32)\n    elif (suffix == 'q'):\n        src = ReilRegisterOperand('rax', 64)\n    else:\n        raise Exception(('Invalid instruction suffix: %s' % suffix))\n    if (self._arch_mode == ARCH_X86_MODE_32):\n        dst = ReilRegisterOperand('edi', 32)\n    elif (self._arch_mode == ARCH_X86_MODE_64):\n        dst = ReilRegisterOperand('rdi', 64)\n    else:\n        raise Exception('Invalid architecture mode: %d', self._arch_mode)\n    if instruction.prefix:\n        (counter, loop_start_lbl) = self._rep_prefix_begin(tb, instruction)\n    tb.add(self._builder.gen_stm(src, dst))\n    self._update_strings_dst(tb, dst, src.size, instruction)\n    if instruction.prefix:\n        self._rep_prefix_end(tb, instruction, counter, loop_start_lbl)\n", "label": 1}
{"function": "\n\ndef parse(self, path, a_file=True):\n    output = None\n    tosca = ToscaTemplate(path, None, a_file)\n    version = tosca.version\n    if tosca.version:\n        print(('\\nversion: ' + version))\n    if hasattr(tosca, 'description'):\n        description = tosca.description\n        if description:\n            print(('\\ndescription: ' + description))\n    if hasattr(tosca, 'inputs'):\n        inputs = tosca.inputs\n        if inputs:\n            print('\\ninputs:')\n            for input in inputs:\n                print(('\\t' + input.name))\n    if hasattr(tosca, 'nodetemplates'):\n        nodetemplates = tosca.nodetemplates\n        if nodetemplates:\n            print('\\nnodetemplates:')\n            for node in nodetemplates:\n                print(('\\t' + node.name))\n    if hasattr(tosca, 'outputs'):\n        outputs = tosca.outputs\n        if outputs:\n            print('\\noutputs:')\n            for output in outputs:\n                print(('\\t' + output.name))\n", "label": 1}
{"function": "\n\ndef save(self, request, contact_type=None):\n    'Process form and create DB objects as required'\n    if self.instance:\n        contact = self.instance\n    else:\n        contact = Contact()\n        contact.contact_type = contact_type\n    contact.name = unicode(self.cleaned_data['name'])\n    if ('parent' in self.cleaned_data):\n        contact.parent = self.cleaned_data['parent']\n    if ('related_user' in self.cleaned_data):\n        contact.related_user = self.cleaned_data['related_user']\n    contact.save()\n    if self.instance:\n        contact.contactvalue_set.all().delete()\n    for field in contact.contact_type.fields.all():\n        for form_name in self.cleaned_data:\n            if re.match(str((('^' + field.name) + '___\\\\d+$')), form_name):\n                if isinstance(self.fields[form_name], forms.FileField):\n                    value = ContactValue(field=field, contact=contact, value=self._handle_uploaded_file(form_name))\n                    if isinstance(self.fields[form_name], forms.ImageField):\n                        self._image_resize(value.value)\n                elif ((field.field_type == 'picture') and isinstance(self.fields[form_name], forms.ChoiceField)):\n                    if (self.cleaned_data[form_name] != 'delete'):\n                        value = ContactValue(field=field, contact=contact, value=self.cleaned_data[form_name])\n                else:\n                    value = ContactValue(field=field, contact=contact, value=self.cleaned_data[form_name])\n                value.save()\n    return contact\n", "label": 1}
{"function": "\n\ndef toggle(request):\n    if request.user.is_anonymous():\n        return redirect('/login/')\n    key = [k for k in request.POST.keys() if ('check' in k)][0].split('.')[0]\n    (verb, pk) = key.split('-')\n    blast = get_object_or_404(Blast, pk=pk)\n    blast.set_viewing_user(request.user)\n    if (not blast.viewing_user_can_mark_done()):\n        return HttpResponse('You are not allowed to check off that task')\n    if (verb == 'check'):\n        blast.done = True\n    if (verb == 'uncheck'):\n        blast.done = False\n    blast.save()\n    return redirect((request.POST.get('back_to', '') or '/'))\n", "label": 0}
{"function": "\n\ndef _convert_header_value(self, value):\n    if isinstance(value, bytes_type):\n        pass\n    elif isinstance(value, unicode_type):\n        value = value.encode('utf-8')\n    elif isinstance(value, numbers.Integral):\n        return str(value)\n    elif isinstance(value, datetime.datetime):\n        return httputil.format_timestamp(value)\n    else:\n        raise TypeError(('Unsupported header value %r' % value))\n    if ((len(value) > 4000) or RequestHandler._INVALID_HEADER_CHAR_RE.search(value)):\n        raise ValueError('Unsafe header value %r', value)\n    return value\n", "label": 0}
{"function": "\n\ndef _filter_by_date(journal, after, before):\n    'return a list of entries after date\\n\\n    :param before: A naive datetime representing a UTC time.\\n    :param after: A naive datetime representing a UTC time\\n    '\n    if ((after is None) and (before is None)):\n        return journal\n    return [item for item in journal if (((after is None) or (item['Creation Date'] >= after)) and ((before is None) or (item['Creation Date'] < before)))]\n", "label": 0}
{"function": "\n\ndef lineReceived(self, line):\n    '\\n        IMC2 -> Evennia\\n\\n        Triggered when text is received from the IMC2 network. Figures out\\n        what to do with the packet. This deals with the following\\n\\n        Args:\\n            line (str): Incoming text.\\n\\n        '\n    line = line.strip()\n    if (not self.is_authenticated):\n        self._imc_login(line)\n        return\n    packet = pck.IMC2Packet(self.mudname, packet_str=line)\n    if (packet.packet_type == 'is-alive'):\n        self.imc2_mudlist.update_mud_from_packet(packet)\n    elif (packet.packet_type == 'keepalive-request'):\n        self.send_packet(pck.IMC2PacketIsAlive())\n    elif (packet.packet_type == 'ice-msg-b'):\n        self.data_out(text=line, packettype='broadcast')\n    elif (packet.packet_type == 'whois-reply'):\n        self._whois_reply(packet)\n    elif (packet.packet_type == 'close-notify'):\n        self.imc2_mudlist.remove_mud_from_packet(packet)\n    elif (packet.packet_type == 'ice-update'):\n        self.imc2_chanlist.update_channel_from_packet(packet)\n    elif (packet.packet_type == 'ice-destroy'):\n        self.imc2_chanlist.remove_channel_from_packet(packet)\n    elif (packet.packet_type == 'tell'):\n        pass\n", "label": 1}
{"function": "\n\ndef _process(self, p, element, ranges={\n    \n}):\n    if isinstance(element.data, np.ndarray):\n        if ('dataframe' in Dataset.datatype):\n            el_data = element.table('dataframe')\n        else:\n            el_data = element.table('ndelement')\n    el_data = element.data\n    dims = [d for d in element.dimensions() if _is_number(element.range(d)[0])]\n    permuted_dims = [(d1, d2) for d1 in dims for d2 in dims[::(- 1)]]\n    data = {\n        \n    }\n    for (d1, d2) in permuted_dims:\n        if (d1 == d2):\n            if (p.diagonal_type is Histogram):\n                bin_range = ranges.get(d1.name, element.range(d1))\n                el = element.hist(dimension=d1.name, bin_range=bin_range, adjoin=False)\n            else:\n                values = element.dimension_values(d1)\n                el = p.diagonal_type(values, kdims=[d1])\n        else:\n            el = p.chart_type(el_data, kdims=[d1], vdims=[d2])\n        data[(d1.name, d2.name)] = el\n    return data\n", "label": 1}
{"function": "\n\ndef add_configuration(options):\n    '\\n    interactively add a new configuration\\n\\n    '\n    if (options.username != None):\n        username = options.username\n    else:\n        username = prompt('Username: ')\n    if (options.password != None):\n        password = options.password\n    else:\n        password = prompt('Password: ', hide_input=(not options.show_password))\n    if (options.app_url != None):\n        app_url = options.app_url\n    else:\n        app_url = prompt('App URL (default: https://app.jut.io just hit enter): ')\n    if (app_url.strip() == ''):\n        app_url = 'https://app.jut.io'\n    section = ('%s@%s' % (username, app_url))\n    if config.exists(section):\n        raise JutException(('Configuration for \"%s\" already exists' % section))\n    token_manager = auth.TokenManager(username=username, password=password, app_url=app_url)\n    authorization = authorizations.get_authorization(token_manager, app_url=app_url)\n    client_id = authorization['client_id']\n    client_secret = authorization['client_secret']\n    deployment_name = default_deployment(app_url, client_id, client_secret)\n    config.add(section, **{\n        'app_url': app_url,\n        'deployment_name': deployment_name,\n        'username': username,\n        'client_id': client_id,\n        'client_secret': client_secret,\n    })\n    if options.default:\n        config.set_default(name=section)\n    else:\n        default_configuration(interactive=False)\n", "label": 0}
{"function": "\n\ndef update_if_changed(self, entity, post):\n    'Find out if the post has more +1 or reshares.'\n    changed = False\n    if (entity.plus_oners != post['object']['plusoners']['totalItems']):\n        changed = True\n        entity.plus_oners = post['object']['plusoners']['totalItems']\n    if (entity.resharers != post['object']['resharers']['totalItems']):\n        changed = True\n        entity.resharers = post['object']['resharers']['totalItems']\n    if (entity.comments != post['object']['replies']['totalItems']):\n        changed = True\n        entity.comments = post['object']['replies']['totalItems']\n    content = post['object']['content']\n    if ('annotation' in post):\n        content += (' ' + post['annotation'])\n    prod_group = entity.get_product_groups(content)\n    act_type = entity.get_activity_types(content)\n    if (sorted(entity.product_group) != sorted(prod_group)):\n        changed = True\n        entity.product_group = prod_group\n    if (sorted(entity.activity_type) != sorted(act_type)):\n        changed = True\n        entity.activity_type = act_type\n    if changed:\n        return entity\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef fields_for_document(document, fields=None, exclude=None, formfield_callback=None):\n    '\\n    Returns a ``SortedDict`` containing form fields for the given model.\\n\\n    ``fields`` is an optional list of field names. If provided, only the named\\n    fields will be included in the returned fields.\\n\\n    ``exclude`` is an optional list of field names. If provided, the named\\n    fields will be excluded from the returned fields, even if they are listed\\n    in the ``fields`` argument.\\n    '\n    field_list = []\n    structure = document.structure\n    for (field_name, field_type) in structure.items():\n        if (fields and (not (field_name in fields))):\n            continue\n        if (exclude and (field_name in exclude)):\n            continue\n        form_field = None\n        if formfield_callback:\n            form_field = formfield_callback(document, field_name)\n        if (not form_field):\n            form_field = formfield_for_document_field(document, field_name)\n        if form_field:\n            field_list.append((field_name, form_field))\n    field_dict = SortedDict(field_list)\n    if fields:\n        field_dict = SortedDict([(f, field_dict.get(f)) for f in fields if ((not exclude) or (exclude and (f not in exclude)))])\n    return field_dict\n", "label": 1}
{"function": "\n\ndef _broadcast(self, arys):\n    'Perform numpy ufunc broadcasting\\n        '\n    shapelist = [a.shape for a in arys]\n    shape = _multi_broadcast(*shapelist)\n    for (i, ary) in enumerate(arys):\n        if (ary.shape == shape):\n            pass\n        elif self.is_device_array(ary):\n            arys[i] = self.broadcast_device(ary, shape)\n        else:\n            ax_differs = [ax for ax in range(len(shape)) if ((ax >= ary.ndim) or (ary.shape[ax] != shape[ax]))]\n            missingdim = (len(shape) - len(ary.shape))\n            strides = (([0] * missingdim) + list(ary.strides))\n            for ax in ax_differs:\n                strides[ax] = 0\n            strided = np.lib.stride_tricks.as_strided(ary, shape=shape, strides=strides)\n            arys[i] = self.force_array_layout(strided)\n    return arys\n", "label": 1}
{"function": "\n\ndef constrain_stationary_multivariate(unconstrained, variance, transform_variance=False, prefix=None):\n    use_list = (type(unconstrained) == list)\n    if use_list:\n        unconstrained = np.concatenate(unconstrained, axis=1)\n    (k_endog, order) = unconstrained.shape\n    order //= k_endog\n    if (order < 1):\n        raise ValueError('Must have order at least 1')\n    if (k_endog < 1):\n        raise ValueError('Must have at least 1 endogenous variable')\n    if (prefix is None):\n        (prefix, dtype, _) = find_best_blas_type([unconstrained, variance])\n    dtype = prefix_dtype_map[prefix]\n    unconstrained = np.asfortranarray(unconstrained, dtype=dtype)\n    variance = np.asfortranarray(variance, dtype=dtype)\n    sv_constrained = prefix_sv_map[prefix](unconstrained, order, k_endog)\n    (constrained, variance) = prefix_pacf_map[prefix](sv_constrained, variance, transform_variance, order, k_endog)\n    constrained = np.array(constrained, dtype=dtype)\n    variance = np.array(variance, dtype=dtype)\n    if use_list:\n        constrained = [constrained[:k_endog, (i * k_endog):((i + 1) * k_endog)] for i in range(order)]\n    return (constrained, variance)\n", "label": 0}
{"function": "\n\ndef handle_starttag(self, tag, attrs):\n    if (tag == 'meta'):\n        http_equiv = content = None\n        for (k, v) in attrs:\n            if (k == 'http-equiv'):\n                http_equiv = string.lower(v)\n            elif (k == 'content'):\n                content = v\n        if ((http_equiv == 'content-type') and content):\n            header = mimetools.Message(StringIO.StringIO(('%s: %s\\n\\n' % (http_equiv, content))))\n            encoding = header.getparam('charset')\n            if encoding:\n                self.encoding = encoding\n    if (tag in AUTOCLOSE):\n        if (self.__stack and (self.__stack[(- 1)] == tag)):\n            self.handle_endtag(tag)\n    self.__stack.append(tag)\n    attrib = {\n        \n    }\n    if attrs:\n        for (k, v) in attrs:\n            attrib[string.lower(k)] = v\n    self.__builder.start(tag, attrib)\n    if (tag in IGNOREEND):\n        self.__stack.pop()\n        self.__builder.end(tag)\n", "label": 1}
{"function": "\n\ndef _get_tokens(self, cli, text):\n    m = self.compiled_grammar.match_prefix(text)\n    if m:\n        characters = [[self.default_token, c] for c in text]\n        for v in m.variables():\n            lexer = self.lexers.get(v.varname)\n            if lexer:\n                document = Document(text[v.start:v.stop])\n                lexer_tokens_for_line = lexer.lex_document(cli, document)\n                lexer_tokens = []\n                for i in range(len(document.lines)):\n                    lexer_tokens.extend(lexer_tokens_for_line(i))\n                    lexer_tokens.append((Token, '\\n'))\n                if lexer_tokens:\n                    lexer_tokens.pop()\n                i = v.start\n                for (t, s) in lexer_tokens:\n                    for c in s:\n                        if (characters[i][0] == self.default_token):\n                            characters[i][0] = t\n                        i += 1\n        trailing_input = m.trailing_input()\n        if trailing_input:\n            for i in range(trailing_input.start, trailing_input.stop):\n                characters[i][0] = Token.TrailingInput\n        return characters\n    else:\n        return [(Token, text)]\n", "label": 1}
{"function": "\n\ndef _model_shorthand(self, args):\n    accum = []\n    for arg in args:\n        if isinstance(arg, Node):\n            accum.append(arg)\n        elif isinstance(arg, Query):\n            accum.append(arg)\n        elif isinstance(arg, ModelAlias):\n            accum.extend(arg.get_proxy_fields())\n        elif (isclass(arg) and issubclass(arg, Model)):\n            accum.extend(arg._meta.sorted_fields)\n    return accum\n", "label": 0}
{"function": "\n\n@classmethod\ndef _from_pb(cls, pb, set_key=True, ent=None, key=None):\n    'Internal helper to create an entity from an EntityProto protobuf.'\n    if (not isinstance(pb, entity_pb.EntityProto)):\n        raise TypeError(('pb must be a EntityProto; received %r' % pb))\n    if (ent is None):\n        ent = cls()\n    if ((key is None) and pb.key().path().element_size()):\n        key = Key(reference=pb.key())\n    if ((key is not None) and (set_key or key.id() or key.parent())):\n        ent._key = key\n    projection = []\n    for (indexed, plist) in ((True, pb.property_list()), (False, pb.raw_property_list())):\n        for p in plist:\n            if (p.meaning() == entity_pb.Property.INDEX_VALUE):\n                projection.append(p.name())\n            ent._get_property_for(p, indexed)._deserialize(ent, p)\n    ent._set_projection(projection)\n    return ent\n", "label": 1}
{"function": "\n\ndef _get_host_handlers(self, request):\n    host = request.host.lower().split(':')[0]\n    matches = []\n    for (pattern, handlers) in self.handlers:\n        if pattern.match(host):\n            matches.extend(handlers)\n    if ((not matches) and ('X-Real-Ip' not in request.headers)):\n        for (pattern, handlers) in self.handlers:\n            if pattern.match(self.default_host):\n                matches.extend(handlers)\n    return (matches or None)\n", "label": 0}
{"function": "\n\ndef dict_error_formatting(errors, index=None):\n    '\\n    Formats all dictionary error messages for both single and bulk requests\\n    '\n    formatted_error_list = []\n    top_level_error_keys = ['links', 'status', 'code', 'detail', 'source', 'meta']\n    resource_object_identifiers = ['type', 'id']\n    if (index is None):\n        index = ''\n    else:\n        index = (str(index) + '/')\n    for (error_key, error_description) in errors.iteritems():\n        if isinstance(error_description, basestring):\n            error_description = [error_description]\n        if (error_key in top_level_error_keys):\n            formatted_error_list.extend(({\n                error_key: description,\n            } for description in error_description))\n        elif (error_key in resource_object_identifiers):\n            formatted_error_list.extend([{\n                'source': {\n                    'pointer': ('/data/{}'.format(index) + error_key),\n                },\n                'detail': reason,\n            } for reason in error_description])\n        elif (error_key == 'non_field_errors'):\n            formatted_error_list.extend([{'detail': description for description in error_description}])\n        else:\n            formatted_error_list.extend([{\n                'source': {\n                    'pointer': ('/data/{}attributes/'.format(index) + error_key),\n                },\n                'detail': reason,\n            } for reason in error_description])\n    return formatted_error_list\n", "label": 1}
{"function": "\n\ndef isdatasimilar(data1, data2, verb=False, atol=ATOL, rtol=RTOL):\n    '\\n    Check that two sets of NMR data are equal within a tolerance.\\n\\n    Parameters\\n    ----------\\n    data1 : ndarray\\n        First array of NMR data\\n    data2 : ndarray\\n        Second array of NMR data\\n    verb : bool, optional\\n        Set True for verbose reporting.\\n    atol : float, optional\\n        The absolute tolerent parameter to pass to numpy.allclose.\\n    rtol : float, optional\\n        The relative tolenance parameter to pass to numpy.allclose.\\n\\n    Returns\\n    -------\\n    r1 : bool\\n        True is data1 and data2 are similar, False if they differ.\\n\\n    '\n    r = True\n    if (data1.dtype != data2.dtype):\n        r = False\n        if verb:\n            print('Dtypes do not match:', data1.dtype, data2.dtype)\n    if (data1.shape != data2.shape):\n        r = False\n        if verb:\n            print('Shapes do not match:', data1.shape, data2.shape)\n    if (np.allclose(data1, data2, rtol=rtol, atol=atol) is False):\n        r = False\n        if verb:\n            print('Data does not match')\n    return r\n", "label": 0}
{"function": "\n\ndef log_actor_firing(self, actor_id, action_method, tokens_produced, tokens_consumed, production):\n    ' Trace actor firing\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_ACTOR_FIRING in logger.events)):\n            if ((not logger.actors) or (actor_id in logger.actors)):\n                data = {\n                    \n                }\n                data['timestamp'] = time.time()\n                data['node_id'] = self.node.id\n                data['type'] = 'actor_fire'\n                data['actor_id'] = actor_id\n                data['action_method'] = action_method\n                data['produced'] = tokens_produced\n                data['consumed'] = tokens_consumed\n                if (self.LOG_ACTION_RESULT in logger.events):\n                    data['action_result'] = production\n                if (logger.connection is not None):\n                    if (not logger.connection.connection_lost):\n                        logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                    else:\n                        disconnected.append(user_id)\n                elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                    msg = {\n                        'cmd': 'logevent',\n                        'msgid': logger.handle,\n                        'header': None,\n                        'data': ('data: %s\\n\\n' % json.dumps(data)),\n                    }\n                    self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef _load_filter_group(self, s, group):\n    s = s.replace('    ', '\\t')\n    nl = '(?:\\n|\\r|\\r\\n)'\n    it = re.finditer(('^def filter\\\\(%s(?P<plural>s?)\\\\):(?P<flags>\\\\s*#.*)?%s\\t\"\"\"(?P<name>[^\"]*)\"\"\"%s(?P<body>(?:\\t.*%s)*)' % (self.signature, nl, nl, nl)), s, re.M)\n    for i in it:\n        name = i.group('name')\n        body = [x[1:] for x in re.split(nl, i.group('body'))]\n        if (body[(- 1)] == ''):\n            body = body[:(- 1)]\n        flag_string = i.group('flags')\n        active = False\n        on_exception = False\n        combined = (i.group('plural') == 's')\n        if flag_string:\n            flags = flag_string.strip()[1:].split(',')\n            for f in flags:\n                f = f.strip()\n                if (f == 'ACTIVE'):\n                    active = True\n                elif (f == 'EXCEPTION_TRUE'):\n                    on_exception = True\n        self.add_filter(name, body, active, combined, on_exception, group)\n", "label": 0}
{"function": "\n\ndef genLabel(self, role=None, fallbackToQname=False, fallbackToXlinkLabel=False, lang=None, strip=False, linkrole=None):\n    from arelle import XbrlConst\n    if (role is None):\n        role = XbrlConst.genStandardLabel\n    if (role == XbrlConst.conceptNameLabelRole):\n        return str(self.qname)\n    labelsRelationshipSet = self.modelXbrl.relationshipSet(XbrlConst.elementLabel, linkrole)\n    if labelsRelationshipSet:\n        label = labelsRelationshipSet.label(self, role, lang)\n        if (label is not None):\n            if strip:\n                return label.strip()\n            return Locale.rtlString(label, lang=lang)\n    if fallbackToQname:\n        return str(self.qname)\n    elif (fallbackToXlinkLabel and hasattr(self, 'xlinkLabel')):\n        return self.xlinkLabel\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef startup(self, root):\n    ' Initialization during setup.\\n\\n        Args\\n        ----\\n        root : `System`\\n           System containing variables.\\n        '\n    pathname = root.pathname\n    if (MPI and root.is_active()):\n        rrank = root.comm.rank\n        rowned = root._owning_ranks\n    self._record_p = self._record_u = self._record_r = False\n    for recorder in self._recorders:\n        recorder.startup(root)\n        if (not recorder._parallel):\n            self._has_serial_recorders = True\n        pnames = recorder._filtered[pathname]['p']\n        unames = recorder._filtered[pathname]['u']\n        rnames = recorder._filtered[pathname]['r']\n        if pnames:\n            self._record_p = True\n        if unames:\n            self._record_u = True\n        if rnames:\n            self._record_r = True\n        if MPI:\n            pnames = {n for n in pnames if (rrank == rowned[n])}\n            unames = {n for n in unames if (rrank == rowned[n])}\n            rnames = {n for n in rnames if (rrank == rowned[n])}\n            if recorder._parallel:\n                recorder._filtered[pathname]['p'] = pnames\n                recorder._filtered[pathname]['u'] = unames\n                recorder._filtered[pathname]['r'] = rnames\n        self._vars_to_record['pnames'].update(pnames)\n        self._vars_to_record['unames'].update(unames)\n        self._vars_to_record['rnames'].update(rnames)\n", "label": 1}
{"function": "\n\ndef __calculate_view_index(self, window, history_entry):\n    group = history_entry['group']\n    if ((group < 0) or (group >= window.num_groups())):\n        group = self.calling_view_index[0]\n    max_index = len(window.views_in_group(group))\n    saved_index = history_entry['index']\n    if (self.USE_SAVED_POSITION and (saved_index >= 0) and (saved_index <= max_index)):\n        index = saved_index\n    elif (self.NEW_TAB_POSITION == 'first'):\n        index = 0\n    elif (self.NEW_TAB_POSITION == 'last'):\n        index = max_index\n    elif self.calling_view_index:\n        index = (self.calling_view_index[1] + 1)\n    else:\n        index = 0\n    return (group, index)\n", "label": 1}
{"function": "\n\ndef fprop(self, inputs, inference=False, beta=0.0):\n    \"\\n        TODO:  Handle final layers that don't own their own outputs (bias, activation)\\n        \"\n    x = inputs\n    if inference:\n        inference_revert_list = []\n    for l in self.layers:\n        l.revert_list = []\n        altered_tensor = l.be.distribute_data(x, l.parallelism)\n        if altered_tensor:\n            if inference:\n                inference_revert_list.append((l, altered_tensor))\n            else:\n                l.revert_list.append(altered_tensor)\n        if ((l is self.layers[(- 1)]) and (beta != 0)):\n            x = l.fprop(x, inference, beta=beta)\n        else:\n            x = l.fprop(x, inference)\n    if inference:\n        for (layer, tensor) in inference_revert_list:\n            layer.be.revert_tensor(tensor)\n    return x\n", "label": 1}
{"function": "\n\ndef prepare_value(self, key, value):\n    if isinstance(value, bool):\n        type_prefix = 'b'\n    elif isinstance(value, six.integer_types):\n        type_prefix = 'i'\n    elif isinstance(value, six.string_types):\n        type_prefix = 's'\n    elif isinstance(value, float):\n        type_prefix = 'f'\n    elif isinstance(value, dict):\n        type_prefix = 'o'\n        value = self.prepare_object(value)\n    elif isinstance(value, list):\n        type_prefix = 'l'\n    elif isinstance(value, (datetime, date)):\n        type_prefix = 'd'\n    elif isinstance(value, uuid.UUID):\n        type_prefix = 'u'\n        value = value.hex\n    else:\n        raise TypeError(('cannot index values of type %s' % type(value)))\n    return (('%s_%s' % (type_prefix, key)), value)\n", "label": 1}
{"function": "\n\ndef loadTestsFromName(self, name, module=None):\n    root = self.getRootSuite()\n    if (name == 'suite'):\n        return root\n    all_tests = []\n    for (testcase, testname) in find_all_tests(root):\n        if ((testname == name) or testname.endswith(('.' + name)) or ((('.' + name) + '.') in testname) or testname.startswith((name + '.'))):\n            all_tests.append(testcase)\n    if (not all_tests):\n        raise LookupError(('could not find test case for \"%s\"' % name))\n    if (len(all_tests) == 1):\n        return all_tests[0]\n    rv = unittest.TestSuite()\n    for test in all_tests:\n        rv.addTest(test)\n    return rv\n", "label": 1}
{"function": "\n\ndef execute(self, args):\n    self.set_up_output_directory(args)\n    add_log_file(settings.log_file)\n    if os.path.isfile(args.agenda):\n        agenda = Agenda(args.agenda)\n        settings.agenda = args.agenda\n        shutil.copy(args.agenda, settings.meta_directory)\n        if (len(agenda.workloads) == 0):\n            raise ConfigError('No workloads specified')\n    elif (('.' in args.agenda) or (os.sep in args.agenda)):\n        raise ConfigError('Agenda \"{}\" does not exist.'.format(args.agenda))\n    else:\n        self.logger.debug('{} is not a file; assuming workload name.'.format(args.agenda))\n        agenda = Agenda()\n        agenda.add_workload_entry(args.agenda)\n    if args.instruments_to_disable:\n        if ('instrumentation' not in agenda.config):\n            agenda.config['instrumentation'] = []\n        for itd in args.instruments_to_disable:\n            self.logger.debug('Updating agenda to disable {}'.format(itd))\n            agenda.config['instrumentation'].append('~{}'.format(itd))\n    basename = 'config_'\n    for (file_number, path) in enumerate(settings.get_config_paths(), 1):\n        file_ext = os.path.splitext(path)[1]\n        shutil.copy(path, os.path.join(settings.meta_directory, ((basename + str(file_number)) + file_ext)))\n    executor = Executor()\n    executor.execute(agenda, selectors={\n        'ids': args.only_run_ids,\n    })\n", "label": 1}
{"function": "\n\ndef deploy_app(self, app_id, app_dir):\n    success = True\n    success = self._create(os.path.join(app_dir, 'namespace.json'))\n    for f in os.listdir(app_dir):\n        if (f != 'namespace.json'):\n            path = os.path.join(app_dir, f)\n            success = (success and self._create(path, namespace=app_id))\n            if (not success):\n                error_log(self.TAG, 'Could not deploy {0} on Kubernetes cluster'.format(path))\n    success = (success and self._register_proxy_route(app_id))\n    if (not success):\n        error_log(self.TAG, 'Could not deploy {} on Kubernetes cluster'.format(path))\n        return None\n    lookup_url = self._get_lookup_url()\n    app_url = urljoin(('https://' + lookup_url), app_id)\n    info_log(self.TAG, 'Access app at: \\n   {}'.format(app_url))\n    return app_url\n", "label": 0}
{"function": "\n\ndef extract_dates(obj):\n    'extract ISO8601 dates from unpacked JSON'\n    if isinstance(obj, dict):\n        obj = dict(obj)\n        for (k, v) in obj.items():\n            obj[k] = extract_dates(v)\n    elif isinstance(obj, (list, tuple)):\n        obj = [extract_dates(o) for o in obj]\n    elif isinstance(obj, str):\n        if ISO8601_PAT.match(obj):\n            obj = datetime.strptime(obj, ISO8601)\n    return obj\n", "label": 0}
{"function": "\n\ndef _do_one_inner_iteration(self, A, b, **kwargs):\n    '\\n        This method solves AX = b and returns the result to the corresponding\\n        algorithm.\\n        '\n    logger.info('Solving AX = b for the sparse matrices')\n    if (A is None):\n        A = self.A\n    if (b is None):\n        b = self.b\n    if (self._iterative_solver is None):\n        X = sprslin.spsolve(A, b)\n    else:\n        if (self._iterative_solver not in ['cg', 'gmres']):\n            raise Exception(('GenericLinearTransport does not support the' + ' requested iterative solver!'))\n        params = kwargs.copy()\n        solver_params = ['x0', 'tol', 'maxiter', 'xtype', 'M', 'callback']\n        [params.pop(item, None) for item in kwargs.keys() if (item not in solver_params)]\n        tol = kwargs.get('tol')\n        if (tol is None):\n            tol = 1e-20\n        params['tol'] = tol\n        if (self._iterative_solver == 'cg'):\n            result = sprslin.cg(A, b, **params)\n        elif (self._iterative_solver == 'gmres'):\n            result = sprslin.gmres(A, b, **params)\n        X = result[0]\n        self._iterative_solver_info = result[1]\n    return X\n", "label": 1}
{"function": "\n\ndef _build_header(self, raw_msg, raw_headers):\n    '\\n        Builds the header for this Process-level RPC conversation.\\n        https://confluence.oceanobservatories.org/display/syseng/CIAD+COI+OV+Common+Message+Format\\n        '\n    header = EndpointUnit._build_header(self, raw_msg, raw_headers)\n    header.update({\n        'sender-name': (self._process.name or 'unnamed-process'),\n        'sender': self._process.id,\n    })\n    if hasattr(self._process, 'process_type'):\n        header.update({\n            'sender-type': (self._process.process_type or 'unknown-process-type'),\n        })\n        if ((self._process.process_type == 'service') and hasattr(self.channel, '_send_name')):\n            header.update({\n                'sender-service': ('%s,%s' % (self.channel._send_name.exchange, self._process.name)),\n            })\n    context = self.get_context()\n    if isinstance(context, dict):\n        new_header = self.build_security_headers(context)\n        header.update(new_header)\n    else:\n        container_id = BaseEndpoint._get_container_instance().id\n        header['origin-container-id'] = container_id\n        if ('conv-id' in raw_headers):\n            header['original-conv-id'] = raw_headers['conv-id']\n    return header\n", "label": 0}
{"function": "\n\ndef _visitor(self, data, dirname, filesindir):\n    prune = []\n    self.insert_dirname(dirname)\n    for filename in filesindir:\n        if os.path.isdir(os.path.join(dirname, filename)):\n            if ((filename in ('.git', '.hg', '_build')) or filename.endswith('.egg-info')):\n                prune.append(filename)\n        else:\n            (name, ext) = os.path.splitext(filename)\n            if (not ((ext in ('.pyc',)) or filename.startswith('.'))):\n                (root, ext) = os.path.splitext(filename)\n                if (ext == '.py'):\n                    self.insert_filename(dirname, filename, ext)\n                else:\n                    prune.append(filename)\n            else:\n                prune.append(filename)\n    for filename in prune:\n        filesindir.remove(filename)\n", "label": 1}
{"function": "\n\ndef grad(self, inputs, gout):\n    (x,) = inputs\n    (gz,) = gout\n    if (x.dtype not in continuous_dtypes):\n        return [x.zeros_like(dtype=theano.config.floatX)]\n    if self.structured:\n        if (self.axis is None):\n            r = (gz * theano.sparse.sp_ones_like(x))\n        elif (self.axis == 0):\n            r = col_scale(theano.sparse.sp_ones_like(x), gz)\n        elif (self.axis == 1):\n            r = row_scale(theano.sparse.sp_ones_like(x), gz)\n        else:\n            raise ValueError('Illegal value for self.axis.')\n    else:\n        o_format = x.format\n        x = dense_from_sparse(x)\n        if _is_sparse_variable(gz):\n            gz = dense_from_sparse(gz)\n        if (self.axis is None):\n            r = tensor.second(x, gz)\n        else:\n            ones = tensor.ones_like(x)\n            if (self.axis == 0):\n                r = (tensor.addbroadcast(gz.dimshuffle('x', 0), 0) * ones)\n            elif (self.axis == 1):\n                r = (tensor.addbroadcast(gz.dimshuffle(0, 'x'), 1) * ones)\n            else:\n                raise ValueError('Illegal value for self.axis.')\n        r = SparseFromDense(o_format)(r)\n    return [r]\n", "label": 1}
{"function": "\n\ndef _validate_float_string(what):\n    if ((what[0] == '-') or (what[0] == '+')):\n        what = what[1:]\n    if what.isdigit():\n        return\n    (left, right) = what.split('.')\n    if ((left == '') and (right == '')):\n        raise dns.exception.FormError\n    if ((not (left == '')) and (not left.isdigit())):\n        raise dns.exception.FormError\n    if ((not (right == '')) and (not right.isdigit())):\n        raise dns.exception.FormError\n", "label": 1}
{"function": "\n\ndef func(self):\n    'Define command'\n    caller = self.caller\n    if ((not self.args) and ('edit' not in self.switches)):\n        caller.msg('Usage: @desc [<obj> =] <description>')\n        return\n    if ('edit' in self.switches):\n        self.edit_handler()\n        return\n    if self.rhs:\n        obj = caller.search(self.lhs)\n        if (not obj):\n            return\n        desc = self.rhs\n    else:\n        obj = (caller.location or self.msg(\"{rYou can't describe oblivion.{n\"))\n        if (not obj):\n            return\n        desc = self.args\n    obj.db.desc = desc\n    caller.msg(('The description was set on %s.' % obj.get_display_name(caller)))\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    self.meta = self.meta.copy()\n    if (kwargs.get('client') is not None):\n        self.meta.client = kwargs.get('client')\n    else:\n        self.meta.client = boto3.client(self.meta.service_name)\n    for (i, value) in enumerate(args):\n        setattr(self, ('_' + self.meta.identifiers[i]), value)\n    for (name, value) in kwargs.items():\n        if (name == 'client'):\n            continue\n        if (name not in self.meta.identifiers):\n            raise ValueError('Unknown keyword argument: {0}'.format(name))\n        setattr(self, ('_' + name), value)\n    for identifier in self.meta.identifiers:\n        if (getattr(self, identifier) is None):\n            raise ValueError('Required parameter {0} not set'.format(identifier))\n", "label": 0}
{"function": "\n\ndef wait_on_done(self, interval=2, timeout=((3600 * 24) * 7), **kwargs):\n    \"\\n        :param interval: Number of seconds between queries to the analysis's state\\n        :type interval: integer\\n        :param timeout: Maximum amount of time to wait, in seconds, until the analysis is done (or at least partially failed)\\n        :type timeout: integer\\n        :raises: :exc:`~dxpy.exceptions.DXError` if the timeout is reached before the analysis has finished running, or :exc:`~dxpy.exceptions.DXJobFailureError` if some job in the analysis has failed\\n\\n        Waits until the analysis has finished running.\\n        \"\n    elapsed = 0\n    while True:\n        state = self._get_state(**kwargs)\n        if (state == 'done'):\n            break\n        if (state in ['failed', 'partially_failed']):\n            desc = self.describe(**kwargs)\n            err_msg = 'Analysis has failed because of {failureReason}: {failureMessage}'.format(**desc)\n            if ((desc.get('failureFrom') != None) and (desc['failureFrom']['id'] != desc['id'])):\n                err_msg += ' (failure from {id})'.format(id=desc['failureFrom']['id'])\n            raise DXJobFailureError(err_msg)\n        if (state == 'terminated'):\n            raise DXJobFailureError('Analysis was terminated.')\n        if ((elapsed >= timeout) or (elapsed < 0)):\n            raise DXJobFailureError('Reached timeout while waiting for the analysis to finish')\n        time.sleep(interval)\n        elapsed += interval\n", "label": 1}
{"function": "\n\ndef verify_html(html):\n    in_tag = False\n    for c in html:\n        if (c == '<'):\n            if in_tag:\n                return False\n            in_tag = True\n        elif (c == '>'):\n            if (not in_tag):\n                return False\n            in_tag = False\n    if in_tag:\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef __replace(node, repl):\n    modified = False\n    reduction = 0\n    if ((node.type == 'identifier') and getattr(node, 'parent', None)):\n        if ((node.parent.type in ('dot', 'property_init')) and (type(node.value) is str) and __matcher.match(node.value)):\n            if (node.value in repl):\n                reduction = ((reduction + len(node.value)) - len(repl[node.value]))\n                node.value = repl[node.value]\n                modified = True\n            elif node.value.endswith('__'):\n                pass\n            else:\n                raise Error(node.value, node.line)\n    for child in node:\n        if (child != None):\n            (subModified, subReduction) = __replace(child, repl)\n            modified = (modified or subModified)\n            reduction = (reduction + subReduction)\n    return (modified, reduction)\n", "label": 1}
{"function": "\n\ndef _get_field(self, question):\n    '\\n        Create a form field based on a question.\\n        '\n    if (question.type == 'M'):\n        choices = ([('', '')] + [(c, c) for c in question.get_possible_answers()])\n        return forms.ChoiceField(label=question.text, required=question.required, widget=forms.Select(attrs={\n            'class': 'select_choice_field',\n        }), choices=choices)\n    if (question.type == 'R'):\n        choices = [(c, c) for c in question.get_possible_answers()]\n        return forms.ChoiceField(label=question.text, required=question.required, widget=forms.RadioSelect(attrs={\n            'class': 'radio_choice_field',\n        }), choices=choices)\n    if (question.type == 'C'):\n        choices = [(c, c) for c in question.get_possible_answers()]\n        return forms.MultipleChoiceField(label=question.text, required=question.required, widget=forms.CheckboxSelectMultiple(attrs={\n            'class': 'checkbox_choice_field',\n        }), choices=choices)\n    elif (question.type == 'S'):\n        return forms.CharField(label=question.text, required=question.required, widget=forms.TextInput(attrs={\n            'class': 'short_text_field',\n        }))\n    elif (question.type == 'L'):\n        return forms.CharField(label=question.text, required=question.required, widget=forms.Textarea(attrs={\n            'class': 'long_text_field',\n        }))\n", "label": 1}
{"function": "\n\ndef is_short_syllable(w, before=None):\n    ' A short syllable in a word is either:\\n        - a vowel followed by a non-vowel other than w, x or Y and preceded by a non-vowel\\n        - a vowel at the beginning of the word followed by a non-vowel. \\n        Checks the three characters before the given index in the word (or entire word if None).\\n    '\n    if (before != None):\n        i = (((before < 0) and (len(w) + before)) or before)\n        return is_short_syllable(w[max(0, (i - 3)):i])\n    if ((len(w) == 3) and is_consonant(w[0]) and is_vowel(w[1]) and is_consonant(w[2]) and (w[2] not in 'wxY')):\n        return True\n    if ((len(w) == 2) and is_vowel(w[0]) and is_consonant(w[1])):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef determine_rights(type, name):\n    rl = Rights()\n    if (type == 'user'):\n        rl.add('refresh')\n        rl.add('resolve')\n        rl.add('info')\n    elif (type in ['sa', 'authority+sa']):\n        rl.add('authority')\n        rl.add('sa')\n    elif (type in ['ma', 'authority+ma', 'cm', 'authority+cm', 'sm', 'authority+sm']):\n        rl.add('authority')\n        rl.add('ma')\n    elif (type == 'authority'):\n        rl.add('authority')\n        rl.add('sa')\n        rl.add('ma')\n    elif (type == 'slice'):\n        rl.add('refresh')\n        rl.add('embed')\n        rl.add('bind')\n        rl.add('control')\n        rl.add('info')\n    elif (type == 'component'):\n        rl.add('operator')\n    return rl\n", "label": 0}
{"function": "\n\ndef rel_for_model(self, model, field_obj=None, multi=False):\n    is_field = isinstance(field_obj, Field)\n    is_node = ((not is_field) and isinstance(field_obj, Node))\n    if multi:\n        accum = []\n    for field in self.sorted_fields:\n        if (isinstance(field, ForeignKeyField) and (field.rel_model == model)):\n            is_match = ((field_obj is None) or (is_field and (field_obj.name == field.name)) or (is_node and (field_obj._alias == field.name)))\n            if is_match:\n                if (not multi):\n                    return field\n                accum.append(field)\n    if multi:\n        return accum\n", "label": 1}
{"function": "\n\ndef get_template_from_request(request, obj=None, no_current_page=False):\n    '\\n    Gets a valid template from different sources or falls back to the default\\n    template.\\n    '\n    template = None\n    if (len(get_cms_setting('TEMPLATES')) == 1):\n        return get_cms_setting('TEMPLATES')[0][0]\n    if (hasattr(request, 'POST') and ('template' in request.POST)):\n        template = request.POST['template']\n    elif (hasattr(request, 'GET') and ('template' in request.GET)):\n        template = request.GET['template']\n    if ((not template) and (obj is not None)):\n        template = obj.get_template()\n    if ((not template) and (not no_current_page) and hasattr(request, 'current_page')):\n        current_page = request.current_page\n        if hasattr(current_page, 'get_template'):\n            template = current_page.get_template()\n    if ((template is not None) and (template in dict(get_cms_setting('TEMPLATES')).keys())):\n        if ((template == constants.TEMPLATE_INHERITANCE_MAGIC) and obj):\n            return obj.get_template()\n        return template\n    return get_cms_setting('TEMPLATES')[0][0]\n", "label": 1}
{"function": "\n\ndef _join_multi(self, other, how, return_indexers=True):\n    from .multi import MultiIndex\n    self_is_mi = isinstance(self, MultiIndex)\n    other_is_mi = isinstance(other, MultiIndex)\n    self_names = [n for n in self.names if (n is not None)]\n    other_names = [n for n in other.names if (n is not None)]\n    overlap = list((set(self_names) & set(other_names)))\n    if (not len(overlap)):\n        raise ValueError('cannot join with no level specified and no overlapping names')\n    if (len(overlap) > 1):\n        raise NotImplementedError('merging with more than one level overlap on a multi-index is not implemented')\n    jl = overlap[0]\n    if (not (self_is_mi and other_is_mi)):\n        flip_order = False\n        if self_is_mi:\n            (self, other) = (other, self)\n            flip_order = True\n            how = {\n                'right': 'left',\n                'left': 'right',\n            }.get(how, how)\n        level = other.names.index(jl)\n        result = self._join_level(other, level, how=how, return_indexers=return_indexers)\n        if flip_order:\n            if isinstance(result, tuple):\n                return (result[0], result[2], result[1])\n        return result\n    raise NotImplementedError('merging with both multi-indexes is not implemented')\n", "label": 1}
{"function": "\n\ndef itemactivated(self, item):\n    if (not item):\n        return\n    for i in range(0, (self.viewport().height() / 5)):\n        if (self.itemAt(QPoint(0, (i * 5))) == item):\n            break\n    else:\n        return\n    for j in range(0, 30):\n        if (self.itemAt(QPoint(0, ((i * 5) + j))) != item):\n            break\n    self.emit(SIGNAL('customContextMenuRequested(const QPoint&)'), QPoint(50, (((i * 5) + j) - 1)))\n", "label": 0}
{"function": "\n\ndef _generate(self):\n    for child in self.children:\n        if isinstance(child, Fragment):\n            for event in child._generate():\n                (yield event)\n        elif isinstance(child, Stream):\n            for event in child:\n                (yield event)\n        else:\n            if (not isinstance(child, basestring)):\n                child = unicode(child)\n            (yield (TEXT, child, (None, (- 1), (- 1))))\n", "label": 0}
{"function": "\n\ndef enqueue(self):\n    now = int(time.time())\n    queue_items = []\n    if (self.amqp_queue_size < 100000):\n        queue_items_a = queue_items.append\n        LOGGER.debug(('%s:%s' % (self.heap[0][0], now)))\n        while ((self.heap[0][0] < now) and (len(queue_items) < 1000)):\n            job = heappop(self.heap)\n            uuid = UUID(bytes=job[1][0])\n            if (not (uuid.hex in self.unscheduled_items)):\n                queue_items_a(job[1][0])\n                new_job = ((now + job[1][1]), job[1])\n                heappush(self.heap, new_job)\n            else:\n                self.unscheduled_items.remove(uuid.hex)\n    else:\n        LOGGER.critical(('AMQP queue is at or beyond max limit (%d/100000)' % self.amqp_queue_size))\n    if queue_items:\n        LOGGER.info(('Found %d new uuids, adding them to the queue' % len(queue_items)))\n        msgs = [Content(uuid) for uuid in queue_items]\n        deferreds = [self.chan.basic_publish(exchange=self.amqp_exchange, content=msg) for msg in msgs]\n        d = DeferredList(deferreds, consumeErrors=True)\n        d.addCallbacks(self._addToQueueComplete, self._addToQueueErr)\n    else:\n        self.enqueueCallLater = reactor.callLater(1, self.enqueue)\n", "label": 0}
{"function": "\n\ndef perform(self, node, inputs, outputs):\n    (x_data, x_indices, x_indptr, x_shape, g_data, g_indices, g_indptr, g_shape) = inputs\n    (g_out,) = outputs\n    if ((len(x_indptr) - 1) == x_shape[0]):\n        sp_dim = x_shape[1]\n    else:\n        sp_dim = x_shape[0]\n    g_row = numpy.zeros(sp_dim, dtype=g_data.dtype)\n    gout_data = numpy.zeros(x_data.shape, dtype=node.outputs[0].dtype)\n    for i in range((len(x_indptr) - 1)):\n        for j_ptr in range(g_indptr[i], g_indptr[(i + 1)]):\n            g_row[g_indices[j_ptr]] += g_data[j_ptr]\n        for j_ptr in range(x_indptr[i], x_indptr[(i + 1)]):\n            gout_data[j_ptr] = g_row[x_indices[j_ptr]]\n        for j_ptr in range(g_indptr[i], g_indptr[(i + 1)]):\n            g_row[g_indices[j_ptr]] = 0\n    if (self.kmap is None):\n        g_out[0] = gout_data\n    else:\n        grad = numpy.zeros_like(x_data)\n        grad[self.kmap] = gout_data\n        g_out[0] = grad\n", "label": 0}
{"function": "\n\ndef _reload_stream(self, key, val):\n    parts = key.split('.', 1)\n    stream_type = ('stdout' if (parts[0] == 'stdout_stream') else 'stderr')\n    old_stream = (self.stream_redirector.get_stream(stream_type) if self.stream_redirector else None)\n    if (stream_type == 'stdout'):\n        self.stdout_stream_conf[parts[1]] = val\n        new_stream = get_stream(self.stdout_stream_conf, reload=True)\n        self.stdout_stream = new_stream\n    else:\n        self.stderr_stream_conf[parts[1]] = val\n        new_stream = get_stream(self.stderr_stream_conf, reload=True)\n        self.stderr_stream = new_stream\n    if self.stream_redirector:\n        self.stream_redirector.change_stream(stream_type, new_stream)\n    else:\n        self.stream_redirector = self._redirector_class(self.stdout_stream, self.stderr_stream, loop=self.loop)\n    if old_stream:\n        if hasattr(old_stream, 'close'):\n            old_stream.close()\n        return 0\n    self.stream_redirector.start()\n    return 1\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return (other and (self.__class__ is other.__class__) and (self.fieldname == other.fieldname) and (self.text == other.text) and (self.minsimilarity == other.minsimilarity) and (self.prefixlength == other.prefixlength) and (self.boost == other.boost))\n", "label": 0}
{"function": "\n\ndef config_for_set(uset, app, defaults=None):\n    '\\n    This is a helper function for `configure_uploads` that extracts the\\n    configuration for a single set.\\n\\n    :param uset: The upload set.\\n    :param app: The app to load the configuration from.\\n    :param defaults: A dict with keys `url` and `dest` from the\\n                     `UPLOADS_DEFAULT_DEST` and `DEFAULT_UPLOADS_URL`\\n                     settings.\\n    '\n    config = app.config\n    prefix = ('UPLOADED_%s_' % uset.name.upper())\n    using_defaults = False\n    if (defaults is None):\n        defaults = dict(dest=None, url=None)\n    allow_extns = tuple(config.get((prefix + 'ALLOW'), ()))\n    deny_extns = tuple(config.get((prefix + 'DENY'), ()))\n    destination = config.get((prefix + 'DEST'))\n    base_url = config.get((prefix + 'URL'))\n    if (destination is None):\n        if uset.default_dest:\n            destination = uset.default_dest(app)\n        if (destination is None):\n            if (defaults['dest'] is not None):\n                using_defaults = True\n                destination = os.path.join(defaults['dest'], uset.name)\n            else:\n                raise RuntimeError(('no destination for set %s' % uset.name))\n    if ((base_url is None) and using_defaults and defaults['url']):\n        base_url = ((addslash(defaults['url']) + uset.name) + '/')\n    return UploadConfiguration(destination, base_url, allow_extns, deny_extns)\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    if (len(args) != 0):\n        raise CommandError(\"Command doesn't accept any arguments\")\n    locale = options.get('locale')\n    domain = options.get('domain')\n    verbosity = int(options.get('verbosity'))\n    process_all = options.get('all')\n    extensions = (options.get('extensions') or ['html'])\n    if (domain == 'djangojs'):\n        extensions = []\n    else:\n        extensions = handle_extensions(extensions)\n    if ('.js' in extensions):\n        raise CommandError(\"JavaScript files should be examined by using the special 'djangojs' domain only.\")\n    if process_all:\n        if os.path.isdir(os.path.join('conf', 'locale')):\n            localedir = os.path.abspath(os.path.join('conf', 'locale'))\n        elif os.path.isdir('locale'):\n            localedir = os.path.abspath('locale')\n        else:\n            raise CommandError('This script should be run from the Transifex project tree.')\n        locale_dirs = filter(os.path.isdir, glob.glob(('%s/*' % localedir)))\n        for locale_dir in locale_dirs:\n            locale = os.path.basename(locale_dir)\n            make_messages(locale, domain, verbosity, False, extensions)\n    else:\n        make_messages(locale, domain, verbosity, process_all, extensions)\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Run the pipeline.'\n\n    def _run_valid(process_valid, run_valid):\n        'Set/maintain the valid state of the run.'\n        if ((not process_valid) and run_valid):\n            return False\n        return run_valid\n    valid = True\n    for processor in self.pipeline:\n        if isinstance(self.data, datatable.DataTable):\n            (_valid, _, self.data) = processor.run(self.data, is_table=True, encoding=self.encoding, decode_strategy=self.decode_strategy)\n        else:\n            (_valid, _, self.data) = processor.run(self.data_source, is_table=False, decode_strategy=self.decode_strategy, encoding=self.encoding, format=self.format)\n        valid = _run_valid(_valid, valid)\n        if ((not valid) and self.break_on_invalid_processor):\n            break\n        if self.data:\n            self.data.replay()\n    self.set_report_meta()\n    if self.post_task:\n        self.post_task(self)\n    return (valid, self.report)\n", "label": 0}
{"function": "\n\ndef drawline(self, start_pos, end_pos, char=' ', fgcolor=None, bgcolor=None):\n    if (fgcolor is None):\n        fgcolor = self._fgcolor\n    else:\n        fgcolor = getpygamecolor(fgcolor)\n    if (bgcolor is None):\n        bgcolor = self._bgcolor\n    else:\n        bgcolor = getpygamecolor(bgcolor)\n    (x0, y0) = start_pos\n    (x1, y1) = end_pos\n    isSteep = (abs((y1 - y0)) > abs((x1 - x0)))\n    if isSteep:\n        (x0, y0) = (y0, x0)\n        (x1, y1) = (y1, x1)\n    if (x0 > x1):\n        (x0, x1) = (x1, x0)\n        (y0, y1) = (y1, y0)\n    if (y0 < y1):\n        ystep = 1\n    else:\n        ystep = (- 1)\n    xdelta = (x1 - x0)\n    ydelta = abs((y1 - y0))\n    error = ((- xdelta) / 2)\n    y = y0\n    for x in range(x0, (x1 + 1)):\n        if isSteep:\n            self.putchar(char, y, x, fgcolor, bgcolor)\n        else:\n            self.putchar(char, x, y, fgcolor, bgcolor)\n        error = (error + ydelta)\n        if (error > 0):\n            y = (y + ystep)\n            error = (error - xdelta)\n", "label": 1}
{"function": "\n\n@spin_first\ndef db_query(self, query, keys=None):\n    \"Query the Hub's TaskRecord database\\n        \\n        This will return a list of task record dicts that match `query`\\n        \\n        Parameters\\n        ----------\\n        \\n        query : mongodb query dict\\n            The search dict. See mongodb query docs for details.\\n        keys : list of strs [optional]\\n            The subset of keys to be returned.  The default is to fetch everything but buffers.\\n            'msg_id' will *always* be included.\\n        \"\n    if isinstance(keys, str):\n        keys = [keys]\n    content = dict(query=query, keys=keys)\n    self.session.send(self._query_socket, 'db_request', content=content)\n    (idents, msg) = self.session.recv(self._query_socket, 0)\n    if self.debug:\n        pprint(msg)\n    content = msg['content']\n    if (content['status'] != 'ok'):\n        raise self._unwrap_exception(content)\n    records = content['records']\n    buffer_lens = content['buffer_lens']\n    result_buffer_lens = content['result_buffer_lens']\n    buffers = msg['buffers']\n    has_bufs = (buffer_lens is not None)\n    has_rbufs = (result_buffer_lens is not None)\n    for (i, rec) in enumerate(records):\n        if has_bufs:\n            blen = buffer_lens[i]\n            (rec['buffers'], buffers) = (buffers[:blen], buffers[blen:])\n        if has_rbufs:\n            blen = result_buffer_lens[i]\n            (rec['result_buffers'], buffers) = (buffers[:blen], buffers[blen:])\n    return records\n", "label": 0}
{"function": "\n\n@grammar(((int, dict) + str_types))\ndef padding(value):\n    \"int or dict : Padding around visualization\\n\\n        The padding defines the distance between the edge of the\\n        visualization canvas to the visualization box. It does not count as\\n        part of the visualization width/height. Values cannot be negative.\\n\\n        If a dict, padding must have all keys ``''top'``, ``'left'``,\\n        ``'right'``, and ``'bottom'`` with int values.\\n        \"\n    if isinstance(value, dict):\n        required_keys = ['top', 'left', 'right', 'bottom']\n        for key in required_keys:\n            if (key not in value):\n                error = 'Padding must have keys \"{0}\".'.format('\", \"'.join(required_keys))\n                raise ValueError(error)\n            _assert_is_type('padding: {0}'.format(key), value[key], int)\n            if (value[key] < 0):\n                raise ValueError('Padding cannot be negative.')\n    elif isinstance(value, int):\n        if (value < 0):\n            raise ValueError('Padding cannot be negative.')\n    elif (value not in ('auto', 'strict')):\n        raise ValueError('Padding can only be auto or strict.')\n", "label": 0}
{"function": "\n\ndef is_git_dir(d):\n    \" This is taken from the git setup.c:is_git_directory\\n    function.\\n\\n    @throws WorkTreeRepositoryUnsupported if it sees a worktree directory. It's quite hacky to do that here,\\n            but at least clearly indicates that we don't support it.\\n            There is the unlikely danger to throw if we see directories which just look like a worktree dir,\\n            but are none.\"\n    if isdir(d):\n        if (isdir(join(d, 'objects')) and isdir(join(d, 'refs'))):\n            headref = join(d, 'HEAD')\n            return (isfile(headref) or (os.path.islink(headref) and os.readlink(headref).startswith('refs')))\n        elif (isfile(join(d, 'gitdir')) and isfile(join(d, 'commondir')) and isfile(join(d, 'gitfile'))):\n            raise WorkTreeRepositoryUnsupported(d)\n    return False\n", "label": 1}
{"function": "\n\ndef filter_queryset(self, request, queryset, view):\n    '\\n        This applies ordering to the result set\\n        Eg: ?order=title\\n\\n        It also supports reverse ordering\\n        Eg: ?order=-title\\n\\n        And random ordering\\n        Eg: ?order=random\\n        '\n    if ('order' in request.GET):\n        if ('search' in request.GET):\n            raise BadRequestError('ordering with a search query is not supported')\n        order_by = request.GET['order']\n        if (order_by == 'random'):\n            if ('offset' in request.GET):\n                raise BadRequestError('random ordering with offset is not supported')\n            return queryset.order_by('?')\n        if order_by.startswith('-'):\n            reverse_order = True\n            order_by = order_by[1:]\n        else:\n            reverse_order = False\n        if ((order_by == 'id') or (order_by in view.get_api_fields(queryset.model))):\n            queryset = queryset.order_by(order_by)\n        else:\n            raise BadRequestError((\"cannot order by '%s' (unknown field)\" % order_by))\n        if reverse_order:\n            queryset = queryset.reverse()\n    return queryset\n", "label": 1}
{"function": "\n\ndef serve_mogilefs_file(request, key=None):\n    '\\n    Called when a user requests an image.\\n    Either reproxy the path to perlbal, or serve the image outright\\n    '\n    mimetype = (mimetypes.guess_type(key)[0] or 'application/x-octet-stream')\n    client = mogilefs.Client(settings.MOGILEFS_DOMAIN, settings.MOGILEFS_TRACKERS)\n    if (hasattr(settings, 'SERVE_WITH_PERLBAL') and settings.SERVE_WITH_PERLBAL):\n        path = cache.get(key)\n        if (not path):\n            path = client.get_paths(key)\n            cache.set(key, path, 60)\n        if path:\n            response = HttpResponse(content_type=mimetype)\n            response['X-REPROXY-URL'] = path[0]\n        else:\n            response = HttpResponseNotFound()\n    else:\n        file_data = client[key]\n        if file_data:\n            response = HttpResponse(file_data, mimetype=mimetype)\n        else:\n            response = HttpResponseNotFound()\n    return response\n", "label": 0}
{"function": "\n\ndef execute(self, ns, package, _repoid=None, _installed=False):\n\n    def _render_installed(package):\n        result = 'No'\n        if ((software.get_backend(ns) == software.BACKEND_YUM) and (package.InstallDate is not None)):\n            result = package.InstallDate.datetime.strftime('%a %b %d/%Y  %H:%M')\n        elif ((software.get_backend(ns) == software.BACKEND_PACKAGEKIT) and software.is_package_installed(package)):\n            result = 'Yes'\n        return result\n    properties = ['Name', ('Arch', 'Architecture'), 'Version', 'Release', ('Summary', 'Caption'), ('Installed', _render_installed), 'Description']\n    pkgs = [p.to_instance() for p in software.find_package(ns, pkg_spec=package, repoid=_repoid)]\n    pkgs = [p for p in pkgs if ((not _installed) or software.is_package_installed(p))]\n    if (len(pkgs) < 1):\n        raise errors.LmiFailed(('No such package \"%s\" found.' % package))\n    if (len(pkgs) > 1):\n        LOG().warn('More than one package found for \"%s\" : %s', package, ', '.join((p.ElementName for p in pkgs)))\n    return (properties, pkgs[(- 1)])\n", "label": 0}
{"function": "\n\ndef format_multimatches(self, caller, matches):\n    '\\n        Format multiple command matches to a useful error.\\n\\n        This is copied directly from the default method in\\n        evennia.commands.cmdhandler.\\n\\n        '\n    string = 'There were multiple matches:'\n    for (num, match) in enumerate(matches):\n        (candidate, cmd) = match\n        is_channel = (hasattr(cmd, 'is_channel') and cmd.is_channel)\n        if is_channel:\n            is_channel = ' (channel)'\n        else:\n            is_channel = ''\n        is_exit = (hasattr(cmd, 'is_exit') and cmd.is_exit)\n        if (is_exit and cmd.destination):\n            is_exit = (' (exit to %s)' % cmd.destination)\n        else:\n            is_exit = ''\n        id1 = ''\n        id2 = ''\n        if ((not (is_channel or is_exit)) and (hasattr(cmd, 'obj') and (cmd.obj != caller))):\n            id1 = ('%s-' % cmd.obj.name)\n            id2 = (' (%s-%s)' % ((num + 1), candidate.cmdname))\n        else:\n            id1 = ('%s-' % (num + 1))\n            id2 = ''\n        string += ('\\n  %s%s%s%s%s' % (id1, candidate.cmdname, id2, is_channel, is_exit))\n    return string\n", "label": 1}
{"function": "\n\ndef main():\n    args = parse_command_line()\n    plan = []\n\n    def add_to_plan(plan, component_name, build_f, source_directory):\n        plan.append((component_name, build_f, source_directory))\n    add_to_plan(plan, 'icu', build_icu, args.with_icu_sources)\n    add_to_plan(plan, 'qt', build_qt, args.with_qt_sources)\n    add_to_plan(plan, 'sip', build_sip, args.with_sip_sources)\n    add_to_plan(plan, 'pyqt', build_pyqt, args.with_pyqt_sources)\n    if (args.packages != 'all'):\n        plan = [entry for entry in plan if (entry[0] in args.packages)]\n    layout = sdk.get_layout(sdk.platform_root(args.install_root))\n    prep(layout)\n    if args.only_merge:\n        merge(layout)\n        return\n    if args.shell:\n        sdk.start_subshell()\n        return\n    if args.only_scripts:\n        install_scripts(args.install_root)\n        return\n    build(plan, layout, args.debug, args.profile)\n    merge(layout)\n    install_scripts(args.install_root)\n", "label": 0}
{"function": "\n\ndef poll(self):\n    self._logger.debug('Requesting list of deployments')\n    deployments = self._get_deployments()\n    if (deployments is None):\n        self._logger.info('No deployments found')\n        return\n    if (len(deployments) is 0):\n        self._logger.info('Empty list of deployments')\n        return\n    last_deployment = deployments[0]\n    last_assembled_date = self._to_date(last_deployment['assembled'])\n    index_date = self._get_last_date()\n    self._logger.debug(('Index date is %s' % index_date))\n    if (index_date is None):\n        self._logger.info('Initializing index')\n        self._set_last_date(last_assembled_date)\n        index_date = self._get_last_date()\n    if (last_assembled_date > index_date):\n        self._logger.info('Found deployments to trigger')\n        for deployment in deployments:\n            if (self._to_date(deployment['assembled']) > index_date):\n                self._logger.info(('Raising trigger for %s' % deployment['id']))\n                self._dispatch_trigger_for_payload(deployment)\n            else:\n                break\n        self._set_last_date(last_assembled_date)\n    else:\n        self._logger.debug(('No new deployments comparing %s' % time.strftime('%Y-%m-%dT%H:%M:%S', last_assembled_date)))\n        self._logger.debug(('and %s' % time.strftime('%Y-%m-%dT%H:%M:%S', index_date)))\n", "label": 0}
{"function": "\n\ndef write(self, data):\n    'Output the given string over the serial port.'\n    if (not self.hComPort):\n        raise portNotOpenError\n    data = bytes(data)\n    if data:\n        n = win32.DWORD()\n        err = win32.WriteFile(self.hComPort, data, len(data), ctypes.byref(n), self._overlappedWrite)\n        if ((not err) and (win32.GetLastError() != win32.ERROR_IO_PENDING)):\n            raise SerialException(('WriteFile failed (%s)' % ctypes.WinError()))\n        if (self._writeTimeout != 0):\n            err = win32.GetOverlappedResult(self.hComPort, self._overlappedWrite, ctypes.byref(n), True)\n            if (n.value != len(data)):\n                raise writeTimeoutError\n        return n.value\n    else:\n        return 0\n", "label": 0}
{"function": "\n\ndef _reload(mod, _larch=None, **kws):\n    'reload a module, either larch or python'\n    if (_larch is None):\n        raise Warning((\"cannot reload module '%s' -- larch broken?\" % mod))\n    modname = None\n    if (mod in _larch.symtable._sys.modules.values()):\n        for (k, v) in _larch.symtable._sys.modules.items():\n            if (v == mod):\n                modname = k\n    elif (mod in sys.modules.values()):\n        for (k, v) in sys.modules.items():\n            if (v == mod):\n                modname = k\n    elif ((mod in _larch.symtable._sys.modules.keys()) or (mod in sys.modules.keys())):\n        modname = mod\n    if (modname is not None):\n        return _larch.import_module(modname, do_reload=True)\n", "label": 1}
{"function": "\n\ndef write(self):\n    q = {\n        \n    }\n    if (self._host_name is not None):\n        Util.set_by_path(q, 'HostName', self._host_name)\n    if (self._password is not None):\n        Util.set_by_path(q, 'Password', self._password)\n    if (len(self._ssh_keys) > 0):\n        Util.set_by_path(q, 'SSHKey.PublicKey', '\\n'.join(self._ssh_keys))\n    if (self._ip_address is not None):\n        Util.set_by_path(q, 'UserIPAddress', self._ip_address)\n    if (self._default_route is not None):\n        Util.set_by_path(q, 'UserSubnet.DefaultRoute', self._default_route)\n    if (self._network_mask_len is not None):\n        Util.set_by_path(q, 'UserSubnet.NetworkMaskLen', self._network_mask_len)\n    if (0 < len(self._scripts)):\n        notes = []\n        for script in self._scripts:\n            notes.append({\n                'ID': script._id(),\n            })\n        Util.set_by_path(q, 'Notes', notes)\n    path = (('/disk/' + self._disk_id) + '/config')\n    self._client.request('PUT', path, q)\n    return self\n", "label": 1}
{"function": "\n\ndef save_instance(form, instance, fields=None, fail_message='saved', commit=True, exclude=None):\n    if form.errors:\n        raise ValueError((\"The %s could not be %s because the data didn't validate.\" % ('object', fail_message)))\n    cleaned_data = form.cleaned_data\n    for (field_name, field_type) in instance.structure.items():\n        if (fields and (field_name not in fields)):\n            continue\n        if (exclude and (field_name in exclude)):\n            continue\n        instance[field_name] = cleaned_data[field_name]\n    if commit:\n        instance.save(validate=True)\n    return instance\n", "label": 0}
{"function": "\n\ndef validate_absolute_path(self, root, absolute_path):\n    \"Validate and return the absolute path.\\n\\n        ``root`` is the configured path for the `StaticFileHandler`,\\n        and ``path`` is the result of `get_absolute_path`\\n\\n        This is an instance method called during request processing,\\n        so it may raise `HTTPError` or use methods like\\n        `RequestHandler.redirect` (return None after redirecting to\\n        halt further processing).  This is where 404 errors for missing files\\n        are generated.\\n\\n        This method may modify the path before returning it, but note that\\n        any such modifications will not be understood by `make_static_url`.\\n\\n        In instance methods, this method's result is available as\\n        ``self.absolute_path``.\\n\\n        .. versionadded:: 3.1\\n        \"\n    root = os.path.abspath(root)\n    if (not (absolute_path + os.path.sep).startswith(root)):\n        raise HTTPError(403, '%s is not in root static directory', self.path)\n    if (os.path.isdir(absolute_path) and (self.default_filename is not None)):\n        if (not self.request.path.endswith('/')):\n            self.redirect((self.request.path + '/'), permanent=True)\n            return\n        absolute_path = os.path.join(absolute_path, self.default_filename)\n    if (not os.path.exists(absolute_path)):\n        raise HTTPError(404)\n    if (not os.path.isfile(absolute_path)):\n        raise HTTPError(403, '%s is not a file', self.path)\n    return absolute_path\n", "label": 0}
{"function": "\n\ndef _load_registers(self):\n    registers_all = (((((self.regs_seg + self.regs_fpu) + self.regs_mmx) + self.regs_flags) + self.regs_debug) + self.regs_control)\n    if (self._arch_mode == ARCH_X86_MODE_32):\n        registers_all += ((self.regs_32 + self.regs_xmm_32) + self.regs_ymm_32)\n        registers_gp_all = self.regs_32\n        self._registers_gp_base = [name for (name, _) in self.regs_32_base]\n    else:\n        registers_all += ((self.regs_64 + self.regs_xmm_64) + self.regs_ymm_64)\n        registers_gp_all = self.regs_64\n        self._registers_gp_base = [name for (name, _) in self.regs_64_base]\n    for (name, size) in registers_all:\n        self._registers_all.append(name)\n        self._registers_size[name] = size\n    for (name, size) in registers_gp_all:\n        self._registers_gp_all.append(name)\n        self._registers_size[name] = size\n    self._registers_flags = [name for (name, _) in self.regs_flags]\n", "label": 0}
{"function": "\n\ndef _dmp_rr_trivial_gcd(f, g, u, K):\n    'Handle trivial cases in GCD algorithm over a ring. '\n    zero_f = dmp_zero_p(f, u)\n    zero_g = dmp_zero_p(g, u)\n    if_contain_one = (dmp_one_p(f, u, K) or dmp_one_p(g, u, K))\n    if (zero_f and zero_g):\n        return tuple(dmp_zeros(3, u, K))\n    elif zero_f:\n        if K.is_nonnegative(dmp_ground_LC(g, u, K)):\n            return (g, dmp_zero(u), dmp_one(u, K))\n        else:\n            return (dmp_neg(g, u, K), dmp_zero(u), dmp_ground((- K.one), u))\n    elif zero_g:\n        if K.is_nonnegative(dmp_ground_LC(f, u, K)):\n            return (f, dmp_one(u, K), dmp_zero(u))\n        else:\n            return (dmp_neg(f, u, K), dmp_ground((- K.one), u), dmp_zero(u))\n    elif if_contain_one:\n        return (dmp_one(u, K), f, g)\n    elif query('USE_SIMPLIFY_GCD'):\n        return _dmp_simplify_gcd(f, g, u, K)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef load_stdlib():\n    'Scans sys.path for standard library modules.\\n    '\n    if _stdlib:\n        return _stdlib\n    prefixes = tuple({os.path.abspath(p) for p in (sys.prefix, getattr(sys, 'real_prefix', sys.prefix), getattr(sys, 'base_prefix', sys.prefix))})\n    for sp in sys.path:\n        if (not sp):\n            continue\n        _import_paths.append(os.path.abspath(sp))\n    stdpaths = tuple({p for p in _import_paths if (p.startswith(prefixes) and ('site-packages' not in p))})\n    _stdlib.update(sys.builtin_module_names)\n    for stdpath in stdpaths:\n        if (not os.path.isdir(stdpath)):\n            continue\n        for item in os.listdir(stdpath):\n            if (item.startswith('.') or (item == 'site-packages')):\n                continue\n            p = os.path.join(stdpath, item)\n            if ((not os.path.isdir(p)) and (not item.endswith(('.py', '.so')))):\n                continue\n            _stdlib.add(item.split('.', 1)[0])\n    return _stdlib\n", "label": 1}
{"function": "\n\ndef loadConfig(configFilename, host=None, port=None, basepath=None, logpath=None):\n    result = Config()\n    result.fromJson(configFilename)\n    if ((host is not None) and ('host' not in result)):\n        result.host = host\n    if ((port is not None) and ('port' not in result)):\n        result.port = port\n    if ((basepath is not None) and ('basepath' not in result)):\n        result.basepath = basepath\n    if ((logpath is not None) and ('logpath' not in result)):\n        result.logpath = logpath\n    if ('auth_timeout' not in result):\n        result.auth_timeout = 300\n    if ('require_vouch' not in result):\n        result.require_vouch = False\n    return result\n", "label": 1}
{"function": "\n\ndef _wait_for_bot_presense(self, online):\n    for _ in range(10):\n        time.sleep(2)\n        if (online and self._is_testbot_online()):\n            break\n        if ((not online) and (not self._is_testbot_online())):\n            break\n    else:\n        raise AssertionError('test bot is still {}'.format(('offline' if online else 'online')))\n", "label": 0}
{"function": "\n\ndef tick(self, e):\n    if ((e in key_directions) and ((abs(key_directions[e][0]) + abs(self.direction[0])) < 2) and ((abs(key_directions[e][1]) + abs(self.direction[1])) < 2)):\n        self.direction = key_directions[e]\n    self.advance_snake()\n    if (self.snake_parts[0] == self.apple):\n        self.new_apple()\n    elif ((not ((0 <= self.snake_parts[0][0] < self.height) and (0 <= self.snake_parts[0][1] < self.width))) or (self.snake_parts[0] in self.snake_parts[1:])):\n        return True\n    else:\n        self.snake_parts.pop()\n", "label": 0}
{"function": "\n\ndef transform_first_chunk(self, status_code, headers, chunk, finishing):\n    if ('Vary' in headers):\n        headers['Vary'] += b', Accept-Encoding'\n    else:\n        headers['Vary'] = b'Accept-Encoding'\n    if self._gzipping:\n        ctype = _unicode(headers.get('Content-Type', '')).split(';')[0]\n        self._gzipping = (self._compressible_type(ctype) and ((not finishing) or (len(chunk) >= self.MIN_LENGTH)) and ('Content-Encoding' not in headers))\n    if self._gzipping:\n        headers['Content-Encoding'] = 'gzip'\n        self._gzip_value = BytesIO()\n        self._gzip_file = gzip.GzipFile(mode='w', fileobj=self._gzip_value)\n        chunk = self.transform_chunk(chunk, finishing)\n        if ('Content-Length' in headers):\n            if finishing:\n                headers['Content-Length'] = str(len(chunk))\n            else:\n                del headers['Content-Length']\n    return (status_code, headers, chunk)\n", "label": 1}
{"function": "\n\ndef _meijerint_definite_3(f, x):\n    '\\n    Try to integrate f dx from zero to infinity.\\n\\n    This function calls _meijerint_definite_4 to try to compute the\\n    integral. If this fails, it tries using linearity.\\n    '\n    res = _meijerint_definite_4(f, x)\n    if (res and (res[1] != False)):\n        return res\n    if f.is_Add:\n        _debug('Expanding and evaluating all terms.')\n        ress = [_meijerint_definite_4(g, x) for g in f.args]\n        if all(((r is not None) for r in ress)):\n            conds = []\n            res = S(0)\n            for (r, c) in ress:\n                res += r\n                conds += [c]\n            c = And(*conds)\n            if (c != False):\n                return (res, c)\n", "label": 1}
{"function": "\n\ndef validate_b2_file_name(name):\n    '\\n    Raises a ValueError if the name is not a valid B2 file name.\\n\\n    :param name: a string\\n    :return: None\\n    '\n    if (not isinstance(name, six.string_types)):\n        raise ValueError('file name must be a string, not bytes')\n    name_utf8 = name.encode('utf-8')\n    if (len(name_utf8) < 1):\n        raise ValueError('file name too short (0 utf-8 bytes)')\n    if (1000 < len(name_utf8)):\n        raise ValueError('file name too long (more than 1000 utf-8 bytes)')\n    if (name[0] == '/'):\n        raise ValueError(\"file names must not start with '/'\")\n    if (name[(- 1)] == '/'):\n        raise ValueError(\"file names must not end with '/'\")\n    if ('\\\\' in name):\n        raise ValueError(\"file names must not contain '\\\\'\")\n    if ('//' in name):\n        raise ValueError(\"file names must not contain '//'\")\n    if (chr(127) in name):\n        raise ValueError('file names must not contain DEL')\n    if any(((250 < len(segment)) for segment in name_utf8.split(six.b('/')))):\n        raise ValueError(\"file names segments (between '/') can be at most 250 utf-8 bytes\")\n", "label": 1}
{"function": "\n\ndef clear_attachments(schema_or_doc):\n    if (schema_or_doc and ('_attachments' in schema_or_doc)):\n        del schema_or_doc['_attachments']\n    if (schema_or_doc and ('case_attachments' in schema_or_doc)):\n        del schema_or_doc['case_attachments']\n    if schema_or_doc:\n        for action in schema_or_doc.get('actions', []):\n            if (('attachments' in action) and ('updated_unknown_properties' in action)):\n                del action['attachments']\n    return schema_or_doc\n", "label": 1}
{"function": "\n\ndef etree_to_dict(etree, namespace=None, tag_list=True, convert_camelcase=False):\n    '\\n    Convert an etree to a dict.\\n   \\n    **Keyword arguments:**\\n     * *namespace* -- XML Namespace to be removed from tag names (Default None)\\n    '\n    children = etree.getchildren()\n    if (len(children) == 0):\n        return etree.text\n    children_dict = {\n        \n    }\n    for element in children:\n        tag = element.tag\n        if (namespace is not None):\n            tag = tag.replace(namespace, '')\n        if convert_camelcase:\n            tag = camelcase_to_underscore(tag)\n        element_dict = etree_to_dict(element, namespace=namespace, tag_list=tag_list, convert_camelcase=convert_camelcase)\n        if (tag in children_dict):\n            if (not isinstance(children_dict[tag], list)):\n                children_dict[tag] = [children_dict[tag]]\n            children_dict[tag].append(element_dict)\n        elif tag_list:\n            children_dict[tag] = [element_dict]\n        else:\n            children_dict[tag] = element_dict\n    return children_dict\n", "label": 0}
{"function": "\n\ndef parse_message(message):\n    ' Parses the payload by delegating to specialized functions. '\n    events = []\n    if (message['action'] in ['create', 'delete']):\n        events.append(parse_create_or_delete(message))\n    elif (message['action'] == 'change'):\n        if message['change']['diff']:\n            for value in message['change']['diff']:\n                parsed_event = parse_change_event(value, message)\n                if parsed_event:\n                    events.append(parsed_event)\n        if message['change']['comment']:\n            events.append(parse_comment(message))\n    return events\n", "label": 0}
{"function": "\n\ndef _deployment_watch(deployment_id, max_count, interval):\n    '\\n    :param deployment_id: the application id\\n    :type deployment_di: str\\n    :param max_count: maximum number of polling calls\\n    :type max_count: str\\n    :param interval: wait interval in seconds between polling calls\\n    :type interval: str\\n    :returns: process return code\\n    :rtype: int\\n    '\n    if (max_count is not None):\n        max_count = util.parse_int(max_count)\n    interval = (1 if (interval is None) else util.parse_int(interval))\n    client = marathon.create_client()\n    count = 0\n    while ((max_count is None) or (count < max_count)):\n        deployment = client.get_deployment(deployment_id)\n        if (deployment is None):\n            return 0\n        if util.is_windows_platform():\n            os.system('cls')\n        elif ('TERM' in os.environ):\n            os.system('clear')\n        emitter.publish('Deployment update time: {} \\n'.format(time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())))\n        emitter.publish(deployment)\n        time.sleep(interval)\n        count += 1\n    return 0\n", "label": 0}
{"function": "\n\ndef get(self, n):\n    'Get n-gram.'\n    if self.capitalword:\n        return\n    if ((n < 1) or (n > self.N_GRAM) or (len(self.grams) < n)):\n        return\n    if (n == 1):\n        ch = self.grams[(- 1)]\n        if (ch == ' '):\n            return\n        return ch\n    else:\n        return self.grams[(- n):]\n", "label": 0}
{"function": "\n\ndef _SetValues(self, values):\n    \"Set values from supplied dictionary or list.\\n\\n    Args:\\n      values: A Row, dict indexed by column name, or list.\\n\\n    Raises:\\n      TypeError: Argument is not a list or dict, or list is not equal row\\n      length or dictionary keys don't match.\\n    \"\n\n    def _ToStr(value):\n        'Convert individul list entries to string.'\n        if isinstance(value, (list, tuple)):\n            result = []\n            for val in value:\n                result.append(str(val))\n            return result\n        else:\n            return str(value)\n    if isinstance(values, Row):\n        if (self._keys != values.header):\n            raise TypeError('Attempt to append row with mismatched header.')\n        self._values = copy.deepcopy(values.values)\n    elif isinstance(values, dict):\n        for key in self._keys:\n            if (key not in values):\n                raise TypeError('Dictionary key mismatch with row.')\n        for key in self._keys:\n            self[key] = _ToStr(values[key])\n    elif (isinstance(values, list) or isinstance(values, tuple)):\n        if (len(values) != len(self._values)):\n            raise TypeError('Supplied list length != row length')\n        for (index, value) in enumerate(values):\n            self._values[index] = _ToStr(value)\n    else:\n        raise TypeError('Supplied argument must be Row, dict or list, not %s', type(values))\n", "label": 1}
{"function": "\n\ndef find_dropped_items(self, wanted=None, near=None):\n    '\\n        If ``near`` is a Vector3, the items are sorted by distance to ``near``.\\n        '\n    items = []\n    for item in self.entities.objects:\n        slot = slot_from_item(item)\n        if (not slot):\n            continue\n        if near:\n            items.append((item, slot))\n        else:\n            (yield (item, slot))\n    if near:\n        dist = near.dist_sq\n        for (d, item, slot) in sorted(((dist(Vec(i)), i, s) for (i, s) in items)):\n            (yield (item, slot))\n", "label": 0}
{"function": "\n\ndef form_fields(self):\n    '\\n        Return fields of default form.\\n\\n        Fill some fields with reasonable values.\\n        '\n    fields = dict(self.form.fields)\n    for elem in self.form.inputs:\n        if (not elem.get('name')):\n            continue\n        if elem.get('disabled'):\n            if (elem.name in fields):\n                del fields[elem.name]\n        elif (elem.tag == 'select'):\n            if (fields[elem.name] is None):\n                if len(elem.value_options):\n                    fields[elem.name] = elem.value_options[0]\n        elif (getattr(elem, 'type', None) == 'radio'):\n            if (fields[elem.name] is None):\n                fields[elem.name] = elem.get('value')\n        elif (getattr(elem, 'type', None) == 'checkbox'):\n            if (not elem.checked):\n                if (elem.name is not None):\n                    if (elem.name in fields):\n                        del fields[elem.name]\n    return fields\n", "label": 1}
{"function": "\n\ndef _refresh(self):\n    'Refreshes the cursor with more data from Mongo.\\n\\n        Returns the length of self.__data after refresh. Will exit early if\\n        self.__data is already non-empty. Raises OperationFailure when the\\n        cursor cannot be refreshed due to an error on the query.\\n        '\n    if (len(self.__data) or self.__killed):\n        return len(self.__data)\n    if (self.__id is None):\n        ntoreturn = self.__batch_size\n        if self.__limit:\n            if self.__batch_size:\n                ntoreturn = min(self.__limit, self.__batch_size)\n            else:\n                ntoreturn = self.__limit\n        self.__send_message(message.query(self.__query_options(), self.__collection.full_name, self.__skip, ntoreturn, self.__query_spec(), self.__fields, self.__uuid_subtype))\n        if (not self.__id):\n            self.__killed = True\n    elif self.__id:\n        if self.__limit:\n            limit = (self.__limit - self.__retrieved)\n            if self.__batch_size:\n                limit = min(limit, self.__batch_size)\n        else:\n            limit = self.__batch_size\n        if self.__exhaust:\n            self.__send_message(None)\n        else:\n            self.__send_message(message.get_more(self.__collection.full_name, limit, self.__id))\n    else:\n        self.__killed = True\n    return len(self.__data)\n", "label": 1}
{"function": "\n\ndef _handle_bmp_session(self, socket):\n    self._socket = socket\n    init_info = {\n        'type': bmp.BMP_INIT_TYPE_STRING,\n        'value': 'This is Ryu BGP BMP message',\n    }\n    init_msg = bmp.BMPInitiation([init_info])\n    self._send(init_msg)\n    peer_manager = self._core_service.peer_manager\n    for peer in (p for p in peer_manager.iterpeers if p.in_established()):\n        msg = self._construct_peer_up_notification(peer)\n        self._send(msg)\n        for path in peer._adj_rib_in.values():\n            msg = self._construct_route_monitoring(peer, path)\n            self._send(msg)\n    while True:\n        ret = self._socket.recv(1)\n        if (len(ret) == 0):\n            LOG.debug('BMP socket is closed. retry connecting..')\n            self._socket = None\n            self._connect_retry_event.set()\n            break\n", "label": 0}
{"function": "\n\ndef _iter_rows(self):\n    model_meta = self.model_class._meta\n    if self._validate_fields:\n        valid_fields = (set(model_meta.fields.keys()) | set(model_meta.fields.values()))\n\n        def validate_field(field):\n            if (field not in valid_fields):\n                raise KeyError(('\"%s\" is not a recognized field.' % field))\n    defaults = model_meta._default_dict\n    callables = model_meta._default_callables\n    for row_dict in self._rows:\n        field_row = defaults.copy()\n        seen = set()\n        for key in row_dict:\n            if self._validate_fields:\n                validate_field(key)\n            if (key in model_meta.fields):\n                field = model_meta.fields[key]\n            else:\n                field = key\n            field_row[field] = row_dict[key]\n            seen.add(field)\n        if callables:\n            for field in callables:\n                if (field not in seen):\n                    field_row[field] = callables[field]()\n        (yield field_row)\n", "label": 1}
{"function": "\n\ndef virtual_memory():\n    (total, free, buffers, shared, _, _) = _psutil_linux.get_sysinfo()\n    cached = active = inactive = None\n    f = open('/proc/meminfo', 'r')\n    try:\n        for line in f:\n            if line.startswith('Cached:'):\n                cached = (int(line.split()[1]) * 1024)\n            elif line.startswith('Active:'):\n                active = (int(line.split()[1]) * 1024)\n            elif line.startswith('Inactive:'):\n                inactive = (int(line.split()[1]) * 1024)\n            if ((cached is not None) and (active is not None) and (inactive is not None)):\n                break\n        else:\n            raise RuntimeError('line(s) not found')\n    finally:\n        f.close()\n    avail = ((free + buffers) + cached)\n    used = (total - free)\n    percent = usage_percent((total - avail), total, _round=1)\n    return nt_virtmem_info(total, avail, percent, used, free, active, inactive, buffers, cached)\n", "label": 1}
{"function": "\n\ndef do376(self, irc, msg):\n    if self.disabled(irc):\n        return\n    nick = self._getNick(irc.network)\n    if (nick not in self.registryValue('nicks')):\n        return\n    nickserv = self.registryValue('NickServ')\n    if (not nickserv):\n        self.log.warning('NickServ is unset, cannot identify.')\n        return\n    password = self._getNickServPassword(nick)\n    if (not password):\n        self.log.warning('Password for %s is unset, cannot identify.', nick)\n        return\n    if (not nick):\n        self.log.warning('Cannot identify without a nick being set.  Set supybot.plugins.Services.nick.')\n        return\n    if ircutils.strEqual(irc.nick, nick):\n        self._doIdentify(irc)\n    else:\n        self._doGhost(irc)\n", "label": 0}
{"function": "\n\ndef annotations(self, imageset=None, classes=None, nodifficult=False):\n    if (imageset is None):\n        imageset = self.imageset()\n    elif isinstance(imageset, str):\n        imageset = self.imageset(imageset)\n    for file in imageset:\n        logger.info('Reading {0}'.format(file))\n        file = os.path.join(self.root, 'Annotations', '{0}.xml'.format(file))\n        if (not file.endswith('.xml')):\n            continue\n        tree = ElementTree.parse(file)\n        filename = tree.find('filename').text.strip()\n        for object in tree.findall('object'):\n            label = object.find('name').text.strip()\n            if (classes and (label not in classes)):\n                continue\n            if nodifficult:\n                difficult = object.find('difficult').text\n                difficult = bool(int(difficult))\n                if difficult:\n                    continue\n            xtl = int(object.find('bndbox/xmin').text)\n            ytl = int(object.find('bndbox/ymin').text)\n            xbr = int(object.find('bndbox/xmax').text)\n            ybr = int(object.find('bndbox/ymax').text)\n            (yield Box(xtl, ytl, xbr, ybr, label=label, image=filename))\n", "label": 1}
{"function": "\n\n@api.route('/login', methods=['POST'])\ndef login():\n    if current_user.is_authenticated():\n        return jsonify(flag='success')\n    username = request.form.get('username')\n    password = request.form.get('password')\n    if (username and password):\n        (user, authenticated) = User.authenticate(username, password)\n        if (user and authenticated):\n            if login_user(user, remember='y'):\n                return jsonify(flag='success')\n    current_app.logger.debug(('login(api) failed, username: %s.' % username))\n    return jsonify(flag='fail', msg='Sorry, try again.')\n", "label": 0}
{"function": "\n\ndef dump_message(self, msg, exclusion_list=None, local_only=True, cap=None, exclude_cap=None):\n    if (not exclusion_list):\n        exclusion_list = list()\n    if local_only:\n        ctx = get_context()\n    for m in self.members:\n        if (m.client in exclusion_list):\n            continue\n        if (local_only and (m.client.servername != ctx.conf.name)):\n            continue\n        if (cap and (cap not in m.client.caps)):\n            continue\n        if (exclude_cap and (exclude_cap in m.client.caps)):\n            continue\n        m.client.dump_message(msg)\n", "label": 1}
{"function": "\n\ndef range_validator(self, column):\n    '\\n        Returns range validator based on column type and column info min and\\n        max arguments\\n\\n        :param column: SQLAlchemy Column object\\n        '\n    min_ = column.info.get('min')\n    max_ = column.info.get('max')\n    if ((min_ is not None) or (max_ is not None)):\n        if (is_number(column.type) or is_number_range(column.type)):\n            return self.get_validator('number_range', min=min_, max=max_)\n        elif is_date_column(column):\n            return self.get_validator('date_range', min=min_, max=max_)\n        elif isinstance(column.type, sa.types.Time):\n            return self.get_validator('time_range', min=min_, max=max_)\n", "label": 0}
{"function": "\n\ndef _parse_long(self):\n    buf = []\n    self.delimiter = self.c\n    self.consume()\n    if (self.c == EOF):\n        return ['', '', '', '']\n    buf.append(self._match_pattern())\n    if (self.c != EOF):\n        self.consume()\n        buf.append(self._match_pattern())\n    else:\n        buf.append('')\n    if (self.c != EOF):\n        self.consume()\n    if ((self.c != EOF) and (self.c in self.FLAG)):\n        buf.append(self._match_flags())\n    else:\n        buf.append('')\n    if (self.c != EOF):\n        self._match_white_space()\n        buf.append(self._match_count())\n    else:\n        buf.append('')\n    self._match_white_space()\n    if (self.c != EOF):\n        raise SyntaxError('Trailing characters.')\n    return buf\n", "label": 0}
{"function": "\n\ndef can_view_app(req, dom):\n    if ((not dom) or (not dom.is_snapshot) or (not dom.published)):\n        return False\n    if ((not dom.is_approved) and ((not getattr(req, 'couch_user', '')) or (not req.couch_user.is_domain_admin(dom.copied_from.name)))):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef __getattr__(self, attrname):\n    'Dynamically get a subproperty.'\n    prop = self._modelclass._properties.get(attrname)\n    if ((prop is None) or (prop._code_name != attrname)):\n        for prop in self._modelclass._properties.values():\n            if (prop._code_name == attrname):\n                break\n        else:\n            prop = None\n    if (prop is None):\n        raise AttributeError(('Model subclass %s has no attribute %s' % (self._modelclass.__name__, attrname)))\n    prop_copy = copy.copy(prop)\n    prop_copy._name = ((self._name + '.') + prop_copy._name)\n    setattr(self, attrname, prop_copy)\n    return prop_copy\n", "label": 0}
{"function": "\n\n@transform(buildLCA, suffix('.taxa.gz'), '.taxa.readcounts')\ndef countContributingReads(infile, outfile):\n    '\\n    count number of reads with a taxnomic assignment\\n    '\n    levels = ['phylum', 'class', 'order', 'family', 'genus', 'species']\n    result = collections.OrderedDict()\n    for level in levels:\n        result[level] = 0\n    inf = IOTools.openFile(infile)\n    header = inf.readline().split('\\t')\n    indices = [3, 5, 7, 9, 11, 13]\n    total = 0\n    for line in inf.readlines():\n        total += 1\n        data = line[:(- 1)].split('\\t')\n        (phylum, _class, order, family, genus, species) = [data[i] for i in indices]\n        if (phylum != 'NA'):\n            result['phylum'] += 1\n        if (_class != 'NA'):\n            result['class'] += 1\n        if (order != 'NA'):\n            result['order'] += 1\n        if (family != 'NA'):\n            result['family'] += 1\n        if (genus != 'NA'):\n            result['genus'] += 1\n        if (species != 'NA'):\n            result['species'] += 1\n    outf = open(outfile, 'w')\n    outf.write('level\\tn_reads\\tpct_reads\\n')\n    for (level, count) in result.iteritems():\n        (nreads, prop) = (count, (float(count) / total))\n        outf.write(('\\t'.join([level, str(nreads), str((prop * 100))]) + '\\n'))\n    outf.close()\n", "label": 1}
{"function": "\n\ndef extract_docstring(self):\n    ' Extract a module-level docstring\\n        '\n    lines = open(self.filename).readlines()\n    start_row = 0\n    if lines[0].startswith('#!'):\n        lines.pop(0)\n        start_row = 1\n    docstring = ''\n    first_par = ''\n    tokens = tokenize.generate_tokens(lines.__iter__().next)\n    for (tok_type, tok_content, _, (erow, _), _) in tokens:\n        tok_type = token.tok_name[tok_type]\n        if (tok_type in ('NEWLINE', 'COMMENT', 'NL', 'INDENT', 'DEDENT')):\n            continue\n        elif (tok_type == 'STRING'):\n            docstring = eval(tok_content)\n            paragraphs = '\\n'.join((line.rstrip() for line in docstring.split('\\n'))).split('\\n\\n')\n            if (len(paragraphs) > 0):\n                first_par = paragraphs[0]\n        break\n    thumbloc = None\n    for (i, line) in enumerate(docstring.split('\\n')):\n        m = re.match('^_thumb: (\\\\.\\\\d+),\\\\s*(\\\\.\\\\d+)', line)\n        if m:\n            thumbloc = (float(m.group(1)), float(m.group(2)))\n            break\n    if (thumbloc is not None):\n        self.thumbloc = thumbloc\n        docstring = '\\n'.join([l for l in docstring.split('\\n') if (not l.startswith('_thumb'))])\n    self.docstring = docstring\n    self.short_desc = first_par\n    self.end_line = ((erow + 1) + start_row)\n", "label": 1}
{"function": "\n\n@classmethod\ndef for_diff(cls, diff):\n    '\\n        Determines what type of diff this represents\\n        '\n    if diff.new_file:\n        return cls.A\n    elif diff.deleted_file:\n        return cls.D\n    elif diff.renamed:\n        return cls.R\n    elif (diff.a_blob and diff.b_blob and (diff.a_blob != diff.b_blob)):\n        return cls.M\n    return cls.U\n", "label": 0}
{"function": "\n\ndef save(self):\n    '\\n        Save the order, create or update the contact information\\n        (if available) and return the saved order instance\\n        '\n    order = super(BaseCheckoutForm, self).save(commit=False)\n    contact = self.shop.contact_from_user(self.request.user)\n    if contact:\n        order.user = contact.user\n    elif self.request.user.is_authenticated():\n        order.user = self.request.user\n    if ((self.cleaned_data.get('create_account') and (not contact)) or ((not contact) and self.request.user.is_authenticated())):\n        password = None\n        email = self.cleaned_data.get('email')\n        if (not self.request.user.is_authenticated()):\n            password = User.objects.make_random_password()\n            params = {\n                'email': email,\n                'password': password,\n            }\n            if (getattr(User, 'USERNAME_FIELD', 'username') == 'username'):\n                params['username'] = email[:30]\n            user = User.objects.create_user(**params)\n            user = auth.authenticate(username=email, password=password)\n            auth.login(self.request, user)\n        else:\n            user = self.request.user\n        contact = self.shop.contact_model(user=user)\n        order.user = user\n        signals.contact_created.send(sender=self.shop, user=user, contact=contact, password=password, request=self.request)\n    order.save()\n    if contact:\n        contact.update_from_order(order, request=self.request)\n        contact.save()\n    return order\n", "label": 1}
{"function": "\n\ndef parse_args(args, options):\n    'Parse arguments from command-line to set options.'\n    long_opts = ['help', 'oauth', 'save-dir=', 'api-rate', 'timeline=', 'mentions=', 'favorites', 'follow-redirects', 'redirect-sites=', 'dms=', 'isoformat']\n    short_opts = 'hos:at:m:vfr:d:i'\n    (opts, extra_args) = getopt(args, short_opts, long_opts)\n    for (opt, arg) in opts:\n        if (opt in ('-h', '--help')):\n            print(__doc__)\n            raise SystemExit(0)\n        elif (opt in ('-o', '--oauth')):\n            options['oauth'] = True\n        elif (opt in ('-s', '--save-dir')):\n            options['save-dir'] = arg\n        elif (opt in ('-a', '--api-rate')):\n            options['api-rate'] = True\n        elif (opt in ('-t', '--timeline')):\n            options['timeline'] = arg\n        elif (opt in ('-m', '--mentions')):\n            options['mentions'] = arg\n        elif (opt in ('-v', '--favorites')):\n            options['favorites'] = True\n        elif (opt in ('-f', '--follow-redirects')):\n            options['follow-redirects'] = True\n        elif (opt in ('-r', '--redirect-sites')):\n            options['redirect-sites'] = arg\n        elif (opt in ('-d', '--dms')):\n            options['dms'] = arg\n        elif (opt in ('-i', '--isoformat')):\n            options['isoformat'] = True\n    options['extra_args'] = extra_args\n", "label": 1}
{"function": "\n\ndef get_primitive(self, primitive):\n    if (primitive == 'USER'):\n        auth = (self.request.authorization and self.request.authorization[1])\n        userpass = ((auth and base64.b64decode(auth)) or None)\n        return ((userpass and userpass.split(':')[0]) or None)\n    if (primitive == 'ALL_DATA'):\n        return self.get_raw_data()\n    if (primitive == 'LOGGED_IN_USER'):\n        return self.request.user\n    if (primitive == 'RAW_INVOCATION_ARGS'):\n        return unquote('/'.join(self.path_args))\n    raise Exception('Primitive not supported')\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    if (name == 'CustomViewer'):\n        return type.__new__(cls, name, bases, attrs)\n    ui = {\n        \n    }\n    for (key, value) in list(attrs.items()):\n        if (key.startswith('_') or (key in CustomViewer.__dict__)):\n            continue\n        if (not isinstance(value, (MethodType, FunctionType))):\n            ui[key] = attrs.pop(key)\n    attrs['ui'] = ui\n    attrs.setdefault('name', name)\n    udfs = {\n        \n    }\n    for (nm, value) in list(attrs.items()):\n        dscr = CustomViewer.__dict__.get(nm, None)\n        if isinstance(dscr, UserDefinedFunction):\n            udfs[nm] = attrs.pop(nm)\n    result = type.__new__(cls, name, bases, attrs)\n    for (k, v) in udfs.items():\n        udf_decorator = getattr(result, k)\n        udf_decorator(v)\n    result._build_widget_subclass()\n    return result\n", "label": 1}
{"function": "\n\n@verbose\ndef __init__(self, vertices, pos=None, values=None, hemi=None, comment='', name=None, filename=None, subject=None, color=None, verbose=None):\n    if (not isinstance(hemi, string_types)):\n        raise ValueError(('hemi must be a string, not %s' % type(hemi)))\n    vertices = np.asarray(vertices)\n    if np.any((np.diff(vertices.astype(int)) <= 0)):\n        raise ValueError('Vertices must be ordered in increasing order.')\n    if (color is not None):\n        from matplotlib.colors import colorConverter\n        color = colorConverter.to_rgba(color)\n    if (values is None):\n        values = np.ones(len(vertices))\n    else:\n        values = np.asarray(values)\n    if (pos is None):\n        pos = np.zeros((len(vertices), 3))\n    else:\n        pos = np.asarray(pos)\n    if (not (len(vertices) == len(values) == len(pos))):\n        raise ValueError('vertices, values and pos need to have same length (number of vertices)')\n    if ((name is None) and (filename is not None)):\n        name = op.basename(filename[:(- 6)])\n    self.vertices = vertices\n    self.pos = pos\n    self.values = values\n    self.hemi = hemi\n    self.comment = comment\n    self.verbose = verbose\n    self.subject = _check_subject(None, subject, False)\n    self.color = color\n    self.name = name\n    self.filename = filename\n", "label": 1}
{"function": "\n\ndef serialize_order_params(self):\n    res = {\n        \n    }\n    if self.order:\n        res['order'] = self.order\n    if self.mode:\n        res['mode'] = self.mode\n    if self.nested_path:\n        res['nested_path'] = self.nested_path\n    if self.nested_filter:\n        res['nested_filter'] = self.nested_filter.serialize()\n    if self.missing:\n        res['missing'] = self.missing\n    if (self.ignore_unmapped is not None):\n        res['ignore_unmapped'] = self.ignore_unmapped\n    return res\n", "label": 0}
{"function": "\n\ndef describe_instances(self, parameters, pending=False):\n    \"\\n    Retrieves the list of running instances that have been instantiated using a\\n    particular EC2 keyname. The target keyname is read from the input parameter\\n    map. (Also see documentation for the BaseAgent class).\\n\\n    Args:\\n      parameters: A dictionary containing the 'keyname' parameter.\\n      pending: Indicates we also want the pending instances.\\n    Returns:\\n      A tuple of the form (public_ips, private_ips, instances) where each\\n      member is a list.\\n    \"\n    instance_ids = []\n    public_ips = []\n    private_ips = []\n    conn = self.open_connection(parameters)\n    reservations = conn.get_all_instances()\n    instances = [i for r in reservations for i in r.instances]\n    for i in instances:\n        if (((i.state == 'running') or (pending and (i.state == 'pending'))) and (i.key_name == parameters[self.PARAM_KEYNAME])):\n            instance_ids.append(i.id)\n            public_ips.append(i.public_dns_name)\n            private_ips.append(i.private_dns_name)\n    return (public_ips, private_ips, instance_ids)\n", "label": 0}
{"function": "\n\ndef strip_parens(s):\n    if ((not s) or (s[0] != '(')):\n        return s\n    ct = i = 0\n    l = len(s)\n    while (i < l):\n        if ((s[i] == '(') and (s[(l - 1)] == ')')):\n            ct += 1\n            i += 1\n            l -= 1\n        else:\n            break\n    if ct:\n        unbalanced_ct = 0\n        required = 0\n        for i in range(ct, (l - ct)):\n            if (s[i] == '('):\n                unbalanced_ct += 1\n            elif (s[i] == ')'):\n                unbalanced_ct -= 1\n            if (unbalanced_ct < 0):\n                required += 1\n                unbalanced_ct = 0\n            if (required == ct):\n                break\n        ct -= required\n    if (ct > 0):\n        return s[ct:(- ct)]\n    return s\n", "label": 1}
{"function": "\n\ndef summary(self, name=None):\n    if (len(self) > 0):\n        head = self[0]\n        if (hasattr(head, 'format') and (not isinstance(head, compat.string_types))):\n            head = head.format()\n        tail = self[(- 1)]\n        if (hasattr(tail, 'format') and (not isinstance(tail, compat.string_types))):\n            tail = tail.format()\n        index_summary = (', %s to %s' % (pprint_thing(head), pprint_thing(tail)))\n    else:\n        index_summary = ''\n    if (name is None):\n        name = type(self).__name__\n    return ('%s: %s entries%s' % (name, len(self), index_summary))\n", "label": 0}
{"function": "\n\ndef pop_or_apply_selected_stashes(self, cmd):\n    repo = self.get_repo()\n    if (not repo):\n        return\n    goto = None\n    stashes = self.get_selected_stashes()\n    if stashes:\n        for (name, title) in stashes:\n            if sublime.ok_cancel_dialog(('Are you sure you want to %s %s?' % (cmd, title)), ('%s stash' % cmd.capitalize())):\n                (exit, stdout, stderr) = self.git(['stash', cmd, '-q', ('stash@{%s}' % name)], cwd=repo)\n                if (exit != 0):\n                    sublime.error_message(self.format_error_message(stderr))\n        if (cmd == 'apply'):\n            region = self.view.line(self.get_first_point())\n            goto = ('point:%s' % region.begin())\n        else:\n            goto = self.logical_goto_next_stash()\n    self.update_status(goto)\n", "label": 0}
{"function": "\n\ndef show_growth(limit=10, peak_stats={\n    \n}, shortnames=True):\n    \"Show the increase in peak object counts since last call.\\n\\n    Limits the output to ``limit`` largest deltas.  You may set ``limit`` to\\n    None to see all of them.\\n\\n    Uses and updates ``peak_stats``, a dictionary from type names to previously\\n    seen peak object counts.  Usually you don't need to pay attention to this\\n    argument.\\n\\n    The caveats documented in :func:`typestats` apply.\\n\\n    Example:\\n\\n        >>> show_growth()\\n        wrapper_descriptor       970       +14\\n        tuple                  12282       +10\\n        dict                    1922        +7\\n        ...\\n\\n    .. versionadded:: 1.5\\n\\n    .. versionchanged:: 1.8\\n       New parameter: ``shortnames``.\\n\\n    \"\n    gc.collect()\n    stats = typestats(shortnames=shortnames)\n    deltas = {\n        \n    }\n    for (name, count) in iteritems(stats):\n        old_count = peak_stats.get(name, 0)\n        if (count > old_count):\n            deltas[name] = (count - old_count)\n            peak_stats[name] = count\n    deltas = sorted(deltas.items(), key=operator.itemgetter(1), reverse=True)\n    if limit:\n        deltas = deltas[:limit]\n    if deltas:\n        width = max((len(name) for (name, count) in deltas))\n        for (name, delta) in deltas:\n            print(('%-*s%9d %+9d' % (width, name, stats[name], delta)))\n", "label": 0}
{"function": "\n\ndef date_parser(timestr, parserinfo=None, **kwargs):\n    '\\n    Uses dateutil.parser.parse, but also handles monthly dates of the form\\n    1999m4, 1999:m4, 1999:mIV, 1999mIV and the same for quarterly data\\n    with q instead of m. It is not case sensitive. The default for annual\\n    data is the end of the year, which also differs from dateutil.\\n    '\n    flags = (re.IGNORECASE | re.VERBOSE)\n    if re.search(_q_pattern, timestr, flags):\n        (y, q) = timestr.replace(':', '').lower().split('q')\n        (month, day) = _quarter_to_day[q.upper()]\n        year = int(y)\n    elif re.search(_m_pattern, timestr, flags):\n        (y, m) = timestr.replace(':', '').lower().split('m')\n        (month, day) = _month_to_day[m.upper()]\n        year = int(y)\n        if (_is_leap(y) and (month == 2)):\n            day += 1\n    elif re.search(_y_pattern, timestr, flags):\n        (month, day) = (12, 31)\n        year = int(timestr)\n    elif (hasattr(pandas_datetools, 'parser') and (not callable(pandas_datetools.parser))):\n        return pandas_datetools.parser.parse(timestr, parserinfo, **kwargs)\n    else:\n        from dateutil import parser\n        return parser.parse(timestr, parserinfo, **kwargs)\n    return datetime.datetime(year, month, day)\n", "label": 0}
{"function": "\n\ndef run(self):\n    client = self.client\n    container = self.container\n    object_names = self.object_names\n    cname = utils.get_name(container)\n    ident = self.client.identity\n    headers = {\n        'X-Auth-Token': ident.token,\n        'Content-Type': 'text/plain',\n    }\n    uri = '/?bulk-delete=1'\n    key_map = {\n        'Number Not Found': 'not_found',\n        'Response Status': 'status',\n        'Errors': 'errors',\n        'Number Deleted': 'deleted',\n        'Response Body': None,\n    }\n    batch = []\n    while object_names:\n        batch[:] = object_names[:MAX_BULK_DELETE]\n        del object_names[:MAX_BULK_DELETE]\n        obj_paths = (('%s/%s' % (cname, nm)) for nm in batch)\n        body = '\\n'.join(obj_paths)\n        (resp, resp_body) = self.client.method_delete(uri, data=body, headers=headers)\n        for (k, v) in six.iteritems(resp_body):\n            if (key_map[k] == 'errors'):\n                status_code = int(resp_body.get('Response Status', '200')[:3])\n                if ((status_code != 200) and (not v)):\n                    self.results['errors'].extend([[resp_body.get('Response Body'), resp_body.get('Response Status')]])\n                else:\n                    self.results['errors'].extend(v)\n            elif (key_map[k] in ('deleted', 'not_found')):\n                self.results[key_map[k]] += int(v)\n            elif key_map[k]:\n                self.results[key_map[k]] = v\n    self.completed = True\n", "label": 1}
{"function": "\n\ndef on_change_input(self, fgraph, app, i, old_r, new_r, reason):\n    '\\n        app.inputs[i] changed from old_r to new_r.\\n\\n        '\n    if (app == 'output'):\n        pass\n    else:\n        if (app not in self.debug_all_apps):\n            raise ProtocolError('change without import')\n        self.clients[old_r][app] -= 1\n        if (self.clients[old_r][app] == 0):\n            del self.clients[old_r][app]\n        self.clients.setdefault(new_r, OrderedDict()).setdefault(app, 0)\n        self.clients[new_r][app] += 1\n        for (o_idx, i_idx_list) in iteritems(getattr(app.op, 'view_map', OrderedDict())):\n            if (len(i_idx_list) > 1):\n                raise NotImplementedError()\n            i_idx = i_idx_list[0]\n            output = app.outputs[o_idx]\n            if (i_idx == i):\n                if (app.inputs[i_idx] is not new_r):\n                    raise ProtocolError('wrong new_r on change')\n                self.view_i[output] = new_r\n                self.view_o[old_r].remove(output)\n                if (not self.view_o[old_r]):\n                    del self.view_o[old_r]\n                self.view_o.setdefault(new_r, OrderedSet()).add(output)\n    self.stale_droot = True\n", "label": 1}
{"function": "\n\ndef convert(values):\n    ' convert the numpy values to a list '\n    dtype = values.dtype\n    if is_categorical_dtype(values):\n        return values\n    elif is_object_dtype(dtype):\n        return values.ravel().tolist()\n    if needs_i8_conversion(dtype):\n        values = values.view('i8')\n    v = values.ravel()\n    if (compressor == 'zlib'):\n        _check_zlib()\n        if (dtype == np.object_):\n            return v.tolist()\n        v = v.tostring()\n        return ExtType(0, zlib.compress(v))\n    elif (compressor == 'blosc'):\n        _check_blosc()\n        if (dtype == np.object_):\n            return v.tolist()\n        v = v.tostring()\n        return ExtType(0, blosc.compress(v, typesize=dtype.itemsize))\n    return ExtType(0, v.tostring())\n", "label": 0}
{"function": "\n\ndef _format_node(self, op):\n    formatted_args = []\n\n    def visit(what, extra_indents=0):\n        if isinstance(what, ir.Expr):\n            result = self._format_subexpr(what)\n        else:\n            result = self._indent(str(what))\n        if (extra_indents > 0):\n            result = util.indent(result, self.indent_size)\n        formatted_args.append(result)\n    arg_names = getattr(op, '_arg_names', None)\n    if (arg_names is None):\n        for arg in op.args:\n            if isinstance(arg, list):\n                for x in arg:\n                    visit(x)\n            else:\n                visit(arg)\n    else:\n        for (arg, name) in zip(op.args, arg_names):\n            if (name is not None):\n                name = self._indent('{0}:'.format(name))\n            if isinstance(arg, list):\n                if ((name is not None) and (len(arg) > 0)):\n                    formatted_args.append(name)\n                    indents = 1\n                else:\n                    indents = 0\n                for x in arg:\n                    visit(x, extra_indents=indents)\n            else:\n                if (name is not None):\n                    formatted_args.append(name)\n                    indents = 1\n                else:\n                    indents = 0\n                visit(arg, extra_indents=indents)\n    opname = type(op).__name__\n    type_display = self._get_type_display(op)\n    opline = ('%s[%s]' % (opname, type_display))\n    return '\\n'.join(([opline] + formatted_args))\n", "label": 1}
{"function": "\n\ndef format(self, value):\n    if isinstance(value, set):\n        value = list(value)\n    return [self.container.output(idx, (val if ((isinstance(val, dict) or (self.container.attribute and hasattr(val, self.container.attribute))) and (not isinstance(self.container, Nested)) and (not (type(self.container) is Raw))) else value)) for (idx, val) in enumerate(value)]\n", "label": 0}
{"function": "\n\ndef get_firefox_exe():\n    ' Get the path of the Firefox executable\\n\\n    If the path could not be found, returns None. You may still be able\\n    to launch using \"firefox\" though.\\n    '\n    paths = []\n    if sys.platform.startswith('win'):\n        for basepath in ('C:\\\\Program Files\\\\', 'C:\\\\Program Files (x86)\\\\'):\n            paths.append((basepath + 'Mozilla Firefox\\\\firefox.exe'))\n            paths.append((basepath + 'Mozilla\\\\Firefox\\\\firefox.exe'))\n            paths.append((basepath + 'Firefox\\\\firefox.exe'))\n    elif sys.platform.startswith('linux'):\n        paths.append('/usr/lib/firefox/firefox')\n        paths.append('/usr/lib64/firefox/firefox')\n        paths.append('/usr/lib/iceweasel/iceweasel')\n        paths.append('/usr/lib64/iceweasel/iceweasel')\n    elif sys.platform.startswith('darwin'):\n        paths.append('/Applications/Firefox.app/Contents/MacOS/firefox')\n    for path in paths:\n        if op.isfile(path):\n            return path\n    else:\n        return None\n", "label": 0}
{"function": "\n\ndef _handle_function_node(self, func_node):\n    func_name_node = func_node['left']\n    traverse(func_name_node, on_enter=self._find_variable_like_nodes)\n    self._scope_tree_builder.enter_new_scope(ScopeVisibility.FUNCTION_LOCAL)\n    has_variadic = False\n    param_nodes = func_node['rlist']\n    for param_node in param_nodes:\n        if (param_node['value'] == '...'):\n            has_variadic = True\n        else:\n            self._scope_tree_builder.handle_new_parameter_found(param_node)\n    self._scope_tree_builder.handle_new_parameters_list_and_length_found()\n    if has_variadic:\n        self._scope_tree_builder.handle_new_index_parameters_found((len(param_nodes) - 1))\n    attr = func_node['attr']\n    is_declared_with_range = (attr['range'] is not 0)\n    if is_declared_with_range:\n        self._scope_tree_builder.handle_new_range_parameters_found()\n    is_declared_with_dict = ((attr['dict'] is not 0) or (NodeType(func_name_node['type']) in FunctionNameNodesDeclaringVariableSelf))\n    if is_declared_with_dict:\n        self._scope_tree_builder.handle_new_dict_parameter_found()\n    func_body_nodes = func_node['body']\n    for func_body_node in func_body_nodes:\n        traverse(func_body_node, on_enter=self._enter_handler, on_leave=self._leave_handler)\n    return SKIP_CHILDREN\n", "label": 0}
{"function": "\n\ndef __call__(self, func=None, timeout=None, wait=None, message=None):\n    if (func is None):\n        return (lambda func: self(func, timeout, wait, message))\n    if func():\n        return\n    now = self.getnow()\n    sleep = self.getsleep()\n    if (timeout is None):\n        timeout = self.timeout\n    if (wait is None):\n        wait = self.wait\n    wait = float(wait)\n    deadline = (now() + timeout)\n    while 1:\n        sleep(wait)\n        if func():\n            return\n        if (now() > deadline):\n            raise self.TimeOutWaitingFor((message or getattr(func, '__doc__') or getattr(func, '__name__')))\n", "label": 1}
{"function": "\n\ndef update_active_tasks(self):\n    self.metric_sources = []\n    for (service_name, service) in self.collector_config.services.iteritems():\n        (service_record, created) = Service.objects.get_or_create(name=service_name, defaults={\n            'metric_url': service.metric_url,\n        })\n        if (not created):\n            service_record.active = True\n            service_record.save()\n        for (cluster_name, cluster) in service.clusters.iteritems():\n            (cluster_record, created) = Cluster.objects.get_or_create(service=service_record, name=cluster_name)\n            if (not created):\n                cluster_record.active = True\n                cluster_record.save()\n            for job_name in service.jobs:\n                (job_record, created) = Job.objects.get_or_create(cluster=cluster_record, name=job_name)\n                if (not created):\n                    job_record.active = True\n                    job_record.save()\n                job = cluster.jobs[job_name]\n                port = (job.base_port + 1)\n                hosts = job.hosts\n                for (host_id, host) in hosts.iteritems():\n                    host_name = job.hostnames[host_id]\n                    for instance_id in range(host.instance_num):\n                        task_id = deploy_utils.get_task_id(hosts, host_id, instance_id)\n                        instance_port = deploy_utils.get_base_port(port, instance_id)\n                        (task_record, created) = Task.objects.get_or_create(job=job_record, task_id=task_id, defaults={\n                            'host': host_name,\n                            'port': instance_port,\n                        })\n                        if ((not created) or (task_record.host != host_name) or (task_record.port != instance_port)):\n                            task_record.active = True\n                            task_record.host = host_name\n                            task_record.port = instance_port\n                            task_record.save()\n                        self.metric_sources.append(MetricSource(self.collector_config, task_record))\n", "label": 1}
{"function": "\n\ndef scan_file(self, fp):\n    current_file_offset = 0\n    while True:\n        (data, dlen) = fp.read_block()\n        if (not data):\n            break\n        current_block_offset = 0\n        block_start = (fp.tell() - dlen)\n        self.status.completed = (block_start - fp.offset)\n        for r in self.magic.scan(data, dlen):\n            if (r.offset < current_block_offset):\n                continue\n            relative_offset = (r.offset + r.adjust)\n            r.offset = (block_start + relative_offset)\n            r.file = fp\n            self.result(r=r)\n            if (r.valid and (r.jump > 0) and (not self.dumb_scan)):\n                absolute_jump_offset = (r.offset + r.jump)\n                current_block_offset = (relative_offset + r.jump)\n                if (absolute_jump_offset >= fp.tell()):\n                    fp.seek((r.offset + r.jump))\n                    break\n", "label": 1}
{"function": "\n\ndef get_file_generator(environ, root_path, config=None):\n    logname = log_name(environ)\n    logpath = safe_path(root_path, logname)\n    if (logpath is None):\n        raise UnsafePath()\n    file_generator = None\n    use_files = (util.parse_param(environ, 'source', default='all') != 'swift')\n    if (use_files and does_file_exist(logpath)):\n        file_generator = DiskIterableBuffer(logname, logpath, config)\n    elif (logname and (logname[(- 1)] == '/')):\n        file_generator = SwiftIterableBuffer(os.path.join(logname, 'index.html'), config)\n        if (not file_generator.obj):\n            file_generator = SwiftIterableBuffer(logname, config)\n    else:\n        file_generator = SwiftIterableBuffer(logname, config)\n        if (not file_generator.obj):\n            file_generator = SwiftIterableBuffer(os.path.join(logname, 'index.html'), config)\n    if ((not file_generator) or (not file_generator.obj)):\n        if (config.has_section('general') and config.has_option('general', 'generate_folder_index') and config.getboolean('general', 'generate_folder_index')):\n            index_generator = IndexIterableBuffer(logname, logpath, config)\n            if (len(index_generator.file_list) > 0):\n                return index_generator\n        raise NoSuchFile()\n    return file_generator\n", "label": 1}
{"function": "\n\ndef _set_foreground(self, foreground):\n    color = None\n    flags = 0\n    for part in foreground.split(','):\n        part = part.strip()\n        if (part in _ATTRIBUTES):\n            if (flags & _ATTRIBUTES[part]):\n                raise AttrSpecError((('Setting %s specified more than' + 'once in foreground (%s)') % (repr(part), repr(foreground))))\n            flags |= _ATTRIBUTES[part]\n            continue\n        if (part in ('', 'default')):\n            scolor = 0\n        elif (part in _BASIC_COLORS):\n            scolor = _BASIC_COLORS.index(part)\n            flags |= _FG_BASIC_COLOR\n        elif (self._value & _HIGH_88_COLOR):\n            scolor = _parse_color_88(part)\n            flags |= _FG_HIGH_COLOR\n        else:\n            scolor = _parse_color_256(part)\n            flags |= _FG_HIGH_COLOR\n        if (scolor is None):\n            raise AttrSpecError((('Unrecognised color specification %s' + 'in foreground (%s)') % (repr(part), repr(foreground))))\n        if (color is not None):\n            raise AttrSpecError((('More than one color given for ' + 'foreground (%s)') % (repr(foreground),)))\n        color = scolor\n    if (color is None):\n        color = 0\n    self._value = (((self._value & (~ _FG_MASK)) | color) | flags)\n", "label": 1}
{"function": "\n\ndef select_related_descend(field, restricted, requested, reverse=False):\n    '\\n    Returns True if this field should be used to descend deeper for\\n    select_related() purposes. Used by both the query construction code\\n    (sql.query.fill_related_selections()) and the model instance creation code\\n    (query.get_cached_row()).\\n\\n    Arguments:\\n     * field - the field to be checked\\n     * restricted - a boolean field, indicating if the field list has been\\n       manually restricted using a requested clause)\\n     * requested - The select_related() dictionary.\\n     * reverse - boolean, True if we are checking a reverse select related\\n    '\n    if (not field.rel):\n        return False\n    if (field.rel.parent_link and (not reverse)):\n        return False\n    if restricted:\n        if (reverse and (field.related_query_name() not in requested)):\n            return False\n        if ((not reverse) and (field.name not in requested)):\n            return False\n    if ((not restricted) and field.null):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef GuessHistoryPaths(self, username):\n    'Take a user and return guessed full paths to History files.\\n\\n    Args:\\n      username: Username as string.\\n\\n    Returns:\\n      A list of strings containing paths to look for history files in.\\n\\n    Raises:\\n      OSError: On invalid system in the Schema\\n    '\n    client = aff4.FACTORY.Open(self.client_id, token=self.token)\n    system = client.Get(client.Schema.SYSTEM)\n    user_info = flow_utils.GetUserInfo(client, username)\n    if (not user_info):\n        self.Error('Could not find homedir for user {0}'.format(username))\n        return\n    paths = []\n    if (system == 'Windows'):\n        path = '{app_data}\\\\{sw}\\\\User Data\\\\Default\\\\'\n        for sw_path in ['Google\\\\Chrome', 'Chromium']:\n            paths.append(path.format(app_data=user_info.special_folders.local_app_data, sw=sw_path))\n    elif (system == 'Linux'):\n        path = '{homedir}/.config/{sw}/Default/'\n        for sw_path in ['google-chrome', 'chromium']:\n            paths.append(path.format(homedir=user_info.homedir, sw=sw_path))\n    elif (system == 'Darwin'):\n        path = '{homedir}/Library/Application Support/{sw}/Default/'\n        for sw_path in ['Google/Chrome', 'Chromium']:\n            paths.append(path.format(homedir=user_info.homedir, sw=sw_path))\n    else:\n        raise OSError('Invalid OS for Chrome History')\n    return paths\n", "label": 0}
{"function": "\n\ndef render_content(self, content, request, response, **kwargs):\n    if (hasattr(content, 'interface') and ((content.interface is True) or hasattr(content.interface, 'http'))):\n        if (content.interface is True):\n            content(request, response, api_version=None, **kwargs)\n        else:\n            content.interface.http(request, response, api_version=None, **kwargs)\n        return\n    content = self.transform_data(content, request, response)\n    content = self.outputs(content, **self._arguments(self._params_for_outputs, request, response))\n    if hasattr(content, 'read'):\n        size = None\n        if (hasattr(content, 'name') and os.path.isfile(content.name)):\n            size = os.path.getsize(content.name)\n        if (request.range and size):\n            (start, end) = request.range\n            if (end < 0):\n                end = (size + end)\n            end = min(end, size)\n            length = ((end - start) + 1)\n            content.seek(start)\n            response.data = content.read(length)\n            response.status = falcon.HTTP_206\n            response.content_range = (start, end, size)\n            content.close()\n        else:\n            response.stream = content\n            if size:\n                response.stream_len = size\n    else:\n        response.data = content\n", "label": 1}
{"function": "\n\ndef receiveMessage(self, msg, sender):\n    if (msg == 'Best Friend'):\n        if (not self.bestFriend):\n            self.bestFriend = self.createActor(Lassie)\n        self.send(self.bestFriend, msg)\n        return\n    if (msg == 'Best Friend Says'):\n        if (not self.bestFriend):\n            self.send(sender, 'no best friend')\n        else:\n            self.waiter = sender\n            self.send(self.bestFriend, 'what')\n    elif (self.bestFriend and (self.bestFriend == sender)):\n        if self.waiter:\n            if (sender != self.bestFriend):\n                self.send(self.waiter, 'ERROR: lcl/rmt address comparison not epimorphic.')\n            else:\n                self.send(self.waiter, msg)\n            self.waiter = None\n        else:\n            pass\n    elif (sender != self.myAddress):\n        self.send(sender, 'Greetings.')\n", "label": 1}
{"function": "\n\ndef format_option(self, option):\n    old_help = option.help\n    default = option.default\n    if (isinstance(default, str) and (' ' in default)):\n        default = repr(default)\n    if (option.help is None):\n        option.help = ('Specify a %s.' % option.type)\n    if (option.type == 'choice'):\n        choices = []\n        for choice in option.choices:\n            if (choice == option.default):\n                if (' ' in choice):\n                    choice = repr(choice)\n                choice = (('[' + choice) + ']')\n            elif (' ' in choice):\n                choice = repr(choice)\n            choices.append(choice)\n        option.help = ('%s Choices: %s.' % (option.help, ', '.join(choices)))\n    elif (default != NO_DEFAULT):\n        if (option.action == 'store_false'):\n            option.help = ('%s Default: %s.' % (option.help, (not default)))\n        else:\n            option.help = ('%s Default: %s.' % (option.help, default))\n    result = TitledHelpFormatter.format_option(self, option)\n    option.help = old_help\n    return result\n", "label": 1}
{"function": "\n\ndef add(name, uid=None, gid=None, groups=None, home=None, shell=None, fullname=None, createhome=True, **kwargs):\n    \"\\n    Add a user to the minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' user.add name <uid> <gid> <groups> <home> <shell>\\n    \"\n    if info(name):\n        raise CommandExecutionError(\"User '{0}' already exists\".format(name))\n    if salt.utils.contains_whitespace(name):\n        raise SaltInvocationError('Username cannot contain whitespace')\n    if (uid is None):\n        uid = _first_avail_uid()\n    if (gid is None):\n        gid = 20\n    if (home is None):\n        home = '/Users/{0}'.format(name)\n    if (shell is None):\n        shell = '/bin/bash'\n    if (fullname is None):\n        fullname = ''\n    if (not isinstance(uid, int)):\n        raise SaltInvocationError('uid must be an integer')\n    if (not isinstance(gid, int)):\n        raise SaltInvocationError('gid must be an integer')\n    name_path = '/Users/{0}'.format(name)\n    _dscl([name_path, 'UniqueID', uid])\n    _dscl([name_path, 'PrimaryGroupID', gid])\n    _dscl([name_path, 'UserShell', shell])\n    _dscl([name_path, 'NFSHomeDirectory', home])\n    _dscl([name_path, 'RealName', fullname])\n    if createhome:\n        __salt__['file.mkdir'](home, user=uid, group=gid)\n    time.sleep(1)\n    if groups:\n        chgroups(name, groups)\n    return True\n", "label": 1}
{"function": "\n\n@property\ndef legacy_modes(self):\n    args = ['+']\n    for i in self.props.keys():\n        if ((self.props[i] != False) and (i in channel_property_items) and (not ((i == 'ban') or (i == 'quiet') or (i == 'invite-exemption') or (i == 'exemption')))):\n            args[0] += channel_property_items[i]\n            if (self.props[i] != True):\n                args.append(self.props[i])\n    return ' '.join(args)\n", "label": 1}
{"function": "\n\ndef getcoordinatesatpixel(self, pixelx, pixely=None, onscreen=True):\n    \"\\n        Given the pixel x and y coordinates relative to the PygCurse screen's origin, return the cell x and y coordinates that it is over. (Useful for finding what cell the mouse cursor is over.)\\n\\n        Returns (None, None) if the pixel coordinates are not over the screen.\\n        \"\n    if (type(pixelx) in (tuple, list)):\n        if (type(pixely) == bool):\n            onscreen = pixely\n        (pixelx, pixely) = pixelx\n    if ((onscreen and ((pixelx < 0) or (pixelx >= (self._width * self._cellwidth)))) or ((pixely < 0) or (pixely >= (self._height * self._cellheight)))):\n        return (None, None)\n    return (int((pixelx / self._cellwidth)), int((pixely / self._cellheight)))\n", "label": 0}
{"function": "\n\ndef _verts_within_dist(graph, sources, max_dist):\n    'Find all vertices wihin a maximum geodesic distance from source\\n\\n    Parameters\\n    ----------\\n    graph : scipy.sparse.csr_matrix\\n        Sparse matrix with distances between adjacent vertices.\\n    sources : list of int\\n        Source vertices.\\n    max_dist : float\\n        Maximum geodesic distance.\\n\\n    Returns\\n    -------\\n    verts : array\\n        Vertices within max_dist.\\n    dist : array\\n        Distances from source vertex.\\n    '\n    dist_map = {\n        \n    }\n    verts_added_last = []\n    for source in sources:\n        dist_map[source] = 0\n        verts_added_last.append(source)\n    while (len(verts_added_last) > 0):\n        verts_added = []\n        for i in verts_added_last:\n            v_dist = dist_map[i]\n            row = graph[i, :]\n            neighbor_vert = row.indices\n            neighbor_dist = row.data\n            for (j, d) in zip(neighbor_vert, neighbor_dist):\n                n_dist = (v_dist + d)\n                if (j in dist_map):\n                    if (n_dist < dist_map[j]):\n                        dist_map[j] = n_dist\n                elif (n_dist <= max_dist):\n                    dist_map[j] = n_dist\n                    verts_added.append(j)\n        verts_added_last = verts_added\n    verts = np.sort(np.array(list(dist_map.keys()), dtype=np.int))\n    dist = np.array([dist_map[v] for v in verts])\n    return (verts, dist)\n", "label": 1}
{"function": "\n\ndef _parse(self, node, alias_map, conv):\n    node_type = getattr(node, '_node_type', None)\n    unknown = False\n    if (node_type in self._parse_map):\n        (sql, params) = self._parse_map[node_type](node, alias_map, conv)\n        unknown = (node_type in self._unknown_types)\n    elif isinstance(node, (list, tuple)):\n        (sql, params) = self.parse_node_list(node, alias_map, conv)\n        sql = ('(%s)' % sql)\n    elif isinstance(node, Model):\n        sql = self.interpolation\n        if (conv and isinstance(conv, ForeignKeyField) and (not isinstance(conv.to_field, ForeignKeyField))):\n            params = [conv.to_field.db_value(getattr(node, conv.to_field.name))]\n        else:\n            params = [node._get_pk_value()]\n    elif ((isclass(node) and issubclass(node, Model)) or isinstance(node, ModelAlias)):\n        entity = node.as_entity().alias(alias_map[node])\n        (sql, params) = self.parse_node(entity, alias_map, conv)\n    else:\n        (sql, params) = self._parse_default(node, alias_map, conv)\n        unknown = True\n    return (sql, params, unknown)\n", "label": 1}
{"function": "\n\ndef _sock_state_cb(self, fd, readable, writable):\n    if (readable or writable):\n        if readable:\n            self.loop.add_reader(fd, self._handle_event, fd, READ)\n            self._read_fds.add(fd)\n        if writable:\n            self.loop.add_writer(fd, self._handle_event, fd, WRITE)\n            self._write_fds.add(fd)\n        if (self._timer is None):\n            self._timer = self.loop.call_later(1.0, self._timer_cb)\n    else:\n        if (fd in self._read_fds):\n            self._read_fds.discard(fd)\n            self.loop.remove_reader(fd)\n        if (fd in self._write_fds):\n            self._write_fds.discard(fd)\n            self.loop.remove_writer(fd)\n        if ((not self._read_fds) and (not self._write_fds) and (self._timer is not None)):\n            self._timer.cancel()\n            self._timer = None\n", "label": 1}
{"function": "\n\ndef get_language(tree, headers, domain=None):\n    lang = None\n    if (headers and ('content-language' in headers)):\n        lang = headers['content-language']\n    if ((lang is None) and ('lang' in tree.attrib)):\n        lang = tree.attrib['lang']\n    if (lang is None):\n        page_txt = (' '.join(tree.xpath('//p/text()')) + ' '.join(tree.xpath('//div/text()')))\n        if (not page_txt):\n            page_txt = tree.text_content()\n        lang = langdetect.detect(page_txt)\n    if ((lang is None) and (domain is not None)):\n        lang = domain.split('.')[(- 1)]\n        if (lang == 'com'):\n            return 'en'\n    return (lang[:2] or 'en')\n", "label": 1}
{"function": "\n\ndef __init__(self, event_id, event_type, event_timestamp=None, **kwargs):\n    if (event_type not in SUPPORTED_HISTORY_EVENT_TYPES):\n        raise NotImplementedError(\"HistoryEvent does not implement attributes for type '{0}'\".format(event_type))\n    self.event_id = event_id\n    self.event_type = event_type\n    if event_timestamp:\n        self.event_timestamp = event_timestamp\n    else:\n        self.event_timestamp = unix_time()\n    self.event_attributes = {\n        \n    }\n    for (key, value) in kwargs.items():\n        if value:\n            camel_key = underscores_to_camelcase(key)\n            if (key == 'task_list'):\n                value = {\n                    'name': value,\n                }\n            elif (key == 'workflow_type'):\n                value = {\n                    'name': value.name,\n                    'version': value.version,\n                }\n            elif (key == 'activity_type'):\n                value = value.to_short_dict()\n            self.event_attributes[camel_key] = value\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return (other and (self.__class__ is other.__class__) and (self.fieldname == other.fieldname) and (self.start == other.start) and (self.end == other.end) and (self.startexcl == other.startexcl) and (self.endexcl == other.endexcl) and (self.boost == other.boost))\n", "label": 0}
{"function": "\n\ndef heuristics(e, z, z0, dir):\n    rv = None\n    if (abs(z0) is S.Infinity):\n        rv = limit(e.subs(z, (1 / z)), z, S.Zero, ('+' if (z0 is S.Infinity) else '-'))\n        if isinstance(rv, Limit):\n            return\n    elif (e.is_Mul or e.is_Add or e.is_Pow or e.is_Function):\n        r = []\n        for a in e.args:\n            l = limit(a, z, z0, dir)\n            if (l.has(S.Infinity) and (l.is_finite is None)):\n                return\n            elif isinstance(l, Limit):\n                return\n            elif (l is S.NaN):\n                return\n            else:\n                r.append(l)\n        if r:\n            rv = e.func(*r)\n            if (rv is S.NaN):\n                return\n    return rv\n", "label": 1}
{"function": "\n\ndef func(self):\n    'Set the aliases.'\n    caller = self.caller\n    if (not self.lhs):\n        string = 'Usage: @alias <obj> [= [alias[,alias ...]]]'\n        self.caller.msg(string)\n        return\n    objname = self.lhs\n    obj = caller.search(objname)\n    if (not obj):\n        return\n    if (self.rhs is None):\n        aliases = obj.aliases.all()\n        if aliases:\n            caller.msg((\"Aliases for '%s': %s\" % (obj.get_display_name(caller), ', '.join(aliases))))\n        else:\n            caller.msg((\"No aliases exist for '%s'.\" % obj.get_display_name(caller)))\n        return\n    if (not obj.access(caller, 'edit')):\n        caller.msg(\"You don't have permission to do that.\")\n        return\n    if (not self.rhs):\n        old_aliases = obj.aliases.all()\n        if old_aliases:\n            caller.msg(('Cleared aliases from %s: %s' % (obj.get_display_name(caller), ', '.join(old_aliases))))\n            obj.aliases.clear()\n        else:\n            caller.msg('No aliases to clear.')\n        return\n    old_aliases = obj.aliases.all()\n    new_aliases = [alias.strip().lower() for alias in self.rhs.split(',') if alias.strip()]\n    old_aliases.extend(new_aliases)\n    aliases = list(set(old_aliases))\n    obj.aliases.add(aliases)\n    obj.at_cmdset_get(force_init=True)\n    caller.msg((\"Alias(es) for '%s' set to %s.\" % (obj.get_display_name(caller), str(obj.aliases))))\n", "label": 1}
{"function": "\n\ndef _remove_hstore_virtual_fields(self):\n    ' remove hstore virtual fields from class '\n    cls = self.model\n    if hasattr(cls, '_hstore_virtual_fields'):\n        for field_name in cls._hstore_virtual_fields.keys():\n            delattr(cls, field_name)\n        delattr(cls, '_hstore_virtual_fields')\n    if (parse_version(get_version()[0:3]) >= parse_version('1.8')):\n        hstore_fields = []\n        for field in getattr(cls._meta, 'virtual_fields'):\n            if hasattr(field, 'hstore_field_name'):\n                hstore_fields.append(field)\n        for field in hstore_fields:\n            getattr(cls._meta, 'virtual_fields').remove(field)\n        fields = [f for f in cls._meta.fields if (not hasattr(f, 'hstore_field_name'))]\n        cls._meta.fields = cls._meta.fields.__class__(fields)\n    else:\n        for meta_fields in ['fields', 'local_fields', 'virtual_fields']:\n            hstore_fields = []\n            for field in getattr(cls._meta, meta_fields):\n                if hasattr(field, 'hstore_field_name'):\n                    hstore_fields.append(field)\n            for field in hstore_fields:\n                getattr(cls._meta, meta_fields).remove(field)\n", "label": 1}
{"function": "\n\ndef main():\n    arguments = docopt(__doc__)\n    ctx = app.test_request_context()\n    ctx.push()\n    if arguments['user']:\n        if arguments['create']:\n            create_user(arguments['<email>'], interactive=True)\n        elif arguments['activate']:\n            activate_user(arguments['<email_or_id>'])\n        elif arguments['deactivate']:\n            deactivate_user(arguments['<email_or_id>'])\n        elif arguments['delete']:\n            delete_user(arguments['<email_or_id>'])\n    elif arguments['db']:\n        if arguments['create']:\n            create_db()\n        if arguments['destroy']:\n            destroy_db()\n    ctx.pop()\n", "label": 1}
{"function": "\n\ndef save(self):\n    if (not self.data['choice']):\n        return\n    t = self.report\n    model = loads(t.model)\n    classobj = model.get_class_object()\n    xfield = classobj._meta.get_field_by_name(self.field_name)[0]\n    field = model.get_field(self.field_name)\n    if (not field.filters):\n        field.filters = []\n    c = self.data['choice']\n    type = xfield.get_internal_type()\n    if (type == 'ManyToManyField'):\n        c = xfield.related.parent_model.objects.filter(pk=c)[0]\n    if (type == 'ForeignKey'):\n        c = xfield.related.parent_model.objects.filter(pk=c)[0]\n    display_choice = c\n    if hasattr(self.fields['choice'], 'choices'):\n        for (choice, dc) in self.fields['choice'].choices:\n            if (unicode(c) == unicode(choice)):\n                display_choice = dc\n                break\n    for (choice, dc) in self.fields['operand'].choices:\n        if (unicode(self.data['operand']) == unicode(choice)):\n            display_operand = dc\n            break\n    display = ('%s %s %s' % (field.get_human_name(), display_operand, unicode(display_choice)))\n    field.filters.append({\n        'choice': c,\n        'operand': self.data['operand'],\n        'display': display,\n    })\n    t.model = dumps(model)\n    t.save()\n", "label": 1}
{"function": "\n\ndef write_to_hdf5(group, name, value):\n    filename = group.file.filename\n    if isinstance(value, dict):\n        dict_grp = group[name]\n        for (k, v) in value.items():\n            write_to_hdf5(dict_grp, k, v)\n    elif isinstance(value, VariableTree):\n        vtree_grp = group[name]\n        for k in value.list_vars():\n            write_to_hdf5(vtree_grp, k, value.get(k))\n    elif isinstance(value, np.ndarray):\n        dset = group[name]\n        dset[:] = value[:]\n    elif isinstance(value, list):\n        if (len(value) > 0):\n            if isinstance(value[0], str):\n                dset = group[name]\n                for (i, v) in enumerate(value):\n                    dset[i] = value[i]\n        else:\n            pass\n    elif (value == None):\n        pass\n    elif isinstance(value, (np.float64, float)):\n        dset = group[name]\n        dset[()] = value\n    elif isinstance(value, int):\n        dset = group[name]\n        dset[()] = value\n    elif isinstance(value, str):\n        dset = group[name]\n        dset[()] = value\n        sys.stdout.flush()\n    elif isinstance(value, bool):\n        dset = group[name]\n        dset[()] = value\n", "label": 1}
{"function": "\n\ndef parse_stories(lines, only_supporting=False):\n    'Parse stories provided in the bAbi tasks format\\n\\n    If only_supporting is true, only the sentences that support the answer are kept.\\n    '\n    data = []\n    story = []\n    for line in lines:\n        line = line.decode('utf-8').strip()\n        (nid, line) = line.split(' ', 1)\n        nid = int(nid)\n        if (nid == 1):\n            story = []\n        if ('\\t' in line):\n            (q, a, supporting) = line.split('\\t')\n            q = tokenize(q)\n            substory = None\n            if only_supporting:\n                supporting = map(int, supporting.split())\n                substory = [story[(i - 1)] for i in supporting]\n            else:\n                substory = [x for x in story if x]\n            data.append((substory, q, a))\n            story.append('')\n        else:\n            sent = tokenize(line)\n            story.append(sent)\n    return data\n", "label": 0}
{"function": "\n\ndef transform_targeting_data_to_dfp(targeting):\n    '\\n    Convert dictionary-representation for a SUDS creative object into a\\n    Parselmouth representation of a Creative\\n\\n    @param targeting: dict, dictionary-representation of a DFP SUDS response\\n        for a single creative\\n    @return: parselmouth.delivery.TargetingData\\n    '\n    dfp_targeting = {\n        \n    }\n    if targeting.inventory:\n        dfp_targeting['inventoryTargeting'] = transform_inventory_targeting_to_dfp(targeting.inventory)\n    if targeting.geography:\n        dfp_targeting['geoTargeting'] = transform_geography_targeting_to_dfp(targeting.geography)\n    if targeting.day_part:\n        dfp_targeting['dayPartTargeting'] = transform_day_part_targeting_to_dfp(targeting.day_part)\n    if targeting.technology:\n        dfp_targeting['technologyTargeting'] = transform_technology_targeting_to_dfp(targeting.technology)\n    if targeting.video_content:\n        dfp_targeting['contentTargeting'] = transform_video_content_targeting_to_dfp(targeting.video_content)\n    if targeting.video_position:\n        dfp_targeting['videoPositionTargeting'] = transform_video_position_targeting_to_dfp(targeting.video_position)\n    if targeting.custom:\n        dfp_targeting['customTargeting'] = transform_custom_targeting_to_dfp(targeting.custom)\n    return dfp_targeting\n", "label": 0}
{"function": "\n\n@wsgify\ndef __call__(self, req):\n    path = os.path.abspath(os.path.join(self.path, req.path_info.lstrip('/')))\n    if (os.path.isdir(path) and self.index_page):\n        return self.index(req, path)\n    if (self.index_page and self.hide_index_with_redirect and path.endswith((os.path.sep + self.index_page))):\n        new_url = req.path_url.rsplit('/', 1)[0]\n        new_url += '/'\n        if req.query_string:\n            new_url += ('?' + req.query_string)\n        return Response(status=301, location=new_url)\n    if (not os.path.isfile(path)):\n        return exc.HTTPNotFound(comment=path)\n    elif (not path.startswith(self.path)):\n        return exc.HTTPForbidden()\n    else:\n        return self.make_fileapp(path)\n", "label": 1}
{"function": "\n\n@pypi_packages.route('/sync')\ndef sync():\n    \"Sync local database with data from PyPI's API through a Celery task.\"\n    if redis.exists(POLL_SIMPLE_THROTTLE):\n        flash.warning('Already sycned once within the past hour. Not syncing until lock expires.')\n        return redirect(url_for('.index'))\n    task = update_package_list.delay()\n    for _ in range(int((WAIT_UP_TO / SLEEP_FOR))):\n        time.sleep(SLEEP_FOR)\n        if (not task.ready()):\n            continue\n        results = task.get(propagate=False)\n        if isinstance(results, Exception):\n            if (str(results) == 'Failed to acquire lock.'):\n                flash.info('Task scheduled, any new packages will appear within 1 minute.')\n                return redirect(url_for('.index'))\n            raise results\n        if (not results):\n            flash.info('No new packages found.')\n            return redirect(url_for('.index'))\n        if (len(results) < 5):\n            flash.info('New packages: {}'.format(', '.join(results)))\n        else:\n            flash.modal('New packages found:\\n{}'.format(', '.join(results)))\n        return redirect(url_for('.index'))\n    flash.info('Task scheduled, any new packages will appear within 15 minutes.')\n    return redirect(url_for('.index'))\n", "label": 0}
{"function": "\n\ndef set_editing(self, permissions, auth=None, log=False):\n    'Set the editing permissions for this node.\\n\\n        :param auth: All the auth information including user, API key\\n        :param bool permissions: True = publicly editable\\n        :param bool save: Whether to save the privacy change\\n        :param bool log: Whether to add a NodeLog for the privacy change\\n            if true the node object is also saved\\n        '\n    node = self.owner\n    if (permissions and (not self.is_publicly_editable)):\n        if node.is_public:\n            self.is_publicly_editable = True\n        else:\n            raise NodeStateError('Private components cannot be made publicly editable.')\n    elif ((not permissions) and self.is_publicly_editable):\n        self.is_publicly_editable = False\n    else:\n        raise NodeStateError('Desired permission change is the same as current setting.')\n    if log:\n        node.add_log(action=(NodeLog.MADE_WIKI_PUBLIC if self.is_publicly_editable else NodeLog.MADE_WIKI_PRIVATE), params={\n            'project': node.parent_id,\n            'node': node._primary_key,\n        }, auth=auth, save=False)\n        node.save()\n    self.save()\n", "label": 0}
{"function": "\n\ndef get_field_info(self, field):\n    'Adds `related_to` and `nullable` to the metadata response.'\n    field_info = OrderedDict()\n    for attr in ('required', 'read_only', 'default', 'label'):\n        field_info[attr] = getattr(field, attr)\n    if (field_info['default'] is empty):\n        field_info['default'] = None\n    field_info['nullable'] = field.allow_null\n    if hasattr(field, 'choices'):\n        field_info['choices'] = [{\n            'value': choice_value,\n            'display_name': force_text(choice_name, strings_only=True),\n        } for (choice_value, choice_name) in field.choices.items()]\n    many = False\n    if isinstance(field, DynamicRelationField):\n        field = field.serializer\n    if isinstance(field, ListSerializer):\n        field = field.child\n        many = True\n    if isinstance(field, ModelSerializer):\n        type = ('many' if many else 'one')\n        field_info['related_to'] = field.get_plural_name()\n    else:\n        type = self.label_lookup[field]\n    field_info['type'] = type\n    return field_info\n", "label": 1}
{"function": "\n\ndef checkPrivileges(self, irc, channel):\n    if self.disabled(irc):\n        return\n    chanserv = self.registryValue('ChanServ')\n    on = ('on %s' % irc.network)\n    if (chanserv and self.registryValue('ChanServ.op', channel)):\n        if (irc.nick not in irc.state.channels[channel].ops):\n            self.log.info('Requesting op from %s in %s %s.', chanserv, channel, on)\n            irc.sendMsg(ircmsgs.privmsg(chanserv, ('op %s' % channel)))\n    if (chanserv and self.registryValue('ChanServ.halfop', channel)):\n        if (irc.nick not in irc.state.channels[channel].halfops):\n            self.log.info('Requesting halfop from %s in %s %s.', chanserv, channel, on)\n            irc.sendMsg(ircmsgs.privmsg(chanserv, ('halfop %s' % channel)))\n    if (chanserv and self.registryValue('ChanServ.voice', channel)):\n        if (irc.nick not in irc.state.channels[channel].voices):\n            self.log.info('Requesting voice from %s in %s %s.', chanserv, channel, on)\n            irc.sendMsg(ircmsgs.privmsg(chanserv, ('voice %s' % channel)))\n", "label": 1}
{"function": "\n\ndef init(self):\n    self.one_of_many = None\n    if self.search_for_opcodes:\n        self.magic_files = [self.config.settings.user.binarch, self.config.settings.system.binarch]\n    if (((not self.magic_files) and (not self.raw_bytes)) or self.explicit_signature_scan):\n        self.magic_files += (self.config.settings.user.magic + self.config.settings.system.magic)\n    self.magic = binwalk.core.magic.Magic(include=self.include_filters, exclude=self.exclude_filters, invalid=self.show_invalid)\n    if self.raw_bytes:\n        raw_signatures = []\n        for raw_bytes in self.raw_bytes:\n            raw_signatures.append(('0    string    %s    %s' % (raw_bytes, raw_bytes)))\n        binwalk.core.common.debug(('Parsing raw signatures: %s' % str(raw_signatures)))\n        self.magic.parse(raw_signatures)\n    if self.magic_files:\n        binwalk.core.common.debug(('Loading magic files: %s' % str(self.magic_files)))\n        for f in self.magic_files:\n            self.magic.load(f)\n    self.VERBOSE = ['Signatures:', len(self.magic.signatures)]\n", "label": 1}
{"function": "\n\ndef init(args):\n    encoding = args.get('--encoding')\n    extra_ignore_dirs = args.get('--ignore')\n    if extra_ignore_dirs:\n        extra_ignore_dirs = extra_ignore_dirs.split(',')\n    candidates = get_all_imports(args['<path>'], encoding=encoding, extra_ignore_dirs=extra_ignore_dirs)\n    candidates = get_pkg_names(candidates)\n    logging.debug(('Found imports: ' + ', '.join(candidates)))\n    pypi_server = 'https://pypi.python.org/pypi/'\n    proxy = None\n    if args['--pypi-server']:\n        pypi_server = args['--pypi-server']\n    if args['--proxy']:\n        proxy = {\n            'http': args['--proxy'],\n            'https': args['--proxy'],\n        }\n    if args['--use-local']:\n        logging.debug('Getting package information ONLY from local installation.')\n        imports = get_import_local(candidates, encoding=encoding)\n    else:\n        logging.debug('Getting packages information from Local/PyPI')\n        local = get_import_local(candidates, encoding=encoding)\n        difference = [x for x in candidates if (x.lower() not in [z['name'].lower() for z in local])]\n        imports = (local + get_imports_info(difference, proxy=proxy, pypi_server=pypi_server))\n    path = (args['--savepath'] if args['--savepath'] else os.path.join(args['<path>'], 'requirements.txt'))\n    if ((not args['--savepath']) and (not args['--force']) and os.path.exists(path)):\n        logging.warning('Requirements.txt already exists, use --force to overwrite it')\n        return\n    generate_requirements_file(path, imports)\n    logging.info(('Successfully saved requirements file in ' + path))\n", "label": 1}
{"function": "\n\ndef _replacement(self, node):\n    if isinstance(node.left, ast.Constant):\n        if ((node.type == 'and') and (node.left.value == True)):\n            return node.right\n        if ((node.type == 'or') and (node.left.value == False)):\n            return node.right\n    elif isinstance(node.right, ast.Constant):\n        if ((node.type == 'and') and (node.right.value == True)):\n            return node.left\n        if ((node.type == 'or') and (node.right.value == False)):\n            return node.left\n    return None\n", "label": 1}
{"function": "\n\ndef _translate_movs_suffix(self, tb, instruction, suffix):\n    if (self._arch_mode == ARCH_X86_MODE_32):\n        src = ReilRegisterOperand('esi', 32)\n        dst = ReilRegisterOperand('edi', 32)\n    elif (self._arch_mode == ARCH_X86_MODE_64):\n        src = ReilRegisterOperand('rsi', 64)\n        dst = ReilRegisterOperand('rdi', 64)\n    else:\n        raise Exception('Invalid architecture mode: %d', self._arch_mode)\n    if (suffix == 'b'):\n        data_size = 8\n    elif (suffix == 'w'):\n        data_size = 16\n    elif (suffix == 'd'):\n        data_size = 32\n    elif (suffix == 'q'):\n        data_size = 64\n    else:\n        raise Exception(('Invalid instruction suffix: %s' % suffix))\n    if instruction.prefix:\n        (counter, loop_start_lbl) = self._rep_prefix_begin(tb, instruction)\n    tmp0 = tb.temporal(data_size)\n    tb.add(self._builder.gen_ldm(src, tmp0))\n    tb.add(self._builder.gen_stm(tmp0, dst))\n    self._update_strings_src_and_dst(tb, src, dst, data_size)\n    if instruction.prefix:\n        self._rep_prefix_end(tb, instruction, counter, loop_start_lbl)\n", "label": 1}
{"function": "\n\ndef ask_for_package(self, class_name, package=None, default_value=''):\n    StatusManager().hide_status('ask_package_description')\n    class_path = '.'.join([str(x) for x in [package, class_name] if x])\n    if (not package):\n        self.organize_step_four()\n    elif isinstance(package, int):\n        sublime.set_timeout((lambda : self.view.window().show_input_panel(('Package for class \"%s\":' % class_name), default_value, (lambda pkg: self.ask_for_package(class_name, pkg)), (lambda pkg: self.ask_description(pkg, class_name)), (lambda : self.ask_for_package(class_name, None)))), 10)\n    elif JavaUtils().is_class_path(class_path):\n        if (class_path and (class_path not in self.importedTypes)):\n            self.importedTypes.append(class_path)\n        self.organize_step_four()\n    else:\n        sublime.error_message('Invalid package naming')\n        self.ask_for_package(class_name, (- 1), package)\n", "label": 0}
{"function": "\n\ndef _get_vpn_gateways_external_ips(context, neutron):\n    vpcs = {vpc['id']: vpc for vpc in db_api.get_items(context, 'vpc')}\n    external_ips = {\n        \n    }\n    routers = neutron.list_routers(tenant_id=context.project_id)['routers']\n    for router in routers:\n        info = router['external_gateway_info']\n        if info:\n            for ip in info['external_fixed_ips']:\n                if netaddr.valid_ipv4(ip['ip_address']):\n                    external_ips[router['id']] = ip['ip_address']\n    return {vgw['id']: external_ips.get(vpcs[vgw['vpc_id']]['os_id']) for vgw in db_api.get_items(context, 'vgw') if vgw['vpc_id']}\n", "label": 0}
{"function": "\n\ndef load_config(config_file=None, **kwargs):\n    '\\n    Load the configuration for a given file object or name\\n\\n    The config_file can either be a file object, string or None. If it is None\\n    the default `mkdocs.yml` filename will loaded.\\n\\n    Extra kwargs are passed to the configuration to replace any default values\\n    unless they themselves are None.\\n    '\n    options = kwargs.copy()\n    for (key, value) in options.copy().items():\n        if (value is None):\n            options.pop(key)\n    config_file = _open_config_file(config_file)\n    options['config_file_path'] = getattr(config_file, 'name', '')\n    from mkdocs import config\n    cfg = Config(schema=config.DEFAULT_SCHEMA)\n    cfg.load_file(config_file)\n    cfg.load_dict(options)\n    (errors, warnings) = cfg.validate()\n    for (config_name, warning) in warnings:\n        log.warning(\"Config value: '%s'. Warning: %s\", config_name, warning)\n    for (config_name, error) in errors:\n        log.error(\"Config value: '%s'. Error: %s\", config_name, error)\n    for (key, value) in cfg.items():\n        log.debug(\"Config value: '%s' = %r\", key, value)\n    if (len(errors) > 0):\n        raise exceptions.ConfigurationError('Aborted with {0} Configuration Errors!'.format(len(errors)))\n    elif (cfg['strict'] and (len(warnings) > 0)):\n        raise exceptions.ConfigurationError(\"Aborted with {0} Configuration Warnings in 'strict' mode!\".format(len(warnings)))\n    return cfg\n", "label": 1}
{"function": "\n\ndef test_urls(self):\n    api = Api()\n    api.register(NoteResource())\n    api.register(UserResource())\n    patterns = api.urls\n    self.assertEqual(len(patterns), 3)\n    self.assertEqual(sorted([pattern.name for pattern in patterns if hasattr(pattern, 'name')]), ['api_v1_top_level'])\n    self.assertEqual([[pattern.name for pattern in include.url_patterns if hasattr(pattern, 'name')] for include in patterns if hasattr(include, 'reverse_dict')], [['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail'], ['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail']])\n    api = Api(api_name='v2')\n    api.register(NoteResource())\n    api.register(UserResource())\n    patterns = api.urls\n    self.assertEqual(len(patterns), 3)\n    self.assertEqual(sorted([pattern.name for pattern in patterns if hasattr(pattern, 'name')]), ['api_v2_top_level'])\n    self.assertEqual([[pattern.name for pattern in include.url_patterns if hasattr(pattern, 'name')] for include in patterns if hasattr(include, 'reverse_dict')], [['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail'], ['api_dispatch_list', 'api_get_schema', 'api_get_multiple', 'api_dispatch_detail']])\n", "label": 1}
{"function": "\n\ndef main(argv=None):\n    args = parse_args((argv or sys.argv))\n    if (not args):\n        exit(1)\n    config_filename = os.path.join(config.config_dir(), confit.CONFIG_FILENAME)\n    if (not os.path.exists(config_filename)):\n        print(('Missing configuration file %s.' % config_filename))\n    if (not os.path.exists(config['link_root_dir'].as_filename())):\n        print(('Error: links root directory \"%s\" does not exist.' % config['link_root_dir']))\n        exit(1)\n    linkers = [Linker(field) for field in args['by']]\n    for fpath in recursive_glob(args['media_src']):\n        item = brain.search_filename(fpath, args['by'])\n        if item:\n            for linker in linkers:\n                linker.flink(item)\n        else:\n            print(('No Open Movie Database matching for %s' % fpath))\n", "label": 1}
{"function": "\n\n@register.inclusion_tag('org/box_plache.html')\ndef box_plache(diarys, user):\n    'summarize paid, unpaid time for given period of diaries'\n    list = {\n        \n    }\n    paylist = {\n        \n    }\n    billable_hours = 0\n    total_hours = 0\n    sum = 0\n    for o in diarys:\n        a = o.author.username\n        if (a in list):\n            list[a] += o.length.hour\n        else:\n            list[a] = o.length.hour\n            paylist[a] = 0\n        total_hours += o.length.hour\n        if ((o.task.id == 22) or ((o.task.id == 23) and (o.event != None) and (o.event.require_technician or o.event.require_video))):\n            paylist[a] += o.length.hour\n            billable_hours += o.length.hour\n    tariff = 3.5\n    objects = []\n    for o in list:\n        a = {\n            \n        }\n        a['name'] = o\n        a['time'] = list[o]\n        a['paytime'] = paylist[o]\n        a['money'] = (paylist[o] * tariff)\n        objects.append(a)\n        sum += (paylist[o] * tariff)\n    objects.sort((lambda a, b: int((b['money'] - a['money']))))\n    return {\n        'objects': objects,\n        'user': user,\n        'billable_hours': billable_hours,\n        'total_hours': total_hours,\n        'sum': sum,\n    }\n", "label": 1}
{"function": "\n\ndef _get_objects(self, show=None, order_by=None, name=None, func_name=None, silk_request=None, filters=None):\n    if (not filters):\n        filters = []\n    if (not show):\n        show = self.default_show\n    manager = Profile.objects\n    if silk_request:\n        query_set = manager.filter(request=silk_request)\n    else:\n        query_set = manager.all()\n    if (not order_by):\n        order_by = self.defualt_order_by\n    if (order_by == 'Recent'):\n        query_set = query_set.order_by('-start_time')\n    elif (order_by == 'Name'):\n        query_set = query_set.order_by('-name')\n    elif (order_by == 'Function Name'):\n        query_set = query_set.order_by('-func_name')\n    elif (order_by == 'Num. Queries'):\n        query_set = query_set.annotate(num_queries=Count('queries')).order_by('-num_queries')\n    elif (order_by == 'Time'):\n        query_set = query_set.order_by('-time_taken')\n    elif (order_by == 'Time on queries'):\n        query_set = query_set.annotate(db_time=Sum('queries__time_taken')).order_by('-db_time')\n    elif order_by:\n        raise RuntimeError(('Unknown order_by: \"%s\"' % order_by))\n    if func_name:\n        query_set = query_set.filter(func_name=func_name)\n    if name:\n        query_set = query_set.filter(name=name)\n    for f in filters:\n        query_set = f.contribute_to_query_set(query_set)\n        query_set = query_set.filter(f)\n    return list(query_set[:show])\n", "label": 1}
{"function": "\n\ndef _doGhost(self, irc, nick=None):\n    if self.disabled(irc):\n        return\n    if (nick is None):\n        nick = self._getNick(irc.network)\n    if (nick not in self.registryValue('nicks')):\n        return\n    nickserv = self.registryValue('NickServ')\n    password = self._getNickServPassword(nick)\n    ghostDelay = self.registryValue('ghostDelay')\n    if (not ghostDelay):\n        return\n    if ((not nickserv) or (not password)):\n        s = 'Tried to ghost without a NickServ or password set.'\n        self.log.warning(s)\n        return\n    if (self.sentGhost and (time.time() < (self.sentGhost + ghostDelay))):\n        self.log.warning(('Refusing to send GHOST more than once every %s seconds.' % ghostDelay))\n    elif (not password):\n        self.log.warning('Not ghosting: no password set.')\n        return\n    else:\n        self.log.info('Sending ghost (current nick: %s; ghosting: %s)', irc.nick, nick)\n        ghost = ('GHOST %s %s' % (nick, password))\n        irc.sendMsg(ircmsgs.privmsg(nickserv, ghost))\n        self.sentGhost = time.time()\n", "label": 1}
{"function": "\n\ndef meijerint_indefinite(f, x):\n    '\\n    Compute an indefinite integral of ``f`` by rewriting it as a G function.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.integrals.meijerint import meijerint_indefinite\\n    >>> from sympy import sin\\n    >>> from sympy.abc import x\\n    >>> meijerint_indefinite(sin(x), x)\\n    -cos(x)\\n    '\n    from sympy import hyper, meijerg\n    results = []\n    for a in sorted((_find_splitting_points(f, x) | {S(0)}), key=default_sort_key):\n        res = _meijerint_indefinite_1(f.subs(x, (x + a)), x)\n        if (not res):\n            continue\n        res = res.subs(x, (x - a))\n        if _has(res, hyper, meijerg):\n            results.append(res)\n        else:\n            return res\n    if f.has(HyperbolicFunction):\n        _debug('Try rewriting hyperbolics in terms of exp.')\n        rv = meijerint_indefinite(_rewrite_hyperbolics_as_exp(f), x)\n        if rv:\n            if (not (type(rv) is list)):\n                return collect(factor_terms(rv), rv.atoms(exp))\n            results.extend(rv)\n    if results:\n        return next(ordered(results))\n", "label": 0}
{"function": "\n\ndef update(self, surface):\n    pa = pygame.PixelArray(surface)\n    if hasattr(self.machine, 'lights'):\n        for light in self.machine.lights:\n            if (light.x and light.y):\n                if pa[(light.x, light.y)]:\n                    light.on()\n                else:\n                    light.off()\n    if hasattr(self.machine, 'leds'):\n        for led in self.machine.leds:\n            pass\n", "label": 0}
{"function": "\n\ndef is_valid(self):\n    missing = (self.depends_on - set(self.context.keys()))\n    if missing:\n        raise exceptions.WorkflowValidationError(('Unable to complete the workflow. The values %s are required but not present.' % ', '.join(missing)))\n    checked_steps = []\n    if ('general_processes' in self.context):\n        checked_steps = self.context['general_processes']\n    enabled_services = set([])\n    for process_name in checked_steps:\n        enabled_services.add(str(process_name).split(':')[0])\n    steps_valid = True\n    for step in self.steps:\n        process_name = str(getattr(step, 'process_name', None))\n        if ((process_name not in enabled_services) and (not isinstance(step, GeneralConfig))):\n            continue\n        if (not step.action.is_valid()):\n            steps_valid = False\n            step.has_errors = True\n    if (not steps_valid):\n        return steps_valid\n    return self.validate(self.context)\n", "label": 1}
{"function": "\n\ndef get_driver_info(self):\n    ' Return list of driver info dictionaries. '\n    top = self._cfg_map.keys()[0].parent\n    while top.parent:\n        top = top.parent\n    prefix_drop = 0\n    driver_info = []\n    for (driver, (ins, outs)) in sorted(self._cfg_map.items(), key=(lambda item: item[0].get_pathname())):\n        name = driver.get_pathname()[prefix_drop:]\n        info = dict(name=name, _id=id(driver), recording=(ins + outs))\n        if hasattr(driver, 'get_parameters'):\n            info['parameters'] = [str(param) for param in driver.get_parameters().values()]\n        if hasattr(driver, 'eval_objectives'):\n            info['objectives'] = [key for key in driver.get_objectives()]\n        if hasattr(driver, 'eval_responses'):\n            info['responses'] = [key for key in driver.get_responses()]\n        if hasattr(driver, 'get_eq_constraints'):\n            info['eq_constraints'] = [str(con) for con in driver.get_eq_constraints().values()]\n        if hasattr(driver, 'get_ineq_constraints'):\n            info['ineq_constraints'] = [str(con) for con in driver.get_ineq_constraints().values()]\n        driver_info.append(info)\n    return driver_info\n", "label": 1}
{"function": "\n\ndef _split_command(self):\n    self._words = []\n    self._word_index = None\n    cursor = 1\n    breaks = re.escape(self._word_breaks)\n    matches = re.findall(('([^%s]*)([$%s]*)' % (breaks, breaks)), self._command_line)\n    if (not matches):\n        return\n    for (index, match) in enumerate(matches):\n        whole_match = (match[0] + match[1])\n        cursor += len(whole_match)\n        word = match[0]\n        if ((self._word_index is None) and (cursor >= self._char_index)):\n            self._word_index = index\n            cursor_word_offset = (self._char_index - ((cursor - len(match[1])) - 1))\n            if (cursor_word_offset < 0):\n                word = word[:(len(word) + cursor_word_offset)]\n            elif (cursor_word_offset > 0):\n                self._word_index += 1\n                self._words.append(word)\n                self._words.append('')\n                continue\n        if word:\n            self._words.append(word)\n    if (self._word_index > (len(self._words) - 1)):\n        self._word_index = (len(self._words) - 1)\n", "label": 1}
{"function": "\n\ndef email(self, comment, content_object, request):\n    content_object = comment.content_object\n    if content_object.blog.editorial_moderators:\n        editorial_moderators = content_object.blog.editorial_moderators\n        editor_emails = []\n        for editor in editorial_moderators.all():\n            eds = editor.tenure_set.filter(current=True)\n            for ed in eds.all():\n                if ed.editor.email:\n                    editor_emails.append(ed.editor.email)\n    if content_object.blog.staff_moderators:\n        staff_moderators = content_object.blog.staff_moderators\n        staff_emails = []\n        for staffer in staff_moderators.all():\n            staff_emails.append(staffer.email)\n    moderators = content_object.blog.moderators\n    moderators = moderators.split(',')\n    for each_moderator in moderators:\n        each_moderator = each_moderator.strip()\n    moderators.extend(staff_emails)\n    moderators.extend(editor_emails)\n    context = {\n        'comment': comment,\n        'content_object': content_object,\n    }\n    subject = ('New comment awaiting moderation on \"%s\"' % content_object)\n    render_email_and_send(context=context, message_template='blogs/comment_notification_email.txt', subject=subject, recipients=moderators)\n", "label": 0}
{"function": "\n\ndef cli():\n    args = arg_setup()\n    ez = EZMomi(**vars(args))\n    kwargs = vars(args)\n    if (kwargs['mode'] == 'list'):\n        ez.list_objects()\n    elif (kwargs['mode'] == 'clone'):\n        ez.clone()\n    elif (kwargs['mode'] == 'destroy'):\n        ez.destroy()\n    elif (kwargs['mode'] == 'listSnapshots'):\n        ez.listSnapshots()\n    elif (kwargs['mode'] == 'createSnapshot'):\n        ez.createSnapshot()\n    elif (kwargs['mode'] == 'removeSnapshot'):\n        ez.removeSnapshot()\n    elif (kwargs['mode'] == 'revertSnapshot'):\n        ez.revertSnapshot()\n    elif (kwargs['mode'] == 'status'):\n        ez.status()\n    elif (kwargs['mode'] == 'shutdown'):\n        ez.shutdown()\n    elif (kwargs['mode'] == 'powerOff'):\n        ez.powerOff()\n    elif (kwargs['mode'] == 'powerOn'):\n        ez.powerOn()\n", "label": 1}
{"function": "\n\ndef _scan_file(filename, sentinel, source_type='import'):\n    'Generator that performs the actual scanning of files.\\n\\n    Yeilds a tuple containing import type, import path, and an extra file\\n    that should be scanned. Extra file scans should be the file or directory\\n    that relates to the import name.\\n    '\n    filename = os.path.abspath(filename)\n    real_filename = os.path.realpath(filename)\n    if (os.path.getsize(filename) <= max_file_size):\n        if ((real_filename not in sentinel) and os.path.isfile(filename)):\n            sentinel.add(real_filename)\n            basename = os.path.basename(filename)\n            (scope, imports) = ast_scan_file(filename)\n            if ((scope is not None) and (imports is not None)):\n                for imp in imports:\n                    (yield (source_type, imp.module, None))\n                if (('INSTALLED_APPS' in scope) and (basename == 'settings.py')):\n                    log.info('Found Django settings: %s', filename)\n                    for item in django.handle_django_settings(filename):\n                        (yield item)\n            else:\n                log.warn('Could not scan imports from: %s', filename)\n    else:\n        log.warn('File size too large: %s', filename)\n", "label": 1}
{"function": "\n\ndef get_error_html(self, status_code, **kwargs):\n    self.set_header('Content-Type', 'application/json')\n    if (status_code == 400):\n        resp = {\n            'error': 400,\n            'reason': 'bad_request',\n        }\n    elif (status_code == 404):\n        resp = {\n            'error': 404,\n            'reason': 'not_found',\n        }\n    elif (status_code == 409):\n        resp = {\n            'error': 409,\n            'reason': 'conflict',\n        }\n    elif (status_code == 401):\n        resp = {\n            'error': 401,\n            'reason': 'unauthorized',\n        }\n    elif (status_code == 403):\n        resp = {\n            'error': 403,\n            'reason': 'forbidden',\n        }\n    else:\n        resp = {\n            'error': status_code,\n            'reason': httplib.responses[status_code],\n        }\n    if (self.settings.get('debug') and ('exc_info' in kwargs)):\n        exc_info = traceback.format_exception(*kwargs['exc_info'])\n        resp['exc_info'] = exc_info\n    return json.dumps(resp)\n", "label": 0}
{"function": "\n\ndef _handle_exception(self, invocation, error, traceback=None):\n    if self.logger.isEnabledFor(logging.DEBUG):\n        self.logger.debug('Got exception for request %s: %s: %s', invocation.request, type(error).__name__, error)\n    if isinstance(error, (AuthenticationError, IOError, HazelcastInstanceNotActiveError)):\n        if self._try_retry(invocation):\n            return\n    if is_retryable_error(error):\n        if (invocation.request.is_retryable() or self._is_redo_operation):\n            if self._try_retry(invocation):\n                return\n    invocation.set_exception(error, traceback)\n", "label": 0}
{"function": "\n\ndef set_cookie(self, name, value, domain=None, expires=None, path='/', expires_days=None, **kwargs):\n    'Sets the given cookie name/value with the given options.\\n\\n        Additional keyword arguments are set on the Cookie.Morsel\\n        directly.\\n        See http://docs.python.org/library/cookie.html#morsel-objects\\n        for available attributes.\\n        '\n    name = escape.native_str(name)\n    value = escape.native_str(value)\n    if re.search('[\\\\x00-\\\\x20]', (name + value)):\n        raise ValueError(('Invalid cookie %r: %r' % (name, value)))\n    if (not hasattr(self, '_new_cookie')):\n        self._new_cookie = Cookie.SimpleCookie()\n    if (name in self._new_cookie):\n        del self._new_cookie[name]\n    self._new_cookie[name] = value\n    morsel = self._new_cookie[name]\n    if domain:\n        morsel['domain'] = domain\n    if ((expires_days is not None) and (not expires)):\n        expires = (datetime.datetime.utcnow() + datetime.timedelta(days=expires_days))\n    if expires:\n        morsel['expires'] = httputil.format_timestamp(expires)\n    if path:\n        morsel['path'] = path\n    for (k, v) in kwargs.items():\n        if (k == 'max_age'):\n            k = 'max-age'\n        morsel[k] = v\n", "label": 1}
{"function": "\n\ndef generic_done(self, result):\n    if (self.may_change_files and self.active_view() and self.active_view().file_name()):\n        if self.active_view().is_dirty():\n            result = 'WARNING: Current view is dirty.\\n\\n'\n        else:\n            print('reverting')\n            position = self.active_view().viewport_position()\n            self.active_view().run_command('revert')\n            do_when((lambda : (not self.active_view().is_loading())), (lambda : self.active_view().set_viewport_position(position, False)))\n    view = self.active_view()\n    if (view and view.settings().get('live_git_annotations')):\n        self.view.run_command('git_annotate')\n    if (not result.strip()):\n        return\n    self.panel(result)\n", "label": 0}
{"function": "\n\ndef clean_target_dict(data):\n    \"\\n    There are many unimplemented fields throughout parselmouth,\\n    for example some types of targeting are not yet implemented.\\n    In these cases, we must clean the dictionary before we pass\\n    it back to DFP. Fields like X.Type exist in many places, and these\\n    must be replaced with 'xsi_type' for exaample. Also, unneeded fields\\n    sometimes raise errors when passed back to DFP.\\n\\n    @param data: dict\\n    @return: dict\\n    \"\n    if (not isinstance(data, dict)):\n        return data\n    at_bottom = True\n    cln_dict = {\n        \n    }\n    for (_key, _val) in data.items():\n        if isinstance(_val, dict):\n            at_bottom = False\n            cln_dict[_key] = clean_target_dict(_val)\n        elif isinstance(_val, list):\n            at_bottom = False\n            cln_dict[_key] = [clean_target_dict(d) for d in _val]\n        elif (_key.find('_Type') >= 0):\n            cln_dict['xsi_type'] = _val\n        else:\n            cln_dict[_key] = _val\n    if at_bottom:\n        clean_dict = {\n            \n        }\n        for (_key, _val) in cln_dict.items():\n            if ((_key.find('id') >= 0) or (_key.find('Id') >= 0)):\n                clean_dict[_key] = _val\n            elif (_key == 'xsi_type'):\n                clean_dict[_key] = _val\n    else:\n        clean_dict = cln_dict\n    return clean_dict\n", "label": 1}
{"function": "\n\ndef finish_task(dsk, key, state, results, sortkey, delete=True, release_data=release_data):\n    '\\n    Update execution state after a task finishes\\n\\n    Mutates.  This should run atomically (with a lock).\\n    '\n    for dep in sorted(state['dependents'][key], key=sortkey, reverse=True):\n        s = state['waiting'][dep]\n        s.remove(key)\n        if (not s):\n            del state['waiting'][dep]\n            state['ready'].append(dep)\n    for dep in state['dependencies'][key]:\n        if (dep in state['waiting_data']):\n            s = state['waiting_data'][dep]\n            s.remove(key)\n            if ((not s) and (dep not in results)):\n                if DEBUG:\n                    from chest.core import nbytes\n                    print(('Key: %s\\tDep: %s\\t NBytes: %.2f\\t Release' % (key, dep, sum((map(nbytes, state['cache'].values()) / 1000000.0)))))\n                release_data(dep, state, delete=delete)\n        elif (delete and (dep not in results)):\n            release_data(dep, state, delete=delete)\n    state['finished'].add(key)\n    state['running'].remove(key)\n    return state\n", "label": 1}
{"function": "\n\ndef _fix_unknown_dimension(self, input_shape, output_shape):\n    'Find and replace a single missing dimension in an output shape\\n        given an input shape.\\n\\n        A near direct port of the internal numpy function _fix_unknown_dimension\\n        in numpy/core/src/multiarray/shape.c\\n\\n        # Arguments\\n            input_shape: shape of array being reshaped\\n\\n            output_shape: desired shape of the array with at most\\n                a single -1 which indicates a dimension that should be\\n                derived from the input shape.\\n\\n        # Returns\\n            The new output shape with a -1 replaced with its computed value.\\n\\n            Raises a ValueError if the total array size of the output_shape is\\n            different then the input_shape, or more then one unknown dimension\\n            is specified.\\n        '\n    output_shape = list(output_shape)\n    msg = 'total size of new array must be unchanged'\n    (known, unknown) = (1, None)\n    for (index, dim) in enumerate(output_shape):\n        if (dim < 0):\n            if (unknown is None):\n                unknown = index\n            else:\n                raise ValueError('can only specify one unknown dimension')\n        else:\n            known *= dim\n    original = np.prod(input_shape, dtype=int)\n    if (unknown is not None):\n        if ((known == 0) or ((original % known) != 0)):\n            raise ValueError(msg)\n        output_shape[unknown] = (original // known)\n    elif (original != known):\n        raise ValueError(msg)\n    return tuple(output_shape)\n", "label": 0}
{"function": "\n\ndef get_db_prep_lookup(self, lookup_type, value, connection, prepared=False):\n    if (not prepared):\n        value = self.get_prep_lookup(lookup_type, value)\n    if hasattr(value, 'get_compiler'):\n        value = value.get_compiler(connection=connection)\n    if (hasattr(value, 'as_sql') or hasattr(value, '_as_sql')):\n        if hasattr(value, 'relabel_aliases'):\n            return value\n        if hasattr(value, 'as_sql'):\n            (sql, params) = value.as_sql()\n        else:\n            (sql, params) = value._as_sql(connection=connection)\n        return QueryWrapper(('(%s)' % sql), params)\n    if (lookup_type in ['exact', 'gt', 'lt', 'gte', 'lte']):\n        return [self._pk_trace(value, 'get_db_prep_lookup', lookup_type, connection=connection, prepared=prepared)]\n    if (lookup_type in ('range', 'in')):\n        return [self._pk_trace(v, 'get_db_prep_lookup', lookup_type, connection=connection, prepared=prepared) for v in value]\n    elif (lookup_type == 'isnull'):\n        return []\n    raise TypeError(('Related Field has invalid lookup: %s' % lookup_type))\n", "label": 1}
{"function": "\n\ndef convert_iob_to_iobes(iob_tags):\n    'Converts a sequence of IOB tags into IOBES tags.'\n    iobes_tags = []\n    for (tag, next_tag) in zip(iob_tags, (iob_tags[1:] + [None])):\n        if (tag == 'O'):\n            iobes_tags.append('O')\n        elif tag.startswith('B'):\n            if ((next_tag is not None) and next_tag.startswith('I')):\n                iobes_tags.append(tag)\n            else:\n                iobes_tags.append(('S-%s' % tag[2:]))\n        elif tag.startswith('I'):\n            if (next_tag == tag):\n                iobes_tags.append(tag)\n            else:\n                iobes_tags.append(('E-%s' % tag[2:]))\n        else:\n            raise ValueError(('Unknown tag: %s' % tag))\n    return iobes_tags\n", "label": 0}
{"function": "\n\n@dispatch(Slice, (Select, Selectable, ColumnElement))\ndef compute_up(expr, data, **kwargs):\n    index = expr.index[0]\n    if isinstance(index, slice):\n        start = (index.start or 0)\n        if (start < 0):\n            raise ValueError('start value of slice cannot be negative with a SQL backend')\n        stop = index.stop\n        if ((stop is not None) and (stop < 0)):\n            raise ValueError('stop value of slice cannot be negative with a SQL backend.')\n        if ((index.step is not None) and (index.step != 1)):\n            raise ValueError('step parameter in slice objects not supported with SQL backend')\n    elif isinstance(index, (np.integer, numbers.Integral)):\n        if (index < 0):\n            raise ValueError('integer slice cannot be negative for the SQL backend')\n        start = index\n        stop = (start + 1)\n    else:\n        raise TypeError(('type %r not supported for slicing wih SQL backend' % type(index).__name__))\n    warnings.warn('The order of the result set from a Slice expression computed against the SQL backend is not deterministic.')\n    if (stop is None):\n        return select(data).offset(start)\n    else:\n        return select(data).offset(start).limit((stop - start))\n", "label": 1}
{"function": "\n\ndef statuses_resolve_uids(twitter, tl):\n    'Resolve user ids to screen names from statuses.'\n    user_ids = []\n    for t in tl:\n        rt = t.get('retweeted_status')\n        if (rt and (not rt['user'].get('screen_name'))):\n            user_ids.append(rt['user']['id'])\n        if (not t['user'].get('screen_name')):\n            user_ids.append(t['user']['id'])\n    names = lookup(twitter, list(set(user_ids)))\n    new_tl = []\n    for t in tl:\n        rt = t.get('retweeted_status')\n        if (rt and (not rt['user'].get('screen_name'))):\n            name = names[rt['user']['id']]\n            t['retweeted_status']['user']['screen_name'] = name\n        if (not t['user'].get('screen_name')):\n            name = names[t['user']['id']]\n            t['user']['screen_name'] = name\n        new_tl.append(t)\n    return new_tl\n", "label": 1}
{"function": "\n\n@classmethod\ndef setUpClass(cls):\n    rc = controller.RootController()\n    actual_out = util.capture_stdout(rc.execute, ['show', 'statistics'])\n    TestShowStatistics.output_list = test_util.get_separate_output(actual_out, 'Statistics')\n    TestShowStatistics.is_bar_present = False\n    for item in TestShowStatistics.output_list:\n        if ('~~test Bin Statistics~~' in item):\n            TestShowStatistics.test_bin_stats = item\n        elif ('~~bar Bin Statistics~~' in item):\n            TestShowStatistics.bar_bin_stats = item\n            TestShowStatistics.is_bar_present = True\n        elif ('~~Service Statistics~~' in item):\n            TestShowStatistics.service_stats = item\n        elif ('~~bar Namespace Statistics~~' in item):\n            TestShowStatistics.bar_namespace_stats = item\n            TestShowStatistics.is_bar_present = True\n        elif ('~~test Namespace Statistics~~' in item):\n            TestShowStatistics.test_namespace_stats = item\n        elif ('~~XDR Statistics~~' in item):\n            TestShowStatistics.xdr_stats = item\n", "label": 0}
{"function": "\n\ndef _find(self):\n    isect = self.intersection\n    slop = self.slop\n    current = []\n    while ((not current) and (isect.id is not None)):\n        poses = self._poses()\n        current = poses[0]\n        for poslist in poses[1:]:\n            newpositions = []\n            for newpos in poslist:\n                start = bisect_left(current, (newpos - slop))\n                end = bisect_right(current, newpos)\n                for curpos in current[start:end]:\n                    delta = (newpos - curpos)\n                    if (delta <= slop):\n                        newpositions.append(newpos)\n            current = newpositions\n            if (not current):\n                break\n        if (not current):\n            isect.next()\n    self.count = len(current)\n    self.id = isect.id\n", "label": 1}
{"function": "\n\ndef subscribe(self, callback, preroll_grpos_range=0, preroll_frames=0, preroll_from_frame=None, flag_mask=0):\n    pos = None\n    if ((preroll_grpos_range > 0) or (preroll_frames > 0)):\n        raw_pos = self._scan_from_end(preroll_grpos_range, frames=preroll_frames, flag_mask=flag_mask)\n        if (raw_pos is not None):\n            pos = (raw_pos - self._s.data_offset)\n    elif (preroll_from_frame is not None):\n        pos = (preroll_from_frame - self._s.data_offset)\n        if (not (0 <= pos < len(self._s.data))):\n            e = InvalidFrameNumber(('frame %r' % preroll_from_frame))\n            return defer.fail(e)\n    if (pos is not None):\n        for f in self._s.data[pos:]:\n            callback(*f)\n    self._s.data_listeners.add(callback)\n    return defer.succeed(callback)\n", "label": 0}
{"function": "\n\ndef fill(self, char=' ', fgcolor=None, bgcolor=None, region=None):\n    (x, y, width, height) = self.getregion(region)\n    if ((x, y, width, height) == (None, None, None, None)):\n        return\n    fgcolor = (((fgcolor is not None) and getpygamecolor(fgcolor)) or self._fgcolor)\n    bgcolor = (((bgcolor is not None) and getpygamecolor(bgcolor)) or self._bgcolor)\n    for ix in range(x, (x + width)):\n        for iy in range(y, (y + height)):\n            if (char is not None):\n                self._screenchar[ix][iy] = char\n            if (fgcolor is not None):\n                self._screenfgcolor[ix][iy] = fgcolor\n            if (bgcolor is not None):\n                self._screenbgcolor[ix][iy] = bgcolor\n            self._screendirty[ix][iy] = True\n    if self._autoupdate:\n        self.update()\n", "label": 1}
{"function": "\n\ndef getUpperLeftY(self, height, y, yPrev, yNext, yMin, yMax, yMid, yMouse):\n    if (AnnotationLocation.AT_THE_MOUSE == self.location):\n        if (GChartConsts.NAI == yMouse):\n            result = Double.NaN\n        else:\n            result = yMouse\n    elif (AnnotationLocation.AT_THE_MOUSE_SNAP_TO_X == self.location):\n        if (GChartConsts.NAI == yMouse):\n            result = Double.NaN\n        else:\n            result = yMouse\n    elif (AnnotationLocation.AT_THE_MOUSE_SNAP_TO_Y == self.location):\n        if (GChartConsts.NAI == yMouse):\n            result = Double.NaN\n        else:\n            result = y\n    elif ((AnnotationLocation.NORTHWEST == self.location) or (AnnotationLocation.NORTH == self.location) or (AnnotationLocation.NORTHEAST == self.location)):\n        result = yMin\n    elif ((AnnotationLocation.SOUTHWEST == self.location) or (AnnotationLocation.SOUTH == self.location) or (AnnotationLocation.SOUTHEAST == self.location)):\n        result = yMax\n    else:\n        result = ((yMin + yMax) / 2)\n    return result\n", "label": 1}
{"function": "\n\ndef set_message(data):\n    '\\n    Set status and message on the given dictionary.\\n    The message is translated according to the hash map.\\n\\n    :param data: status and message hash/key pair\\n    :type data: dict\\n    :returns: the passed dict with \"message\" or \"errors\" field updated\\n    '\n    success = data['success']\n    message = ''\n    if success:\n        if (('message' in data) and (len(data['message']) == 2)):\n            hash_map = data['message'][0]\n            message_code = data['message'][1]\n            data['message'] = getattr(g, hash_map)(message_code)\n    elif (('errors' in data) and (len(data['errors']) > 0)):\n        error_tuple = data['errors'][0].get('code')\n        if (error_tuple and (len(error_tuple) == 2)):\n            hash_map = error_tuple[0]\n            error_code = error_tuple[1]\n            data['errors'][0]['code'] = error_code\n            data['errors'][0]['message'] = getattr(g, hash_map)(error_code)\n    return data\n", "label": 0}
{"function": "\n\ndef log_application_destroy(self, application_id):\n    ' Trace application destroy\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_APPLICATION_DESTROY in logger.events)):\n            data = {\n                \n            }\n            data['timestamp'] = time.time()\n            data['node_id'] = self.node.id\n            data['type'] = 'application_destroy'\n            data['application_id'] = application_id\n            if (logger.connection is not None):\n                if (not logger.connection.connection_lost):\n                    logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                else:\n                    disconnected.append(user_id)\n            elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                msg = {\n                    'cmd': 'logevent',\n                    'msgid': logger.handle,\n                    'header': None,\n                    'data': ('data: %s\\n\\n' % json.dumps(data)),\n                }\n                self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef create_context_assertions(self, include=None, exclude=None, exact=None):\n    '\\n        Returns a tuple of context assertion callbacks for the given arguments.\\n        \\n        '\n    include = (include or ())\n    exclude = (exclude or ())\n    exact = (exact if isinstance(exact, collections.Mapping) else dict((exact or {\n        \n    })))\n    return ((((functools.partial(self.assertContextIncludes, names=include),) if include else ()) + ((functools.partial(self.assertContextExcludes, names=exclude),) if exclude else ())) + ((functools.partial(self.assertContextExact, context=exact),) if exact else ()))\n", "label": 0}
{"function": "\n\ndef parse_ps_output(output):\n    'Parse the output of `ravtest ps` and return a list of\\n    (project, application, running) tuples.\\n    '\n    result = []\n    project = app = info = None\n    for line in output.splitlines():\n        line = line.strip()\n        if ((not line) and app):\n            result.append((project, app, info))\n            app = None\n            continue\n        if line.startswith('== '):\n            project = line[(line.find(':') + 3):(- 1)]\n        elif line.startswith('=== '):\n            app = line[(line.find(':') + 3):(- 1)]\n            info = {\n                \n            }\n        if (not app):\n            continue\n        if ('VMs' in line):\n            info['vms'] = int(line.split()[0])\n    return result\n", "label": 0}
{"function": "\n\ndef OnKey(self, event):\n    '        Key event handler. If the key is in the ASCII range, write it to the\\n        serial port. Newline handling and local echo is also done here.\\n        '\n    code = event.GetUnicodeKey()\n    if (code < 256):\n        code = event.GetKeyCode()\n    if (code == 13):\n        if self.settings.echo:\n            self.text_ctrl_output.AppendText('\\n')\n        if (self.settings.newline == NEWLINE_CR):\n            self.serial.write(b'\\r')\n        elif (self.settings.newline == NEWLINE_LF):\n            self.serial.write(b'\\n')\n        elif (self.settings.newline == NEWLINE_CRLF):\n            self.serial.write(b'\\r\\n')\n    else:\n        char = unichr(code)\n        if self.settings.echo:\n            self.WriteText(char)\n        self.serial.write(char.encode('UTF-8', 'replace'))\n", "label": 0}
{"function": "\n\ndef select(self, ensemble, x):\n    if ensemble.in_agreement(x):\n        return (Ensemble([ensemble.classifiers[0]]), None)\n    classifiers = ensemble.classifiers\n    [idx] = self.knn.kneighbors(x, return_distance=False)\n    (X, y) = (self.Xval[idx], self.yval[idx])\n    d = {\n        \n    }\n    scores = [clf.score(X, y) for clf in ensemble.classifiers]\n    for (i, scr) in enumerate(scores):\n        d[scr] = ((d[scr] + [i]) if (scr in d) else [i])\n    best_scores = sorted([k for k in d.iterkeys()], reverse=True)\n    if (len(d[best_scores[0]]) == 1):\n        i = d[best_scores[0]][0]\n        return (Ensemble([classifiers[i]]), None)\n    options = None\n    for (j, score) in enumerate(best_scores):\n        pred = [classifiers[i].predict(x) for i in d[score]]\n        pred = np.asarray(pred).flatten()\n        bincount = np.bincount(pred)\n        if (options != None):\n            for i in range(len(bincount)):\n                bincount[i] = (bincount[i] if (i in options) else 0)\n        imx = np.argmax(bincount)\n        votes = np.argwhere((bincount == bincount[imx])).flatten()\n        count = len(votes)\n        if (count == 1):\n            return (Ensemble([classifiers[np.argmax((pred == imx))]]), None)\n        elif (options == None):\n            options = votes\n    return (Ensemble([classifiers[np.argmax(scores)]]), None)\n", "label": 1}
{"function": "\n\ndef __call__(self):\n    'Calls the wrapped function through the lens of a CLI ran command'\n    for requirement in self.requires:\n        conclusion = requirement(request=sys.argv, module=self.api.module)\n        if (conclusion and (conclusion is not True)):\n            return self.output(conclusion)\n    pass_to_function = vars(self.parser.parse_known_args()[0])\n    for (option, directive) in self.directives.items():\n        arguments = ((self.defaults[option],) if (option in self.defaults) else ())\n        pass_to_function[option] = directive(*arguments, api=self.api, argparse=self.parser, interface=self)\n    if getattr(self, 'validate_function', False):\n        errors = self.validate_function(pass_to_function)\n        if errors:\n            return self.output(errors)\n    if hasattr(self.interface, 'karg'):\n        karg_values = pass_to_function.pop(self.interface.karg, ())\n        result = self.interface(*karg_values, **pass_to_function)\n    else:\n        result = self.interface(**pass_to_function)\n    return self.output(result)\n", "label": 1}
{"function": "\n\ndef print_dict(dct, dict_property='Property', wrap=0, dict_value='Value', json_flag=False):\n    'Print a `dict` as a table of two columns.\\n\\n    :param dct: `dict` to print\\n    :param dict_property: name of the first column\\n    :param wrap: wrapping for the second column\\n    :param dict_value: header label for the value (second) column\\n    :param json_flag: print `dict` as JSON instead of table\\n    '\n    if json_flag:\n        print(json.dumps(dct, indent=4, separators=(',', ': ')))\n        return\n    pt = prettytable.PrettyTable([dict_property, dict_value])\n    pt.align = 'l'\n    for (k, v) in sorted(dct.items()):\n        if isinstance(v, dict):\n            v = six.text_type(v)\n        if (wrap > 0):\n            v = textwrap.fill(six.text_type(v), wrap)\n        if (v and isinstance(v, six.string_types) and ('\\\\n' in v)):\n            lines = v.strip().split('\\\\n')\n            col1 = k\n            for line in lines:\n                pt.add_row([col1, line])\n                col1 = ''\n        else:\n            pt.add_row([k, v])\n    if six.PY3:\n        print(encodeutils.safe_encode(pt.get_string()).decode())\n    else:\n        print(encodeutils.safe_encode(pt.get_string()))\n", "label": 1}
{"function": "\n\ndef putchar(self, char, x=None, y=None, fgcolor=None, bgcolor=None):\n    '\\n        Print a single character to the coordinates on the surface. This function does not move the cursor.\\n        '\n    if (type(char) not in (str, unicode)):\n        raise Exception(('Argument 1 must be str or unicode, not %s' % type(char)))\n    if (char == ''):\n        return\n    if (x is None):\n        x = self._cursorx\n    if (y is None):\n        y = self._cursory\n    if ((x < 0) or (y < 0) or (x >= self._width) or (y >= self._height)):\n        return None\n    if (fgcolor is not None):\n        self._screenfgcolor[x][y] = getpygamecolor(fgcolor)\n    if (bgcolor is not None):\n        self._screenbgcolor[x][y] = getpygamecolor(bgcolor)\n    self._screenchar[x][y] = char[0]\n    self._screendirty[x][y] = True\n    if self._autoupdate:\n        self.update()\n    return char\n", "label": 1}
{"function": "\n\ndef analyse_action(func):\n    'Analyse a function.'\n    description = (inspect.getdoc(func) or 'undocumented action')\n    arguments = []\n    (args, varargs, kwargs, defaults) = inspect.getargspec(func)\n    if (varargs or kwargs):\n        raise TypeError('variable length arguments for action not allowed.')\n    if (len(args) != len((defaults or ()))):\n        raise TypeError('not all arguments have proper definitions')\n    for (idx, (arg, definition)) in enumerate(zip(args, (defaults or ()))):\n        if arg.startswith('_'):\n            raise TypeError('arguments may not start with an underscore')\n        if (not isinstance(definition, tuple)):\n            shortcut = None\n            default = definition\n        else:\n            (shortcut, default) = definition\n        argument_type = argument_types[type(default)]\n        if (isinstance(default, bool) and (default is True)):\n            arg = ('no-' + arg)\n        arguments.append((arg.replace('_', '-'), shortcut, default, argument_type))\n    return (func, description, arguments)\n", "label": 1}
{"function": "\n\ndef _find_handler(self):\n    app = self.application\n    handlers = app._get_host_handlers(self.request)\n    if (not handlers):\n        self.handler_class = RedirectHandler\n        self.handler_kwargs = dict(url=(('http://' + app.default_host) + '/'))\n        return\n    for spec in handlers:\n        match = spec.regex.match(self.request.path)\n        if match:\n            self.handler_class = spec.handler_class\n            self.handler_kwargs = spec.kwargs\n            if spec.regex.groups:\n                if spec.regex.groupindex:\n                    self.path_kwargs = dict(((str(k), _unquote_or_none(v)) for (k, v) in match.groupdict().items()))\n                else:\n                    self.path_args = [_unquote_or_none(s) for s in match.groups()]\n            return\n    if app.settings.get('default_handler_class'):\n        self.handler_class = app.settings['default_handler_class']\n        self.handler_kwargs = app.settings.get('default_handler_args', {\n            \n        })\n    else:\n        self.handler_class = ErrorHandler\n        self.handler_kwargs = dict(status_code=404)\n", "label": 1}
{"function": "\n\ndef angle(self, x, y):\n    result = Double.NaN\n    if (x == 0):\n        if (y > 0):\n            result = (math.pi / 2.0)\n        elif (y < 0):\n            result = ((3 * math.pi) / 2.0)\n    elif ((x > 0) and (y >= 0)):\n        result = math.atan((y / x))\n    elif ((x < 0) and (y >= 0)):\n        result = (math.pi - math.atan(((- y) / x)))\n    elif ((x < 0) and (y < 0)):\n        result = (math.pi + math.atan((y / x)))\n    elif ((x > 0) and (y < 0)):\n        result = ((2 * math.pi) - math.atan(((- y) / x)))\n    return result\n", "label": 1}
{"function": "\n\ndef build(self, input_shape):\n    input_dim = input_shape[1]\n    self.input_spec = [InputSpec(dtype=K.floatx(), shape=(None, input_dim))]\n    self.W = self.init((self.nb_feature, input_dim, self.output_dim), name='{}_W'.format(self.name))\n    if self.bias:\n        self.b = K.zeros((self.nb_feature, self.output_dim), name='{}_b'.format(self.name))\n        self.trainable_weights = [self.W, self.b]\n    else:\n        self.trainable_weights = [self.W]\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if (self.bias and self.b_regularizer):\n        self.b_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    if self.activity_regularizer:\n        self.activity_regularizer.set_layer(self)\n        self.regularizers.append(self.activity_regularizer)\n    self.constraints = {\n        \n    }\n    if self.W_constraint:\n        self.constraints[self.W] = self.W_constraint\n    if (self.bias and self.b_constraint):\n        self.constraints[self.b] = self.b_constraint\n    if (self.initial_weights is not None):\n        self.set_weights(self.initial_weights)\n        del self.initial_weights\n", "label": 1}
{"function": "\n\ndef split_filenames():\n    for filename in glob.glob(test_filenames):\n        if ((not filename.endswith('.py')) and (not filename.endswith('.pyc')) and (not filename.endswith('.result')) and (not filename.endswith('.ipynb')) and (not filename.endswith('.log')) and (not filename.startswith('__'))):\n            (stem, ext) = filename.split('.', 1)\n            (yield (os.path.split(filename)[1], stem, ext))\n", "label": 0}
{"function": "\n\ndef feature(self, feature):\n    if ((feature.status == 'skipped') and (not self.show_skipped)):\n        return\n    feature_filename = self.make_feature_filename(feature)\n    classname = feature_filename\n    report = FeatureReportData(feature, feature_filename)\n    suite = ElementTree.Element('testsuite')\n    feature_name = (feature.name or feature_filename)\n    suite.set('name', ('%s.%s' % (classname, feature_name)))\n    for scenario in feature:\n        if isinstance(scenario, ScenarioOutline):\n            scenario_outline = scenario\n            self._process_scenario_outline(scenario_outline, report)\n        else:\n            self._process_scenario(scenario, report)\n    for testcase in report.testcases:\n        suite.append(testcase)\n    suite.set('tests', _text(report.counts_tests))\n    suite.set('errors', _text(report.counts_errors))\n    suite.set('failures', _text(report.counts_failed))\n    suite.set('skipped', _text(report.counts_skipped))\n    suite.set('time', _text(round(feature.duration, 6)))\n    if (not os.path.exists(self.config.junit_directory)):\n        os.makedirs(self.config.junit_directory)\n    tree = ElementTreeWithCDATA(suite)\n    report_dirname = self.config.junit_directory\n    report_basename = ('TESTS-%s.xml' % feature_filename)\n    report_filename = os.path.join(report_dirname, report_basename)\n    tree.write(codecs.open(report_filename, 'wb'), 'UTF-8')\n", "label": 0}
{"function": "\n\ndef _reload_on_update(modify_times):\n    if _reload_attempted:\n        return\n    if (process.task_id() is not None):\n        return\n    for module in sys.modules.values():\n        if (not isinstance(module, types.ModuleType)):\n            continue\n        path = getattr(module, '__file__', None)\n        if (not path):\n            continue\n        if (path.endswith('.pyc') or path.endswith('.pyo')):\n            path = path[:(- 1)]\n        _check_file(modify_times, path)\n    for path in _watched_files:\n        _check_file(modify_times, path)\n", "label": 1}
{"function": "\n\ndef get_extended_cost_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances')\n    extended_cost_matrix = [[0 for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n    project_id = filter_properties['project_id']\n    host_racks_map = solver_utils.get_host_racks_map(hosts)\n    affinity_racks = set([])\n    affinity_hosts = set([])\n    for i in xrange(num_hosts):\n        host_name = hosts[i].host\n        host_racks = host_racks_map.get(host_name, set([]))\n        if (project_id in hosts[i].projects):\n            affinity_hosts.add(host_name)\n            affinity_racks = affinity_racks.union(host_racks)\n    for i in xrange(num_hosts):\n        host_name = hosts[i].host\n        host_racks = host_racks_map.get(host_name, set([]))\n        if ((not any([(rack in affinity_racks) for rack in host_racks])) and (host_name not in affinity_hosts)):\n            extended_cost_matrix[i] = [1 for j in xrange((num_instances + 1))]\n        else:\n            LOG.debug(_('%(host)s is in tenant affinity rack.'), {\n                'host': host_name,\n            })\n    return extended_cost_matrix\n", "label": 1}
{"function": "\n\ndef obj_load_attr(self, attrname):\n    if (attrname not in INSTANCE_OPTIONAL_ATTRS):\n        raise exception.ObjectActionError(action='obj_load_attr', reason=('attribute %s not lazy-loadable' % attrname))\n    if (not self._context):\n        raise exception.OrphanedObjectError(method='obj_load_attr', objtype=self.obj_name())\n    LOG.debug(\"Lazy-loading '%(attr)s' on %(name)s uuid %(uuid)s\", {\n        'attr': attrname,\n        'name': self.obj_name(),\n        'uuid': self.uuid,\n    })\n    if (attrname == 'fault'):\n        self._load_fault()\n    elif (attrname == 'numa_topology'):\n        self._load_numa_topology()\n    elif (attrname == 'pci_requests'):\n        self._load_pci_requests()\n    elif (attrname == 'vcpu_model'):\n        self._load_vcpu_model()\n    elif (attrname == 'ec2_ids'):\n        self._load_ec2_ids()\n    elif (attrname == 'migration_context'):\n        self._load_migration_context()\n    elif (attrname == 'security_groups'):\n        self._load_security_groups()\n    elif (attrname == 'pci_devices'):\n        self._load_pci_devices()\n    elif ('flavor' in attrname):\n        self._load_flavor()\n    elif ((attrname == 'services') and self.deleted):\n        self.services = objects.ServiceList(self._context)\n    else:\n        self._load_generic(attrname)\n    self.obj_reset_changes([attrname])\n", "label": 1}
{"function": "\n\ndef handle_starttag(self, tag, attrs):\n    if (tag == 'a'):\n        href = [v for (k, v) in attrs if (k == 'href')]\n        if href:\n            self.in_a = True\n            self.url = href[0]\n    elif ((tag == 'img') and self.in_a):\n        src = [v for (k, v) in attrs if (k == 'src')]\n        if src:\n            self.text += (' [image:%s] ' % src[0])\n", "label": 1}
{"function": "\n\ndef skip_column(self, column):\n    '\\n        Whether or not to skip column in the generation process.\\n\\n        :param column_property: SQLAlchemy Column object\\n        '\n    if ((not self.meta.include_foreign_keys) and column.foreign_keys):\n        return True\n    if ((not self.meta.include_primary_keys) and column.primary_key):\n        return True\n    if ((not self.meta.include_datetimes_with_default) and isinstance(column.type, sa.types.DateTime) and column.default):\n        return True\n    if isinstance(column.type, types.TSVectorType):\n        return True\n    if (self.meta.only_indexed_fields and (not self.has_index(column))):\n        return True\n    if (not isinstance(column, sa.Column)):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    apps = options.get('apps').split(',')\n    if ('all' in apps):\n        apps = self.all_apps\n    if options.get('flush', False):\n        self.stdout.write('Flush database..')\n        call_command('flush', interactive=False)\n        self.stdout.write('Flush finished.')\n    self.generate_users_and_groups()\n    if ('backoffice' in apps):\n        self.generate_back_office()\n    if ('datacenter' in apps):\n        self.generate_data_center()\n    if ('supports' in apps):\n        self.generate_support()\n    if ('licences' in apps):\n        self.generate_licence()\n    if ('transitions' in apps):\n        self.generate_transitions()\n    root = UserFactory(username='ralph', is_superuser=True, is_staff=True)\n    root.set_password('ralph')\n    root.save()\n    if options.get('flush', False):\n        call_command('sitetree_resync_apps', interactive=False)\n    self.stdout.write('done')\n", "label": 1}
{"function": "\n\ndef _has_got_message(self, channel, match, start=None, end=None):\n    if channel.startswith('C'):\n        match = six.text_type('\\\\<@{}\\\\>: {}').format(self.driver_userid, match)\n    oldest = (start or self._start_ts)\n    latest = (end or time.time())\n    func = (self.slacker.channels.history if channel.startswith('C') else self.slacker.im.history)\n    response = func(channel, oldest=oldest, latest=latest)\n    for msg in response.body['messages']:\n        if ((msg['type'] == 'message') and re.match(match, msg['text'], re.DOTALL)):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef norm_words(words):\n    if (not args.no_lowercase):\n        words = (w.lower() for w in words)\n    if (not args.punctuation):\n        words = (w.strip(string.punctuation) for w in words)\n        words = (w for w in words if w)\n    if stopset:\n        words = (w for w in words if (w.lower() not in stopset))\n    if (not isinstance(words, list)):\n        words = list(words)\n    if args.ngrams:\n        return functools.reduce(operator.add, [(words if (n == 1) else list(ngrams(words, n))) for n in args.ngrams])\n    else:\n        return words\n", "label": 1}
{"function": "\n\ndef to_trait(self, notification_body):\n    values = [match for match in self.fields.find(notification_body) if (match.value is not None)]\n    if (self.plugin is not None):\n        value_map = [('.'.join(self._get_path(match)), match.value) for match in values]\n        value = self.plugin.trait_value(value_map)\n    else:\n        value = (values[0].value if values else None)\n    if (value is None):\n        return None\n    if ((self.trait_type != Datatype.text) and (value == '')):\n        return None\n    value = self.trait_type.convert(value)\n    return Trait(self.name, self.trait_type, value)\n", "label": 1}
{"function": "\n\ndef remove_unreferenced_images(asset, default=True):\n\n    def _unreference_image(imageName):\n        if (type(imageName) is StringType):\n            if (imageName in unreferenced_images):\n                unreferenced_images.remove(imageName)\n    images = asset.asset['images']\n    unreferenced_images = set(asset.asset['images'].iterkeys())\n    materials = asset.asset['materials']\n    effects = asset.asset['effects']\n    for (v, _) in effects.iteritems():\n        effect = asset.retrieve_effect(v)\n        parameters = effect.get('parameters', None)\n        if parameters:\n            for value in parameters.itervalues():\n                _unreference_image(value)\n    for (v, _) in materials.iteritems():\n        material = asset.retrieve_material(v, default)\n        parameters = material.get('parameters', None)\n        if parameters:\n            for value in parameters.itervalues():\n                _unreference_image(value)\n    for i in unreferenced_images:\n        del images[i]\n", "label": 0}
{"function": "\n\ndef test_pendingCallbacksClearQueueAndMoreRunsRunAdditionalQueuedOnMoreCompletions(self):\n    numExtras = len(self.extraTransmitIds)\n    self.test_extraPendingCallbackCompletionsDoNothing()\n    for _moreExtras in range((numExtras + 3)):\n        self.testTrans.forTestingCompleteAPendingIntent(SendStatus.Sent)\n    expectedCBCount = (2 * (numExtras + 3))\n    self.assertEqual(self.successCBcalls, expectedCBCount)\n    self.assertEqual((MAX_PENDING_TRANSMITS + numExtras), len(self.testTrans.intents))\n    self.assertEqual(numExtras, len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n    for count in self.extraTransmitIds:\n        self.testTrans.scheduleTransmit(None, TransmitIntent(ActorAddress(3.5), count, self.successCB, self.failureCB))\n    self.assertEqual(self.successCBcalls, expectedCBCount)\n    self.assertEqual((MAX_PENDING_TRANSMITS + (2 * numExtras)), len(self.testTrans.intents))\n    self.assertEqual((2 * numExtras), len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n    for _extras in range(numExtras):\n        self.testTrans.forTestingCompleteAPendingIntent(SendStatus.Sent)\n    expectedCBCount += numExtras\n    self.assertEqual(self.successCBcalls, expectedCBCount)\n    self.assertEqual(((MAX_PENDING_TRANSMITS + numExtras) + numExtras), len(self.testTrans.intents))\n    self.assertEqual((2 * numExtras), len([I for I in self.testTrans.intents if (I.message in self.extraTransmitIds)]))\n", "label": 1}
{"function": "\n\ndef update(self, config):\n    for (command, keys) in config.items():\n        if (command == 'name'):\n            continue\n        command = command.replace('-', ' ')\n        if (type(keys) != type([])):\n            keys = [keys]\n        self.commandmap[command] = keys\n    self.keytree = Key(None)\n    for (command, keys) in self.commandmap.items():\n        for key in keys:\n            if isinstance(key, list):\n                tree = self.keytree\n                for (i, innerkey) in enumerate(key):\n                    tree = tree.addKey(innerkey)\n                    if ((i + 1) == len(key)):\n                        tree.commands.append(command)\n            else:\n                tree = self.keytree.addKey(key)\n                tree.commands.append(command)\n", "label": 1}
{"function": "\n\ndef find_packages(where='.', exclude=()):\n    'Return a list all Python packages found within directory \\'where\\'\\n\\n    \\'where\\' should be supplied as a \"cross-platform\" (i.e. URL-style) path; it\\n    will be converted to the appropriate local path syntax.  \\'exclude\\' is a\\n    sequence of package names to exclude; \\'*\\' can be used as a wildcard in the\\n    names, such that \\'foo.*\\' will exclude all subpackages of \\'foo\\' (but not\\n    \\'foo\\' itself).\\n    '\n    out = []\n    stack = [(convert_path(where), '')]\n    while stack:\n        (where, prefix) = stack.pop(0)\n        for name in os.listdir(where):\n            fn = os.path.join(where, name)\n            if (('.' not in name) and os.path.isdir(fn) and os.path.isfile(os.path.join(fn, '__init__.py'))):\n                out.append((prefix + name))\n                stack.append((fn, ((prefix + name) + '.')))\n    for pat in (list(exclude) + ['ez_setup']):\n        from fnmatch import fnmatchcase\n        out = [item for item in out if (not fnmatchcase(item, pat))]\n    return out\n", "label": 1}
{"function": "\n\ndef __init__(self, branch=None, upstream=None, import_branch=None, extra_branches=None, *args, **kwargs):\n    if (not extra_branches):\n        extra_branches = []\n    self._branch = branch\n    self._upstream = upstream\n    self._import_branch = import_branch\n    self._extra_branches = extra_branches\n    super(ImportUpstream, self).__init__(*args, **kwargs)\n    if self.is_detached():\n        raise ImportUpstreamError(\"In 'detached HEAD' state\")\n    if self.repo.bare:\n        raise ImportUpstreamError('Cannot perform imports in bare repos')\n    if (self.branch == 'HEAD'):\n        self._branch = str(self.repo.active_branch)\n    branches = [self.branch, self.upstream]\n    branches.extend(self.extra_branches)\n    invalid_ref = False\n    for branch in branches:\n        if (not any((head for head in self.repo.heads if (head.name == branch)))):\n            msg = \"Specified ref does not exist: '%s'\"\n            self.log.error(msg, branch)\n            invalid_ref = True\n    if invalid_ref:\n        raise ImportUpstreamError('Invalid ref')\n", "label": 1}
{"function": "\n\ndef render_plugin(self, context=None, placeholder=None, admin=False, processors=None):\n    (instance, plugin) = self.get_plugin_instance()\n    request = None\n    current_app = None\n    if context:\n        request = context.get('request', None)\n        if request:\n            current_app = getattr(request, 'current_app', None)\n        if (not current_app):\n            current_app = (context.current_app if context else None)\n    if (instance and (not (admin and (not plugin.admin_preview)))):\n        if ((not placeholder) or (not isinstance(placeholder, Placeholder))):\n            placeholder = instance.placeholder\n        placeholder_slot = placeholder.slot\n        context = PluginContext(context, instance, placeholder, current_app=current_app)\n        context = plugin.render(context, instance, placeholder_slot)\n        page = None\n        if request:\n            page = request.current_page\n        plugin.cms_plugin_instance = instance\n        context['allowed_child_classes'] = plugin.get_child_classes(placeholder_slot, page)\n        context['allowed_parent_classes'] = plugin.get_parent_classes(placeholder_slot, page)\n        if plugin.render_plugin:\n            template = plugin._get_render_template(context, instance, placeholder)\n            if (not template):\n                raise ValidationError(('plugin has no render_template: %s' % plugin.__class__))\n        else:\n            template = None\n        return render_plugin(context, instance, placeholder, template, processors, current_app)\n    else:\n        from cms.middleware.toolbar import toolbar_plugin_processor\n        if (processors and (toolbar_plugin_processor in processors)):\n            if (not placeholder):\n                placeholder = self.placeholder\n            context = PluginContext(context, self, placeholder, current_app=current_app)\n            template = None\n            return render_plugin(context, self, placeholder, template, processors, current_app)\n    return ''\n", "label": 1}
{"function": "\n\ndef iterunpack(source, field, newfields, include_original, missing):\n    it = iter(source)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    if (field in flds):\n        field_index = flds.index(field)\n    elif (isinstance(field, int) and (field < len(flds))):\n        field_index = field\n        field = flds[field_index]\n    else:\n        raise ArgumentError('field invalid: must be either field name or index')\n    outhdr = list(flds)\n    if (not include_original):\n        outhdr.remove(field)\n    if isinstance(newfields, (list, tuple)):\n        outhdr.extend(newfields)\n        nunpack = len(newfields)\n    elif isinstance(newfields, int):\n        nunpack = newfields\n        newfields = [(text_type(field) + text_type((i + 1))) for i in range(newfields)]\n        outhdr.extend(newfields)\n    elif (newfields is None):\n        nunpack = 0\n    else:\n        raise ArgumentError('newfields argument must be list or tuple of field names, or int (number of values to unpack)')\n    (yield tuple(outhdr))\n    for row in it:\n        value = row[field_index]\n        if include_original:\n            out_row = list(row)\n        else:\n            out_row = [v for (i, v) in enumerate(row) if (i != field_index)]\n        nvals = len(value)\n        if (nunpack > 0):\n            if (nvals >= nunpack):\n                newvals = value[:nunpack]\n            else:\n                newvals = (list(value) + ([missing] * (nunpack - nvals)))\n            out_row.extend(newvals)\n        (yield tuple(out_row))\n", "label": 1}
{"function": "\n\ndef move_to_section(self, which, where=None):\n    if (which in range(1, 5)):\n        sections = self.get_sections()\n        if (sections and (len(sections) >= which)):\n            section = sections[(which - 1)]\n            self.move_to_region(section)\n    elif (which in list(SECTIONS.keys())):\n        sections = self.get_sections()\n        for section in sections:\n            if (self.section_at_region(section) == which):\n                self.move_to_region(section)\n                return\n    elif (which in ('next', 'prev')):\n        point = self.get_first_point()\n        sections = self.get_sections()\n        if (point and sections):\n            next = self.next_or_prev_region(which, sections, point)\n            self.move_to_region(next)\n", "label": 1}
{"function": "\n\ndef _connect_params(self, database=None, server=None, user=None, password=None):\n    database = (database or self.config.get('default'))\n    db_config = self.config.get(database, {\n        \n    })\n    params = {\n        'database': (db_config.get('database') or database),\n        'server': (server or db_config.get('server')),\n        'user': (user or db_config.get('user')),\n        'password': (password or db_config.get('password')),\n    }\n    unspecified = [param for (param, value) in params.iteritems() if (value is None)]\n    if unspecified:\n        raise Exception(('Must specify or configure in config.yaml: %s' % ', '.join(unspecified)))\n    return params\n", "label": 1}
{"function": "\n\ndef __search(node, coll=None):\n    if (coll is None):\n        coll = set()\n    if ((node.type == 'assign') and (node[0].type == 'dot')):\n        if (node[0][1].type == 'identifier'):\n            name = node[0][1].value\n            if ((type(name) is str) and __matcher.match(name)):\n                coll.add(name)\n    elif (node.type == 'property_init'):\n        name = node[0].value\n        if ((type(name) is str) and __matcher.match(name)):\n            coll.add(name)\n    for child in node:\n        if (child != None):\n            __search(child, coll)\n    return coll\n", "label": 1}
{"function": "\n\ndef render(self):\n    translate((width / 2), (height / 2))\n    pushes = 0\n    repeats = 1\n    self.steps += 12\n    if (self.steps > len(self.production)):\n        self.steps = len(self.production)\n    for i in range(self.steps):\n        step = self.production[i]\n        if (step == 'F'):\n            stroke(255, 60)\n            for j in range(repeats):\n                line(0, 0, 0, (- self.drawLength))\n                noFill()\n                translate(0, (- self.drawLength))\n            repeats = 1\n        elif (step == '+'):\n            for j in range(repeats):\n                rotate(self.theta)\n            repeats = 1\n        elif (step == '-'):\n            for j in range(repeats):\n                rotate((- self.theta))\n            repeats = 1\n        elif (step == '['):\n            pushes += 1\n            pushMatrix()\n        elif (step == ']'):\n            popMatrix()\n            pushes -= 1\n        elif ((ord(step) >= 48) and (ord(step) <= 57)):\n            repeats = (ord(step) - 48)\n    while (pushes > 0):\n        popMatrix()\n        pushes -= 1\n", "label": 1}
{"function": "\n\ndef updateuptime(self, server):\n    now = time.mktime(datetime.datetime.now().timetuple())\n    servercameback = time.mktime(server.timeservercameback.timetuple())\n    difference = (now - servercameback)\n    MINUTE = 60\n    HOUR = (MINUTE * 60)\n    DAY = (HOUR * 24)\n    days = int((difference / DAY))\n    hours = int(((difference % DAY) / HOUR))\n    minutes = int(((difference % HOUR) / MINUTE))\n    seconds = int((difference % MINUTE))\n    string = ''\n    if (days > 0):\n        string += (((str(days) + ' ') + (((days == 1) and 'day') or 'days')) + ', ')\n    if ((len(string) > 0) or (hours > 0)):\n        string += (((str(hours) + ' ') + (((hours == 1) and 'hour') or 'hours')) + ', ')\n    if ((len(string) > 0) or (minutes > 0)):\n        string += (((str(minutes) + ' ') + (((minutes == 1) and 'minute') or 'minutes')) + ', ')\n    string += ((str(seconds) + ' ') + (((seconds == 1) and 'second') or 'seconds'))\n    server.uptime = string\n    server.put()\n", "label": 1}
{"function": "\n\ndef get_l3_agent_candidates(self, context, sync_router, l3_agents, ignore_admin_state=False):\n    \"Get the valid l3 agents for the router from a list of l3_agents.\\n\\n        It will not return agents in 'dvr' mode for a dvr router as dvr\\n        routers are not explicitly scheduled to l3 agents on compute nodes\\n        \"\n    candidates = []\n    is_router_distributed = sync_router.get('distributed', False)\n    for l3_agent in l3_agents:\n        if ((not ignore_admin_state) and (not l3_agent.admin_state_up)):\n            continue\n        agent_conf = self.get_configuration_dict(l3_agent)\n        agent_mode = agent_conf.get(n_const.L3_AGENT_MODE, n_const.L3_AGENT_MODE_LEGACY)\n        if ((agent_mode == n_const.L3_AGENT_MODE_DVR) or ((agent_mode == n_const.L3_AGENT_MODE_LEGACY) and is_router_distributed)):\n            continue\n        router_id = agent_conf.get('router_id', None)\n        if (router_id and (router_id != sync_router['id'])):\n            continue\n        handle_internal_only_routers = agent_conf.get('handle_internal_only_routers', True)\n        gateway_external_network_id = agent_conf.get('gateway_external_network_id', None)\n        ex_net_id = (sync_router['external_gateway_info'] or {\n            \n        }).get('network_id')\n        if (((not ex_net_id) and (not handle_internal_only_routers)) or (ex_net_id and gateway_external_network_id and (ex_net_id != gateway_external_network_id))):\n            continue\n        candidates.append(l3_agent)\n    return candidates\n", "label": 1}
{"function": "\n\ndef pagination(cl):\n    (paginator, page_num) = (cl.paginator, cl.page_num)\n    pagination_required = (((not cl.show_all) or (not cl.can_show_all)) and cl.multi_page)\n    if (not pagination_required):\n        page_range = []\n    else:\n        ON_EACH_SIDE = 3\n        ON_ENDS = 2\n        if (paginator.pages <= 10):\n            page_range = range(paginator.pages)\n        else:\n            page_range = []\n            if (page_num > (ON_EACH_SIDE + ON_ENDS)):\n                page_range.extend(range(0, (ON_EACH_SIDE - 1)))\n                page_range.append(DOT)\n                page_range.extend(range((page_num - ON_EACH_SIDE), (page_num + 1)))\n            else:\n                page_range.extend(range(0, (page_num + 1)))\n            if (page_num < (((paginator.pages - ON_EACH_SIDE) - ON_ENDS) - 1)):\n                page_range.extend(range((page_num + 1), ((page_num + ON_EACH_SIDE) + 1)))\n                page_range.append(DOT)\n                page_range.extend(range((paginator.pages - ON_ENDS), paginator.pages))\n            else:\n                page_range.extend(range((page_num + 1), paginator.pages))\n    need_show_all_link = (cl.can_show_all and (not cl.show_all) and cl.multi_page)\n    return {\n        'cl': cl,\n        'pagination_required': pagination_required,\n        'show_all_url': (need_show_all_link and cl.get_query_string({\n            ALL_VAR: '',\n        })),\n        'page_range': page_range,\n        'ALL_VAR': ALL_VAR,\n        '1': 1,\n    }\n", "label": 1}
{"function": "\n\ndef update_with_json(self, jsondict):\n    ' Update the receiver with data in a JSON dictionary.\\n        '\n    if (jsondict is None):\n        return\n    if (not isinstance(jsondict, dict)):\n        logging.warning('Non-dict type {} fed to `update_with_json` on {}'.format(type(jsondict), type(self)))\n        return\n    found = set(['resourceType', 'fhir_comments'])\n    nonoptionals = set()\n    for (name, jsname, typ, is_list, of_many, not_optional) in self.elementProperties():\n        if (not (jsname in jsondict)):\n            if not_optional:\n                nonoptionals.add((of_many or jsname))\n            continue\n        if hasattr(typ, 'with_json_and_owner'):\n            setattr(self, name, typ.with_json_and_owner(jsondict[jsname], self))\n        else:\n            setattr(self, name, jsondict[jsname])\n        found.add(jsname)\n        found.add(('_' + jsname))\n        if (of_many is not None):\n            found.add(of_many)\n    if (len((nonoptionals - found)) > 0):\n        for miss in (nonoptionals - found):\n            logging.warning(\"Non-optional property '{}' on {} is missing from JSON\".format(miss, self))\n    if (len((set(jsondict.keys()) - found)) > 0):\n        for supflu in (set(jsondict.keys()) - found):\n            logging.warning(\"Superfluous entry '{}' in JSON for {}\".format(supflu, self))\n", "label": 1}
{"function": "\n\ndef is_forest(edges, n_vertices=None):\n    if ((n_vertices is not None) and (len(edges) > (n_vertices - 1))):\n        return False\n    n_vertices = (np.max(edges) + 1)\n    parents = (- np.ones(n_vertices))\n    visited = np.zeros(n_vertices, dtype=np.bool)\n    neighbors = [[] for i in range(n_vertices)]\n    for edge in edges:\n        neighbors[edge[0]].append(edge[1])\n        neighbors[edge[1]].append(edge[0])\n    lonely = 0\n    while (lonely < n_vertices):\n        for i in range(lonely, n_vertices):\n            if (not visited[i]):\n                queue = [i]\n                lonely = (i + 1)\n                visited[i] = True\n                break\n            lonely = n_vertices\n        while queue:\n            node = queue.pop()\n            for neighbor in neighbors[node]:\n                if (not visited[neighbor]):\n                    parents[neighbor] = node\n                    queue.append(neighbor)\n                    visited[neighbor] = True\n                elif (not (parents[node] == neighbor)):\n                    return False\n    return True\n", "label": 1}
{"function": "\n\ndef list_dir(root_dir, dirs_only=False, include_special=False):\n    '\\n    List directory.\\n    :param root_dir: string: directory to list\\n    :param dirs_only: boolean\\n    :param include_special: boolean\\n    :return: list\\n    '\n    root_dir = ('.' if (not root_dir) else root_dir)\n    res = []\n    if ('~' in root_dir):\n        root_dir = os.path.expanduser(root_dir)\n    if (not os.path.exists(root_dir)):\n        (root_dir, _) = os.path.split(root_dir)\n    if os.path.exists(root_dir):\n        for name in os.listdir(root_dir):\n            path = os.path.join(root_dir, name)\n            if ((not include_special) and name.startswith('.')):\n                continue\n            if (dirs_only and (not os.path.isdir(path))):\n                continue\n            res.append(name)\n    return res\n", "label": 1}
{"function": "\n\ndef main(numpreds=100, numdocs=2000, printp=0):\n    preds = gen_predicates(numpreds)\n    docs = gen_docs(numdocs)\n    s1 = make_set(preds)\n    s2 = make_set_optimized(preds)\n    if printp:\n        print('Predicates:')\n        for p in preds:\n            print('\\t', p.predicate)\n    start = time.time()\n    total = 0\n    for d in docs:\n        total += len(s1.evaluate(d))\n    end = time.time()\n    print(('(Naive) Evaluated %d docs across %d predicates in %0.3f seconds' % (numdocs, numpreds, (end - start))))\n    print(('(Naive) Total of %d predicates matched' % total))\n    start = time.time()\n    total_o = 0\n    for d in docs:\n        total_o += len(s2.evaluate(d))\n    end = time.time()\n    print(('(Opt) Evaluated %d docs across %d predicates in %0.3f seconds' % (numdocs, numpreds, (end - start))))\n    print(('(Opt) Total of %d predicates matched' % total_o))\n    if (total != total_o):\n        print('Mismatch! Differing inputs:')\n        for d in docs:\n            r1 = s1.evaluate(d)\n            r2 = s2.evaluate(d)\n            if (r1 != r2):\n                print('Input:', repr(d))\n                print('Naive:')\n                for p in r1:\n                    print('\\t', p.predicate)\n                print('Opt:')\n                for p in r2:\n                    print('\\t', p.predicate)\n                print()\n        sys.exit(1)\n", "label": 1}
{"function": "\n\ndef parseSite(self):\n    print(('Beginning EatManga check: %s' % self.manga))\n    url = ('http://eatmanga.com/Manga-Scan/%s' % self.fixFormatting(self.manga))\n    if self.verbose_FLAG:\n        print(url)\n    source = getSourceCode(url, self.proxy)\n    self.chapters = EatManga.re_getChapters.findall(source)\n    self.chapters.reverse()\n    if (not self.chapters):\n        raise self.MangaNotFound\n    lowerRange = 0\n    for i in range(0, len(self.chapters)):\n        if ('upcoming' in self.chapters[i][0]):\n            del self.chapters[i]\n            continue\n        self.chapters[i] = (('http://eatmanga.com%s' % self.chapters[i][0]), self.chapters[i][2], self.chapters[i][2])\n        if (not self.auto):\n            print(('(%i) %s' % ((i + 1), self.chapters[i][1])))\n        elif (self.lastDownloaded == self.chapters[i][1]):\n            lowerRange = (i + 1)\n    upperRange = len(self.chapters)\n    if (not self.auto):\n        self.chapters_to_download = self.selectChapters(self.chapters)\n    else:\n        if (lowerRange == upperRange):\n            raise self.NoUpdates\n        for i in range(lowerRange, upperRange):\n            self.chapters_to_download.append(i)\n    self.isPrependMangaName = True\n    return\n", "label": 1}
{"function": "\n\ndef filtered_out(self, item, filters):\n    if (filters is None):\n        return False\n    for filter in filters:\n        filter_name = self.FILTER_MAP.get(filter['name'])\n        if (filter_name is None):\n            raise exception.InvalidParameterValue(value=filter['name'], parameter='filter', reason='invalid filter')\n        values = self.get_values_by_filter(filter_name, item)\n        if (not values):\n            return True\n        filter_values = filter['value']\n        for filter_value in filter_values:\n            if any((self.is_filtering_value_found(filter_value, value) for value in values)):\n                break\n        else:\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef __init__(self, class_):\n    self.class_ = class_\n    self.info = {\n        \n    }\n    self.new_init = None\n    self.local_attrs = {\n        \n    }\n    self.originals = {\n        \n    }\n    self._bases = [mgr for mgr in [manager_of_class(base) for base in self.class_.__bases__ if isinstance(base, type)] if (mgr is not None)]\n    for base in self._bases:\n        self.update(base)\n    self.dispatch._events._new_classmanager_instance(class_, self)\n    for basecls in class_.__mro__:\n        mgr = manager_of_class(basecls)\n        if (mgr is not None):\n            self.dispatch._update(mgr.dispatch)\n    self.manage()\n    self._instrument_init()\n    if ('__del__' in class_.__dict__):\n        util.warn(('__del__() method on class %s will cause unreachable cycles and memory leaks, as SQLAlchemy instrumentation often creates reference cycles.  Please remove this method.' % class_))\n", "label": 1}
{"function": "\n\ndef get_config_paths():\n    'Return the paths to each :file:`.reviewboardrc` influencing the cwd.\\n\\n    A list of paths to :file:`.reviewboardrc` files will be returned, where\\n    each subsequent list entry should have lower precedence than the previous.\\n    i.e. configuration found in files further up the list will take precedence.\\n\\n    Configuration in the paths set in :envvar:`$RBTOOLS_CONFIG_PATH` will take\\n    precedence over files found in the current working directory or its\\n    parents.\\n    '\n    config_paths = []\n    for path in os.environ.get('RBTOOLS_CONFIG_PATH', '').split(os.pathsep):\n        if (not path):\n            continue\n        filename = os.path.realpath(os.path.join(path, CONFIG_FILE))\n        if (os.path.exists(filename) and (filename not in config_paths)):\n            config_paths.append(filename)\n    for path in walk_parents(os.getcwd()):\n        filename = os.path.realpath(os.path.join(path, CONFIG_FILE))\n        if (os.path.exists(filename) and (filename not in config_paths)):\n            config_paths.append(filename)\n    home_config_path = os.path.realpath(os.path.join(get_home_path(), CONFIG_FILE))\n    if (os.path.exists(home_config_path) and (home_config_path not in config_paths)):\n        config_paths.append(home_config_path)\n    return config_paths\n", "label": 1}
{"function": "\n\ndef print_env_short_list(ctx, param, value):\n    if ((not value) or ctx.resilient_parsing):\n        return\n    nova_creds = config.run_config()\n    row = 1\n    for nova_env in nova_creds.keys():\n        executable = 'nova'\n        auth_url = ''\n        for (param, value) in sorted(nova_creds[nova_env].items()):\n            if (param.upper() == 'OS_EXECUTABLE'):\n                executable = value\n            if ((param.upper() == 'OS_AUTH_URL') and ('inova' in value)):\n                executable = 'inova'\n            if (param.upper() == 'OS_AUTH_URL'):\n                auth_url = value\n        color = 'green'\n        if ((row % 2) == 0):\n            color = 'red'\n        env = click.style(nova_env, fg=color)\n        click.echo(('%s (%s) @ %s' % (env, executable, auth_url)))\n        row += 1\n    ctx.exit()\n", "label": 1}
{"function": "\n\n@classmethod\ndef process(c, request, name=None):\n    '\\n        process uses the current request to determine which menus\\n        should be visible, which are selected, etc.\\n        '\n    c.load_menus()\n    c.sort_menus()\n    if (name is None):\n        items = {\n            \n        }\n        for name in c.items:\n            items[name] = c.process(request, name)\n        return items\n    if (name not in c.items):\n        return []\n    items = copy.deepcopy(c.items[name])\n    curitem = None\n    for item in items:\n        item.process(request)\n        if item.visible:\n            item.selected = False\n            if item.match_url(request):\n                if ((curitem is None) or (len(curitem.url) < len(item.url))):\n                    curitem = item\n    if (curitem is not None):\n        curitem.selected = True\n    visible = [item for item in items if item.visible]\n    if getattr(settings, 'MENU_SELECT_PARENTS', False):\n\n        def is_child_selected(item):\n            for child in item.children:\n                if (child.selected or is_child_selected(child)):\n                    return True\n        for item in visible:\n            if is_child_selected(item):\n                item.selected = True\n    return visible\n", "label": 1}
{"function": "\n\ndef validate(self, r):\n    '\\n        Called automatically by self.result.\\n        '\n    if self.show_invalid:\n        r.valid = True\n    elif r.valid:\n        if (not r.description):\n            r.valid = False\n        if (r.size and ((r.size + r.offset) > r.file.size)):\n            r.valid = False\n        if (r.jump and ((r.jump + r.offset) > r.file.size)):\n            r.valid = False\n    if r.valid:\n        if (r.id == self.one_of_many):\n            r.display = False\n        elif r.many:\n            self.one_of_many = r.id\n        else:\n            self.one_of_many = None\n", "label": 1}
{"function": "\n\ndef get_parler_languages_from_django_cms(cms_languages=None):\n    \"\\n    Converts django CMS' setting CMS_LANGUAGES into PARLER_LANGUAGES. Since\\n    CMS_LANGUAGES is a strict superset of PARLER_LANGUAGES, we do a bit of\\n    cleansing to remove irrelevant items.\\n    \"\n    valid_keys = ['code', 'fallbacks', 'hide_untranslated', 'redirect_on_fallback']\n    if cms_languages:\n        if (sys.version_info < (3, 0, 0)):\n            int_types = (int, long)\n        else:\n            int_types = int\n        parler_languages = copy.deepcopy(cms_languages)\n        for (site_id, site_config) in cms_languages.items():\n            if (site_id and ((not isinstance(site_id, int_types)) and (site_id != 'default'))):\n                del parler_languages[site_id]\n                continue\n            if (site_id == 'default'):\n                for (key, value) in site_config.items():\n                    if (key not in valid_keys):\n                        del parler_languages['default'][key]\n            else:\n                for (i, lang_config) in enumerate(site_config):\n                    for (key, value) in lang_config.items():\n                        if (key not in valid_keys):\n                            del parler_languages[site_id][i][key]\n        return parler_languages\n    return None\n", "label": 1}
{"function": "\n\ndef get_permissions(self, request, resources, actions=None):\n    if (not actions):\n        actions = ('read', 'update', 'create', 'delete')\n    if (not isinstance(actions, (list, tuple))):\n        actions = (actions,)\n    if (not isinstance(resources, (list, tuple))):\n        resources = (resources,)\n    obj = {\n        \n    }\n    if (not request.cache.user.is_superuser()):\n        permissions = self.get_permission_objects(request)\n        for resource in resources:\n            perms = {\n                \n            }\n            for action in actions:\n                perms[action] = has_permission(request, permissions, resource, action)\n            obj[resource] = perms\n    else:\n        for resource in resources:\n            obj[resource] = dict(((a, True) for a in actions))\n    return obj\n", "label": 1}
{"function": "\n\ndef unstash_index(sync=False, branch=None):\n    'Returns an unstash index if one is available.'\n    repo_check()\n    stash_list = repo.git.execute([git, 'stash', 'list'])\n    if (branch is None):\n        branch = get_current_branch_name()\n    for stash in stash_list.splitlines():\n        verb = ('syncing' if sync else 'switching')\n        if ((('Legit' in stash) and ('On {0}:'.format(branch) in stash) and (verb in stash)) or (('GitHub' in stash) and ('On {0}:'.format(branch) in stash) and (verb in stash))):\n            return stash[7]\n", "label": 1}
{"function": "\n\ndef _intersect(self, other):\n    from sympy.solvers.diophantine import diophantine\n    if (self.base_set is S.Integers):\n        g = None\n        if (isinstance(other, ImageSet) and (other.base_set is S.Integers)):\n            g = other.lamda.expr\n            m = other.lamda.variables[0]\n        elif (other is S.Integers):\n            m = g = Dummy('x')\n        if (g is not None):\n            f = self.lamda.expr\n            n = self.lamda.variables[0]\n            (a, b) = (Dummy('a'), Dummy('b'))\n            (f, g) = (f.subs(n, a), g.subs(m, b))\n            solns_set = diophantine((f - g))\n            if (solns_set == set()):\n                return EmptySet()\n            solns = list(diophantine((f - g)))\n            if (len(solns) != 1):\n                return\n            nsol = solns[0][0]\n            t = nsol.free_symbols.pop()\n            return imageset(Lambda(n, f.subs(a, nsol.subs(t, n))), S.Integers)\n    if (other == S.Reals):\n        from sympy.solvers.solveset import solveset_real\n        from sympy.core.function import expand_complex\n        if (len(self.lamda.variables) > 1):\n            return None\n        f = self.lamda.expr\n        n = self.lamda.variables[0]\n        n_ = Dummy(n.name, real=True)\n        f_ = f.subs(n, n_)\n        (re, im) = f_.as_real_imag()\n        im = expand_complex(im)\n        return imageset(Lambda(n_, re), self.base_set.intersect(solveset_real(im, n_)))\n", "label": 1}
{"function": "\n\ndef create_html(tree, meta_data):\n    new_html = etree.Element('html')\n    w_namespace = get_namespace(tree, 'w')\n    visited_nodes = []\n    _strip_tag(tree, ('%ssectPr' % w_namespace))\n    for el in tree.iter():\n        if (el in visited_nodes):\n            continue\n        header_value = is_header(el, meta_data)\n        if is_header(el, meta_data):\n            p_text = get_element_content(el, meta_data)\n            if (p_text == ''):\n                continue\n            new_html.append(etree.XML(('<%s>%s</%s>' % (header_value, p_text, header_value))))\n        elif (el.tag == ('%sp' % w_namespace)):\n            if is_title(el):\n                continue\n            if is_li(el, meta_data):\n                li_nodes = get_single_list_nodes_data(el, meta_data)\n                (new_el, list_visited_nodes) = build_list(li_nodes, meta_data)\n                visited_nodes.extend(list_visited_nodes)\n            else:\n                p_text = get_element_content(el, meta_data)\n                if (p_text == ''):\n                    continue\n                new_el = etree.XML(('<p>%s</p>' % p_text))\n            new_html.append(new_el)\n        elif (el.tag == ('%stbl' % w_namespace)):\n            (table_el, table_visited_nodes) = build_table(el, meta_data)\n            visited_nodes.extend(table_visited_nodes)\n            new_html.append(table_el)\n            continue\n        visited_nodes.append(el)\n    result = etree.tostring(new_html, method='html', with_tail=True)\n    return _make_void_elements_self_close(result)\n", "label": 1}
{"function": "\n\ndef _get_type_display(self, expr=None):\n    if (expr is None):\n        expr = self.expr\n    if isinstance(expr, ir.Node):\n        expr = expr.to_expr()\n    if isinstance(expr, ir.TableExpr):\n        return 'table'\n    elif isinstance(expr, ir.ArrayExpr):\n        return ('array(%s)' % expr.type())\n    elif isinstance(expr, ir.SortExpr):\n        return 'array-sort'\n    elif isinstance(expr, (ir.ScalarExpr, ir.AnalyticExpr)):\n        return ('%s' % expr.type())\n    elif isinstance(expr, ir.ExprList):\n        list_args = [self._get_type_display(arg) for arg in expr.op().args]\n        return ', '.join(list_args)\n    else:\n        raise NotImplementedError\n", "label": 1}
{"function": "\n\n@ensure_tag(['p'])\ndef get_font_size(p, styles_dict):\n    w_namespace = get_namespace(p, 'w')\n    r = p.find(('%sr' % w_namespace))\n    if (r is None):\n        return None\n    rpr = r.find(('%srPr' % w_namespace))\n    if (rpr is None):\n        return None\n    size = rpr.find(('%ssz' % w_namespace))\n    if (size is None):\n        pPr = p.find(('%spPr' % w_namespace))\n        if (pPr is None):\n            return None\n        pStyle = pPr.find(('%spStyle' % w_namespace))\n        if (pStyle is None):\n            return None\n        pStyle = pStyle.get(('%sval' % w_namespace))\n        font_size = None\n        style_value = styles_dict.get(pStyle, None)\n        if (style_value is None):\n            return None\n        if ('font_size' in style_value):\n            font_size = styles_dict[pStyle]['font_size']\n        while (font_size is None):\n            old_pStyle = pStyle\n            if (pStyle not in styles_dict):\n                break\n            if ('based_on' not in styles_dict[pStyle]):\n                break\n            pStyle = styles_dict[pStyle]['based_on']\n            if (old_pStyle == pStyle):\n                break\n            if (pStyle not in styles_dict):\n                break\n            font_size = styles_dict[pStyle]['font_size']\n        return font_size\n    return size.get(('%sval' % w_namespace))\n", "label": 1}
{"function": "\n\ndef create_mult(self, res_list, actor_id=None):\n    'Creates a list of resources from objects. Objects may have _id in it to predetermine their ID.\\n        Returns a list of 2-tuples (resource_id, rev)'\n    cur_time = get_ion_ts()\n    id_list = []\n    for resobj in res_list:\n        lcsm = get_restype_lcsm(resobj.type_)\n        resobj.lcstate = (lcsm.initial_state if lcsm else LCS.DEPLOYED)\n        resobj.availability = (lcsm.initial_availability if lcsm else AS.AVAILABLE)\n        resobj.ts_created = cur_time\n        resobj.ts_updated = cur_time\n        id_list.append((resobj._id if ('_id' in resobj) else create_unique_resource_id()))\n    res = self.rr_store.create_mult(res_list, id_list, allow_ids=True)\n    rid_list = [(rid, rrv) for (success, rid, rrv) in res]\n    if (actor_id and (actor_id != 'anonymous')):\n        assoc_list = []\n        for (resobj, (rid, rrv)) in zip(res_list, rid_list):\n            resobj._id = rid\n            assoc_list.append((resobj, PRED.hasOwner, actor_id))\n        self.create_association_mult(assoc_list)\n    for (resobj, (rid, rrv)) in zip(res_list, rid_list):\n        self.event_pub.publish_event(event_type='ResourceModifiedEvent', origin=rid, origin_type=resobj.type_, mod_type=ResourceModificationType.CREATE)\n    return rid_list\n", "label": 1}
{"function": "\n\ndef get_near(self, user, size):\n    self._read_leaderboard()\n    scores = self.scores\n    if (len(scores) == 0):\n        return self.create_response(True, True, [])\n    if (not (user.username in self.user_scores)):\n        return self.get_top_players(user, size)\n    index = None\n    for (i, r) in enumerate(scores):\n        if (r.user == user.username):\n            index = i\n            break\n    start = (index - int(floor((size * 0.5))))\n    end = (index + int(ceil((size * 0.5))))\n    num_scores = len(scores)\n    if (start < 0):\n        end -= start\n        start = 0\n        if (end > num_scores):\n            end = num_scores\n    elif (end > num_scores):\n        start -= (end - num_scores)\n        end = num_scores\n        if (start < 0):\n            start = 0\n    leaderboard = []\n    player = None\n    for i in xrange(start, end, 1):\n        s = scores[i]\n        username = s.user\n        row = self._get_row(username, s)\n        if (username == user.username):\n            player = row\n        leaderboard.append(row)\n    if (player is None):\n        player = self._get_user_row(user)\n    self._rank_leaderboard(leaderboard, self._get_rank(leaderboard[0]['score']))\n    top = (start == 0)\n    bottom = (end == num_scores)\n    return self.create_response(top, bottom, leaderboard, player)\n", "label": 1}
{"function": "\n\ndef get_l3_agents(self, context, active=None, filters=None):\n    query = context.session.query(agents_db.Agent)\n    query = query.filter((agents_db.Agent.agent_type == constants.AGENT_TYPE_L3))\n    if (active is not None):\n        query = query.filter((agents_db.Agent.admin_state_up == active))\n    if filters:\n        for (key, value) in six.iteritems(filters):\n            column = getattr(agents_db.Agent, key, None)\n            if column:\n                if (not value):\n                    return []\n                query = query.filter(column.in_(value))\n        agent_modes = filters.get('agent_modes', [])\n        if agent_modes:\n            agent_mode_key = '\"agent_mode\": \"'\n            configuration_filter = [agents_db.Agent.configurations.contains(('%s%s\"' % (agent_mode_key, agent_mode))) for agent_mode in agent_modes]\n            query = query.filter(or_(*configuration_filter))\n    return [l3_agent for l3_agent in query if agentschedulers_db.AgentSchedulerDbMixin.is_eligible_agent(active, l3_agent)]\n", "label": 1}
{"function": "\n\ndef addmul_number_dicts(series):\n    'Multiply dictionaries by a numeric values and add them together.\\n\\n:parameter series: a tuple of two elements tuples. Each serie is of the form::\\n\\n        (weight,dictionary)\\n\\n    where ``weight`` is a number and ``dictionary`` is a dictionary with\\n    numeric values.\\n:parameter skip: optional list of field names to skip.\\n\\nOnly common fields are aggregated. If a field has a non-numeric value it is\\nnot included either.'\n    if (not series):\n        return\n    vtype = value_type((s[1] for s in series))\n    if (vtype == 1):\n        return sum(((weight * float(d)) for (weight, d) in series))\n    elif (vtype == 3):\n        keys = set(series[0][1])\n        for serie in series[1:]:\n            keys.intersection_update(serie[1])\n        results = {\n            \n        }\n        for key in keys:\n            key_series = tuple(((weight, d[key]) for (weight, d) in series))\n            result = addmul_number_dicts(key_series)\n            if (result is not None):\n                results[key] = result\n        return results\n", "label": 1}
{"function": "\n\ndef get(self, validator):\n    if isinstance(validator, Validator):\n        return validator\n    if isinstance(validator, basestring):\n        if (validator in self.validators):\n            return self.validators[validator]()\n        elif (validator == 'string'):\n            return None\n        else:\n            raise Exception(('Unable to find a validator with name \"%s\"' % validator))\n    if (validator is basestring):\n        return None\n    if (validator is int):\n        return Integer()\n    if (validator is float):\n        return Float()\n    if (validator is bool):\n        return Boolean()\n    return None\n", "label": 1}
{"function": "\n\ndef Q(*predicates, **query):\n    '\\n    Handles situations where :class:`hunter.Query` objects (or other callables) are passed in as positional arguments.\\n    Conveniently converts that to an :class:`hunter.And` predicate.\\n    '\n    optional_actions = query.pop('actions', [])\n    if ('action' in query):\n        optional_actions.append(query.pop('action'))\n    for p in predicates:\n        if (not callable(p)):\n            raise TypeError('Predicate {0!r} is not callable.'.format(p))\n    for a in optional_actions:\n        if (not callable(a)):\n            raise TypeError('Action {0!r} is not callable.'.format(a))\n    if predicates:\n        predicates = tuple(((p() if (inspect.isclass(p) and issubclass(p, Action)) else p) for p in predicates))\n        if any((isinstance(p, CodePrinter) for p in predicates)):\n            if (CodePrinter in optional_actions):\n                optional_actions.remove(CodePrinter)\n        if query:\n            predicates += (Query(**query),)\n        result = And(*predicates)\n    else:\n        result = Query(**query)\n    if optional_actions:\n        result = When(result, *optional_actions)\n    return result\n", "label": 1}
{"function": "\n\ndef __init__(self, rdclass, rdtype, latitude, longitude, altitude):\n    super(GPOS, self).__init__(rdclass, rdtype)\n    if (isinstance(latitude, float) or isinstance(latitude, int) or isinstance(latitude, long)):\n        latitude = str(latitude)\n    if (isinstance(longitude, float) or isinstance(longitude, int) or isinstance(longitude, long)):\n        longitude = str(longitude)\n    if (isinstance(altitude, float) or isinstance(altitude, int) or isinstance(altitude, long)):\n        altitude = str(altitude)\n    _validate_float_string(latitude)\n    _validate_float_string(longitude)\n    _validate_float_string(altitude)\n    self.latitude = latitude\n    self.longitude = longitude\n    self.altitude = altitude\n", "label": 1}
{"function": "\n\ndef motion_notify_event(self, widget, event):\n    button = self.kbdmouse_mask\n    if event.is_hint:\n        return\n    else:\n        (x, y, state) = (event.x_root, event.y_root, event.state)\n    if (state & gtk.gdk.BUTTON1_MASK):\n        button |= 1\n    elif (state & gtk.gdk.BUTTON2_MASK):\n        button |= 2\n    elif (state & gtk.gdk.BUTTON3_MASK):\n        button |= 4\n    if ((button & 1) and (self.selected_child is not None)):\n        bnch = self.selected_child\n        subwin = bnch.subwin\n        if (bnch.action == 'move'):\n            x = int((subwin.x + (x - bnch.dx)))\n            y = int((subwin.y + (y - bnch.dy)))\n            self.move(subwin.frame, x, y)\n        elif (bnch.action == 'resize'):\n            wd = int((subwin.width + (x - bnch.dx)))\n            ht = int((subwin.height + (y - bnch.dy)))\n            subwin.frame.set_size_request(wd, ht)\n    return True\n", "label": 1}
{"function": "\n\ndef step_1b(w):\n    ' Step 1b handles -ed and -ing suffixes (or -edly and -ingly).\\n        Removes double consonants at the end of the stem and adds -e to some words.\\n    '\n    if (w.endswith('y') and w.endswith(('edly', 'ingly'))):\n        w = w[:(- 2)]\n    if w.endswith(('ed', 'ing')):\n        if w.endswith('ied'):\n            return (((len(w) == 4) and w[:(- 1)]) or w[:(- 2)])\n        if w.endswith('eed'):\n            return ((R1(w).endswith('eed') and w[:(- 1)]) or w)\n        for suffix in ('ed', 'ing'):\n            if (w.endswith(suffix) and has_vowel(w[:(- len(suffix))])):\n                w = w[:(- len(suffix))]\n                if w.endswith(('at', 'bl', 'iz')):\n                    return (w + 'e')\n                if is_double_consonant(w[(- 2):]):\n                    return w[:(- 1)]\n                if is_short(w):\n                    return (w + 'e')\n    return w\n", "label": 1}
{"function": "\n\ndef _FormatHumanReadableSize(self, size):\n    'Formats the size as a human readable string.\\n\\n    Args:\\n      size: The size in bytes.\\n\\n    Returns:\\n      A human readable string of the size.\\n    '\n    magnitude_1000 = 0\n    size_1000 = float(size)\n    while (size_1000 >= 1000):\n        size_1000 /= 1000\n        magnitude_1000 += 1\n    magnitude_1024 = 0\n    size_1024 = float(size)\n    while (size_1024 >= 1024):\n        size_1024 /= 1024\n        magnitude_1024 += 1\n    size_string_1000 = None\n    if ((magnitude_1000 > 0) and (magnitude_1000 <= 7)):\n        size_string_1000 = '{0:.1f}{1:s}'.format(size_1000, self._UNITS_1000[magnitude_1000])\n    size_string_1024 = None\n    if ((magnitude_1024 > 0) and (magnitude_1024 <= 7)):\n        size_string_1024 = '{0:.1f}{1:s}'.format(size_1024, self._UNITS_1024[magnitude_1024])\n    if ((not size_string_1000) or (not size_string_1024)):\n        return '{0:d} B'.format(size)\n    return '{0:s} / {1:s} ({2:d} B)'.format(size_string_1024, size_string_1000, size)\n", "label": 1}
{"function": "\n\ndef prepare_blasts(blasts, user=None, bundle=False):\n    blasts = list(blasts.select_related('user'))\n    for blast in blasts:\n        blast.set_viewing_user(user)\n    if bundle:\n        new_blasts = []\n        current_bundle = []\n        current_bundle_date = None\n        for blast in blasts:\n            if (blast.short and ((not current_bundle_date) or (blast.created.date() == current_bundle_date))):\n                current_bundle.append(blast)\n                current_bundle_date = blast.created.date()\n            else:\n                if current_bundle:\n                    new_blasts.append(BlastBundle(current_bundle))\n                    current_bundle = []\n                    current_bundle_date = None\n                new_blasts.append(blast)\n        if current_bundle:\n            new_blasts.append(BlastBundle(current_bundle))\n        blasts = new_blasts\n    return blasts\n", "label": 1}
{"function": "\n\ndef evalModify(ctx, u):\n    originalctx = ctx\n    if u.using:\n        otherDefault = False\n        for d in u.using:\n            if d.default:\n                if (not otherDefault):\n                    dg = Graph()\n                    ctx = ctx.pushGraph(dg)\n                    otherDefault = True\n                ctx.load(d.default, default=True)\n            elif d.named:\n                g = d.named\n                ctx.load(g, default=False)\n    if ((not u.using) and u.withClause):\n        g = ctx.dataset.get_context(u.withClause)\n        ctx = ctx.pushGraph(g)\n    res = evalPart(ctx, u.where)\n    if u.using:\n        if otherDefault:\n            ctx = originalctx\n        if u.withClause:\n            g = ctx.dataset.get_context(u.withClause)\n            ctx = ctx.pushGraph(g)\n    for c in res:\n        dg = ctx.graph\n        if u.delete:\n            dg -= _fillTemplate(u.delete.triples, c)\n            for (g, q) in u.delete.quads.iteritems():\n                cg = ctx.dataset.get_context(c.get(g))\n                cg -= _fillTemplate(q, c)\n        if u.insert:\n            dg += _fillTemplate(u.insert.triples, c)\n            for (g, q) in u.insert.quads.iteritems():\n                cg = ctx.dataset.get_context(c.get(g))\n                cg += _fillTemplate(q, c)\n", "label": 1}
{"function": "\n\ndef build(self, input_shape):\n    self.input_spec = [InputSpec(dtype=K.floatx(), shape=((None,) + input_shape[1:]))]\n    input_dim = input_shape[2]\n    self.W = self.init((input_dim, self.output_dim), name='{}_W'.format(self.name))\n    if self.bias:\n        self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n        self.trainable_weights = [self.W, self.b]\n    self.regularizers = []\n    if self.W_regularizer:\n        self.W_regularizer.set_param(self.W)\n        self.regularizers.append(self.W_regularizer)\n    if (self.bias and self.b_regularizer):\n        self.b_regularizer.set_param(self.b)\n        self.regularizers.append(self.b_regularizer)\n    if self.activity_regularizer:\n        self.activity_regularizer.set_layer(self)\n        self.regularizers.append(self.activity_regularizer)\n    self.constraints = {\n        \n    }\n    if self.W_constraint:\n        self.constraints[self.W] = self.W_constraint\n    if (self.bias and self.b_constraint):\n        self.constraints[self.b] = self.b_constraint\n    if (self.initial_weights is not None):\n        self.set_weights(self.initial_weights)\n        del self.initial_weights\n", "label": 1}
{"function": "\n\n@must_be_valid_project\n@must_not_be_registration\n@must_have_addon('github', 'node')\ndef github_hook_callback(node_addon, **kwargs):\n    'Add logs for commits from outside OSF.\\n\\n    '\n    if (request.json is None):\n        return {\n            \n        }\n    verify_hook_signature(node_addon, request.data, request.headers)\n    node = (kwargs['node'] or kwargs['project'])\n    payload = request.json\n    for commit in payload.get('commits', []):\n        if (commit['message'] and (commit['message'] in MESSAGES.values())):\n            continue\n        _id = commit['id']\n        date = dateparse(commit['timestamp'])\n        committer = commit['committer']['name']\n        for path in commit.get('added', []):\n            add_hook_log(node, node_addon, ('github_' + NodeLog.FILE_ADDED), path, date, committer, include_urls=True, sha=_id)\n        for path in commit.get('modified', []):\n            add_hook_log(node, node_addon, ('github_' + NodeLog.FILE_UPDATED), path, date, committer, include_urls=True, sha=_id)\n        for path in commit.get('removed', []):\n            add_hook_log(node, node_addon, ('github_' + NodeLog.FILE_REMOVED), path, date, committer)\n    node.save()\n", "label": 1}
{"function": "\n\ndef highlight_html_differences(s1, s2):\n    differ = diff_match_patch()\n    ops = differ.diff_main(s1, s2)\n    differ.diff_cleanupSemantic(ops)\n    retval = ''\n    in_tag = False\n    idx = 0\n    while (idx < len(ops)):\n        (op, text) = ops[idx]\n        next_op = None\n        if (idx != (len(ops) - 1)):\n            (next_op, next_text) = ops[(idx + 1)]\n        if ((op == diff_match_patch.DIFF_DELETE) and (next_op == diff_match_patch.DIFF_INSERT)):\n            (chunks, in_tag) = chunkize(next_text, in_tag)\n            retval += highlight_chunks(chunks, highlight_replaced)\n            idx += 1\n        elif ((op == diff_match_patch.DIFF_INSERT) and (next_op == diff_match_patch.DIFF_DELETE)):\n            (chunks, in_tag) = chunkize(text, in_tag)\n            retval += highlight_chunks(chunks, highlight_replaced)\n            idx += 1\n        elif (op == diff_match_patch.DIFF_DELETE):\n            retval += highlight_deleted('&nbsp;')\n        elif (op == diff_match_patch.DIFF_INSERT):\n            (chunks, in_tag) = chunkize(text, in_tag)\n            retval += highlight_chunks(chunks, highlight_inserted)\n        elif (op == diff_match_patch.DIFF_EQUAL):\n            (chunks, in_tag) = chunkize(text, in_tag)\n            retval += text\n        idx += 1\n    if (not verify_html(retval)):\n        from zerver.lib.actions import internal_send_message\n        logging.getLogger('').error('HTML diff produced mal-formed HTML')\n        if (settings.ERROR_BOT is not None):\n            subject = ('HTML diff failure on %s' % (platform.node(),))\n            internal_send_message(settings.ERROR_BOT, 'stream', 'errors', subject, 'HTML diff produced malformed HTML')\n        return s2\n    return retval\n", "label": 1}
{"function": "\n\ndef _PrettyIndent(self, elem, level=0):\n    'Prettifies an element tree in-place'\n    i = ('\\n' + (level * '  '))\n    if len(elem):\n        if ((not elem.text) or (not elem.text.strip())):\n            elem.text = (i + '  ')\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n        for elem in elem:\n            self._PrettyIndent(elem, (level + 1))\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n    elif (level and ((not elem.tail) or (not elem.tail.strip()))):\n        elem.tail = i\n", "label": 1}
{"function": "\n\ndef _scale_to_res(self):\n    ' Change self._A and _extent to render an image whose\\n        resolution is matched to the eventual rendering.'\n    ax = self.axes\n    shp = self._full_res.shape\n    (x0, x1, sx, y0, y1, sy) = extract_matched_slices(ax, shp)\n    if ((self._bounds is not None) and (sx >= self._sx) and (sy >= self._sy) and (x0 >= self._bounds[0]) and (x1 <= self._bounds[1]) and (y0 >= self._bounds[2]) and (y1 <= self._bounds[3])):\n        return\n    self._A = self._full_res[y0:y1:sy, x0:x1:sx]\n    self._A = cbook.safe_masked_invalid(self._A)\n    if (self.origin == 'upper'):\n        self.set_extent([(x0 - 0.5), (x1 - 0.5), (y1 - 0.5), (y0 - 0.5)])\n    else:\n        self.set_extent([(x0 - 0.5), (x1 - 0.5), (y0 - 0.5), (y1 - 0.5)])\n    self._sx = sx\n    self._sy = sy\n    self._bounds = (x0, x1, y0, y1)\n    self.changed()\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    sep = ''\n    string = ''\n    if self._modifier:\n        string = (self._modifier + ' ')\n    if self._segment:\n        string += (self._segment + ':')\n    string += '['\n    if self._base:\n        string += self._base\n    if self._index:\n        if self._base:\n            string += ((sep + '+') + sep)\n        string += self._index\n        string += (((sep + '*') + sep) + str(self._scale))\n    if (self._displacement != 0):\n        if (self._base or self._index):\n            string += ((sep + '+') + sep)\n        imm_hex = hex((self._displacement & ((2 ** 32) - 1)))\n        string += (imm_hex[:(- 1)] if (imm_hex[(- 1)] == 'L') else imm_hex)\n    string += ']'\n    return string\n", "label": 1}
{"function": "\n\ndef _get_format_from_style(self, token, style):\n    ' Returns a QTextCharFormat for token by reading a Pygments style.\\n        '\n    result = QtGui.QTextCharFormat()\n    for (key, value) in list(style.style_for_token(token).items()):\n        if value:\n            if (key == 'color'):\n                result.setForeground(self._get_brush(value))\n            elif (key == 'bgcolor'):\n                result.setBackground(self._get_brush(value))\n            elif (key == 'bold'):\n                result.setFontWeight(QtGui.QFont.Bold)\n            elif (key == 'italic'):\n                result.setFontItalic(True)\n            elif (key == 'underline'):\n                result.setUnderlineStyle(QtGui.QTextCharFormat.SingleUnderline)\n            elif (key == 'sans'):\n                result.setFontStyleHint(QtGui.QFont.SansSerif)\n            elif (key == 'roman'):\n                result.setFontStyleHint(QtGui.QFont.Times)\n            elif (key == 'mono'):\n                result.setFontStyleHint(QtGui.QFont.TypeWriter)\n    return result\n", "label": 1}
{"function": "\n\ndef export_users(users, workbook, mimic_upload=False):\n    data_prefix = ('data: ' if mimic_upload else 'd.')\n    user_keys = ('user_id', 'username', 'is_active', 'name', 'groups')\n    user_rows = []\n    fields = set()\n    for user in users:\n        user_row = {\n            \n        }\n        for key in user_keys:\n            if (key == 'username'):\n                user_row[key] = user.raw_username\n            elif (key == 'name'):\n                user_row[key] = user.full_name\n            elif (key == 'groups'):\n                user_row[key] = ', '.join(user.get_group_ids())\n            else:\n                user_row[key] = getattr(user, key)\n        for key in user.user_data:\n            user_row[('%s%s' % (data_prefix, key))] = user.user_data[key]\n        user_rows.append(user_row)\n        fields.update(user_row.keys())\n    workbook.open('Users', (list(user_keys) + sorted((fields - set(user_keys)))))\n    for user_row in user_rows:\n        workbook.write_row('Users', user_row)\n", "label": 1}
{"function": "\n\ndef visit(self):\n    '\\n        This function *visits* its children in a recursive\\n        way.\\n\\n        It will first *visit* the children that\\n        that have a z-order value less than 0.\\n\\n        Then it will call the :meth:`draw` method to\\n        draw itself.\\n\\n        And finally it will *visit* the rest of the\\n        children (the ones with a z-value bigger\\n        or equal than 0)\\n\\n        Before *visiting* any children it will call\\n        the :meth:`transform` method to apply any possible\\n        transformations.\\n        '\n    if (not self.visible):\n        return\n    position = 0\n    if (self.grid and self.grid.active):\n        self.grid.before_draw()\n    if (self.children and (self.children[0][0] < 0)):\n        gl.glPushMatrix()\n        self.transform()\n        for (z, c) in self.children:\n            if (z >= 0):\n                break\n            position += 1\n            c.visit()\n        gl.glPopMatrix()\n    self.draw()\n    if (position < len(self.children)):\n        gl.glPushMatrix()\n        self.transform()\n        for (z, c) in self.children[position:]:\n            c.visit()\n        gl.glPopMatrix()\n    if (self.grid and self.grid.active):\n        self.grid.after_draw(self.camera)\n", "label": 1}
{"function": "\n\ndef rebase(self, base, shallow=False, reuse_ext=False, copy=False, relative=False):\n    'Rebases block to new base\\n\\n        :param shallow: Limit copy to top of source backing chain\\n        :param reuse_ext: Reuse existing external file of a copy\\n        :param copy: Start a copy job\\n        :param relative: Keep backing chain referenced using relative names\\n        '\n    flags = ((shallow and libvirt.VIR_DOMAIN_BLOCK_REBASE_SHALLOW) or 0)\n    flags |= ((reuse_ext and libvirt.VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT) or 0)\n    flags |= ((copy and libvirt.VIR_DOMAIN_BLOCK_REBASE_COPY) or 0)\n    flags |= ((relative and libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE) or 0)\n    return self._guest._domain.blockRebase(self._disk, base, self.REBASE_DEFAULT_BANDWIDTH, flags=flags)\n", "label": 1}
{"function": "\n\ndef _create_mail(self, email):\n    'A helper method that creates mail for sending.'\n    if (not email.recipients()):\n        return False\n    from_email = sanitize_address(email.from_email, email.encoding)\n    recipients = [sanitize_address(addr, email.encoding) for addr in email.recipients()]\n    mail = sendgrid.Mail()\n    mail.add_to(recipients)\n    mail.add_cc(email.cc)\n    mail.add_bcc(email.bcc)\n    mail.set_text(email.body)\n    mail.set_subject(email.subject)\n    mail.set_from(from_email)\n    if isinstance(email, EmailMultiAlternatives):\n        for alt in email.alternatives:\n            if (alt[1] == 'text/html'):\n                mail.set_html(alt[0])\n    for attachment in email.attachments:\n        if isinstance(attachment, MIMEBase):\n            mail.add_attachment_stream(attachment.get_filename(), attachment.get_payload())\n        elif isinstance(attachment, tuple):\n            mail.add_attachment_stream(attachment[0], attachment[1])\n    return mail\n", "label": 1}
{"function": "\n\ndef _trim(docstring, return_lines=False):\n    '\\n    Trim of unnecessary leading indentations.\\n    '\n    if (not docstring):\n        return ''\n    lines = docstring.expandtabs().splitlines()\n    indent = _min_indent(lines)\n    trimmed = [lines[0].strip()]\n    if (indent < _maxsize):\n        for line in lines[1:]:\n            trimmed.append(line[indent:].rstrip())\n    while (trimmed and (not trimmed[(- 1)])):\n        trimmed.pop()\n    while (trimmed and (not trimmed[0])):\n        trimmed.pop(0)\n    if return_lines:\n        return trimmed\n    else:\n        return '\\n'.join(trimmed)\n", "label": 1}
{"function": "\n\ndef get_result(self):\n    what = self.expr.op()\n    if self.memoize:\n        self._memoize_tables()\n    if (isinstance(what, ops.TableNode) and what.has_schema()):\n        if ((not self.memoize) and (what in self.memo)):\n            text = ('Table: %s' % self.memo.get_alias(what))\n        elif isinstance(what, ops.PhysicalTable):\n            text = self._format_table(what)\n        else:\n            text = self._format_node(what)\n    elif isinstance(what, ops.TableColumn):\n        text = self._format_column(self.expr)\n    elif isinstance(what, ir.Node):\n        text = self._format_node(what)\n    elif isinstance(what, ops.Literal):\n        text = ('Literal[%s] %s' % (self._get_type_display(), str(what.value)))\n    if (isinstance(self.expr, ir.ValueExpr) and (self.expr._name is not None)):\n        text = '{0} = {1}'.format(self.expr.get_name(), text)\n    if self.memoize:\n        alias_to_text = [(self.memo.aliases[x], self.memo.formatted[x], self.memo.ops[x]) for x in self.memo.formatted]\n        alias_to_text.sort()\n        refs = [((x + '\\n') + y) for (x, y, op) in alias_to_text if (not op.equals(what))]\n        text = '\\n\\n'.join((refs + [text]))\n    return self._indent(text, self.base_level)\n", "label": 1}
{"function": "\n\ndef __init__(self, host, port=None, key_file=None, cert_file=None, strict=_strict_sentinel, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None, **_3to2kwargs):\n    if ('check_hostname' in _3to2kwargs):\n        check_hostname = _3to2kwargs['check_hostname']\n        del _3to2kwargs['check_hostname']\n    else:\n        check_hostname = None\n    if ('context' in _3to2kwargs):\n        context = _3to2kwargs['context']\n        del _3to2kwargs['context']\n    else:\n        context = None\n    super(HTTPSConnection, self).__init__(host, port, strict, timeout, source_address)\n    self.key_file = key_file\n    self.cert_file = cert_file\n    if (context is None):\n        context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n        context.options |= ssl.OP_NO_SSLv2\n    will_verify = (context.verify_mode != ssl.CERT_NONE)\n    if (check_hostname is None):\n        check_hostname = will_verify\n    elif (check_hostname and (not will_verify)):\n        raise ValueError('check_hostname needs a SSL context with either CERT_OPTIONAL or CERT_REQUIRED')\n    if (key_file or cert_file):\n        context.load_cert_chain(cert_file, key_file)\n    self._context = context\n    self._check_hostname = check_hostname\n", "label": 1}
{"function": "\n\n@eventmgr_rfc1459.message('TOPIC', min_params=1, update_idle=True)\ndef m_TOPIC(cli, ev_msg):\n    chanlist = ev_msg['params'][0].split(',')\n    for chan in chanlist:\n        if (not validate_chan(chan)):\n            cli.dump_numeric('479', [chan, 'Illegal channel name'])\n            return\n        ch = cli.ctx.chmgr.get(chan, create=False)\n        if (not ch):\n            cli.dump_numeric('403', [chan, 'No such channel'])\n            continue\n        if (not ch.has_member(cli)):\n            cli.dump_numeric('442', [ch.name, \"You're not on that channel\"])\n            continue\n        if (len(ev_msg['params']) == 1):\n            if ch.topic:\n                cli.dump_numeric('332', [ch.name, ch.topic])\n                cli.dump_numeric('333', [ch.name, ch.topic_setter, ch.topic_ts])\n                continue\n            cli.dump_numeric('331', [ch.name, 'No topic is set'])\n            continue\n        if (not ch.props.get('op-topic')):\n            cli.dump_numeric('482', [ch.name, \"You're not a channel operator\"])\n            continue\n        topic = ev_msg['params'][1]\n        topiclen = cli.ctx.conf.limits.get('topic', None)\n        if (topiclen and (len(ev_msg['params'][1]) > topiclen)):\n            topic = topic[:topiclen]\n        ch.topic = topic\n        ch.topic_setter = cli.hostmask\n        ch.topic_ts = cli.ctx.current_ts\n        ch.dump_message(RFC1459Message.from_data('TOPIC', source=cli, params=[ch.name, ch.topic]))\n", "label": 1}
{"function": "\n\ndef __new__(cls, key, secret=None, api_version=DEFAULT_API_VERSION, **kwargs):\n    if (cls is OpenNebulaNodeDriver):\n        if (api_version in ['1.4']):\n            cls = OpenNebula_1_4_NodeDriver\n        elif (api_version in ['2.0', '2.2']):\n            cls = OpenNebula_2_0_NodeDriver\n        elif (api_version in ['3.0']):\n            cls = OpenNebula_3_0_NodeDriver\n        elif (api_version in ['3.2']):\n            cls = OpenNebula_3_2_NodeDriver\n        elif (api_version in ['3.6']):\n            cls = OpenNebula_3_6_NodeDriver\n        elif (api_version in ['3.8']):\n            cls = OpenNebula_3_8_NodeDriver\n            if ('plain_auth' not in kwargs):\n                kwargs['plain_auth'] = cls.plain_auth\n            else:\n                cls.plain_auth = kwargs['plain_auth']\n        else:\n            raise NotImplementedError(('No OpenNebulaNodeDriver found for API version %s' % api_version))\n        return super(OpenNebulaNodeDriver, cls).__new__(cls)\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    super_new = super(CategoryMetaclass, cls).__new__\n    parents = [b for b in bases if (isinstance(b, CategoryMetaclass) and (not ((b.__name__ == 'NewBase') and (b.__mro__ == (b, object)))))]\n    if (not parents):\n        return super_new(cls, name, bases, attrs)\n    new_cls = super_new(cls, name, bases, attrs)\n    if (not hasattr(new_cls, 'name')):\n        raise ImproperlyConfigured(('No name attribute defined in %s.' % new_cls))\n    if (not hasattr(new_cls, 'slug')):\n        raise ImproperlyConfigured(('No slug attribute defined in %s.' % new_cls))\n    if (not hasattr(new_cls, 'dashboard')):\n        raise ImproperlyConfigured(('No dashboard attribute defined in %s.' % new_cls))\n    if (not re.match('^[\\\\w-]+$', new_cls.slug)):\n        raise ImproperlyConfigured(('The slug attribute defined in %s contains invalid chars.' % new_cls))\n    return new_cls\n", "label": 1}
{"function": "\n\ndef _plot_update_evoked(params, bools):\n    ' update the plot evoked lines\\n    '\n    (picks, evoked) = [params[k] for k in ('picks', 'evoked')]\n    times = (evoked.times * 1000.0)\n    projs = [proj for (ii, proj) in enumerate(params['projs']) if (ii in np.where(bools)[0])]\n    params['proj_bools'] = bools\n    new_evoked = evoked.copy()\n    new_evoked.info['projs'] = []\n    new_evoked.add_proj(projs)\n    new_evoked.apply_proj()\n    for (ax, t) in zip(params['axes'], params['ch_types_used']):\n        this_scaling = params['scalings'][t]\n        idx = [picks[i] for i in range(len(picks)) if (params['types'][i] == t)]\n        D = (this_scaling * new_evoked.data[idx, :])\n        if (params['plot_type'] == 'butterfly'):\n            for (line, di) in zip(ax.lines, D):\n                line.set_data(times, di)\n        else:\n            ax.images[0].set_data(D)\n    params['fig'].canvas.draw()\n", "label": 1}
{"function": "\n\ndef phys_tokens(toks):\n    \"Return all physical tokens, even line continuations.\\n\\n    tokenize.generate_tokens() doesn't return a token for the backslash that\\n    continues lines.  This wrapper provides those tokens so that we can\\n    re-create a faithful representation of the original source.\\n\\n    Returns the same values as generate_tokens()\\n\\n    \"\n    last_line = None\n    last_lineno = (- 1)\n    last_ttype = None\n    for (ttype, ttext, (slineno, scol), (elineno, ecol), ltext) in toks:\n        if (last_lineno != elineno):\n            if (last_line and last_line.endswith('\\\\\\n')):\n                inject_backslash = True\n                if (last_ttype == tokenize.COMMENT):\n                    inject_backslash = False\n                elif (ttype == token.STRING):\n                    if (('\\n' in ttext) and (ttext.split('\\n', 1)[0][(- 1)] == '\\\\')):\n                        inject_backslash = False\n                if inject_backslash:\n                    ccol = (len(last_line.split('\\n')[(- 2)]) - 1)\n                    (yield (99999, '\\\\\\n', (slineno, ccol), (slineno, (ccol + 2)), last_line))\n            last_line = ltext\n            last_ttype = ttype\n        (yield (ttype, ttext, (slineno, scol), (elineno, ecol), ltext))\n        last_lineno = elineno\n", "label": 1}
{"function": "\n\ndef _validate_for_numeric_binop(self, other, op, opstr):\n    '\\n        return valid other, evaluate or raise TypeError\\n        if we are not of the appropriate type\\n\\n        internal method called by ops\\n        '\n    from pandas.tseries.offsets import DateOffset\n    if (not self._is_numeric_dtype):\n        raise TypeError('cannot evaluate a numeric op {opstr} for type: {typ}'.format(opstr=opstr, typ=type(self)))\n    if isinstance(other, Index):\n        if (not other._is_numeric_dtype):\n            raise TypeError('cannot evaluate a numeric op {opstr} with type: {typ}'.format(opstr=type(self), typ=type(other)))\n    elif (isinstance(other, np.ndarray) and (not other.ndim)):\n        other = other.item()\n    if isinstance(other, (Index, ABCSeries, np.ndarray)):\n        if (len(self) != len(other)):\n            raise ValueError('cannot evaluate a numeric op with unequal lengths')\n        other = _values_from_object(other)\n        if (other.dtype.kind not in ['f', 'i']):\n            raise TypeError('cannot evaluate a numeric op with a non-numeric dtype')\n    elif isinstance(other, (DateOffset, np.timedelta64, Timedelta, datetime.timedelta)):\n        pass\n    elif isinstance(other, (Timestamp, np.datetime64)):\n        pass\n    elif (not (is_float(other) or is_integer(other))):\n        raise TypeError('can only perform ops with scalar values')\n    return other\n", "label": 1}
{"function": "\n\ndef handle_response(self, payload):\n    ' Handle a tunnel response\\n        '\n    if ('msgid' in payload):\n        msgid = payload['msgid']\n        if (msgid in self.connections):\n            if (('cmd' in payload) and ('header' in payload) and ('data' in payload)):\n                cmd = payload['cmd']\n                if (cmd == 'httpresp'):\n                    self.send_response(msgid, payload['header'], payload['data'], True)\n                    return\n                elif (cmd == 'logresp'):\n                    self.send_response(msgid, payload['header'], payload['data'], False)\n                    return\n                elif (cmd == 'logevent'):\n                    result = self.send_response(msgid, payload['header'], payload['data'], False)\n                    if (not result):\n                        msg = {\n                            'cmd': 'logclose',\n                        }\n                        self.tunnel.send(msg)\n                    return\n    _log.error(('Unknown control proxy response %s' % payload))\n", "label": 1}
{"function": "\n\ndef get_metric_datapoints_for(engine, db_name, hostname, url, metric_name=None, granurality=None, from_option=None):\n    datapoints = {\n        \n    }\n    if (engine == 'mongodb'):\n        graphs = MONGODB_METRICS\n    elif (engine == 'mysql'):\n        graphs = MYSQL_METRICS\n    elif (engine == 'redis'):\n        graphs = REDIS_METRICS\n    else:\n        graphs = None\n    newgraph = []\n    for graph in graphs:\n        newserie = []\n        if (metric_name is None):\n            zoomtype = ''\n        elif (graph['name'] != metric_name):\n            continue\n        else:\n            zoomtype = 'x'\n        for serie in graph['series']:\n            datapoints = get_graphite_metrics_datapoints(from_option, engine, db_name, hostname, serie['data'], granurality, url=url, normalize_series=graph['normalize_series'])\n            if datapoints:\n                newserie.append({\n                    'name': serie['name'],\n                    'data': datapoints,\n                })\n            else:\n                newserie.append({\n                    'name': serie['name'],\n                    'data': [],\n                })\n        newgraph.append({\n            'name': graph['name'],\n            'series': str(ast.literal_eval(json.dumps(newserie))),\n            'type': graph['type'],\n            'tooltip_point_format': graph['tooltip_point_format'],\n            'y_axis_title': graph['y_axis_title'],\n            'stacking': graph['stacking'],\n            'graph_name': graph['graph_name'],\n            'zoomtype': zoomtype,\n            'normalize_series': graph['normalize_series'],\n        })\n    return newgraph\n", "label": 1}
{"function": "\n\ndef trace(*predicates, **options):\n    '\\n    Starts tracing. Can be used as a context manager (with slightly incorrect semantics - it starts tracing\\n    before ``__enter__`` is called).\\n\\n    Parameters:\\n        *predicates (callables): Runs actions if **all** of the given predicates match.\\n    Keyword Args:\\n        clear_env_var: Disables tracing in subprocess. Default: ``False``.\\n        threading_support: Enable tracing *new* threads. Default: ``False``.\\n        action: Action to run if all the predicates return ``True``. Default: ``CodePrinter``.\\n        actions: Actions to run (in case you want more than 1).\\n    '\n    global _last_tracer\n    clear_env_var = options.pop('clear_env_var', False)\n    threading_support = (options.pop('threading_support', False) or options.pop('threads_support', False) or options.pop('thread_support', False) or options.pop('threadingsupport', False) or options.pop('threadssupport', False) or options.pop('threadsupport', False) or options.pop('threading', False) or options.pop('threads', False) or options.pop('thread', False))\n    predicate = _prepare_predicate(*predicates, **options)\n    if clear_env_var:\n        os.environ.pop('PYTHONHUNTER', None)\n    _last_tracer = Tracer(threading_support)\n\n    @atexit.register\n    def atexit_cleanup(ref=weakref.ref(_last_tracer)):\n        maybe_tracer = ref()\n        if (maybe_tracer is not None):\n            maybe_tracer.stop()\n    return _last_tracer.trace(predicate)\n", "label": 1}
{"function": "\n\ndef all(self, category=None, return_key_and_category=False):\n    '\\n        Get all tags in this handler.\\n\\n        Args:\\n            category (str, optional): The Tag category to limit the\\n                request to. Note that `None` is the valid, default\\n                category.\\n            return_key_and_category (bool, optional): Return a list of\\n                tuples `[(key, category), ...]`.\\n\\n        Returns:\\n            tags (list): A list of tag keys `[tagkey, tagkey, ...]` or\\n                a list of tuples `[(key, category), ...]` if\\n                `return_key_and_category` is set.\\n\\n        '\n    if ((self._cache is None) or (not _TYPECLASS_AGGRESSIVE_CACHE)):\n        self._recache()\n    if category:\n        category = (category.strip().lower() if (category is not None) else None)\n        matches = [tag for tag in self._cache.values() if (tag.db_category == category)]\n    else:\n        matches = self._cache.values()\n    if matches:\n        matches = sorted(matches, key=(lambda o: o.id))\n        if return_key_and_category:\n            return [(to_str(p.db_key), to_str(p.db_category)) for p in matches]\n        else:\n            return [to_str(p.db_key) for p in matches]\n    return []\n", "label": 1}
{"function": "\n\ndef dictify(values, colnames):\n    \"\\n    Convert a list of values into a dictionary based upon given column names.\\n\\n    If the column name starts with an '@', the value is assumed to be a comma\\n    separated list.\\n\\n    If the name starts with a '#', the value is assumed to be an int.\\n\\n    If the name starts with '@#', the value is assumed to  a comma separated\\n    list of ints.\\n\\n    \"\n    d = {\n        \n    }\n    for i in xrange(len(colnames)):\n        key = colnames[i]\n        split = False\n        num = False\n        if (key[0] == '@'):\n            key = key[1:]\n            split = True\n        if (key[0] == '#'):\n            key = key[1:]\n            num = True\n        if (i < len(values)):\n            if (num and split):\n                val = [int(x) for x in values[i].rstrip(',').split(',')]\n            elif num:\n                val = int(values[i])\n            elif split:\n                val = values[i].rstrip(',').split(',')\n            else:\n                val = values[i]\n            d[key] = val\n        else:\n            d[key] = None\n    return d\n", "label": 1}
{"function": "\n\ndef __init__(self, function):\n    self.spec = getattr(function, 'original', function)\n    self.arguments = introspect.arguments(function)\n    self._function = function\n    self.is_coroutine = introspect.is_coroutine(self.spec)\n    if self.is_coroutine:\n        self.spec = getattr(self.spec, '__wrapped__', self.spec)\n    self.takes_kargs = introspect.takes_kargs(self.spec)\n    self.takes_kwargs = introspect.takes_kwargs(self.spec)\n    self.parameters = introspect.arguments(self.spec, (1 if self.takes_kargs else 0))\n    if self.takes_kargs:\n        self.karg = self.parameters[(- 1)]\n    self.defaults = {\n        \n    }\n    for (index, default) in enumerate(reversed((self.spec.__defaults__ or ()))):\n        self.defaults[self.parameters[(- (index + 1))]] = default\n    self.required = self.parameters[:((- len((self.spec.__defaults__ or ()))) or None)]\n    if (introspect.is_method(self.spec) or introspect.is_method(function)):\n        self.required = self.required[1:]\n        self.parameters = self.parameters[1:]\n    self.transform = self.spec.__annotations__.get('return', None)\n    self.directives = {\n        \n    }\n    self.input_transformations = {\n        \n    }\n    for (name, transformer) in self.spec.__annotations__.items():\n        if isinstance(transformer, str):\n            continue\n        elif hasattr(transformer, 'directive'):\n            self.directives[name] = transformer\n            continue\n        if hasattr(transformer, 'load'):\n            transformer = MarshmallowSchema(transformer)\n        elif hasattr(transformer, 'deserialize'):\n            transformer = transformer.deserialize\n        self.input_transformations[name] = transformer\n", "label": 1}
{"function": "\n\ndef __init__(self, log_category='jcli'):\n    CmdProtocol.__init__(self, log_category)\n    self.authentication = {\n        'username': None,\n        'password': None,\n        'printedPassword': None,\n        'auth': False,\n    }\n    if ('persist' not in self.commands):\n        self.commands.append('persist')\n    if ('load' not in self.commands):\n        self.commands.append('load')\n    if ('user' not in self.commands):\n        self.commands.append('user')\n    if ('group' not in self.commands):\n        self.commands.append('group')\n    if ('filter' not in self.commands):\n        self.commands.append('filter')\n    if ('mointerceptor' not in self.commands):\n        self.commands.append('mointerceptor')\n    if ('mtinterceptor' not in self.commands):\n        self.commands.append('mtinterceptor')\n    if ('morouter' not in self.commands):\n        self.commands.append('morouter')\n    if ('mtrouter' not in self.commands):\n        self.commands.append('mtrouter')\n    if ('smppccm' not in self.commands):\n        self.commands.append('smppccm')\n    if ('httpccm' not in self.commands):\n        self.commands.append('httpccm')\n    if ('stats' not in self.commands):\n        self.commands.append('stats')\n", "label": 1}
{"function": "\n\n@staticmethod\ndef normalize_series_by_unit(series):\n    \"Transform series' values into a more human readable form:\\n        1) Determine the data point with the maximum value\\n        2) Decide the unit appropriate for this value (normalize it)\\n        3) Convert other values to this new unit, if necessary\\n        \"\n    if (not series):\n        return series\n    source_unit = target_unit = series[0]['unit']\n    if (not units.is_supported(source_unit)):\n        return series\n    maximum = max([d['y'] for point in series for d in point['data']])\n    unit = units.normalize(maximum, source_unit)[1]\n    if units.is_larger(unit, target_unit):\n        target_unit = unit\n        for (i, point) in enumerate(series[:]):\n            if (point['unit'] != target_unit):\n                series[i]['unit'] = target_unit\n                for (j, d) in enumerate(point['data'][:]):\n                    series[i]['data'][j]['y'] = units.convert(d['y'], source_unit, target_unit, fmt=True)[0]\n    return series\n", "label": 1}
{"function": "\n\ndef _separatevars_dict(expr, symbols):\n    if symbols:\n        if (not all((t.is_Atom for t in symbols))):\n            raise ValueError('symbols must be Atoms.')\n        symbols = list(symbols)\n    elif (symbols is None):\n        return {\n            'coeff': expr,\n        }\n    else:\n        symbols = list(expr.free_symbols)\n        if (not symbols):\n            return None\n    ret = dict(((i, []) for i in (symbols + ['coeff'])))\n    for i in Mul.make_args(expr):\n        expsym = i.free_symbols\n        intersection = set(symbols).intersection(expsym)\n        if (len(intersection) > 1):\n            return None\n        if (len(intersection) == 0):\n            ret['coeff'].append(i)\n        else:\n            ret[intersection.pop()].append(i)\n    for (k, v) in ret.items():\n        ret[k] = Mul(*v)\n    return ret\n", "label": 1}
{"function": "\n\ndef figshare_hgrid_data(node_settings, auth, parent=None, **kwargs):\n    node = node_settings.owner\n    if (node_settings.figshare_type == 'project'):\n        item = Figshare.from_settings(node_settings.user_settings).project(node_settings, node_settings.figshare_id)\n    else:\n        item = Figshare.from_settings(node_settings.user_settings).article(node_settings, node_settings.figshare_id)\n    if ((not node_settings.figshare_id) or (not node_settings.has_auth) or (not item)):\n        return\n    node_settings.figshare_title = (item.get('title') or item['items'][0]['title'])\n    node_settings.save()\n    return [rubeus.build_addon_root(node_settings, '{0}:{1}'.format((node_settings.figshare_title or 'Unnamed {0}'.format((node_settings.figshare_type or ''))), node_settings.figshare_id), permissions=auth, nodeUrl=node.url, nodeApiUrl=node.api_url, extra={\n        'status': (item.get('articles') or item['items'])[0]['status'].lower(),\n    })]\n", "label": 1}
{"function": "\n\n@task\ndef deploy(type=None):\n    'deploy your site, support rsync / ftp / github pages\\n\\n    run deploy:\\n        $ fab deploy\\n\\n    run deploy with specific type(not supported specify multiple types):\\n        $ fab deploy:type=rsync\\n\\n    '\n    if (('deploy' not in configs) or (not isinstance(configs['deploy'], list))):\n        do_exit('Warning: deploy not set right in _config.yml')\n    if (type and (type not in SUPPORTED_DEPLOY_TYPES)):\n        do_exit('Warning: supported deploy type: {0}'.format(', '.join(SUPPORTED_DEPLOY_TYPES)))\n    deploy_configs = configs['deploy']\n    done = False\n    for deploy_item in deploy_configs:\n        deploy_type = deploy_item.pop('type')\n        if (type and (deploy_type != type)):\n            continue\n        func_name = 'deploy_{0}'.format(deploy_type)\n        func = globals().get(func_name)\n        if (not func):\n            do_exit('Warning: not supprt {0} deploy method'.format(deploy_type))\n        func(deploy_item)\n        done = True\n    if (not done):\n        if type:\n            do_exit('Warning: specific deploy type not configured yet')\n        else:\n            print(blue('do nothing...'))\n", "label": 1}
{"function": "\n\ndef evaluate_unlabeled_dependency(gold_file, punctuation):\n    '\\n    Evaluate unlabeled accuracy per token.\\n    '\n    md = Metadata.load_from_file('unlabeled_dependency')\n    nn = taggers.load_network(md)\n    reader = taggers.create_reader(md, gold_file)\n    logger = logging.getLogger('Logger')\n    logger.debug('Loaded network')\n    logger.debug(nn.description())\n    logger.info('Starting test...')\n    hits = 0\n    num_tokens = 0\n    sentence_hits = 0\n    num_sentences = 0\n    for (sent, heads) in zip(reader.sentences, reader.heads):\n        sent_codified = reader.codify_sentence(sent)\n        answer = nn.tag_sentence(sent_codified)\n        correct_sentence = True\n        for (i, (net_tag, gold_tag)) in enumerate(zip(answer, heads)):\n            token = sent[i]\n            if (punctuation and is_punctuation(token)):\n                continue\n            if ((net_tag == gold_tag) or ((gold_tag == i) and (net_tag == len(sent)))):\n                hits += 1\n            else:\n                correct_sentence = False\n            num_tokens += 1\n        if correct_sentence:\n            sentence_hits += 1\n        num_sentences += 1\n    accuracy = (float(hits) / num_tokens)\n    sent_accuracy = ((100 * float(sentence_hits)) / num_sentences)\n    print(('%d hits out of %d' % (hits, num_tokens)))\n    print(('%d sentences completely correct (%f%%)' % (sentence_hits, sent_accuracy)))\n    print(('Accuracy: %f%%' % (100 * accuracy)))\n", "label": 1}
{"function": "\n\ndef get_program(view):\n    plat = sublime.platform()\n    prog = sget('prog')\n    if prog:\n        return prog\n    defaults = sget('defaults')\n    pt = (view.sel()[0].begin() if (len(view.sel()) > 0) else 0)\n    for d in defaults:\n        match_platform = (plat == d.get('platform', plat))\n        scopes = d.get('scopes', None)\n        match_scopes = ((not scopes) or any([(view.score_selector(pt, s) > 0) for s in scopes]))\n        if (match_platform and match_scopes and ('prog' in d)):\n            return d.get('prog')\n    return None\n", "label": 1}
{"function": "\n\ndef _resolve_signature(self):\n    'Resolve signature.\\n        May have ambiguous case.\\n        '\n    matches = []\n    if self.scalarpos:\n        for formaltys in self.typemap:\n            match_map = []\n            for (i, (formal, actual)) in enumerate(zip(formaltys, self.argtypes)):\n                if (actual is None):\n                    actual = np.asarray(self.args[i]).dtype\n                match_map.append((actual == formal))\n            if all(match_map):\n                matches.append(formaltys)\n    if (not matches):\n        matches = []\n        for formaltys in self.typemap:\n            all_matches = all((((actual is None) or (formal == actual)) for (formal, actual) in zip(formaltys, self.argtypes)))\n            if all_matches:\n                matches.append(formaltys)\n    if (not matches):\n        raise TypeError(\"No matching version.  GPU ufunc requires array arguments to have the exact types.  This behaves like regular ufunc with casting='no'.\")\n    if (len(matches) > 1):\n        raise TypeError('Failed to resolve ufunc due to ambiguous signature. Too many untyped scalars. Use numpy dtype object to type tag.')\n    self.argtypes = matches[0]\n", "label": 1}
{"function": "\n\ndef determine_linker(self, target, src):\n    if isinstance(target, build.StaticLibrary):\n        return self.build.static_linker\n    if (len(self.build.compilers) == 1):\n        return self.build.compilers[0]\n    cpp = None\n    for c in self.build.compilers:\n        if (c.get_language() == 'cpp'):\n            cpp = c\n            break\n    if (cpp is not None):\n        for s in src:\n            if c.can_compile(s):\n                return cpp\n    for c in self.build.compilers:\n        if (c.get_language() == 'vala'):\n            continue\n        for s in src:\n            if c.can_compile(s):\n                return c\n    raise RuntimeError('Unreachable code')\n", "label": 1}
{"function": "\n\ndef on_query_completions(self, view, prefix, locations):\n    if self.IsValidScope(view):\n        settings = SublimePapyrus.GetSettings()\n        if (settings and settings.get('intelligent_code_completion', True)):\n            if self.completionRunning:\n                return\n            elif self.linterRunning:\n                return\n            self.completionRunning = True\n            completions = None\n            if (not view.find('scriptname', 0, sublime.IGNORECASE)):\n                path = view.file_name()\n                if path:\n                    (_, name) = os.path.split(path)\n                    completions = [('scriptname\\tscript header', ('ScriptName %s' % name[:name.rfind('.')]))]\n                else:\n                    completions = [('scriptname\\tscript header', 'ScriptName ')]\n            else:\n                completions = self.Completions(view, prefix, locations)\n            if completions:\n                completions = list(set(completions))\n            elif (completions == None):\n                completions = []\n            completions = (completions, (sublime.INHIBIT_WORD_COMPLETIONS | sublime.INHIBIT_EXPLICIT_COMPLETIONS))\n            self.completionRunning = False\n            return completions\n", "label": 1}
{"function": "\n\ndef get_change_for_type(verbose_name, change, field):\n    if isinstance(field, fields.files.ImageField):\n        change = ImageChange(('Current %(verbose_name)s / New %(verbose_name)s' % {\n            'verbose_name': verbose_name,\n        }), field, change)\n    else:\n        (value1, value2) = change\n        if (sys.version < '3'):\n            if (value1 and ((type(value1) is str) or (type(value1) is unicode))):\n                value1 = value1.encode('utf-8')\n            if (value2 and ((type(value2) is str) or (type(value2) is unicode))):\n                value2 = value2.encode('utf-8')\n        change = TextChange(verbose_name, field, (str(value1), str(value2)))\n    return change\n", "label": 1}
{"function": "\n\ndef _get_custom_types(self, type_definitions, imports=None):\n    'Handle custom types defined in imported template files\\n\\n        This method loads the custom type definitions referenced in \"imports\"\\n        section of the TOSCA YAML template.\\n        '\n    custom_defs = {\n        \n    }\n    type_defs = []\n    if (not isinstance(type_definitions, list)):\n        type_defs.append(type_definitions)\n    else:\n        type_defs = type_definitions\n    if (not imports):\n        imports = self._tpl_imports()\n    if imports:\n        custom_defs = toscaparser.imports.ImportsLoader(imports, self.path, type_defs, self.tpl).get_custom_defs()\n        if (not custom_defs):\n            return\n    for type_def in type_defs:\n        if (type_def != IMPORTS):\n            inner_custom_types = (self.tpl.get(type_def) or {\n                \n            })\n            if inner_custom_types:\n                custom_defs.update(inner_custom_types)\n    return custom_defs\n", "label": 1}
{"function": "\n\ndef _get_result_wrapper(self):\n    if self._tuples:\n        return self.database.get_result_wrapper(RESULTS_TUPLES)\n    elif self._dicts:\n        return self.database.get_result_wrapper(RESULTS_DICTS)\n    elif self._aggregate_rows:\n        return self.database.get_result_wrapper(RESULTS_AGGREGATE_MODELS)\n    has_joins = (self.lhs._joins or self.rhs._joins)\n    is_naive = (self.lhs._naive or self.rhs._naive or self._naive)\n    if (is_naive or (not has_joins) or self.verify_naive()):\n        return self.database.get_result_wrapper(RESULTS_NAIVE)\n    else:\n        return self.database.get_result_wrapper(RESULTS_MODELS)\n", "label": 1}
{"function": "\n\ndef resolve_credentials(user, token, file):\n    if (file is not None):\n        if os.path.isfile(file):\n            return read_credentials_file(file)\n    if ((user is None) or (len(user.strip()) == 0)):\n        git_user = input('Please enter your Github username: ').strip()\n    else:\n        git_user = user\n    if (git_user == ''):\n        show_error_and_exit('Empty username provided!')\n    if ((token is None) or (len(token.strip()) == 0)):\n        git_token = getpass(('Please enter your Github Personal access token with read:org ' + 'permissions: ')).strip()\n    else:\n        git_token = token\n    if (git_token == ''):\n        show_error_and_exit('Empty token provided!')\n    return (git_user, git_token)\n", "label": 1}
{"function": "\n\ndef get_font_sizes_dict(tree, styles_dict):\n    font_sizes_dict = defaultdict(int)\n    for p in tree.xpath('//w:p', namespaces=tree.nsmap):\n        if is_natural_header(p, styles_dict):\n            continue\n        if _is_li(p):\n            continue\n        font_size = get_font_size(p, styles_dict)\n        if (font_size is None):\n            continue\n        font_sizes_dict[font_size] += 1\n    most_used_font_size = (- 1)\n    highest_count = (- 1)\n    for (size, count) in font_sizes_dict.items():\n        if (count > highest_count):\n            highest_count = count\n            most_used_font_size = size\n    result = {\n        \n    }\n    for size in font_sizes_dict:\n        if (size is None):\n            continue\n        if (int(size) > int(most_used_font_size)):\n            result[size] = 'h2'\n        else:\n            result[size] = None\n    return result\n", "label": 1}
{"function": "\n\n@classmethod\ndef _session_flush(cls, session, flush_context, instances):\n    for obj in session.deleted:\n        class_ = obj.__class__\n        tracked_columns = cls.mapped_entities.get(class_, tuple())\n        for col in tracked_columns:\n            value = getattr(obj, col)\n            if (value is not None):\n                session._depot_old = getattr(session, '_depot_old', set())\n                session._depot_old.update(value.files)\n    for obj in session.new.union(session.dirty):\n        class_ = obj.__class__\n        tracked_columns = cls.mapped_entities.get(class_, tuple())\n        for col in tracked_columns:\n            history = get_history(obj, col)\n            added_files = itertools.chain(*(f.files for f in history.added if (f is not None)))\n            deleted_files = itertools.chain(*(f.files for f in history.deleted if (f is not None)))\n            session._depot_new = getattr(session, '_depot_new', set())\n            session._depot_new.update(added_files)\n            session._depot_old = getattr(session, '_depot_old', set())\n            session._depot_old.update(deleted_files)\n", "label": 1}
{"function": "\n\ndef areDisjointRanges(self, x1, x2, y1, y2):\n    result = False\n    if (((x1 < y1) and (x2 < y1) and (x1 < y2) and (x2 < y2)) or ((y1 < x1) and (y2 < x1) and (y1 < x2) and (y2 < x2))):\n        result = True\n    return result\n", "label": 1}
{"function": "\n\ndef _scan_from_end(self, grpos_range, frames=None, flag_mask=0):\n    if (not self._s.data):\n        return None\n    pos = (len(self._s.data) - 1)\n    if (grpos_range > 0):\n        grpos = self._s.data[pos][0]\n        target_grpos = (grpos - grpos_range)\n        while (pos > 0):\n            pos -= 1\n            f = self._s.data[pos]\n            if (f[0] < target_grpos):\n                pos += 1\n                break\n    elif (frames > 0):\n        pos = max(0, (len(self._s.data) - frames))\n    if (flag_mask < 0):\n        mask = (- flag_mask)\n        fpos = pos\n        while (fpos >= 0):\n            f = self._s.data[fpos]\n            if (f[1] & mask):\n                break\n            fpos -= 1\n        if (fpos >= 0):\n            pos = fpos\n    elif (flag_mask > 0):\n        mask = flag_mask\n        fpos = pos\n        end_pos = (len(self._s.data) - 1)\n        while (fpos <= end_pos):\n            f = self._s.data[fpos]\n            if (f[1] & mask):\n                break\n            fpos += 1\n        if (fpos <= end_pos):\n            pos = fpos\n    return (self._s.data_offset + pos)\n", "label": 1}
{"function": "\n\n@unittest.skip(\"Disabled: orphan subnets don't migrate because tenant is specified in filter.\")\ndef test_subnets_exist_on_dst(self):\n    \"Validate deleted tenant's subnets were migrated.\"\n    tenants_subnets = []\n    for (tenant_name, tenant) in self.deleted_tenants:\n        all_subnets = self.dst_cloud.neutronclient.list_subnets()\n        dst_admin_subnets = [subnet for subnet in all_subnets['subnets'] if (subnet['tenant_id'] == self.dst_tenants[self.dst_cloud.tenant])]\n        net_list = []\n        for network in tenant['networks']:\n            net_list.append(network['subnets'])\n        src_tenant_subnets_list = sum(net_list, [])\n        migrated_subnets = []\n        for src_subnet in src_tenant_subnets_list:\n            for dst_subnet in dst_admin_subnets:\n                if (src_subnet['name'] == dst_subnet['name']):\n                    migrated_subnets.append(src_subnet['name'])\n        src_tenant_net_names = [subnet['name'] for subnet in src_tenant_subnets_list]\n        non_migrated_subnets = (set(src_tenant_net_names) ^ set(migrated_subnets))\n        if non_migrated_subnets:\n            tenants_subnets.append({\n                tenant_name: non_migrated_subnets,\n            })\n    if tenants_subnets:\n        msg = \"Tenant's subnets do not exist on destination, but should be!\"\n        self.fail(msg.format(tenants_subnets))\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    if isinstance(key, slice):\n        (start, stop, step) = (key.start, key.stop, key.step)\n        if (step != None):\n            raise IndexError('SQL backend dos not support steps in slices')\n        if (key.start == None):\n            start = 0\n        if (key.stop == None):\n            stop = len(self)\n        if (start < 0):\n            start = (len(self) + start)\n        if (stop < 0):\n            stop = (len(self) + stop)\n        qs = copy.copy(self)\n        if start:\n            qs.offset(start)\n        qs.limit((stop - start))\n        qs.objects = None\n        qs.count = None\n        return qs\n    if (self.deserialized_objects is None):\n        self.get_deserialized_objects()\n    return self.deserialized_objects[key]\n", "label": 1}
{"function": "\n\ndef normalize(self):\n    text = self.text\n    if (text == '*'):\n        return Every(boost=self.boost)\n    if (('*' not in text) and ('?' not in text)):\n        return Term(self.fieldname, self.text, boost=self.boost)\n    elif (('?' not in text) and text.endswith('*') and (text.find('*') == (len(text) - 1)) and ((len(text) < 2) or (text[(- 2)] != '\\\\'))):\n        return Prefix(self.fieldname, self.text[:(- 1)], boost=self.boost)\n    else:\n        return self\n", "label": 1}
{"function": "\n\ndef raw_test():\n    all_hids = hid.find_all_hid_devices()\n    if all_hids:\n        while True:\n            print('Choose a device to monitor raw input reports:\\n')\n            print('0 => Exit')\n            for (index, device) in enumerate(all_hids):\n                device_name = unicode('{0.vendor_name} {0.product_name}(vID=0x{1:04x}, pID=0x{2:04x})'.format(device, device.vendor_id, device.product_id))\n                print('{0} => {1}'.format((index + 1), device_name))\n            print((\"\\n\\tDevice ('0' to '%d', '0' to exit?) [press enter after number]:\" % len(all_hids)))\n            index_option = raw_input()\n            if (index_option.isdigit() and (int(index_option) <= len(all_hids))):\n                break\n        int_option = int(index_option)\n        if int_option:\n            device = all_hids[(int_option - 1)]\n            try:\n                device.open()\n                device.set_raw_data_handler(sample_handler)\n                print('\\nWaiting for data...\\nPress any (system keyboard) key to stop...')\n                while ((not kbhit()) and device.is_plugged()):\n                    sleep(0.5)\n                return\n            finally:\n                device.close()\n    else:\n        print(\"There's not any non system HID class device available\")\n", "label": 1}
{"function": "\n\ndef product_mul(self, other, method=0):\n    'Helper function for Product simplification'\n    from sympy.concrete.products import Product\n    if (type(self) == type(other)):\n        if (method == 0):\n            if (self.limits == other.limits):\n                return Product((self.function * other.function), *self.limits)\n        elif (method == 1):\n            if (simplify((self.function - other.function)) == 0):\n                if (len(self.limits) == len(other.limits) == 1):\n                    i = self.limits[0][0]\n                    x1 = self.limits[0][1]\n                    y1 = self.limits[0][2]\n                    j = other.limits[0][0]\n                    x2 = other.limits[0][1]\n                    y2 = other.limits[0][2]\n                    if (i == j):\n                        if (x2 == (y1 + 1)):\n                            return Product(self.function, (i, x1, y2))\n                        elif (x1 == (y2 + 1)):\n                            return Product(self.function, (i, x2, y1))\n    return Mul(self, other)\n", "label": 1}
{"function": "\n\ndef items(self, srs=None, profile=None):\n    'supports dict-like items() access'\n    items = []\n    if ((not srs) and (not profile)):\n        for item in self.contents:\n            items.append((item, self.contents[item]))\n    elif (srs and profile):\n        for item in self.contents:\n            if ((self.contents[item].srs == srs) and (self.contents[item].profile == profile)):\n                items.append((item, self.contents[item]))\n    elif srs:\n        for item in self.contents:\n            if (self.contents[item].srs == srs):\n                items.append((item, self.contents[item]))\n    elif profile:\n        for item in self.contents:\n            if (self.contents[item].profile == profile):\n                items.append((item, self.contents[item]))\n    return items\n", "label": 1}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef _start(self):\n    'Start.\\n        '\n    if self.pending_socket_event:\n        return\n    if (not self.is_stopped()):\n        if (len(self.processes) < self.numprocesses):\n            self.reap_processes()\n            (yield self.spawn_processes())\n        return\n    found_wids = len(self._found_wids)\n    if ((not self._found_wids) and (not self.call_hook('before_start'))):\n        logger.debug('Aborting startup')\n        return\n    self._status = 'starting'\n    if (self.stdout_stream and hasattr(self.stdout_stream, 'open')):\n        self.stdout_stream.open()\n    if (self.stderr_stream and hasattr(self.stderr_stream, 'open')):\n        self.stderr_stream.open()\n    self._create_redirectors()\n    self.reap_processes()\n    (yield self.spawn_processes())\n    if ((not self.processes) or (not self.call_hook('after_start'))):\n        logger.debug('Aborting startup')\n        (yield self._stop(True))\n        return\n    self._status = 'active'\n    if found_wids:\n        logger.info(('%s already running' % self.name))\n    else:\n        logger.info(('%s started' % self.name))\n    self.notify_event('start', {\n        'time': time.time(),\n    })\n", "label": 1}
{"function": "\n\n@gen.coroutine\n@util.debuglog\ndef _reload(self, graceful=True, sequential=False):\n    ' reload\\n        '\n    if ((not graceful) and sequential):\n        logger.warn('with graceful=False, sequential=True is ignored')\n    if (self.prereload_fn is not None):\n        self.prereload_fn(self)\n    if (not graceful):\n        (yield self._restart())\n        return\n    if self.is_stopped():\n        (yield self._start())\n    elif self.send_hup:\n        for process in self.processes.values():\n            logger.info(('SENDING HUP to %s' % process.pid))\n            process.send_signal(signal.SIGHUP)\n    elif sequential:\n        active_processes = self.get_active_processes()\n        for process in active_processes:\n            (yield self.kill_process(process))\n            self.reap_process(process.pid)\n            self.spawn_process()\n            (yield tornado_sleep(self.warmup_delay))\n    else:\n        for i in range(self.numprocesses):\n            self.spawn_process()\n        (yield self.manage_processes())\n    self.notify_event('reload', {\n        'time': time.time(),\n    })\n    logger.info('%s reloaded', self.name)\n", "label": 1}
{"function": "\n\ndef url(self, name, force=False):\n    '\\n        Returns the real URL in DEBUG mode.\\n        '\n    if (settings.DEBUG and (not force)):\n        (hashed_name, fragment) = (name, '')\n    else:\n        (clean_name, fragment) = urldefrag(name)\n        cache_key = self.cache_key(name)\n        hashed_name = self.cache.get(cache_key)\n        if (hashed_name is None):\n            hashed_name = self.hashed_name(clean_name).replace('\\\\', '/')\n            self.cache.set(cache_key, hashed_name)\n    final_url = super(CachedFilesMixin, self).url(hashed_name)\n    query_fragment = ('?#' in name)\n    if (fragment or query_fragment):\n        urlparts = list(urlsplit(final_url))\n        if (fragment and (not urlparts[4])):\n            urlparts[4] = fragment\n        if (query_fragment and (not urlparts[3])):\n            urlparts[2] += '?'\n        final_url = urlunsplit(urlparts)\n    return unquote(final_url)\n", "label": 1}
{"function": "\n\ndef installed_packages(local=False):\n    installed = []\n    for dist in get_installed_distributions(local_only=local):\n        pym = PyModule(dist.project_name, dist.version, dist.location)\n        if dist.has_metadata('top_level.txt'):\n            pym.set_import_names(list(dist.get_metadata_lines('top_level.txt')))\n        pym.local = dist_is_local(dist)\n        pym.user = dist_in_usersite(dist)\n        pym._dependencies = [dep.project_name for dep in dist.requires()]\n        for filename in iter_dist_files(dist):\n            if (not filename.startswith(dist.location)):\n                if is_script(filename):\n                    pym.installed_scripts.append(filename)\n                else:\n                    pym.installed_files.append(filename)\n        if (pym.installed_scripts or (pym.name in ignore_packages)):\n            pym.hidden = True\n        installed.append(pym)\n    for pym in installed[:]:\n        for dep in pym._dependencies:\n            if (dep == 'argparse'):\n                continue\n            pymc = find_package(dep, installed, True)\n            if (not pymc):\n                pymc = PyModule(dep, 'MISSING', missing=True)\n                installed.append(pymc)\n            pymc.add_dependant(pym)\n            pym.add_dependency(pymc)\n    return installed\n", "label": 1}
{"function": "\n\ndef print_usage(actions):\n    'Print the usage information.  (Help screen)'\n    actions = actions.items()\n    actions.sort()\n    print(('usage: %s <action> [<options>]' % basename(sys.argv[0])))\n    print(('       %s --help' % basename(sys.argv[0])))\n    print()\n    print('actions:')\n    for (name, (func, doc, arguments)) in actions:\n        print(('  %s:' % name))\n        for line in doc.splitlines():\n            print(('    %s' % line))\n        if arguments:\n            print()\n        for (arg, shortcut, default, argtype) in arguments:\n            if isinstance(default, bool):\n                print(('    %s' % ((((shortcut and ('-%s, ' % shortcut)) or '') + '--') + arg)))\n            else:\n                print(('    %-30s%-10s%s' % (((((shortcut and ('-%s, ' % shortcut)) or '') + '--') + arg), argtype, default)))\n        print()\n", "label": 1}
{"function": "\n\ndef check_model_params_dict(params):\n    if (type(params) is not dict):\n        raise TypeError('params should be a dict')\n    keys = ['mu', 'kappa']\n    for key in keys:\n        if (key not in params):\n            raise KeyError(('missing key in params: %r' % (key,)))\n    for (key, value) in six.iteritems(params):\n        if (key not in keys):\n            raise KeyError(('invalid params key: %r' % (key,)))\n        if (not isinstance(value, (float, numpy.float64))):\n            raise TypeError(('%r should be float' % (key,)))\n        if (key == 'kappa'):\n            if (value <= 0.0):\n                raise ValueError('kappa should be greater than 0')\n        elif (key != 'mu'):\n            raise KeyError(('Invalid params key: %r' % (key,)))\n        elif ((value < 0.0) or (value > (2 * pi))):\n            raise ValueError('mu should be in [0,2*pi]')\n", "label": 1}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    'Defines how calling the function locally should be handled'\n    for requirement in self.requires:\n        lacks_requirement = self.check_requirements()\n        if lacks_requirement:\n            return (self.outputs(lacks_requirement) if self.outputs else lacks_requirement)\n    for (index, argument) in enumerate(args):\n        kwargs[self.parameters[index]] = argument\n    if (not getattr(self, 'skip_directives', False)):\n        for (parameter, directive) in self.directives.items():\n            if (parameter in kwargs):\n                continue\n            arguments = ((self.defaults[parameter],) if (parameter in self.defaults) else ())\n            kwargs[parameter] = directive(*arguments, api=self.api, api_version=self.version, interface=self)\n    if (not getattr(self, 'skip_validation', False)):\n        errors = self.validate(kwargs)\n        if errors:\n            errors = {\n                'errors': errors,\n            }\n            if getattr(self, 'on_invalid', False):\n                errors = self.on_invalid(errors)\n            outputs = getattr(self, 'invalid_outputs', self.outputs)\n            return (outputs(errors) if outputs else errors)\n    result = self.interface(**kwargs)\n    if self.transform:\n        result = self.transform(result)\n    return (self.outputs(result) if self.outputs else result)\n", "label": 1}
{"function": "\n\n@property\ndef status_human(self):\n    '\\n        Human readable status\\n\\n        :return:\\n\\n            * `DOWNLOADING`: the task is downloading files\\n            * `BEING TRANSFERRED`: the task is being transferred\\n            * `TRANSFERRED`: the task has been transferred to downloads                     directory\\n            * `SEARCHING RESOURCES`: the task is searching resources\\n            * `FAILED`: the task is failed\\n            * `DELETED`: the task is deleted\\n            * `UNKNOWN STATUS`\\n\\n        :rtype: str\\n\\n        '\n    res = None\n    if self._deleted:\n        return 'DELETED'\n    if (self.status == 1):\n        res = 'DOWNLOADING'\n    elif (self.status == 2):\n        if (self.move == 0):\n            res = 'BEING TRANSFERRED'\n        elif (self.move == 1):\n            res = 'TRANSFERRED'\n        elif (self.move == 2):\n            res = 'PARTIALLY TRANSFERRED'\n    elif (self.status == 4):\n        res = 'SEARCHING RESOURCES'\n    elif (self.status == (- 1)):\n        res = 'FAILED'\n    if (res is not None):\n        return res\n    return 'UNKNOWN STATUS'\n", "label": 1}
{"function": "\n\ndef _add_credentials_for_data_sources(ds_list, configs):\n    username = password = None\n    for src in ds_list:\n        if ((src.type == 'swift') and hasattr(src, 'credentials')):\n            if ('user' in src.credentials):\n                username = src.credentials['user']\n            if ('password' in src.credentials):\n                password = src.credentials['password']\n            break\n    if ((configs.get(sw.HADOOP_SWIFT_USERNAME, None) is None) and (username is not None)):\n        configs[sw.HADOOP_SWIFT_USERNAME] = username\n    if ((configs.get(sw.HADOOP_SWIFT_PASSWORD, None) is None) and (password is not None)):\n        configs[sw.HADOOP_SWIFT_PASSWORD] = password\n", "label": 1}
{"function": "\n\ndef gather_parameters(self, request, response, api_version=None, **input_parameters):\n    'Gathers and returns all parameters that will be used for this endpoint'\n    input_parameters.update(request.params)\n    if (self.parse_body and (request.content_length is not None)):\n        body = request.stream\n        (content_type, encoding) = separate_encoding(request.content_type)\n        body_formatter = (body and self.api.http.input_format(content_type))\n        if body_formatter:\n            body = (body_formatter(body, encoding) if (encoding is not None) else body_formatter(body))\n        if ('body' in self.parameters):\n            input_parameters['body'] = body\n        if isinstance(body, dict):\n            input_parameters.update(body)\n    elif ('body' in self.parameters):\n        input_parameters['body'] = None\n    if ('request' in self.parameters):\n        input_parameters['request'] = request\n    if ('response' in self.parameters):\n        input_parameters['response'] = response\n    if ('api_version' in self.parameters):\n        input_parameters['api_version'] = api_version\n    for (parameter, directive) in self.directives.items():\n        arguments = ((self.defaults[parameter],) if (parameter in self.defaults) else ())\n        input_parameters[parameter] = directive(*arguments, response=response, request=request, api=self.api, api_version=api_version, interface=self)\n    return input_parameters\n", "label": 1}
{"function": "\n\ndef prepare_predicate(next, token):\n    token = next()\n    if (token[0] == '@'):\n        token = next()\n        if token[0]:\n            raise SyntaxError('invalid attribute predicate')\n        key = token[1]\n        token = next()\n        if (token[0] == ']'):\n\n            def select(context, result):\n                for elem in result:\n                    if (elem.get(key) is not None):\n                        (yield elem)\n        elif (token[0] == '='):\n            value = next()[0]\n            if ((value[:1] == \"'\") or (value[:1] == '\"')):\n                value = value[1:(- 1)]\n            else:\n                raise SyntaxError('invalid comparision target')\n            token = next()\n\n            def select(context, result):\n                for elem in result:\n                    if (elem.get(key) == value):\n                        (yield elem)\n        if (token[0] != ']'):\n            raise SyntaxError('invalid attribute predicate')\n    elif (not token[0]):\n        tag = token[1]\n        token = next()\n        if (token[0] != ']'):\n            raise SyntaxError('invalid node predicate')\n\n        def select(context, result):\n            for elem in result:\n                if (elem.find(tag) is not None):\n                    (yield elem)\n    else:\n        raise SyntaxError('invalid predicate')\n    return select\n", "label": 1}
{"function": "\n\ndef _parse_headers(self, lines):\n    lastheader = ''\n    lastvalue = []\n    for (lineno, line) in enumerate(lines):\n        if (line[0] in ' \\t'):\n            if (not lastheader):\n                defect = errors.FirstHeaderLineIsContinuationDefect(line)\n                self._cur.defects.append(defect)\n                continue\n            lastvalue.append(line)\n            continue\n        if lastheader:\n            lhdr = EMPTYSTRING.join(lastvalue)[:(- 1)].rstrip('\\r\\n')\n            self._cur[lastheader] = lhdr\n            (lastheader, lastvalue) = ('', [])\n        if line.startswith('From '):\n            if (lineno == 0):\n                mo = NLCRE_eol.search(line)\n                if mo:\n                    line = line[:(- len(mo.group(0)))]\n                self._cur.set_unixfrom(line)\n                continue\n            elif (lineno == (len(lines) - 1)):\n                self._input.unreadline(line)\n                return\n            else:\n                defect = errors.MisplacedEnvelopeHeaderDefect(line)\n                self._cur.defects.append(defect)\n                continue\n        i = line.find(':')\n        if (i < 0):\n            defect = errors.MalformedHeaderDefect(line)\n            self._cur.defects.append(defect)\n            continue\n        lastheader = line[:i]\n        lastvalue = [line[(i + 1):].lstrip()]\n    if lastheader:\n        self._cur[lastheader] = EMPTYSTRING.join(lastvalue).rstrip('\\r\\n')\n", "label": 1}
{"function": "\n\ndef _find_parenthesis(self, position, forward=True):\n    \" If 'forward' is True (resp. False), proceed forwards\\n            (resp. backwards) through the line that contains 'position' until an\\n            unmatched closing (resp. opening) parenthesis is found. Returns a\\n            tuple containing the position of this parenthesis (or -1 if it is\\n            not found) and the number commas (at depth 0) found along the way.\\n        \"\n    commas = depth = 0\n    document = self._text_edit.document()\n    char = document.characterAt(position)\n    while ((category(char) != 'Cc') and (position > 0)):\n        if ((char == ',') and (depth == 0)):\n            commas += 1\n        elif (char == ')'):\n            if (forward and (depth == 0)):\n                break\n            depth += 1\n        elif (char == '('):\n            if ((not forward) and (depth == 0)):\n                break\n            depth -= 1\n        position += (1 if forward else (- 1))\n        char = document.characterAt(position)\n    else:\n        position = (- 1)\n    return (position, commas)\n", "label": 1}
{"function": "\n\ndef method_for(self):\n    for i in range(5):\n        for j in range(5):\n            if (j == 4):\n                break\n        else:\n            print('this should not show')\n    else:\n        print('ok')\n    for i in range(5):\n        if (i == 1):\n            break\n        for j in range(5):\n            pass\n        else:\n            print('ok')\n    else:\n        print('this should not show')\n", "label": 1}
{"function": "\n\ndef badge_badge(request, format, nick):\n    view = api.actor_get(request.user, nick)\n    presence = api.presence_get(request.user, view.nick)\n    if (not presence):\n        line = 'Offline'\n        light = 'gray'\n        location = ''\n    else:\n        line = presence.extra.get('status', 'Offline')\n        light = presence.extra.get('light', 'gray')\n        location = presence.extra.get('location', '')\n    if (format == 'image'):\n        return http.HttpResponseRedirect(('/images/badge_%s.gif' % light))\n    if (format == 'js-small'):\n        multiline = (len(line) > 17)\n        truncated_line = (((len(line) > 30) and ('%s...' % line[:27])) or line)\n        content_type = 'text/javascript'\n        template_path = 'js_small.js'\n    elif ((format == 'js-medium') or (format == 'js-large')):\n        truncated_line = (((len(line) > 40) and ('%s...' % line[:27])) or line)\n        content_type = 'text/javascript'\n        template_path = ('%s.js' % format.replace('-', '_'))\n    elif (format == 'json'):\n        content_type = 'text/javascript'\n        template_path = 'badge.json'\n    elif (format == 'xml'):\n        content_type = 'application/xml'\n        template_path = 'badge.xml'\n    c = template.RequestContext(request, locals())\n    t = loader.get_template(('badge/templates/%s' % template_path))\n    r = http.HttpResponse(t.render(c))\n    r['Content-type'] = content_type\n    return r\n", "label": 1}
{"function": "\n\ndef read(self, n=None):\n    'Read at most *n* characters from this stream.\\n\\n        If *n* is ``None``, return all available characters.\\n        '\n    response = ''\n    while ((n is None) or (n > 0)):\n        c = self.stream.read(1)\n        if (c == ''):\n            break\n        elif (c == '<'):\n            c += self.stream.read(1)\n            if (c == '<?'):\n                while True:\n                    q = self.stream.read(1)\n                    if (q == '>'):\n                        break\n            else:\n                response += c\n                if (n is not None):\n                    n -= len(c)\n        else:\n            response += c\n            if (n is not None):\n                n -= 1\n    return response\n", "label": 1}
{"function": "\n\ndef get(self, key, default=None, category=None, return_tagobj=False):\n    '\\n        Get the tag for the given key or list of tags.\\n\\n        Args:\\n            key (str or list): The tag or tags to retrieve.\\n            default (any, optional): The value to return in case of no match.\\n            category (str, optional): The Tag category to limit the\\n                request to. Note that `None` is the valid, default\\n                category.\\n            return_tagobj (bool, optional): Return the Tag object itself\\n                instead of a string representation of the Tag.\\n\\n        Returns:\\n            tags (str, TagObject or list): The matches, either string\\n                representations of the tags or the Tag objects themselves\\n                depending on `return_tagobj`.\\n\\n        '\n    if ((self._cache is None) or (not _TYPECLASS_AGGRESSIVE_CACHE)):\n        self._recache()\n    ret = []\n    category = (category.strip().lower() if (category is not None) else None)\n    searchkey = [(('%s-%s' % (key.strip().lower(), category)) if (key is not None) else None) for key in make_iter(key)]\n    ret = [val for val in (self._cache.get(keystr) for keystr in searchkey) if val]\n    ret = ([to_str(tag.db_data) for tag in ret] if return_tagobj else ret)\n    return (ret[0] if (len(ret) == 1) else (ret if ret else default))\n", "label": 1}
{"function": "\n\ndef get_result_wrapper(self, wrapper_type):\n    if (wrapper_type == RESULTS_NAIVE):\n        return (_ModelQueryResultWrapper if self.use_speedups else NaiveQueryResultWrapper)\n    elif (wrapper_type == RESULTS_MODELS):\n        return ModelQueryResultWrapper\n    elif (wrapper_type == RESULTS_TUPLES):\n        return (_TuplesQueryResultWrapper if self.use_speedups else TuplesQueryResultWrapper)\n    elif (wrapper_type == RESULTS_DICTS):\n        return (_DictQueryResultWrapper if self.use_speedups else DictQueryResultWrapper)\n    elif (wrapper_type == RESULTS_AGGREGATE_MODELS):\n        return AggregateQueryResultWrapper\n    else:\n        return (_ModelQueryResultWrapper if self.use_speedups else NaiveQueryResultWrapper)\n", "label": 1}
{"function": "\n\ndef getMatchedNodes(tr):\n    matchedLeafs = []\n    for (tree, outcome) in zip(tr.trees, tr.targets):\n        matchedLeaf = {\n            'text': [],\n            'tail': [],\n            'attrib': [],\n            'content': [],\n        }\n        for x in tree.iter():\n            if (x.text and (outcome in x.text)):\n                matchedLeaf['text'].append(x)\n            if (x.tail and (outcome in x.tail)):\n                matchedLeaf['tail'].append(x)\n            if (x.attrib and any([(outcome in y) for y in x.attrib.values()])):\n                matchedLeaf['attrib'].append(x)\n        matchedLeafs.append(matchedLeaf)\n    return matchedLeafs\n", "label": 1}
{"function": "\n\ndef filter(self, *args, **kwargs):\n    dq_node = Node()\n    if args:\n        dq_node &= reduce(operator.and_, [a.clone() for a in args])\n    if kwargs:\n        dq_node &= DQ(**kwargs)\n    q = deque([dq_node])\n    dq_joins = set()\n    while q:\n        curr = q.popleft()\n        if (not isinstance(curr, Expression)):\n            continue\n        for (side, piece) in (('lhs', curr.lhs), ('rhs', curr.rhs)):\n            if isinstance(piece, DQ):\n                (query, joins) = self.convert_dict_to_node(piece.query)\n                dq_joins.update(joins)\n                expression = reduce(operator.and_, query)\n                expression._negated = piece._negated\n                expression._alias = piece._alias\n                setattr(curr, side, expression)\n            else:\n                q.append(piece)\n    dq_node = dq_node.rhs\n    query = self.clone()\n    for field in dq_joins:\n        if isinstance(field, ForeignKeyField):\n            (lm, rm) = (field.model_class, field.rel_model)\n            field_obj = field\n        elif isinstance(field, ReverseRelationDescriptor):\n            (lm, rm) = (field.field.rel_model, field.rel_model)\n            field_obj = field.field\n        query = query.ensure_join(lm, rm, field_obj)\n    return query.where(dq_node)\n", "label": 1}
{"function": "\n\ndef toposort(data):\n    'Dependencies are expressed as a dictionary whose keys are items\\nand whose values are a set of dependent items. Output is a list of\\nsets in topological order. The first set consists of items with no\\ndependences, each subsequent set consists of items that depend upon\\nitems in the preceeding sets.\\n'\n    if (len(data) == 0):\n        return\n    data = data.copy()\n    for (k, v) in data.items():\n        v.discard(k)\n    extra_items_in_deps = (_reduce(set.union, data.values()) - set(data.keys()))\n    data.update({item: set() for item in extra_items_in_deps})\n    while True:\n        ordered = set((item for (item, dep) in data.items() if (len(dep) == 0)))\n        if (not ordered):\n            break\n        (yield ordered)\n        data = {item: (dep - ordered) for (item, dep) in data.items() if (item not in ordered)}\n    if (len(data) != 0):\n        raise ValueError('Cyclic dependencies exist among these items: {}'.format(', '.join((repr(x) for x in data.items()))))\n", "label": 1}
{"function": "\n\ndef _configure_merge(self):\n    '\\n        Helper function for configuring shapes depending on the merge concatenation type\\n        '\n    in_shapes = [l.out_shape for l in self.layers]\n    if (self.merge == 'recurrent'):\n        catdims = [xs[1] for xs in in_shapes]\n        self.out_shape = (in_shapes[0][0], sum(catdims))\n        stride_size = self.be.bsz\n    elif (self.merge == 'depth'):\n        catdims = [xs[0] for xs in in_shapes]\n        self.out_shape = ((sum(catdims),) + in_shapes[0][1:])\n        stride_size = np.prod(in_shapes[0][1:])\n    elif (self.merge == 'stack'):\n        catdims = [(xs if isinstance(xs, int) else np.prod(xs)) for xs in in_shapes]\n        self.out_shape = sum(catdims)\n        stride_size = 1\n    end_idx = [(idx * stride_size) for idx in np.cumsum(catdims)]\n    start_idx = ([0] + end_idx[:(- 1)])\n    self.slices = [slice(s, e) for (s, e) in zip(start_idx, end_idx)]\n", "label": 1}
{"function": "\n\ndef _scan_directory(directory, sentinel, depth=0):\n    'Basically os.listdir with some filtering.\\n    '\n    directory = os.path.abspath(directory)\n    real_directory = os.path.realpath(directory)\n    if ((depth < max_directory_depth) and (real_directory not in sentinel) and os.path.isdir(directory)):\n        sentinel.add(real_directory)\n        for item in os.listdir(directory):\n            if (item in ('.', '..')):\n                continue\n            p = os.path.abspath(os.path.join(directory, item))\n            if ((os.path.isdir(p) and _dir_ignore.search(p)) or (os.path.isfile(p) and _ext_ignore.search(p))):\n                continue\n            (yield p)\n", "label": 1}
{"function": "\n\ndef filter_agents(agents, stack_outputs, override=None):\n    deployed_agents = {\n        \n    }\n    for agent in agents.values():\n        stack_values = _get_stack_values(stack_outputs, agent['id'], ['ip'])\n        if override:\n            stack_values.update(override(agent))\n        if (not stack_values.get('ip')):\n            LOG.info('Ignore non-deployed agent: %s', agent)\n            continue\n        agent.update(stack_values)\n        if ((agent.get('mode') == 'slave') and (not agent.get('ip'))):\n            LOG.info('IP address is missing in agent: %s', agent)\n            continue\n        deployed_agents[agent['id']] = agent\n    result = {\n        \n    }\n    for agent in deployed_agents.values():\n        if ((agent.get('mode') == 'alone') or ((agent.get('mode') == 'master') and (agent.get('slave_id') in deployed_agents)) or ((agent.get('mode') == 'slave') and (agent.get('master_id') in deployed_agents))):\n            result[agent['id']] = agent\n    return result\n", "label": 1}
{"function": "\n\ndef save(self, force_insert=False, only=None):\n    field_dict = dict(self._data)\n    if (self._meta.primary_key is not False):\n        pk_field = self._meta.primary_key\n        pk_value = self._get_pk_value()\n    else:\n        pk_field = pk_value = None\n    if only:\n        field_dict = self._prune_fields(field_dict, only)\n    elif (self._meta.only_save_dirty and (not force_insert)):\n        field_dict = self._prune_fields(field_dict, self.dirty_fields)\n        if (not field_dict):\n            self._dirty.clear()\n            return False\n    self._populate_unsaved_relations(field_dict)\n    if ((pk_value is not None) and (not force_insert)):\n        if self._meta.composite_key:\n            for pk_part_name in pk_field.field_names:\n                field_dict.pop(pk_part_name, None)\n        else:\n            field_dict.pop(pk_field.name, None)\n        rows = self.update(**field_dict).where(self._pk_expr()).execute()\n    elif (pk_field is None):\n        self.insert(**field_dict).execute()\n        rows = 1\n    else:\n        pk_from_cursor = self.insert(**field_dict).execute()\n        if (pk_from_cursor is not None):\n            pk_value = pk_from_cursor\n        self._set_pk_value(pk_value)\n        rows = 1\n    self._dirty.clear()\n    return rows\n", "label": 1}
{"function": "\n\ndef db_type(self, field):\n    '\\n        Provides a choice to continue using db.Key just for primary key\\n        storage or to use it for all references (ForeignKeys and other\\n        relations).\\n\\n        We also force the \"string\" db_type (plain string storage) if a\\n        field is to be indexed, and the \"text\" db_type (db.Text) if\\n        it\\'s registered as unindexed.\\n        '\n    if self.connection.settings_dict.get('STORE_RELATIONS_AS_DB_KEYS'):\n        if (field.primary_key or (field.rel is not None)):\n            return 'key'\n    else:\n        if field.primary_key:\n            return 'key'\n        if (field.rel is not None):\n            related_field = field.rel.get_related_field()\n            if (related_field.get_internal_type() == 'AutoField'):\n                return 'integer'\n            else:\n                return related_field.db_type(connection=self.connection)\n    db_type = field.db_type(connection=self.connection)\n    if (db_type in ('string', 'text')):\n        indexes = get_model_indexes(field.model)\n        if (field.attname in indexes['indexed']):\n            return 'string'\n        elif (field.attname in indexes['unindexed']):\n            return 'text'\n    return db_type\n", "label": 1}
{"function": "\n\n@csrf_exempt_m\n@require_POST_m\ndef fps_ipn_handler(self, request):\n    uri = request.build_absolute_uri()\n    parsed_url = urlparse.urlparse(uri)\n    resp = self.fps_connection.verify_signature(UrlEndPoint=('%s://%s%s' % (parsed_url.scheme, parsed_url.netloc, parsed_url.path)), HttpParameters=request.body)\n    if (not (resp.VerifySignatureResult.VerificationStatus == 'Success')):\n        return HttpResponseForbidden()\n    data = dict(map((lambda x: x.split('=')), request.body.split('&')))\n    for (key, val) in data.items():\n        data[key] = urllib.unquote_plus(val)\n    if AmazonFPSResponse.objects.filter(transactionId=data['transactionId']).count():\n        resp = AmazonFPSResponse.objects.get(transactionId=data['transactionId'])\n    else:\n        resp = AmazonFPSResponse()\n    for (key, val) in data.items():\n        attr_exists = hasattr(resp, key)\n        if (attr_exists and (not callable(getattr(resp, key, None)))):\n            if (key == 'transactionDate'):\n                val = datetime.datetime(*time.localtime(float(val))[:6])\n            setattr(resp, key, val)\n    resp.save()\n    if (resp.statusCode == 'Success'):\n        transaction_was_successful.send(sender=self.__class__, type=data['operation'], response=resp)\n    elif (not ('Pending' in resp.statusCode)):\n        transaction_was_unsuccessful.send(sender=self.__class__, type=data['operation'], response=resp)\n    return HttpResponse(resp.statusCode)\n", "label": 1}
{"function": "\n\ndef get_activity_slice(self, start=None, stop=None, rehydrate=True):\n    '\\n        Retrieves a slice of aggregated activities and annotates them as read and/or seen.\\n        '\n    activities = super(BaseNotificationFeed, self).get_activity_slice(start, stop, rehydrate)\n    if (activities and (self.markers_storage_class is not None)):\n        if (self.track_unseen and self.track_unread):\n            (unseen_ids, unread_ids) = self.feed_markers.get('unseen', 'unread')\n        elif self.track_unseen:\n            unseen_ids = self.feed_markers.get('unseen')\n        elif self.track_unread:\n            unread_ids = self.feed_markers.get('unread')\n        for activity in activities:\n            if self.track_unseen:\n                activity.is_seen = (activity.serialization_id not in unseen_ids)\n            if self.track_unread:\n                activity.is_read = (activity.serialization_id not in unread_ids)\n    return activities\n", "label": 1}
{"function": "\n\ndef migrate_app(self, app_doc):\n    modules = [m for m in app_doc['modules'] if (m.get('module_type', '') == 'advanced')]\n    should_save = False\n    for module in modules:\n        forms = module['forms']\n        for form in forms:\n            load_actions = form.get('actions', {\n                \n            }).get('load_update_cases', [])\n            for action in load_actions:\n                preload = action['preload']\n                if (preload and preload.values()[0].startswith('/')):\n                    action['preload'] = {v: k for (k, v) in preload.items()}\n                    should_save = True\n    return (Application.wrap(app_doc) if should_save else None)\n", "label": 1}
{"function": "\n\n@spin_first\ndef abort(self, jobs=None, targets=None, block=None):\n    'Abort specific jobs from the execution queues of target(s).\\n        \\n        This is a mechanism to prevent jobs that have already been submitted\\n        from executing.\\n        \\n        Parameters\\n        ----------\\n        \\n        jobs : msg_id, list of msg_ids, or AsyncResult\\n            The jobs to be aborted\\n        \\n        \\n        '\n    block = (self.block if (block is None) else block)\n    targets = self._build_targets(targets)[0]\n    msg_ids = []\n    if isinstance(jobs, (str, AsyncResult)):\n        jobs = [jobs]\n    bad_ids = [obj for obj in jobs if (not isinstance(obj, (str, AsyncResult)))]\n    if bad_ids:\n        raise TypeError(('Invalid msg_id type %r, expected str or AsyncResult' % bad_ids[0]))\n    for j in jobs:\n        if isinstance(j, AsyncResult):\n            msg_ids.extend(j.msg_ids)\n        else:\n            msg_ids.append(j)\n    content = dict(msg_ids=msg_ids)\n    for t in targets:\n        self.session.send(self._control_socket, 'abort_request', content=content, ident=t)\n    error = False\n    if block:\n        self._flush_ignored_control()\n        for i in range(len(targets)):\n            (idents, msg) = self.session.recv(self._control_socket, 0)\n            if self.debug:\n                pprint(msg)\n            if (msg['content']['status'] != 'ok'):\n                error = self._unwrap_exception(msg['content'])\n    else:\n        self._ignored_control_replies += len(targets)\n    if error:\n        raise error\n", "label": 1}
{"function": "\n\n@classmethod\ndef text(cls, max_nb_chars=200):\n    \"\\n        Generate a text string.\\n        Depending on the $maxNbChars, returns a string made of words, sentences, or paragraphs.\\n        :example 'Sapiente sunt omnis. Ut pariatur ad autem ducimus et. Voluptas rem voluptas sint modi dolorem amet.'\\n        :param max_nb_chars Maximum number of characters the text should contain (minimum 5)\\n        :return string\\n        \"\n    text = []\n    if (max_nb_chars < 5):\n        raise ValueError('text() can only generate text of at least 5 characters')\n    if (max_nb_chars < 25):\n        while (not text):\n            size = 0\n            while (size < max_nb_chars):\n                word = ((' ' if size else '') + cls.word())\n                text.append(word)\n                size += len(word)\n            text.pop()\n        text[0] = (text[0][0].upper() + text[0][1:])\n        last_index = (len(text) - 1)\n        text[last_index] += '.'\n    elif (max_nb_chars < 100):\n        while (not text):\n            size = 0\n            while (size < max_nb_chars):\n                sentence = ((' ' if size else '') + cls.sentence())\n                text.append(sentence)\n                size += len(sentence)\n            text.pop()\n    else:\n        while (not text):\n            size = 0\n            while (size < max_nb_chars):\n                paragraph = (('\\n' if size else '') + cls.paragraph())\n                text.append(paragraph)\n                size += len(paragraph)\n            text.pop()\n    return ''.join(text)\n", "label": 1}
{"function": "\n\ndef render(pieces, style):\n    if pieces['error']:\n        return {\n            'version': 'unknown',\n            'full-revisionid': pieces.get('long'),\n            'dirty': None,\n            'error': pieces['error'],\n        }\n    if ((not style) or (style == 'default')):\n        style = 'pep440'\n    if (style == 'pep440'):\n        rendered = render_pep440(pieces)\n    elif (style == 'pep440-pre'):\n        rendered = render_pep440_pre(pieces)\n    elif (style == 'pep440-post'):\n        rendered = render_pep440_post(pieces)\n    elif (style == 'pep440-old'):\n        rendered = render_pep440_old(pieces)\n    elif (style == 'git-describe'):\n        rendered = render_git_describe(pieces)\n    elif (style == 'git-describe-long'):\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError((\"unknown style '%s'\" % style))\n    return {\n        'version': rendered,\n        'full-revisionid': pieces['long'],\n        'dirty': pieces['dirty'],\n        'error': None,\n    }\n", "label": 1}
{"function": "\n\ndef _file_gen(dname, fmatch=bool, dmatch=None):\n    'A generator returning files under the given directory, with optional\\n    file and directory filtering.\\n\\n    Args\\n    ----\\n    fmatch : predicate funct\\n        A predicate function that returns True on a match.\\n        This is used to match files only.\\n\\n    dmatch : predicate funct\\n        A predicate function that returns True on a match.\\n        This is used to match directories only.\\n    '\n    if ((dmatch is not None) and (not dmatch(dname))):\n        return\n    for (path, dirlist, filelist) in os.walk(dname):\n        if (dmatch is not None):\n            newdl = [d for d in dirlist if dmatch(d)]\n            if (len(newdl) != len(dirlist)):\n                dirlist[:] = newdl\n        for name in [f for f in filelist if fmatch(f)]:\n            (yield join(path, name))\n", "label": 1}
{"function": "\n\ndef import_ssl(keyring, certdata, cn=None, email=None):\n    '\\n    certdata should be pem-formated certificate data to be added to the\\n    keyring. The return value will be the sha1 fingerprint of the certificate\\n    added.\\n    '\n    (out, err, ret) = run_command(['openssl', 'x509', '-noout', '-inform', 'pem', '-sha1', '-fingerprint', '-subject'], input=certdata)\n    fingerprint = None\n    subject = None\n    for line in out.split('\\n'):\n        data = line.split('=', 1)\n        if (data[0] == 'SHA1 Fingerprint'):\n            fingerprint = data[1].replace(':', '')\n        if (data[0] == 'subject'):\n            subject = data[1].split('/')\n    if ((fingerprint is None) or (subject is None)):\n        raise ValueError('OpensSSL failed to parse ssl certificate.')\n    if ((cn and (not ('CN={cn}'.format(cn=cn) in subject))) or (email and (not ('emailAddress={email}'.format(email=email) in subject)))):\n        raise ValueError(('Incorrect subject of ssl certificate (cn=\"%s\", emailAddress=\"%s\", subject=\"%s\")' % (cn, email, subject)))\n    keyring = open(keyring, 'a')\n    fcntl.lockf(keyring, fcntl.LOCK_EX)\n    keyring.write(certdata)\n    keyring.close()\n    return fingerprint\n", "label": 1}
{"function": "\n\n@util.debuglog\n@gen.coroutine\ndef _stop(self, close_output_streams=False, for_shutdown=False):\n    if self.is_stopped():\n        return\n    self._status = 'stopping'\n    skip = (for_shutdown and self.use_papa)\n    if (not skip):\n        logger.debug(('stopping the %s watcher' % self.name))\n        logger.debug(('gracefully stopping processes [%s] for %ss' % (self.name, self.graceful_timeout)))\n        self.call_hook('before_stop')\n        (yield self.kill_processes())\n        self.reap_processes()\n    if self.stream_redirector:\n        self.stream_redirector.stop()\n        self.stream_redirector = None\n    if close_output_streams:\n        if (self.stdout_stream and hasattr(self.stdout_stream, 'close')):\n            self.stdout_stream.close()\n        if (self.stderr_stream and hasattr(self.stderr_stream, 'close')):\n            self.stderr_stream.close()\n    if skip:\n        logger.info('%s left running in papa', self.name)\n    else:\n        if (self.evpub_socket is not None):\n            self.notify_event('stop', {\n                'time': time.time(),\n            })\n        self._status = 'stopped'\n        self.call_hook('after_stop')\n        logger.info('%s stopped', self.name)\n", "label": 1}
{"function": "\n\ndef __new__(cls, sets, polar=False):\n    from sympy import sin, cos\n    (x, y, r, theta) = symbols('x, y, r, theta', cls=Dummy)\n    I = S.ImaginaryUnit\n    polar = sympify(polar)\n    if (polar == False):\n        if (all((_a.is_FiniteSet for _a in sets.args)) and (len(sets.args) == 2)):\n            complex_num = []\n            for x in sets.args[0]:\n                for y in sets.args[1]:\n                    complex_num.append((x + (I * y)))\n            obj = FiniteSet(*complex_num)\n        else:\n            obj = ImageSet.__new__(cls, Lambda((x, y), (x + (I * y))), sets)\n        obj._variables = (x, y)\n        obj._expr = (x + (I * y))\n    elif (polar == True):\n        new_sets = []\n        if (not sets.is_ProductSet):\n            for k in sets.args:\n                new_sets.append(k)\n        else:\n            new_sets.append(sets)\n        for (k, v) in enumerate(new_sets):\n            from sympy.sets import ProductSet\n            new_sets[k] = ProductSet(v.args[0], normalize_theta_set(v.args[1]))\n        sets = Union(*new_sets)\n        obj = ImageSet.__new__(cls, Lambda((r, theta), (r * (cos(theta) + (I * sin(theta))))), sets)\n        obj._variables = (r, theta)\n        obj._expr = (r * (cos(theta) + (I * sin(theta))))\n    else:\n        raise ValueError('polar should be either True or False')\n    obj._sets = sets\n    obj._polar = polar\n    return obj\n", "label": 1}
{"function": "\n\ndef run(self, edit, discard='item'):\n    repo = self.get_repo()\n    if (not repo):\n        return\n    goto = None\n    if (discard == 'section'):\n        points = self.get_all_points()\n        sections = set([self.section_at_point(p) for p in points])\n        all_files = self.get_all_files()\n        if (STASHES in sections):\n            self.discard_all_stashes(repo)\n        if (UNTRACKED_FILES in sections):\n            self.discard_all_untracked(repo)\n        files = [i for i in all_files if (i[0] in (UNSTAGED_CHANGES, STAGED_CHANGES))]\n        if files:\n            self.discard_files(repo, files)\n    elif (discard == 'item'):\n        files = self.get_selected_files()\n        stashes = self.get_selected_stashes()\n        if files:\n            self.discard_files(repo, files)\n        if stashes:\n            self.discard_stashes(repo, stashes)\n        goto = self.logical_goto_next_file()\n    elif (discard == 'all'):\n        self.discard_all(repo)\n    self.update_status(goto)\n", "label": 1}
{"function": "\n\n@require_instance_manager\ndef add(self, *tags):\n    str_tags = set([t for t in tags if (not isinstance(t, self.through.tag_model()))])\n    tag_objs = (set(tags) - str_tags)\n    if str_tags:\n        q = models.Q()\n        for str_tag in str_tags:\n            q |= models.Q(name__iexact=str_tag)\n        existing = self.through.tag_model().objects.filter(q)\n        tag_objs.update(existing)\n        existing_low = [t.name.lower() for t in existing]\n        new_tags = [t for t in str_tags if (t.lower() not in existing_low)]\n        for new_tag in new_tags:\n            tag_objs.add(self.through.tag_model().objects.create(name=new_tag))\n    for tag in tag_objs:\n        self.through.objects.get_or_create(tag=tag, **self._lookup_kwargs())\n", "label": 1}
{"function": "\n\ndef parse_range_header(value, make_inclusive=True):\n    'Parses a range header into a :class:`~werkzeug.datastructures.Range`\\n    object.  If the header is missing or malformed `None` is returned.\\n    `ranges` is a list of ``(start, stop)`` tuples where the ranges are\\n    non-inclusive.\\n\\n    .. versionadded:: 0.7\\n    '\n    if ((not value) or ('=' not in value)):\n        return None\n    ranges = []\n    last_end = 0\n    (units, rng) = value.split('=', 1)\n    units = units.strip().lower()\n    for item in rng.split(','):\n        item = item.strip()\n        if ('-' not in item):\n            return None\n        if item.startswith('-'):\n            if (last_end < 0):\n                return None\n            begin = int(item)\n            end = None\n            last_end = (- 1)\n        elif ('-' in item):\n            (begin, end) = item.split('-', 1)\n            begin = int(begin)\n            if ((begin < last_end) or (last_end < 0)):\n                return None\n            if end:\n                end = (int(end) + 1)\n                if (begin >= end):\n                    return None\n            else:\n                end = None\n            last_end = end\n        ranges.append((begin, end))\n    return Range(units, ranges)\n", "label": 1}
{"function": "\n\ndef log_actor_destroy(self, actor_id):\n    ' Trace actor destroy\\n        '\n    disconnected = []\n    for (user_id, logger) in self.loggers.iteritems():\n        if ((not logger.events) or (self.LOG_ACTOR_DESTROY in logger.events)):\n            if ((not logger.actors) or (actor_id in logger.actors)):\n                data = {\n                    \n                }\n                data['timestamp'] = time.time()\n                data['node_id'] = self.node.id\n                data['type'] = 'actor_destroy'\n                data['actor_id'] = actor_id\n                if (logger.connection is not None):\n                    if (not logger.connection.connection_lost):\n                        logger.connection.send(('data: %s\\n\\n' % json.dumps(data)))\n                    else:\n                        disconnected.append(user_id)\n                elif ((self.tunnel_client is not None) and (logger.handle is not None)):\n                    msg = {\n                        'cmd': 'logevent',\n                        'msgid': logger.handle,\n                        'header': None,\n                        'data': ('data: %s\\n\\n' % json.dumps(data)),\n                    }\n                    self.tunnel_client.send(msg)\n    for user_id in disconnected:\n        del self.loggers[user_id]\n", "label": 1}
{"function": "\n\ndef process(self, request):\n    '\\n        process determines if this item should visible, if its selected, etc...\\n        '\n    self.check(request)\n    if (not self.visible):\n        return\n    if callable(self.title):\n        self.title = self.title(request)\n    if (self.slug is None):\n        if (sys.version_info > (3, 0)):\n            self.slug = slugify(self.title)\n        else:\n            self.slug = slugify(unicode(self.title))\n    if callable(self.children):\n        children = list(self.children(request))\n    else:\n        children = list(self.children)\n    for child in children:\n        child.parent = self\n        child.process(request)\n    self.children = [child for child in children if child.visible]\n    self.children.sort(key=(lambda child: child.weight))\n    hide_empty = getattr(settings, 'MENU_HIDE_EMPTY', False)\n    if (hide_empty and (len(self.children) == 0)):\n        self.visible = False\n        return\n    curitem = None\n    for item in self.children:\n        item.selected = False\n        if item.match_url(request):\n            if ((curitem is None) or (len(curitem.url) < len(item.url))):\n                curitem = item\n    if (curitem is not None):\n        curitem.selected = True\n", "label": 1}
{"function": "\n\ndef perform(self, logfile=None):\n    'Fetch all of the added Request objects.\\n\\n        Return two lists. The first list is a list of responses that\\n        completed. The second is list of the requests that errored.\\n        '\n    m = pycurl.CurlMulti()\n    requests = []\n    num_q = num_urls = len(self._requests)\n    reqs = self._requests\n    self._requests = []\n    for req in reqs:\n        (c, resp) = req.get_requester()\n        c.resp = resp\n        m.add_handle(c)\n        requests.append(c)\n    del reqs\n    while 1:\n        (ret, num_handles) = m.perform()\n        if (ret != pycurl.E_CALL_MULTI_PERFORM):\n            break\n    num_handles = num_urls\n    while num_handles:\n        ret = m.select(5.0)\n        if (ret == (- 1)):\n            continue\n        while 1:\n            (ret, num_handles) = m.perform()\n            if (ret != pycurl.E_CALL_MULTI_PERFORM):\n                break\n    goodlist = []\n    errlist = []\n    while 1:\n        (num_q, ok_list, err_list) = m.info_read(num_urls)\n        for c in ok_list:\n            resp = c.resp\n            del c.resp\n            resp.error = None\n            m.remove_handle(c)\n            resp.finalize(c)\n            goodlist.append(resp)\n        for (c, errno, errmsg) in err_list:\n            resp = c.resp\n            del c.resp\n            resp.error = (errno, errmsg)\n            m.remove_handle(c)\n            errlist.append(resp)\n        if (num_q == 0):\n            break\n    m.close()\n    return (goodlist, errlist)\n", "label": 1}
{"function": "\n\ndef __init__(self, bucket=None, access_key=None, secret_key=None, headers=None, calling_format=None, cache=None, base_url=None):\n    if (bucket is None):\n        bucket = settings.AWS_STORAGE_BUCKET_NAME\n    if (calling_format is None):\n        calling_format = getattr(settings, 'AWS_CALLING_FORMAT', CallingFormat.SUBDOMAIN)\n    self.bucket = bucket\n    if ((not access_key) and (not secret_key)):\n        (access_key, secret_key) = self._get_access_keys()\n    self.connection = AWSAuthConnection(access_key, secret_key, calling_format=calling_format)\n    default_headers = getattr(settings, HEADERS, [])\n    if isinstance(default_headers, dict):\n        default_headers = [('.*', default_headers)]\n    if headers:\n        default_headers = (list(headers) + list(default_headers))\n    self.headers = []\n    for value in default_headers:\n        self.headers.append((re.compile(value[0]), value[1]))\n    if (cache is not None):\n        self.cache = cache\n    else:\n        cache = getattr(settings, 'CUDDLYBUDDLY_STORAGE_S3_CACHE', None)\n        if (cache is not None):\n            self.cache = self._get_cache_class(cache)()\n        else:\n            self.cache = None\n    if (base_url is None):\n        if (not self.static):\n            base_url = settings.MEDIA_URL\n        else:\n            base_url = settings.STATIC_URL\n    self.base_url = base_url\n", "label": 1}
{"function": "\n\ndef get_or_create(self, data):\n    descriptor = self.get_descriptor(data)\n    urls = self.get_urls(data)\n    read_response = self.requester.get(urls['read'])\n    if (read_response.status_code != 200):\n        message = 'Read error GET ({}): {}\\n{}'.format(read_response.status_code, urls['read'], read_response.text)\n        raise ValueError(message)\n    read_response_data = read_response.json()\n    pks = [item['id'] for item in read_response_data]\n    if (pks == []):\n        create_response = self.requester.post(urls['create'], data)\n        if (create_response.status_code != 201):\n            message = 'Create error POST ({}): {}\\n{}\\n{}'.format(create_response.status_code, urls['create'], data, create_response.text)\n            raise ValueError(message)\n        create_response_data = create_response.json()\n        if self.many:\n            pks = [response_data['id'] for response_data in create_response_data]\n        else:\n            pk = create_response_data['id']\n        if self.verbose:\n            if self.many:\n                print('Created {} ({}, pks={})'.format(self.item_name, descriptor, pks))\n            else:\n                print('Created {} ({}, pk={})'.format(self.item_name, descriptor, pk))\n        return (create_response_data, True)\n    else:\n        if ((len(pks) > 1) and (not self.many)):\n            message = 'Found multiple {} instances ({}) for {}'.format(self.item_name, pks, descriptor)\n            warnings.warn(message)\n        if self.verbose:\n            print('Existing {} ({}, pks={})'.format(self.item_name, descriptor, pks))\n        return (read_response_data, False)\n", "label": 1}
{"function": "\n\ndef checkReservation(self, resource, start_time, end_time):\n    self._checkArgs(resource, start_time, end_time)\n    if ((start_time is not None) and (end_time is not None) and (start_time > end_time)):\n        raise error.PayloadError('Invalid request: Reverse duration (end time before start time)')\n    if (start_time is not None):\n        now = datetime.datetime.utcnow()\n        if (start_time < now):\n            delta = (now - start_time)\n            stamp = str(start_time).rsplit('.')[0]\n            variables = [('startTime', start_time.isoformat())]\n            raise error.PayloadError(('Invalid request: Start time in the past (Startime: %s, Delta: %s)' % (stamp, str(delta))), variables=variables)\n        if (start_time > datetime.datetime(2025, 1, 1)):\n            raise error.PayloadError('Invalid request: Start time after year 2025')\n    for (c_resource, c_start_time, c_end_time) in self.reservations:\n        if (resource == c_resource):\n            if self._resourceOverlap(c_start_time, c_end_time, start_time, end_time):\n                raise error.STPUnavailableError(('Resource %s not available in specified time span' % resource))\n", "label": 1}
{"function": "\n\ndef isitemsimilar(v1, v2, verb=False, dtol=DTOL):\n    '\\n    Compare two values for differences\\n\\n    See :py:func:`isdicsimilar` for Parameters.\\n\\n    '\n    r = True\n    if (type(v1) != type(v2)):\n        r = False\n        if verb:\n            print('Item has different type', type(v1), type(v2))\n    elif isinstance(v1, dict):\n        r = (r and isdicsimilar(v1, v2, verb=verb, dtol=dtol))\n    elif isinstance(v1, list):\n        r = (r and islistsimilar(v1, v2, verb=verb, dtol=dtol))\n    elif isinstance(v1, (int, float)):\n        if (abs((v1 - v2)) > dtol):\n            r = False\n            if verb:\n                print('Key mismatch:', v1, v2)\n    elif (v1 != v2):\n        r = False\n        if verb:\n            print('Key mismatch:', v1, v2)\n    return r\n", "label": 1}
{"function": "\n\ndef datadiff(self, data1, data2, path=None, ignore_keys=[], compare_sorted=False):\n    if (path is None):\n        path = []\n\n    def fail(msg, failed_path):\n        self.fail('Path \"{0}\": {1}'.format('->'.join(failed_path), msg))\n    if ((not isinstance(data1, dict)) or (not isinstance(data2, dict))):\n        if isinstance(data1, (list, tuple)):\n            newpath = path[:]\n            if compare_sorted:\n                data1 = sorted(data1)\n                data2 = sorted(data2)\n            for (i, keys) in enumerate(izip(data1, data2)):\n                newpath.append(str(i))\n                self.datadiff(keys[0], keys[1], newpath, ignore_keys, compare_sorted)\n                newpath.pop()\n        elif (data1 != data2):\n            err = 'Values differ: {0} != {1}'.format(str(data1), str(data2))\n            fail(err, path)\n    else:\n        newpath = path[:]\n        if (len(data1) != len(data2)):\n            fail('Dicts have different keys number: {0} != {1}'.format(len(data1), len(data2)), path)\n        for (key1, key2) in zip(sorted(data1), sorted(data2)):\n            if (key1 != key2):\n                err = 'Keys differ: {0} != {1}'.format(str(key1), str(key2))\n                fail(err, path)\n            if (key1 in ignore_keys):\n                continue\n            newpath.append(key1)\n            self.datadiff(data1[key1], data2[key2], newpath, ignore_keys, compare_sorted)\n            newpath.pop()\n", "label": 1}
{"function": "\n\ndef start(self, current, selections):\n    current.progress.begin('Creating spike density estimation')\n    start = (float(self.start_time) * self.unit)\n    stop = None\n    if self.stop_enabled:\n        stop = (float(self.stop) * self.unit)\n    kernel_size = (float(self.kernel_size) * self.unit)\n    optimize_steps = 0\n    if self.optimize_enabled:\n        optimize_steps = self.optimize_steps\n    minimum_kernel = (self.minimum_kernel * self.unit)\n    maximum_kernel = (self.maximum_kernel * self.unit)\n    events = None\n    if (self.data_source == 0):\n        trains = current.spike_trains_by_unit()\n        if self.align_enabled:\n            events = current.labeled_events(self.align)\n    else:\n        trains = {\n            \n        }\n        if self.align_enabled:\n            events = {\n                \n            }\n        for s in selections:\n            trains[neo.Unit(s.name)] = s.spike_trains()\n            if self.align_enabled:\n                events.update(s.labeled_events(self.align))\n    if events:\n        for s in events:\n            events[s] = events[s][0]\n    plot.sde(trains, events, start, stop, kernel_size, optimize_steps, minimum_kernel, maximum_kernel, None, self.unit, current.progress)\n", "label": 1}
{"function": "\n\ndef on_key_press(self, event):\n    if (event.key == 'q'):\n        self.show(False, False)\n        self.app.quit()\n        return\n    elif ((event.key == 'p') or (event.key == ' ')):\n        self._paused = (not self._paused)\n        self.update_timer_state()\n    elif (event.key == 's'):\n        img = _screenshot()\n        self.write_img(img)\n    elif (event.key == 'a'):\n        print(('Size/pos args: --size %dx%d --pos %d,%d' % (self.physical_size[0], self.physical_size[1], self.position[0], self.position[1])))\n    elif (event.key == 'f'):\n        self._profile = (not self._profile)\n        if self._profile:\n\n            def print_profile(fps):\n                print(('%.2f ms/frame' % (1000.0 / float(fps))))\n                return False\n            self.measure_fps(1.0, print_profile)\n        else:\n            self.measure_fps(1.0, False)\n        self.update_timer_state()\n    elif ((event.key == keys.LEFT) or (event.key == keys.RIGHT)):\n        self._paused = True\n        self.update_timer_state()\n        step = (1.0 / 60.0)\n        if (keys.ALT in event.modifiers):\n            step *= 0.1\n            if (keys.SHIFT in event.modifiers):\n                step *= 0.1\n        else:\n            if (keys.SHIFT in event.modifiers):\n                step *= 10.0\n            if (keys.CONTROL in event.modifiers):\n                step *= 100.0\n        if (event.key == keys.LEFT):\n            step *= (- 1.0)\n        self.program['iGlobalTime'] += step\n        self.print_t()\n        self.update()\n", "label": 1}
{"function": "\n\ndef gettile(self, x, y, z, id=None, title=None, srs=None, mimetype=None, timeout=None):\n    if ((not id) and (not title) and (not srs)):\n        raise ValueError('either id or title and srs must be specified')\n    if id:\n        return self._gettilefromset(self.contents[id].tilemap.tilesets, x, y, z, self.contents[id].tilemap.extension, timeout=timeout)\n    elif (title and srs):\n        for tm in self.contents.values():\n            if ((tm.title == title) and (tm.srs == srs)):\n                if mimetype:\n                    if (tm.tilemap.mimetype == mimetype):\n                        return self._gettilefromset(tm.tilemap.tilesets, x, y, z, tm.tilemap.extension, timeout=timeout)\n                else:\n                    return self._gettilefromset(tm.tilemap.tilesets, x, y, z, tm.tilemap.extension, timeout=timeout)\n        else:\n            raise ValueError(('cannot find %s with projection %s for zoomlevel %i' % (title, srs, z)))\n    elif (title or srs):\n        ValueError('both title and srs must be specified')\n    raise ValueError(('Specified Tile with id %s, title %s\\n                projection %s format %s at zoomlevel %i cannot be found' % (id, title, srs, format, z)))\n", "label": 1}
{"function": "\n\n@classmethod\ndef size(cls, val, str_cache, object_cache, traits_cache):\n    if (val in object_cache):\n        index = object_cache.index(val)\n        return AMF3Integer.size((index << 1))\n    else:\n        object_cache.append(val)\n        size = 0\n        traits = type(val)\n        if (traits in traits_cache):\n            index = traits_cache.index(traits)\n            size += AMF3Integer.size(((index << 2) | 1))\n        else:\n            header = 3\n            if traits.__dynamic__:\n                header |= (2 << 2)\n            if traits.__externalizable__:\n                header |= (1 << 2)\n            header |= (len(traits.__members__) << 4)\n            size += AMF3Integer.size(header)\n            if isinstance(val, AMF3Object):\n                size += U8.size\n            else:\n                size += AMF3String.size(traits.__name__, cache=str_cache)\n                traits_cache.append(traits)\n            for member in traits.__members__:\n                size += AMF3String.size(member, cache=str_cache)\n        for member in traits.__members__:\n            value = getattr(val, member)\n            size += AMF3Value.size(value, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n        if traits.__dynamic__:\n            if isinstance(val, AMF3Object):\n                iterator = val.items()\n            else:\n                iterator = val.__dict__.items()\n            for (key, value) in iterator:\n                if (key in traits.__members__):\n                    continue\n                size += AMF3String.size(key, cache=str_cache)\n                size += AMF3Value.size(value, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n            size += U8.size\n        return size\n", "label": 1}
{"function": "\n\ndef build(self, skip_features=False, managed=False):\n    if hasattr(self.session._internal, 'namespace'):\n        namespace_instance = self.session._internal.namespace['instance']\n        if hasattr(namespace_instance, 'after_request'):\n            getattr(namespace_instance, 'after_request')(self, self.session)\n    if (self.namespace and (not skip_features)):\n        for feature in self.namespace['features']:\n            feature._handle_response(self)\n    if (not isinstance(self.result, UnformattedResponse)):\n        if self.function:\n            self.result = self.function['format'](self.result)\n        if isinstance(self.output_formatter, type):\n            self.output_formatter = self.output_formatter(sapi_request=self.sapi_request, callback=self.callback)\n        if isinstance(self.wrapper, type):\n            self.wrapper = self.wrapper(sapi_request=self.sapi_request)\n        wrapper_result = self.wrapper._build(errors=self.errors, result=self.result)\n        formatter_result = self.output_formatter.build(wrapper_result)\n    else:\n        self.mimetype = self.result.mimetype\n        formatter_result = self.result.content\n    result = {\n        'result': formatter_result,\n        'mimetype': self.mimetype,\n    }\n    if managed:\n        return result\n    else:\n        return self._build_response_obj(self.sapi_request, result)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.LIST):\n                self.column_names = []\n                (_etype10, _size7) = iprot.readListBegin()\n                for _i11 in xrange(_size7):\n                    _elem12 = iprot.readString()\n                    self.column_names.append(_elem12)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.slice_range = SliceRange()\n                self.slice_range.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    quiet = options.get('quiet', False)\n    codes = self.get_codes()\n    if (not quiet):\n        if codes:\n            self.stdout.write(('Will now delete codes: %s \\n' % codes))\n        else:\n            self.stdout.write('No Object codes to delete. \\n')\n    revisions = self.get_revisions()\n    if (not quiet):\n        if codes:\n            self.stdout.write((\"Will now delete additional doc's revisions in docs: %s \\n\" % [d[0] for d in revisions]))\n        else:\n            self.stdout.write('No additional revision files to delete. \\n')\n    if (codes or revisions):\n        processor = core.document_processor.DocumentProcessor()\n        user = User.objects.filter(is_superuser=True)[0]\n        for code in codes:\n            processor.delete(code, {\n                'user': user,\n            })\n            if (not processor.errors):\n                if (not quiet):\n                    self.stdout.write(('Permanently deleted object with code: %s' % code))\n            else:\n                if (not quiet):\n                    self.stdout.write(processor.errors)\n                raise (Exception, processor.errors)\n        for rev in revisions:\n            processor.delete(rev[0], {\n                'user': user,\n                'delete_revision': rev[1],\n            })\n", "label": 1}
{"function": "\n\ndef _reindent_stats(tokens):\n    \"Return list of (lineno, indentlevel) pairs.\\n\\n    One for each stmt and comment line. indentlevel is -1 for comment lines, as\\n    a signal that tokenize doesn't know what to do about them; indeed, they're\\n    our headache!\\n\\n    \"\n    find_stmt = 1\n    level = 0\n    stats = []\n    for t in tokens:\n        token_type = t[0]\n        sline = t[2][0]\n        line = t[4]\n        if (token_type == tokenize.NEWLINE):\n            find_stmt = 1\n        elif (token_type == tokenize.INDENT):\n            find_stmt = 1\n            level += 1\n        elif (token_type == tokenize.DEDENT):\n            find_stmt = 1\n            level -= 1\n        elif (token_type == tokenize.COMMENT):\n            if find_stmt:\n                stats.append((sline, (- 1)))\n        elif (token_type == tokenize.NL):\n            pass\n        elif find_stmt:\n            find_stmt = 0\n            if line:\n                stats.append((sline, level))\n    return stats\n", "label": 1}
{"function": "\n\ndef configure_uploads(app, upload_sets):\n    \"\\n    Call this after the app has been configured. It will go through all the\\n    upload sets, get their configuration, and store the configuration on the\\n    app. It will also register the uploads module if it hasn't been set. This\\n    can be called multiple times with different upload sets.\\n    \\n    .. versionchanged:: 0.1.3\\n       The uploads module/blueprint will only be registered if it is needed\\n       to serve the upload sets.\\n    \\n    :param app: The `~flask.Flask` instance to get the configuration from.\\n    :param upload_sets: The `UploadSet` instances to configure.\\n    \"\n    if isinstance(upload_sets, UploadSet):\n        upload_sets = (upload_sets,)\n    if (not hasattr(app, 'upload_set_config')):\n        app.upload_set_config = {\n            \n        }\n    set_config = app.upload_set_config\n    defaults = dict(dest=app.config.get('UPLOADS_DEFAULT_DEST'), url=app.config.get('UPLOADS_DEFAULT_URL'))\n    for uset in upload_sets:\n        config = config_for_set(uset, app, defaults)\n        set_config[uset.name] = config\n    should_serve = any(((s.base_url is None) for s in set_config.itervalues()))\n    if using_blueprints:\n        if (('_uploads' not in app.blueprints) and should_serve):\n            app.register_blueprint(uploads_mod)\n    elif (('_uploads' not in app.modules) and should_serve):\n        app.register_module(uploads_mod)\n", "label": 1}
{"function": "\n\ndef conceptsUsed(self):\n    conceptsUsed = set((f.qname for f in self.modelXbrl.factsInInstance))\n    for cntx in self.modelXbrl.contexts.values():\n        for dim in cntx.qnameDims.values():\n            conceptsUsed.add(dim.dimensionQname)\n            if dim.isExplicit:\n                conceptsUsed.add(dim.memberQname)\n            else:\n                conceptsUsed.add(dim.typedMember.qname)\n    for (defaultDim, defaultDimMember) in self.modelXbrl.qnameDimensionDefaults.items():\n        conceptsUsed.add(defaultDim)\n        conceptsUsed.add(defaultDimMember)\n    for roleTypes in (self.modelXbrl.roleTypes, self.modelXbrl.arcroleTypes):\n        for modelRoleTypes in roleTypes.values():\n            for modelRoleType in modelRoleTypes:\n                for qn in modelRoleType.usedOns:\n                    conceptsUsed.add(qn)\n    for relationshipSetKey in self.relationshipSets:\n        relationshipSet = self.modelXbrl.relationshipSet(*relationshipSetKey)\n        for rel in relationshipSet.modelRelationships:\n            if isinstance(rel.fromModelObject, ModelConcept):\n                conceptsUsed.add(rel.fromModelObject)\n            if isinstance(rel.toModelObject, ModelConcept):\n                conceptsUsed.add(rel.toModelObject)\n    for qn in (XbrlConst.qnXbrliIdentifier, XbrlConst.qnXbrliPeriod, XbrlConst.qnXbrliUnit):\n        conceptsUsed.add(qn)\n    conceptsUsed -= {None}\n    return conceptsUsed\n", "label": 1}
{"function": "\n\n@classmethod\ndef validate(cls, value):\n    if (not value):\n        return value\n    value = os.path.abspath(value)\n    dirname = os.path.dirname(value)\n    if os.path.isfile(value):\n        if (not os.access(value, os.W_OK)):\n            raise config_option.BadValue('You do not have write permissions')\n        if (not os.access(dirname, os.W_OK)):\n            raise config_option.BadValue(('You do not have write permissions for \"%s\"' % dirname))\n        return value\n    elif os.path.isdir(value):\n        raise config_option.BadValue(('\"%s\" is a directory' % value))\n    else:\n        if os.path.isdir(dirname):\n            if (not os.access(dirname, os.W_OK)):\n                raise config_option.BadValue(('You do not have write permissions for \"%s\"' % dirname))\n            return value\n        previous_dir = os.path.dirname(dirname)\n        if (not os.path.isdir(previous_dir)):\n            raise config_option.BadValue(('\"%s\" not found' % value))\n        if (not os.access(previous_dir, os.W_OK)):\n            raise config_option.BadValue(('You do not have write permissions for \"%s\"' % previous_dir))\n        return value\n", "label": 1}
{"function": "\n\ndef validate(self):\n    'Validate arguments, raises UrlArgsValidationError if something is wrong'\n    args = self.request.args\n    if (len(args) == 0):\n        raise UrlArgsValidationError('Mandatory arguments not found, please refer to the HTTPAPI specifications.')\n    for arg in args:\n        if (arg not in self.fields):\n            raise UrlArgsValidationError(('Argument [%s] is unknown.' % arg))\n        for field in self.fields:\n            fieldData = self.fields[field]\n            if (field in args):\n                if (isinstance(args[field][0], int) or isinstance(args[field][0], float)):\n                    value = str(args[field][0])\n                else:\n                    value = args[field][0]\n                if (('pattern' in self.fields[field]) and (self.fields[field]['pattern'].match(value) is None)):\n                    raise UrlArgsValidationError(('Argument [%s] has an invalid value: [%s].' % (field, value)))\n            elif (not fieldData['optional']):\n                raise UrlArgsValidationError(('Mandatory argument [%s] is not found.' % field))\n    return True\n", "label": 1}
{"function": "\n\ndef SetRepo(self, repo):\n    if (self.repo and (self.repo == self.mainRepo)):\n        self.mainRepo = self.repo\n        self.mainRepoSelection = [self.rows[row][0].commit.sha1 for row in self.selection]\n    repo_changed = (self.repo != repo)\n    if repo_changed:\n        self.selection = []\n        self.Scroll(0, 0)\n    if (not repo.parent):\n        self.mainRepo = repo\n    self.repo = repo\n    self.commits = self.repo.get_log(['--topo-order', '--all'])\n    self.CreateLogGraph()\n    if (repo_changed and (self.repo != self.mainRepo)):\n        for version in self.mainRepoSelection:\n            submodule_version = self.repo.parent.get_submodule_version(self.repo.name, version)\n            if submodule_version:\n                rows = [r for r in self.rows if (r[0].commit.sha1 == submodule_version)]\n                if rows:\n                    self.selection.append(self.rows.index(rows[0]))\n    self.SetVirtualSize(((- 1), ((len(self.rows) + 1) * LINH)))\n    self.SetScrollRate(LINH, LINH)\n    self.Refresh()\n", "label": 1}
{"function": "\n\ndef apply(self):\n    start_tags = []\n    for node in self.document.traverse(docutils.nodes.raw):\n        if (node['format'] != 'html'):\n            continue\n        start_match = self._start_re.match(node.astext())\n        if (not start_match):\n            continue\n        class_match = self._class_re.match(start_match.group(2))\n        if (not class_match):\n            continue\n        admonition_class = class_match.group(1)\n        if (admonition_class == 'info'):\n            admonition_class = 'note'\n        start_tags.append((node, admonition_class))\n    for (node, admonition_class) in reversed(start_tags):\n        content = []\n        for sibling in node.traverse(include_self=False, descend=False, siblings=True, ascend=False):\n            end_tag = (isinstance(sibling, docutils.nodes.raw) and (sibling['format'] == 'html') and self._end_re.match(sibling.astext()))\n            if end_tag:\n                admonition_node = AdmonitionNode(classes=['admonition', admonition_class])\n                admonition_node.extend(content)\n                parent = node.parent\n                parent.replace(node, admonition_node)\n                for n in content:\n                    parent.remove(n)\n                parent.remove(sibling)\n                break\n            else:\n                content.append(sibling)\n", "label": 1}
{"function": "\n\ndef __init__(self, channel, body_value, properties=None, auto_id=False, opinionated=False):\n    'Create a new instance of the Message object.'\n    super(Message, self).__init__(channel, 'Message')\n    self.properties = (properties or {\n        \n    })\n    if isinstance(body_value, memoryview):\n        self.body = bytes(body_value)\n    else:\n        self.body = self._auto_serialize(body_value)\n    if ((opinionated or auto_id) and ('message_id' not in self.properties)):\n        if auto_id:\n            raise DeprecationWarning('Use opinionated instead of auto_id')\n        self._add_auto_message_id()\n    if opinionated:\n        if ('timestamp' not in self.properties):\n            self._add_timestamp()\n    if ('timestamp' in self.properties):\n        self.properties['timestamp'] = self._as_datetime(self.properties['timestamp'])\n    if self._invalid_properties:\n        msg = ('Invalid property: %s' % self._invalid_properties[0])\n        raise KeyError(msg)\n", "label": 1}
{"function": "\n\ndef delete_color(self, color, palette_type, palette_name):\n    'Delete color.'\n    if (palette_type == '__special__'):\n        if (palette_name == 'Favorites'):\n            favs = util.get_favs()['colors']\n            if (color in favs):\n                favs.remove(color)\n                util.save_palettes(favs, favs=True)\n                self.show_colors(palette_type, palette_name, delete=True, update=False)\n    elif (palette_type in ('__global__', '__project__')):\n        if (palette_type == '__global__'):\n            color_palettes = util.get_palettes()\n        else:\n            color_palettes = util.get_project_palettes(self.view.window())\n        for palette in color_palettes:\n            if (palette_name == palette['name']):\n                if (color in palette['colors']):\n                    palette['colors'].remove(color)\n                    if (palette_type == '__global__'):\n                        util.save_palettes(color_palettes)\n                    else:\n                        util.save_project_palettes(self.view.window(), color_palettes)\n                    self.show_colors(palette_type, palette_name, delete=True, update=False)\n                    break\n", "label": 1}
{"function": "\n\ndef set_visible_views(self, indices, data, viewport):\n    view_opts = self.view_opts\n    (new, remaining, old) = self.recycleview.view_adapter.set_visible_views(indices, data, view_opts)\n    remove = self.remove_widget\n    view_indices = self.view_indices\n    for (_, widget) in old:\n        remove(widget)\n        del view_indices[widget]\n    refresh_view_layout = self.refresh_view_layout\n    for (index, widget) in new:\n        opt = view_opts[index]\n        refresh_view_layout(index, opt['pos'], opt['pos_hint'], opt['size'], opt['size_hint'], widget, viewport)\n    add = self.add_widget\n    for (index, widget) in new:\n        view_indices[widget] = index\n        if (widget.parent is None):\n            add(widget)\n    changed = False\n    for (index, widget) in new:\n        opt = view_opts[index]\n        if (changed or ((widget.size == opt['size']) and (widget.size_hint == opt['size_hint']) and (widget.pos_hint == opt['pos_hint']))):\n            continue\n        changed = True\n    if changed:\n        self._size_needs_update = True\n        self.recycleview.refresh_from_layout(view_size=True)\n", "label": 1}
{"function": "\n\ndef _get_parent(self, path, ref, parent_type):\n    ' Return the path of the parent of type \"parent_type\" for the object\\n        in \"path\" with id \"ref\". Returns an empty string if no parent extists.\\n        '\n    parts = path.split('/')\n    if ((parent_type.lower() == 'block') or (parts[(- 4)] == (parent_type.lower() + 's'))):\n        return '/'.join(parts[:(- 2)])\n    object_folder = parts[(- 2)]\n    parent_folder = parts[(- 4)]\n    if (parent_folder in ('recordingchannels', 'units')):\n        block_path = '/'.join(parts[:(- 6)])\n    else:\n        block_path = '/'.join(parts[:(- 4)])\n    if (parent_type.lower() in ('recordingchannel', 'unit')):\n        path = (block_path + '/recordingchannelgroups')\n        for n in self._data.iterNodes(path):\n            if (not ('_type' in n._v_attrs)):\n                continue\n            p = self._search_parent(('%s/%ss' % (n._v_pathname, parent_type.lower())), object_folder, ref)\n            if (p != ''):\n                return p\n        return ''\n    if (parent_type.lower() == 'segment'):\n        path = (block_path + '/segments')\n    elif (parent_type.lower() in ('recordingchannelgroup', 'recordingchannelgroups')):\n        path = (block_path + '/recordingchannelgroups')\n    else:\n        return ''\n    return self._search_parent(path, object_folder, ref)\n", "label": 1}
{"function": "\n\ndef _parse_request_line(self, line):\n    request_line = _read_request_line_dict(line)\n    if (not request_line):\n        return\n    uri = urlparse(request_line['uri'])\n    if uri.scheme:\n        self.request.protocol = uri.scheme\n    if uri.netloc:\n        if (':' in uri.netloc):\n            (self.request.host, self.request.port) = uri.netloc.split(':')\n        else:\n            self.request.host = uri.netloc\n    if uri.port:\n        self.request.port = uri.port\n    if uri.path:\n        self.request.path = uri.path\n    if uri.query:\n        query = parse_qs(uri.query)\n        for key in query:\n            self.request.query[key] = query[key]\n    if ('method' in request_line):\n        self.request.method = request_line['method']\n", "label": 1}
{"function": "\n\ndef AddLine(self, line):\n    'Adds a line of text to the block. Paragraph type is auto-determined.'\n    if self.paragraphs.IsType(paragraph.CodeBlock):\n        if line.startswith('<'):\n            self.paragraphs.Close()\n            line = line[1:].lstrip()\n            if line:\n                self.AddLine(line)\n            return\n        if (line[:1] not in ' \\t'):\n            self.paragraphs.Close()\n            self.AddLine(line)\n            return\n        self.paragraphs.AddLine(line)\n        return\n    self._ParseArgs(line)\n    if (not line.strip()):\n        self.paragraphs.SetType(paragraph.BlankLine)\n        return\n    match = regex.list_item.match((line or ''))\n    if match:\n        leader = match.group(1)\n        self.paragraphs.Close()\n        line = regex.list_item.sub('', line)\n        self.paragraphs.SetType(paragraph.ListItem, leader)\n        self.paragraphs.AddLine(line)\n        return\n    if (line and (line[:1] in ' \\t')):\n        if self.paragraphs.IsType(paragraph.ListItem):\n            self.paragraphs.AddLine(line.lstrip())\n            return\n    elif self.paragraphs.IsType(paragraph.ListItem):\n        self.paragraphs.Close()\n    self.paragraphs.SetType(paragraph.TextParagraph)\n    if ((line == '>') or line.endswith(' >')):\n        line = line[:(- 1)].rstrip()\n        if line:\n            self.paragraphs.AddLine(line)\n        self.paragraphs.SetType(paragraph.CodeBlock)\n        return\n    self.paragraphs.AddLine(line)\n", "label": 1}
{"function": "\n\ndef _parse_metadata(context, repos, record):\n    'parse metadata formats'\n    if isinstance(record, str):\n        exml = etree.fromstring(record, context.parser)\n    elif hasattr(record, 'getroot'):\n        exml = record.getroot()\n    else:\n        exml = record\n    root = exml.tag\n    LOGGER.debug('Serialized metadata, parsing content model')\n    if (root == ('{%s}MD_Metadata' % context.namespaces['gmd'])):\n        return [_parse_iso(context, repos, exml)]\n    elif (root == '{http://www.isotc211.org/2005/gmi}MI_Metadata'):\n        return [_parse_iso(context, repos, exml)]\n    elif (root == 'metadata'):\n        return [_parse_fgdc(context, repos, exml)]\n    elif (root == ('{%s}TRANSFER' % context.namespaces['gm03'])):\n        return [_parse_gm03(context, repos, exml)]\n    elif (root == ('{%s}Record' % context.namespaces['csw'])):\n        return [_parse_dc(context, repos, exml)]\n    elif (root == ('{%s}RDF' % context.namespaces['rdf'])):\n        return [_parse_dc(context, repos, exml)]\n    elif (root == ('{%s}DIF' % context.namespaces['dif'])):\n        pass\n    else:\n        raise RuntimeError('Unsupported metadata format')\n", "label": 1}
{"function": "\n\ndef __init__(self, identifier, buckets=None, **kwargs):\n    super(SongProxy, self).__init__()\n    buckets = (buckets or [])\n    self.id = identifier\n    self._object_type = 'song'\n    kwargs = dict(((str(k), v) for (k, v) in kwargs.iteritems()))\n    if kwargs.has_key('track_id'):\n        self.track_id = kwargs['track_id']\n    if kwargs.has_key('tag'):\n        self.tag = kwargs['tag']\n    if kwargs.has_key('score'):\n        self.score = kwargs['score']\n    if kwargs.has_key('audio'):\n        self.audio = kwargs['audio']\n    if kwargs.has_key('release_image'):\n        self.release_image = kwargs['release_image']\n    core_attrs = ['title', 'artist_name', 'artist_id']\n    if (not all(((ca in kwargs) for ca in core_attrs))):\n        profile = self.get_attribute('profile', **{\n            'id': self.id,\n            'bucket': buckets,\n        })\n        kwargs.update(profile.get('songs')[0])\n    [self.__dict__.update({\n        ca: kwargs.pop(ca),\n    }) for ca in core_attrs]\n    self.cache.update(kwargs)\n", "label": 1}
{"function": "\n\ndef init_app(self, app, dsn=None, logging=None, level=None, logging_exclusions=None, wrap_wsgi=None, register_signal=None):\n    if (dsn is not None):\n        self.dsn = dsn\n    if (level is not None):\n        self.level = level\n    if (wrap_wsgi is not None):\n        self.wrap_wsgi = wrap_wsgi\n    elif (self.wrap_wsgi is None):\n        if (app and app.debug):\n            self.wrap_wsgi = False\n        else:\n            self.wrap_wsgi = True\n    if (register_signal is not None):\n        self.register_signal = register_signal\n    if (logging is not None):\n        self.logging = logging\n    if (logging_exclusions is not None):\n        self.logging_exclusions = logging_exclusions\n    if (not self.client):\n        self.client = make_client(self.client_cls, app, self.dsn)\n    if self.logging:\n        kwargs = {\n            \n        }\n        if (self.logging_exclusions is not None):\n            kwargs['exclude'] = self.logging_exclusions\n        setup_logging(SentryHandler(self.client, level=self.level), **kwargs)\n    if self.wrap_wsgi:\n        app.wsgi_app = SentryMiddleware(app.wsgi_app, self.client)\n    app.before_request(self.before_request)\n    if self.register_signal:\n        got_request_exception.connect(self.handle_exception, sender=app)\n        request_finished.connect(self.after_request, sender=app)\n    if (not hasattr(app, 'extensions')):\n        app.extensions = {\n            \n        }\n    app.extensions['sentry'] = self\n", "label": 1}
{"function": "\n\ndef test_numpy_geometric(self):\n    geom = jit_unary('np.random.geometric')\n    self.assertRaises(ValueError, geom, (- 1.0))\n    self.assertRaises(ValueError, geom, 0.0)\n    self.assertRaises(ValueError, geom, 1.001)\n    N = 200\n    r = [geom(1.0) for i in range(N)]\n    self.assertPreciseEqual(r, ([1] * N))\n    r = [geom(0.9) for i in range(N)]\n    n = r.count(1)\n    self.assertGreaterEqual(n, (N // 2))\n    self.assertLess(n, N)\n    self.assertFalse([i for i in r if (i > 1000)])\n    r = [geom(0.4) for i in range(N)]\n    self.assertTrue([i for i in r if (i > 4)])\n    r = [geom(0.01) for i in range(N)]\n    self.assertTrue([i for i in r if (i > 50)])\n    r = [geom(1e-15) for i in range(N)]\n    self.assertTrue([i for i in r if (i > (2 ** 32))])\n", "label": 1}
{"function": "\n\n@synchronizedDeferred(busy)\n@deferredAsThread\ndef garbageCheck(self):\n    'Check for file patterns that are removeable'\n    watchDict = copy.deepcopy(self.watchDict)\n    for (directory, garbageList) in watchDict.iteritems():\n        if (not os.path.exists(directory)):\n            continue\n        for (pattern, limit) in garbageList:\n            self.cleanupLinks(directory)\n            files = [os.path.join(directory, f) for f in os.listdir(directory) if re.search(pattern, f)]\n            files = sorted(files)\n            if (len(files) > int(limit)):\n                log(('These files matched:\\n\\t%s' % '\\n\\t'.join(files)))\n            while (len(files) > int(limit)):\n                oldfile = files.pop(0)\n                log(('Deleting %s' % oldfile))\n                if os.path.islink(oldfile):\n                    continue\n                if os.path.isdir(oldfile):\n                    for (base, dirs, myfiles) in os.walk(oldfile, topdown=False):\n                        for name in myfiles:\n                            os.remove(os.path.join(base, name))\n                        for name in dirs:\n                            os.rmdir(os.path.join(base, name))\n                    os.rmdir(oldfile)\n                else:\n                    os.unlink(oldfile)\n        self.cleanupLinks(directory)\n", "label": 1}
{"function": "\n\ndef _insert(self, data):\n    if isinstance(data, list):\n        return [self._insert(item) for item in data]\n    if (not all((isinstance(k, string_types) for k in data))):\n        raise ValueError('Document keys must be strings')\n    if ('_id' not in data):\n        data['_id'] = ObjectId()\n    object_id = data['_id']\n    if isinstance(object_id, dict):\n        object_id = helpers.hashdict(object_id)\n    if (object_id in self._documents):\n        raise DuplicateKeyError('Duplicate Key Error', 11000)\n    for unique in self._uniques:\n        find_kwargs = {\n            \n        }\n        for (key, direction) in unique:\n            if (key in data):\n                find_kwargs[key] = data[key]\n        answer = self.find(find_kwargs)\n        if (answer.count() > 0):\n            raise DuplicateKeyError('Duplicate Key Error', 11000)\n    self._documents[object_id] = self._internalize_dict(data)\n    return data['_id']\n", "label": 1}
{"function": "\n\ndef fix_file(filename, options=None, output=None):\n    if (not options):\n        options = parse_args([filename])\n    original_source = readlines_from_file(filename)\n    fixed_source = original_source\n    if (options.in_place or output):\n        encoding = detect_encoding(filename)\n    if output:\n        output = codecs.getwriter(encoding)((output.buffer if hasattr(output, 'buffer') else output))\n        output = LineEndingWrapper(output)\n    fixed_source = fix_lines(fixed_source, options, filename=filename)\n    if options.diff:\n        new = io.StringIO(fixed_source)\n        new = new.readlines()\n        diff = get_diff_text(original_source, new, filename)\n        if output:\n            output.write(diff)\n            output.flush()\n        else:\n            return diff\n    elif options.in_place:\n        fp = open_with_encoding(filename, encoding=encoding, mode='w')\n        fp.write(fixed_source)\n        fp.close()\n    elif output:\n        output.write(fixed_source)\n        output.flush()\n    else:\n        return fixed_source\n", "label": 1}
{"function": "\n\ndef _check_var(self, doc):\n    '\\n        Run checks on the variable whose documentation is C{var} and\\n        whose name is C{name}.\\n        \\n        @param doc: The documentation for the variable to check.\\n        @type doc: L{APIDoc}\\n        @rtype: C{None}\\n        '\n    if (self._checks & DocChecker.VAR):\n        if ((self._checks & (DocChecker.DESCR | DocChecker.TYPE)) and (doc.descr in (None, UNKNOWN)) and (doc.type_descr in (None, UNKNOWN)) and (doc.docstring in (None, UNKNOWN))):\n            self.warning('Undocumented', doc)\n        else:\n            if ((self._checks & DocChecker.DESCR) and (doc.descr in (None, UNKNOWN))):\n                self.warning('No description', doc)\n            if ((self._checks & DocChecker.TYPE) and (doc.type_descr in (None, UNKNOWN))):\n                self.warning('No type information', doc)\n", "label": 1}
{"function": "\n\ndef suggest(self):\n    pattern = QtCore.QRegExp('\\\\w+$')\n    cursor = self._editor.textCursor()\n    block = cursor.block()\n    text = block.text()\n    if ((not self._room) or (not self._room.users) or (not text) or cursor.hasSelection()):\n        return False\n    blockText = QtCore.QString(text[:(cursor.position() - block.position())])\n    matchPosition = blockText.indexOf(pattern)\n    if (matchPosition < 0):\n        return False\n    word = blockText[matchPosition:]\n    if word.trimmed().isEmpty():\n        return False\n    matchingUserNames = []\n    for user in self._room.users:\n        if QtCore.QString(user['name']).startsWith(word, QtCore.Qt.CaseInsensitive):\n            matchingUserNames.append(user['name'])\n    if (len(matchingUserNames) == 1):\n        self._replace(cursor, word, (matchingUserNames[0] + (': ' if (matchPosition == 0) else ' ')))\n    else:\n        menu = QtGui.QMenu('Suggestions', self._editor)\n        for userName in matchingUserNames:\n            action = QtGui.QAction(userName, menu)\n            action.setData((cursor, word, userName, matchPosition))\n            self.connect(action, QtCore.SIGNAL('triggered()'), self._userSelected)\n            menu.addAction(action)\n        menu.popup(self._editor.mapToGlobal(self._editor.cursorRect().center()))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_line(line):\n    line = line.rstrip()\n    fields = line.split('\\t')\n    (contig, start, stop) = (fields[0], int(fields[1]), int(fields[2]))\n    n = len(fields)\n    name = ((fields[3] or None) if (n >= 4) else None)\n    score = ((fields[4] or None) if (n >= 5) else None)\n    strand = ((fields[5] or None) if (n >= 6) else None)\n    thick_start = ((fields[6] or None) if (n >= 7) else None)\n    thick_end = ((fields[7] or None) if (n >= 8) else None)\n    item_rgb = ((fields[8] or None) if (n >= 9) else None)\n    return BedRecord(contig, start, stop, name, score, strand, thick_start, thick_end, item_rgb)\n", "label": 1}
{"function": "\n\ndef _plot_timeseries(ax, ch_idx, tmin, tmax, vmin, vmax, ylim, data, color, times, vline=None, x_label=None, y_label=None, colorbar=False, hline=None):\n    'Aux function to show time series on topo split across multiple axes'\n    import matplotlib.pyplot as plt\n    picker_flag = False\n    for (data_, color_) in zip(data, color):\n        if (not picker_flag):\n            ax.plot(times, data_[ch_idx], color_, picker=1000000000.0)\n            picker_flag = True\n        else:\n            ax.plot(times, data_[ch_idx], color_)\n    if vline:\n        for x in vline:\n            plt.axvline(x, color='w', linewidth=0.5)\n    if hline:\n        for y in hline:\n            plt.axhline(y, color='w', linewidth=0.5)\n    if (x_label is not None):\n        plt.xlabel(x_label)\n    if (y_label is not None):\n        if isinstance(y_label, list):\n            plt.ylabel(y_label[ch_idx])\n        else:\n            plt.ylabel(y_label)\n    if colorbar:\n        plt.colorbar()\n", "label": 1}
{"function": "\n\ndef get_fields_to_translatable_models(model):\n    if (model in _F2TM_CACHE):\n        return _F2TM_CACHE[model]\n    results = []\n    if NEW_META_API:\n        for f in model._meta.get_fields():\n            if (f.is_relation and f.related_model):\n                if (get_translatable_fields_for_model(f.related_model) is not None):\n                    results.append((f.name, f.related_model))\n    else:\n        for field_name in model._meta.get_all_field_names():\n            (field_object, modelclass, direct, m2m) = model._meta.get_field_by_name(field_name)\n            if (direct and isinstance(field_object, RelatedField)):\n                if (get_translatable_fields_for_model(field_object.related.parent_model) is not None):\n                    results.append((field_name, field_object.related.parent_model))\n            if isinstance(field_object, RelatedObject):\n                if (get_translatable_fields_for_model(field_object.model) is not None):\n                    results.append((field_name, field_object.model))\n    _F2TM_CACHE[model] = dict(results)\n    return _F2TM_CACHE[model]\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = Archive()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = InvalidOperation()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef serialize(self, previous):\n    params = {\n        \n    }\n    unsaved_keys = (self._unsaved_values or set())\n    previous = (previous or self._previous or {\n        \n    })\n    for (k, v) in self.items():\n        if ((k == 'id') or (isinstance(k, str) and k.startswith('_'))):\n            continue\n        elif isinstance(v, APIResource):\n            continue\n        elif hasattr(v, 'serialize'):\n            params[k] = v.serialize(previous.get(k, None))\n        elif (k in unsaved_keys):\n            params[k] = _compute_diff(v, previous.get(k, None))\n        elif ((k == 'additional_owners') and (v is not None)):\n            params[k] = _serialize_list(v, previous.get(k, None))\n    return params\n", "label": 1}
{"function": "\n\ndef _extract_monitor_data(self, monitor):\n    if (self._monitor_type is not None):\n        raise TypeError('Your result `%s` already extracted data from a `%s` monitor. Please use a new empty result for a new monitor.')\n    self._monitor_type = monitor.__class__.__name__\n    if isinstance(monitor, SpikeCounter):\n        self._extract_spike_counter(monitor)\n    elif isinstance(monitor, VanRossumMetric):\n        self._extract_van_rossum_metric(monitor)\n    elif isinstance(monitor, PopulationSpikeCounter):\n        self._extract_population_spike_counter(monitor)\n    elif isinstance(monitor, StateSpikeMonitor):\n        self._extract_state_spike_monitor(monitor)\n    elif isinstance(monitor, PopulationRateMonitor):\n        self._extract_population_rate_monitor(monitor)\n    elif isinstance(monitor, ISIHistogramMonitor):\n        self._extract_isi_hist_monitor(monitor)\n    elif isinstance(monitor, SpikeMonitor):\n        self._extract_spike_monitor(monitor)\n    elif isinstance(monitor, MultiStateMonitor):\n        self._extract_multi_state_monitor(monitor)\n    elif isinstance(monitor, StateMonitor):\n        self._extract_state_monitor(monitor)\n    else:\n        raise ValueError(('Monitor Type %s is not supported (yet)' % str(type(monitor))))\n", "label": 1}
{"function": "\n\ndef iter_source_code(paths, config, skipped):\n    'Iterate over all Python source files defined in paths.'\n    for path in paths:\n        if os.path.isdir(path):\n            if should_skip(path, config, os.getcwd()):\n                skipped.append(path)\n                continue\n            for (dirpath, dirnames, filenames) in os.walk(path, topdown=True):\n                for dirname in list(dirnames):\n                    if should_skip(dirname, config, dirpath):\n                        skipped.append(dirname)\n                        dirnames.remove(dirname)\n                for filename in filenames:\n                    if filename.endswith('.py'):\n                        if should_skip(filename, config, dirpath):\n                            skipped.append(filename)\n                        else:\n                            (yield os.path.join(dirpath, filename))\n        else:\n            (yield path)\n", "label": 1}
{"function": "\n\ndef isnpint(ctx, x):\n    '\\n        Determine if *x* is a nonpositive integer.\\n        '\n    if (not x):\n        return True\n    if hasattr(x, '_mpf_'):\n        (sign, man, exp, bc) = x._mpf_\n        return (sign and (exp >= 0))\n    if hasattr(x, '_mpc_'):\n        return ((not x.imag) and ctx.isnpint(x.real))\n    if (type(x) in int_types):\n        return (x <= 0)\n    if isinstance(x, ctx.mpq):\n        (p, q) = x._mpq_\n        if (not p):\n            return True\n        return ((q == 1) and (p <= 0))\n    return ctx.isnpint(ctx.convert(x))\n", "label": 1}
{"function": "\n\ndef _children_updated(self, object, name, event):\n    ' Handles the children of a node being changed.\\n        '\n    name = name[:(- 6)]\n    self.log_change(self._get_undo_item, object, name, event)\n    start = event.index\n    n = len(event.added)\n    end = (start + len(event.removed))\n    tree = self._tree\n    for (expanded, node, nid) in self._object_info_for(object, name):\n        children = node.get_children(object)\n        if expanded:\n            for cnid in self._nodes_for(nid)[start:end]:\n                self._delete_node(cnid)\n            remaining = (len(children) - len(event.removed))\n            child_index = 0\n            for child in event.added:\n                (child, child_node) = self._node_for(child)\n                if (child_node is not None):\n                    insert_index = ((start + child_index) if (start <= remaining) else None)\n                    self._insert_node(nid, insert_index, child_node, child)\n                    child_index += 1\n        else:\n            dummy = getattr(nid, '_dummy', None)\n            if ((dummy is None) and (len(children) > 0)):\n                nid._dummy = QtGui.QTreeWidgetItem(nid)\n            elif ((dummy is not None) and (len(children) == 0)):\n                nid.removeChild(dummy)\n                del nid._dummy\n        if node.can_auto_open(object):\n            nid.setExpanded(True)\n", "label": 1}
{"function": "\n\ndef get_search_results(self, req, terms, filters):\n    if (not ('milestone' in filters)):\n        return\n    term_regexps = search_to_regexps(terms)\n    milestone_realm = Resource(self.realm)\n    for (name, due, completed, description) in MilestoneCache(self.env).milestones.itervalues():\n        if all(((r.search(description) or r.search(name)) for r in term_regexps)):\n            milestone = milestone_realm(id=name)\n            if ('MILESTONE_VIEW' in req.perm(milestone)):\n                dt = (completed if completed else (due if due else datetime_now(utc)))\n                (yield (get_resource_url(self.env, milestone, req.href), get_resource_name(self.env, milestone), dt, '', shorten_result(description, terms)))\n    for result in AttachmentModule(self.env).get_search_results(req, milestone_realm, terms):\n        (yield result)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.settings = BootstrapSettings()\n                self.settings.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef setup_app(app, config):\n    \"Configure flask app.\\n\\n    :param app: Flask app object\\n    :param config: api's config\\n    :type  config: dict\\n    \"\n    flask_config = (config.get('flask') or {\n        \n    })\n    for (key, value) in flask_config.items():\n        if (key == 'PERMANENT_SESSION_LIFETIME'):\n            app.config[key] = datetime.timedelta(days=int(value))\n        else:\n            app.config[key] = value\n    app_config = (config.get('app') or {\n        \n    })\n    for (key, value) in app_config.items():\n        app.config[key] = value\n    app.config['plugins'] = (app.config.get('plugins') or {\n        \n    })\n    for (name, config) in app.config['plugins'].items():\n        plugin_class = get_cls_with_path(config.pop('class'))\n        plugin = plugin_class(config)\n        app.config['plugins'][name] = plugin\n    default_class_sign = ''\n    app.config['responses'] = (app.config.get('responses') or {\n        \n    })\n    for (name, class_path) in app.config['responses'].items():\n        if (name == 'default'):\n            default_class_sign = class_path\n        else:\n            response_class = get_cls_with_path(class_path)\n            app.config['responses'][name] = response_class\n    app.config['responses']['default'] = app.config['responses'][default_class_sign]\n", "label": 1}
{"function": "\n\ndef __new__(cls, j, m):\n    j = sympify(j)\n    m = sympify(m)\n    if j.is_number:\n        if ((2 * j) != int((2 * j))):\n            raise ValueError(('j must be integer or half-integer, got: %s' % j))\n        if (j < 0):\n            raise ValueError(('j must be >= 0, got: %s' % j))\n    if m.is_number:\n        if ((2 * m) != int((2 * m))):\n            raise ValueError(('m must be integer or half-integer, got: %s' % m))\n    if (j.is_number and m.is_number):\n        if (abs(m) > j):\n            raise ValueError(('Allowed values for m are -j <= m <= j, got j, m: %s, %s' % (j, m)))\n        if (int((j - m)) != (j - m)):\n            raise ValueError(('Both j and m must be integer or half-integer, got j, m: %s, %s' % (j, m)))\n    return State.__new__(cls, j, m)\n", "label": 1}
{"function": "\n\ndef get_previous_link(self):\n    if (not self.has_previous):\n        return None\n    if (self.cursor and (not self.cursor.reverse) and (self.cursor.offset != 0)):\n        compare = self._get_position_from_instance(self.page[0], self.ordering)\n    else:\n        compare = self.previous_position\n    offset = 0\n    for item in self.page:\n        position = self._get_position_from_instance(item, self.ordering)\n        if (position != compare):\n            break\n        compare = position\n        offset += 1\n    else:\n        if (not self.has_next):\n            offset = self.page_size\n            position = None\n        elif self.cursor.reverse:\n            offset = (self.cursor.offset + self.page_size)\n            position = self.next_position\n        else:\n            offset = 0\n            position = self.next_position\n    cursor = Cursor(offset=offset, reverse=True, position=position)\n    return self.encode_cursor(cursor)\n", "label": 1}
{"function": "\n\ndef envs(ignore_cache=False):\n    '\\n    Return a list of refs that can be used as environments\\n    '\n    if (not ignore_cache):\n        env_cache = os.path.join(__opts__['cachedir'], 'hgfs/envs.p')\n        cache_match = salt.fileserver.check_env_cache(__opts__, env_cache)\n        if (cache_match is not None):\n            return cache_match\n    ret = set()\n    for repo in init():\n        repo['repo'].open()\n        if (repo['branch_method'] in ('branches', 'mixed')):\n            for branch in _all_branches(repo['repo']):\n                branch_name = branch[0]\n                if (branch_name == repo['base']):\n                    branch_name = 'base'\n                ret.add(branch_name)\n        if (repo['branch_method'] in ('bookmarks', 'mixed')):\n            for bookmark in _all_bookmarks(repo['repo']):\n                bookmark_name = bookmark[0]\n                if (bookmark_name == repo['base']):\n                    bookmark_name = 'base'\n                ret.add(bookmark_name)\n        ret.update([x[0] for x in _all_tags(repo['repo'])])\n        repo['repo'].close()\n    return [x for x in sorted(ret) if _env_is_exposed(x)]\n", "label": 1}
{"function": "\n\ndef __init__(self, typename=None, critical=None, value=None, subject=None, issuer=None, _ext=None):\n    if (_ext is not None):\n        ext = _ext\n    elif ((subject is None) and (issuer is None)):\n        ext = crypto.X509Extension(typename, critical, value)\n    elif ((subject is not None) and (issuer is None)):\n        subject = subject._cert\n        ext = crypto.X509Extension(typename, critical, value, subject=subject)\n    elif ((subject is None) and (issuer is not None)):\n        issuer = issuer._cert\n        ext = crypto.X509Extension(typename, critical, value, issuer=issuer)\n    elif ((subject is not None) and (issuer is not None)):\n        issuer = issuer._cert\n        ext = crypto.X509Extension(typename, critical, value, subject=subject, issuer=issuer)\n    self._ext = ext\n", "label": 1}
{"function": "\n\ndef _check_workers(self):\n    while (not self.stopped.is_set()):\n        for (worker, info) in self.worker_tracker.workers.iteritems():\n            if ((int(time.time()) - info.last_update) > HEARTBEAT_CHECK_INTERVAL):\n                info.continous_register = 0\n                if (info.status == RUNNING):\n                    info.status = HANGUP\n                elif (info.status == HANGUP):\n                    info.status = STOPPED\n                    self.black_list.append(worker)\n                    for job in self.job_tracker.running_jobs:\n                        self.job_tracker.remove_worker(job, worker)\n            elif (info.continous_register >= CONTINOUS_HEARTBEAT):\n                if (info.status != RUNNING):\n                    info.status = RUNNING\n                if (worker in self.black_list):\n                    self.black_list.remove(worker)\n                for job in self.job_tracker.running_jobs:\n                    if (not client_call(worker, 'has_job')):\n                        client_call(worker, 'prepare', job)\n                        client_call(worker, 'run_job', job)\n                    self.job_tracker.add_worker(job, worker)\n        self.stopped.wait(HEARTBEAT_CHECK_INTERVAL)\n", "label": 1}
{"function": "\n\ndef perform_search(self, dir, s=None, start=None, update_search_start=False):\n    self.cancel_highlight()\n    if (s is None):\n        s = self.last_search_string\n        if (s is None):\n            self.ui.message('No previous search term.')\n            return False\n    else:\n        self.last_search_string = s\n    if (start is None):\n        start = self.search_start\n    case_insensitive = (s.lower() == s)\n    if (start > len(self.ui.source)):\n        start = 0\n    i = ((start + dir) % len(self.ui.source))\n    if (i >= len(self.ui.source)):\n        i = 0\n    while (i != start):\n        sline = self.ui.source[i].text\n        if case_insensitive:\n            sline = sline.lower()\n        if (s in sline):\n            sl = self.ui.source[i]\n            sl.set_highlight(True)\n            self.highlight_line = sl\n            self.ui.source.set_focus(i)\n            if update_search_start:\n                self.search_start = i\n            return True\n        i = ((i + dir) % len(self.ui.source))\n    return False\n", "label": 1}
{"function": "\n\ndef maximalSquare(self, matrix):\n    if (not matrix):\n        return 0\n    (m, n) = (len(matrix), len(matrix[0]))\n    size = [[0 for j in xrange(n)] for i in xrange(2)]\n    max_size = 0\n    for j in xrange(n):\n        if (matrix[0][j] == '1'):\n            size[0][j] = 1\n        max_size = max(max_size, size[0][j])\n    for i in xrange(1, m):\n        if (matrix[i][0] == '1'):\n            size[(i % 2)][0] = 1\n        else:\n            size[(i % 2)][0] = 0\n        for j in xrange(1, n):\n            if (matrix[i][j] == '1'):\n                size[(i % 2)][j] = (min(size[(i % 2)][(j - 1)], size[((i - 1) % 2)][j], size[((i - 1) % 2)][(j - 1)]) + 1)\n                max_size = max(max_size, size[(i % 2)][j])\n            else:\n                size[(i % 2)][j] = 0\n    return (max_size * max_size)\n", "label": 1}
{"function": "\n\ndef resolve_columns(self, row, fields=()):\n    '\\n        This routine is necessary so that distances and geometries returned\\n        from extra selection SQL get resolved appropriately into Python\\n        objects.\\n        '\n    values = []\n    aliases = self.extra_select.keys()\n    if self.aggregates:\n        aliases.extend([None for i in xrange(len(self.aggregates))])\n    rn_offset = 0\n    if SpatialBackend.oracle:\n        if ((self.high_mark is not None) or self.low_mark):\n            rn_offset = 1\n    index_start = (rn_offset + len(aliases))\n    values = [self.convert_values(v, self.extra_select_fields.get(a, None)) for (v, a) in izip(row[rn_offset:index_start], aliases)]\n    if (SpatialBackend.oracle or getattr(self, 'geo_values', False)):\n        for (value, field) in izip(row[index_start:], fields):\n            values.append(self.convert_values(value, field))\n    else:\n        values.extend(row[index_start:])\n    return tuple(values)\n", "label": 1}
{"function": "\n\ndef addAndFixActions(startDict, actions):\n    curDict = copy.copy(startDict)\n    for action in actions:\n        new_ops = []\n        for op in action.db_operations:\n            if (op.vtType == 'add'):\n                if ((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId))):\n                    curDict[(op.db_what, op.db_objectId)] = op\n                    new_ops.append(op)\n            elif (op.vtType == 'change'):\n                if (curDict.has_key((op.db_what, op.db_oldObjId)) and ((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId)))):\n                    del curDict[(op.db_what, op.db_oldObjId)]\n                    curDict[(op.db_what, op.db_newObjId)] = op\n                    new_ops.append(op)\n            elif (op.vtType == 'delete'):\n                if (((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId))) and curDict.has_key((op.db_what, op.db_objectId))):\n                    del curDict[(op.db_what, op.db_objectId)]\n                    new_ops.append(op)\n        action.db_operations = new_ops\n    return curDict\n", "label": 1}
{"function": "\n\ndef GetHandlerType(self):\n    'Get handler type of mapping.\\n\\n    Returns:\\n      Handler type determined by which handler id attribute is set.\\n\\n    Raises:\\n      UnknownHandlerType: when none of the no handler id attributes are set.\\n\\n      UnexpectedHandlerAttribute: when an unexpected attribute is set for the\\n        discovered handler type.\\n\\n      HandlerTypeMissingAttribute: when the handler is missing a\\n        required attribute for its handler type.\\n\\n      MissingHandlerAttribute: when a URL handler is missing an attribute\\n    '\n    if (getattr(self, HANDLER_API_ENDPOINT) is not None):\n        mapping_type = HANDLER_API_ENDPOINT\n    else:\n        for id_field in URLMap.ALLOWED_FIELDS.iterkeys():\n            if (getattr(self, id_field) is not None):\n                mapping_type = id_field\n                break\n        else:\n            raise appinfo_errors.UnknownHandlerType(('Unknown url handler type.\\n%s' % str(self)))\n    allowed_fields = URLMap.ALLOWED_FIELDS[mapping_type]\n    for attribute in self.ATTRIBUTES.iterkeys():\n        if ((getattr(self, attribute) is not None) and (not ((attribute in allowed_fields) or (attribute in URLMap.COMMON_FIELDS) or (attribute == mapping_type)))):\n            raise appinfo_errors.UnexpectedHandlerAttribute(('Unexpected attribute \"%s\" for mapping type %s.' % (attribute, mapping_type)))\n    if ((mapping_type == HANDLER_STATIC_FILES) and (not self.upload)):\n        raise appinfo_errors.MissingHandlerAttribute(('Missing \"%s\" attribute for URL \"%s\".' % (UPLOAD, self.url)))\n    return mapping_type\n", "label": 1}
{"function": "\n\ndef parse_long(tokens, options):\n    (raw, eq, value) = tokens.move().partition('=')\n    value = (None if (eq == value == '') else value)\n    opt = [o for o in options if (o.long and o.long.startswith(raw))]\n    if (len(opt) < 1):\n        if (tokens.error is DocoptExit):\n            raise tokens.error(('%s is not recognized' % raw))\n        else:\n            o = Option(None, raw, (1 if (eq == '=') else 0))\n            options.append(o)\n            return [o]\n    if (len(opt) > 1):\n        raise tokens.error(('%s is not a unique prefix: %s?' % (raw, ', '.join((('%s' % o.long) for o in opt)))))\n    opt = copy(opt[0])\n    if (opt.argcount == 1):\n        if (value is None):\n            if (tokens.current() is None):\n                raise tokens.error(('%s requires argument' % opt.name))\n            value = tokens.move()\n    elif (value is not None):\n        raise tokens.error(('%s must not have an argument' % opt.name))\n    opt.value = (value or True)\n    return [opt]\n", "label": 1}
{"function": "\n\ndef handle_msg(self, proto, msg):\n    if (msg[0:3] not in ['GET', 'SET', 'ADD', 'RES', 'UPD']):\n        return\n    if (msg[0:3] in ['GET']):\n        (_, key) = msg.split(' ', 1)\n        key = json.loads(key)\n        if (key in self.dict):\n            item = self.dict[key]\n            proto.send(('RES ' + json.dumps([key, item])))\n        else:\n            proto.send(('RES ' + json.dumps([key, None])))\n    if (msg[0:3] in ['SET']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        self.dict[key] = value\n    if (msg[0:3] in ['ADD']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if (key not in self.dict):\n            self.dict[key] = []\n        self.dict[key].append(value)\n    if (msg[0:3] in ['RES']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if (key in self.queue):\n            self.queue[key].put(value)\n    if (msg[0:3] in ['UPD']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if ((key not in self.dict) or (self.dict[key] is not value)):\n            self.dict[key] = value\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _from_db_object(context, fixedip, db_fixedip, expected_attrs=None):\n    if (expected_attrs is None):\n        expected_attrs = []\n    for field in fixedip.fields:\n        if (field == 'default_route'):\n            continue\n        if (field not in FIXED_IP_OPTIONAL_ATTRS):\n            fixedip[field] = db_fixedip[field]\n    if ('instance' in expected_attrs):\n        fixedip.instance = (objects.Instance._from_db_object(context, objects.Instance(context), db_fixedip['instance']) if db_fixedip['instance'] else None)\n    if ('network' in expected_attrs):\n        fixedip.network = (objects.Network._from_db_object(context, objects.Network(context), db_fixedip['network']) if db_fixedip['network'] else None)\n    if ('virtual_interface' in expected_attrs):\n        db_vif = db_fixedip['virtual_interface']\n        vif = (objects.VirtualInterface._from_db_object(context, objects.VirtualInterface(context), db_fixedip['virtual_interface']) if db_vif else None)\n        fixedip.virtual_interface = vif\n    if ('floating_ips' in expected_attrs):\n        fixedip.floating_ips = obj_base.obj_make_list(context, objects.FloatingIPList(context), objects.FloatingIP, db_fixedip['floating_ips'])\n    fixedip._context = context\n    fixedip.obj_reset_changes()\n    return fixedip\n", "label": 1}
{"function": "\n\ndef clean(self):\n    cleaned = super(AuthorizeRequestTokenForm, self).clean()\n    t = Token.objects.get(id=cleaned.get('obj_id'))\n    default_scopes = t.scope.split(' ')\n    scopes = cleaned.get('scopes')\n    if (not scopes):\n        raise forms.ValidationError('You need to select permissions for the client')\n    if (('statements/read/mine' in scopes) and ('statements/read' in scopes)):\n        raise forms.ValidationError(\"'statements/read/mine' and 'statements/read' are conflicting scope values. choose one.\")\n    if ('all' in default_scopes):\n        return cleaned\n    elif ('all' in scopes):\n        raise forms.ValidationError(\"Can't raise permissions beyond what the consumer registered.\")\n    if (set(scopes) != set(default_scopes)):\n        nomatch = [k for k in scopes if (k not in default_scopes)]\n        if (not (('all/read' in nomatch) or (('statements/read' in nomatch) and ('all/read' in default_scopes)) or (('statements/read/mine' in nomatch) and (('all/read' in default_scopes) or ('statements/read' in default_scopes))))):\n            raise forms.ValidationError(\"Can't raise permissions beyond what the consumer registered.\")\n    return cleaned\n", "label": 1}
{"function": "\n\ndef elemwise(op, *args, **kwargs):\n    ' Elementwise operation for dask.Dataframes '\n    columns = kwargs.pop('columns', no_default)\n    _name = ('elemwise-' + tokenize(op, kwargs, *args))\n    args = _maybe_from_pandas(args)\n    from .multi import _maybe_align_partitions\n    args = _maybe_align_partitions(args)\n    dasks = [arg for arg in args if isinstance(arg, (_Frame, Scalar))]\n    dfs = [df for df in dasks if isinstance(df, _Frame)]\n    divisions = dfs[0].divisions\n    n = (len(divisions) - 1)\n    other = [(i, arg) for (i, arg) in enumerate(args) if (not isinstance(arg, (_Frame, Scalar)))]\n    if other:\n        op2 = partial_by_order(op, other)\n    else:\n        op2 = op\n    keys = [((d._keys() * n) if isinstance(d, Scalar) else d._keys()) for d in dasks]\n    dsk = dict((((_name, i), ((op2,) + frs)) for (i, frs) in enumerate(zip(*keys))))\n    dsk = merge(dsk, *[d.dask for d in dasks])\n    if (columns is no_default):\n        if ((len(dfs) >= 2) and (len(dasks) != len(dfs))):\n            msg = 'elemwise with 2 or more DataFrames and Scalar is not supported'\n            raise NotImplementedError(msg)\n        columns = _emulate(op, *args, **kwargs)\n    return _Frame(dsk, _name, columns, divisions)\n", "label": 1}
{"function": "\n\ndef _swap_generator(self, coro):\n    'Internal use only.\\n        '\n    self._lock.acquire()\n    cid = coro._id\n    coro = self._coros.get(cid, None)\n    if (coro is None):\n        logger.warning('invalid coroutine %s to swap', cid)\n        self._lock.release()\n        return (- 1)\n    if (coro._callers or (not coro._hot_swappable)):\n        logger.debug('postponing hot swapping of %s', str(coro))\n        self._lock.release()\n        return 0\n    else:\n        coro._timeout = None\n        if (coro._state is None):\n            coro._generator = coro._swap_generator\n            coro._value = None\n            if (coro._complete == 0):\n                coro._complete = None\n            elif isinstance(coro._complete, Event):\n                coro._complete.clear()\n            self._scheduled.add(cid)\n            coro._state = AsynCoro._Scheduled\n            coro._hot_swappable = False\n        else:\n            coro._exceptions.append((HotSwapException, HotSwapException(coro._swap_generator)))\n            if (coro._state in (AsynCoro._Suspended, AsynCoro._AwaitMsg_)):\n                self._suspended.discard(cid)\n                self._scheduled.add(cid)\n                coro._state = AsynCoro._Scheduled\n        coro._swap_generator = None\n        if (self._polling and (len(self._scheduled) == 1)):\n            self._notifier.interrupt()\n    self._lock.release()\n    return 0\n", "label": 1}
{"function": "\n\ndef data(self, index, role=QtCore.Qt.DisplayRole):\n    if (not index.isValid()):\n        return QtCore.QVariant()\n    obj = index.internalPointer()\n    obj_type = obj.get_type()\n    if (role == QtCore.Qt.DisplayRole):\n        if (obj.get_type() in (Object.GROUP, Object.USER)):\n            return QtCore.QVariant(obj.get_display_name())\n    elif (role == QtCore.Qt.UserRole):\n        if (obj_type == Object.GROUP):\n            return QtCore.QVariant(obj.gid)\n        elif (obj_type == Object.USER):\n            return QtCore.QVariant(obj.uri)\n    elif (role == QtCore.Qt.ToolTipRole):\n        if (obj_type == Object.USER):\n            tool_tip = ('%s (URI: %s)' % (obj.get_display_name(), obj.uri))\n            return QtCore.QVariant(tool_tip)\n    elif (role == QtCore.Qt.DecorationRole):\n        if (obj_type == Object.USER):\n            return QtGui.QPixmap('../icons/exit.png')\n    return QtCore.QVariant()\n", "label": 1}
{"function": "\n\ndef url_for_static(filename=None, **kwargs):\n    from uliweb import settings, application\n    from uliweb.core.SimpleFrame import get_url_adapter\n    from urlparse import urlparse, urlunparse, urljoin\n    import urllib\n    domain = application.domains.get('static', {\n        \n    })\n    ver = settings.GLOBAL.STATIC_VER\n    if ver:\n        kwargs['ver'] = ver\n    external = kwargs.pop('_external', False)\n    if (not external):\n        external = domain.get('display', False)\n    if filename.startswith('/'):\n        if filename.endswith('/'):\n            filename = filename[:(- 1)]\n        if kwargs:\n            filename += ('?' + urllib.urlencode(kwargs))\n        if external:\n            return urljoin(domain.get('domain', ''), filename)\n        return filename\n    r = urlparse(filename)\n    if (r.scheme or r.netloc):\n        x = list(r)\n        if kwargs:\n            x[4] = urllib.urlencode(kwargs)\n            return urlunparse(x)\n        else:\n            return filename\n    kwargs['filename'] = filename\n    url_adapter = get_url_adapter('static')\n    return url_adapter.build('uliweb.contrib.staticfiles.static', kwargs, force_external=external)\n", "label": 1}
{"function": "\n\ndef _ch_neighbor_connectivity(ch_names, neighbors):\n    'Compute sensor connectivity matrix\\n\\n    Parameters\\n    ----------\\n    ch_names : list of str\\n        The channel names.\\n    neighbors : list of list\\n        A list of list of channel names. The neighbors to\\n        which the channels in ch_names are connected with.\\n        Must be of the same length as ch_names.\\n\\n    Returns\\n    -------\\n    ch_connectivity : scipy.sparse matrix\\n        The connectivity matrix.\\n    '\n    if (len(ch_names) != len(neighbors)):\n        raise ValueError('`ch_names` and `neighbors` must have the same length')\n    set_neighbors = set([c for d in neighbors for c in d])\n    rest = (set(ch_names) - set_neighbors)\n    if (len(rest) > 0):\n        raise ValueError('Some of your neighbors are not present in the list of channel names')\n    for neigh in neighbors:\n        if ((not isinstance(neigh, list)) and (not all((isinstance(c, string_types) for c in neigh)))):\n            raise ValueError('`neighbors` must be a list of lists of str')\n    ch_connectivity = np.eye(len(ch_names), dtype=bool)\n    for (ii, neigbs) in enumerate(neighbors):\n        ch_connectivity[(ii, [ch_names.index(i) for i in neigbs])] = True\n    ch_connectivity = sparse.csr_matrix(ch_connectivity)\n    return ch_connectivity\n", "label": 1}
{"function": "\n\n@command\ndef module(self, input='', search_type='prefix', project=None, file=None, module=None, deps=None, sandbox=None, cabal=False, db=None, package=None, source=False, standalone=False):\n    q = {\n        'input': input,\n        'type': search_type,\n    }\n    fs = []\n    if project:\n        fs.append({\n            'project': project,\n        })\n    if file:\n        fs.append({\n            'file': file,\n        })\n    if module:\n        fs.append({\n            'module': module,\n        })\n    if deps:\n        fs.append({\n            'deps': deps,\n        })\n    if sandbox:\n        fs.append({\n            'cabal': {\n                'sandbox': sandbox,\n            },\n        })\n    if cabal:\n        fs.append({\n            'cabal': 'cabal',\n        })\n    if db:\n        fs.append({\n            'db': encode_package_db(db),\n        })\n    if package:\n        fs.append({\n            'package': package,\n        })\n    if source:\n        fs.append('sourced')\n    if standalone:\n        fs.append('standalone')\n    return cmd('module', {\n        'query': q,\n        'filters': fs,\n    }, parse_modules)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if (not isinstance(other, Vulnerability)):\n        raise TypeError((\"Expected Vulnerability, got '%s' instead\" % type(other)))\n    if (other.cves != self.cves):\n        return False\n    if (other.threat != self.threat):\n        return False\n    if (other.name != self.name):\n        return False\n    if (other.cvss != self.cvss):\n        return False\n    if (other.description != self.description):\n        return False\n    if (other.id != self.id):\n        return False\n    if (other.level != self.level):\n        return False\n    if (other.references != self.references):\n        return False\n    for (host, port) in self.hosts:\n        for (o_host, o_port) in other.hosts:\n            if ((o_host == host) and (o_port == port)):\n                break\n        else:\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef receive(self, category=None, timeout=None, alarm_value=None):\n    \"Similar to 'receive' of Coro, except it retrieves (waiting,\\n        if necessary) messages in given 'category'.\\n        \"\n    c = self._categories.get(category, None)\n    if c:\n        msg = c.popleft()\n        raise StopIteration(msg)\n    if timeout:\n        start = _time()\n    while 1:\n        msg = (yield self._coro.receive(timeout=timeout, alarm_value=alarm_value))\n        if (msg == alarm_value):\n            raise StopIteration(msg)\n        for categorize in reversed(self._categorize):\n            c = categorize(msg)\n            if (c == category):\n                raise StopIteration(msg)\n            if (c is not None):\n                bucket = self._categories.get(c, None)\n                if (not bucket):\n                    bucket = self._categories[c] = collections.deque()\n                bucket.append(msg)\n                break\n        else:\n            self._categories[None].append(msg)\n        if timeout:\n            now = _time()\n            timeout -= (now - start)\n            start = now\n", "label": 1}
{"function": "\n\n@classmethod\ndef email_url_config(cls, url, backend=None):\n    'Parses an email URL.'\n    config = {\n        \n    }\n    url = (urlparse.urlparse(url) if (not isinstance(url, cls.URL_CLASS)) else url)\n    path = url.path[1:]\n    path = path.split('?', 2)[0]\n    config.update({\n        'EMAIL_FILE_PATH': path,\n        'EMAIL_HOST_USER': url.username,\n        'EMAIL_HOST_PASSWORD': url.password,\n        'EMAIL_HOST': url.hostname,\n        'EMAIL_PORT': _cast_int(url.port),\n    })\n    if backend:\n        config['EMAIL_BACKEND'] = backend\n    elif (url.scheme not in cls.EMAIL_SCHEMES):\n        raise ImproperlyConfigured(('Invalid email schema %s' % url.scheme))\n    elif (url.scheme in cls.EMAIL_SCHEMES):\n        config['EMAIL_BACKEND'] = cls.EMAIL_SCHEMES[url.scheme]\n    if (url.scheme in ('smtps', 'smtp+tls')):\n        config['EMAIL_USE_TLS'] = True\n    elif (url.scheme == 'smtp+ssl'):\n        config['EMAIL_USE_SSL'] = True\n    if url.query:\n        config_options = {\n            \n        }\n        for (k, v) in urlparse.parse_qs(url.query).items():\n            opt = {\n                k.upper(): _cast_int(v[0]),\n            }\n            if (k.upper() in cls._EMAIL_BASE_OPTIONS):\n                config.update(opt)\n            else:\n                config_options.update(opt)\n        config['OPTIONS'] = config_options\n    return config\n", "label": 1}
{"function": "\n\ndef _check_failure_put_connections(self, conns, req, nodes, min_conns):\n    '\\n        Identify any failed connections and check minimum connection count.\\n        '\n    if ((req.if_none_match is not None) and ('*' in req.if_none_match)):\n        statuses = [conn.resp.status for conn in conns if conn.resp]\n        if (HTTP_PRECONDITION_FAILED in statuses):\n            self.app.logger.debug(_('Object PUT returning 412, %(statuses)r'), {\n                'statuses': statuses,\n            })\n            raise HTTPPreconditionFailed(request=req)\n    if any((conn for conn in conns if (conn.resp and (conn.resp.status == HTTP_CONFLICT)))):\n        status_times = [('%(status)s (%(timestamp)s)' % {\n            'status': conn.resp.status,\n            'timestamp': HeaderKeyDict(conn.resp.getheaders()).get('X-Backend-Timestamp', 'unknown'),\n        }) for conn in conns if conn.resp]\n        self.app.logger.debug(_('Object PUT returning 202 for 409: %(req_timestamp)s <= %(timestamps)r'), {\n            'req_timestamp': req.timestamp.internal,\n            'timestamps': ', '.join(status_times),\n        })\n        raise HTTPAccepted(request=req)\n    self._check_min_conn(req, conns, min_conns)\n", "label": 1}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.set_package(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            self.set_capability(d.getPrefixedString())\n            continue\n        if (tt == 24):\n            self.set_status(d.getVarInt32())\n            continue\n        if (tt == 34):\n            self.set_internal_message(d.getPrefixedString())\n            continue\n        if (tt == 42):\n            self.set_admin_message(d.getPrefixedString())\n            continue\n        if (tt == 50):\n            self.set_error_message(d.getPrefixedString())\n            continue\n        if (tt == 58):\n            self.set_scheduled_time(d.getPrefixedString())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef _cleanse_tags(self, message):\n    'Using BeautifulSoup, remove or modify improper tags & attributes'\n    soup = BeautifulSoup(message, 'lxml')\n    for tag in soup.findAll():\n        if (tag.name not in self.VALID_TAGS):\n            tag.hidden = True\n        for (attr, value) in dict(tag.attrs).iteritems():\n            if (attr not in self.VALID_ATTRS):\n                del tag.attrs[attr]\n            if (attr == 'src'):\n                parsed_src = urlparse(value)\n                valid_netlocs = settings.ALLOWED_HOSTS\n                valid_netlocs.append('')\n                if (parsed_src.netloc not in valid_netlocs):\n                    tag.hidden = True\n            if (attr == 'href'):\n                parsed_src = urlparse(value)\n                if (parsed_src.scheme not in self.VALID_SCHEMES):\n                    tag.hidden = True\n        if (self.ADD_MAX_WIDTH and (tag.name == 'img')):\n            tag.attrs['style'] = 'max-width: 100%;'\n    return unicode(soup).encode('utf-8', errors='ignore')\n", "label": 1}
{"function": "\n\ndef __rebuildAsAssignment(node, firstVarStatement):\n    'Rebuilds the items of a var statement into a assignment list and moves declarations to the given var statement'\n    assignment = Node.Node(node.tokenizer, 'semicolon')\n    assignmentList = Node.Node(node.tokenizer, 'comma')\n    assignment.append(assignmentList, 'expression')\n    for child in list(node):\n        if hasattr(child, 'name'):\n            if hasattr(child, 'initializer'):\n                assign = __createSimpleAssignment(child.name, child.initializer)\n                assignmentList.append(assign)\n            firstVarStatement.append(child)\n        else:\n            for identifier in child.names:\n                firstVarStatement.append(__createDeclaration(identifier.value))\n            if hasattr(child, 'initializer'):\n                assign = __createMultiAssignment(child.names, child.initializer)\n                assignmentList.append(assign)\n            node.remove(child)\n    if (len(assignmentList) > 0):\n        node.parent.replace(node, assignment)\n    elif (getattr(node, 'rel', None) == 'iterator'):\n        if hasattr(child, 'name'):\n            node.parent.replace(node, __createIdentifier(child.name))\n        else:\n            node.parent.replace(node, child.names)\n    else:\n        if hasattr(node, 'rel'):\n            Console.warn(('Remove related node (%s) from parent: %s' % (node.rel, node)))\n        node.parent.remove(node)\n    if (len(assignmentList) == 1):\n        assignment.replace(assignmentList, assignmentList[0])\n", "label": 1}
{"function": "\n\ndef _post_parse(self):\n    for test_id in self._unknown_entities:\n        matcher = (lambda i: ((i == test_id) or i.startswith(('%s.' % test_id))))\n        known_ids = filter(matcher, self._tests)\n        for id_ in known_ids:\n            if (self._tests[id_]['status'] == 'init'):\n                self._tests[id_]['status'] = self._unknown_entities[test_id]['status']\n            if self._unknown_entities[test_id].get('reason'):\n                self._tests[id_]['reason'] = self._unknown_entities[test_id]['reason']\n            elif self._unknown_entities[test_id].get('traceback'):\n                self._tests[id_]['traceback'] = self._unknown_entities[test_id]['traceback']\n    for test_id in self._expected_failures:\n        if self._tests.get(test_id):\n            if (self._tests[test_id]['status'] == 'fail'):\n                self._tests[test_id]['status'] = 'xfail'\n                if self._expected_failures[test_id]:\n                    self._tests[test_id]['reason'] = self._expected_failures[test_id]\n            elif (self._tests[test_id]['status'] == 'success'):\n                self._tests[test_id]['status'] = 'uxsuccess'\n    for test_id in self._tests:\n        for file_name in ['traceback', 'reason']:\n            if (file_name in self._tests[test_id]):\n                self._tests[test_id][file_name] = encodeutils.safe_decode(self._tests[test_id][file_name])\n    self._is_parsed = True\n", "label": 1}
{"function": "\n\ndef _restore(self, obj):\n    if has_tag(obj, tags.B64):\n        restore = self._restore_base64\n    elif has_tag(obj, tags.BYTES):\n        restore = self._restore_quopri\n    elif has_tag(obj, tags.ID):\n        restore = self._restore_id\n    elif has_tag(obj, tags.REF):\n        restore = self._restore_ref\n    elif has_tag(obj, tags.ITERATOR):\n        restore = self._restore_iterator\n    elif has_tag(obj, tags.TYPE):\n        restore = self._restore_type\n    elif has_tag(obj, tags.REPR):\n        restore = self._restore_repr\n    elif has_tag(obj, tags.REDUCE):\n        restore = self._restore_reduce\n    elif has_tag(obj, tags.OBJECT):\n        restore = self._restore_object\n    elif has_tag(obj, tags.FUNCTION):\n        restore = self._restore_function\n    elif util.is_list(obj):\n        restore = self._restore_list\n    elif has_tag(obj, tags.TUPLE):\n        restore = self._restore_tuple\n    elif has_tag(obj, tags.SET):\n        restore = self._restore_set\n    elif util.is_dictionary(obj):\n        restore = self._restore_dict\n    else:\n        restore = (lambda x: x)\n    return restore(obj)\n", "label": 1}
{"function": "\n\ndef __init__(self, *fields, **attributes):\n    if self.abstract:\n        raise TypeError('abstract nodes are not instanciable')\n    if fields:\n        if (len(fields) != len(self.fields)):\n            if (not self.fields):\n                raise TypeError(('%r takes 0 arguments' % self.__class__.__name__))\n            raise TypeError(('%r takes 0 or %d argument%s' % (self.__class__.__name__, len(self.fields), (((len(self.fields) != 1) and 's') or ''))))\n        for (name, arg) in izip(self.fields, fields):\n            setattr(self, name, arg)\n    for attr in self.attributes:\n        setattr(self, attr, attributes.pop(attr, None))\n    if attributes:\n        raise TypeError(('unknown attribute %r' % iter(attributes).next()))\n", "label": 1}
{"function": "\n\ndef listdir(self, path, start_time=None, end_time=None):\n    '\\n        Get an iterable with S3 folder contents.\\n        Iterable contains paths relative to queried path.\\n\\n        :param start_time: Optional argument to copy files with modified dates after start_time\\n        :param end_time: Optional argument to copy files with modified dates before end_time\\n        '\n    (bucket, key) = self._path_to_bucket_and_key(path)\n    s3_bucket = self.s3.get_bucket(bucket, validate=True)\n    key_path = self._add_path_delimiter(key)\n    key_path_len = len(key_path)\n    for item in s3_bucket.list(prefix=key_path):\n        last_modified_date = time.strptime(item.last_modified, '%Y-%m-%dT%H:%M:%S.%fZ')\n        if (((not start_time) and (not end_time)) or (start_time and (not end_time) and (start_time < last_modified_date)) or ((not start_time) and end_time and (last_modified_date < end_time)) or (start_time and end_time and (start_time < last_modified_date < end_time))):\n            (yield (self._add_path_delimiter(path) + item.key[key_path_len:]))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 3):\n            if (ftype == TType.STRING):\n                self.column_family = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.super_column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _process_dependent_arguments(self):\n    'Convert incoming configuration arguments to their\\n        proper form.\\n\\n        Callables are resolved, ORM annotations removed.\\n\\n        '\n    for attr in ('order_by', 'primaryjoin', 'secondaryjoin', 'secondary', '_user_defined_foreign_keys', 'remote_side'):\n        attr_value = getattr(self, attr)\n        if util.callable(attr_value):\n            setattr(self, attr, attr_value())\n    for attr in ('primaryjoin', 'secondaryjoin'):\n        val = getattr(self, attr)\n        if (val is not None):\n            setattr(self, attr, _orm_deannotate(expression._only_column_elements(val, attr)))\n    if ((self.order_by is not False) and (self.order_by is not None)):\n        self.order_by = [expression._only_column_elements(x, 'order_by') for x in util.to_list(self.order_by)]\n    self._user_defined_foreign_keys = util.column_set((expression._only_column_elements(x, 'foreign_keys') for x in util.to_column_set(self._user_defined_foreign_keys)))\n    self.remote_side = util.column_set((expression._only_column_elements(x, 'remote_side') for x in util.to_column_set(self.remote_side)))\n    self.target = self.mapper.mapped_table\n", "label": 1}
{"function": "\n\ndef get_task(self, taskname):\n    task = getattr(self.pavement, taskname, None)\n    if (not task):\n        for finder in self.task_finders:\n            task = finder.get_task(taskname)\n            if task:\n                break\n    if (not task):\n        task = _import_task(taskname)\n    if (not task):\n        all_tasks = self.get_tasks()\n        matches = [t for t in all_tasks if (t.shortname == taskname)]\n        if (len(matches) > 1):\n            matched_names = [t.name for t in matches]\n            raise BuildFailure(('Ambiguous task name %s (%s)' % (taskname, matched_names)))\n        elif matches:\n            task = matches[0]\n    return task\n", "label": 1}
{"function": "\n\ndef _update_document_fields_positional(self, doc, fields, spec, updater, subdocument=None):\n    'Implements the $set behavior on an existing document'\n    for (k, v) in iteritems(fields):\n        if ('$' in k):\n            field_name_parts = k.split('.')\n            if (not subdocument):\n                current_doc = doc\n                subspec = spec\n                for part in field_name_parts[:(- 1)]:\n                    if (part == '$'):\n                        subspec = subspec.get('$elemMatch', subspec)\n                        for item in current_doc:\n                            if filter_applies(subspec, item):\n                                current_doc = item\n                                break\n                        continue\n                    new_spec = {\n                        \n                    }\n                    for el in subspec:\n                        if el.startswith(part):\n                            if (len(el.split('.')) > 1):\n                                new_spec['.'.join(el.split('.')[1:])] = subspec[el]\n                            else:\n                                new_spec = subspec[el]\n                    subspec = new_spec\n                    current_doc = current_doc[part]\n                subdocument = current_doc\n                if ((field_name_parts[(- 1)] == '$') and isinstance(subdocument, list)):\n                    for (i, doc) in enumerate(subdocument):\n                        if filter_applies(subspec, doc):\n                            subdocument[i] = v\n                            break\n                    continue\n            updater(subdocument, field_name_parts[(- 1)], v)\n            continue\n        self._update_document_single_field(doc, k, v, updater)\n    return subdocument\n", "label": 1}
{"function": "\n\ndef update(self, keys):\n    '\\n        Update arrow position.\\n        '\n    if self.allow_input:\n        if (keys[pg.K_DOWN] and (not keys[pg.K_UP]) and (self.index == 0)):\n            self.index = 1\n            self.allow_input = False\n            self.notify(c.CLICK)\n        elif (keys[pg.K_UP] and (not keys[pg.K_DOWN]) and (self.index == 1)):\n            self.index = 0\n            self.allow_input = False\n            self.notify(c.CLICK)\n        self.rect.y = self.pos_list[self.index]\n    if ((not keys[pg.K_DOWN]) and (not keys[pg.K_UP])):\n        self.allow_input = True\n", "label": 1}
{"function": "\n\ndef get_field_parameters(self):\n    params = {\n        \n    }\n    if self.nullable:\n        params['null'] = True\n    if ((self.field_class is ForeignKeyField) or (self.name != self.db_column)):\n        params['db_column'] = (\"'%s'\" % self.db_column)\n    if (self.primary_key and (self.field_class is not PrimaryKeyField)):\n        params['primary_key'] = True\n    if self.is_foreign_key():\n        params['rel_model'] = self.rel_model\n        if self.to_field:\n            params['to_field'] = (\"'%s'\" % self.to_field)\n        if self.related_name:\n            params['related_name'] = (\"'%s'\" % self.related_name)\n    if (not self.is_primary_key()):\n        if self.unique:\n            params['unique'] = 'True'\n        elif (self.index and (not self.is_foreign_key())):\n            params['index'] = 'True'\n    return params\n", "label": 1}
{"function": "\n\ndef build_suite(self, test_labels=None, extra_tests=None, **kwargs):\n    suite = TestSuite()\n    test_labels = (test_labels or ['.'])\n    extra_tests = (extra_tests or [])\n    discover_kwargs = {\n        \n    }\n    if (self.pattern is not None):\n        discover_kwargs['pattern'] = self.pattern\n    if (self.top_level is not None):\n        discover_kwargs['top_level_dir'] = self.top_level\n    for label in test_labels:\n        kwargs = discover_kwargs.copy()\n        tests = None\n        label_as_path = os.path.abspath(label)\n        if (not os.path.exists(label_as_path)):\n            tests = self.test_loader.loadTestsFromName(label)\n        elif (os.path.isdir(label_as_path) and (not self.top_level)):\n            top_level = label_as_path\n            while True:\n                init_py = os.path.join(top_level, '__init__.py')\n                if os.path.exists(init_py):\n                    try_next = os.path.dirname(top_level)\n                    if (try_next == top_level):\n                        break\n                    top_level = try_next\n                    continue\n                break\n            kwargs['top_level_dir'] = top_level\n        if (not (tests and tests.countTestCases())):\n            tests = self.test_loader.discover(start_dir=label, **kwargs)\n            self.test_loader._top_level_dir = None\n        suite.addTests(tests)\n    for test in extra_tests:\n        suite.addTest(test)\n    return reorder_suite(suite, self.reorder_by)\n", "label": 1}
{"function": "\n\ndef mpf_exp(x, prec, rnd=round_fast):\n    (sign, man, exp, bc) = x\n    if man:\n        mag = (bc + exp)\n        wp = (prec + 14)\n        if sign:\n            man = (- man)\n        if ((prec > 600) and (exp >= 0)):\n            e = mpf_e((wp + int((1.45 * mag))))\n            return mpf_pow_int(e, (man << exp), prec, rnd)\n        if (mag < (- wp)):\n            return mpf_perturb(fone, sign, prec, rnd)\n        if (mag > 1):\n            wpmod = (wp + mag)\n            offset = (exp + wpmod)\n            if (offset >= 0):\n                t = (man << offset)\n            else:\n                t = (man >> (- offset))\n            lg2 = ln2_fixed(wpmod)\n            (n, t) = divmod(t, lg2)\n            n = int(n)\n            t >>= mag\n        else:\n            offset = (exp + wp)\n            if (offset >= 0):\n                t = (man << offset)\n            else:\n                t = (man >> (- offset))\n            n = 0\n        man = exp_basecase(t, wp)\n        return from_man_exp(man, (n - wp), prec, rnd)\n    if (not exp):\n        return fone\n    if (x == fninf):\n        return fzero\n    return x\n", "label": 1}
{"function": "\n\ndef inet_ntop(af, packed_ip):\n    'Convert an packed IP address of the given family to string format.'\n    if (af == AF_INET):\n        return inet_ntoa(packed_ip)\n    elif (af == AF_INET6):\n        if ((len(packed_ip) != 16) or (not _is_str(packed_ip))):\n            raise ValueError('invalid length of packed IP address string')\n        tokens = [('%x' % i) for i in _unpack('>8H', packed_ip)]\n        words = list(_unpack('>8H', packed_ip))\n        int_val = 0\n        for (i, num) in enumerate(reversed(words)):\n            word = num\n            word = (word << (16 * i))\n            int_val = (int_val | word)\n        if ((65535 < int_val <= 4294967295) or ((int_val >> 32) == 65535)):\n            packed_ipv4 = _pack('>2H', *[int(i, 16) for i in tokens[(- 2):]])\n            ipv4_str = inet_ntoa(packed_ipv4)\n            tokens = (tokens[0:(- 2)] + [ipv4_str])\n        return ':'.join(_compact_ipv6_tokens(tokens))\n    else:\n        raise ValueError(('unknown address family %d' % af))\n", "label": 1}
{"function": "\n\ndef gpx_fields_to_xml(instance, tag, version, custom_attributes=None):\n    fields = instance.gpx_10_fields\n    if (version == '1.1'):\n        fields = instance.gpx_11_fields\n    tag_open = bool(tag)\n    body = ''\n    if tag:\n        body = ('\\n<' + tag)\n        if custom_attributes:\n            for (key, value) in custom_attributes.items():\n                body += (' %s=\"%s\"' % (key, mod_utils.make_str(value)))\n    for gpx_field in fields:\n        if isinstance(gpx_field, str):\n            if tag_open:\n                body += '>'\n                tag_open = False\n            if (gpx_field[0] == '/'):\n                body += ('<%s>' % gpx_field)\n            else:\n                body += ('\\n<%s' % gpx_field)\n                tag_open = True\n        else:\n            value = getattr(instance, gpx_field.name)\n            if gpx_field.attribute:\n                body += (' ' + gpx_field.to_xml(value, version))\n            elif value:\n                if tag_open:\n                    body += '>'\n                    tag_open = False\n                xml_value = gpx_field.to_xml(value, version)\n                if xml_value:\n                    body += xml_value\n    if tag:\n        if tag_open:\n            body += '>'\n        body += (('</' + tag) + '>')\n    return body\n", "label": 1}
{"function": "\n\ndef load_directives(self, inherits):\n    \"there has got to be a better way\\n           plugin pattern away!!!\\n           \\n           #1 this could be a comprehension but it'd be hard\\n              to read.\\n           #2 maybe we should put a dummy sys.modules in place\\n              while doing this to limit junk in the name space.\\n        \"\n    m = self.__class__.__module__\n    exclude = ['__init__.py']\n    relpath = sys.modules[m].__file__\n    reldir = os.path.split(relpath)[0]\n    fulldir = os.path.realpath(reldir)\n    pyfiles = []\n    for f in os.listdir(fulldir):\n        ff = os.path.join(fulldir, f)\n        if os.path.isfile(ff):\n            pyfiles.append(f)\n    pyfiles = [f for f in pyfiles if f.endswith('.py')]\n    pyfiles = [f for f in pyfiles if (f not in exclude)]\n    pyfiles = [f[:(- 3)] for f in pyfiles]\n    candidates = []\n    for py in pyfiles:\n        m = __import__(py, globals(), locals(), 'romeo.directives')\n        for i in dir(m):\n            attr = getattr(m, i)\n            if (not (type(attr) == types.TypeType)):\n                continue\n            matches = [cls for cls in inherits if issubclass(attr, cls) if (attr.__name__ != cls.__name__)]\n            if (len(matches) == len(inherits)):\n                candidates.append(attr)\n    for c in candidates:\n        cinst = c(c.name, self.root, *c.init_args(self), **c.init_kwargs(self))\n        self.directives.append(cinst)\n", "label": 1}
{"function": "\n\ndef filter(self, iterable, prereleases=None):\n    if (prereleases is None):\n        prereleases = self.prereleases\n    if self._specs:\n        for spec in self._specs:\n            iterable = spec.filter(iterable, prereleases=bool(prereleases))\n        return iterable\n    else:\n        filtered = []\n        found_prereleases = []\n        for item in iterable:\n            if (not isinstance(item, (LegacyVersion, Version))):\n                parsed_version = parse(item)\n            else:\n                parsed_version = item\n            if isinstance(parsed_version, LegacyVersion):\n                continue\n            if (parsed_version.is_prerelease and (not prereleases)):\n                if (not filtered):\n                    found_prereleases.append(item)\n            else:\n                filtered.append(item)\n        if ((not filtered) and found_prereleases and (prereleases is None)):\n            return found_prereleases\n        return filtered\n", "label": 1}
{"function": "\n\n@login_required\ndef editor(request, id):\n    '\\n    Display the article editor.\\n    '\n    article = (get_object_or_404(Article, pk=id) if id else None)\n    if (article and (not article.can_edit(request))):\n        raise Http404\n    if (request.method == 'POST'):\n        form = EditorForm(instance=article, data=request.POST)\n        if form.is_valid():\n            creating = (article is None)\n            article = form.save(commit=False)\n            if creating:\n                article.author = request.user\n            article.save()\n            messages.info(request, ('The article has been saved.' if creating else 'Your changes to the article have been saved.'))\n            if (('action' in request.POST) and (request.POST['action'] == 'continue')):\n                return redirect('articles:editor', article.id)\n            else:\n                return redirect(article)\n    else:\n        form = EditorForm(instance=article)\n    return render(request, 'articles/editor.html', {\n        'title': (('Edit \"%s\"' % article) if article else 'New Article'),\n        'form': form,\n        'description': ('Use the form below to %s.' % ('edit the article' if article else 'create an article')),\n    })\n", "label": 1}
{"function": "\n\ndef _update_an_article(self, postid):\n    (afile, aname) = self.conf.get_article(postid, self.args.type)\n    if self.args.output:\n        self._write_html_file(afile)\n        return\n    (html, meta, txt, medias) = self._get_and_update_article_content(afile)\n    if (not html):\n        return\n    resultclass = WordPressPost\n    if (self.args.type == 'page'):\n        postid = meta.postid\n        resultclass = WordPressPage\n    post = self.wpcall(GetPost(postid, result_class=resultclass))\n    if (not post):\n        slog.warning(('No post \"%s\"!' % postid))\n        return\n    slog.info('Old article:')\n    self.print_results(post)\n    post.title = meta.title\n    post.user = meta.author\n    post.slug = meta.nicename\n    post.date = meta.date\n    post.content = html\n    post.post_status = meta.poststatus\n    if meta.modified:\n        post.date_modified = meta.modified\n    terms = self.cache.get_terms_from_meta(meta.category, meta.tags)\n    if terms:\n        post.terms = terms\n    elif (self.args.type == 'post'):\n        slog.warning('Please provide some terms.')\n        return\n    succ = self.wpcall(EditPost(postid, post))\n    if (succ == None):\n        return\n    if succ:\n        slog.info(('Update %s successfully!' % postid))\n    else:\n        slog.info(('Update %s fail!' % postid))\n", "label": 1}
{"function": "\n\ndef _filter_requirements(lines_iter, filter_names=None, filter_sys_version=False):\n    for line in lines_iter:\n        line = line.strip()\n        if ((not line) or line.startswith('#')):\n            continue\n        match = REQ_PATTERN.match(line)\n        if (match is None):\n            raise AssertionError((\"Could not parse requirement: '%s'\" % line))\n        name = match.group('name')\n        if ((filter_names is not None) and (name not in filter_names)):\n            continue\n        if (filter_sys_version and match.group('pyspec')):\n            (pycomp, pyspec) = match.group('pycomp', 'pyspec')\n            comp = STR_TO_CMP[pycomp]\n            pyver_spec = StrictVersion(pyspec)\n            if comp(SYS_VERSION, pyver_spec):\n                (yield line.split(';')[0])\n            continue\n        (yield line)\n", "label": 1}
{"function": "\n\ndef parse(self, node):\n    '\\n        Get tag name, indentantion and correct attributes of a node according\\n        to its class\\n        '\n    node_class_name = node.__class__.__name__\n    spec = self.rst_terms[node_class_name]\n    tag_name = (spec[0] or node_class_name)\n    use_name_in_class = ((len(spec) > 3) and spec[3])\n    indent = (spec[4] if (len(spec) > 4) else True)\n    if use_name_in_class:\n        node['classes'].insert(0, node_class_name)\n    attributes = {\n        \n    }\n    replacements = {\n        'refuri': 'href',\n        'uri': 'src',\n        'refid': 'href',\n        'morerows': 'rowspan',\n        'morecols': 'colspan',\n        'classes': 'class',\n        'ids': 'id',\n    }\n    ignores = ('names', 'dupnames', 'bullet', 'enumtype', 'colwidth', 'stub', 'backrefs', 'auto', 'anonymous')\n    for (k, v) in node.attributes.items():\n        if ((k in ignores) or (not v)):\n            continue\n        if (k in replacements):\n            k = replacements[k]\n        if isinstance(v, list):\n            v = ' '.join(v)\n        attributes[k] = v\n    if getattr(self, 'next_elem_attr', None):\n        attributes.update(self.next_elem_attr)\n        del self.next_elem_attr\n    return (tag_name, indent, attributes)\n", "label": 1}
{"function": "\n\ndef find_referenced_templates(ast):\n    'Finds all the referenced templates from the AST.  This will return an\\n    iterator over all the hardcoded template extensions, inclusions and\\n    imports.  If dynamic inheritance or inclusion is used, `None` will be\\n    yielded.\\n\\n    >>> from jinja2 import Environment, meta\\n    >>> env = Environment()\\n    >>> ast = env.parse(\\'{% extends \"layout.html\" %}{% include helper %}\\')\\n    >>> list(meta.find_referenced_templates(ast))\\n    [\\'layout.html\\', None]\\n\\n    This function is useful for dependency tracking.  For example if you want\\n    to rebuild parts of the website after a layout template has changed.\\n    '\n    for node in ast.find_all((nodes.Extends, nodes.FromImport, nodes.Import, nodes.Include)):\n        if (not isinstance(node.template, nodes.Const)):\n            if isinstance(node.template, (nodes.Tuple, nodes.List)):\n                for template_name in node.template.items:\n                    if isinstance(template_name, nodes.Const):\n                        if isinstance(template_name.value, string_types):\n                            (yield template_name.value)\n                    else:\n                        (yield None)\n            else:\n                (yield None)\n            continue\n        if isinstance(node.template.value, string_types):\n            (yield node.template.value)\n        elif (isinstance(node, nodes.Include) and isinstance(node.template.value, (tuple, list))):\n            for template_name in node.template.value:\n                if isinstance(template_name, string_types):\n                    (yield template_name)\n        else:\n            (yield None)\n", "label": 1}
{"function": "\n\ndef sufficient_options(self):\n    'Check if all required options are present.\\n\\n        :raises: AuthPluginOptionsMissing\\n        '\n    has_token = (self.opts.get('token') or self.opts.get('auth_token'))\n    no_auth = (has_token and self.opts.get('endpoint'))\n    has_tenant = (self.opts.get('tenant_id') or self.opts.get('tenant_name'))\n    has_credential = (self.opts.get('username') and has_tenant and self.opts.get('password') and self.opts.get('auth_url'))\n    missing = (not (no_auth or has_credential))\n    if missing:\n        missing_opts = []\n        opts = ['token', 'endpoint', 'username', 'password', 'auth_url', 'tenant_id', 'tenant_name']\n        for opt in opts:\n            if (not self.opts.get(opt)):\n                missing_opts.append(opt)\n        raise exceptions.AuthPluginOptionsMissing(missing_opts)\n", "label": 1}
{"function": "\n\ndef addFile(self, relPath, fullPath, distname, override=False):\n    fileName = os.path.basename(relPath)\n    fileExtension = os.path.splitext(fileName)[1]\n    if self.__package:\n        fileId = ('%s/' % self.__package)\n    else:\n        fileId = ''\n    if ((fileExtension in classExtensions) and (distname == 'classes')):\n        fileId += os.path.splitext(relPath)[0]\n        construct = jasy.item.Class.ClassItem\n        dist = self.classes\n    elif ((fileExtension in translationExtensions) and (distname == 'translations')):\n        fileId += os.path.splitext(relPath)[0]\n        construct = jasy.item.Translation.TranslationItem\n        dist = self.translations\n    elif (fileName in docFiles):\n        fileId += os.path.dirname(relPath)\n        fileId = fileId.strip('/')\n        construct = jasy.item.Doc.DocItem\n        dist = self.docs\n    else:\n        fileId += relPath\n        construct = jasy.item.Asset.AssetItem\n        dist = self.assets\n    if (construct != jasy.item.Asset.AssetItem):\n        fileId = fileId.replace('/', '.')\n    if ((fileId in dist) and (not override)):\n        raise UserError(('Item ID was registered before: %s' % fileId))\n    item = construct(self, fileId).attach(fullPath)\n    Console.debug(('Registering %s %s' % (item.kind, fileId)))\n    dist[fileId] = item\n", "label": 1}
{"function": "\n\ndef _fill_remote(cur, keep_files):\n    'Add references to remote Keep files if present and not local.\\n    '\n    if isinstance(cur, (list, tuple)):\n        return [_fill_remote(x, keep_files) for x in cur]\n    elif isinstance(cur, dict):\n        out = {\n            \n        }\n        for (k, v) in cur.items():\n            out[k] = _fill_remote(v, keep_files)\n        return out\n    elif (isinstance(cur, basestring) and os.path.splitext(cur)[(- 1)] and (not os.path.exists(cur))):\n        for test_keep in keep_files:\n            if test_keep.endswith(cur):\n                return test_keep\n        return cur\n    else:\n        return cur\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kwargs):\n    evaluate = kwargs.get('evaluate', global_evaluate[0])\n    if iterable(args[0]):\n        if (isinstance(args[0], Point) and (not evaluate)):\n            return args[0]\n        args = args[0]\n    coords = Tuple(*args)\n    if any(((a.is_number and im(a)) for a in coords)):\n        raise ValueError('Imaginary coordinates not permitted.')\n    if evaluate:\n        coords = coords.xreplace(dict([(f, simplify(nsimplify(f, rational=True))) for f in coords.atoms(Float)]))\n    if (len(coords) == 2):\n        return Point2D(coords, **kwargs)\n    if (len(coords) == 3):\n        return Point3D(coords, **kwargs)\n    return GeometryEntity.__new__(cls, *coords)\n", "label": 1}
{"function": "\n\ndef iter_version_links(html, name):\n    '\\n    Iterate through version links (in order) within HTML.\\n\\n    Filtering out links that don\\'t \"look\" like versions.\\n\\n    Either yields hrefs to be recursively searches or tuples of (name, href)\\n    that match the given name.\\n    '\n    soup = BeautifulSoup(html)\n    for node in soup.findAll('a'):\n        if (node.get('href') is None):\n            continue\n        try:\n            (guessed_name, _) = guess_name_and_version(node.text)\n        except ValueError:\n            href = node['href']\n            for extension in ['.tar.gz', '.zip']:\n                if href.endswith(extension):\n                    (yield (basename(href), href))\n                    break\n            else:\n                if (node.get('rel') == 'download'):\n                    (yield href)\n        else:\n            if (guessed_name.replace('_', '-').lower() != name.replace('_', '-').lower()):\n                continue\n            (yield (node.text, node['href']))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _tensAdd_flatten(args):\n    if (not all((isinstance(x, TensExpr) for x in args))):\n        args1 = []\n        for x in args:\n            if isinstance(x, TensExpr):\n                if isinstance(x, TensAdd):\n                    args1.extend(list(x.args))\n                else:\n                    args1.append(x)\n        args1 = [x for x in args1 if (isinstance(x, TensExpr) and x.coeff)]\n        args2 = [x for x in args if (not isinstance(x, TensExpr))]\n        t1 = TensMul.from_data(Add(*args2), [], [], [])\n        args = ([t1] + args1)\n    a = []\n    for x in args:\n        if isinstance(x, TensAdd):\n            a.extend(list(x.args))\n        else:\n            a.append(x)\n    args = [x for x in a if x.coeff]\n    return args\n", "label": 1}
{"function": "\n\ndef read_fasta_lengths(ifile):\n    'Generate sequence ID,length from stream ifile'\n    id = None\n    seqLength = 0\n    isEmpty = True\n    for line in ifile:\n        if ('>' == line[0]):\n            if ((id is not None) and (seqLength > 0)):\n                (yield (id, seqLength))\n                isEmpty = False\n            id = line[1:].split()[0]\n            seqLength = 0\n        elif (id is not None):\n            for word in line.split():\n                seqLength += len(word)\n    if ((id is not None) and (seqLength > 0)):\n        (yield (id, seqLength))\n    elif isEmpty:\n        raise IOError('no readable sequence in FASTA file!')\n", "label": 1}
{"function": "\n\ndef configure_dhcp_for_network(self, network):\n    if (not network.admin_state_up):\n        return\n    enable_metadata = self.dhcp_driver_cls.should_enable_metadata(self.conf, network)\n    dhcp_network_enabled = False\n    for subnet in network.subnets:\n        if subnet.enable_dhcp:\n            if self.call_driver('enable', network):\n                dhcp_network_enabled = True\n                self.cache.put(network)\n            break\n    if (enable_metadata and dhcp_network_enabled):\n        for subnet in network.subnets:\n            if ((subnet.ip_version == 4) and subnet.enable_dhcp):\n                self.enable_isolated_metadata_proxy(network)\n                break\n    elif ((not self.conf.force_metadata) and (not self.conf.enable_isolated_metadata)):\n        self.disable_isolated_metadata_proxy(network)\n", "label": 1}
{"function": "\n\ndef lex(s, name=None, trim_whitespace=True, line_offset=0, delimeters=None):\n    if (delimeters is None):\n        delimeters = (Template.default_namespace['start_braces'], Template.default_namespace['end_braces'])\n    in_expr = False\n    chunks = []\n    last = 0\n    last_pos = ((line_offset + 1), 1)\n    token_re = re.compile(('%s|%s' % (re.escape(delimeters[0]), re.escape(delimeters[1]))))\n    for match in token_re.finditer(s):\n        expr = match.group(0)\n        pos = find_position(s, match.end(), last, last_pos)\n        if ((expr == delimeters[0]) and in_expr):\n            raise TemplateError(('%s inside expression' % delimeters[0]), position=pos, name=name)\n        elif ((expr == delimeters[1]) and (not in_expr)):\n            raise TemplateError(('%s outside expression' % delimeters[1]), position=pos, name=name)\n        if (expr == delimeters[0]):\n            part = s[last:match.start()]\n            if part:\n                chunks.append(part)\n            in_expr = True\n        else:\n            chunks.append((s[last:match.start()], last_pos))\n            in_expr = False\n        last = match.end()\n        last_pos = pos\n    if in_expr:\n        raise TemplateError(('No %s to finish last expression' % delimeters[1]), name=name, position=last_pos)\n    part = s[last:]\n    if part:\n        chunks.append(part)\n    if trim_whitespace:\n        chunks = trim_lex(chunks)\n    return chunks\n", "label": 1}
{"function": "\n\ndef test05_ManyActorsUniqueAddress(self):\n    asys = ActorSystem()\n    addresses = [asys.createActor(Juliet) for n in range(100)]\n    uniqueAddresses = set(addresses)\n    if (len(addresses) != len(uniqueAddresses)):\n        duplicates = [A for A in uniqueAddresses if (len([X for X in addresses if (X == A)]) > 1)]\n        print(('Duplicates: %s' % map(str, duplicates)))\n        if duplicates:\n            for each in duplicates:\n                print(('... %s at: %s' % (str(each), str([N for (N, A) in enumerate(addresses) if (A == each)]))))\n        print('Note: if this is a UDPTransport test, be advised that Linux occasionally does seem to assign the same UDP port multiple times.  Linux bug?')\n    self.assertEqual(len(addresses), len(uniqueAddresses))\n", "label": 1}
{"function": "\n\ndef add_spatial_info(features, add_x=True, add_y=True, inplace=False, dtype=None):\n    \"\\n    Adds spatial information to image features (which should contain a frames\\n    attribute in the format created by extract_image_features).\\n\\n    Adds a feature for x (if add_x) and y (if add_y), which are relative (x, y)\\n    locations within the image of the feature between 0 and 1 (inclusive).\\n\\n    Returns a new Features object with these additional features, or modifies\\n    features and returns None if inplace is True.\\n\\n    If dtype is not None, the resulting array will have that dtype. Otherwise,\\n    it will maintain features.dtype if it's a float type, or float32 if not.\\n    \"\n    if ((not add_x) and (not add_y)):\n        return (None if inplace else features)\n    indices = []\n    if add_x:\n        indices.append(0)\n    if add_y:\n        indices.append(1)\n    if (dtype is None):\n        dtype = features.dtype\n        if (dtype.kind != 'f'):\n            dtype = np.float32\n    spatial = np.asarray(np.vstack(features.frames)[:, indices], dtype=dtype)\n    spatial /= spatial.max(axis=0)\n    new_feats = np.hstack((features._features, spatial))\n    if inplace:\n        features._features = new_feats\n        features._refresh_features()\n    else:\n        return Features(new_feats, n_pts=features._n_pts, categories=features.categories, names=features.names, **dict(((k, features.data[k]) for k in features._extra_names)))\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if (self is other):\n        return True\n    return (isinstance(other, self.__class__) and (self._numeric_id == other._numeric_id) and (self._id == other._id) and (self._cells_table == other._cells_table) and (self._contents_table == other._contents_table) and (self._deps_table == other._deps_table) and (self._renames == other._renames) and (self._deltas == other._deltas) and (self._cell_count == other._cell_count) and (self._content_count == other._content_count))\n", "label": 1}
{"function": "\n\ndef get_dot_completions(view, prefix, position, info):\n    if ((not get_setting(view, 'fw1_enabled')) or (len(info['dot_context']) == 0)):\n        return None\n    if extends_fw1(view):\n        if (info['dot_context'][(- 1)].name == 'variables'):\n            key = '.'.join([symbol.name for symbol in reversed(info['dot_context'])])\n            if (key in fw1['settings']):\n                return CompletionList(fw1['settings'][key], 1, False)\n        if (info['dot_context'][(- 1)].name in ['renderdata', 'renderer']):\n            return CompletionList(fw1['methods']['renderdata'], 1, False)\n    if (get_file_type(view) == 'controller'):\n        if ((len(info['dot_context']) > 1) and (info['dot_context'][(- 2)].name in ['renderdata', 'renderer'])):\n            return CompletionList(fw1['methods']['renderdata'], 1, False)\n        if (info['dot_context'][(- 1)].name in ['fw', 'framework']):\n            return CompletionList(fw1['methods']['calls'], 1, False)\n    return None\n", "label": 1}
{"function": "\n\n@login_required\n@transaction.atomic\ndef delete_posts(request, topic_id):\n    topic = Topic.objects.select_related().get(pk=topic_id)\n    if forum_moderated_by(topic, request.user):\n        deleted = False\n        post_list = request.POST.getlist('post')\n        for post_id in post_list:\n            if (not deleted):\n                deleted = True\n            delete_post(request, post_id)\n        if deleted:\n            messages.success(request, _('Post deleted.'))\n            return HttpResponseRedirect(topic.get_absolute_url())\n    last_post = topic.posts.latest()\n    if request.user.is_authenticated():\n        topic.update_read(request.user)\n    posts = topic.posts.all().select_related()\n    initial = {\n        \n    }\n    if request.user.is_authenticated():\n        initial = {\n            'markup': request.user.forum_profile.markup,\n        }\n    form = AddPostForm(topic=topic, initial=initial)\n    moderator = (request.user.is_superuser or (request.user in topic.forum.moderators.all()))\n    if (request.user.is_authenticated() and (request.user in topic.subscribers.all())):\n        subscribed = True\n    else:\n        subscribed = False\n    return render(request, 'djangobb_forum/delete_posts.html', {\n        'topic': topic,\n        'last_post': last_post,\n        'form': form,\n        'moderator': moderator,\n        'subscribed': subscribed,\n        'posts_page': get_page(posts, request, forum_settings.TOPIC_PAGE_SIZE),\n    })\n", "label": 1}
{"function": "\n\n@register.simple_tag\ndef aggregate_section_totals(section_name, results_arr, daily):\n    'Hackish function to do a summation of a section in the org_report'\n    startindex = (- 1)\n    endindex = (- 1)\n    for itemarr in results_arr:\n        if (itemarr[1] == section_name):\n            startindex = results_arr.index(itemarr)\n            continue\n        if (startindex >= 0):\n            if (itemarr[1] != None):\n                endindex = results_arr.index(itemarr)\n                break\n    summation = []\n    section_arr = []\n    if (endindex == (- 1)):\n        section_arr = results_arr[startindex:]\n    else:\n        section_arr = results_arr[startindex:(endindex + 1)]\n    for itemarr in section_arr:\n        if (summation == []):\n            summation = (summation + itemarr[(- 1)])\n        else:\n            for i in range(0, len(itemarr[(- 1)])):\n                summation[i] += itemarr[(- 1)][i]\n    ret = ''\n    if daily:\n        for item in summation:\n            ret += ('<td style=\"background:#99FFFF\"><strong>%d</strong></td>' % item)\n    else:\n        sum = 0\n        for item in summation:\n            sum += item\n        ret = ('<td>%d</td>' % sum)\n    return ret\n", "label": 1}
{"function": "\n\n@staticmethod\ndef budget_monitoring_percentage(row):\n    '\\n            Virtual Field to show the percentage used of the Budget\\n        '\n    if hasattr(row, 'budget_monitoring'):\n        row = row.budget_monitoring\n    if hasattr(row, 'planned'):\n        planned = row.planned\n        if (planned == 0.0):\n            return current.messages['NONE']\n    else:\n        planned = None\n    if hasattr(row, 'value'):\n        actual = row.value\n    else:\n        actual = None\n    if ((planned is not None) and (actual is not None)):\n        percentage = ((actual / planned) * 100)\n        return ('%s %%' % '{0:.2f}'.format(percentage))\n    if hasattr(row, 'id'):\n        table = current.s3db.budget_monitoring\n        r = current.db((table.id == row.id)).select(table.planned, table.value, limitby=(0, 1)).first()\n        if r:\n            planned = r.planned\n            if (planned == 0.0):\n                return current.messages['NONE']\n            percentage = ((r.value / planned) * 100)\n            return ('%s %%' % percentage)\n    return current.messages['NONE']\n", "label": 1}
{"function": "\n\ndef _construct_keymaps(self, config):\n    '\\n        Construct keymaps for handling input\\n        '\n    keymap = {\n        \n    }\n    move_keymap = {\n        \n    }\n    for key in config.move_left:\n        keymap[key] = self._move\n        move_keymap[key] = 7\n    for key in config.move_up:\n        keymap[key] = self._move\n        move_keymap[key] = 1\n    for key in config.move_right:\n        keymap[key] = self._move\n        move_keymap[key] = 3\n    for key in config.move_down:\n        keymap[key] = self._move\n        move_keymap[key] = 5\n    for key in config.start:\n        keymap[key] = self._menu\n    for key in config.action_a:\n        keymap[key] = self._action_a\n    for key in config.back:\n        keymap[key] = self._back\n    for key in config.left_shoulder:\n        keymap[key] = self._shoulder_left\n    for key in config.right_shoulder:\n        keymap[key] = self._shoulder_right\n    for key in config.mode_1:\n        keymap[key] = self._zoom_out\n    for key in config.mode_2:\n        keymap[key] = self._zoom_in\n    return (keymap, move_keymap)\n", "label": 1}
{"function": "\n\ndef create_table_constraints(self, table):\n    constraints = []\n    if table.primary_key:\n        constraints.append(table.primary_key)\n    constraints.extend([c for c in table._sorted_constraints if (c is not table.primary_key)])\n    return ', \\n\\t'.join((p for p in (self.process(constraint) for constraint in constraints if (((constraint._create_rule is None) or constraint._create_rule(self)) and ((not self.dialect.supports_alter) or (not getattr(constraint, 'use_alter', False))))) if (p is not None)))\n", "label": 1}
{"function": "\n\ndef __dir__(self):\n    'return list of member names'\n    cls_members = []\n    cname = self.__class__.__name__\n    if ((cname != 'SymbolTable') and hasattr(self, '__class__')):\n        cls_members = dir(self.__class__)\n    dict_keys = [key for key in self.__dict__ if (key not in cls_members)]\n    return [key for key in (cls_members + dict_keys) if ((not key.startswith('_SymbolTable_')) and (not key.startswith('_Group_')) and (not key.startswith(('_%s_' % cname))) and (not (key.startswith('__') and key.endswith('__'))) and (key not in self.__private))]\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.LIST):\n                self.expressions = []\n                (_etype17, _size14) = iprot.readListBegin()\n                for _i18 in xrange(_size14):\n                    _elem19 = IndexExpression()\n                    _elem19.read(iprot)\n                    self.expressions.append(_elem19)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.start_key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef run(self, show_trees=False):\n    '\\n        Sentences in the test suite are divided into two classes:\\n         - grammatical (``accept``) and\\n         - ungrammatical (``reject``).\\n        If a sentence should parse accordng to the grammar, the value of\\n        ``trees`` will be a non-empty list. If a sentence should be rejected\\n        according to the grammar, then the value of ``trees`` will be None.\\n        '\n    for test in self.suite:\n        print((test['doc'] + ':'), end=' ')\n        for key in ['accept', 'reject']:\n            for sent in test[key]:\n                tokens = sent.split()\n                trees = list(self.cp.parse(tokens))\n                if (show_trees and trees):\n                    print()\n                    print(sent)\n                    for tree in trees:\n                        print(tree)\n                if (key == 'accept'):\n                    if (trees == []):\n                        raise ValueError((\"Sentence '%s' failed to parse'\" % sent))\n                    else:\n                        accepted = True\n                elif trees:\n                    raise ValueError((\"Sentence '%s' received a parse'\" % sent))\n                else:\n                    rejected = True\n        if (accepted and rejected):\n            print('All tests passed!')\n", "label": 1}
{"function": "\n\ndef maximalSquare(self, matrix):\n    if (not matrix):\n        return 0\n    (H, W) = (0, 1)\n    table = [[[0, 0] for j in xrange(len(matrix[0]))] for i in xrange(len(matrix))]\n    for i in reversed(xrange(len(matrix))):\n        for j in reversed(xrange(len(matrix[i]))):\n            if (matrix[i][j] == '1'):\n                (h, w) = (1, 1)\n                if ((i + 1) < len(matrix)):\n                    h = (table[(i + 1)][j][H] + 1)\n                if ((j + 1) < len(matrix[i])):\n                    w = (table[i][(j + 1)][W] + 1)\n                table[i][j] = [h, w]\n    s = [[0 for j in xrange(len(matrix[0]))] for i in xrange(len(matrix))]\n    max_square_area = 0\n    for i in reversed(xrange(len(matrix))):\n        for j in reversed(xrange(len(matrix[i]))):\n            side = min(table[i][j][H], table[i][j][W])\n            if (matrix[i][j] == '1'):\n                if (((i + 1) < len(matrix)) and ((j + 1) < len(matrix[(i + 1)]))):\n                    side = min((s[(i + 1)][(j + 1)] + 1), side)\n                s[i][j] = side\n                max_square_area = max(max_square_area, (side * side))\n    return max_square_area\n", "label": 1}
{"function": "\n\ndef download(self):\n    subdata = self.http.request('get', self.url, cookies=self.options.cookies)\n    data = None\n    if (self.subtype == 'tt'):\n        data = self.tt(subdata)\n    if (self.subtype == 'json'):\n        data = self.json(subdata)\n    if (self.subtype == 'sami'):\n        data = self.sami(subdata)\n    if (self.subtype == 'smi'):\n        data = self.smi(subdata)\n    if (self.subtype == 'wrst'):\n        data = self.wrst(subdata)\n    if (self.subtype == 'raw'):\n        if is_py2:\n            data = subdata.text.encode('utf-8')\n        else:\n            data = subdata.text\n    if ((platform.system() == 'Windows') and is_py3):\n        file_d = output(self.options, 'srt', mode='wt', encoding='utf-8')\n    else:\n        file_d = output(self.options, 'srt', mode='wt')\n    if (hasattr(file_d, 'read') is False):\n        return\n    file_d.write(data)\n    file_d.close()\n", "label": 1}
{"function": "\n\ndef flatten_data(self, resource_object, parser_context, is_list):\n    '\\n        Flattens data objects, making attributes and relationships fields the same level as id and type.\\n        '\n    relationships = resource_object.get('relationships')\n    is_relationship = parser_context.get('is_relationship')\n    request_method = parser_context['request'].method\n    if (is_relationship and (request_method == 'POST')):\n        if (not relationships):\n            raise JSONAPIException(source={\n                'pointer': '/data/relationships',\n            }, detail=NO_RELATIONSHIPS_ERROR)\n    elif (('attributes' not in resource_object) and (request_method != 'DELETE')):\n        raise JSONAPIException(source={\n            'pointer': '/data/attributes',\n        }, detail=NO_ATTRIBUTES_ERROR)\n    object_id = resource_object.get('id')\n    object_type = resource_object.get('type')\n    if (is_list and (request_method == 'DELETE')):\n        if (object_id is None):\n            raise JSONAPIException(source={\n                'pointer': '/data/id',\n            }, detail=NO_ID_ERROR)\n        if (object_type is None):\n            raise JSONAPIException(source={\n                'pointer': '/data/type',\n            }, detail=NO_TYPE_ERROR)\n    attributes = resource_object.get('attributes')\n    parsed = {\n        'id': object_id,\n        'type': object_type,\n    }\n    if attributes:\n        parsed.update(attributes)\n    if relationships:\n        relationships = self.flatten_relationships(relationships)\n        parsed.update(relationships)\n    return parsed\n", "label": 1}
{"function": "\n\ndef parse_options(self, arg):\n    '\\n        Parse options with the argv\\n\\n        :param arg: one arg from argv\\n        '\n    if (not arg.startswith('-')):\n        return False\n    value = None\n    if ('=' in arg):\n        (arg, value) = arg.split('=')\n    for option in self._option_list:\n        if (arg not in (option.shortname, option.longname)):\n            continue\n        action = option.action\n        if action:\n            action()\n        if (option.key == option.shortname):\n            self._results[option.key] = True\n            return True\n        if (option.boolean and option.default):\n            self._results[option.key] = False\n            return True\n        if option.boolean:\n            self._results[option.key] = True\n            return True\n        if (not value):\n            if self._argv:\n                value = self._argv[0]\n                self._argv = self._argv[1:]\n        if (not value):\n            raise RuntimeError(('Missing value for: %s' % option.name))\n        self._results[option.key] = option.to_python(value)\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I64):\n                self.latency_ns = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I64):\n                self.cpu_time_ns = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.cardinality = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I64):\n                self.memory_used = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    if ('REQUEST_URI' not in environ):\n        environ['REQUEST_URI'] = (quote(environ.get('SCRIPT_NAME', '')) + quote(environ.get('PATH_INFO', '')))\n    if self.include_os_environ:\n        cgi_environ = os.environ.copy()\n    else:\n        cgi_environ = {\n            \n        }\n    for name in environ:\n        if ((name.upper() == name) and isinstance(environ[name], str)):\n            cgi_environ[name] = environ[name]\n    if (self.query_string is not None):\n        old = cgi_environ.get('QUERY_STRING', '')\n        if old:\n            old += '&'\n        cgi_environ['QUERY_STRING'] = (old + self.query_string)\n    cgi_environ['SCRIPT_FILENAME'] = self.script\n    proc = subprocess.Popen([self.script], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=cgi_environ, cwd=os.path.dirname(self.script))\n    writer = CGIWriter(environ, start_response)\n    if (select and (sys.platform != 'win32')):\n        proc_communicate(proc, stdin=StdinReader.from_environ(environ), stdout=writer, stderr=environ['wsgi.errors'])\n    else:\n        (stdout, stderr) = proc.communicate(StdinReader.from_environ(environ).read())\n        if stderr:\n            environ['wsgi.errors'].write(stderr)\n        writer.write(stdout)\n    if (not writer.headers_finished):\n        start_response(writer.status, writer.headers)\n    return []\n", "label": 1}
{"function": "\n\ndef arg2nodes(self, args, node_factory=_null, lookup_list=_null, **kw):\n    if (node_factory is _null):\n        node_factory = self.fs.File\n    if (lookup_list is _null):\n        lookup_list = self.lookup_list\n    if (not args):\n        return []\n    args = SCons.Util.flatten(args)\n    nodes = []\n    for v in args:\n        if SCons.Util.is_String(v):\n            n = None\n            for l in lookup_list:\n                n = l(v)\n                if (n is not None):\n                    break\n            if (n is not None):\n                if SCons.Util.is_String(n):\n                    kw['raw'] = 1\n                    n = self.subst(n, **kw)\n                    if node_factory:\n                        n = node_factory(n)\n                if SCons.Util.is_List(n):\n                    nodes.extend(n)\n                else:\n                    nodes.append(n)\n            elif node_factory:\n                kw['raw'] = 1\n                v = node_factory(self.subst(v, **kw))\n                if SCons.Util.is_List(v):\n                    nodes.extend(v)\n                else:\n                    nodes.append(v)\n        else:\n            nodes.append(v)\n    return nodes\n", "label": 1}
{"function": "\n\ndef equals(self, other):\n    other = sympify(other)\n    if (isinstance(other, TensMul) and (other._coeff == 0)):\n        return all(((x._coeff == 0) for x in self.args))\n    if isinstance(other, TensExpr):\n        if (self.rank != other.rank):\n            return False\n    if isinstance(other, TensAdd):\n        if (set(self.args) != set(other.args)):\n            return False\n        else:\n            return True\n    t = (self - other)\n    if (not isinstance(t, TensExpr)):\n        return (t == 0)\n    elif isinstance(t, TensMul):\n        return (t._coeff == 0)\n    else:\n        return all(((x._coeff == 0) for x in t.args))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.groupName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef checkNode(graph, superpipeline, source, target, nodeType, expectedDataType, values, isInput):\n    errors = er.consistencyErrors()\n    data = superpipeline.pipelineConfigurationData[superpipeline.pipeline]\n    longFormArgument = graph.CM_getArgumentAttribute(graph.graph, source, target, 'longFormArgument')\n    dataType = graph.CM_getArgumentAttribute(graph.graph, source, target, 'dataType')\n    isLinkOnly = graph.CM_getArgumentAttribute(graph.graph, source, target, 'isLinkOnly')\n    if (not isLinkOnly):\n        if (not expectedDataType):\n            expectedDataType = dataType\n        elif (expectedDataType != dataType):\n            print('dataConsistency.checkNode - 1', dataType, expectedDataType)\n            exit(0)\n        for value in values:\n            if (not isCorrectDataType(value, expectedDataType)):\n                print('dataConsistency.checkNode - 2', longFormArgument, value, dataType, type(value))\n                exit(0)\n            if (nodeType == 'file'):\n                if (not graph.CM_getArgumentAttribute(graph.graph, source, target, 'isStub')):\n                    extensions = graph.CM_getArgumentAttribute(graph.graph, source, target, 'extensions')\n                    if extensions:\n                        task = (target if isInput else source)\n                        fileNodeId = (source if isInput else target)\n                        if (not checkExtensions(value, extensions)):\n                            if (longFormArgument in data.longFormArguments.keys()):\n                                shortFormArgument = data.longFormArguments[longFormArgument].shortFormArgument\n                                errors.invalidExtensionPipeline(longFormArgument, shortFormArgument, value, extensions)\n                            else:\n                                shortFormArgument = graph.CM_getArgumentAttribute(graph.graph, source, target, 'shortFormArgument')\n                                errors.invalidExtension(task, longFormArgument, shortFormArgument, value, extensions)\n        return expectedDataType\n", "label": 1}
{"function": "\n\ndef tag(self, data):\n    now = [((('', 'BOS'), ('', 'BOS')), 0.0, [])]\n    for w in data:\n        stage = {\n            \n        }\n        not_found = True\n        for s in self.status:\n            if (self.uni.freq((w, s)) != 0):\n                not_found = False\n                break\n        if not_found:\n            for s in self.status:\n                for pre in now:\n                    stage[(pre[0][1], (w, s))] = (pre[1], (pre[2] + [s]))\n            now = list(map((lambda x: (x[0], x[1][0], x[1][1])), stage.items()))\n            continue\n        for s in self.status:\n            for pre in now:\n                p = (pre[1] + self.log_prob(pre[0][0], pre[0][1], (w, s)))\n                if ((not ((pre[0][1], (w, s)) in stage)) or (p > stage[(pre[0][1], (w, s))][0])):\n                    stage[(pre[0][1], (w, s))] = (p, (pre[2] + [s]))\n        now = list(map((lambda x: (x[0], x[1][0], x[1][1])), stage.items()))\n    return zip(data, max(now, key=(lambda x: x[1]))[2])\n", "label": 1}
{"function": "\n\ndef _write_viewing_info(self, group):\n    if ((self.peeloff_origin is not None) and (self.inside_observer is not None)):\n        raise Exception('Cannot specify inside observer and peeloff origin at the same time')\n    if (self.inside_observer is not None):\n        group.attrs['inside_observer'] = bool2str(True)\n        self._write_inside_observer(group)\n        if (self.viewing_angles == []):\n            self.set_viewing_angles([90.0], [0.0])\n        if (self.image and (self.xmin < self.xmax)):\n            raise ValueError('longitudes should increase towards the left for inside observers')\n        if (self.d_min < 0.0):\n            if (self.d_min != (- np.inf)):\n                raise ValueError('Lower limit of depth should be positive for inside observer')\n            self.d_min = 0.0\n        if (self.d_max < 0.0):\n            raise ValueError('Upper limit of depth should be positive for inside observer')\n    elif (len(self.viewing_angles) > 0):\n        group.attrs['inside_observer'] = bool2str(False)\n        if (self.peeloff_origin is None):\n            self.set_peeloff_origin((0.0, 0.0, 0.0))\n        self._write_peeloff_origin(group)\n    else:\n        raise Exception('Need to specify either observer position, or viewing angles')\n    self._write_ignore_optical_depth(group)\n    self._write_viewing_angles(group)\n    self._write_depth(group)\n", "label": 1}
{"function": "\n\ndef line_contains_open_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)/\\\\*', line) and (not re.match('^(\\\\+|\\\\-)(.*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(.*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef _filter_metadata(metadata, **kwargs):\n    if (not isinstance(metadata, dict)):\n        return metadata\n    filtered_metadata = {\n        \n    }\n    for (key, value) in metadata.items():\n        if (key == '_self'):\n            default_value = value.get('default_value', None)\n            if (default_value is None):\n                default_callback_params = value.get('default_callback_params', {\n                    \n                })\n                callback_params = dict(kwargs)\n                if default_callback_params:\n                    callback_params.update(default_callback_params)\n                default_callback = value.get('default_callback', None)\n                if default_callback:\n                    default_value = default_callback(key, **callback_params)\n            options = value.get('options', None)\n            if (options is None):\n                options_callback_params = value.get('options_callback_params', {\n                    \n                })\n                callback_params = dict(kwargs)\n                if options_callback_params:\n                    callback_params.update(options_callback_params)\n                options_callback = value.get('options_callback', None)\n                if options_callback:\n                    options = options_callback(key, **callback_params)\n            filtered_metadata[key] = value\n            if (default_value is not None):\n                filtered_metadata[key]['default_value'] = default_value\n            if (options is not None):\n                filtered_metadata[key]['options'] = options\n        else:\n            filtered_metadata[key] = _filter_metadata(value, **kwargs)\n    return filtered_metadata\n", "label": 1}
{"function": "\n\ndef find_corpus_fileids(root, regexp):\n    if (not isinstance(root, PathPointer)):\n        raise TypeError('find_corpus_fileids: expected a PathPointer')\n    regexp += '$'\n    if isinstance(root, ZipFilePathPointer):\n        fileids = [name[len(root.entry):] for name in root.zipfile.namelist() if (not name.endswith('/'))]\n        items = [name for name in fileids if re.match(regexp, name)]\n        return sorted(items)\n    elif isinstance(root, FileSystemPathPointer):\n        items = []\n        kwargs = {\n            \n        }\n        if (not py25()):\n            kwargs = {\n                'followlinks': True,\n            }\n        for (dirname, subdirs, fileids) in os.walk(root.path, **kwargs):\n            prefix = ''.join((('%s/' % p) for p in _path_from(root.path, dirname)))\n            items += [(prefix + fileid) for fileid in fileids if re.match(regexp, (prefix + fileid))]\n            if ('.svn' in subdirs):\n                subdirs.remove('.svn')\n        return sorted(items)\n    else:\n        raise AssertionError((\"Don't know how to handle %r\" % root))\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef action_for_event_by_user(self, event, handler, current_state):\n    actions_by_user = {\n        \n    }\n    users_dict = (yield self.store.are_guests(self.rules_by_user.keys()))\n    filtered_by_user = (yield handler.filter_events_for_clients(users_dict.items(), [event], {\n        event.event_id: current_state,\n    }))\n    room_members = (yield self.store.get_users_in_room(self.room_id))\n    evaluator = PushRuleEvaluatorForEvent(event, len(room_members))\n    condition_cache = {\n        \n    }\n    display_names = {\n        \n    }\n    for ev in current_state.values():\n        nm = ev.content.get('displayname', None)\n        if (nm and (ev.type == EventTypes.Member)):\n            display_names[ev.state_key] = nm\n    for (uid, rules) in self.rules_by_user.items():\n        display_name = display_names.get(uid, None)\n        filtered = filtered_by_user[uid]\n        if (len(filtered) == 0):\n            continue\n        if (filtered[0].sender == uid):\n            continue\n        for rule in rules:\n            if (('enabled' in rule) and (not rule['enabled'])):\n                continue\n            matches = _condition_checker(evaluator, rule['conditions'], uid, display_name, condition_cache)\n            if matches:\n                actions = [x for x in rule['actions'] if (x != 'dont_notify')]\n                if (actions and ('notify' in actions)):\n                    actions_by_user[uid] = actions\n                break\n    defer.returnValue(actions_by_user)\n", "label": 1}
{"function": "\n\ndef create_tree_from_coverage(cov, strip_prefix=None, path_aliases=None, cover=[], exclude=[]):\n    'Create a tree with coverage statistics.\\n\\n    Takes a coverage.coverage() instance.\\n\\n    Returns the root node of the tree.\\n    '\n    root = CoverageNode()\n    if path_aliases:\n        apply_path_aliases(cov, dict([alias.partition('=')[::2] for alias in path_aliases]))\n    for filename in cov.data.measured_files():\n        if (not any(((pattern in filename.replace('/', '.')) for pattern in cover))):\n            continue\n        if any(((pattern in filename.replace('/', '.')) for pattern in exclude)):\n            continue\n        if (strip_prefix and filename.startswith(strip_prefix)):\n            short_name = filename[len(strip_prefix):]\n            short_name = short_name.replace('/', os.path.sep)\n            short_name = short_name.lstrip(os.path.sep)\n        else:\n            short_name = cov.file_locator.relative_filename(filename)\n        tree_index = filename_to_list(short_name.replace(os.path.sep, '.'))\n        if (('tests' in tree_index) or ('ftests' in tree_index)):\n            continue\n        root.set_at(tree_index, CoverageCoverageNode(cov, filename))\n    return root\n", "label": 1}
{"function": "\n\ndef frictionforce(rbtdef, ifunc=None):\n    'Generate friction forces (Coulomb/viscouse model plus offset).'\n    if (not ifunc):\n        ifunc = identity\n    fric = zeros(rbtdef.dof, 1)\n    if ((rbtdef.frictionmodel is None) or (len(rbtdef.frictionmodel) == 0)):\n        pass\n    else:\n        askedterms = set(rbtdef.frictionmodel)\n        if askedterms.issubset(_frictionterms):\n            if ('viscous' in askedterms):\n                for i in range(rbtdef.dof):\n                    fric[i] += (rbtdef.fv[i] * rbtdef.dq[i])\n            if ('Coulomb' in askedterms):\n                for i in range(rbtdef.dof):\n                    fric[i] += (rbtdef.fc[i] * sign(rbtdef.dq[i]))\n            if ('offset' in askedterms):\n                for i in range(rbtdef.dof):\n                    fric[i] += rbtdef.fo[i]\n            fric[i] = ifunc(fric[i])\n        else:\n            raise Exception((\"Friction model terms '%s' not understanded. Use None or a combination of %s.\" % (str((askedterms - _frictionterms)), _frictionterms)))\n    return fric\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.threadName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.threadStringRepresentation = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.isDaemon = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.LIST):\n                self.stackTrace = []\n                (_etype3, _size0) = iprot.readListBegin()\n                for _i4 in xrange(_size0):\n                    _elem5 = StackTraceElement()\n                    _elem5.read(iprot)\n                    self.stackTrace.append(_elem5)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef analyze_manifest(self):\n    self.manifest_files = mf = {\n        \n    }\n    if (not self.distribution.include_package_data):\n        return\n    src_dirs = {\n        \n    }\n    for package in (self.packages or ()):\n        src_dirs[assert_relative(self.get_package_dir(package))] = package\n    self.run_command('egg_info')\n    ei_cmd = self.get_finalized_command('egg_info')\n    for path in ei_cmd.filelist.files:\n        (d, f) = os.path.split(assert_relative(path))\n        prev = None\n        oldf = f\n        while (d and (d != prev) and (d not in src_dirs)):\n            prev = d\n            (d, df) = os.path.split(d)\n            f = os.path.join(df, f)\n        if (d in src_dirs):\n            if (path.endswith('.py') and (f == oldf)):\n                continue\n            mf.setdefault(src_dirs[d], []).append(path)\n", "label": 1}
{"function": "\n\ndef _prettifyETree(self, elem):\n    ' Recursively add linebreaks to ElementTree children. '\n    i = '\\n'\n    if (markdown.isBlockLevel(elem.tag) and (elem.tag not in ['code', 'pre'])):\n        if (((not elem.text) or (not elem.text.strip())) and len(elem) and markdown.isBlockLevel(elem[0].tag)):\n            elem.text = i\n        for e in elem:\n            if markdown.isBlockLevel(e.tag):\n                self._prettifyETree(e)\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n    if ((not elem.tail) or (not elem.tail.strip())):\n        elem.tail = i\n", "label": 1}
{"function": "\n\ndef write(self, s):\n    self._checkWritable()\n    if self.closed:\n        raise ValueError('write to closed file')\n    if (not isinstance(s, unicode)):\n        raise TypeError((\"can't write %s to text stream\" % s.__class__.__name__))\n    length = len(s)\n    haslf = ((self._writetranslate or self._line_buffering) and ('\\n' in s))\n    if (haslf and self._writetranslate and (self._writenl != '\\n')):\n        s = s.replace('\\n', self._writenl)\n    encoder = (self._encoder or self._get_encoder())\n    b = encoder.encode(s)\n    self.buffer.write(b)\n    if (self._line_buffering and (haslf or ('\\r' in s))):\n        self.flush()\n    self._snapshot = None\n    if self._decoder:\n        self._decoder.reset()\n    return length\n", "label": 1}
{"function": "\n\ndef geo_apps(namespace=True, runtests=False):\n    '\\n    Returns a list of GeoDjango test applications that reside in\\n    `django.contrib.gis.tests` that can be used with the current\\n    database and the spatial libraries that are installed.\\n    '\n    from django.db import connection\n    from django.contrib.gis.geos import GEOS_PREPARE\n    from django.contrib.gis.gdal import HAS_GDAL\n    apps = ['geoapp', 'relatedapp']\n    if (not connection.ops.mysql):\n        apps.append('distapp')\n    if (connection.ops.postgis and connection.ops.geography):\n        apps.append('geogapp')\n    if HAS_GDAL:\n        if (connection.ops.postgis and GEOS_PREPARE):\n            apps.append('geo3d')\n        apps.append('layermap')\n    if runtests:\n        return [('django.contrib.gis.tests', app) for app in apps]\n    elif namespace:\n        return [('django.contrib.gis.tests.%s' % app) for app in apps]\n    else:\n        return apps\n", "label": 1}
{"function": "\n\ndef dfs(self, s, cur, left, pi, i, rmcnt, ret):\n    '\\n        Remove parenthesis\\n        backtracking, post-check\\n        :param s: original string\\n        :param cur: current string builder\\n        :param left: number of remaining left parentheses in s[0..i] not consumed by \")\"\\n        :param pi: last removed char\\n        :param i: current index\\n        :param rmcnt: number of remaining removals needed\\n        :param ret: results\\n        '\n    if ((left < 0) or (rmcnt < 0) or (i > len(s))):\n        return\n    if (i == len(s)):\n        if ((rmcnt == 0) and (left == 0)):\n            ret.append(cur)\n        return\n    if (s[i] not in ('(', ')')):\n        self.dfs(s, (cur + s[i]), left, None, (i + 1), rmcnt, ret)\n    elif (pi == s[i]):\n        while ((i < len(s)) and pi and (pi == s[i])):\n            (i, rmcnt) = ((i + 1), (rmcnt - 1))\n        self.dfs(s, cur, left, pi, i, rmcnt, ret)\n    else:\n        self.dfs(s, cur, left, s[i], (i + 1), (rmcnt - 1), ret)\n        L = ((left + 1) if (s[i] == '(') else (left - 1))\n        self.dfs(s, (cur + s[i]), L, None, (i + 1), rmcnt, ret)\n", "label": 1}
{"function": "\n\ndef convert(self, model, field, field_args, multiple=False):\n    kwargs = {\n        'label': unicode((field.verbose_name or field.name or '')),\n        'description': (field.help_text or ''),\n        'validators': [],\n        'filters': [],\n        'default': field.default,\n    }\n    if field_args:\n        kwargs.update(field_args)\n    if field.required:\n        if (isinstance(field, IntField) or isinstance(field, FloatField)):\n            kwargs['validators'].append(validators.InputRequired())\n        else:\n            kwargs['validators'].append(validators.Required())\n    else:\n        kwargs['validators'].append(validators.Optional())\n    if field.choices:\n        kwargs['choices'] = field.choices\n        if isinstance(field, IntField):\n            kwargs['coerce'] = int\n        if (not multiple):\n            return f.SelectField(**kwargs)\n        else:\n            return f.SelectMultipleField(**kwargs)\n    ftype = type(field).__name__\n    if hasattr(field, 'to_form_field'):\n        return field.to_form_field(model, kwargs)\n    if (ftype in self.converters):\n        return self.converters[ftype](model, field, kwargs)\n", "label": 1}
{"function": "\n\n@classmethod\ndef update_next(cls, seconds, now):\n    'Setup theme for next update.'\n    cls.next_change = None\n    cls.day = None\n    closest = None\n    lowest = None\n    for t in cls.themes:\n        if ((seconds < t.time) and ((closest is None) or (t.time < closest.time))):\n            closest = t\n        if ((lowest is None) or (t.time < lowest.time)):\n            lowest = t\n    if (closest is not None):\n        cls.next_change = closest\n    elif (lowest is not None):\n        cls.next_change = lowest\n    if (lowest is not None):\n        cls.lowest = lowest\n    if ((cls.next_change.time == cls.lowest.time) and (seconds < cls.lowest.time)):\n        cls.day = (- 1)\n    else:\n        cls.day = now.day\n    debug_log(('Today = %d' % cls.day))\n    debug_log(('%s - Next Change @ %s' % (time.ctime(), str(cls.next_change))))\n", "label": 1}
{"function": "\n\n@classmethod\ndef read(cls, fd, str_cache=None, object_cache=None, traits_cache=None):\n    type_ = U8.read(fd)\n    if (str_cache is None):\n        str_cache = []\n    if (object_cache is None):\n        object_cache = []\n    if (traits_cache is None):\n        traits_cache = []\n    if ((type_ == AMF3_TYPE_UNDEFINED) or (type_ == AMF3_TYPE_NULL)):\n        return None\n    elif (type_ == AMF3_TYPE_FALSE):\n        return False\n    elif (type_ == AMF3_TYPE_TRUE):\n        return True\n    elif (type_ == AMF3_TYPE_STRING):\n        return AMF3String.read(fd, cache=str_cache)\n    elif (type_ == AMF3_TYPE_ARRAY):\n        return AMF3ArrayPacker.read(fd, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif (type_ == AMF3_TYPE_OBJECT):\n        return AMF3ObjectPacker.read(fd, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif (type_ == AMF3_TYPE_DATE):\n        return AMF3DatePacker.read(fd, cache=object_cache)\n    elif (type_ in cls.Readers):\n        return cls.Readers[type_].read(fd)\n    else:\n        raise IOError('Unhandled AMF3 type: {0}'.format(hex(type_)))\n", "label": 1}
{"function": "\n\ndef __delitem__(self, key):\n    if (isinstance(key, slice) or self._haswildcard(key)):\n        if isinstance(key, slice):\n            indices = range(*key.indices(len(self)))\n            if (key.step and (key.step < 0)):\n                indices = reversed(indices)\n        else:\n            indices = self._wildcardmatch(key)\n        for idx in reversed(indices):\n            del self[idx]\n        return\n    elif isinstance(key, string_types):\n        key = Card.normalize_keyword(key)\n        indices = self._keyword_indices\n        if (key not in self._keyword_indices):\n            indices = self._rvkc_indices\n        if (key not in indices):\n            raise KeyError((\"Keyword '%s' not found.\" % key))\n        for idx in reversed(indices[key]):\n            del self[idx]\n        return\n    idx = self._cardindex(key)\n    card = self._cards[idx]\n    keyword = card.keyword\n    del self._cards[idx]\n    indices = self._keyword_indices[keyword]\n    indices.remove(idx)\n    if (not indices):\n        del self._keyword_indices[keyword]\n    if (card.field_specifier is not None):\n        indices = self._rvkc_indices[card.rawkeyword]\n        indices.remove(idx)\n        if (not indices):\n            del self._rvkc_indices[card.rawkeyword]\n    self._updateindices(idx, increment=False)\n    self._modified = True\n", "label": 1}
{"function": "\n\ndef populate_database(self, model, level=1, parents=(None,)):\n    n_siblings = self.siblings_per_level[(level - 1)]\n    for parent in parents:\n        if (model in (TreePlace, TreebeardALPlace)):\n            bulk = [model(parent=parent) for _ in range(n_siblings)]\n            model.objects.bulk_create(bulk)\n            objects = model.objects.filter(parent=parent)\n        elif (model in (TreebeardMPPlace, TreebeardNSPlace)):\n            if (parent is not None):\n                parent = model.objects.get(pk=parent.pk)\n            objects = [(model.add_root() if (parent is None) else parent.add_child()) for _ in range(n_siblings)]\n        else:\n            objects = [model.objects.create(parent=parent) for _ in range(n_siblings)]\n        (yield model.objects.count())\n        if (level < len(self.siblings_per_level)):\n            for count in self.populate_database(model, (level + 1), objects):\n                (yield count)\n", "label": 1}
{"function": "\n\ndef classify(url):\n    '\\n    Classifies an URL and returns the corresponding WebEntity derivative.\\n    '\n    original = url\n    if isinstance(url, WebEntity):\n        return url\n    if (not isinstance(url, str)):\n        raise TypeError(('%s not valid type for 4chan URL' % type(url)))\n    if url.startswith('/'):\n        url = url.lstrip('/')\n    if url.endswith('/'):\n        url = url.rstrip('/')\n    if (not ('4chan.org' in url)):\n        url = '{}://{}/{}'.format(Links.scheme, Links.netloc, url)\n    if (not (url.startswith('http://') or url.startswith('https://'))):\n        url = '{}://{}'.format(Links.scheme, url)\n    path = urlparse.urlparse(url).path\n    match = Links.thread_pattern.match(path)\n    if match:\n        return Thread(*match.groups())\n    match = Links.page_pattern.match(path)\n    if match:\n        return Page(*match.groups())\n    match = Links.board_pattern.match(path)\n    if match:\n        return Board(*match.groups())\n    raise ValueError(('invalid 4chan URL: %s' % repr(original)))\n", "label": 1}
{"function": "\n\ndef _get_option_tuples(self, option_string):\n    result = []\n    chars = self.prefix_chars\n    if ((option_string[0] in chars) and (option_string[1] in chars)):\n        if ('=' in option_string):\n            (option_prefix, explicit_arg) = option_string.split('=', 1)\n        else:\n            option_prefix = option_string\n            explicit_arg = None\n        for option_string in self._option_string_actions:\n            if option_string.startswith(option_prefix):\n                action = self._option_string_actions[option_string]\n                tup = (action, option_string, explicit_arg)\n                result.append(tup)\n    elif ((option_string[0] in chars) and (option_string[1] not in chars)):\n        option_prefix = option_string\n        explicit_arg = None\n        short_option_prefix = option_string[:2]\n        short_explicit_arg = option_string[2:]\n        for option_string in self._option_string_actions:\n            if (option_string == short_option_prefix):\n                action = self._option_string_actions[option_string]\n                tup = (action, option_string, short_explicit_arg)\n                result.append(tup)\n            elif option_string.startswith(option_prefix):\n                action = self._option_string_actions[option_string]\n                tup = (action, option_string, explicit_arg)\n                result.append(tup)\n    else:\n        self.error((_('unexpected option string: %s') % option_string))\n    return result\n", "label": 1}
{"function": "\n\ndef _raise_ssl_error(self, ssl, result):\n    if (self._context._verify_helper is not None):\n        self._context._verify_helper.raise_if_problem()\n    if (self._context._npn_advertise_helper is not None):\n        self._context._npn_advertise_helper.raise_if_problem()\n    if (self._context._npn_select_helper is not None):\n        self._context._npn_select_helper.raise_if_problem()\n    if (self._context._alpn_select_helper is not None):\n        self._context._alpn_select_helper.raise_if_problem()\n    error = _lib.SSL_get_error(ssl, result)\n    if (error == _lib.SSL_ERROR_WANT_READ):\n        raise WantReadError()\n    elif (error == _lib.SSL_ERROR_WANT_WRITE):\n        raise WantWriteError()\n    elif (error == _lib.SSL_ERROR_ZERO_RETURN):\n        raise ZeroReturnError()\n    elif (error == _lib.SSL_ERROR_WANT_X509_LOOKUP):\n        raise WantX509LookupError()\n    elif (error == _lib.SSL_ERROR_SYSCALL):\n        if (_lib.ERR_peek_error() == 0):\n            if (result < 0):\n                if (platform == 'win32'):\n                    errno = _ffi.getwinerror()[0]\n                else:\n                    errno = _ffi.errno\n                raise SysCallError(errno, errorcode.get(errno))\n            else:\n                raise SysCallError((- 1), 'Unexpected EOF')\n        else:\n            _raise_current_error()\n    elif (error == _lib.SSL_ERROR_NONE):\n        pass\n    else:\n        _raise_current_error()\n", "label": 1}
{"function": "\n\ndef log(self, level, msg, *args, **kw):\n    if args:\n        if kw:\n            raise TypeError('You may give positional or keyword arguments, not both')\n    args = (args or kw)\n    rendered = None\n    for (consumer_level, consumer) in self.consumers:\n        if self.level_matches(level, consumer_level):\n            if (self.in_progress_hanging and (consumer in (sys.stdout, sys.stderr))):\n                self.in_progress_hanging = False\n                sys.stdout.write('\\n')\n                sys.stdout.flush()\n            if (rendered is None):\n                if args:\n                    rendered = (msg % args)\n                else:\n                    rendered = msg\n                rendered = ((' ' * self.indent) + rendered)\n            if hasattr(consumer, 'write'):\n                consumer.write((rendered + '\\n'))\n            else:\n                consumer(rendered)\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw_args):\n    args = [sympify(arg) for arg in args]\n    args = TensMul._flatten(args)\n    is_canon_bp = kw_args.get('is_canon_bp', False)\n    if (not any([isinstance(arg, TensExpr) for arg in args])):\n        tids = TIDS([], [], [])\n    else:\n        tids_list = [arg._tids for arg in args if isinstance(arg, (Tensor, TensMul))]\n        if (len(tids_list) == 1):\n            for arg in args:\n                if (not isinstance(arg, Tensor)):\n                    continue\n                is_canon_bp = kw_args.get('is_canon_bp', arg._is_canon_bp)\n        tids = reduce((lambda a, b: (a * b)), tids_list)\n    if any([isinstance(arg, TensAdd) for arg in args]):\n        add_args = TensAdd._tensAdd_flatten(args)\n        return TensAdd(*add_args)\n    coeff = reduce((lambda a, b: (a * b)), ([S.One] + [arg for arg in args if (not isinstance(arg, TensExpr))]))\n    args = tids.get_tensors()\n    if (coeff != 1):\n        args = ([coeff] + args)\n    if (len(args) == 1):\n        return args[0]\n    obj = Basic.__new__(cls, *args)\n    obj._types = []\n    for t in tids.components:\n        obj._types.extend(t._types)\n    obj._tids = tids\n    obj._ext_rank = (len(obj._tids.free) + (2 * len(obj._tids.dum)))\n    obj._coeff = coeff\n    obj._is_canon_bp = is_canon_bp\n    return obj\n", "label": 1}
{"function": "\n\ndef processArgs(self, parsedArguments):\n    \"Update the Setup's config to reflect parsedArguments.\\n\\n        parsedArguments - the result of an argparse parse action.\\n        \"\n    parsed = parsedArguments\n    if (('stackLogfile' in parsed) and parsed.stackLogfile):\n        self.config.backgroundStackTraceLoopFilename = parsed.stackLogfile\n    if (('memoryLogfile' in parsed) and parsed.memoryLogfile):\n        self.config.backgroundMemoryUsageLoopFilename = parsed.memoryLogfile\n    if (('logging' in parsed) and parsed.logging):\n        self.config.setLoggingLevel(parsed.logging, parsed.background_logging)\n    if (('target' in parsed) and parsed.target):\n        self.config.target = parsed.target\n    if (('dataroot' in parsed) and parsed.dataroot):\n        path = os.path.expanduser(parsed.dataroot)\n        self.config.setRootDataDir(path)\n    if (('datarootsubdir' in parsed) and parsed.datarootsubdir):\n        path = os.path.join(self.config.rootDataDir, parsed.datarootsubdir)\n        self.config.setRootDataDir(path)\n    if (('baseport' in parsed) and parsed.baseport):\n        self.config.setAllPorts(parsed.baseport)\n    self.parsedArguments = parsedArguments\n", "label": 1}
{"function": "\n\ndef parse(self, argv=None):\n    '\\n        Parse argv of terminal\\n\\n        :param argv: default is sys.argv\\n        '\n    if (not argv):\n        argv = sys.argv\n    elif isinstance(argv, str):\n        argv = argv.split()\n    self._argv = argv[1:]\n    if (not self._argv):\n        self.validate_options()\n        if self._command_func:\n            self._command_func(**self._results)\n            return True\n        return False\n    cmd = self._argv[0]\n    if (not cmd.startswith('-')):\n        for command in self._command_list:\n            if (isinstance(command, Command) and (command._name == cmd)):\n                command._parent = self\n                return command.parse(self._argv)\n    _positional_index = 0\n    while self._argv:\n        arg = self._argv[0]\n        self._argv = self._argv[1:]\n        if (not self.parse_options(arg)):\n            self._args_results.append(arg)\n            if (len(self._positional_list) > _positional_index):\n                key = self._positional_list[_positional_index]\n                self._results[key] = arg\n                _positional_index += 1\n    self.validate_options()\n    if (self._parent and isinstance(self._parent, Command)):\n        self._parent._args_results = self._args_results\n    if self._command_func:\n        self._command_func(**self._results)\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef _key_opts(self):\n    '\\n        Return options for the ssh command base for Salt to call\\n        '\n    options = ['KbdInteractiveAuthentication=no']\n    if self.passwd:\n        options.append('PasswordAuthentication=yes')\n    else:\n        options.append('PasswordAuthentication=no')\n    if (self.opts.get('_ssh_version', (0,)) > (4, 9)):\n        options.append('GSSAPIAuthentication=no')\n    options.append('ConnectTimeout={0}'.format(self.timeout))\n    if self.opts.get('ignore_host_keys'):\n        options.append('StrictHostKeyChecking=no')\n    if self.opts.get('no_host_keys'):\n        options.extend(['StrictHostKeyChecking=no', 'UserKnownHostsFile=/dev/null'])\n    known_hosts = self.opts.get('known_hosts_file')\n    if (known_hosts and os.path.isfile(known_hosts)):\n        options.append('UserKnownHostsFile={0}'.format(known_hosts))\n    if self.port:\n        options.append('Port={0}'.format(self.port))\n    if self.priv:\n        options.append('IdentityFile={0}'.format(self.priv))\n    if self.user:\n        options.append('User={0}'.format(self.user))\n    if self.identities_only:\n        options.append('IdentitiesOnly=yes')\n    ret = []\n    for option in options:\n        ret.append('-o {0} '.format(option))\n    return ''.join(ret)\n", "label": 1}
{"function": "\n\ndef get_query_string(p, new_params=None, remove=None):\n    '\\n    Add and remove query parameters. From `django.contrib.admin`.\\n    '\n    if (new_params is None):\n        new_params = {\n            \n        }\n    if (remove is None):\n        remove = []\n    for r in remove:\n        for k in list(p.keys()):\n            if (k == r):\n                del p[k]\n    for (k, v) in list(new_params.items()):\n        if ((k in p) and (v is None)):\n            del p[k]\n        elif (v is not None):\n            p[k] = v\n    return ('?' + '&'.join([('%s=%s' % (urlquote(k), urlquote(v))) for (k, v) in p.items()]))\n", "label": 1}
{"function": "\n\ndef format_progress_line(self):\n    show_percent = self.show_percent\n    info_bits = []\n    if self.length_known:\n        bar_length = int((self.pct * self.width))\n        bar = (self.fill_char * bar_length)\n        bar += (self.empty_char * (self.width - bar_length))\n        if (show_percent is None):\n            show_percent = (not self.show_pos)\n    elif self.finished:\n        bar = (self.fill_char * self.width)\n    else:\n        bar = list((self.empty_char * (self.width or 1)))\n        if (self.time_per_iteration != 0):\n            bar[int((((math.cos((self.pos * self.time_per_iteration)) / 2.0) + 0.5) * self.width))] = self.fill_char\n        bar = ''.join(bar)\n    if self.show_pos:\n        info_bits.append(self.format_pos())\n    if show_percent:\n        info_bits.append(self.format_pct())\n    if (self.show_eta and self.eta_known and (not self.finished)):\n        info_bits.append(self.format_eta())\n    if (self.item_show_func is not None):\n        item_info = self.item_show_func(self.current_item)\n        if (item_info is not None):\n            info_bits.append(item_info)\n    return (self.bar_template % {\n        'label': self.label,\n        'bar': bar,\n        'info': self.info_sep.join(info_bits),\n    }).rstrip()\n", "label": 1}
{"function": "\n\ndef __init__(self, kernel, itspace, *args, **kwargs):\n    base.ParLoop.__init__(self, kernel, itspace, *args, **kwargs)\n    self.__unwound_args = []\n    self.__unique_args = []\n    self._arg_dict = {\n        \n    }\n    seen = set()\n    c = 0\n    for arg in self._actual_args:\n        if arg._is_mat:\n            for a in arg:\n                self.__unwound_args.append(a)\n        elif (arg._is_vec_map or arg._uses_itspace):\n            for (d, m) in zip(arg.data, arg.map):\n                for i in range(m.arity):\n                    a = d(arg.access, m[i])\n                    a.position = arg.position\n                    self.__unwound_args.append(a)\n        else:\n            for a in arg:\n                self.__unwound_args.append(a)\n        if arg._is_dat:\n            key = (arg.data, arg.map)\n            if arg._is_indirect:\n                arg._which_indirect = c\n                if (arg._is_vec_map or arg._flatten):\n                    c += arg.map.arity\n                elif arg._uses_itspace:\n                    c += self._it_space.extents[arg.idx.index]\n                else:\n                    c += 1\n            if (key not in seen):\n                self.__unique_args.append(arg)\n                seen.add(key)\n        else:\n            self.__unique_args.append(arg)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse_content(node):\n    \" Parse content from input node and returns ContentHandler object\\n        it'll look like:\\n\\n            - template:\\n                - file:\\n                    - temple: path\\n\\n            or something\\n\\n        \"\n    output = ContentHandler()\n    is_template_path = False\n    is_template_content = False\n    is_file = False\n    is_done = False\n    while (node and (not is_done)):\n        if isinstance(node, basestring):\n            output.content = node\n            output.setup(node, is_file=is_file, is_template_path=is_template_path, is_template_content=is_template_content)\n            return output\n        elif ((not isinstance(node, dict)) and (not isinstance(node, list))):\n            raise TypeError('Content must be a string, dictionary, or list of dictionaries')\n        is_done = True\n        flat = lowercase_keys(flatten_dictionaries(node))\n        for (key, value) in flat.items():\n            if (key == 'template'):\n                if isinstance(value, basestring):\n                    if is_file:\n                        value = os.path.abspath(value)\n                    output.content = value\n                    is_template_content = (is_template_content or (not is_file))\n                    output.is_template_content = is_template_content\n                    output.is_template_path = is_file\n                    output.is_file = is_file\n                    return output\n                else:\n                    is_template_content = True\n                    node = value\n                    is_done = False\n                    break\n            elif (key == 'file'):\n                if isinstance(value, basestring):\n                    output.content = os.path.abspath(value)\n                    output.is_file = True\n                    output.is_template_content = is_template_content\n                    return output\n                else:\n                    is_file = True\n                    node = value\n                    is_done = False\n                    break\n    raise Exception('Invalid configuration for content.')\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.device = TrafficControlledDevice()\n                self.device.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.timeout = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef step(self, dependency_states, step_context):\n    input_node = self._input_node()\n    input_state = dependency_states.get(input_node, None)\n    if ((input_state is None) or (type(input_state) == Waiting)):\n        return Waiting([input_node])\n    elif (type(input_state) == Throw):\n        return input_state\n    elif (type(input_state) == Noop):\n        return Noop('Could not compute {} in order to project its fields.'.format(input_node))\n    elif (type(input_state) != Return):\n        State.raise_unrecognized(input_state)\n    input_product = input_state.value\n    values = []\n    for field in self.fields:\n        values.append(getattr(input_product, field))\n    if ((len(values) == 1) and (type(values[0]) is self.projected_subject)):\n        projected_subject = values[0]\n    else:\n        projected_subject = self.projected_subject(*values)\n    output_node = self._output_node(step_context, projected_subject)\n    output_state = dependency_states.get(output_node, None)\n    if ((output_state is None) or (type(output_state) == Waiting)):\n        return Waiting([input_node, output_node])\n    elif (type(output_state) == Noop):\n        return Noop('Successfully projected, but no source of output product for {}.'.format(output_node))\n    elif (type(output_state) in [Throw, Return]):\n        return output_state\n    else:\n        raise State.raise_unrecognized(output_state)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = QueryNotFoundException()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.error2 = BeeswaxException()\n                self.error2.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef parse(self, lines):\n    'parse the input lines from a robots.txt file.\\n           We allow that a user-agent: line is not preceded by\\n           one or more blank lines.'\n    state = 0\n    linenumber = 0\n    entry = Entry()\n    for line in lines:\n        linenumber += 1\n        if (not line):\n            if (state == 1):\n                entry = Entry()\n                state = 0\n            elif (state == 2):\n                self._add_entry(entry)\n                entry = Entry()\n                state = 0\n        i = line.find('#')\n        if (i >= 0):\n            line = line[:i]\n        line = line.strip()\n        if (not line):\n            continue\n        line = line.split(':', 1)\n        if (len(line) == 2):\n            line[0] = line[0].strip().lower()\n            line[1] = urllib.unquote(line[1].strip())\n            if (line[0] == 'user-agent'):\n                if (state == 2):\n                    self._add_entry(entry)\n                    entry = Entry()\n                entry.useragents.append(line[1])\n                state = 1\n            elif (line[0] == 'disallow'):\n                if (state != 0):\n                    entry.rulelines.append(RuleLine(line[1], False))\n                    state = 2\n            elif (line[0] == 'allow'):\n                if (state != 0):\n                    entry.rulelines.append(RuleLine(line[1], True))\n                    state = 2\n    if (state == 2):\n        self._add_entry(entry)\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    new_class = type.__new__(cls, name, bases, attrs)\n    delegate_class = new_class.__delegate_class__\n    if delegate_class:\n        delegated_attrs = {\n            \n        }\n        for klass in reversed(inspect.getmro(delegate_class)):\n            delegated_attrs.update(klass.__dict__)\n        for (attrname, delegate_attr) in delegated_attrs.items():\n            if (attrname not in attrs):\n                if getattr(delegate_attr, 'is_async_method', False):\n                    sync_method = Sync(attrname, delegate_attr.has_write_concern)\n                    setattr(new_class, attrname, sync_method)\n                elif isinstance(delegate_attr, Unwrap):\n                    sync_method = Sync(attrname, delegate_attr.prop.has_write_concern)\n                    setattr(new_class, attrname, sync_method)\n                elif getattr(delegate_attr, 'is_motorcursor_chaining_method', False):\n                    wrapper = WrapOutgoing()\n                    wrapper.name = attrname\n                    setattr(new_class, attrname, wrapper)\n                elif isinstance(delegate_attr, ReadOnlyPropertyDescriptor):\n                    setattr(new_class, attrname, delegate_attr)\n    for (name, attr) in attrs.items():\n        if isinstance(attr, (MotorAttributeFactory, SynchroProperty, WrapOutgoing)):\n            attr.name = name\n    return new_class\n", "label": 1}
{"function": "\n\ndef __new__(mcs, cls_name, cls_bases, cls_attrs):\n    for (name, attr) in cls_attrs.items():\n        if (getattr(attr, '_unguarded', False) or (name in mcs.ALWAYS_UNGUARDED)):\n            continue\n        if name.startswith('_'):\n            continue\n        if isinstance(attr, type):\n            continue\n        is_property = isinstance(attr, property)\n        if (not (callable(attr) or is_property)):\n            continue\n        if is_property:\n            property_methods = defaultdict(None)\n            for fn_name in ('fdel', 'fset', 'fget'):\n                prop_fn = getattr(cls_attrs[name], fn_name, None)\n                if (prop_fn is not None):\n                    if getattr(prop_fn, '_unguarded', False):\n                        property_methods[fn_name] = prop_fn\n                    else:\n                        property_methods[fn_name] = pre_verify(prop_fn)\n            cls_attrs[name] = property(**property_methods)\n        else:\n            cls_attrs[name] = pre_verify(attr)\n    return super(_PageObjectMetaclass, mcs).__new__(mcs, cls_name, cls_bases, cls_attrs)\n", "label": 1}
{"function": "\n\ndef _get_desktop_streams(self, channel_id):\n    password = self.options.get('password')\n    channel = self._get_module_info('channel', channel_id, password, schema=_channel_schema)\n    if (not isinstance(channel.get('stream'), list)):\n        raise NoStreamsError(self.url)\n    streams = {\n        \n    }\n    for provider in channel['stream']:\n        if (provider['name'] == 'uhs_akamai'):\n            continue\n        provider_url = provider['url']\n        provider_name = provider['name']\n        for (stream_index, stream_info) in enumerate(provider['streams']):\n            stream = None\n            stream_height = int(stream_info.get('height', 0))\n            stream_name = stream_info.get('description')\n            if (not stream_name):\n                if (stream_height > 0):\n                    if (not stream_info.get('isTranscoded')):\n                        stream_name = '{0}p+'.format(stream_height)\n                    else:\n                        stream_name = '{0}p'.format(stream_height)\n                else:\n                    stream_name = 'live'\n            if (stream_name in streams):\n                provider_name_clean = provider_name.replace('uhs_', '')\n                stream_name += '_alt_{0}'.format(provider_name_clean)\n            if provider_name.startswith('uhs_'):\n                stream = UHSStream(self.session, channel_id, self.url, provider_name, stream_index, password)\n            elif provider_url.startswith('rtmp'):\n                playpath = stream_info['streamName']\n                stream = self._create_rtmp_stream(provider_url, playpath)\n            if stream:\n                streams[stream_name] = stream\n    return streams\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('Shaping')\n    if (self.rate is not None):\n        oprot.writeFieldBegin('rate', TType.I32, 1)\n        oprot.writeI32(self.rate)\n        oprot.writeFieldEnd()\n    if (self.delay is not None):\n        oprot.writeFieldBegin('delay', TType.STRUCT, 2)\n        self.delay.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.loss is not None):\n        oprot.writeFieldBegin('loss', TType.STRUCT, 3)\n        self.loss.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.reorder is not None):\n        oprot.writeFieldBegin('reorder', TType.STRUCT, 4)\n        self.reorder.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.corruption is not None):\n        oprot.writeFieldBegin('corruption', TType.STRUCT, 5)\n        self.corruption.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.iptables_options is not None):\n        oprot.writeFieldBegin('iptables_options', TType.LIST, 6)\n        oprot.writeListBegin(TType.STRING, len(self.iptables_options))\n        for iter6 in self.iptables_options:\n            oprot.writeString(iter6)\n        oprot.writeListEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef get_insert_pt(self, view):\n    sel = view.sel()\n    pt = sel[0].end()\n    content = view.substr(view.line(pt))\n    wrap_width = get_wrap_width(view)\n    if (len(content) <= wrap_width):\n        return None\n    if view.settings().get('auto_wrap_beyond_only', False):\n        if (view.rowcol(pt)[1] < wrap_width):\n            return None\n    default = ['\\\\[', '\\\\(', '\\\\{', ' ', '\\\\n']\n    if view.score_selector(pt, 'text.tex.latex'):\n        default = (['\\\\\\\\left\\\\\\\\.', '\\\\\\\\left.', '\\\\\\\\\\\\{'] + default)\n    break_chars = '|'.join(view.settings().get('auto_wrap_break_patterns', default))\n    results = re.finditer(break_chars, content)\n    indices = ([m.start(0) for m in results] + [len(content)])\n    index = next((x[0] for x in enumerate(indices) if (x[1] > wrap_width)))\n    if (view.settings().get('auto_wrap_break_long_word', True) and (index > 0)):\n        return (view.line(pt).begin() + indices[(index - 1)])\n    else:\n        if (index == (len(indices) - 1)):\n            return None\n        return (view.line(pt).begin() + indices[index])\n", "label": 1}
{"function": "\n\ndef initialize(self, context):\n    cmd_options = {\n        \n    }\n    if (context.device.get_sdk_version() >= 23):\n        if self.app_names:\n            cmd_options['-a'] = ','.join(self.app_names)\n        if self.buffer_size:\n            cmd_options['-b'] = self.buffer_size\n        if self.use_circular_buffer:\n            cmd_options['-c'] = None\n        if self.kernel_functions:\n            cmd_options['-k'] = ','.join(self.kernel_functions)\n        if self.ignore_signals:\n            cmd_options['-n'] = None\n        opt_string = ''.join(['{} {} '.format(name, (value or '')) for (name, value) in cmd_options.iteritems()])\n        self.start_cmd = 'atrace --async_start {} {}'.format(opt_string, ' '.join(self.categories))\n        self.output_file = os.path.join(self.device.working_directory, 'atrace.txt')\n        self.stop_cmd = 'atrace --async_stop {} > {}'.format(('-z' if self.compress_trace else ''), self.output_file)\n        available_categories = [cat.strip().split(' - ')[0] for cat in context.device.execute('atrace --list_categories').splitlines()]\n        for category in self.categories:\n            if (category not in available_categories):\n                raise ConfigError(\"Unknown category '{}'; Must be one of: {}\".format(category, available_categories))\n    else:\n        raise InstrumentError('Only android devices with an API level >= 23 can use systrace properly')\n", "label": 1}
{"function": "\n\ndef activate(self, test=False):\n    if (self.is_active and (not test)):\n        return True\n    logger.debug('Site activation started')\n    for dashboard in self.dashboards:\n        for report in dashboard.reports:\n            ct = ContentType.objects.get_for_model(report.model)\n            (report.object, created) = Report.objects.get_or_create(key=report.key, contenttype=ct)\n            if created:\n                logger.debug(('Reportobject for report %s created' % report.key))\n    register_settings = list(self.settings.keys())\n    for setting in Configuration.objects.all():\n        key = '.'.join((setting.app_label, setting.field_name))\n        if (key in self.settings):\n            if (not setting.active):\n                setting.active = True\n                setting.save()\n            register_settings.remove(key)\n        elif setting.active:\n            setting.active = False\n            setting.save()\n    if register_settings:\n        logger.debug('Need to register new settings')\n        for setting in register_settings:\n            (app, name) = setting.split('.', 1)\n            Configuration.objects.create(app_label=app, field_name=name)\n            logger.debug(('Registered setting %s' % setting))\n    self.is_active = True\n    logger.debug('Site is now active')\n    return True\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    a = self.msg\n    b = ''\n    if self.http_scheme:\n        b += ('%s://' % self.http_scheme)\n    if self.http_host:\n        b += self.http_host\n    if self.http_port:\n        b += (':%s' % self.http_port)\n    if self.http_path:\n        b += self.http_path\n    if self.http_query:\n        b += ('?%s' % self.http_query)\n    if self.http_status:\n        if b:\n            b = ('%s %s' % (b, self.http_status))\n        else:\n            b = str(self.http_status)\n    if self.http_reason:\n        if b:\n            b = ('%s %s' % (b, self.http_reason))\n        else:\n            b = ('- %s' % self.http_reason)\n    if self.http_device:\n        if b:\n            b = ('%s: device %s' % (b, self.http_device))\n        else:\n            b = ('device %s' % self.http_device)\n    if self.http_response_content:\n        if (len(self.http_response_content) <= 60):\n            b += ('   %s' % self.http_response_content)\n        else:\n            b += ('  [first 60 chars of response] %s' % self.http_response_content[:60])\n    return ((b and ('%s: %s' % (a, b))) or a)\n", "label": 1}
{"function": "\n\ndef __init__(self, parent=None, **traits):\n    super(TableEditorToolbar, self).__init__(**traits)\n    editor = self.editor\n    factory = editor.factory\n    actions = []\n    if (factory.sortable and (not factory.sort_model)):\n        actions.append(self.no_sort)\n    if ((not editor.in_column_mode) and factory.reorderable):\n        actions.append(self.move_up)\n        actions.append(self.move_down)\n    if (editor.in_row_mode and (factory.search is not None)):\n        actions.append(self.search)\n    if factory.editable:\n        if ((factory.row_factory is not None) and (not factory.auto_add)):\n            actions.append(self.add)\n        if ((factory.deletable != False) and (not editor.in_column_mode)):\n            actions.append(self.delete)\n    if factory.configurable:\n        actions.append(self.prefs)\n    if (len(actions) > 0):\n        toolbar = ToolBar(*actions, image_size=(16, 16), show_tool_names=False, show_divider=False)\n        self.control = toolbar.create_tool_bar(parent, self)\n        self.control.SetBackgroundColour(parent.GetBackgroundColour())\n        self.control.SetSize(wx.Size((23 * len(actions)), 16))\n", "label": 1}
{"function": "\n\ndef on_changed(self, which):\n    if (not hasattr(self, '_lpl')):\n        self.add_dterm('_lpl', maximum(multiply(a=multiply()), 0.0))\n    if (not hasattr(self, 'ldn')):\n        self.ldn = LightDotNormal((self.v.r.size / 3))\n    if (not hasattr(self, 'vn')):\n        logger.info('LambertianPointLight using auto-normals. This will be slow for derivative-free computations.')\n        self.vn = VertNormals(f=self.f, v=self.v)\n        self.vn.needs_autoupdate = True\n    if (('v' in which) and hasattr(self.vn, 'needs_autoupdate') and self.vn.needs_autoupdate):\n        self.vn.v = self.v\n    ldn_args = {k: getattr(self, k) for k in which if (k in ('light_pos', 'v', 'vn'))}\n    if (len(ldn_args) > 0):\n        self.ldn.set(**ldn_args)\n        self._lpl.a.a.a = self.ldn.reshape(((- 1), 1))\n    if (('num_verts' in which) or ('light_color' in which)):\n        self._lpl.a.a.b = self.light_color.reshape((1, self.num_channels))\n    if ('vc' in which):\n        self._lpl.a.b = self.vc.reshape(((- 1), self.num_channels))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.a = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.b = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_application_key_ != x.has_application_key_):\n        return 0\n    if (self.has_application_key_ and (self.application_key_ != x.application_key_)):\n        return 0\n    if (self.has_message_ != x.has_message_):\n        return 0\n    if (self.has_message_ and (self.message_ != x.message_)):\n        return 0\n    if (self.has_tag_ != x.has_tag_):\n        return 0\n    if (self.has_tag_ and (self.tag_ != x.tag_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef strChanger(s, a):\n    sList = list(s)\n    for x in a:\n        for i in range(len(s)):\n            if (((i + 1) < len(s)) and (((i + 1) % x) == 0)):\n                if ((ord(sList[(i + 1)]) < 122) and (ord(sList[(i + 1)]) >= 90)):\n                    sList[(i + 1)] = chr((ord(sList[(i + 1)]) + 1))\n                elif (ord(sList[(i + 1)]) == 122):\n                    sList[(i + 1)] = chr(97)\n                elif ((ord(sList[(i + 1)]) < 90) and (ord(sList[(i + 1)]) >= 65)):\n                    sList[(i + 1)] = chr((ord(sList[(i + 1)]) + 1))\n                elif (ord(sList[(i + 1)]) == 90):\n                    sList[(i + 1)] = chr(65)\n    newS = ''.join(sList)\n    return newS\n", "label": 1}
{"function": "\n\ndef print_help(self):\n    '\\n        Print the help menu.\\n        '\n    print(('\\n  %s %s' % ((self._title or self._name), (self._version or ''))))\n    if self._usage:\n        print(('\\n  %s' % self._usage))\n    else:\n        cmd = self._name\n        if (hasattr(self, '_parent') and isinstance(self._parent, Command)):\n            cmd = ('%s %s' % (self._parent._name, cmd))\n        if self._command_list:\n            usage = ('Usage: %s <command> [option]' % cmd)\n        else:\n            usage = ('Usage: %s [option]' % cmd)\n        pos = ' '.join([('<%s>' % name) for name in self._positional_list])\n        print(('\\n  %s %s' % (usage, pos)))\n    arglen = max((len(o.name) for o in self._option_list))\n    arglen += 2\n    self.print_title('\\n  Options:\\n')\n    for o in self._option_list:\n        print(('    %s %s' % (_pad(o.name, arglen), (o.description or ''))))\n    print('')\n    if self._command_list:\n        self.print_title('  Commands:\\n')\n        for cmd in self._command_list:\n            if isinstance(cmd, Command):\n                name = _pad(cmd._name, arglen)\n                desc = (cmd._description or '')\n                print(('    %s %s' % (_pad(name, arglen), desc)))\n        print('')\n    if self._help_footer:\n        print(self._help_footer)\n        print('')\n    return self\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    'Return the string value of the header.'\n    self._normalize()\n    uchunks = []\n    lastcs = None\n    lastspace = None\n    for (string, charset) in self._chunks:\n        nextcs = charset\n        if (nextcs == _charset.UNKNOWN8BIT):\n            original_bytes = string.encode('ascii', 'surrogateescape')\n            string = original_bytes.decode('ascii', 'replace')\n        if uchunks:\n            hasspace = (string and self._nonctext(string[0]))\n            if (lastcs not in (None, 'us-ascii')):\n                if ((nextcs in (None, 'us-ascii')) and (not hasspace)):\n                    uchunks.append(SPACE)\n                    nextcs = None\n            elif ((nextcs not in (None, 'us-ascii')) and (not lastspace)):\n                uchunks.append(SPACE)\n        lastspace = (string and self._nonctext(string[(- 1)]))\n        lastcs = nextcs\n        uchunks.append(string)\n    return EMPTYSTRING.join(uchunks)\n", "label": 1}
{"function": "\n\ndef __unicode__(self):\n    qs = ['SELECT']\n    if self.distinct_fields:\n        if self.count:\n            qs += ['DISTINCT COUNT({0})'.format(', '.join(['\"{0}\"'.format(f) for f in self.distinct_fields]))]\n        else:\n            qs += ['DISTINCT {0}'.format(', '.join(['\"{0}\"'.format(f) for f in self.distinct_fields]))]\n    elif self.count:\n        qs += ['COUNT(*)']\n    else:\n        qs += [(', '.join(['\"{0}\"'.format(f) for f in self.fields]) if self.fields else '*')]\n    qs += ['FROM', self.table]\n    if self.where_clauses:\n        qs += [self._where]\n    if (self.order_by and (not self.count)):\n        qs += ['ORDER BY {0}'.format(', '.join((six.text_type(o) for o in self.order_by)))]\n    if self.limit:\n        qs += ['LIMIT {0}'.format(self.limit)]\n    if self.allow_filtering:\n        qs += ['ALLOW FILTERING']\n    return ' '.join(qs)\n", "label": 1}
{"function": "\n\ndef _run_server_as_subprocess():\n    args = sys.argv[2:]\n    (options, remainder) = getopt.getopt(args, '', ['debug', 'reload', 'host=', 'port=', 'server=', 'baseurl=', 'runtimepath='])\n    host = 'localhost'\n    port = 8080\n    debug = True\n    reloader = False\n    server = 'kokoro'\n    runtime_path = '.runtime/'\n    base_url = '/'\n    for (opt, arg) in options:\n        if (opt[0:2] == '--'):\n            opt = opt[2:]\n        if (opt == 'debug'):\n            debug = True\n        elif (opt == 'reload'):\n            reloader = True\n        elif (opt == 'host'):\n            host = arg\n        elif (opt == 'port'):\n            port = arg\n        elif (opt == 'server'):\n            server = arg\n        elif (opt == 'runtimepath'):\n            runtime_path = arg\n        elif (opt == 'baseurl'):\n            base_url = arg\n    SCRIPT_PATH = os.path.abspath(__file__)\n    RUN_COMMAND = ('%s %s' % (sys.executable, SCRIPT_PATH))\n    ARGUMENTS = ('run_server_once --host=%s --port=%d --server=%s --baseurl=%s --runtimepath=%s' % (host, port, server, base_url, runtime_path))\n    if reloader:\n        ARGUMENTS += ' --reload'\n    if debug:\n        ARGUMENTS += ' --debug'\n    RUN_COMMAND = ((RUN_COMMAND + ' ') + ARGUMENTS)\n    if hasattr(os, 'setsid'):\n        return subprocess.Popen(RUN_COMMAND, shell=True, preexec_fn=os.setsid)\n    else:\n        return subprocess.Popen(RUN_COMMAND, shell=True)\n", "label": 1}
{"function": "\n\ndef _parse_binary(stream, ptr=0):\n    i = ptr\n    deserialized = {\n        \n    }\n    while (i < len(stream)):\n        c = stream[i]\n        if (c == BIN_END):\n            return (deserialized, i)\n        (nodename, i) = _readtonull(stream, (i + 1))\n        if (c == BIN_NONE):\n            (deserialized[nodename], i) = _parse_binary(stream, (i + 1))\n        elif (c == BIN_STRING):\n            (deserialized[nodename], i) = _readtonull(stream, (i + 1))\n        elif (c == BIN_WIDESTRING):\n            raise Exception('NYI')\n        elif ((c == BIN_INT32) or (c == BIN_COLOR) or (c == BIN_POINTER)):\n            if ((len(stream) - i) < 4):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('i', stream, (i + 1))\n            (deserialized[nodename], i) = (value, (i + 4))\n        elif (c == BIN_UINT64):\n            if ((len(stream) - i) < 8):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('q', stream, (i + 1))\n            (deserialized[nodename], i) = (value, (i + 8))\n        elif (c == BIN_FLOAT32):\n            if ((len(stream) - i) < 4):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('f', stream, (i + 1))\n            (deserialized[nodename], i) = (0, (i + 4))\n        else:\n            raise Exception('Unknown KV type')\n        i += 1\n    return (deserialized, i)\n", "label": 1}
{"function": "\n\ndef update(self):\n    'Get the latest state of the sensor.'\n    data = ecobee.NETWORK\n    data.update()\n    for sensor in data.ecobee.get_remote_sensors(self.index):\n        for item in sensor['capability']:\n            if ((item['type'] == self.type) and (self.type == 'temperature') and (self.sensor_name == sensor['name'])):\n                self._state = (float(item['value']) / 10)\n            elif ((item['type'] == self.type) and (self.type == 'humidity') and (self.sensor_name == sensor['name'])):\n                self._state = item['value']\n            elif ((item['type'] == self.type) and (self.type == 'occupancy') and (self.sensor_name == sensor['name'])):\n                self._state = item['value']\n", "label": 1}
{"function": "\n\ndef create_slug(self, model_instance, add):\n    if (not isinstance(self._populate_from, (list, tuple))):\n        self._populate_from = (self._populate_from,)\n    slug_field = model_instance._meta.get_field(self.attname)\n    if (add or self.overwrite):\n        slug_for_field = (lambda field: self.slugify_func(getattr(model_instance, field)))\n        slug = self.separator.join(map(slug_for_field, self._populate_from))\n        next = 2\n    else:\n        slug = getattr(model_instance, self.attname)\n        return slug\n    slug_len = slug_field.max_length\n    if slug_len:\n        slug = slug[:slug_len]\n    slug = self._slug_strip(slug)\n    original_slug = slug\n    if self.allow_duplicates:\n        return slug\n    queryset = self.get_queryset(model_instance.__class__, slug_field)\n    if model_instance.pk:\n        queryset = queryset.exclude(pk=model_instance.pk)\n    kwargs = {\n        \n    }\n    for params in model_instance._meta.unique_together:\n        if (self.attname in params):\n            for param in params:\n                kwargs[param] = getattr(model_instance, param, None)\n    kwargs[self.attname] = slug\n    while ((not slug) or queryset.filter(**kwargs)):\n        slug = original_slug\n        end = ('%s%s' % (self.separator, next))\n        end_len = len(end)\n        if (slug_len and ((len(slug) + end_len) > slug_len)):\n            slug = slug[:(slug_len - end_len)]\n            slug = self._slug_strip(slug)\n        slug = ('%s%s' % (slug, end))\n        kwargs[self.attname] = slug\n        next += 1\n    return slug\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_class_or_file_name_ != x.has_class_or_file_name_):\n        return 0\n    if (self.has_class_or_file_name_ and (self.class_or_file_name_ != x.class_or_file_name_)):\n        return 0\n    if (self.has_line_number_ != x.has_line_number_):\n        return 0\n    if (self.has_line_number_ and (self.line_number_ != x.line_number_)):\n        return 0\n    if (self.has_function_name_ != x.has_function_name_):\n        return 0\n    if (self.has_function_name_ and (self.function_name_ != x.function_name_)):\n        return 0\n    if (len(self.variables_) != len(x.variables_)):\n        return 0\n    for (e1, e2) in zip(self.variables_, x.variables_):\n        if (e1 != e2):\n            return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef user_defined_setup(config, inventory):\n    'Apply user defined entries from config into inventory.\\n\\n    :param config: ``dict``  User defined information\\n    :param inventory: ``dict``  Living dictionary of inventory\\n    '\n    hvs = inventory['_meta']['hostvars']\n    for (key, value) in config.iteritems():\n        if key.endswith('hosts'):\n            if (key not in inventory):\n                inventory[key] = {\n                    'hosts': [],\n                }\n            if (value is None):\n                return\n            for (_key, _value) in value.iteritems():\n                if (_key not in inventory['_meta']['hostvars']):\n                    inventory['_meta']['hostvars'][_key] = {\n                        \n                    }\n                hvs[_key].update({\n                    'ansible_ssh_host': _value['ip'],\n                    'container_address': _value['ip'],\n                    'is_metal': True,\n                    'physical_host_group': key,\n                })\n                properties = hvs[_key].get('properties')\n                if ((not properties) or (not isinstance(properties, dict))):\n                    hvs[_key]['properties'] = dict()\n                hvs[_key]['properties'].update({\n                    'is_metal': True,\n                })\n                if ('host_vars' in _value):\n                    for (_k, _v) in _value['host_vars'].items():\n                        hvs[_key][_k] = _v\n                append_if(array=USED_IPS, item=_value['ip'])\n                append_if(array=inventory[key]['hosts'], item=_key)\n", "label": 1}
{"function": "\n\ndef on_response(self, response):\n    if (not self.first_server_chunk_received):\n        self.first_server_chunk_received = True\n        if ((not self.first_client_chunk_received) and (response.startswith('220 ') or response.startswith('220-'))):\n            self.smtp_detected = True\n    if (not self.smtp_detected):\n        return response\n    if self.ehlo_response_pending:\n        self.ehlo_response_pending = False\n        if (not response.startswith('250-')):\n            return response\n        lines = [l.rstrip() for l in response.splitlines()]\n        starttls_line_index = (- 1)\n        for i in range(len(lines)):\n            line = lines[i]\n            if line[4:].lower().startswith('starttls'):\n                starttls_line_index = i\n                break\n        else:\n            self.smtp_detected = False\n            self.log(logging.DEBUG, 'No STARTTLS in EHLO response')\n            return response\n        if (starttls_line_index == (len(lines) - 1)):\n            lines = lines[:starttls_line_index]\n            lines[(- 1)] = ((lines[(- 1)][0:3] + ' ') + lines[(- 1)][4:])\n        else:\n            lines = (lines[:starttls_line_index] + lines[(starttls_line_index + 1):])\n        response = ('\\r\\n'.join(lines) + '\\r\\n')\n        self.server_starttls_stripped = True\n        self.log(logging.DEBUG, 'Stripped STARTTLS from EHLO response')\n    return response\n", "label": 1}
{"function": "\n\ndef root(self, request, url):\n    '\\n        DEPRECATED. This function is the old way of handling URL resolution, and\\n        is deprecated in favor of real URL resolution -- see ``get_urls()``.\\n\\n        This function still exists for backwards-compatibility; it will be\\n        removed in Django 1.3.\\n        '\n    import warnings\n    warnings.warn('AdminSite.root() is deprecated; use include(admin.site.urls) instead.', DeprecationWarning)\n    if ((request.method == 'GET') and (not request.path.endswith('/'))):\n        return http.HttpResponseRedirect((request.path + '/'))\n    if settings.DEBUG:\n        self.check_dependencies()\n    self.root_path = re.sub((re.escape(url) + '$'), '', request.path)\n    url = url.rstrip('/')\n    if (url == 'logout'):\n        return self.logout(request)\n    if (not self.has_permission(request)):\n        return self.login(request)\n    if (url == ''):\n        return self.index(request)\n    elif (url == 'password_change'):\n        return self.password_change(request)\n    elif (url == 'password_change/done'):\n        return self.password_change_done(request)\n    elif (url == 'jsi18n'):\n        return self.i18n_javascript(request)\n    elif url.startswith('r/'):\n        from django.contrib.contenttypes.views import shortcut\n        return shortcut(request, *url.split('/')[1:])\n    elif ('/' in url):\n        return self.model_page(request, *url.split('/', 2))\n    else:\n        return self.app_index(request, url)\n    raise http.Http404('The requested admin page does not exist.')\n", "label": 1}
{"function": "\n\ndef _do_setup(self, data):\n    added_magic = []\n    for line in data.split('\\n'):\n        if ((line == 'info/names=') or (line.strip() == '')):\n            continue\n        (name, documentation) = line.split(' ', 1)\n        if (name == 'config/*'):\n            continue\n        if name.endswith('/*'):\n            bits = name[:(- 2)].split('/')\n            takes_arg = True\n        else:\n            bits = name.split('/')\n            takes_arg = False\n        mine = self\n        for bit in bits[:(- 1)]:\n            bit = bit.replace('-', '_')\n            if (bit in mine.attrs):\n                mine = mine.attrs[bit]\n                if (not isinstance(mine, MagicContainer)):\n                    raise RuntimeError(('Already had something: %s for %s' % (bit, name)))\n            else:\n                c = MagicContainer(bit)\n                added_magic.append(c)\n                mine._add_attribute(bit, c)\n                mine = c\n        n = bits[(- 1)].replace('-', '_')\n        if (n in mine.attrs):\n            raise RuntimeError(('Already had something: %s for %s' % (n, name)))\n        mine._add_attribute(n, ConfigMethod('/'.join(bits), self.protocol, takes_arg))\n    for c in added_magic:\n        c._setup_complete()\n    return None\n", "label": 1}
{"function": "\n\ndef build(ctx):\n    if ctx.options.dump_state:\n        ctx.db.dump_database()\n        return 0\n    if ctx.options.delete_function:\n        if (not ctx.db.delete_function(ctx.options.delete_function)):\n            raise fbuild.Error(('function %r not cached' % ctx.options.delete_function))\n        return 0\n    if ctx.options.delete_file:\n        if (not ctx.db.delete_file(ctx.options.delete_file)):\n            raise fbuild.Error(('file %r not cached' % ctx.options.delete_file))\n        return 0\n    targets = (ctx.args or ['build'])\n    if ('install' in targets):\n        if (targets[(- 1)] != 'install'):\n            raise fbuild.Error('install must be last target')\n        if (not (set(targets) - {'configure', 'install'})):\n            targets.insert((targets.index('install') - 1), 'build')\n    for target_name in targets:\n        if (target_name == 'install'):\n            install_files(ctx)\n        else:\n            target = fbuild.target.find(target_name)\n            target.function(ctx)\n    return 0\n", "label": 1}
{"function": "\n\ndef formfield_for_dbfield(self, db_field, request, **kwargs):\n    \"\\n        Hook for specifying the form Field instance for a given database Field\\n        instance.\\n\\n        If kwargs are given, they're passed to the form Field's constructor.\\n        \"\n    if db_field.choices:\n        return self.formfield_for_choice_field(db_field, request, **kwargs)\n    if (db_field.many_to_many or isinstance(db_field, models.ForeignKey)):\n        if (db_field.__class__ in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[db_field.__class__], **kwargs)\n        if isinstance(db_field, models.ForeignKey):\n            formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)\n        elif db_field.many_to_many:\n            formfield = self.formfield_for_manytomany(db_field, request, **kwargs)\n        if (formfield and (db_field.name not in self.raw_id_fields)):\n            related_modeladmin = self.admin_site._registry.get(db_field.remote_field.model)\n            wrapper_kwargs = {\n                \n            }\n            if related_modeladmin:\n                wrapper_kwargs.update(can_add_related=related_modeladmin.has_add_permission(request), can_change_related=related_modeladmin.has_change_permission(request), can_delete_related=related_modeladmin.has_delete_permission(request))\n            formfield.widget = widgets.RelatedFieldWidgetWrapper(formfield.widget, db_field.remote_field, self.admin_site, **wrapper_kwargs)\n        return formfield\n    for klass in db_field.__class__.mro():\n        if (klass in self.formfield_overrides):\n            kwargs = dict(copy.deepcopy(self.formfield_overrides[klass]), **kwargs)\n            return db_field.formfield(**kwargs)\n    return db_field.formfield(**kwargs)\n", "label": 1}
{"function": "\n\ndef render_plugin(context, instance, placeholder, template, processors=None, current_app=None):\n    \"\\n    Renders a single plugin and applies the post processors to it's rendered\\n    content.\\n    \"\n    if current_app:\n        context['request'].current_app = current_app\n    if (not processors):\n        processors = []\n    if isinstance(template, six.string_types):\n        content = render_to_string(template, flatten_context(context))\n    elif (isinstance(template, Template) or (hasattr(template, 'template') and hasattr(template, 'render') and isinstance(template.template, Template))):\n        content = template.render(context)\n    else:\n        content = ''\n    for processor in iterload_objects(get_cms_setting('PLUGIN_PROCESSORS')):\n        content = processor(instance, placeholder, content, context)\n    for processor in processors:\n        content = processor(instance, placeholder, content, context)\n    for processor in DEFAULT_PLUGIN_PROCESSORS:\n        content = processor(instance, placeholder, content, context)\n    return content\n", "label": 1}
{"function": "\n\ndef pstream(self, stream, indent=0, multiline=False):\n    ' Pretty-formats the layout item to a stream.\\n        '\n    call = (self.__class__.__name__ + '(')\n    indent += len(call)\n    stream.write(call)\n    args = [(None, arg) for arg in self.pargs()]\n    traits = []\n    for (name, trait) in sorted(self.traits().iteritems()):\n        if ((not trait.pretty_skip) and (not trait.transient)):\n            value = getattr(self, name)\n            if (trait.default != value):\n                traits.append((name, value))\n    traits.sort()\n    args.extend(traits)\n    for (i, (name, value)) in enumerate(args):\n        arg_indent = indent\n        if name:\n            arg_indent += (len(name) + 1)\n            stream.write((name + '='))\n        if isinstance(value, LayoutItem):\n            value.pstream(stream, arg_indent, multiline)\n        else:\n            stream.write(repr(value))\n        if (i < (len(args) - 1)):\n            stream.write(',')\n            if multiline:\n                stream.write(('\\n' + (indent * ' ')))\n            else:\n                stream.write(' ')\n    stream.write(')')\n", "label": 1}
{"function": "\n\ndef geo_apps(namespace=True, runtests=False):\n    '\\n    Returns a list of GeoDjango test applications that reside in\\n    `django.contrib.gis.tests` that can be used with the current\\n    database and the spatial libraries that are installed.\\n    '\n    from django.db import connection\n    from django.contrib.gis.geos import GEOS_PREPARE\n    from django.contrib.gis.gdal import HAS_GDAL\n    apps = ['geoapp', 'relatedapp']\n    if (not connection.ops.mysql):\n        apps.append('distapp')\n    if (connection.ops.postgis and connection.ops.geography):\n        apps.append('geogapp')\n    if HAS_GDAL:\n        apps.extend(['geoadmin', 'layermap', 'inspectapp'])\n        if (connection.ops.postgis and GEOS_PREPARE):\n            apps.append('geo3d')\n    if runtests:\n        return [('django.contrib.gis.tests', app) for app in apps]\n    elif namespace:\n        return [('django.contrib.gis.tests.%s' % app) for app in apps]\n    else:\n        return apps\n", "label": 1}
{"function": "\n\ndef _build_http_request(url, method, headers=None, encoding=None, params=empty_params):\n    '\\n    Make an HTTP request and return an HTTP response.\\n    '\n    opts = {\n        'headers': (headers or {\n            \n        }),\n    }\n    if params.query:\n        opts['params'] = params.query\n    if ((params.body is not None) or params.data or params.files):\n        if (encoding == 'application/json'):\n            if (params.body is not None):\n                opts['json'] = params.body\n            else:\n                opts['json'] = params.data\n        elif (encoding == 'multipart/form-data'):\n            opts['data'] = params.data\n            opts['files'] = params.files\n        elif (encoding == 'application/x-www-form-urlencoded'):\n            opts['data'] = params.data\n        elif (encoding == 'application/octet-stream'):\n            opts['data'] = params.body\n            content_type = _get_content_type(params.body)\n            if content_type:\n                opts['headers']['content-type'] = content_type\n    request = requests.Request(method, url, **opts)\n    request = request.prepare()\n    return request\n", "label": 1}
{"function": "\n\ndef _process_dns_floatingip_update_precommit(self, context, floatingip_data):\n    plugin = manager.NeutronManager.get_service_plugins().get(service_constants.L3_ROUTER_NAT)\n    if (not utils.is_extension_supported(plugin, dns.Dns.get_alias())):\n        return\n    if (not self.dns_driver):\n        return\n    dns_data_db = context.session.query(FloatingIPDNS).filter_by(floatingip_id=floatingip_data['id']).one_or_none()\n    if (dns_data_db and dns_data_db['dns_name']):\n        return\n    (current_dns_name, current_dns_domain) = self._get_requested_state_for_external_dns_service_update(context, floatingip_data)\n    if dns_data_db:\n        if ((dns_data_db['published_dns_name'] != current_dns_name) or (dns_data_db['published_dns_domain'] != current_dns_domain)):\n            dns_actions_data = DNSActionsData(previous_dns_name=dns_data_db['published_dns_name'], previous_dns_domain=dns_data_db['published_dns_domain'])\n            if (current_dns_name and current_dns_domain):\n                dns_data_db['published_dns_name'] = current_dns_name\n                dns_data_db['published_dns_domain'] = current_dns_domain\n                dns_actions_data.current_dns_name = current_dns_name\n                dns_actions_data.current_dns_domain = current_dns_domain\n            else:\n                context.session.delete(dns_data_db)\n            return dns_actions_data\n        else:\n            return\n    if (current_dns_name and current_dns_domain):\n        context.session.add(FloatingIPDNS(floatingip_id=floatingip_data['id'], dns_name='', dns_domain='', published_dns_name=current_dns_name, published_dns_domain=current_dns_domain))\n        return DNSActionsData(current_dns_name=current_dns_name, current_dns_domain=current_dns_domain)\n", "label": 1}
{"function": "\n\ndef _generic_factor_list(expr, gens, args, method):\n    'Helper function for :func:`sqf_list` and :func:`factor_list`. '\n    options.allowed_flags(args, ['frac', 'polys'])\n    opt = options.build_options(gens, args)\n    expr = sympify(expr)\n    if (isinstance(expr, Expr) and (not expr.is_Relational)):\n        (numer, denom) = together(expr).as_numer_denom()\n        (cp, fp) = _symbolic_factor_list(numer, opt, method)\n        (cq, fq) = _symbolic_factor_list(denom, opt, method)\n        if (fq and (not opt.frac)):\n            raise PolynomialError(('a polynomial expected, got %s' % expr))\n        _opt = opt.clone(dict(expand=True))\n        for factors in (fp, fq):\n            for (i, (f, k)) in enumerate(factors):\n                if (not f.is_Poly):\n                    (f, _) = _poly_from_expr(f, _opt)\n                    factors[i] = (f, k)\n        fp = _sorted_factors(fp, method)\n        fq = _sorted_factors(fq, method)\n        if (not opt.polys):\n            fp = [(f.as_expr(), k) for (f, k) in fp]\n            fq = [(f.as_expr(), k) for (f, k) in fq]\n        coeff = (cp / cq)\n        if (not opt.frac):\n            return (coeff, fp)\n        else:\n            return (coeff, fp, fq)\n    else:\n        raise PolynomialError(('a polynomial expected, got %s' % expr))\n", "label": 1}
{"function": "\n\ndef _filtered_candidates(self, plugin, context, r_hd_binding_db):\n    candidates_dict = {c.id: c for c in self.get_candidates(plugin, context, r_hd_binding_db)}\n    if candidates_dict:\n        r_b = r_hd_binding_db.router.redundancy_binding\n        if r_b:\n            if r_b.user_router.hosting_info.hosting_device_id:\n                del candidates_dict[r_b.user_router.hosting_info.hosting_device_id]\n            for rr_b in r_b.user_router.redundancy_bindings:\n                rr = rr_b.redundancy_router\n                if ((rr.id != r_b.redundancy_router_id) and rr.hosting_info.hosting_device_id):\n                    del candidates_dict[rr.hosting_info.hosting_device_id]\n        elif (r_hd_binding_db.role == ROUTER_ROLE_HA_REDUNDANCY):\n            return []\n        for rr_b in r_hd_binding_db.router.redundancy_bindings:\n            rr = rr_b.redundancy_router\n            if rr.hosting_info.hosting_device_id:\n                del candidates_dict[rr.hosting_info.hosting_device_id]\n    return candidates_dict.values()\n", "label": 1}
{"function": "\n\n@classmethod\ndef target_info(cls, obj, ansi=False):\n    if isinstance(obj, type):\n        return ''\n    targets = obj.traverse(cls.get_target)\n    (elements, containers) = zip(*targets)\n    element_set = set((el for el in elements if (el is not None)))\n    container_set = set((c for c in containers if (c is not None)))\n    element_info = None\n    if (len(element_set) == 1):\n        element_info = ('Element: %s' % list(element_set)[0])\n    elif (len(element_set) > 1):\n        element_info = ('Elements:\\n   %s' % '\\n   '.join(sorted(element_set)))\n    container_info = None\n    if (len(container_set) == 1):\n        container_info = ('Container: %s' % list(container_set)[0])\n    elif (len(container_set) > 1):\n        container_info = ('Containers:\\n   %s' % '\\n   '.join(sorted(container_set)))\n    heading = cls.heading('Target Specifications', ansi=ansi, char='-')\n    target_header = '\\nTargets in this object available for customization:\\n'\n    if (element_info and container_info):\n        target_info = ('%s\\n\\n%s' % (element_info, container_info))\n    else:\n        target_info = (element_info if element_info else container_info)\n    target_footer = '\\nTo see the options info for one of these target specifications,\\nwhich are of the form {type}[.{group}[.{label}]], do holoviews.help({type}).'\n    return '\\n'.join([heading, target_header, target_info, target_footer])\n", "label": 1}
{"function": "\n\n@property\ndef type(self):\n    path = self.short_path\n    if any((path.startswith(prefix) for prefix in DOCKER_PREFIXES)):\n        return 'docker'\n    elif path.startswith('/lxc/'):\n        return 'lxc'\n    elif path.startswith('/user.slice/'):\n        (_, parent, name) = path.rsplit('/', 2)\n        if parent.endswith('.scope'):\n            if os.path.isdir(('/home/%s/.local/share/lxc/%s' % (self.owner, name))):\n                return 'lxc-user'\n        return 'systemd'\n    elif ((path == '/user.slice') or (path == '/system.slice') or path.startswith('/system.slice/')):\n        return 'systemd'\n    elif (regexp_ovz_container.match(path) and (path != '/0') and HAS_OPENVZ):\n        return 'openvz'\n    else:\n        return '-'\n", "label": 1}
{"function": "\n\ndef determine_result(self, returncode, returnsignal, output, isTimeout):\n    output = '\\n'.join(output)\n    status = result.RESULT_UNKNOWN\n    if self.allInText(['FALSE_DEREF'], output):\n        status = result.RESULT_FALSE_DEREF\n    elif self.allInText(['FALSE_FREE'], output):\n        status = result.RESULT_FALSE_FREE\n    elif self.allInText(['FALSE_MEMTRACK'], output):\n        status = result.RESULT_FALSE_MEMTRACK\n    elif self.allInText(['FALSE_OVERFLOW'], output):\n        status = result.RESULT_FALSE_OVERFLOW\n    elif self.allInText(['FALSE'], output):\n        status = result.RESULT_FALSE_REACH\n    elif ('TRUE' in output):\n        status = result.RESULT_TRUE_PROP\n    if (status == result.RESULT_UNKNOWN):\n        if isTimeout:\n            status = 'TIMEOUT'\n        elif output.endswith(('Z3 Error 9', 'Z3 Error 9\\n')):\n            status = 'ERROR (Z3 Error 9)'\n        elif output.endswith(('error', 'error\\n')):\n            status = 'ERROR'\n        elif ('Encountered Z3 conversion error:' in output):\n            status = 'ERROR (Z3 conversion error)'\n    return status\n", "label": 1}
{"function": "\n\ndef _load_00(b, classes):\n    identifier = b[0]\n    if isinstance(identifier, str):\n        identifier = ord(identifier)\n    if (identifier == _SPEC):\n        return _load_spec(b)\n    elif (identifier == _INT_32):\n        return _load_int_32(b)\n    elif ((identifier == _INT) or (identifier == _INT_NEG)):\n        return _load_int(b)\n    elif (identifier == _FLOAT):\n        return _load_float(b)\n    elif (identifier == _COMPLEX):\n        return _load_complex(b)\n    elif (identifier == _STR):\n        return _load_str(b)\n    elif (identifier == _BYTES):\n        return _load_bytes(b)\n    elif (identifier == _TUPLE):\n        return _load_tuple(b, classes)\n    elif (identifier == _NAMEDTUPLE):\n        return _load_namedtuple_00(b, classes)\n    elif (identifier == _LIST):\n        return _load_list(b, classes)\n    elif (identifier == _NPARRAY):\n        return _load_np_array(b)\n    elif (identifier == _DICT):\n        return _load_dict(b, classes)\n    elif (identifier == _GETSTATE):\n        return _load_getstate(b, classes)\n    else:\n        raise BFLoadError(\"unknown identifier '{}'\".format(hex(identifier)))\n", "label": 1}
{"function": "\n\ndef visit_delete(self, delete_stmt, **kw):\n    self.stack.append({\n        'correlate_froms': set([delete_stmt.table]),\n        'iswrapper': False,\n        'asfrom_froms': set([delete_stmt.table]),\n    })\n    self.isdelete = True\n    text = 'DELETE '\n    if delete_stmt._prefixes:\n        text += self._generate_prefixes(delete_stmt, delete_stmt._prefixes, **kw)\n    text += 'FROM '\n    table_text = delete_stmt.table._compiler_dispatch(self, asfrom=True, iscrud=True)\n    if delete_stmt._hints:\n        dialect_hints = dict([(table, hint_text) for ((table, dialect), hint_text) in delete_stmt._hints.items() if (dialect in ('*', self.dialect.name))])\n        if (delete_stmt.table in dialect_hints):\n            table_text = self.format_from_hint_text(table_text, delete_stmt.table, dialect_hints[delete_stmt.table], True)\n    else:\n        dialect_hints = None\n    text += table_text\n    if delete_stmt._returning:\n        self.returning = delete_stmt._returning\n        if self.returning_precedes_values:\n            text += (' ' + self.returning_clause(delete_stmt, delete_stmt._returning))\n    if (delete_stmt._whereclause is not None):\n        t = delete_stmt._whereclause._compiler_dispatch(self)\n        if t:\n            text += (' WHERE ' + t)\n    if (self.returning and (not self.returning_precedes_values)):\n        text += (' ' + self.returning_clause(delete_stmt, delete_stmt._returning))\n    self.stack.pop((- 1))\n    return text\n", "label": 1}
{"function": "\n\ndef _create_interfaces(self):\n    interfaces = []\n    type_interfaces = None\n    if isinstance(self.type_definition, RelationshipType):\n        if isinstance(self.entity_tpl, dict):\n            if (self.INTERFACES in self.entity_tpl):\n                type_interfaces = self.entity_tpl[self.INTERFACES]\n            else:\n                for (rel_def, value) in self.entity_tpl.items():\n                    if (rel_def != 'type'):\n                        rel_def = self.entity_tpl.get(rel_def)\n                        rel = None\n                        if isinstance(rel_def, dict):\n                            rel = rel_def.get('relationship')\n                        if rel:\n                            if (self.INTERFACES in rel):\n                                type_interfaces = rel[self.INTERFACES]\n                                break\n    else:\n        type_interfaces = self.type_definition.get_value(self.INTERFACES, self.entity_tpl)\n    if type_interfaces:\n        for (interface_type, value) in type_interfaces.items():\n            for (op, op_def) in value.items():\n                iface = InterfacesDef(self.type_definition, interfacetype=interface_type, node_template=self, name=op, value=op_def)\n                interfaces.append(iface)\n    return interfaces\n", "label": 1}
{"function": "\n\ndef dropMimeData(self, mime_data, action, row, column, parent):\n    ' Reimplemented to allow items to be moved.\\n        '\n    if (action == QtCore.Qt.IgnoreAction):\n        return False\n    data = mime_data.data(tabular_mime_type)\n    if ((not data.isNull()) and (action == QtCore.Qt.MoveAction)):\n        id_and_rows = map(int, str(data).split(' '))\n        table_id = id_and_rows[0]\n        if (table_id == id(self)):\n            current_rows = id_and_rows[1:]\n            self.moveRows(current_rows, parent.row())\n            return True\n    data = PyMimeData.coerce(mime_data).instance()\n    if (data is not None):\n        if (not isinstance(data, list)):\n            data = [data]\n        editor = self._editor\n        object = editor.object\n        name = editor.name\n        adapter = editor.adapter\n        if ((row == (- 1)) and parent.isValid()):\n            row = parent.row()\n        if ((row == (- 1)) and (adapter.len(object, name) == 0)):\n            row = 0\n        if all((adapter.get_can_drop(object, name, row, item) for item in data)):\n            for item in reversed(data):\n                self.dropItem(item, row)\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef parse_rule_list_any(self, rules, is_separator, parent, level, begin):\n    if ((self.printer is not None) and (not is_separator)):\n        self.printer(level, (('== Rule list once [' + str(len(rules))) + '] =='))\n    index = 0\n    for rule in rules:\n        index += 1\n        if ((self.printer is not None) and (not is_separator)):\n            self.printer(level, ((((('> Once [' + str(index)) + '/') + str(len(rules))) + '] ') + str(begin)))\n        parse_output = self.parse_rule(rule, is_separator, parent, (level + 1), begin)\n        if parse_output['successive_match']:\n            if ((self.printer is not None) and (not is_separator)):\n                self.printer(level, '> Once Success')\n            return parse_output\n    if ((self.printer is not None) and (not is_separator)):\n        self.printer(level, '> Once Failed')\n    return parse_output\n", "label": 1}
{"function": "\n\ndef show_list(self, connection, app_names=None):\n    '\\n        Shows a list of all migrations on the system, or only those of\\n        some named apps.\\n        '\n    loader = MigrationLoader(connection, ignore_no_migrations=True)\n    graph = loader.graph\n    if app_names:\n        invalid_apps = []\n        for app_name in app_names:\n            if (app_name not in loader.migrated_apps):\n                invalid_apps.append(app_name)\n        if invalid_apps:\n            raise CommandError(('No migrations present for: %s' % ', '.join(invalid_apps)))\n    else:\n        app_names = sorted(loader.migrated_apps)\n    for app_name in app_names:\n        self.stdout.write(app_name, self.style.MIGRATE_LABEL)\n        shown = set()\n        for node in graph.leaf_nodes(app_name):\n            for plan_node in graph.forwards_plan(node):\n                if ((plan_node not in shown) and (plan_node[0] == app_name)):\n                    title = plan_node[1]\n                    if graph.nodes[plan_node].replaces:\n                        title += (' (%s squashed migrations)' % len(graph.nodes[plan_node].replaces))\n                    if (plan_node in loader.applied_migrations):\n                        self.stdout.write((' [X] %s' % title))\n                    else:\n                        self.stdout.write((' [ ] %s' % title))\n                    shown.add(plan_node)\n        if (not shown):\n            self.stdout.write(' (no migrations)', self.style.ERROR)\n", "label": 1}
{"function": "\n\ndef finddirs(pattern, path='.', exclude=None, recursive=True):\n    'Find directories that match *pattern* in *path*'\n    import fnmatch\n    import os\n    if recursive:\n        for (root, dirnames, filenames) in os.walk(path):\n            for pat in _to_list(pattern):\n                for dirname in fnmatch.filter(dirnames, pat):\n                    dirpath = join(abspath(root), dirname)\n                    for excl in _to_list(exclude):\n                        if (excl and fnmatch.fnmatch(dirpath, excl)):\n                            break\n                    else:\n                        (yield dirpath)\n    else:\n        for pat in _to_list(pattern):\n            for dirname in fnmatch.filter(listdirs(path), pat):\n                dirpath = join(abspath(path), dirname)\n                for excl in _to_list(exclude):\n                    if (excl and fnmatch.fnmatch(dirpath, excl)):\n                        break\n                else:\n                    (yield dirpath)\n", "label": 1}
{"function": "\n\ndef compute_scores(self, sequence):\n    num_states = self.get_num_states()\n    length = len(sequence.x)\n    emission_scores = np.zeros([length, num_states])\n    initial_scores = np.zeros(num_states)\n    transition_scores = np.zeros([(length - 1), num_states, num_states])\n    final_scores = np.zeros(num_states)\n    for tag_id in xrange(num_states):\n        initial_features = self.feature_mapper.get_initial_features(sequence, tag_id)\n        score = 0.0\n        for feat_id in initial_features:\n            score += self.parameters[feat_id]\n        initial_scores[tag_id] = score\n    for pos in xrange(length):\n        for tag_id in xrange(num_states):\n            emission_features = self.feature_mapper.get_emission_features(sequence, pos, tag_id)\n            score = 0.0\n            for feat_id in emission_features:\n                score += self.parameters[feat_id]\n            emission_scores[(pos, tag_id)] = score\n        if (pos > 0):\n            for tag_id in xrange(num_states):\n                for prev_tag_id in xrange(num_states):\n                    transition_features = self.feature_mapper.get_transition_features(sequence, pos, tag_id, prev_tag_id)\n                    score = 0.0\n                    for feat_id in transition_features:\n                        score += self.parameters[feat_id]\n                    transition_scores[((pos - 1), tag_id, prev_tag_id)] = score\n    for prev_tag_id in xrange(num_states):\n        final_features = self.feature_mapper.get_final_features(sequence, prev_tag_id)\n        score = 0.0\n        for feat_id in final_features:\n            score += self.parameters[feat_id]\n        final_scores[prev_tag_id] = score\n    return (initial_scores, transition_scores, final_scores, emission_scores)\n", "label": 1}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (new_r not in self.shape_of):\n        self.init_r(new_r)\n    self.update_shape(new_r, r)\n    for (shpnode, idx) in (r.clients + [(node, i)]):\n        if isinstance(getattr(shpnode, 'op', None), Shape_i):\n            idx = shpnode.op.i\n            repl = self.shape_of[new_r][idx]\n            if (repl.owner is shpnode):\n                continue\n            if (repl.owner and (repl.owner.inputs[0] is shpnode.inputs[0]) and isinstance(repl.owner.op, Shape_i) and (repl.owner.op.i == shpnode.op.i)):\n                continue\n            if (shpnode.outputs[0] in theano.gof.graph.ancestors([repl])):\n                raise InconsistencyError(('This substitution would insert a cycle in the graph:node: %s, i: %i, r: %s, new_r: %s' % (node, i, r, new_r)))\n            self.scheduled[shpnode] = new_r\n    unscheduled = [k for (k, v) in self.scheduled.items() if (v == r)]\n    for k in unscheduled:\n        del self.scheduled[k]\n    for v in self.shape_of_reverse_index.get(r, []):\n        for (ii, svi) in enumerate(self.shape_of.get(v, [])):\n            if (svi == r):\n                self.set_shape_i(v, ii, new_r)\n    self.shape_of_reverse_index[r] = set()\n", "label": 1}
{"function": "\n\n@non_atomic_requests\ndef reporter(request):\n    query = request.GET.get('guid')\n    if query:\n        qs = None\n        if query.isdigit():\n            qs = Addon.with_unlisted.filter(id=query)\n        if (not qs):\n            qs = Addon.with_unlisted.filter(slug=query)\n        if (not qs):\n            qs = Addon.with_unlisted.filter(guid=query)\n        if ((not qs) and (len(query) > 4)):\n            qs = CompatReport.objects.filter(guid__startswith=query)\n        if qs:\n            guid = qs[0].guid\n            addon = Addon.with_unlisted.get(guid=guid)\n            if (addon.is_listed or owner_or_unlisted_reviewer(request, addon)):\n                return redirect('compat.reporter_detail', guid)\n    addons = (Addon.with_unlisted.filter(authors=request.user) if request.user.is_authenticated() else [])\n    return render(request, 'compat/reporter.html', dict(query=query, addons=addons))\n", "label": 1}
{"function": "\n\ndef get_index_dtype(arrays=(), maxval=None, check_contents=False):\n    '\\n    Based on input (integer) arrays `a`, determine a suitable index data\\n    type that can hold the data in the arrays.\\n\\n    Parameters\\n    ----------\\n    arrays : tuple of array_like\\n        Input arrays whose types/contents to check\\n    maxval : float, optional\\n        Maximum value needed\\n    check_contents : bool, optional\\n        Whether to check the values in the arrays and not just their types.\\n        Default: False (check only the types)\\n\\n    Returns\\n    -------\\n    dtype : dtype\\n        Suitable index data type (int32 or int64)\\n\\n    '\n    int32max = np.iinfo(np.int32).max\n    dtype = np.intc\n    if (maxval is not None):\n        if (maxval > int32max):\n            dtype = np.int64\n    if isinstance(arrays, np.ndarray):\n        arrays = (arrays,)\n    for arr in arrays:\n        arr = np.asarray(arr)\n        if (arr.dtype > np.int32):\n            if check_contents:\n                if (arr.size == 0):\n                    continue\n                elif np.issubdtype(arr.dtype, np.integer):\n                    maxval = arr.max()\n                    minval = arr.min()\n                    if ((minval >= np.iinfo(np.int32).min) and (maxval <= np.iinfo(np.int32).max)):\n                        continue\n            dtype = np.int64\n            break\n    return dtype\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    _name_list = [self._mock_new_name]\n    _parent = self._mock_new_parent\n    last = self\n    dot = '.'\n    if (_name_list == ['()']):\n        dot = ''\n    seen = set()\n    while (_parent is not None):\n        last = _parent\n        _name_list.append((_parent._mock_new_name + dot))\n        dot = '.'\n        if (_parent._mock_new_name == '()'):\n            dot = ''\n        _parent = _parent._mock_new_parent\n        if (id(_parent) in seen):\n            break\n        seen.add(id(_parent))\n    _name_list = list(reversed(_name_list))\n    _first = (last._mock_name or 'mock')\n    if (len(_name_list) > 1):\n        if (_name_list[1] not in ('()', '().')):\n            _first += '.'\n    _name_list[0] = _first\n    name = ''.join(_name_list)\n    name_string = ''\n    if (name not in ('mock', 'mock.')):\n        name_string = (' name=%r' % name)\n    spec_string = ''\n    if (self._spec_class is not None):\n        spec_string = ' spec=%r'\n        if self._spec_set:\n            spec_string = ' spec_set=%r'\n        spec_string = (spec_string % self._spec_class.__name__)\n    return (\"<%s%s%s id='%s'>\" % (type(self).__name__, name_string, spec_string, id(self)))\n", "label": 1}
{"function": "\n\ndef _rpm(b, r, manager, package, version, entry, pathname):\n    '\\n    Resolve dependencies on RPM-based systems.\\n    '\n    p = subprocess.Popen(['rpm', '-qf', pathname], close_fds=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    p.communicate()\n    if (0 == p.returncode):\n        return\n    if pattern_egg.search(entry):\n        p = subprocess.Popen(['rpm', '--qf=%{VERSION}-%{RELEASE}.%{ARCH}', '-q', 'python'], close_fds=True, stdout=subprocess.PIPE)\n        (stdout, stderr) = p.communicate()\n        if (0 != p.returncode):\n            return\n        versions = b.packages['yum']['python']\n        if (stdout not in versions):\n            versions.add(stdout)\n        if (not r.ignore_package('python', package)):\n            b.add_package('python', package, version)\n    elif (pattern_egginfo.search(entry) and os.path.exists(os.path.join(pathname, 'installed-files.txt'))):\n        p = subprocess.Popen(['rpm', '-q', 'python-pip'], close_fds=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        p.communicate()\n        if (0 != p.returncode):\n            if (not r.ignore_package('pip', package)):\n                b.add_package('pip', package, version)\n        elif (not r.ignore_package('python-pip', package)):\n            b.add_package('python-pip', package, version)\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw_args):\n    args = [sympify(x) for x in args if x]\n    args = TensAdd._tensAdd_flatten(args)\n    if (not args):\n        return S.Zero\n    if ((len(args) == 1) and (not isinstance(args[0], TensExpr))):\n        return args[0]\n    args = TensAdd._tensAdd_check_automatrix(args)\n    TensAdd._tensAdd_check(args)\n    if ((len(args) == 1) and isinstance(args[0], TensMul)):\n        obj = Basic.__new__(cls, *args, **kw_args)\n        return obj\n    args = [canon_bp(x) for x in args if x]\n    args = [x for x in args if x]\n    if (not args):\n        return S.Zero\n    if (len(args) == 1):\n        return args[0]\n\n    def sort_key(t):\n        x = get_tids(t)\n        return (x.components, x.free, x.dum)\n    args.sort(key=sort_key)\n    args = TensAdd._tensAdd_collect_terms(args)\n    if (not args):\n        return S.Zero\n    if (len(args) == 1):\n        return args[0]\n    obj = Basic.__new__(cls, *args, **kw_args)\n    return obj\n", "label": 1}
{"function": "\n\ndef _extend_string(self, type_, defaults, spec):\n    'Extend a string-type declaration with standard SQL CHARACTER SET /\\n        COLLATE annotations and MySQL specific extensions.\\n\\n        '\n\n    def attr(name):\n        return getattr(type_, name, defaults.get(name))\n    if attr('charset'):\n        charset = ('CHARACTER SET %s' % attr('charset'))\n    elif attr('ascii'):\n        charset = 'ASCII'\n    elif attr('unicode'):\n        charset = 'UNICODE'\n    else:\n        charset = None\n    if attr('collation'):\n        collation = ('COLLATE %s' % type_.collation)\n    elif attr('binary'):\n        collation = 'BINARY'\n    else:\n        collation = None\n    if attr('national'):\n        return ' '.join([c for c in ('NATIONAL', spec, collation) if (c is not None)])\n    return ' '.join([c for c in (spec, charset, collation) if (c is not None)])\n", "label": 1}
{"function": "\n\ndef _process(self):\n    '\\n        Coroutine implementing the key match algorithm. Key strokes are sent\\n        into this generator, and it calls the appropriate handlers.\\n        '\n    buffer = []\n    retry = False\n    while True:\n        if retry:\n            retry = False\n        else:\n            buffer.append((yield))\n        if buffer:\n            is_prefix_of_longer_match = self._is_prefix_of_longer_match(buffer)\n            matches = self._get_matches(buffer)\n            if (matches and matches[(- 1)].eager(self._cli_ref())):\n                is_prefix_of_longer_match = False\n            if ((not is_prefix_of_longer_match) and matches):\n                self._call_handler(matches[(- 1)], key_sequence=buffer)\n                buffer = []\n            elif ((not is_prefix_of_longer_match) and (not matches)):\n                retry = True\n                found = False\n                for i in range(len(buffer), 0, (- 1)):\n                    matches = self._get_matches(buffer[:i])\n                    if matches:\n                        self._call_handler(matches[(- 1)], key_sequence=buffer[:i])\n                        buffer = buffer[i:]\n                        found = True\n                if (not found):\n                    buffer = buffer[1:]\n", "label": 1}
{"function": "\n\ndef init_widget(self):\n    ' Initialize the underlying QWidget object.\\n\\n        '\n    super(QtWidget, self).init_widget()\n    d = self.declaration\n    if d.background:\n        self.set_background(d.background)\n    if d.foreground:\n        self.set_foreground(d.foreground)\n    if d.font:\n        self.set_font(d.font)\n    if (d.show_focus_rect is not None):\n        self.set_show_focus_rect(d.show_focus_rect)\n    if ((- 1) not in d.minimum_size):\n        self.set_minimum_size(d.minimum_size)\n    if ((- 1) not in d.maximum_size):\n        self.set_maximum_size(d.maximum_size)\n    if d.tool_tip:\n        self.set_tool_tip(d.tool_tip)\n    if d.status_tip:\n        self.set_status_tip(d.status_tip)\n    self.set_enabled(d.enabled)\n    if (self.widget.parent() or (not d.visible)):\n        self.set_visible(d.visible)\n", "label": 1}
{"function": "\n\ndef subparse(self, end_tokens=None):\n    body = []\n    data_buffer = []\n    add_data = data_buffer.append\n    if (end_tokens is not None):\n        self._end_token_stack.append(end_tokens)\n\n    def flush_data():\n        if data_buffer:\n            lineno = data_buffer[0].lineno\n            body.append(nodes.Output(data_buffer[:], lineno=lineno))\n            del data_buffer[:]\n    try:\n        while self.stream:\n            token = self.stream.current\n            if (token.type == 'data'):\n                if token.value:\n                    add_data(nodes.TemplateData(token.value, lineno=token.lineno))\n                next(self.stream)\n            elif (token.type == 'variable_begin'):\n                next(self.stream)\n                add_data(self.parse_tuple(with_condexpr=True))\n                self.stream.expect('variable_end')\n            elif (token.type == 'block_begin'):\n                flush_data()\n                next(self.stream)\n                if ((end_tokens is not None) and self.stream.current.test_any(*end_tokens)):\n                    return body\n                rv = self.parse_statement()\n                if isinstance(rv, list):\n                    body.extend(rv)\n                else:\n                    body.append(rv)\n                self.stream.expect('block_end')\n            else:\n                raise AssertionError('internal parsing error')\n        flush_data()\n    finally:\n        if (end_tokens is not None):\n            self._end_token_stack.pop()\n    return body\n", "label": 1}
{"function": "\n\ndef _pick_drop_channels(self, idx):\n    from ..io.base import _BaseRaw\n    from ..epochs import _BaseEpochs\n    from ..evoked import Evoked\n    from ..time_frequency import AverageTFR\n    if isinstance(self, (_BaseRaw, _BaseEpochs)):\n        if (not self.preload):\n            raise RuntimeError('If Raw or Epochs, data must be preloaded to drop or pick channels')\n\n    def inst_has(attr):\n        return (getattr(self, attr, None) is not None)\n    if inst_has('picks'):\n        self.picks = self.picks[idx]\n    if inst_has('_cals'):\n        self._cals = self._cals[idx]\n    pick_info(self.info, idx, copy=False)\n    if inst_has('_projector'):\n        self._projector = self._projector[idx][:, idx]\n    if (isinstance(self, _BaseRaw) and inst_has('_data')):\n        self._data = self._data.take(idx, axis=0)\n    elif (isinstance(self, _BaseEpochs) and inst_has('_data')):\n        self._data = self._data.take(idx, axis=1)\n    elif (isinstance(self, AverageTFR) and inst_has('data')):\n        self.data = self.data.take(idx, axis=0)\n    elif isinstance(self, Evoked):\n        self.data = self.data.take(idx, axis=0)\n", "label": 1}
{"function": "\n\ndef loop_iteration(self, timeout=60):\n    'A loop iteration - check any scheduled events\\n        and I/O available and run the handlers.\\n        '\n    (next_timeout, sources_handled) = self._call_timeout_handlers()\n    if self._quit:\n        return sources_handled\n    if (self._timeout is not None):\n        timeout = min(timeout, self._timeout)\n    if (next_timeout is not None):\n        timeout = min(next_timeout, timeout)\n    for handler in list(self._unprepared_handlers):\n        self._configure_io_handler(handler)\n    events = self.poll.poll(timeout)\n    self._timeout = None\n    for (fileno, event) in events:\n        if (event & select.POLLHUP):\n            self._handlers[fileno].handle_hup()\n        if (event & select.POLLNVAL):\n            self._handlers[fileno].handle_nval()\n        if (event & select.POLLIN):\n            self._handlers[fileno].handle_read()\n        elif (event & select.POLLERR):\n            self._handlers[fileno].handle_err()\n        if (event & select.POLLOUT):\n            self._handlers[fileno].handle_write()\n        sources_handled += 1\n        self._configure_io_handler(self._handlers[fileno])\n    return sources_handled\n", "label": 1}
{"function": "\n\n@local_optimizer([GpuFromHost, GpuToGpu, host_from_gpu])\ndef local_cut_gpu_transfers(node):\n    if (isinstance(node.op, GpuFromHost) and node.inputs[0].owner and isinstance(node.inputs[0].owner.op, HostFromGpu)):\n        other = node.inputs[0].owner.inputs[0]\n        if (node.op.context_name == other.type.context_name):\n            return [other]\n        else:\n            return [GpuToGpu(node.op.context_name)(other)]\n    elif (isinstance(node.op, HostFromGpu) and node.inputs[0].owner):\n        n2 = node.inputs[0].owner\n        if isinstance(n2.op, GpuFromHost):\n            return [n2.inputs[0]]\n        if isinstance(n2.op, GpuToGpu):\n            return [host_from_gpu(n2.inputs[0])]\n    elif isinstance(node.op, GpuToGpu):\n        if (node.inputs[0].type.context_name == node.op.context_name):\n            return [node.inputs[0]]\n        if node.inputs[0].owner:\n            n2 = node.inputs[0].owner\n            if isinstance(n2.op, GpuFromHost):\n                return [GpuFromHost(node.op.context_name)(n2.inputs[0])]\n            if isinstance(n2.op, GpuToGpu):\n                if (node.op.context_name == n2.inputs[0].type.context_name):\n                    return [n2.inputs[0]]\n                else:\n                    return [node.op(n2.inputs[0])]\n", "label": 1}
{"function": "\n\n@classmethod\ndef _resolve_conflict(cls, existing, proposed):\n    if (existing.rev is None):\n        return proposed\n    if (proposed.rev is None):\n        return existing\n    if (proposed == existing):\n        if proposed.force:\n            return proposed\n        return existing\n    elif (existing.force and proposed.force):\n        raise cls.IvyResolveConflictingDepsError('Cannot force {}#{};{} to both rev {} and {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n    elif existing.force:\n        logger.debug('Ignoring rev {} for {}#{};{} already forced to {}'.format(proposed.rev, proposed.org, proposed.name, (proposed.classifier or ''), existing.rev))\n        return existing\n    elif proposed.force:\n        logger.debug('Forcing {}#{};{} from {} to {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n        return proposed\n    elif (Revision.lenient(proposed.rev) > Revision.lenient(existing.rev)):\n        logger.debug('Upgrading {}#{};{} from rev {}  to {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n        return proposed\n    else:\n        return existing\n", "label": 1}
{"function": "\n\ndef tabulate(self, request_stats):\n    'Print review request summary and status in a table.\\n\\n        Args:\\n            request_stats (dict):\\n                A dict that contains statistics about each review request.\\n        '\n    if len(request_stats):\n        has_branches = False\n        has_bookmarks = False\n        table = tt.Texttable(get_terminal_size().columns)\n        header = ['Status', 'Review Request']\n        for request in request_stats:\n            if ('branch' in request):\n                has_branches = True\n            if ('bookmark' in request):\n                has_bookmarks = True\n        if has_branches:\n            header.append('Branch')\n        if has_bookmarks:\n            header.append('Bookmark')\n        table.header(header)\n        for request in request_stats:\n            row = [request['status'], request['summary']]\n            if has_branches:\n                row.append((request.get('branch') or ''))\n            if has_bookmarks:\n                row.append((request.get('bookmark') or ''))\n            table.add_row(row)\n        print(table.draw())\n    else:\n        print('No review requests found.')\n    print()\n", "label": 1}
{"function": "\n\ndef __init__(self, host='localhost', port=9200, http_auth=None, use_ssl=False, verify_certs=False, ca_certs=None, client_cert=None, client_key=None, **kwargs):\n    if (not REQUESTS_AVAILABLE):\n        raise ImproperlyConfigured('Please install requests to use RequestsHttpConnection.')\n    super(RequestsHttpConnection, self).__init__(host=host, port=port, **kwargs)\n    self.session = requests.session()\n    if (http_auth is not None):\n        if isinstance(http_auth, (tuple, list)):\n            http_auth = tuple(http_auth)\n        elif isinstance(http_auth, string_types):\n            http_auth = tuple(http_auth.split(':', 1))\n        self.session.auth = http_auth\n    self.base_url = ('http%s://%s:%d%s' % (('s' if use_ssl else ''), host, port, self.url_prefix))\n    self.session.verify = verify_certs\n    if (not client_key):\n        self.session.cert = client_cert\n    elif client_cert:\n        self.session.cert = (client_cert, client_key)\n    if ca_certs:\n        if (not verify_certs):\n            raise ImproperlyConfigured('You cannot pass CA certificates when verify SSL is off.')\n        self.session.verify = ca_certs\n    if (use_ssl and (not verify_certs)):\n        warnings.warn(('Connecting to %s using SSL with verify_certs=False is insecure.' % self.base_url))\n", "label": 1}
{"function": "\n\ndef confirmation(self, client, pdu):\n    if _debug:\n        UDPMultiplexer._debug('confirmation %r %r', client, pdu)\n    if (pdu.pduSource == self.addrTuple):\n        if _debug:\n            UDPMultiplexer._debug('    - from us!')\n        return\n    src = Address(pdu.pduSource)\n    if (client is self.direct):\n        dest = self.address\n    elif (client is self.broadcast):\n        dest = LocalBroadcast()\n    else:\n        raise RuntimeError('confirmation mismatch')\n    if (not pdu.pduData):\n        if _debug:\n            UDPMultiplexer._debug('    - no data')\n        return\n    msg_type = struct.unpack('B', pdu.pduData[:1])[0]\n    if _debug:\n        UDPMultiplexer._debug('    - msg_type: %r', msg_type)\n    if (msg_type == 1):\n        if self.annexH.serverPeer:\n            self.annexH.response(PDU(pdu, source=src, destination=dest))\n    elif (msg_type == 129):\n        if self.annexJ.serverPeer:\n            self.annexJ.response(PDU(pdu, source=src, destination=dest))\n    else:\n        UDPMultiplexer._warning('unsupported message')\n", "label": 1}
{"function": "\n\ndef cflags(static_cflags):\n    result = []\n    if (not OPTION_SHOW_WARNINGS):\n        result.append('-w')\n    if OPTION_DEBUG_GCC:\n        result.append('-g2')\n    if OPTION_STATIC:\n        if (not static_cflags):\n            static_cflags = env_var('CFLAGS')\n        result.extend(static_cflags)\n    else:\n        possible_cflags = flags('cflags')\n        for possible_cflag in possible_cflags:\n            if (not possible_cflag.startswith('-I')):\n                result.append(possible_cflag)\n    if (sys.platform in ('darwin',)):\n        for opt in result:\n            if ('flat_namespace' in opt):\n                break\n        else:\n            result.append('-flat_namespace')\n    return result\n", "label": 1}
{"function": "\n\ndef printHex(s, chunk=0, chunks=0, silent=False, separator='.'):\n    'prints elements of bytes string s in hex notation.\\n\\n       chunk is number of bytes per chunk\\n       0 means no chunking\\n       chunks is the number of chunks per line\\n       0 means no new lines\\n\\n       silent = True means return formatted string but do not print\\n    '\n    if (chunk < 0):\n        raise ValueError('invalid size of chunk')\n    if (chunks < 0):\n        raise ValueError('invalid chunks per line')\n    slen = len(s)\n    if (chunk == 0):\n        chunk = slen\n    if (chunks == 0):\n        line = slen\n    else:\n        line = (chunk * chunks)\n    cc = 0\n    ps = ''\n    for i in range(len(s)):\n        ps += ('%02x' % ord(s[i:(i + 1)]))\n        if (((i + 1) % line) and ((i + 1) % slen)):\n            if (not ((i + 1) % chunk)):\n                ps += ' '\n            else:\n                ps += separator\n        elif ((i + 1) != slen):\n            ps += '\\n'\n    if (not silent):\n        console.terse('{0}\\n'.format(ps))\n    return ps\n", "label": 1}
{"function": "\n\ndef get_parameters(self, packet_count=None):\n    '\\n        Returns the special tshark parameters to be used according to the configuration of this class.\\n        '\n    params = []\n    if self.display_filter:\n        params += [get_tshark_display_filter_flag(self.tshark_path), self.display_filter]\n    if packet_count:\n        params += ['-c', str(packet_count)]\n    if all(self.encryption):\n        params += ['-o', 'wlan.enable_decryption:TRUE', '-o', (((('uat:80211_keys:\"' + self.encryption[1]) + '\",\"') + self.encryption[0]) + '\"')]\n    if self.override_prefs:\n        for (preference_name, preference_value) in self.override_prefs.items():\n            if (all(self.encryption) and (preference_name in ('wlan.enable_decryption', 'uat:80211_keys'))):\n                continue\n            params += ['-o', '{0}:{1}'.format(preference_name, preference_value)]\n    if self.output_file:\n        params += ['-w', self.output_file]\n    if self.decode_as:\n        for (criterion, decode_as_proto) in self.decode_as.items():\n            params += ['-d', ','.join([criterion.strip(), decode_as_proto.strip()])]\n    return params\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.username = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.Pass = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.Remember = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef mpf_atan2(y, x, prec, rnd=round_fast):\n    (xsign, xman, xexp, xbc) = x\n    (ysign, yman, yexp, ybc) = y\n    if (not yman):\n        if ((y == fzero) and (x != fnan)):\n            if (mpf_sign(x) >= 0):\n                return fzero\n            return mpf_pi(prec, rnd)\n        if (y in (finf, fninf)):\n            if (x in (finf, fninf)):\n                return fnan\n            if (y == finf):\n                return mpf_shift(mpf_pi(prec, rnd), (- 1))\n            return mpf_neg(mpf_shift(mpf_pi(prec, negative_rnd[rnd]), (- 1)))\n        return fnan\n    if ysign:\n        return mpf_neg(mpf_atan2(mpf_neg(y), x, prec, negative_rnd[rnd]))\n    if (not xman):\n        if (x == fnan):\n            return fnan\n        if (x == finf):\n            return fzero\n        if (x == fninf):\n            return mpf_pi(prec, rnd)\n        if (y == fzero):\n            return fzero\n        return mpf_shift(mpf_pi(prec, rnd), (- 1))\n    tquo = mpf_atan(mpf_div(y, x, (prec + 4)), (prec + 4))\n    if xsign:\n        return mpf_add(mpf_pi((prec + 4)), tquo, prec, rnd)\n    else:\n        return mpf_pos(tquo, prec, rnd)\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    total_matched = sum(self.event2matched.values())\n    total_timeouts = sum(self.event2timeouts.values())\n    s = []\n    s.append(('Events matched: %d, timed out: %d\\n' % (total_matched, total_timeouts)))\n    s.append('Matches per event type:\\n')\n    for (e, count) in self.sorted_match_counts():\n        s.append(('  %s %d\\n' % (e, count)))\n        if (e == 'ControlMessageReceive'):\n            for (pkt_class, c) in self.msgrecv2matched.iteritems():\n                s.append(('\\t\\t  %s : %d\\n' % (pkt_class, c)))\n        if (e == 'ControlMessageSend'):\n            for (pkt_class, c) in self.msgsend2matched.iteritems():\n                s.append(('\\t\\t  %s : %d\\n' % (pkt_class, c)))\n    s.append('Timeouts per event type:\\n')\n    for (e, count) in self.sorted_timeout_counts():\n        s.append(('  %s %d\\n' % (e, count)))\n        if (e == 'ControlMessageReceive'):\n            for (pkt_class, c) in self.msgrecv2timeouts.iteritems():\n                s.append(('\\t\\t  %s : %d\\n' % (pkt_class, c)))\n        if (e == 'ControlMessageSend'):\n            for (pkt_class, c) in self.msgsend2timeouts.iteritems():\n                s.append(('\\t\\t  %s : %d\\n' % (pkt_class, c)))\n    return ''.join(s)\n", "label": 1}
{"function": "\n\ndef run_cmd(self, util, move_cmd, **kwargs):\n    view = self.view\n    selection = view.sel()\n    count = util.get_count(True)\n    if ('direction' in kwargs):\n        count *= kwargs['direction']\n    orig_cursors = [s for s in selection]\n    view.run_command(move_cmd, kwargs)\n    new_cursors = [s for s in selection]\n    selection.clear()\n    for (old, new) in zip(orig_cursors, new_cursors):\n        if (old < new):\n            selection.add(sublime.Region(old.begin(), new.end()))\n        else:\n            selection.add(sublime.Region(new.begin(), old.end()))\n    cursors = list(selection)\n    regions_overlap = False\n    for (i, c) in enumerate(cursors[1:]):\n        if cursors[i].contains(c.begin()):\n            regions_overlap = True\n            break\n    if regions_overlap:\n        selection.clear()\n        selection.add_all(orig_cursors)\n        return\n    regions = [view.substr(r) for r in view.sel()]\n    kill_ring.add(regions, forward=(count > 0), join=util.state.last_was_kill_cmd())\n    for region in selection:\n        view.erase(util.edit, region)\n", "label": 1}
{"function": "\n\ndef get_managed_entity(content, vimtype, moid=None, name=None):\n    if ((not name) and (not moid)):\n        return\n    container = content.viewManager.CreateContainerView(content.rootFolder, [vimtype], True)\n    count = 0\n    for entity in container.view:\n        if (moid and (entity._moId == moid)):\n            results = entity\n            count += 1\n        elif (name and (entity.name == name)):\n            results = entity\n            count += 1\n        if (count >= 2):\n            raise Exception('Multiple Managed Objects found,                            Check Names or IDs provided are unique')\n        elif (count == 1):\n            return results\n    if name:\n        raise Exception(('Inventory Error: Unable to Find Object (%s): %s' % (vimtype, name)))\n    elif moid:\n        raise Exception(('Inventory Error: Unable to Find Object (%s): %s' % (vimtype, moid)))\n    else:\n        raise Exception(('Inventory Error: No Name or moid provided (%s)' % vimtype))\n", "label": 1}
{"function": "\n\ndef create_decl(self, sigtypes, name, width=None, length=None, lineno=0):\n    self.typecheck_decl(sigtypes, length)\n    decls = []\n    signed = False\n    if ('signed' in sigtypes):\n        signed = True\n    if ('input' in sigtypes):\n        decls.append(Input(name=name, width=width, signed=signed, lineno=lineno))\n    if ('output' in sigtypes):\n        decls.append(Output(name=name, width=width, signed=signed, lineno=lineno))\n    if ('inout' in sigtypes):\n        decls.append(Inout(name=name, width=width, signed=signed, lineno=lineno))\n    if ('wire' in sigtypes):\n        if length:\n            decls.append(WireArray(name=name, width=width, signed=signed, length=length, lineno=lineno))\n        else:\n            decls.append(Wire(name=name, width=width, signed=signed, lineno=lineno))\n    if ('reg' in sigtypes):\n        if length:\n            decls.append(RegArray(name=name, width=width, signed=signed, length=length, lineno=lineno))\n        else:\n            decls.append(Reg(name=name, width=width, signed=signed, lineno=lineno))\n    if ('tri' in sigtypes):\n        decls.append(Tri(name=name, width=width, signed=signed, lineno=lineno))\n    if ('supply0' in sigtypes):\n        decls.append(Supply(name=name, value=IntConst('0', lineno=lineno), width=width, signed=signed, lineno=lineno))\n    if ('supply1' in sigtypes):\n        decls.append(Supply(name=name, value=IntConst('1', lineno=lineno), width=width, signed=signed, lineno=lineno))\n    return decls\n", "label": 1}
{"function": "\n\ndef files(self, itemId, sources=None):\n    ret = {\n        'added': [],\n        'removed': [],\n    }\n    files = self.get('item/{}/files'.format(itemId))\n    if (self.module.params['state'] == 'present'):\n        file_dict = {f['name']: f for f in files}\n        source_dict = {os.path.basename(s): {\n            'path': s,\n            'name': os.path.basename(s),\n            'size': os.path.getsize(s),\n        } for s in sources}\n        source_names = set([(s['name'], s['size']) for s in source_dict.values()])\n        file_names = set([(f['name'], f['size']) for f in file_dict.values()])\n        for (n, _) in (file_names - source_names):\n            self.delete('file/{}'.format(file_dict[n]['_id']))\n            ret['removed'].append(file_dict[n])\n        for (n, _) in (source_names - file_names):\n            self.uploadFileToItem(itemId, source_dict[n]['path'])\n            ret['added'].append(source_dict[n])\n    elif (self.module.params['state'] == 'absent'):\n        for f in files:\n            self.delete('file/{}'.format(f['_id']))\n            ret['removed'].append(f)\n    if ((len(ret['added']) != 0) or (len(ret['removed']) != 0)):\n        self.changed = True\n    return ret\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _encode_params(data):\n    'Encode parameters in a piece of data.\\n\\n        Will successfully encode parameters when passed as a dict or a list of\\n        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\\n        if parameters are supplied as a dict.\\n        '\n    if isinstance(data, (str, bytes)):\n        return data\n    elif hasattr(data, 'read'):\n        return data\n    elif hasattr(data, '__iter__'):\n        result = []\n        for (k, vs) in to_key_val_list(data):\n            if (isinstance(vs, basestring) or (not hasattr(vs, '__iter__'))):\n                vs = [vs]\n            for v in vs:\n                if (v is not None):\n                    result.append(((k.encode('utf-8') if isinstance(k, str) else k), (v.encode('utf-8') if isinstance(v, str) else v)))\n        return urlencode(result, doseq=True)\n    else:\n        return data\n", "label": 1}
{"function": "\n\ndef exportChildren(self, lwrite, level, namespace_='PDFFileObj:', name_='PDFTrailerType', fromsubclass_=False, pretty_print=True):\n    if pretty_print:\n        eol_ = '\\n'\n    else:\n        eol_ = ''\n    if (self.Size is not None):\n        self.Size.export(lwrite, level, 'PDFFileObj:', name_='Size', pretty_print=pretty_print)\n    if (self.Prev is not None):\n        self.Prev.export(lwrite, level, 'PDFFileObj:', name_='Prev', pretty_print=pretty_print)\n    if (self.Root is not None):\n        self.Root.export(lwrite, level, 'PDFFileObj:', name_='Root', pretty_print=pretty_print)\n    if (self.Encrypt is not None):\n        self.Encrypt.export(lwrite, level, 'PDFFileObj:', name_='Encrypt', pretty_print=pretty_print)\n    if (self.Info is not None):\n        self.Info.export(lwrite, level, 'PDFFileObj:', name_='Info', pretty_print=pretty_print)\n    if (self.ID is not None):\n        self.ID.export(lwrite, level, 'PDFFileObj:', name_='ID', pretty_print=pretty_print)\n    if (self.Last_Cross_Reference_Offset is not None):\n        self.Last_Cross_Reference_Offset.export(lwrite, level, 'PDFFileObj:', name_='Last_Cross_Reference_Offset', pretty_print=pretty_print)\n    if (self.Offset is not None):\n        self.Offset.export(lwrite, level, 'PDFFileObj:', name_='Offset', pretty_print=pretty_print)\n    if (self.Hashes is not None):\n        self.Hashes.export(lwrite, level, 'PDFFileObj:', name_='Hashes', pretty_print=pretty_print)\n", "label": 1}
{"function": "\n\ndef has_changes(self):\n    if self.is_dirty:\n        return True\n    for child in self._db_connections:\n        if child.has_changes():\n            return True\n    for child in self._db_annotations:\n        if child.has_changes():\n            return True\n    for child in self._db_abstractions:\n        if child.has_changes():\n            return True\n    for child in self._db_others:\n        if child.has_changes():\n            return True\n    for child in self._db_modules:\n        if child.has_changes():\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef __new__(cls, clsname, bases, methods):\n    attrs = []\n    nested = []\n    elements = []\n    namespaced = []\n    for (k, v) in methods.items():\n        if isinstance(v, Descriptor):\n            ns = getattr(v, 'namespace', None)\n            if ns:\n                namespaced.append((k, ('{%s}%s' % (ns, k))))\n            if getattr(v, 'nested', False):\n                nested.append(k)\n                elements.append(k)\n            elif isinstance(v, Sequence):\n                elements.append(k)\n            elif isinstance(v, Typed):\n                if hasattr(v.expected_type, 'to_tree'):\n                    elements.append(k)\n                else:\n                    attrs.append(k)\n            elif (not isinstance(v, Alias)):\n                attrs.append(k)\n    if (methods.get('__attrs__') is None):\n        methods['__attrs__'] = tuple(attrs)\n    methods['__namespaced__'] = tuple(namespaced)\n    if (methods.get('__nested__') is None):\n        methods['__nested__'] = tuple(sorted(nested))\n    if (methods.get('__elements__') is None):\n        methods['__elements__'] = tuple(sorted(elements))\n    return MetaStrict.__new__(cls, clsname, bases, methods)\n", "label": 1}
{"function": "\n\ndef visit_bindparam(self, bindparam, within_columns_clause=False, literal_binds=False, skip_bind_expression=False, **kwargs):\n    if ((not skip_bind_expression) and bindparam.type._has_bind_expression):\n        bind_expression = bindparam.type.bind_expression(bindparam)\n        return self.process(bind_expression, skip_bind_expression=True)\n    if (literal_binds or (within_columns_clause and self.ansi_bind_rules)):\n        if ((bindparam.value is None) and (bindparam.callable is None)):\n            raise exc.CompileError((\"Bind parameter '%s' without a renderable value not allowed here.\" % bindparam.key))\n        return self.render_literal_bindparam(bindparam, within_columns_clause=True, **kwargs)\n    name = self._truncate_bindparam(bindparam)\n    if (name in self.binds):\n        existing = self.binds[name]\n        if (existing is not bindparam):\n            if ((existing.unique or bindparam.unique) and (not existing.proxy_set.intersection(bindparam.proxy_set))):\n                raise exc.CompileError((\"Bind parameter '%s' conflicts with unique bind parameter of the same name\" % bindparam.key))\n            elif (existing._is_crud or bindparam._is_crud):\n                raise exc.CompileError((\"bindparam() name '%s' is reserved for automatic usage in the VALUES or SET clause of this insert/update statement.   Please use a name other than column name when using bindparam() with insert() or update() (for example, 'b_%s').\" % (bindparam.key, bindparam.key)))\n    self.binds[bindparam.key] = self.binds[name] = bindparam\n    return self.bindparam_string(name, **kwargs)\n", "label": 1}
{"function": "\n\ndef _event_filter_console_keypress(self, event):\n    ' Reimplemented for execution interruption and smart backspace.\\n        '\n    key = event.key()\n    if self._control_key_down(event.modifiers(), include_command=False):\n        if ((key == QtCore.Qt.Key_C) and self._executing):\n            self.interrupt_kernel()\n            return True\n        elif (key == QtCore.Qt.Key_Period):\n            message = 'Are you sure you want to restart the kernel?'\n            self.restart_kernel(message, now=False)\n            return True\n    elif (not (event.modifiers() & QtCore.Qt.AltModifier)):\n        if (key == QtCore.Qt.Key_Backspace):\n            col = self._get_input_buffer_cursor_column()\n            cursor = self._control.textCursor()\n            if ((col > 3) and (not cursor.hasSelection())):\n                text = self._get_input_buffer_cursor_line()[:col]\n                if (text.endswith('    ') and (not text.strip())):\n                    cursor.movePosition(QtGui.QTextCursor.Left, QtGui.QTextCursor.KeepAnchor, 4)\n                    cursor.removeSelectedText()\n                    return True\n    return super(FrontendWidget, self)._event_filter_console_keypress(event)\n", "label": 1}
{"function": "\n\ndef __parse_comment(self):\n    'Scan through a // or /* comment.'\n    comment_start_pos = self.__scanner.position\n    self.__scanner.read_ubyte()\n    if self.__scanner.at_end:\n        self.__error(\"Unexpected character '/'\")\n    c = self.__scanner.read_ubyte()\n    if (c == '/'):\n        while (not self.__scanner.at_end):\n            c = self.__scanner.read_ubyte()\n            if ((c == '\\n') or (c == '\\r')):\n                break\n        if ((c == '\\r') and (self.__scanner.peek_next_ubyte(none_if_bad_index=True) == '\\n')):\n            self.__scanner.read_ubyte()\n    elif (c == '*'):\n        while (not self.__scanner.at_end):\n            c = self.__scanner.read_ubyte()\n            if ((c == '*') and (self.__scanner.peek_next_ubyte(none_if_bad_index=True) == '/')):\n                self.__scanner.read_ubyte()\n                return\n        self.__error('Unterminated comment', comment_start_pos)\n    else:\n        self.__error((\"Unexpected character '/%s'\" % c))\n", "label": 1}
{"function": "\n\ndef handle_elt(self, elt, context):\n    titles = elt.findall('bibl/title')\n    title = []\n    if titles:\n        title = '\\n'.join((title.text.strip() for title in titles))\n    authors = elt.findall('bibl/author')\n    author = []\n    if authors:\n        author = '\\n'.join((author.text.strip() for author in authors))\n    dates = elt.findall('bibl/date')\n    date = []\n    if dates:\n        date = '\\n'.join((date.text.strip() for date in dates))\n    publishers = elt.findall('bibl/publisher')\n    publisher = []\n    if publishers:\n        publisher = '\\n'.join((publisher.text.strip() for publisher in publishers))\n    idnos = elt.findall('bibl/idno')\n    idno = []\n    if idnos:\n        idno = '\\n'.join((idno.text.strip() for idno in idnos))\n    notes = elt.findall('bibl/note')\n    note = []\n    if notes:\n        note = '\\n'.join((note.text.strip() for note in notes))\n    return {\n        'title': title,\n        'author': author,\n        'date': date,\n        'publisher': publisher,\n        'idno': idno,\n        'note': note,\n    }\n", "label": 1}
{"function": "\n\ndef pytest_cmdline_preparse(config, args):\n    if (('PYTEST_VERBOSE' in os.environ) and ('-v' not in args)):\n        args.insert(0, '-v')\n    if (('PYTEST_EXITFIRST' in os.environ) and ('-x' not in args)):\n        args.insert(0, '-x')\n    if (('PYTEST_NOCAPTURE' in os.environ) and ('-s' not in args)):\n        args.insert(0, '-s')\n    if (('PYTEST_TB' in os.environ) and (not any((('--tb' in a) for a in args)))):\n        args.insert(0, ('--tb=' + os.environ['PYTEST_TB']))\n    else:\n        args.insert(0, '--tb=short')\n    if (('PYTEST_NPROCS' in os.environ) and ('-n' not in args)):\n        args.insert(0, ('-n ' + os.environ['PYTEST_NPROCS']))\n    if (('PYTEST_WATCH' in os.environ) and ('-f' not in args)):\n        args.insert(0, '-f')\n    if ('PYTEST_LAZY' in os.environ):\n        args.insert(0, '--lazy')\n    if ('PYTEST_GREEDY' in os.environ):\n        args.insert(0, '--greedy')\n", "label": 1}
{"function": "\n\ndef fix_repeating_arguments(self):\n    'Fix elements that should accumulate/increment values.'\n    either = [list(c.children) for c in self.either.children]\n    for case in either:\n        for e in [c for c in case if (case.count(c) > 1)]:\n            if ((type(e) is Argument) or ((type(e) is Option) and e.argcount)):\n                if (e.value is None):\n                    e.value = []\n                elif (type(e.value) is not list):\n                    e.value = e.value.split()\n            if ((type(e) is Command) or ((type(e) is Option) and (e.argcount == 0))):\n                e.value = 0\n    return self\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_module_ != x.has_module_):\n        return 0\n    if (self.has_module_ and (self.module_ != x.module_)):\n        return 0\n    if (self.has_version_ != x.has_version_):\n        return 0\n    if (self.has_version_ and (self.version_ != x.version_)):\n        return 0\n    if (self.has_instances_ != x.has_instances_):\n        return 0\n    if (self.has_instances_ and (self.instances_ != x.instances_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = TransferStatus()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = InvalidOperation()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef handle_comma(self, token_text):\n    if self.flags.var_line:\n        if (self.is_expression(self.flags.mode) or (self.last_type == 'TK_END_BLOCK')):\n            self.flags.var_line_tainted = False\n        if self.flags.var_line:\n            self.flags.var_line_reindented = True\n        self.append_token(token_text)\n        if self.flags.var_line_tainted:\n            self.flags.var_line_tainted = False\n            self.append_newline()\n        else:\n            self.output_space_before_token = True\n        return\n    if ((self.last_type == 'TK_END_BLOCK') and (self.flags.mode != MODE.Expression)):\n        self.append_token(token_text)\n        if ((self.flags.mode == MODE.ObjectLiteral) and (self.flags.last_text == '}')):\n            self.append_newline()\n        else:\n            self.output_space_before_token = True\n    elif (self.flags.mode == MODE.ObjectLiteral):\n        self.append_token(token_text)\n        self.append_newline()\n    else:\n        self.append_token(token_text)\n        self.output_space_before_token = True\n", "label": 1}
{"function": "\n\ndef find_dependencies(module):\n    if (not CYTHON_INSTALLED):\n        return []\n    base_dir = get_base_dir()\n    package_dir = os.path.join(base_dir, PACKAGE_PATH)\n    includes_dir = os.path.join(base_dir, INCLUDE_PACKAGE_PATH)\n    pxd_files = [os.path.join(includes_dir, filename) for filename in os.listdir(includes_dir) if filename.endswith('.pxd')]\n    if ('etree' in module):\n        pxi_files = [os.path.join(PACKAGE_PATH, filename) for filename in os.listdir(package_dir) if (filename.endswith('.pxi') and ('objectpath' not in filename))]\n        pxd_files = [filename for filename in pxd_files if ('etreepublic' not in filename)]\n    elif ('objectify' in module):\n        pxi_files = [os.path.join(PACKAGE_PATH, 'objectpath.pxi')]\n    else:\n        pxi_files = []\n    return (pxd_files + pxi_files)\n", "label": 1}
{"function": "\n\ndef _complete(test_finder, thing):\n    if (':' in thing):\n        (module, test_part) = thing.split(':')\n        tests = list(test_finder.get_module_tests(module))\n        if ('.' in test_part):\n            return _get_prefixed(strings=tests, prefix=test_part)\n        funcs = [test for test in tests if (test.count('.') == 0)]\n        classes = [test.split('.')[0] for test in tests if ('.' in test)]\n        if (test_part in classes):\n            return ['.']\n        return _get_prefixed(strings=(funcs + classes), prefix=test_part)\n    if os.path.isdir(thing):\n        if ((thing != '.') and (not thing.endswith('/'))):\n            return ['/']\n        return _get_py_or_dirs(thing, '')\n    if os.path.exists(thing):\n        return [':']\n    (directory, file_part) = os.path.split(thing)\n    return _get_py_or_dirs(directory, file_part)\n", "label": 1}
{"function": "\n\ndef update_errors_in_view(view):\n    global g_color_configs, g_default_color\n    file_name = get_file_name_from_view(view)\n    if (file_name is None):\n        return\n    for (idx, config) in enumerate(g_color_configs):\n        region_key = (REGION_KEY_PREFIX + str(idx))\n        scope = (config['scope'] if ('scope' in config) else 'invalid')\n        icon = (config['icon'] if ('icon' in config) else '')\n        default_display = ('fill' if ('scope' in config) else 'none')\n        display = (config['display'] if ('display' in config) else default_display)\n        if g_show_errors:\n            regions = [e.get_region(view) for e in g_errors if ((e.file_name == file_name) and (e.color_index == idx))]\n            view.add_regions(region_key, regions, scope, icon, REGION_FLAGS[display])\n        else:\n            view.erase_regions(region_key)\n", "label": 1}
{"function": "\n\ndef __init__(self, param, cursor, strings_only=False):\n    if (settings.USE_TZ and (isinstance(param, datetime.datetime) and (not isinstance(param, Oracle_datetime)))):\n        if timezone.is_aware(param):\n            warnings.warn(\"The Oracle database adapter received an aware datetime (%s), probably from cursor.execute(). Update your code to pass a naive datetime in the database connection's time zone (UTC by default).\", RemovedInDjango20Warning)\n            param = param.astimezone(timezone.utc).replace(tzinfo=None)\n        param = Oracle_datetime.from_datetime(param)\n    if isinstance(param, datetime.timedelta):\n        param = duration_string(param)\n        if (' ' not in param):\n            param = ('0 ' + param)\n    string_size = 0\n    if (param is True):\n        param = 1\n    elif (param is False):\n        param = 0\n    if hasattr(param, 'bind_parameter'):\n        self.force_bytes = param.bind_parameter(cursor)\n    elif isinstance(param, Database.Binary):\n        self.force_bytes = param\n    else:\n        self.force_bytes = convert_unicode(param, cursor.charset, strings_only)\n        if isinstance(self.force_bytes, six.string_types):\n            string_size = len(force_bytes(param, cursor.charset, strings_only))\n    if hasattr(param, 'input_size'):\n        self.input_size = param.input_size\n    elif (string_size > 4000):\n        self.input_size = Database.CLOB\n    else:\n        self.input_size = None\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    if ((not len(args)) or options['list']):\n        page = options.get('page')\n        per_page = options.get('per-page')\n        ordered_by = [x.strip() for x in options.get('order').split(',')]\n        self.show_options_list(Option.objects.all().order_by(*ordered_by), page, per_page)\n        return\n    option_key = args[0].strip()\n    if options['value_to_add']:\n        (value, action) = self.read_value(options['value_to_add'], *args[1:], **options)\n        if (not add_option(option_key, value)):\n            raise CommandError((\"Cannot add '%s' option, already exists\" % option_key))\n        self.stdout.write(('Add %s: %s %s' % (option_key, value, action)))\n    elif options['value_to_update']:\n        (value, action) = self.read_value(options['value_to_update'], *args[1:], **options)\n        if (not update_option(option_key, value)):\n            raise CommandError((\"Cannot update '%s' option with '%s'\" % (option_key, value)))\n        self.stdout.write(('Update %s: %s %s' % (option_key, value, action)))\n    elif options['delete']:\n        if (len(args) > 1):\n            raise CommandError(('Too much arguments: %s' % args))\n        if (not delete_option(option_key)):\n            raise CommandError((\"Cannot delete '%s' option\" % option_key))\n        self.stdout.write(('Delete %s' % option_key))\n    else:\n        self.show_option_value(option_key)\n", "label": 1}
{"function": "\n\ndef __call__(self, form, field):\n    fdata = (field.data if isinstance(field.data, (list, tuple)) else [field.data])\n    for data in fdata:\n        if ((data is None) or ((self.min is not None) and (data < self.min)) or ((self.max is not None) and (data > self.max))):\n            message = self.message\n            if (message is None):\n                if (self.max is None):\n                    message = field.gettext('Number %(data)s must be at least %(min)s.')\n                elif (self.min is None):\n                    message = field.gettext('Number %(data)s must be at most %(max)s.')\n                else:\n                    message = field.gettext('Number %(data)s must be between %(min)s and %(max)s.')\n            raise validators.ValidationError((message % dict(data=data, min=self.min, max=self.max)))\n", "label": 1}
{"function": "\n\ndef _update_checksum(self, checksum, checksum_keyword='CHECKSUM', datasum_keyword='DATASUM'):\n    \"Update the 'CHECKSUM' and 'DATASUM' keywords in the header (or\\n        keywords with equivalent semantics given by the ``checksum_keyword``\\n        and ``datasum_keyword`` arguments--see for example ``CompImageHDU``\\n        for an example of why this might need to be overridden).\\n        \"\n    modified = (self._header._modified or self._data_loaded)\n    if (checksum == 'remove'):\n        if (checksum_keyword in self._header):\n            del self._header[checksum_keyword]\n        if (datasum_keyword in self._header):\n            del self._header[datasum_keyword]\n    elif (modified or self._new or (checksum and (('CHECKSUM' not in self._header) or ('DATASUM' not in self._header)))):\n        if (checksum == 'datasum'):\n            self.add_datasum(datasum_keyword=datasum_keyword)\n        elif (checksum == 'nonstandard_datasum'):\n            self.add_datasum(blocking='nonstandard', datasum_keyword=datasum_keyword)\n        elif (checksum == 'test'):\n            self.add_datasum(self._datasum_comment, datasum_keyword=datasum_keyword)\n            self.add_checksum(self._checksum_comment, True, checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n        elif (checksum == 'nonstandard'):\n            self.add_checksum(blocking='nonstandard', checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n        elif checksum:\n            self.add_checksum(blocking='standard', checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n", "label": 1}
{"function": "\n\ndef attach_foreignkeys(*object_sets, **kwargs):\n    \"\\n    Shortcut method which handles a pythonic LEFT OUTER JOIN. Allows you to attach the same object type\\n    to multiple different sets of data.\\n\\n    ``attach_foreignkeys((posts, Post.author), (threads, Thread.creator), related=['profile'])``\\n\\n    Works with only ForeignKeys\\n    \"\n    related = kwargs.get('related', [])\n    database = kwargs.get('database', 'default')\n    values = set()\n    model = None\n    for (objects, field) in object_sets:\n        if (not model):\n            model = field.field.rel.to\n        elif (model != field.field.rel.to):\n            raise ValueError(('You cannot attach foreign keys that do not reference the same models (%s != %s)' % (model, field.field.rel.to)))\n        values.update(distinct((getattr(o, field.field.column) for o in objects if (related or (getattr(o, ('_%s_cache' % field.field.name), False) is False)))))\n    if (not values):\n        return\n    qs = model.objects.filter(pk__in=values).using(database)\n    if related:\n        qs = qs.select_related(*related)\n    queryset = queryset_to_dict(qs)\n    for (objects, field) in object_sets:\n        for o in objects:\n            setattr(o, ('_%s_cache' % field.field.name), queryset.get(getattr(o, field.field.column)))\n", "label": 1}
{"function": "\n\ndef skip(inbuf, ftype):\n    if ((ftype == TType.BOOL) or (ftype == TType.BYTE)):\n        inbuf.read(1)\n    elif (ftype == TType.I16):\n        inbuf.read(2)\n    elif (ftype == TType.I32):\n        inbuf.read(4)\n    elif (ftype == TType.I64):\n        inbuf.read(8)\n    elif (ftype == TType.DOUBLE):\n        inbuf.read(8)\n    elif (ftype == TType.STRING):\n        inbuf.read(unpack_i32(inbuf.read(4)))\n    elif ((ftype == TType.SET) or (ftype == TType.LIST)):\n        (v_type, sz) = read_list_begin(inbuf)\n        for i in range(sz):\n            skip(inbuf, v_type)\n    elif (ftype == TType.MAP):\n        (k_type, v_type, sz) = read_map_begin(inbuf)\n        for i in range(sz):\n            skip(inbuf, k_type)\n            skip(inbuf, v_type)\n    elif (ftype == TType.STRUCT):\n        while True:\n            (f_type, fid) = read_field_begin(inbuf)\n            if (f_type == TType.STOP):\n                break\n            skip(inbuf, f_type)\n", "label": 1}
{"function": "\n\n@register_opt('fast_compile')\n@local_optimizer([PdbBreakpoint])\ndef local_gpu_pdbbreakpoint_op(node):\n    if isinstance(node.op, PdbBreakpoint):\n        old_inputs = node.inputs\n        old_outputs = node.outputs\n        new_inputs = node.inputs[:1]\n        input_transfered = []\n        nb_monitored_vars = len(node.outputs)\n        for i in range(nb_monitored_vars):\n            inp = old_inputs[(i + 1)]\n            out = old_outputs[i]\n            input_is_from_gpu = (inp.owner and isinstance(inp.owner.op, HostFromGpu))\n            output_goes_to_gpu = False\n            for c in out.clients:\n                if (c == 'output'):\n                    continue\n                if isinstance(c[0].op, GpuFromHost):\n                    output_goes_to_gpu = True\n                    context_name = c[0].op.context_name\n                    break\n            if input_is_from_gpu:\n                new_inputs.append(inp.owner.inputs[0])\n                input_transfered.append(True)\n            elif output_goes_to_gpu:\n                new_inputs.append(GpuFromHost(context_name)(inp))\n                input_transfered.append(True)\n            else:\n                new_inputs.append(inp)\n                input_transfered.append(False)\n        if (not any(input_transfered)):\n            return False\n        new_op_outputs = node.op(*new_inputs, return_list=True)\n        new_outputs = []\n        for i in range(len(new_op_outputs)):\n            if input_transfered[i]:\n                new_outputs.append(host_from_gpu(new_op_outputs[i]))\n            else:\n                new_outputs.append(new_op_outputs[i])\n        return new_outputs\n    return False\n", "label": 1}
{"function": "\n\ndef iterator(self):\n    fields = self.original_fields\n    if hasattr(self, 'aggregate_names'):\n        fields += tuple((f for f in self.aggregate_names if (f not in fields)))\n    if hasattr(self, 'annotation_names'):\n        fields += tuple((f for f in self.annotation_names if (f not in fields)))\n    for row in super(FallbackValuesListQuerySet, self).iterator():\n        if (self.flat and (len(fields) == 1)):\n            (yield row[fields[0]])\n        else:\n            (yield tuple((row[f] for f in fields)))\n", "label": 1}
{"function": "\n\ndef unpack_file(filename, location, content_type, link):\n    if ((content_type == 'application/zip') or filename.endswith('.zip') or filename.endswith('.pybundle') or zipfile.is_zipfile(filename)):\n        unzip_file(filename, location, flatten=(not filename.endswith('.pybundle')))\n    elif ((content_type == 'application/x-gzip') or tarfile.is_tarfile(filename) or (splitext(filename)[1].lower() in ('.tar', '.tar.gz', '.tar.bz2', '.tgz', '.tbz'))):\n        untar_file(filename, location)\n    elif (content_type and content_type.startswith('text/html') and is_svn_page(file_contents(filename))):\n        from pip.vcs.subversion import Subversion\n        Subversion(('svn+' + link.url)).unpack(location)\n    else:\n        logger.fatal(('Cannot unpack file %s (downloaded from %s, content-type: %s); cannot detect archive format' % (filename, location, content_type)))\n        raise InstallationError(('Cannot determine archive format of %s' % location))\n", "label": 1}
{"function": "\n\ndef _complete_dispatcher(self, dt):\n    'This method is scheduled on all touch up events. It will dispatch\\n        the `on_gesture_complete` event for all completed gestures, and remove\\n        merged gestures from the internal gesture list.'\n    need_cleanup = False\n    gest = self._gestures\n    timeout = self.draw_timeout\n    twin = self.temporal_window\n    get_time = Clock.get_time\n    for (idx, g) in enumerate(gest):\n        if g.was_merged:\n            del gest[idx]\n            continue\n        if ((not g.active) or (g.active_strokes != 0)):\n            continue\n        t1 = (g._update_time + twin)\n        t2 = (get_time() + UNDERSHOOT_MARGIN)\n        if ((not g.accept_stroke()) or (t1 <= t2)):\n            discard = False\n            if ((g.width < 5) and (g.height < 5)):\n                discard = True\n            elif g.single_points_test():\n                discard = True\n            need_cleanup = True\n            g.active = False\n            g._cleanup_time = (get_time() + timeout)\n            if discard:\n                self.dispatch('on_gesture_discard', g)\n            else:\n                self.dispatch('on_gesture_complete', g)\n    if need_cleanup:\n        Clock.schedule_once(self._cleanup, timeout)\n", "label": 1}
{"function": "\n\ndef console_output(self, targets):\n    concrete_targets = set()\n    for target in targets:\n        concrete_target = target.concrete_derived_from\n        concrete_targets.add(concrete_target)\n        if isinstance(concrete_target, ScalaLibrary):\n            concrete_targets.update(concrete_target.java_sources)\n    buildroot = get_buildroot()\n    files = set()\n    output_globs = self.get_options().globs\n    concrete_targets = set([target for target in concrete_targets if (not target.is_synthetic)])\n    for target in concrete_targets:\n        files.add(target.address.build_file.full_path)\n        if (output_globs or target.has_sources()):\n            if output_globs:\n                globs_obj = target.globs_relative_to_buildroot()\n                if globs_obj:\n                    files.update((os.path.join(buildroot, src) for src in globs_obj['globs']))\n            else:\n                files.update((os.path.join(buildroot, src) for src in target.sources_relative_to_buildroot()))\n        if (isinstance(target, JvmApp) and (not output_globs)):\n            files.update(itertools.chain(*[bundle.filemap.keys() for bundle in target.bundles]))\n    return files\n", "label": 1}
{"function": "\n\ndef list_with_base_rules(rawrules):\n    'Combine the list of rules set by the user with the default push rules\\n\\n    :param list rawrules: The rules the user has modified or set.\\n    :returns: A new list with the rules set by the user combined with the\\n        defaults.\\n    '\n    ruleslist = []\n    modified_base_rules = {r['rule_id']: r for r in rawrules if (r['priority_class'] < 0)}\n    rawrules = [r for r in rawrules if (r['priority_class'] >= 0)]\n    current_prio_class = PRIORITY_CLASS_INVERSE_MAP.keys()[(- 1)]\n    ruleslist.extend(make_base_prepend_rules(PRIORITY_CLASS_INVERSE_MAP[current_prio_class], modified_base_rules))\n    for r in rawrules:\n        if (r['priority_class'] < current_prio_class):\n            while (r['priority_class'] < current_prio_class):\n                ruleslist.extend(make_base_append_rules(PRIORITY_CLASS_INVERSE_MAP[current_prio_class], modified_base_rules))\n                current_prio_class -= 1\n                if (current_prio_class > 0):\n                    ruleslist.extend(make_base_prepend_rules(PRIORITY_CLASS_INVERSE_MAP[current_prio_class], modified_base_rules))\n        ruleslist.append(r)\n    while (current_prio_class > 0):\n        ruleslist.extend(make_base_append_rules(PRIORITY_CLASS_INVERSE_MAP[current_prio_class], modified_base_rules))\n        current_prio_class -= 1\n        if (current_prio_class > 0):\n            ruleslist.extend(make_base_prepend_rules(PRIORITY_CLASS_INVERSE_MAP[current_prio_class], modified_base_rules))\n    return ruleslist\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_reminder(reminder):\n    data = {\n        'type': reminder.type.key,\n        'notifications': [],\n    }\n    if reminder.valid_until:\n        for part in ['month', 'day', 'year']:\n            data[('valid_until/%s' % part)] = getattr(reminder.valid_until, part, None)\n    if (reminder.type == 'BAL'):\n        data['balance_threshold'] = ('%.2f' % round(reminder.threshold, 2))\n    elif (reminder.type == 'RIDE'):\n        data['ride_threshold'] = reminder.threshold\n    elif (reminder.type == 'ROUND_TRIP'):\n        data['round_trip_threshold'] = reminder.threshold\n    elif (reminder.type == 'EXP'):\n        data['exp_threshold'] = reminder.threshold\n        data['exp_quantity'] = reminder.quantifier\n    if reminder.send_sms:\n        data['notifications'].append('SMS')\n    if reminder.send_email:\n        data['notifications'].append('EMAIL')\n    for key in data.copy():\n        if (data[key] is None):\n            del data[key]\n    return ReminderForm(MultiDict(data))\n", "label": 1}
{"function": "\n\ndef search_article(keyword, directory, datadir, exclude):\n    '\\n    Search for a keyword in every article within your current directory and\\n    below. Much like recursive grep.\\n    '\n    c = 0\n    r = re.compile(keyword)\n    print('Articles:')\n    for (dirpath, dirs, files) in os.walk(directory):\n        dirs[:] = [d for d in dirs if (d not in exclude)]\n        for fname in files:\n            path = os.path.join(dirpath, fname)\n            if (r.search(path) is not None):\n                print(('* \\x1b[92m%s\\x1b[39m' % os.path.relpath(path, datadir)))\n                c = (c + 1)\n    print('Content:')\n    for (dirpath, dirs, files) in os.walk(directory):\n        dirs[:] = [d for d in dirs if (d not in exclude)]\n        for fname in files:\n            path = os.path.join(dirpath, fname)\n            f = open(path, 'rt')\n            for (i, line) in enumerate(f):\n                if r.search(line):\n                    c = (c + 1)\n                    print(('* \\x1b[92m%s\\x1b[39m: %s' % (os.path.relpath(path, datadir), line.rstrip('\\n'))))\n    return ('Results: %s' % c)\n", "label": 1}
{"function": "\n\ndef _convert_names_to_rpm(self, python_names, only_name):\n    if (not python_names):\n        return ({\n            \n        }, {\n            \n        })\n    cmdline = ((self._start_cmdline() + ['--convert']) + python_names)\n    result = collections.defaultdict(set)\n    conflicts = collections.defaultdict(set)\n    current_source = None\n    for line in sh.execute(cmdline)[0].splitlines():\n        if line.startswith('Requires:'):\n            line = line[len('Requires:'):]\n            if only_name:\n                positions = [line.find('>'), line.find('<'), line.find('=')]\n                positions = sorted([p for p in positions if (p != (- 1))])\n                if positions:\n                    line = line[0:positions[0]]\n            result[current_source].add(line.strip())\n        elif line.startswith('Conflicts:'):\n            line = line[len('Conflicts:'):]\n            conflicts[current_source].add(line.strip())\n        elif line.startswith('# Source:'):\n            current_source = line[len('# Source:'):].strip()\n    found_names = set(result.keys())\n    found_names.update(conflicts.keys())\n    (missing_names, extra_names) = _fetch_missing_extra(python_names, found_names)\n    if missing_names:\n        raise AssertionError(('Python names were lost during conversion: %s' % ', '.join(sorted(missing_names))))\n    if extra_names:\n        raise AssertionError(('Extra python names were found during conversion: %s' % ', '.join(sorted(extra_names))))\n    return (result, conflicts)\n", "label": 1}
{"function": "\n\ndef postgresql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None):\n    'PostgreSQL psycopg2 driver  accepts two syntaxes\\n\\n        Plus a string for .pgpass file\\n        '\n    dsn = []\n    if ((dsn_style is None) or (dsn_style == 'all') or (dsn_style == 'keyvalue')):\n        dsnstr = \"host='{0}' dbname='{2}' user='{3}' password='{4}'\"\n        if (dbport is not None):\n            dsnstr += \" port='{1}'\"\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'kwargs')):\n        dsnstr = \"host='{0}', database='{2}', user='{3}', password='{4}'\"\n        if (dbport is not None):\n            dsnstr += \", port='{1}'\"\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'uri')):\n        if (dbport is not None):\n            dsnstr = 'postgresql://{3}:{4}@{0}:{1}/{2}'\n        else:\n            dsnstr = 'postgresql://{3}:{4}@{0}/{2}'\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'pgpass')):\n        if (dbport is not None):\n            dbport = 5432\n        dsn.append('{0}:{1}:{2}:{3}:{4}'.format(dbhost, dbport, dbname, dbuser, dbpass))\n    return dsn\n", "label": 1}
{"function": "\n\ndef __init__(self, year=None, month=None, day=None, week=None, day_of_week=None, hour=None, minute=None, second=None, start_date=None, end_date=None, timezone=None):\n    if timezone:\n        self.timezone = astimezone(timezone)\n    elif (start_date and start_date.tzinfo):\n        self.timezone = start_date.tzinfo\n    elif (end_date and end_date.tzinfo):\n        self.timezone = end_date.tzinfo\n    else:\n        self.timezone = get_localzone()\n    self.start_date = convert_to_datetime(start_date, self.timezone, 'start_date')\n    self.end_date = convert_to_datetime(end_date, self.timezone, 'end_date')\n    values = dict(((key, value) for (key, value) in six.iteritems(locals()) if ((key in self.FIELD_NAMES) and (value is not None))))\n    self.fields = []\n    assign_defaults = False\n    for field_name in self.FIELD_NAMES:\n        if (field_name in values):\n            exprs = values.pop(field_name)\n            is_default = False\n            assign_defaults = (not values)\n        elif assign_defaults:\n            exprs = DEFAULT_VALUES[field_name]\n            is_default = True\n        else:\n            exprs = '*'\n            is_default = True\n        field_class = self.FIELDS_MAP[field_name]\n        field = field_class(field_name, exprs, is_default)\n        self.fields.append(field)\n", "label": 1}
{"function": "\n\ndef _eval_is_zero(self):\n    if self.function.is_zero:\n        return True\n    got_none = False\n    for l in self.limits:\n        if (len(l) == 3):\n            z = ((l[1] == l[2]) or (l[1] - l[2]).is_zero)\n            if z:\n                return True\n            elif (z is None):\n                got_none = True\n    free = self.function.free_symbols\n    for xab in self.limits:\n        if (len(xab) == 1):\n            free.add(xab[0])\n            continue\n        if ((len(xab) == 2) and (xab[0] not in free)):\n            if xab[1].is_zero:\n                return True\n            elif (xab[1].is_zero is None):\n                got_none = True\n        free.discard(xab[0])\n        for i in xab[1:]:\n            free.update(i.free_symbols)\n    if ((self.function.is_zero is False) and (got_none is False)):\n        return False\n", "label": 1}
{"function": "\n\ndef read_symbol(form):\n    'Read a symbol.'\n    if (len(form) == 0):\n        raise SyntaxError('unexpected EOF while reading symbol')\n    if (not form[0].isalpha()):\n        raise SyntaxError('expected alpha char as first char of symbol')\n    s = ''\n    while (len(form) > 0):\n        ch = form[0]\n        if (not (ch.isalpha() or ch.isdigit() or (ch == '-') or (ch == ':'))):\n            if (s == 't'):\n                return (True, form)\n            elif (s == 'nil'):\n                return (False, form)\n            else:\n                return (Symbol(s), form)\n        else:\n            s = (s + ch)\n            form = form[1:]\n    if (len(s) > 0):\n        return (Symbol(s), form)\n    else:\n        raise SyntaxError('EOF while reading symbol')\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.settings = TrafficControlSetting()\n                self.settings.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.device = TrafficControlledDevice()\n                self.device.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.timeout = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _create_joins(self, source_polymorphic=False, source_selectable=None, dest_polymorphic=False, dest_selectable=None, of_type=None):\n    if (source_selectable is None):\n        if (source_polymorphic and self.parent.with_polymorphic):\n            source_selectable = self.parent._with_polymorphic_selectable\n    aliased = False\n    if (dest_selectable is None):\n        if (dest_polymorphic and self.mapper.with_polymorphic):\n            dest_selectable = self.mapper._with_polymorphic_selectable\n            aliased = True\n        else:\n            dest_selectable = self.mapper.mapped_table\n        if (self._is_self_referential and (source_selectable is None)):\n            dest_selectable = dest_selectable.alias()\n            aliased = True\n    else:\n        aliased = True\n    dest_mapper = (of_type or self.mapper)\n    single_crit = dest_mapper._single_table_criterion\n    aliased = (aliased or (source_selectable is not None))\n    (primaryjoin, secondaryjoin, secondary, target_adapter, dest_selectable) = self._join_condition.join_targets(source_selectable, dest_selectable, aliased, single_crit)\n    if (source_selectable is None):\n        source_selectable = self.parent.local_table\n    if (dest_selectable is None):\n        dest_selectable = self.mapper.local_table\n    return (primaryjoin, secondaryjoin, source_selectable, dest_selectable, secondary, target_adapter)\n", "label": 1}
{"function": "\n\ndef dict_to_xml(self, root_elm, data):\n    for key in data.keys():\n        if (key in self.NO_SEND_FIELDS):\n            continue\n        sub_data = data[key]\n        elm = SubElement(root_elm, key)\n        if isinstance(sub_data, dict):\n            self.dict_to_xml(elm, sub_data)\n        elif (isinstance(sub_data, list) or isinstance(sub_data, tuple)):\n            if isplural(key):\n                for d in sub_data:\n                    self.dict_to_xml(SubElement(elm, singular(key)), d)\n            else:\n                for d in sub_data:\n                    self.dict_to_xml(elm, d)\n        else:\n            if (key in self.BOOLEAN_FIELDS):\n                val = ('true' if sub_data else 'false')\n            elif (key in self.DATE_FIELDS):\n                val = sub_data.strftime('%Y-%m-%dT%H:%M:%S')\n            else:\n                val = six.text_type(sub_data)\n            elm.text = val\n    return root_elm\n", "label": 1}
{"function": "\n\ndef has_method(obj, name):\n    if (not hasattr(obj, name)):\n        return False\n    func = getattr(obj, name)\n    if isinstance(func, types.BuiltinMethodType):\n        return True\n    if (not isinstance(func, (types.MethodType, types.FunctionType))):\n        return False\n    base_type = (obj if is_type(obj) else obj.__class__)\n    original = None\n    for subtype in inspect.getmro(base_type):\n        original = vars(subtype).get(name)\n        if (original is not None):\n            break\n    if (original is None):\n        return False\n    if isinstance(original, staticmethod):\n        return True\n    self_attr = ('__self__' if PY3 else 'im_self')\n    if (not hasattr(func, self_attr)):\n        return False\n    bound_to = getattr(func, self_attr)\n    if isinstance(original, classmethod):\n        return issubclass(base_type, bound_to)\n    return isinstance(obj, type(bound_to))\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_data_rows(self, ar):\n    if ar.param_values.show_callables:\n\n        def flt(v):\n            return True\n    else:\n\n        def flt(v):\n            if isinstance(v, (types.FunctionType, types.GeneratorType, types.UnboundMethodType, types.UnboundMethodType, types.BuiltinMethodType, types.BuiltinFunctionType)):\n                return False\n            return True\n    o = self.get_inspected(ar.param_values.inspected)\n    if isinstance(o, (list, tuple)):\n        for (i, v) in enumerate(o):\n            k = (('[' + str(i)) + ']')\n            (yield Inspected(o, '', k, v))\n    elif isinstance(o, AttrDict):\n        for (k, v) in list(o.items()):\n            (yield Inspected(o, '.', k, v))\n    elif isinstance(o, dict):\n        for (k, v) in list(o.items()):\n            k = (('[' + repr(k)) + ']')\n            (yield Inspected(o, '', k, v))\n    else:\n        for k in dir(o):\n            if (not k.startswith('__')):\n                if ((not ar.quick_search) or (ar.quick_search.lower() in k.lower())):\n                    v = getattr(o, k)\n                    if flt(v):\n                        (yield Inspected(o, '.', k, v))\n", "label": 1}
{"function": "\n\ndef __init__(self, method, uri, version='HTTP/1.0', headers=None, body=None, remote_ip=None, protocol=None, host=None, files=None, connection=None):\n    self.method = method\n    self.uri = uri\n    self.version = version\n    self.headers = (headers or httputil.HTTPHeaders())\n    self.body = (body or '')\n    self.remote_ip = remote_ip\n    if protocol:\n        self.protocol = protocol\n    elif (connection and isinstance(connection.stream, iostream.SSLIOStream)):\n        self.protocol = 'https'\n    else:\n        self.protocol = 'http'\n    if (connection and connection.xheaders):\n        ip = self.headers.get('X-Forwarded-For', self.remote_ip)\n        ip = ip.split(',')[(- 1)].strip()\n        ip = self.headers.get('X-Real-Ip', ip)\n        if netutil.is_valid_ip(ip):\n            self.remote_ip = ip\n        proto = self.headers.get('X-Scheme', self.headers.get('X-Forwarded-Proto', self.protocol))\n        if (proto in ('http', 'https')):\n            self.protocol = proto\n    self.host = (host or self.headers.get('Host') or '127.0.0.1')\n    self.files = (files or {\n        \n    })\n    self.connection = connection\n    self._start_time = time.time()\n    self._finish_time = None\n    (self.path, sep, self.query) = uri.partition('?')\n    self.arguments = parse_qs_bytes(self.query, keep_blank_values=True)\n", "label": 1}
{"function": "\n\ndef clean(self):\n    type = self.cleaned_data['recordType']\n    if (type in ('4', 'Z')):\n        for field in ADDRESS_FIELDS:\n            self.cleaned_data[field] = None\n    if (type == 'Z'):\n        self.cleaned_data['zipExtensionLow'] = None\n        self.cleaned_data['zipExtensionHigh'] = None\n        low = self.cleaned_data['zipCodeLow']\n        high = self.cleaned_data['zipCodeHigh']\n        if ((high is None) or (low is None)):\n            raise ValidationError(_('Zip-5 records need a high and a low.'))\n    elif (type == '4'):\n        low = self.cleaned_data['zipCodeLow']\n        high = self.cleaned_data['zipCodeHigh']\n        low_ext = self.cleaned_data['zipExtensionLow']\n        high_ext = self.cleaned_data['zipExtensionHigh']\n        if ((high is None) or (low is None) or (low_ext is None) or (high_ext is None)):\n            raise ValidationError(_('Zip+4 records need a high and a low for both parts.'))\n    else:\n        for field in ZIP_FIELDS:\n            self.cleaned_data[field] = None\n        for field in ('lowAddress', 'highAddress', 'streetName', 'cityName', 'zipCode', 'plus4'):\n            if (not self.cleaned_data[field]):\n                raise ValidationError(_('Address rocord needs: low, high, street, city, zip, zip+4'))\n    return super(TaxBoundryForm, self).clean()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef validate_field(value, field, new_record=True):\n    '\\n        Validates a field value against a field metadata dictionary. Note: this\\n        is not yet intended to be a full validation. There are sure to be\\n        missing validation cases. Returns a list of validation errors.\\n        '\n    errors = []\n    if new_record:\n        if ((not field['createable']) and (value is not None)):\n            errors.append('Cannot create this field')\n    elif ((not field['updateable']) and (value is not None)):\n        errors.append('Cannot update this field')\n    if ((value is not None) and field.get('restrictedPicklist')):\n        values = [i['value'] for i in field['picklistValues'] if i['active']]\n        if (value not in values):\n            errors.append('Bad value for restricted picklist field')\n    if (new_record and (value is None) and (not field['nillable']) and (not field['defaultedOnCreate']) and (field['type'] != 'boolean')):\n        errors.append('This field is required')\n    return errors\n", "label": 1}
{"function": "\n\ndef merge_undo(self, undo_item):\n    ' Merges two undo items if possible.\\n        '\n    if (isinstance(undo_item, self.__class__) and (self.object is undo_item.object) and (self.name == undo_item.name) and (self.index == undo_item.index)):\n        added = undo_item.added\n        removed = undo_item.removed\n        if ((len(self.added) == len(added)) and (len(self.removed) == len(removed))):\n            for (i, item) in enumerate(self.added):\n                if (item is not added[i]):\n                    break\n            else:\n                for (i, item) in enumerate(self.removed):\n                    if (item is not removed[i]):\n                        break\n                else:\n                    return True\n    return False\n", "label": 1}
{"function": "\n\ndef __init__(self, key, secret=None, secure=True, host=None, path=None, port=None, url=None, *args, **kwargs):\n    '\\n        :inherits: :class:`NodeDriver.__init__`\\n\\n        :param    host: The host where the API can be reached. (required)\\n        :type     host: ``str``\\n\\n        :param    path: The path where the API can be reached. (required)\\n        :type     path: ``str``\\n\\n        :param url: Full URL to the API endpoint. Mutually exclusive with host\\n                    and path argument.\\n        :type url: ``str``\\n        '\n    if url:\n        parsed = urlparse.urlparse(url)\n        path = parsed.path\n        scheme = parsed.scheme\n        split = parsed.netloc.split(':')\n        if (len(split) == 1):\n            host = parsed.netloc\n            port = (443 if (scheme == 'https') else 80)\n        else:\n            host = split[0]\n            port = int(split[1])\n    else:\n        host = (host if host else self.host)\n        path = (path if path else self.path)\n    if (path is not None):\n        self.path = path\n    if (host is not None):\n        self.host = host\n    if ((self.type == Provider.CLOUDSTACK) and ((not host) or (not path))):\n        raise Exception('When instantiating CloudStack driver directly you also need to provide url or host and path argument')\n    super(CloudStackNodeDriver, self).__init__(key=key, secret=secret, secure=secure, host=host, port=port)\n", "label": 1}
{"function": "\n\ndef read_named_ranges(xml_source, workbook):\n    'Read named ranges, excluding poorly defined ranges.'\n    named_ranges = []\n    root = fromstring(xml_source)\n    names_root = root.find(QName('http://schemas.openxmlformats.org/spreadsheetml/2006/main', 'definedNames').text)\n    if (names_root is not None):\n        for name_node in names_root.getchildren():\n            range_name = name_node.get('name')\n            if (name_node.get('hidden', '0') == '1'):\n                continue\n            valid = True\n            for discarded_range in DISCARDED_RANGES:\n                if (discarded_range in range_name):\n                    valid = False\n            for bad_range in BUGGY_NAMED_RANGES:\n                if (bad_range in name_node.text):\n                    valid = False\n            if valid:\n                destinations = split_named_range(name_node.text)\n                new_destinations = []\n                for (worksheet, cells_range) in destinations:\n                    worksheet = workbook.get_sheet_by_name(worksheet)\n                    if worksheet:\n                        new_destinations.append((worksheet, cells_range))\n                named_range = NamedRange(range_name, new_destinations)\n                named_ranges.append(named_range)\n    return named_ranges\n", "label": 1}
{"function": "\n\ndef find_selected_files(schema, metadata):\n    targets = []\n    paths = [('', p) for p in schema.schema['pages']]\n    while len(paths):\n        (prefix, path) = paths.pop(0)\n        if path.get('questions'):\n            paths = (paths + [('', q) for q in path['questions']])\n        elif path.get('type'):\n            qid = path.get('qid', path.get('id'))\n            if (path['type'] == 'object'):\n                paths = (paths + [('{}.{}.value'.format(prefix, qid), p) for p in path['properties']])\n            elif (path['type'] == 'osf-upload'):\n                targets.append('{}.{}'.format(prefix, qid).lstrip('.'))\n    selected = {\n        \n    }\n    for t in targets:\n        parts = t.split('.')\n        value = metadata.get(parts.pop(0))\n        while (value and len(parts)):\n            value = value.get(parts.pop(0))\n        if value:\n            selected[t] = value\n    return selected\n", "label": 1}
{"function": "\n\ndef download(self, config):\n    path = config.path\n    sheet_id = config.id\n    sheet_gid = config.gid\n    service = self._create_service()\n    resp = service.files().get(fileId=sheet_id).execute()\n    ext = os.path.splitext(self.config.path)[1]\n    convert_to = None\n    if (ext == '.json'):\n        convert_to = ext\n        ext = '.csv'\n    elif (ext in ['.yaml', '.yml']):\n        convert_to = ext\n        ext = '.csv'\n    for (mimetype, url) in resp['exportLinks'].iteritems():\n        if (not mimetype.endswith(ext[1:])):\n            continue\n        if self.config.gid:\n            url += '&gid={}'.format(self.config.gid)\n        (resp, content) = service._http.request(url)\n        if (resp.status != 200):\n            self.logger.error('Error {} downloading Google Sheet: {}'.format(resp.status, path))\n            break\n        if (convert_to in ['.json', '.yaml', '.yml']):\n            fp = cStringIO.StringIO()\n            fp.write(content)\n            fp.seek(0)\n            reader = csv.DictReader(fp)\n            kwargs = {\n                \n            }\n            if (convert_to == '.json'):\n                if (self.config.output_style == 'pretty'):\n                    kwargs['indent'] = 2\n                    kwargs['separators'] = (',', ': ')\n                    kwargs['sort_keys'] = True\n                content = json.dumps([row for row in reader], **kwargs)\n            else:\n                content = yaml.safe_dump(list(reader), default_flow_style=False)\n        self.pod.write_file(path, content)\n        self.logger.info('Downloaded Google Sheet -> {}'.format(path))\n", "label": 1}
{"function": "\n\ndef extract(self):\n    d = self.dvm\n    c2 = []\n    port = []\n    for cls in d.get_classes():\n        if ((len(cls.get_fields()) == 3) and (set(['a', 'b', 'c']) == set(map((lambda x: x.name), cls.get_fields()))) and (len(cls.get_methods()) == 1) and cls.get_methods()[0].name.endswith('<clinit>')):\n            clinit = cls.get_methods()[0]\n            cc = []\n            for inst in clinit.get_instructions():\n                if (inst.get_name() == 'const-string'):\n                    c2.append(inst.get_output().split(',')[(- 1)].strip(\" '\"))\n                elif (inst.get_name() == 'const/16'):\n                    port.append(int(inst.get_output().split(',')[(- 1)].strip(\" '\")))\n    servers = []\n    for (i, server) in enumerate(c2):\n        if (len(port) > i):\n            servers.append(((server + ':') + str(port[i])))\n        else:\n            servers.append(server)\n    return {\n        'c2': servers,\n    }\n", "label": 1}
{"function": "\n\ndef write(self, history_line):\n    add = True\n    if ((self.filter_empty.GetValue() == True) and (history_line == '')):\n        add = False\n    if (len(history_line) > 0):\n        if ((self.filter_doc.GetValue() == True) and (history_line[(- 1):] == '?')):\n            add = False\n        if ((self.filter_cmd.GetValue() == True) and (history_line[0] == '!')):\n            add = False\n        if ((self.filter_magic.GetValue() == True) and (history_line[0] == '%')):\n            add = False\n    if add:\n        self.text_ctrl.AppendText((history_line + '\\n'))\n", "label": 1}
{"function": "\n\n@defun_wrapped\ndef _ci_generic(ctx, z):\n    if ctx.isinf(z):\n        if (z == ctx.inf):\n            return ctx.zero\n        if (z == ctx.ninf):\n            return (ctx.pi * 1j)\n    jz = ctx.fmul(ctx.j, z, exact=True)\n    njz = ctx.fneg(jz, exact=True)\n    v = (0.5 * (ctx.ei(jz) + ctx.ei(njz)))\n    zreal = ctx._re(z)\n    zimag = ctx._im(z)\n    if (zreal == 0):\n        if (zimag > 0):\n            v += (ctx.pi * 0.5j)\n        if (zimag < 0):\n            v -= (ctx.pi * 0.5j)\n    if (zreal < 0):\n        if (zimag >= 0):\n            v += (ctx.pi * 1j)\n        if (zimag < 0):\n            v -= (ctx.pi * 1j)\n    if (ctx._is_real_type(z) and (zreal > 0)):\n        v = ctx._re(v)\n    return v\n", "label": 1}
{"function": "\n\ndef to_json_message(self):\n    json_message = {\n        'From': self.__sender,\n        'To': self.__to,\n        'Subject': self.__subject,\n    }\n    if self.__reply_to:\n        json_message['ReplyTo'] = self.__reply_to\n    if self.__cc:\n        json_message['Cc'] = self.__cc\n    if self.__bcc:\n        json_message['Bcc'] = self.__bcc\n    if self.__tag:\n        json_message['Tag'] = self.__tag\n    if self.__html_body:\n        json_message['HtmlBody'] = self.__html_body\n    if self.__text_body:\n        json_message['TextBody'] = self.__text_body\n    if self.__track_opens:\n        json_message['TrackOpens'] = True\n    if (len(self.__custom_headers) > 0):\n        cust_headers = []\n        for (key, value) in self.__custom_headers.items():\n            cust_headers.append({\n                'Name': key,\n                'Value': value,\n            })\n        json_message['Headers'] = cust_headers\n    if (len(self.__attachments) > 0):\n        attachments = []\n        for attachment in self.__attachments:\n            if isinstance(attachment, tuple):\n                attachments.append({\n                    'Name': attachment[0],\n                    'Content': attachment[1],\n                    'ContentType': attachment[2],\n                })\n            elif isinstance(attachment, MIMEBase):\n                attachments.append({\n                    'Name': attachment.get_filename(),\n                    'Content': attachment.get_payload(),\n                    'ContentType': attachment.get_content_type(),\n                })\n        json_message['Attachments'] = attachments\n    return json_message\n", "label": 1}
{"function": "\n\ndef _eval_expand_mul(self, **hints):\n    from sympy import fraction\n    expr = self\n    (n, d) = fraction(expr)\n    if d.is_Mul:\n        (n, d) = [(i._eval_expand_mul(**hints) if i.is_Mul else i) for i in (n, d)]\n        expr = (n / d)\n        if (not expr.is_Mul):\n            return expr\n    (plain, sums, rewrite) = ([], [], False)\n    for factor in expr.args:\n        if factor.is_Add:\n            sums.append(factor)\n            rewrite = True\n        elif factor.is_commutative:\n            plain.append(factor)\n        else:\n            sums.append(Basic(factor))\n    if (not rewrite):\n        return expr\n    else:\n        plain = self.func(*plain)\n        if sums:\n            terms = self.func._expandsums(sums)\n            args = []\n            for term in terms:\n                t = self.func(plain, term)\n                if (t.is_Mul and any((a.is_Add for a in t.args))):\n                    t = t._eval_expand_mul()\n                args.append(t)\n            return Add(*args)\n        else:\n            return plain\n", "label": 1}
{"function": "\n\ndef solve(self, board, i, j):\n    '\\n        dfs\\n        :param board: a 9x9 2D array\\n        :return: Boolean\\n        '\n    if (j >= 9):\n        return self.solve(board, (i + 1), 0)\n    if (i == 9):\n        return True\n    if (board[i][j] == '.'):\n        for num in range(1, 10):\n            num_str = str(num)\n            if (all([(board[i][col] != num_str) for col in xrange(9)]) and all([(board[row][j] != num_str) for row in xrange(9)]) and all([(board[(((i / 3) * 3) + (count / 3))][(((j / 3) * 3) + (count % 3))] != num_str) for count in xrange(9)])):\n                board[i][j] = num_str\n                if (not self.solve(board, i, (j + 1))):\n                    board[i][j] = '.'\n                else:\n                    return True\n    else:\n        return self.solve(board, i, (j + 1))\n    return False\n", "label": 1}
{"function": "\n\ndef before_after_sort(items):\n    \" Sort a sequence of items with 'before', 'after', and 'id' attributes.\\n        \\n    The sort is topological. If an item does not specify a 'before' or 'after',\\n    it is placed after the preceding item.\\n\\n    If a cycle is found in the dependencies, a warning is logged and the order\\n    of the items is undefined.\\n    \"\n    if (len(items) < 2):\n        return items\n    item_map = dict(((item.id, item) for item in items if item.id))\n    pairs = []\n    prev_item = None\n    for item in items:\n        new_pairs = []\n        if (hasattr(item, 'before') and item.before):\n            (parent, child) = (item, item_map.get(item.before))\n            if child:\n                new_pairs.append((parent, child))\n        if (hasattr(item, 'after') and item.after):\n            (parent, child) = (item_map.get(item.after), item)\n            if parent:\n                new_pairs.append((parent, child))\n        if new_pairs:\n            pairs.extend(new_pairs)\n        else:\n            if prev_item:\n                pairs.append((prev_item, item))\n            prev_item = item\n    (result, has_cycle) = topological_sort(pairs)\n    if has_cycle:\n        logger.warning('Cycle in before/after sort for items %r', items)\n    return result\n", "label": 1}
{"function": "\n\ndef linkSamplePredicate(subsample_size, predicate, items1, items2):\n    sample = []\n    predicate_function = predicate.func\n    field = predicate.field\n    red = defaultdict(list)\n    blue = defaultdict(list)\n    for (i, (index, record)) in enumerate(interleave(items1, items2)):\n        if (i == 20000):\n            if ((min(len(red), len(blue)) + len(sample)) < 10):\n                return sample\n        column = record[field]\n        if (not column):\n            (red, blue) = (blue, red)\n            continue\n        block_keys = predicate_function(column)\n        for block_key in block_keys:\n            if blue.get(block_key):\n                pair = sort_pair(blue[block_key].pop(), index)\n                sample.append(pair)\n                subsample_size -= 1\n                if subsample_size:\n                    break\n                else:\n                    return sample\n            else:\n                red[block_key].append(index)\n        (red, blue) = (blue, red)\n    for (index, record) in itertools.islice(items2, len(items1)):\n        column = record[field]\n        if (not column):\n            continue\n        block_keys = predicate_function(column)\n        for block_key in block_keys:\n            if red.get(block_key):\n                pair = sort_pair(red[block_key].pop(), index)\n                sample.append(pair)\n                subsample_size -= 1\n                if subsample_size:\n                    break\n                else:\n                    return sample\n    return sample\n", "label": 1}
{"function": "\n\ndef summary(self):\n    tpg = self.rtsnode\n    status = None\n    msg = []\n    if tpg.has_feature('nexus'):\n        msg.append(str(self.rtsnode.nexus))\n    if (not tpg.enable):\n        return ('disabled', False)\n    if tpg.has_feature('acls'):\n        if (('generate_node_acls' in tpg.list_attributes()) and int(tpg.get_attribute('generate_node_acls'))):\n            msg.append('gen-acls')\n        else:\n            msg.append('no-gen-acls')\n        if tpg.has_feature('auth'):\n            if (not int(tpg.get_attribute('authentication'))):\n                msg.append('no-auth')\n                if int(tpg.get_attribute('generate_node_acls')):\n                    status = True\n            elif (not int(tpg.get_attribute('generate_node_acls'))):\n                msg.append('auth per-acl')\n            else:\n                msg.append('tpg-auth')\n                status = True\n                if (not (tpg.chap_password and tpg.chap_userid)):\n                    status = False\n                if tpg.authenticate_target:\n                    msg.append('mutual auth')\n                else:\n                    msg.append('1-way auth')\n    return (', '.join(msg), status)\n", "label": 1}
{"function": "\n\ndef alignment_is_good(record, a, arange):\n    trimmed_slen = (arange.end - arange.start)\n    if (trimmed_slen < 0):\n        sys.exit('BUG: trimmed_slen bad value\\n')\n    qury_wiggle = (record.qlen * WIGGLE_PCT)\n    if (not (fabs((a.send - a.sstart)) > MIN_ALIGNMENT_SIZE)):\n        return False\n    if ((fabs((a.send - a.sstart)) >= (trimmed_slen * CONTAINED_PCT)) and (record.qlen > record.slen)):\n        return True\n    if ((fabs((a.qend - a.qstart)) > (record.qlen * CONTAINED_PCT)) and (record.slen > record.qlen)):\n        return True\n    if (((a.sstart == arange.start) or (arange.end == a.send)) and ((a.qstart < qury_wiggle) or (a.qend < qury_wiggle) or ((record.qlen - a.qstart) < qury_wiggle) or ((record.qlen - a.qend) < qury_wiggle))):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef check_if_doc_is_linked(doc, method='Delete'):\n    '\\n\\t\\tRaises excption if the given doc(dt, dn) is linked in another record.\\n\\t'\n    from frappe.model.rename_doc import get_link_fields\n    link_fields = get_link_fields(doc.doctype)\n    link_fields = [[lf['parent'], lf['fieldname'], lf['issingle']] for lf in link_fields]\n    for (link_dt, link_field, issingle) in link_fields:\n        if (not issingle):\n            item = frappe.db.get_value(link_dt, {\n                link_field: doc.name,\n            }, ['name', 'parent', 'parenttype', 'docstatus'], as_dict=True)\n            if (item and ((item.parent or item.name) != doc.name) and (((method == 'Delete') and (item.docstatus < 2)) or ((method == 'Cancel') and (item.docstatus == 1)))):\n                frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, (item.parenttype if item.parent else link_dt), (item.parent or item.name)), frappe.LinkExistsError)\n", "label": 1}
{"function": "\n\ndef temp_name(self, expr):\n    c = expr.__class__\n    if (c is PrimCall):\n        return expr.prim.name\n    elif (c is Attribute):\n        if (expr.value.__class__ is Var):\n            return ((names.original(expr.value.name) + '_') + expr.name)\n        else:\n            return expr.name\n    elif (c is Attribute):\n        if (expr.value.__class__ is Var):\n            return ('%s_%s' % (expr.value.name, expr.name))\n        else:\n            return expr.name\n    elif (c is Index):\n        idx_t = expr.index.type\n        if (isinstance(idx_t, SliceT) or (isinstance(idx_t, TupleT) and any((isinstance(elt_t, SliceT) for elt_t in idx_t.elt_types)))):\n            return 'slice'\n        else:\n            return 'elt'\n    elif (c is TupleProj):\n        if (expr.tuple.__class__ is Var):\n            original = names.original(expr.tuple.name)\n            return ('%s%d_elt%d' % (original, names.versions[original], expr.index))\n        else:\n            return ('tuple_elt%d' % expr.index)\n    elif (c is Var):\n        return names.refresh(expr.name)\n    else:\n        return 'temp'\n", "label": 1}
{"function": "\n\ndef on_ssl(self, client_hello):\n    anon_ciphers = [str(c) for c in client_hello.ciphers if ('_anon_' in str(c))]\n    if anon_ciphers:\n        self._handle_bad_ciphers(anon_ciphers, ('Client enabled anonymous TLS/SSL cipher suites %s' % ', '.join(anon_ciphers)))\n    null_ciphers = [str(c) for c in client_hello.ciphers if ('_WITH_NULL_' in str(c))]\n    if null_ciphers:\n        self._handle_bad_ciphers(null_ciphers, ('Client enabled NULL encryption TLS/SSL cipher suites %s' % ', '.join(null_ciphers)))\n    integ_ciphers = [str(c) for c in client_hello.ciphers if str(c).endswith('_NULL')]\n    if integ_ciphers:\n        self._handle_bad_ciphers(integ_ciphers, ('Client enabled NULL integrity TLS/SSL cipher suites %s' % ', '.join(integ_ciphers)))\n    export_ciphers = [str(c) for c in client_hello.ciphers if ('EXPORT' in str(c))]\n    if export_ciphers:\n        self._handle_bad_ciphers(integ_ciphers, ('Client enabled export TLS/SSL cipher suites %s' % ', '.join(export_ciphers)))\n", "label": 1}
{"function": "\n\ndef take_action(self, parsed_args):\n    client = self.get_client()\n    extra_values = v2_0.parse_args_to_dict(self.values_specs)\n    if extra_values:\n        raise exceptions.CommandError((_('Invalid argument(s): --%s') % ', --'.join(extra_values)))\n    tenant_id = (parsed_args.tenant_id or parsed_args.pos_tenant_id)\n    if parsed_args.dry_run:\n        data = client.validate_auto_allocated_topology_requirements(tenant_id)\n    else:\n        data = client.get_auto_allocated_topology(tenant_id)\n    if (self.resource in data):\n        for (k, v) in data[self.resource].items():\n            if isinstance(v, list):\n                value = ''\n                for _item in v:\n                    if value:\n                        value += '\\n'\n                    if isinstance(_item, dict):\n                        value += jsonutils.dumps(_item)\n                    else:\n                        value += str(_item)\n                data[self.resource][k] = value\n            elif (v == 'dry-run=pass'):\n                return (('dry-run',), ('pass',))\n            elif (v is None):\n                data[self.resource][k] = ''\n        return zip(*sorted(data[self.resource].items()))\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef __lt__(self, other):\n    if (self.date == 'infinity'):\n        return False\n    if isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return True\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) < other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date < other.date.replace(tzinfo=self.tz))\n        return (self.date < other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            if (other.end.tz and (self.tz is None)):\n                return (self.date.replace(tzinfo=other.end.tz) < other.end.date)\n            elif (self.tz and (other.end.tz is None)):\n                return (self.date < other.end.date.replace(tzinfo=self.tz))\n            return (self.date < other.end.date)\n        else:\n            return self.__lt__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef get_line_content(self, patch_content, line_number):\n    content = ''\n    content_list = []\n    lines = patch_content.split('\\n')\n    original_line = 0\n    new_line = 0\n    original_end = 0\n    new_end = 0\n    last_content_added = ''\n    for line in lines:\n        if re.match('^@@(\\\\s|\\\\+|\\\\-|\\\\d|,)+@@', line, re.M):\n            begin = self.get_file_modification_begin(line)\n            original_line = begin[0]\n            new_line = begin[1]\n            end = self.get_file_modification_end(line)\n            deletion_end = end[0]\n            addition_end = end[1]\n        else:\n            if (deletion_end > line_number):\n                if (len(content_list) == 2):\n                    break\n                if ((original_line > line_number) and (new_line > line_number)):\n                    break\n            elif (new_line > line_number):\n                break\n            if re.match('^\\\\+.*', line, re.M):\n                if (new_line == line_number):\n                    content_list.append({\n                        'type': 'addition',\n                        'text': line[1:],\n                    })\n                    last_content_added = 'addition'\n                new_line += 1\n            elif re.match('^\\\\-.*', line, re.M):\n                if (original_line == line_number):\n                    content_list.append({\n                        'type': 'deletion',\n                        'text': line[1:],\n                    })\n                    last_content_added = 'deletion'\n                original_line += 1\n            elif ((line != '\\\\ No newline at end of file') and (line != '')):\n                original_line += 1\n                new_line += 1\n    if content_list:\n        content = content_list[(- 1)]\n    return content\n", "label": 1}
{"function": "\n\ndef execute_plugin_method_series(self, name, args=None, kwargs=None, single_response=False):\n    if (args is None):\n        args = []\n        use_kwargs = True\n    if (kwargs is None):\n        kwargs = {\n            \n        }\n        use_kwargs = False\n    if (use_kwargs and single_response):\n        raise RuntimeError('When executing plugins in series using `single` response mode, you must specify only args.')\n    elif (args and kwargs):\n        raise RuntimeError('Plugins can be ran in series using either args or kwargs, not both.')\n    for plugin in self.plugins:\n        if (not hasattr(plugin, name)):\n            continue\n        method = getattr(plugin, name)\n        plugin_result = method(*args, **kwargs)\n        if (plugin_result is not None):\n            if use_kwargs:\n                kwargs = plugin_result\n            elif single_response:\n                args = (plugin_result,)\n            else:\n                args = plugin_result\n    if use_kwargs:\n        return kwargs\n    elif single_response:\n        return args[0]\n    return args\n", "label": 1}
{"function": "\n\ndef visit_Module(self, node):\n    params = node.get_params()\n    for param in params.values():\n        if (param.width is not None):\n            param.width = self.replace_visitor.visit(param.width)\n        param.value = self.replace_visitor.visit(param.value)\n    localparams = node.get_localparams()\n    for localparam in localparams.values():\n        if (localparam.width is not None):\n            localparam.width = self.replace_visitor.visit(localparam.width)\n        localparam.value = self.replace_visitor.visit(localparam.value)\n    ports = node.get_ports()\n    for port in ports.values():\n        if (port.width is not None):\n            port.width = self.replace_visitor.visit(port.width)\n    vars = node.get_vars()\n    for var in vars.values():\n        if (var.width is not None):\n            var.width = self.replace_visitor.visit(var.width)\n    for asg in node.assign:\n        self.visit(asg)\n    for alw in node.always:\n        self.visit(alw)\n    for ini in node.initial:\n        self.visit(ini)\n    for ins in node.instance.values():\n        self.visit(ins)\n    return node\n", "label": 1}
{"function": "\n\ndef __getattr__(self, name):\n    if (name in ('_mock_methods', '_mock_unsafe')):\n        raise AttributeError(name)\n    elif (self._mock_methods is not None):\n        if ((name not in self._mock_methods) or (name in _all_magics)):\n            raise AttributeError(('Mock object has no attribute %r' % name))\n    elif _is_magic(name):\n        raise AttributeError(name)\n    if (not self._mock_unsafe):\n        if name.startswith(('assert', 'assret')):\n            raise AttributeError(name)\n    result = self._mock_children.get(name)\n    if (result is _deleted):\n        raise AttributeError(name)\n    elif (result is None):\n        wraps = None\n        if (self._mock_wraps is not None):\n            wraps = getattr(self._mock_wraps, name)\n        result = self._get_child_mock(parent=self, name=name, wraps=wraps, _new_name=name, _new_parent=self)\n        self._mock_children[name] = result\n    elif isinstance(result, _SpecState):\n        result = create_autospec(result.spec, result.spec_set, result.instance, result.parent, result.name)\n        self._mock_children[name] = result\n    return result\n", "label": 1}
{"function": "\n\ndef GetObjectMetadata(self, bucket_name, object_name, generation=None, provider=None, fields=None):\n    'See CloudApi class for function doc strings.'\n    if generation:\n        generation = long(generation)\n    if (bucket_name in self.buckets):\n        bucket = self.buckets[bucket_name]\n        if ((object_name in bucket.objects) and bucket.objects[object_name]):\n            if generation:\n                if ('versioned' in bucket.objects[object_name]):\n                    for obj in bucket.objects[object_name]['versioned']:\n                        if (obj.root_object.generation == generation):\n                            return obj.root_object\n                if ('live' in bucket.objects[object_name]):\n                    if (bucket.objects[object_name]['live'].root_object.generation == generation):\n                        return bucket.objects[object_name]['live'].root_object\n            elif ('live' in bucket.objects[object_name]):\n                return bucket.objects[object_name]['live'].root_object\n        raise CreateObjectNotFoundException(404, self.provider, bucket_name, object_name)\n    raise CreateBucketNotFoundException(404, self.provider, bucket_name)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = Results()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = QueryNotFoundException()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.error2 = BeeswaxException()\n                self.error2.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef eval_sum(f, limits):\n    from sympy.concrete.delta import deltasummation, _has_simple_delta\n    from sympy.functions import KroneckerDelta\n    (i, a, b) = limits\n    if (f is S.Zero):\n        return S.Zero\n    if (i not in f.free_symbols):\n        return (f * ((b - a) + 1))\n    if (a == b):\n        return f.subs(i, a)\n    if isinstance(f, Piecewise):\n        if (not any(((i in arg.args[1].free_symbols) for arg in f.args))):\n            newargs = []\n            for arg in f.args:\n                newexpr = eval_sum(arg.expr, limits)\n                if (newexpr is None):\n                    return None\n                newargs.append((newexpr, arg.cond))\n            return f.func(*newargs)\n    if (f.has(KroneckerDelta) and _has_simple_delta(f, limits[0])):\n        return deltasummation(f, limits)\n    dif = (b - a)\n    definite = dif.is_Integer\n    if (definite and (dif < 100)):\n        return eval_sum_direct(f, (i, a, b))\n    if isinstance(f, Piecewise):\n        return None\n    value = eval_sum_symbolic(f.expand(), (i, a, b))\n    if (value is not None):\n        return value\n    if definite:\n        return eval_sum_direct(f, (i, a, b))\n", "label": 1}
{"function": "\n\ndef _ContractAperture(self, force=False):\n    \"Attempt to contract the aperture.  By calling this it's assume the aperture\\n    needs to be contracted.\\n\\n    The aperture can be contracted if it's current size is larger than the\\n    min size.\\n    \"\n    if (self._pending_endpoints and (not force)):\n        return\n    num_healthy = len([c for c in self._heap[1:] if c.channel.is_open])\n    if (num_healthy > self._min_size):\n        least_loaded_endpoint = None\n        for n in self._heap[1:]:\n            if (n.channel.is_closed and (n.endpoint not in self._pending_endpoints)):\n                least_loaded_endpoint = n.endpoint\n                break\n        if (not least_loaded_endpoint):\n            for n in self._heap[1:]:\n                if (n.endpoint not in self._pending_endpoints):\n                    least_loaded_endpoint = n.endpoint\n                    break\n        if least_loaded_endpoint:\n            self._idle_endpoints.add(least_loaded_endpoint)\n            super(ApertureBalancerSink, self)._RemoveSink(least_loaded_endpoint)\n            self._log.debug(('Contracting aperture to remove %s' % str(least_loaded_endpoint)))\n            self._UpdateSizeVarz()\n", "label": 1}
{"function": "\n\ndef _parse_pre_yarn_history_log(lines):\n    'Collect useful info from a pre-YARN history file.\\n\\n    See :py:func:`_parse_yarn_history_log` for return format.\\n    '\n    result = {\n        \n    }\n    task_to_counters = {\n        \n    }\n    for record in _parse_pre_yarn_history_records(lines):\n        fields = record['fields']\n        if ((record['type'] == 'Job') and ('COUNTERS' in fields)):\n            result['counters'] = _parse_pre_yarn_counters(fields['COUNTERS'])\n        elif ((record['type'] == 'Task') and ('COUNTERS' in fields) and ('TASKID' in fields)):\n            task_id = fields['TASKID']\n            counters = _parse_pre_yarn_counters(fields['COUNTERS'])\n            task_to_counters[task_id] = counters\n        elif ((record['type'] in ('MapAttempt', 'ReduceAttempt')) and ('TASK_ATTEMPT_ID' in fields) and (fields.get('TASK_STATUS') == 'FAILED') and fields.get('ERROR')):\n            result.setdefault('errors', [])\n            result['errors'].append(dict(hadoop_error=dict(message=fields['ERROR'], start_line=record['start_line'], num_lines=record['num_lines']), attempt_id=fields['TASK_ATTEMPT_ID']))\n    if (('counters' not in result) and task_to_counters):\n        result['counters'] = _sum_counters(*task_to_counters.values())\n    return result\n", "label": 1}
{"function": "\n\ndef _non_dominated_front_old(iterable, key=(lambda x: x), allowequality=True):\n    'Return a subset of items from iterable which are not dominated by any\\n    other item in iterable.'\n    items = list(iterable)\n    keys = dict(((i, key(i)) for i in items))\n    dim = len(list(keys.values())[0])\n    if any(((dim != len(k)) for k in list(keys.values()))):\n        raise ValueError('Wrong tuple size.')\n    dominations = collections.defaultdict((lambda : []))\n    for i in items:\n        for j in items:\n            if allowequality:\n                if all(((keys[i][k] < keys[j][k]) for k in range(dim))):\n                    dominations[i].append(j)\n            elif all(((keys[i][k] <= keys[j][k]) for k in range(dim))):\n                dominations[i].append(j)\n    dominates = (lambda i, j: (j in dominations[i]))\n    res = set()\n    items = set(items)\n    for i in items:\n        res.add(i)\n        for j in list(res):\n            if (i is j):\n                continue\n            if dominates(j, i):\n                res.remove(i)\n                break\n            elif dominates(i, j):\n                res.remove(j)\n    return res\n", "label": 1}
{"function": "\n\ndef _hash_internal(method, salt, password):\n    'Internal password hash helper.  Supports plaintext without salt,\\n    unsalted and salted passwords.  In case salted passwords are used\\n    hmac is used.\\n    '\n    if (method == 'plain'):\n        return (password, method)\n    if isinstance(password, text_type):\n        password = password.encode('utf-8')\n    if method.startswith('pbkdf2:'):\n        args = method[7:].split(':')\n        if (len(args) not in (1, 2)):\n            raise ValueError('Invalid number of arguments for PBKDF2')\n        method = args.pop(0)\n        iterations = ((args and int((args[0] or 0))) or DEFAULT_PBKDF2_ITERATIONS)\n        is_pbkdf2 = True\n        actual_method = ('pbkdf2:%s:%d' % (method, iterations))\n    else:\n        is_pbkdf2 = False\n        actual_method = method\n    hash_func = _hash_funcs.get(method)\n    if (hash_func is None):\n        raise TypeError(('invalid method %r' % method))\n    if is_pbkdf2:\n        if (not salt):\n            raise ValueError('Salt is required for PBKDF2')\n        rv = pbkdf2_hex(password, salt, iterations, hashfunc=hash_func)\n    elif salt:\n        if isinstance(salt, text_type):\n            salt = salt.encode('utf-8')\n        rv = hmac.HMAC(salt, password, hash_func).hexdigest()\n    else:\n        h = hash_func()\n        h.update(password)\n        rv = h.hexdigest()\n    return (rv, actual_method)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _get_mixins_(bases):\n    'Returns the type for creating enum members, and the first inherited\\n        enum class.\\n\\n        bases: the tuple of bases that was given to __new__\\n\\n        '\n    if ((not bases) or (Enum is None)):\n        return (object, Enum)\n    member_type = first_enum = None\n    for base in bases:\n        if ((base is not Enum) and issubclass(base, Enum) and base._member_names_):\n            raise TypeError('Cannot extend enumerations')\n    if (not issubclass(base, Enum)):\n        raise TypeError('new enumerations must be created as `ClassName([mixin_type,] enum_type)`')\n    if (not issubclass(bases[0], Enum)):\n        member_type = bases[0]\n        first_enum = bases[(- 1)]\n    else:\n        for base in bases[0].__mro__:\n            if issubclass(base, Enum):\n                if (first_enum is None):\n                    first_enum = base\n            elif (member_type is None):\n                member_type = base\n    return (member_type, first_enum)\n", "label": 1}
{"function": "\n\n@classmethod\ndef cache_url_config(cls, url, backend=None):\n    'Pulled from DJ-Cache-URL, parse an arbitrary Cache URL.\\n\\n        :param url:\\n        :param backend:\\n        :return:\\n        '\n    url = (urlparse.urlparse(url) if (not isinstance(url, cls.URL_CLASS)) else url)\n    location = url.netloc.split(',')\n    if (len(location) == 1):\n        location = location[0]\n    config = {\n        'BACKEND': cls.CACHE_SCHEMES[url.scheme],\n        'LOCATION': location,\n    }\n    if (url.scheme == 'filecache'):\n        config.update({\n            'LOCATION': (url.netloc + url.path),\n        })\n    if (url.path and (url.scheme in ['memcache', 'pymemcache'])):\n        config.update({\n            'LOCATION': ('unix:' + url.path),\n        })\n    elif url.scheme.startswith('redis'):\n        if url.hostname:\n            scheme = url.scheme.replace('cache', '')\n        else:\n            scheme = 'unix'\n        config['LOCATION'] = (((scheme + '://') + url.netloc) + url.path)\n    if url.query:\n        config_options = {\n            \n        }\n        for (k, v) in urlparse.parse_qs(url.query).items():\n            opt = {\n                k.upper(): _cast_int(v[0]),\n            }\n            if (k.upper() in cls._CACHE_BASE_OPTIONS):\n                config.update(opt)\n            else:\n                config_options.update(opt)\n        config['OPTIONS'] = config_options\n    if backend:\n        config['BACKEND'] = backend\n    return config\n", "label": 1}
{"function": "\n\n@classmethod\ndef wrap(cls, data):\n    should_save = False\n    if ('original_doc' in data):\n        original_doc = data['original_doc']\n        del data['original_doc']\n        should_save = True\n        if original_doc:\n            original_doc = Domain.get_by_name(original_doc)\n            data['copy_history'] = [original_doc._id]\n    if ('license' in data):\n        if (data.get('license', None) == 'public'):\n            data['license'] = 'cc'\n            should_save = True\n    if (('slug' in data) and data['slug']):\n        data['hr_name'] = data['slug']\n        del data['slug']\n    if (('is_test' in data) and isinstance(data['is_test'], bool)):\n        data['is_test'] = ('true' if data['is_test'] else 'false')\n        should_save = True\n    if ('cloudcare_releases' not in data):\n        data['cloudcare_releases'] = 'nostars'\n    if ('location_types' in data):\n        data['obsolete_location_types'] = data.pop('location_types')\n    self = super(Domain, cls).wrap(data)\n    if (self.deployment is None):\n        self.deployment = Deployment()\n    if should_save:\n        self.save()\n    return self\n", "label": 1}
{"function": "\n\ndef select(self, xpath):\n    'Return elements using the webdriver `find_elements_by_xpath` method.\\n\\n        Some XPath features are not supported by the webdriver implementation.\\n        Namely, selecting text content or attributes:\\n          - /some/element/text()\\n          - /some/element/@attribute\\n\\n        This function offers workarounds for both, so it should be safe to use\\n        them as you would with HtmlXPathSelector for simple content extraction.\\n\\n        '\n    xpathev = (self.element if self.element else self.webdriver)\n    ending = _UNSUPPORTED_XPATH_ENDING.match(xpath)\n    atsign = parens = None\n    if ending:\n        (match, atsign, name, parens) = ending.groups()\n        if atsign:\n            xpath = xpath[:((- len(name)) - 2)]\n        elif (parens and (name == 'text')):\n            xpath = xpath[:((- len(name)) - 3)]\n    result = self._make_result(xpathev.find_elements_by_xpath(xpath))\n    if atsign:\n        result = (_NodeAttribute(r.element, name) for r in result)\n    elif (parens and result and (name == 'text')):\n        result = (_TextNode(self.webdriver, r.element) for r in result)\n    return XPathSelectorList(result)\n", "label": 1}
{"function": "\n\ndef _process_files(self, parser, basedir, repository, base_commit_id, request, check_existence=False, limit_to=None):\n    tool = repository.get_scmtool()\n    for f in parser.parse():\n        (source_filename, source_revision) = tool.parse_diff_revision(f.origFile, f.origInfo, moved=f.moved, copied=f.copied)\n        dest_filename = self._normalize_filename(f.newFile, basedir)\n        source_filename = self._normalize_filename(source_filename, basedir)\n        if ((limit_to is not None) and (dest_filename not in limit_to)):\n            continue\n        if ((source_revision != PRE_CREATION) and (source_revision != UNKNOWN) and (not f.binary) and (not f.deleted) and (not f.moved) and (not f.copied) and (check_existence and (not repository.get_file_exists(source_filename, source_revision, base_commit_id=base_commit_id, request=request)))):\n            raise FileNotFoundError(source_filename, source_revision, base_commit_id)\n        f.origFile = source_filename\n        f.origInfo = source_revision\n        f.newFile = dest_filename\n        (yield f)\n", "label": 1}
{"function": "\n\ndef __new__(cls, expr, condition=None, **kwargs):\n    expr = _sympify(expr)\n    if (not kwargs.pop('evaluate', global_evaluate[0])):\n        if (condition is None):\n            obj = Expr.__new__(cls, expr)\n        else:\n            condition = _sympify(condition)\n            obj = Expr.__new__(cls, expr, condition)\n        obj._condition = condition\n        return obj\n    if (not expr.has(RandomSymbol)):\n        return expr\n    if (condition is not None):\n        condition = _sympify(condition)\n    if isinstance(expr, Add):\n        return Add(*[Expectation(a, condition=condition) for a in expr.args])\n    elif isinstance(expr, Mul):\n        rv = []\n        nonrv = []\n        for a in expr.args:\n            if (isinstance(a, RandomSymbol) or a.has(RandomSymbol)):\n                rv.append(a)\n            else:\n                nonrv.append(a)\n        return (Mul(*nonrv) * Expectation(Mul(*rv), condition=condition, evaluate=False))\n    else:\n        if (condition is None):\n            obj = Expr.__new__(cls, expr)\n        else:\n            obj = Expr.__new__(cls, expr, condition)\n        obj._condition = condition\n        return obj\n", "label": 1}
{"function": "\n\ndef find_description(soup):\n    '\\n    '\n    el = soup.find('h3')\n    if (not el):\n        return\n    while (el.name == 'h3'):\n        next_el = el.findNext('h3')\n        if next_el:\n            el = next_el\n        else:\n            break\n    subtitle = el.findNextSibling('b')\n    if subtitle:\n        el = subtitle\n        (yield el.string)\n        el = el.nextSibling.nextSibling\n    el = el.nextSibling\n    while True:\n        if (el.name is not None):\n            return\n        text = el\n        link = el.findNextSibling('a')\n        if link:\n            text += link.string\n            link = link.nextSibling\n            if (link.name is None):\n                text += link.string\n        if text.strip():\n            (yield unicode(text.strip()))\n        for i in range(2):\n            el = el.nextSibling\n            if (not el):\n                return\n            if (el.name != 'br'):\n                break\n        if (el.name == 'br'):\n            el = el.nextSibling\n        if ('Illustrative Examples:' in el):\n            return\n        if ('Cross-References.' in el):\n            return\n", "label": 1}
{"function": "\n\ndef subscribe(self, subscriber, timeout=None):\n    \"Must be used with 'yield', as, for example,\\n        'yield channel.subscribe(coro)'.\\n\\n        Subscribe to receive messages. Senders don't need to\\n        subscribe. A message sent to this channel is delivered to all\\n        subscribers.\\n\\n        Can also be used on remote channels.\\n        \"\n    if ((not isinstance(subscriber, Coro)) and (not isinstance(subscriber, Channel))):\n        logger.warning('invalid subscriber ignored')\n        raise StopIteration((- 1))\n    if (self._location == Channel._asyncoro._location):\n        if (subscriber._location != self._location):\n            if isinstance(subscriber, Coro):\n                subscriber._id = int(subscriber._id)\n                for s in self._subscribers:\n                    if (isinstance(s, Coro) and (s._id == subscriber._id) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n            elif isinstance(subscriber, Channel):\n                for s in self._subscribers:\n                    if (isinstance(s, Channel) and (s._name == subscriber._name) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n        self._subscribers.add(subscriber)\n        (yield self._subscribe_event.set())\n        reply = 0\n    else:\n        kwargs = {\n            'channel': self._name,\n        }\n        kwargs['subscriber'] = subscriber\n        request = _NetRequest('subscribe', kwargs=kwargs, dst=self._location, timeout=timeout)\n        reply = (yield Channel._asyncoro._sync_reply(request))\n    raise StopIteration(reply)\n", "label": 1}
{"function": "\n\ndef _update_defaults(self, defaults):\n    'Updates the given defaults with values from the config files and\\n        the environ. Does a little special handling for certain types of\\n        options (lists).'\n    config = {\n        \n    }\n    for section in ('global', self.name):\n        config.update(self.normalize_keys(self.get_config_section(section)))\n    if (not self.isolated):\n        config.update(self.normalize_keys(self.get_environ_vars()))\n    self.values = optparse.Values(self.defaults)\n    late_eval = set()\n    for (key, val) in config.items():\n        if (not val):\n            continue\n        option = self.get_option(key)\n        if (option is None):\n            continue\n        if (option.action in ('store_true', 'store_false', 'count')):\n            val = strtobool(val)\n        elif (option.action == 'append'):\n            val = val.split()\n            val = [self.check_default(option, key, v) for v in val]\n        elif (option.action == 'callback'):\n            late_eval.add(option.dest)\n            opt_str = option.get_opt_string()\n            val = option.convert_value(opt_str, val)\n            args = (option.callback_args or ())\n            kwargs = (option.callback_kwargs or {\n                \n            })\n            option.callback(option, opt_str, val, self, *args, **kwargs)\n        else:\n            val = self.check_default(option, key, val)\n        defaults[option.dest] = val\n    for key in late_eval:\n        defaults[key] = getattr(self.values, key)\n    self.values = None\n    return defaults\n", "label": 1}
{"function": "\n\ndef _find_candidate_bbs(self, start_address, end_address, mode=BARF_DISASM_RECURSIVE, symbols=None):\n    if (not symbols):\n        symbols = {\n            \n        }\n    bbs = []\n    addrs_to_process = Queue()\n    addrs_processed = set()\n    addrs_to_process.put(start_address)\n    while (not addrs_to_process.empty()):\n        addr_curr = addrs_to_process.get()\n        if ((addr_curr in addrs_processed) or (not ((addr_curr >= start_address) and (addr_curr <= end_address)))):\n            continue\n        bb = self._disassemble_bb(addr_curr, (end_address + 1), symbols)\n        if bb.empty():\n            continue\n        bbs += [bb]\n        addrs_processed.add(addr_curr)\n        if (mode == BARF_DISASM_LINEAR):\n            next_addr = (bb.address + bb.size)\n            if ((not self._bb_ends_in_direct_jmp(bb)) and (not self._bb_ends_in_return(bb)) and (not (next_addr in addrs_processed))):\n                addrs_to_process.put(next_addr)\n        if (mode == BARF_DISASM_RECURSIVE):\n            for (addr, _) in bb.branches:\n                if (not (addr in addrs_processed)):\n                    addrs_to_process.put(addr)\n    return bbs\n", "label": 1}
{"function": "\n\ndef handle(self, request, helper):\n    access_token = helper.fetch_state('data')['access_token']\n    if (self.org is not None):\n        if (not self.client.is_org_member(access_token, self.org['id'])):\n            return helper.error(ERR_NO_ORG_ACCESS)\n    user = self.client.get_user(access_token)\n    if (not user.get('email')):\n        emails = self.client.get_user_emails(access_token)\n        email = [e['email'] for e in emails if (((not REQUIRE_VERIFIED_EMAIL) | e['verified']) and e['primary'])]\n        if (len(email) == 0):\n            if REQUIRE_VERIFIED_EMAIL:\n                msg = ERR_NO_VERIFIED_PRIMARY_EMAIL\n            else:\n                msg = ERR_NO_PRIMARY_EMAIL\n            return helper.error(msg)\n        elif (len(email) > 1):\n            if REQUIRE_VERIFIED_EMAIL:\n                msg = ERR_NO_SINGLE_VERIFIED_PRIMARY_EMAIL\n            else:\n                msg = ERR_NO_SINGLE_PRIMARY_EMAIL\n            return helper.error(msg)\n        else:\n            user['email'] = email[0]\n    if (not user.get('name')):\n        user['name'] = _get_name_from_email(user['email'])\n    helper.bind_state('user', user)\n    return helper.next_step()\n", "label": 1}
{"function": "\n\ndef cut_nodes(graph):\n    '\\n    Return the cut-nodes of the given graph.\\n    \\n    A cut node, or articulation point, is a node of a graph whose removal increases the number of\\n    connected components in the graph.\\n    \\n    @type  graph: graph, hypergraph\\n    @param graph: Graph.\\n        \\n    @rtype:  list\\n    @return: List of cut-nodes.\\n    '\n    recursionlimit = getrecursionlimit()\n    setrecursionlimit(max((len(graph.nodes()) * 2), recursionlimit))\n    if ('hypergraph' == graph.__class__.__name__):\n        return _cut_hypernodes(graph)\n    pre = {\n        \n    }\n    low = {\n        \n    }\n    reply = {\n        \n    }\n    spanning_tree = {\n        \n    }\n    pre[None] = 0\n    for each in graph:\n        if (each not in pre):\n            spanning_tree[each] = None\n            _cut_dfs(graph, spanning_tree, pre, low, [], each)\n    for each in graph:\n        if (spanning_tree[each] is not None):\n            for other in graph[each]:\n                if ((low[other] >= pre[each]) and (spanning_tree[other] == each)):\n                    reply[each] = 1\n        else:\n            children = 0\n            for other in graph:\n                if (spanning_tree[other] == each):\n                    children = (children + 1)\n            if (children >= 2):\n                reply[each] = 1\n    setrecursionlimit(recursionlimit)\n    return list(reply.keys())\n", "label": 1}
{"function": "\n\ndef _mutate_header(self, name, value):\n    header = self._environ.get('HTTP_COOKIE')\n    had_header = (header is not None)\n    header = (header or '')\n    if PY3:\n        header = header.encode('latin-1')\n    bytes_name = bytes_(name, 'ascii')\n    if (value is None):\n        replacement = None\n    else:\n        bytes_val = _quote(bytes_(value, 'utf-8'))\n        replacement = ((bytes_name + b'=') + bytes_val)\n    matches = _rx_cookie.finditer(header)\n    found = False\n    for match in matches:\n        (start, end) = match.span()\n        match_name = match.group(1)\n        if (match_name == bytes_name):\n            found = True\n            if (replacement is None):\n                header = (header[:start].rstrip(b' ;') + header[end:])\n            else:\n                header = ((header[:start] + replacement) + header[end:])\n            break\n    else:\n        if (replacement is not None):\n            if header:\n                header += (b'; ' + replacement)\n            else:\n                header = replacement\n    if header:\n        self._environ['HTTP_COOKIE'] = native_(header, 'latin-1')\n    elif had_header:\n        self._environ['HTTP_COOKIE'] = ''\n    return found\n", "label": 1}
{"function": "\n\ndef CKY(pcfg, norm_words):\n    (x, n) = (([('', '')] + norm_words), len(norm_words))\n    pi = defaultdict(float)\n    bp = defaultdict(tuple)\n    for i in range(1, (n + 1)):\n        for X in pcfg.N:\n            (norm, word) = x[i]\n            if ((X, norm) in pcfg.q1):\n                pi[(i, i, X)] = pcfg.q1[(X, norm)]\n                bp[(i, i, X)] = (X, word, i, i)\n    for l in range(1, n):\n        for i in range(1, ((n - l) + 1)):\n            j = (i + l)\n            for X in pcfg.N:\n                (score, back) = argmax([(((pcfg.q2[(X, Y, Z)] * pi[(i, s, Y)]) * pi[((s + 1), j, Z)]), (X, Y, Z, i, s, j)) for s in range(i, j) for (Y, Z) in pcfg.binary_rules[X] if (pi[(i, s, Y)] > 0.0) if (pi[((s + 1), j, Z)] > 0.0)])\n                if (score > 0.0):\n                    (bp[(i, j, X)], pi[(i, j, X)]) = (back, score)\n    (_, top) = max([(pi[(1, n, X)], bp[(1, n, X)]) for X in pcfg.N])\n    return backtrace(top, bp)\n", "label": 1}
{"function": "\n\n@flake8ext\ndef check_builtins_gettext(logical_line, tokens, filename, lines, noqa):\n    \"Check usage of builtins gettext _().\\n\\n    Okay(neutron/foo.py): from neutron._i18n import _\\n_('foo')\\n    N341(neutron/foo.py): _('foo')\\n    Okay(neutron/_i18n.py): _('foo')\\n    Okay(neutron/i18n.py): _('foo')\\n    Okay(neutron/foo.py): _('foo')  # noqa\\n    \"\n    if noqa:\n        return\n    modulename = os.path.normpath(filename).split('/')[0]\n    if (('%s/tests' % modulename) in filename):\n        return\n    if (os.path.basename(filename) in ('i18n.py', '_i18n.py')):\n        return\n    token_values = [t[1] for t in tokens]\n    i18n_wrapper = ('%s._i18n' % modulename)\n    if ('_' in token_values):\n        i18n_import_line_found = False\n        for line in lines:\n            split_line = [elm.rstrip(',') for elm in line.split()]\n            if ((len(split_line) > 1) and (split_line[0] == 'from') and (split_line[1] == i18n_wrapper) and ('_' in split_line)):\n                i18n_import_line_found = True\n                break\n        if (not i18n_import_line_found):\n            msg = ('N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper)\n            (yield (0, msg))\n", "label": 1}
{"function": "\n\ndef pythonvaluetotime(time_val):\n    'Convert a time or time range from Python datetime to ArcGIS REST server'\n    if (time_val is None):\n        return None\n    elif isinstance(time_val, numeric):\n        return str(long((time_val * 1000.0)))\n    elif isinstance(time_val, date):\n        dtlist = [time_val.year, time_val.month, time_val.day]\n        if isinstance(time_val, datetime.datetime):\n            dtlist += [time_val.hour, time_val.minute, time_val.second]\n        else:\n            dtlist += [0, 0, 0]\n        return long((calendar.timegm(dtlist) * 1000.0))\n    elif (isinstance(time_val, sequence) and (len(time_val) == 2)):\n        if all((isinstance(x, numeric) for x in time_val)):\n            return ','.join((pythonvaluetotime(x) for x in time_val))\n        elif all((isinstance(x, date) for x in time_val)):\n            return ','.join((pythonvaluetotime(x) for x in time_val))\n    raise ValueError(repr(time_val))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.query_id = QueryHandle()\n                self.query_id.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.BOOL):\n                self.start_over = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.fetch_size = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_lexer_for_filename(_fn, code=None, **options):\n    '\\n    Get a lexer for a filename.  If multiple lexers match the filename\\n    pattern, use ``analyze_text()`` to figure out which one is more\\n    appropriate.\\n    '\n    matches = []\n    fn = basename(_fn)\n    for (modname, name, _, filenames, _) in LEXERS.itervalues():\n        for filename in filenames:\n            if fnmatch.fnmatch(fn, filename):\n                if (name not in _lexer_cache):\n                    _load_lexers(modname)\n                matches.append(_lexer_cache[name])\n    for cls in find_plugin_lexers():\n        for filename in cls.filenames:\n            if fnmatch.fnmatch(fn, filename):\n                matches.append(cls)\n    if ((sys.version_info > (3,)) and isinstance(code, bytes)):\n        code = code.decode('latin1')\n\n    def get_rating(cls):\n        d = cls.analyse_text(code)\n        return d\n    if code:\n        matches.sort(key=get_rating)\n    if matches:\n        return matches[(- 1)](**options)\n    raise ClassNotFound(('no lexer for filename %r found' % _fn))\n", "label": 1}
{"function": "\n\ndef versions_from_expanded_variables(variables, tag_prefix, verbose=False):\n    refnames = variables['refnames'].strip()\n    if refnames.startswith('$Format'):\n        if verbose:\n            print('variables are unexpanded, not using')\n        return {\n            \n        }\n    refs = set([r.strip() for r in refnames.strip('()').split(',')])\n    TAG = 'tag: '\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if (not tags):\n        tags = set([r for r in refs if re.search('\\\\d', r)])\n        if verbose:\n            print((\"discarding '%s', no digits\" % ','.join((refs - tags))))\n    if verbose:\n        print(('likely tags: %s' % ','.join(sorted(tags))))\n    for ref in sorted(tags):\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(('picking %s' % r))\n            return {\n                'version': r,\n                'full': variables['full'].strip(),\n            }\n    if verbose:\n        print('no suitable tags, using full revision id')\n    return {\n        'version': variables['full'].strip(),\n        'full': variables['full'].strip(),\n    }\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.finish = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.reversed = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef message_attr(self):\n    '\\n        Returns options dict as sent within WAMP messages.\\n        '\n    options = {\n        \n    }\n    if (self.acknowledge is not None):\n        options['acknowledge'] = self.acknowledge\n    if (self.exclude_me is not None):\n        options['exclude_me'] = self.exclude_me\n    if (self.exclude is not None):\n        options['exclude'] = (self.exclude if (type(self.exclude) == list) else [self.exclude])\n    if (self.exclude_authid is not None):\n        options['exclude_authid'] = (self.exclude_authid if (type(self.exclude_authid) == list) else self.exclude_authid)\n    if (self.exclude_authrole is not None):\n        options['exclude_authrole'] = (self.exclude_authrole if (type(self.exclude_authrole) == list) else self.exclude_authrole)\n    if (self.eligible is not None):\n        options['eligible'] = (self.eligible if (type(self.eligible) == list) else self.eligible)\n    if (self.eligible_authid is not None):\n        options['eligible_authid'] = (self.eligible_authid if (type(self.eligible_authid) == list) else self.eligible_authid)\n    if (self.eligible_authrole is not None):\n        options['eligible_authrole'] = (self.eligible_authrole if (type(self.eligible_authrole) == list) else self.eligible_authrole)\n    return options\n", "label": 1}
{"function": "\n\ndef _search_ds(self, method, *args, **kwargs):\n    'Searches the datastore for a file.'\n    ds_path = kwargs.get('datastorePath')\n    matched_files = set()\n    directory = False\n    dname = ('%s/' % ds_path)\n    for file in _db_content.get('files'):\n        if (file == dname):\n            directory = True\n            break\n    if directory:\n        for file in _db_content.get('files'):\n            if (file.find(ds_path) != (- 1)):\n                if (not file.endswith(ds_path)):\n                    path = file.replace(dname, '', 1).split('/')\n                    if path:\n                        matched_files.add(path[0])\n        if (not matched_files):\n            matched_files.add('/')\n    else:\n        for file in _db_content.get('files'):\n            if (file.find(ds_path) != (- 1)):\n                matched_files.add(ds_path)\n    if matched_files:\n        result = DataObject()\n        result.path = ds_path\n        result.file = []\n        for file in matched_files:\n            matched = DataObject()\n            matched.path = file\n            matched.fileSize = 1024\n            result.file.append(matched)\n        task_mdo = create_task(method, 'success', result=result)\n    else:\n        task_mdo = create_task(method, 'error', error_fault=FileNotFound())\n    return task_mdo.obj\n", "label": 1}
{"function": "\n\ndef create_index(self, asset_files=False):\n    logging.info('Creating new assets index...')\n    if (not asset_files):\n        asset_files = self.find_assets()\n    blueprints = Blueprints(self)\n    items = Items(self)\n    species = Species(self)\n    monsters = Monsters(self)\n    techs = Techs(self)\n    frames = Frames(self)\n    new_index_query = 'insert into assets values (?, ?, ?, ?, ?, ?)'\n    c = self.db.cursor()\n    for asset in asset_files:\n        (yield (asset[0], asset[1]))\n        tmp_data = None\n        if (asset_category(asset[0]) != ''):\n            if asset[0].endswith('.png'):\n                tmp_data = (asset[0], asset[1], 'image', '', '', '')\n            elif blueprints.is_blueprint(asset[0]):\n                tmp_data = blueprints.index_data(asset)\n            elif species.is_species(asset[0]):\n                tmp_data = species.index_data(asset)\n            elif items.is_item(asset[0]):\n                tmp_data = items.index_data(asset)\n            elif monsters.is_monster(asset[0]):\n                tmp_data = monsters.index_data(asset)\n            elif techs.is_tech(asset[0]):\n                tmp_data = techs.index_data(asset)\n            elif frames.is_frames(asset[0]):\n                tmp_data = frames.index_data(asset)\n        else:\n            logging.warning(('Skipping invalid asset (no file extension) %s in %s' % (asset[0], asset[1])))\n        if (tmp_data is not None):\n            c.execute(new_index_query, tmp_data)\n    self.db.commit()\n    logging.info('Finished creating index')\n", "label": 1}
{"function": "\n\ndef django_ordering_comparison(ordering, lhs, rhs):\n    if (not ordering):\n        return (- 1)\n    ASCENDING = 1\n    DESCENDING = 2\n    for (order, direction) in ordering:\n        if (lhs is not None):\n            lhs_value = (lhs.key() if (order == '__key__') else lhs.get(order))\n        else:\n            lhs_value = None\n        if (rhs is not None):\n            rhs_value = (rhs.key() if (order == '__key__') else rhs.get(order))\n        else:\n            rhs_value = None\n        if ((direction == ASCENDING) and (lhs_value != rhs_value)):\n            return ((- 1) if lt(lhs_value, rhs_value) else 1)\n        elif ((direction == DESCENDING) and (lhs_value != rhs_value)):\n            return (1 if lt(lhs_value, rhs_value) else (- 1))\n    return 0\n", "label": 1}
{"function": "\n\ndef _replace_heap(variable, heap):\n    if isinstance(variable, Pointer):\n        while isinstance(variable, Pointer):\n            if (variable.index == 0):\n                variable = None\n            elif (variable.index in heap):\n                variable = heap[variable.index]\n            else:\n                warnings.warn('Variable referenced by pointer not found in heap: variable will be set to None')\n                variable = None\n        (replace, new) = _replace_heap(variable, heap)\n        if replace:\n            variable = new\n        return (True, variable)\n    elif isinstance(variable, np.core.records.recarray):\n        for (ir, record) in enumerate(variable):\n            (replace, new) = _replace_heap(record, heap)\n            if replace:\n                variable[ir] = new\n        return (False, variable)\n    elif isinstance(variable, np.core.records.record):\n        for (iv, value) in enumerate(variable):\n            (replace, new) = _replace_heap(value, heap)\n            if replace:\n                variable[iv] = new\n        return (False, variable)\n    elif isinstance(variable, np.ndarray):\n        if (variable.dtype.type is np.object_):\n            for iv in range(variable.size):\n                (replace, new) = _replace_heap(variable.item(iv), heap)\n                if replace:\n                    variable.itemset(iv, new)\n        return (False, variable)\n    else:\n        return (False, variable)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I64):\n                self.totalMemory = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I64):\n                self.freeMemory = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.maxMemory = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef clone(self, chrom=None, start=None, end=None, name=None, score=None, strand=None, thickStart=None, thickEnd=None, rgb=None, *args):\n    cols = []\n    cols.append((self.chrom if (chrom is None) else chrom))\n    cols.append((self.start if (start is None) else start))\n    cols.append((self.end if (end is None) else end))\n    cols.append((self.name if (name is None) else name))\n    cols.append((self.score if (score is None) else score))\n    cols.append((self.strand if (strand is None) else strand))\n    cols.append((self.thickStart if (thickStart is None) else thickStart))\n    cols.append((self.thickEnd if (thickEnd is None) else thickEnd))\n    cols.append((self.rgb if (rgb is None) else rgb))\n    for (i, val) in enumerate(self.extras):\n        if (len(args) > i):\n            cols.append(args[i])\n        else:\n            cols.append(val)\n    return BedRegion(*cols)\n", "label": 1}
{"function": "\n\ndef _parse_settings(self, parser):\n    if (not parser.has_section('server')):\n        raise exceptions.ConfigurationError(\"The server configuration file does not have a 'server' section.\")\n    settings = [x[0] for x in parser.items('server')]\n    for setting in settings:\n        if (setting not in self._expected_settings):\n            raise exceptions.ConfigurationError(\"Setting '{0}' is not a supported setting. Please remove it from the configuration file.\".format(setting))\n    for setting in self._expected_settings:\n        if (setting not in settings):\n            raise exceptions.ConfigurationError(\"Setting '{0}' is missing from the configuration file.\".format(setting))\n    if parser.has_option('server', 'hostname'):\n        self._set_hostname(parser.get('server', 'hostname'))\n    if parser.has_option('server', 'port'):\n        self._set_port(parser.getint('server', 'port'))\n    if parser.has_option('server', 'certificate_path'):\n        self._set_certificate_path(parser.get('server', 'certificate_path'))\n    if parser.has_option('server', 'key_path'):\n        self._set_key_path(parser.get('server', 'key_path'))\n    if parser.has_option('server', 'ca_path'):\n        self._set_ca_path(parser.get('server', 'ca_path'))\n    if parser.has_option('server', 'auth_suite'):\n        self._set_auth_suite(parser.get('server', 'auth_suite'))\n", "label": 1}
{"function": "\n\ndef _HandleImports(self, element, import_manager=None):\n    'Handles imports for the specified element.\\n\\n    Args:\\n      element: (Property|Parameter) The property we want to set the import for.\\n      import_manager: The import manager to import into if not the implied one.\\n\\n    '\n    element = getattr(element, 'referenced_schema', element)\n    if isinstance(element, data_types.ComplexDataType):\n        parent = element\n        data_type = element\n    else:\n        parent = element.schema\n        data_type = element.data_type\n    data_type = getattr(data_type, 'referenced_schema', data_type)\n    if (not import_manager):\n        if self._InnerModelClassesSupported():\n            while parent.parent:\n                parent = parent.parent\n        import_manager = cpp_import_manager.CppImportManager.ForElement(parent)\n    while isinstance(data_type, (data_types.ArrayDataType, data_types.MapDataType)):\n        data_type = data_type._base_type\n        data_type = getattr(data_type, 'referenced_schema', data_type)\n    if (not data_type):\n        return\n    json_type = data_type.json_type\n    json_format = data_type.values.get('format')\n    if (json_type == 'object'):\n        if (not data_type.parent):\n            import_manager.AddDataType(data_type)\n        return\n    datatype_and_imports = self.language_model.type_map.get((json_type, json_format))\n    if datatype_and_imports:\n        import_definition = datatype_and_imports[1]\n        for required_import in import_definition.imports:\n            if required_import:\n                import_manager.AddImport(required_import)\n        for template_value in import_definition.template_values:\n            element.SetTemplateValue(template_value, True)\n    elif data_type:\n        if (not data_type.parent):\n            import_manager.AddDataType(data_type)\n    return\n", "label": 1}
{"function": "\n\ndef optimize(self):\n    'Optimize certain stacked wildcard patterns.'\n    subpattern = None\n    if ((self.content is not None) and (len(self.content) == 1) and (len(self.content[0]) == 1)):\n        subpattern = self.content[0][0]\n    if ((self.min == 1) and (self.max == 1)):\n        if (self.content is None):\n            return NodePattern(name=self.name)\n        if ((subpattern is not None) and (self.name == subpattern.name)):\n            return subpattern.optimize()\n    if ((self.min <= 1) and isinstance(subpattern, WildcardPattern) and (subpattern.min <= 1) and (self.name == subpattern.name)):\n        return WildcardPattern(subpattern.content, (self.min * subpattern.min), (self.max * subpattern.max), subpattern.name)\n    return self\n", "label": 1}
{"function": "\n\ndef xml_elements_equal(element1, element2, ignore_level1_cdata=False):\n    'Check if two XML elements are equal.\\n\\n    :Parameters:\\n        - `element1`: the first element to compare\\n        - `element2`: the other element to compare\\n        - `ignore_level1_cdata`: if direct text children of the elements\\n          should be ignored for the comparision\\n    :Types:\\n        - `element1`: :etree:`ElementTree.Element`\\n        - `element2`: :etree:`ElementTree.Element`\\n        - `ignore_level1_cdata`: `bool`\\n\\n    :Returntype: `bool`\\n    '\n    if ((None in (element1, element2)) or (element1.tag != element2.tag)):\n        return False\n    attrs1 = list(element1.items())\n    attrs1.sort()\n    attrs2 = list(element2.items())\n    attrs2.sort()\n    if (not ignore_level1_cdata):\n        if (element1.text != element2.text):\n            return False\n    if (attrs1 != attrs2):\n        return False\n    if (len(element1) != len(element2)):\n        return False\n    for (child1, child2) in zip(element1, element2):\n        if (child1.tag != child2.tag):\n            return False\n        if (not ignore_level1_cdata):\n            if (element1.text != element2.text):\n                return False\n        if (not xml_elements_equal(child1, child2)):\n            return False\n    return True\n", "label": 1}
{"function": "\n\n@get('/s/users/{user_id}')\ndef get_user_route(request, user_id):\n    '\\n    Get the user by their ID.\\n    '\n    db_conn = request['db_conn']\n    user = User.get(db_conn, id=user_id)\n    current_user = get_current_user(request)\n    if (not user):\n        return abort(404)\n    data = {\n        \n    }\n    data['user'] = user.deliver(access=('private' if (current_user and (user['id'] == current_user['id'])) else None))\n    if ('posts' in request['params']):\n        data['posts'] = [post.deliver() for post in get_posts_facade(db_conn, user_id=user['id'])]\n    if (('sets' in request['params']) and (user['settings']['view_sets'] == 'public')):\n        u_sets = UserSets.get(db_conn, user_id=user['id'])\n        data['sets'] = [set_.deliver() for set_ in u_sets.list_sets(db_conn)]\n    if (('follows' in request['params']) and (user['settings']['view_follows'] == 'public')):\n        data['follows'] = [follow.deliver() for follow in Follow.list(db_conn, user_id=user['id'])]\n    if ('avatar' in request['params']):\n        size = int(request['params']['avatar'])\n        data['avatar'] = user.get_avatar((size if size else None))\n    return (200, data)\n", "label": 1}
{"function": "\n\ndef uninstall(pkg, package_name, remove_all, app_id, cli, app):\n    'Uninstalls a package.\\n\\n    :param pkg: package manager to uninstall with\\n    :type pkg: PackageManager\\n    :param package_name: The package to uninstall\\n    :type package_name: str\\n    :param remove_all: Whether to remove all instances of the named app\\n    :type remove_all: boolean\\n    :param app_id: App ID of the app instance to uninstall\\n    :type app_id: str\\n    :param init_client: The program to use to run the app\\n    :type init_client: object\\n    :rtype: None\\n    '\n    if ((cli is False) and (app is False)):\n        cli = app = True\n    uninstalled = False\n    installed = installed_packages(pkg, app_id, package_name)\n    installed_cli = next((True for installed_pkg in installed if installed_pkg.get('command')), False)\n    installed_app = next((True for installed_pkg in installed if installed_pkg.get('apps')), False)\n    if (cli and installed_cli):\n        if subcommand.uninstall(package_name):\n            uninstalled = True\n    if (app and installed_app):\n        if pkg.uninstall_app(package_name, remove_all, app_id):\n            uninstalled = True\n    if uninstalled:\n        return None\n    else:\n        msg = 'Package [{}]'.format(package_name)\n        if (app_id is not None):\n            app_id = util.normalize_app_id(app_id)\n            msg += ' with id [{}]'.format(app_id)\n        msg += ' is not installed'\n        raise DCOSException(msg)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.token = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.interval = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.valid_until = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _set_mime_headers(self, headers):\n    for (name, value) in headers:\n        name = name.lower()\n        if (name == 'project-id-version'):\n            parts = value.split(' ')\n            self.project = ' '.join(parts[:(- 1)])\n            self.version = parts[(- 1)]\n        elif (name == 'report-msgid-bugs-to'):\n            self.msgid_bugs_address = value\n        elif (name == 'last-translator'):\n            self.last_translator = value\n        elif (name == 'language-team'):\n            self.language_team = value\n        elif (name == 'content-type'):\n            (mimetype, params) = parse_header(value)\n            if ('charset' in params):\n                self.charset = params['charset'].lower()\n        elif (name == 'plural-forms'):\n            (_, params) = parse_header((' ;' + value))\n            self._num_plurals = int(params.get('nplurals', 2))\n            self._plural_expr = params.get('plural', '(n != 1)')\n        elif (name == 'pot-creation-date'):\n            self.creation_date = _parse_datetime_header(value)\n        elif (name == 'po-revision-date'):\n            if ('YEAR' not in value):\n                self.revision_date = _parse_datetime_header(value)\n", "label": 1}
{"function": "\n\ndef _updateJobProgress(self, job, total, current, message, notify):\n    'Helper for updating job progress information.'\n    state = JobStatus.toNotificationStatus(job['status'])\n    if (current is not None):\n        current = float(current)\n    if (total is not None):\n        total = float(total)\n    if (job['progress'] is None):\n        if (notify and job['userId']):\n            notification = self._createProgressNotification(job, total, current, state, message)\n            notificationId = notification['_id']\n        else:\n            notificationId = None\n        job['progress'] = {\n            'message': message,\n            'total': total,\n            'current': current,\n            'notificationId': notificationId,\n        }\n    else:\n        if (total is not None):\n            job['progress']['total'] = total\n        if (current is not None):\n            job['progress']['current'] = current\n        if (message is not None):\n            job['progress']['message'] = message\n        if (notify and job['userId']):\n            if (job['progress']['notificationId'] is None):\n                notification = self._createProgressNotification(job, total, current, state, message)\n                job['progress']['notificationId'] = notification['_id']\n                self.save(job)\n            else:\n                notification = self.model('notification').load(job['progress']['notificationId'])\n            self.model('notification').updateProgress(notification, state=state, message=job['progress']['message'], current=job['progress']['current'], total=job['progress']['total'])\n", "label": 1}
{"function": "\n\ndef get_cors_headers(options, request_headers, request_method, response_headers):\n    origin_to_set = get_cors_origin(options, request_headers.get('Origin'))\n    headers = MultiDict()\n    if (origin_to_set is None):\n        return headers\n    headers[ACL_ORIGIN] = origin_to_set\n    headers[ACL_EXPOSE_HEADERS] = options.get('expose_headers')\n    if options.get('supports_credentials'):\n        headers[ACL_CREDENTIALS] = 'true'\n    if (request_method == 'OPTIONS'):\n        acl_request_method = request_headers.get(ACL_REQUEST_METHOD, '').upper()\n        if (acl_request_method and (acl_request_method in options.get('methods'))):\n            headers[ACL_ALLOW_HEADERS] = get_allow_headers(options, request_headers.get(ACL_REQUEST_HEADERS))\n            headers[ACL_MAX_AGE] = options.get('max_age')\n            headers[ACL_METHODS] = options.get('methods')\n        else:\n            LOG.info(\"The request's Access-Control-Request-Method header does not match allowed methods. CORS headers will not be applied.\")\n    if options.get('vary_header'):\n        if (headers[ACL_ORIGIN] == '*'):\n            pass\n        elif ((len(options.get('origins')) > 1) or any(map(probably_regex, options.get('origins')))):\n            headers.add('Vary', 'Origin')\n    return MultiDict(((k, v) for (k, v) in headers.items() if v))\n", "label": 1}
{"function": "\n\ndef collect(self):\n    metrics = {\n        \n    }\n    for filepath in self.PROC:\n        if (not os.access(filepath, os.R_OK)):\n            self.log.error('Permission to access %s denied', filepath)\n            continue\n        header = ''\n        data = ''\n        file = open(filepath)\n        if (not file):\n            self.log.error('Failed to open %s', filepath)\n            continue\n        while True:\n            line = file.readline()\n            if (len(line) == 0):\n                break\n            if line.startswith('Udp'):\n                header = line\n                data = file.readline()\n                break\n        file.close()\n        if ((header == '') or (data == '')):\n            self.log.error('%s has no lines with Udp', filepath)\n            continue\n        header = header.split()\n        data = data.split()\n        for i in xrange(1, len(header)):\n            metrics[header[i]] = data[i]\n    for metric_name in metrics.keys():\n        if ((len(self.config['allowed_names']) > 0) and (metric_name not in self.config['allowed_names'])):\n            continue\n        value = metrics[metric_name]\n        value = self.derivative(metric_name, long(value))\n        self.publish(metric_name, value, 0)\n", "label": 1}
{"function": "\n\ndef nodes_for_spec(self, spec):\n    '\\n            Determine nodes for an input_algorithms spec\\n            Taking into account nested specs\\n        '\n    tokens = []\n    if isinstance(spec, sb.create_spec):\n        container = nodes.container(classes=['option_spec_option shortline blue-back'])\n        creates = spec.kls\n        for (name, option) in sorted(spec.kwargs.items(), key=(lambda x: len(x[0]))):\n            para = nodes.paragraph(classes=['option monospaced'])\n            para += nodes.Text('{0} = '.format(name))\n            self.nodes_for_signature(option, para)\n            fields = {\n                \n            }\n            if (creates and hasattr(creates, 'fields') and isinstance(creates.fields, dict)):\n                for (key, val) in creates.fields.items():\n                    if isinstance(key, tuple):\n                        fields[key[0]] = val\n                    else:\n                        fields[key] = val\n            txt = (fields.get(name) or 'No description')\n            viewlist = ViewList()\n            for line in dedent(txt).split('\\n'):\n                viewlist.append(line, name)\n            desc = nodes.section(classes=['description monospaced'])\n            self.state.nested_parse(viewlist, self.content_offset, desc)\n            container += para\n            container += desc\n            container.extend(self.nodes_for_spec(option))\n        tokens.append(container)\n    elif isinstance(spec, sb.optional_spec):\n        tokens.extend(self.nodes_for_spec(spec.spec))\n    elif isinstance(spec, sb.container_spec):\n        tokens.extend(self.nodes_for_spec(spec.spec))\n    elif isinstance(spec, sb.dictof):\n        tokens.extend(self.nodes_for_spec(spec.value_spec))\n    return tokens\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I64):\n                self.timestamp = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.super_column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.predicate = SlicePredicate()\n                self.predicate.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _LDL_sparse(self):\n    'Algorithm for numeric LDL factization, exploiting sparse structure.\\n        '\n    Lrowstruc = self.row_structure_symbolic_cholesky()\n    L = self.eye(self.rows)\n    D = self.zeros(self.rows, self.cols)\n    for i in range(len(Lrowstruc)):\n        for j in Lrowstruc[i]:\n            if (i != j):\n                L[(i, j)] = self[(i, j)]\n                summ = 0\n                for p1 in Lrowstruc[i]:\n                    if (p1 < j):\n                        for p2 in Lrowstruc[j]:\n                            if (p2 < j):\n                                if (p1 == p2):\n                                    summ += ((L[(i, p1)] * L[(j, p1)]) * D[(p1, p1)])\n                            else:\n                                break\n                    else:\n                        break\n                L[(i, j)] -= summ\n                L[(i, j)] /= D[(j, j)]\n            elif (i == j):\n                D[(i, i)] = self[(i, i)]\n                summ = 0\n                for k in Lrowstruc[i]:\n                    if (k < i):\n                        summ += ((L[(i, k)] ** 2) * D[(k, k)])\n                    else:\n                        break\n                D[(i, i)] -= summ\n    return (L, D)\n", "label": 1}
{"function": "\n\ndef _inverse_binarize_thresholding(y, output_type, classes, threshold):\n    'Inverse label binarization transformation using thresholding.'\n    if ((output_type == 'binary') and (y.ndim == 2) and (y.shape[1] > 2)):\n        raise ValueError(\"output_type='binary', but y.shape = {0}\".format(y.shape))\n    if ((output_type != 'binary') and (y.shape[1] != len(classes))):\n        raise ValueError('The number of class is not equal to the number of dimension of y.')\n    classes = np.asarray(classes)\n    if sp.issparse(y):\n        if (threshold > 0):\n            if (y.format not in ('csr', 'csc')):\n                y = y.tocsr()\n            y.data = np.array((y.data > threshold), dtype=np.int)\n            y.eliminate_zeros()\n        else:\n            y = np.array((y.toarray() > threshold), dtype=np.int)\n    else:\n        y = np.array((y > threshold), dtype=np.int)\n    if (output_type == 'binary'):\n        if sp.issparse(y):\n            y = y.toarray()\n        if ((y.ndim == 2) and (y.shape[1] == 2)):\n            return classes[y[:, 1]]\n        elif (len(classes) == 1):\n            return np.repeat(classes[0], len(y))\n        else:\n            return classes[y.ravel()]\n    elif (output_type == 'multilabel-indicator'):\n        return y\n    else:\n        raise ValueError('{0} format is not supported'.format(output_type))\n", "label": 1}
{"function": "\n\ndef _populate_config_resource(self, configuration):\n    'Helper for _build_resource: copy config properties to resource'\n    if (self.allow_jagged_rows is not None):\n        configuration['allowJaggedRows'] = self.allow_jagged_rows\n    if (self.allow_quoted_newlines is not None):\n        configuration['allowQuotedNewlines'] = self.allow_quoted_newlines\n    if (self.create_disposition is not None):\n        configuration['createDisposition'] = self.create_disposition\n    if (self.encoding is not None):\n        configuration['encoding'] = self.encoding\n    if (self.field_delimiter is not None):\n        configuration['fieldDelimiter'] = self.field_delimiter\n    if (self.ignore_unknown_values is not None):\n        configuration['ignoreUnknownValues'] = self.ignore_unknown_values\n    if (self.max_bad_records is not None):\n        configuration['maxBadRecords'] = self.max_bad_records\n    if (self.quote_character is not None):\n        configuration['quote'] = self.quote_character\n    if (self.skip_leading_rows is not None):\n        configuration['skipLeadingRows'] = self.skip_leading_rows\n    if (self.source_format is not None):\n        configuration['sourceFormat'] = self.source_format\n    if (self.write_disposition is not None):\n        configuration['writeDisposition'] = self.write_disposition\n", "label": 1}
{"function": "\n\ndef _check_ellipsis(self, index):\n    'Process indices with Ellipsis. Returns modified index.'\n    if (index is Ellipsis):\n        return (slice(None), slice(None))\n    elif isinstance(index, tuple):\n        for (j, v) in enumerate(index):\n            if (v is Ellipsis):\n                first_ellipsis = j\n                break\n        else:\n            first_ellipsis = None\n        if (first_ellipsis is not None):\n            if (len(index) == 1):\n                return (slice(None), slice(None))\n            elif (len(index) == 2):\n                if (first_ellipsis == 0):\n                    if (index[1] is Ellipsis):\n                        return (slice(None), slice(None))\n                    else:\n                        return (slice(None), index[1])\n                else:\n                    return (index[0], slice(None))\n            tail = ()\n            for v in index[(first_ellipsis + 1):]:\n                if (v is not Ellipsis):\n                    tail = (tail + (v,))\n            nd = (first_ellipsis + len(tail))\n            nslice = max(0, (2 - nd))\n            return ((index[:first_ellipsis] + ((slice(None),) * nslice)) + tail)\n    return index\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.column_name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.op = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.value = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _parse_status_report(self, proto):\n    unpack = proto.read_pkt_line().strip()\n    if (unpack != 'unpack ok'):\n        st = True\n        while (st is not None):\n            st = proto.read_pkt_line()\n        raise SendPackError(unpack)\n    statuses = []\n    errs = False\n    ref_status = proto.read_pkt_line()\n    while ref_status:\n        ref_status = ref_status.strip()\n        statuses.append(ref_status)\n        if (not ref_status.startswith('ok ')):\n            errs = True\n        ref_status = proto.read_pkt_line()\n    if errs:\n        ref_status = {\n            \n        }\n        ok = set()\n        for status in statuses:\n            if (' ' not in status):\n                continue\n            (status, ref) = status.split(' ', 1)\n            if (status == 'ng'):\n                if (' ' in ref):\n                    (ref, status) = ref.split(' ', 1)\n            else:\n                ok.add(ref)\n            ref_status[ref] = status\n        raise UpdateRefsError(('%s failed to update' % ', '.join([ref for ref in ref_status if (ref not in ok)])), ref_status=ref_status)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.url = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.bytes = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __getitem__(self, lst):\n    if (isinstance(lst, tuple) and (len(lst) < 5) and any(((Ellipsis is x) for x in lst))):\n        if ((len(lst) == 2) and (lst[1] is Ellipsis)):\n            return enumFrom(lst[0])\n        elif ((len(lst) == 3) and (lst[2] is Ellipsis)):\n            return enumFromThen(lst[0], lst[1])\n        elif ((len(lst) == 3) and (lst[1] is Ellipsis)):\n            return enumFromTo(lst[0], lst[2])\n        elif ((len(lst) == 4) and (lst[2] is Ellipsis)):\n            return enumFromThenTo(lst[0], lst[1], lst[3])\n        raise SyntaxError(('Invalid list comprehension: %s' % str(lst)))\n    elif (hasattr(lst, 'next') or hasattr(lst, '__next__')):\n        return List(tail=lst)\n    return List(head=lst)\n", "label": 1}
{"function": "\n\ndef parse_weka_output(self, lines):\n    for (i, line) in enumerate(lines):\n        if line.strip().startswith('inst#'):\n            lines = lines[i:]\n            break\n    if (lines[0].split() == ['inst#', 'actual', 'predicted', 'error', 'prediction']):\n        return [line.split()[2].split(':')[1] for line in lines[1:] if line.strip()]\n    elif (lines[0].split() == ['inst#', 'actual', 'predicted', 'error', 'distribution']):\n        return [self.parse_weka_distribution(line.split()[(- 1)]) for line in lines[1:] if line.strip()]\n    elif re.match('^0 \\\\w+ [01]\\\\.[0-9]* \\\\?\\\\s*$', lines[0]):\n        return [line.split()[1] for line in lines if line.strip()]\n    else:\n        for line in lines[:10]:\n            print(line)\n        raise ValueError(('Unhandled output format -- your version of weka may not be supported.\\n  Header: %s' % lines[0]))\n", "label": 1}
{"function": "\n\ndef __enter__(self):\n    'Perform the patch.'\n    (new, spec) = (self.new, self.spec)\n    (spec_set, autospec) = (self.spec_set, self.autospec)\n    kwargs = self.kwargs\n    (original, local) = self.get_original()\n    if ((new is DEFAULT) and (autospec is False)):\n        inherit = False\n        if (spec_set == True):\n            spec_set = original\n            if isinstance(spec_set, ClassTypes):\n                inherit = True\n        elif (spec == True):\n            spec = original\n            if isinstance(spec, ClassTypes):\n                inherit = True\n        new = MagicMock(spec=spec, spec_set=spec_set, **kwargs)\n        if inherit:\n            new.return_value = Mock(spec=spec, spec_set=spec_set)\n    elif (autospec is not False):\n        if (new is not DEFAULT):\n            raise TypeError(\"autospec creates the mock for you. Can't specify autospec and new.\")\n        spec_set = bool(spec_set)\n        _kwargs = {\n            '_name': getattr(original, '__name__', None),\n        }\n        if (autospec is True):\n            autospec = original\n        new = create_autospec(autospec, spec_set, inherit=True, configure=kwargs, **_kwargs)\n    elif self.kwargs:\n        raise TypeError(\"Can't pass kwargs to a mock we aren't creating\")\n    new_attr = new\n    if self.mocksignature:\n        new_attr = mocksignature(original, new)\n    self.temp_original = original\n    self.is_local = local\n    setattr(self.target, self.attribute, new_attr)\n    return new\n", "label": 1}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.set_content(d.getPrefixedString())\n            continue\n        if (tt == 16):\n            self.set_statuscode(d.getVarInt32())\n            continue\n        if (tt == 27):\n            self.add_header().TryMerge(d)\n            continue\n        if (tt == 48):\n            self.set_contentwastruncated(d.getBoolean())\n            continue\n        if (tt == 56):\n            self.set_externalbytessent(d.getVarInt64())\n            continue\n        if (tt == 64):\n            self.set_externalbytesreceived(d.getVarInt64())\n            continue\n        if (tt == 74):\n            self.set_finalurl(d.getPrefixedString())\n            continue\n        if (tt == 80):\n            self.set_apicpumilliseconds(d.getVarInt64())\n            continue\n        if (tt == 88):\n            self.set_apibytessent(d.getVarInt64())\n            continue\n        if (tt == 96):\n            self.set_apibytesreceived(d.getVarInt64())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef parse_f(self, args):\n    if ((len(self.tex_coords) > 1) and (len(self.normals) == 1)):\n        raise PywavefrontException('Found texture coordinates, but no normals')\n    if (self.mesh is None):\n        self.mesh = mesh.Mesh()\n        self.wavefront.add_mesh(self.mesh)\n    if (self.material is None):\n        self.material = material.Material()\n    self.mesh.add_material(self.material)\n    v1 = None\n    vlast = None\n    points = []\n    for (i, v) in enumerate(args[0:]):\n        if (type(v) is bytes):\n            v = v.decode()\n        (v_index, t_index, n_index) = (list(map(int, [(j or 0) for j in v.split('/')])) + [0, 0])[:3]\n        if (v_index < 0):\n            v_index += (len(self.vertices) - 1)\n        if (t_index < 0):\n            t_index += (len(self.tex_coords) - 1)\n        if (n_index < 0):\n            n_index += (len(self.normals) - 1)\n        vertex = ((list(self.tex_coords[t_index]) + list(self.normals[n_index])) + list(self.vertices[v_index]))\n        if (i >= 3):\n            self.material.vertices += (v1 + vlast)\n        self.material.vertices += vertex\n        if (i == 0):\n            v1 = vertex\n        vlast = vertex\n", "label": 1}
{"function": "\n\ndef model_fields(model, fields=None, readonly_fields=None, exclude=None, field_args=None, converter=None):\n    '\\n    Generate a dictionary of WTForms fields for a given MongoEngine model.\\n\\n    See `model_form` docstring for description of parameters.\\n    '\n    from mongoengine.base import BaseDocument\n    if (BaseDocument not in inspect.getmro(model)):\n        raise TypeError('Model must be a MongoEngine Document schema')\n    readonly_fields = (readonly_fields or [])\n    exclude = (exclude or [])\n    converter = (converter or ModelConverter())\n    field_args = (field_args or {\n        \n    })\n    field_names = (fields if fields else model._fields.keys())\n    field_names = (x for x in field_names if (x not in exclude))\n    field_dict = {\n        \n    }\n    for name in field_names:\n        if ((name not in readonly_fields) and (name not in model._fields)):\n            raise KeyError(('\"%s\" is not read-only and does not appear to be a field on the document.' % name))\n        if ((name in model._fields) and (name not in readonly_fields)):\n            model_field = model._fields[name]\n            field = converter.convert(model, model_field, field_args.get(name))\n            if (field is not None):\n                field_dict[name] = field\n    return field_dict\n", "label": 1}
{"function": "\n\ndef decode_message(message, decoder):\n    out = []\n    objects = []\n    mapping = {\n        \n    }\n    in_field = False\n    prev = 0\n    for (index, ch) in enumerate(message):\n        if (not in_field):\n            if (ch == '{'):\n                in_field = True\n                if (prev != index):\n                    out.append(message[prev:index])\n                prev = index\n            elif (ch == '}'):\n                raise FormatException('unmatched }')\n        elif in_field:\n            if (ch == '{'):\n                raise FormatException('{ inside {}')\n            elif (ch == '}'):\n                in_field = False\n                (obj, msgid) = decoder(message[(prev + 1):index])\n                if (msgid is None):\n                    objects.append(obj)\n                    out.append('%s')\n                else:\n                    mapping[msgid] = obj\n                    out.append((('%(' + msgid) + ')s'))\n                prev = (index + 1)\n    if in_field:\n        raise FormatException('unmatched {')\n    if (prev <= index):\n        out.append(message[prev:(index + 1)])\n    result = ''.join(out)\n    if mapping:\n        args = mapping\n    else:\n        args = tuple(objects)\n    return (ugettext(result) % args)\n", "label": 1}
{"function": "\n\ndef _generate_altered_foo_together(self, operation):\n    option_name = operation.option_name\n    for (app_label, model_name) in sorted(self.kept_model_keys):\n        old_model_name = self.renamed_models.get((app_label, model_name), model_name)\n        old_model_state = self.from_state.models[(app_label, old_model_name)]\n        new_model_state = self.to_state.models[(app_label, model_name)]\n        old_value = (old_model_state.options.get(option_name) or set())\n        if old_value:\n            old_value = {tuple((self.renamed_fields.get((app_label, model_name, n), n) for n in unique)) for unique in old_value}\n        new_value = (new_model_state.options.get(option_name) or set())\n        if new_value:\n            new_value = set(new_value)\n        if (old_value != new_value):\n            dependencies = []\n            for foo_togethers in new_value:\n                for field_name in foo_togethers:\n                    field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name)\n                    if (field.remote_field and field.remote_field.model):\n                        dependencies.extend(self._get_dependecies_for_foreign_key(field))\n            self.add_operation(app_label, operation(name=model_name, **{\n                option_name: new_value,\n            }), dependencies=dependencies)\n", "label": 1}
{"function": "\n\ndef find(self, path):\n    '\\n        Generate filenames in path that satisfy criteria specified in\\n        the constructor.\\n        This method is a generator and should be repeatedly called\\n        until there are no more results.\\n        '\n    for (dirpath, dirs, files) in os.walk(path):\n        depth = dirpath[(len(path) + len(os.path.sep)):].count(os.path.sep)\n        if (depth >= self.mindepth):\n            for name in (dirs + files):\n                fstat = None\n                matches = True\n                fullpath = None\n                for criterion in self.criteria:\n                    if ((fstat is None) and (criterion.requires() & _REQUIRES_STAT)):\n                        fullpath = os.path.join(dirpath, name)\n                        fstat = os.stat(fullpath)\n                    if (not criterion.match(dirpath, name, fstat)):\n                        matches = False\n                        break\n                if matches:\n                    if (fullpath is None):\n                        fullpath = os.path.join(dirpath, name)\n                    for action in self.actions:\n                        if ((fstat is None) and (action.requires() & _REQUIRES_STAT)):\n                            fstat = os.stat(fullpath)\n                        result = action.execute(fullpath, fstat, test=self.test)\n                        if (result is not None):\n                            (yield result)\n        if (depth == self.maxdepth):\n            dirs[:] = []\n", "label": 1}
{"function": "\n\ndef _printPrefix(self, want):\n    \"Prints Prefixlen/Netmask.\\n\\n        Not really. In fact it is our universal Netmask/Prefixlen printer.\\n        This is considered an internal function.\\n\\n        want == 0 / None        don't return anything    1.2.3.0\\n        want == 1               /prefix                  1.2.3.0/24\\n        want == 2               /netmask                 1.2.3.0/255.255.255.0\\n        want == 3               -lastip                  1.2.3.0-1.2.3.255\\n        \"\n    if (((self._ipversion == 4) and (self._prefixlen == 32)) or ((self._ipversion == 6) and (self._prefixlen == 128))):\n        if self.NoPrefixForSingleIp:\n            want = 0\n    if (want == None):\n        want = self.WantPrefixLen\n        if (want == None):\n            want = 1\n    if want:\n        if (want == 2):\n            netmask = self.netmask()\n            if (not isinstance(netmask, (int, long))):\n                netmask = netmask.int()\n            return ('/%s' % intToIp(netmask, self._ipversion))\n        elif (want == 3):\n            return ('-%s' % intToIp(((self.ip + self.len()) - 1), self._ipversion))\n        else:\n            return ('/%d' % self._prefixlen)\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef _immediate_dominators(self, node, target_graph=None, reverse_graph=False):\n    if (target_graph is None):\n        target_graph = self.graph\n    if (node not in target_graph):\n        raise AngrCFGError(('Target node %s is not in graph.' % node))\n    graph = networkx.DiGraph(target_graph)\n    if reverse_graph:\n        for n in target_graph.nodes():\n            graph.add_node(n)\n        for (src, dst) in target_graph.edges():\n            graph.add_edge(dst, src)\n    idom = {\n        node: node,\n    }\n    order = list(networkx.dfs_postorder_nodes(graph, node))\n    dfn = {u: i for (i, u) in enumerate(order)}\n    order.pop()\n    order.reverse()\n\n    def intersect(u_, v_):\n        while (u_ != v_):\n            while (dfn[u_] < dfn[v_]):\n                u_ = idom[u_]\n            while (dfn[u_] > dfn[v_]):\n                v_ = idom[v_]\n        return u_\n    changed = True\n    while changed:\n        changed = False\n        for u in order:\n            new_idom = reduce(intersect, (v for v in graph.pred[u] if (v in idom)))\n            if ((u not in idom) or (idom[u] != new_idom)):\n                idom[u] = new_idom\n                changed = True\n    return idom\n", "label": 1}
{"function": "\n\ndef _qname_matches(tag, namespace, qname):\n    \"Logic determines if a QName matches the desired local tag and namespace.\\n  \\n  This is used in XmlElement.get_elements and XmlElement.get_attributes to\\n  find matches in the element's members (among all expected-and-unexpected\\n  elements-and-attributes).\\n  \\n  Args:\\n    expected_tag: string\\n    expected_namespace: string\\n    qname: string in the form '{xml_namespace}localtag' or 'tag' if there is\\n           no namespace.\\n  \\n  Returns:\\n    boolean True if the member's tag and namespace fit the expected tag and\\n    namespace.\\n  \"\n    if (qname is None):\n        member_tag = None\n        member_namespace = None\n    elif qname.startswith('{'):\n        member_namespace = qname[1:qname.index('}')]\n        member_tag = qname[(qname.index('}') + 1):]\n    else:\n        member_namespace = None\n        member_tag = qname\n    return (((tag is None) and (namespace is None)) or ((namespace is None) and (member_tag == tag)) or ((tag is None) and (member_namespace == namespace)) or ((tag is None) and (namespace == '') and (member_namespace is None)) or ((tag == member_tag) and (namespace == member_namespace)) or ((tag == member_tag) and (namespace == '') and (member_namespace is None)))\n", "label": 1}
{"function": "\n\ndef _ip_addrs(interface=None, include_loopback=False, interface_data=None, proto='inet'):\n    '\\n    Return the full list of IP adresses matching the criteria\\n\\n    proto = inet|inet6\\n    '\n    ret = set()\n    ifaces = (interface_data if isinstance(interface_data, dict) else interfaces())\n    if (interface is None):\n        target_ifaces = ifaces\n    else:\n        target_ifaces = dict([(k, v) for (k, v) in six.iteritems(ifaces) if (k == interface)])\n        if (not target_ifaces):\n            log.error('Interface {0} not found.'.format(interface))\n    for ip_info in six.itervalues(target_ifaces):\n        addrs = ip_info.get(proto, [])\n        addrs.extend([addr for addr in ip_info.get('secondary', []) if (addr.get('type') == proto)])\n        for addr in addrs:\n            addr = ipaddress.ip_address(addr.get('address'))\n            if ((not addr.is_loopback) or include_loopback):\n                ret.add(addr)\n    return [str(addr) for addr in sorted(ret)]\n", "label": 1}
{"function": "\n\ndef RC_calc(ctx, x, y, r, pv=True):\n    if (not (ctx.isnormal(x) and ctx.isnormal(y))):\n        if (ctx.isinf(x) or ctx.isinf(y)):\n            return (1 / (x * y))\n        if (y == 0):\n            return ctx.inf\n        if (x == 0):\n            return ((ctx.pi / ctx.sqrt(y)) / 2)\n        raise ValueError\n    if (pv and (ctx._im(y) == 0) and (ctx._re(y) < 0)):\n        return (ctx.sqrt((x / (x - y))) * RC_calc(ctx, (x - y), (- y), r))\n    if (x == y):\n        return (1 / ctx.sqrt(x))\n    extraprec = (2 * max(0, ((- ctx.mag((x - y))) + ctx.mag(x))))\n    ctx.prec += extraprec\n    if (ctx._is_real_type(x) and ctx._is_real_type(y)):\n        x = ctx._re(x)\n        y = ctx._re(y)\n        a = ctx.sqrt((x / y))\n        if (x < y):\n            b = ctx.sqrt((y - x))\n            v = (ctx.acos(a) / b)\n        else:\n            b = ctx.sqrt((x - y))\n            v = (ctx.acosh(a) / b)\n    else:\n        sx = ctx.sqrt(x)\n        sy = ctx.sqrt(y)\n        v = (ctx.acos((sx / sy)) / (ctx.sqrt((1 - (x / y))) * sy))\n    ctx.prec -= extraprec\n    return v\n", "label": 1}
{"function": "\n\ndef process_exception(self, request, exception):\n    if hasattr(social_exceptions, exception.__class__.__name__):\n        if isinstance(exception, AuthCanceled):\n            if request.user.is_anonymous():\n                return redirect('accounts:login')\n            else:\n                return redirect('user:settings', username=request.user.username)\n        elif isinstance(exception, AuthAlreadyAssociated):\n            blurb = 'The {0} account you tried to connect to has already been associated with another account.'\n            print(exception.backend.name)\n            if ('google' in exception.backend.name):\n                blurb = blurb.format('Google')\n            elif ('linkedin' in exception.backend.name):\n                blurb = blurb.format('LinkedIn')\n            elif ('hydroshare' in exception.backend.name):\n                blurb = blurb.format('HydroShare')\n            elif ('facebook' in exception.backend.name):\n                blurb = blurb.format('Facebook')\n            else:\n                blurb = blurb.format('social')\n            messages.success(request, blurb)\n            if request.user.is_anonymous():\n                return redirect('accounts:login')\n            else:\n                return redirect('user:settings', username=request.user.username)\n        elif isinstance(exception, NotAllowedToDisconnect):\n            blurb = 'Unable to disconnect from this social account.'\n            messages.success(request, blurb)\n            if request.user.is_anonymous():\n                return redirect('accounts:login')\n            else:\n                return redirect('user:settings', username=request.user.username)\n", "label": 1}
{"function": "\n\ndef OnKeyPressed(self, e):\n    key = e.GetKeyCode()\n    if ((key not in [wx.WXK_UP, wx.WXK_DOWN]) or (len(self.rows) == 0)):\n        e.Skip()\n        return\n    e.StopPropagation()\n    (start_col, start_row) = self.GetViewStart()\n    size = self.GetClientSize()\n    height = (size.GetHeight() / LINH)\n    if self.selection:\n        current_row = self.selection[0]\n        if (key == wx.WXK_UP):\n            next_row = max((current_row - 1), 0)\n        if (key == wx.WXK_DOWN):\n            next_row = min((current_row + 1), (len(self.rows) - 1))\n        if (e.ShiftDown() and self.allowMultiple):\n            if (next_row in self.selection):\n                self.selection.remove(current_row)\n            else:\n                self.selection.insert(0, next_row)\n        else:\n            self.selection = [next_row]\n    else:\n        next_row = start_row\n        if ((next_row < 0) or (next_row > len(self.rows))):\n            return\n        self.selection = [next_row]\n    if (next_row < start_row):\n        self.Scroll(start_col, (next_row - 1))\n    elif (next_row > ((start_row + height) - 1)):\n        self.Scroll(start_col, ((next_row - height) + 2))\n    event = CommitListEvent(EVT_COMMITLIST_SELECT_type, self.GetId())\n    event.SetCurrentRow(next_row)\n    event.SetSelection(self.selection)\n    self.ProcessEvent(event)\n    self.OnSelectionChanged(next_row, self.selection)\n    self.Refresh()\n", "label": 1}
{"function": "\n\ndef find_sorted_peaks(x):\n    'Find peaks, i.e. local maxima, of an array. Interior points are peaks if\\n    they are greater than both their neighbors, and edge points are peaks if\\n    they are greater than their only neighbor. In the case of ties, we\\n    (arbitrarily) choose the first index in the sequence of equal values as the\\n    peak.\\n\\n    Returns a list of tuples (i, x[i]) of peak indices i and values x[i],\\n    sorted in decreasing order by peak value.\\n     '\n    peak_inds = []\n    nbins = len(x)\n    for i in range(nbins):\n        if ((i == 0) or (x[i] > x[(i - 1)])):\n            if ((i == (nbins - 1)) or (x[i] > x[(i + 1)])):\n                peak_inds.append(i)\n            elif (x[i] == x[(i + 1)]):\n                for j in range((i + 1), nbins):\n                    if (x[j] != x[i]):\n                        if (x[j] < x[i]):\n                            peak_inds.append(i)\n                        break\n                if ((j == (nbins - 1)) and (x[i] == x[j])):\n                    peak_inds.append(i)\n    sorted_peak_inds = sorted(peak_inds, key=(lambda i: x[i]), reverse=True)\n    return list(zip(sorted_peak_inds, x[sorted_peak_inds]))\n", "label": 1}
{"function": "\n\ndef validate_authorize_params(self, input_data):\n    'Validates the authorize parameters given (usually) by GET'\n    client_id = input_data['client_id']\n    response_type = input_data['response_type']\n    state = input_data['state']\n    the_scope = input_data['the_scope']\n    redirect_uri = input_data['redirect_uri']\n    token_type = self.config[self.CONFIG_TOKEN_TYPE]\n    realm = self.config[self.CONFIG_WWW_REALM]\n    stored_client = self.storage.get_client_credentials(client_id)\n    if (not input_data):\n        raise HTTP(412, 'KeyError: All parameters are missing :(.')\n    elif ((not redirect_uri) or (not stored_client['redirect_uri']) or (redirect_uri != stored_client['redirect_uri'])):\n        raise HTTP(418, 'NameError: Invalid or mismatch redirect URI.')\n    elif (not client_id):\n        raise HTTP(412, 'KeyError: Parameter missing; \"client_id\" is required.')\n    elif (not stored_client):\n        raise HTTP(424, 'LookupError: Supplied \"client_id\" is invalid.')\n    elif (not response_type):\n        raise HTTP(412, 'KeyError: Parameter missing; \"response_type\" is required.')\n    elif (response_type != self.RESPONSE_TYPE_AUTH_CODE):\n        raise HTTP(501, 'The response type you requested is unsupported.')\n    elif (the_scope and (not self.check_the_scope(the_scope, self.config[self.CONFIG_SUPPORTED_SCOPES]))):\n        raise HTTP(501, 'The scope you requested is unsupported.')\n    elif ((not state) and self.config[self.CONFIG_ENFORCE_STATE]):\n        raise HTTP(412, 'KeyError: Parameter missing; \"state\" is required.')\n    return input_data\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _filter_proto(msg, make_copy=True):\n    'Filter all byte fields in the message and submessages.'\n    filtered = msg\n    if make_copy:\n        filtered = msg.__class__()\n        filtered.CopyFrom(msg)\n    fields = filtered.ListFields()\n    for (field_name, val) in ((fd.name, val) for (fd, val) in fields if (fd.type == FieldDescriptor.TYPE_BYTES)):\n        setattr(filtered, field_name, bytes(('<%s bytes>' % len(val)), 'utf8'))\n    for field in (val for (fd, val) in fields if (fd.type == FieldDescriptor.TYPE_MESSAGE)):\n        is_repeated = hasattr(field, '__len__')\n        if (not is_repeated):\n            Call._filter_proto(field, make_copy=False)\n        else:\n            for i in range(len(field)):\n                old_fields = [f for f in field]\n                del field[:]\n                field.extend([Call._filter_proto(f, make_copy=False) for f in old_fields])\n    return filtered\n", "label": 1}
{"function": "\n\ndef encode(self, b64=False, always_bytes=True):\n    'Encode the packet for transmission.'\n    if (self.binary and (not b64)):\n        encoded_packet = six.int2byte(self.packet_type)\n    else:\n        encoded_packet = six.text_type(self.packet_type)\n        if (self.binary and b64):\n            encoded_packet = ('b' + encoded_packet)\n    if self.binary:\n        if b64:\n            encoded_packet += base64.b64encode(self.data).decode('utf-8')\n        else:\n            encoded_packet += self.data\n    elif isinstance(self.data, six.string_types):\n        encoded_packet += self.data\n    elif (isinstance(self.data, dict) or isinstance(self.data, list)):\n        encoded_packet += self.json.dumps(self.data, separators=(',', ':'))\n    elif (self.data is not None):\n        encoded_packet += str(self.data)\n    if (always_bytes and (not isinstance(encoded_packet, six.binary_type))):\n        encoded_packet = encoded_packet.encode('utf-8')\n    return encoded_packet\n", "label": 1}
{"function": "\n\ndef _ApplyBarChartStyle(self, chart):\n    'If bar style is specified, fill in the missing data and apply it.'\n    if ((chart.style is None) or (not chart.data)):\n        return {\n            \n        }\n    (bar_thickness, bar_gap, group_gap) = (chart.style.bar_thickness, chart.style.bar_gap, chart.style.group_gap)\n    if ((bar_gap is None) and (group_gap is not None)):\n        bar_gap = max(0, (group_gap / 2))\n        if (not chart.style.use_fractional_gap_spacing):\n            bar_gap = int(bar_gap)\n    if ((group_gap is None) and (bar_gap is not None)):\n        group_gap = max(0, (bar_gap * 2))\n    if (bar_thickness is None):\n        if chart.style.use_fractional_gap_spacing:\n            bar_thickness = 'r'\n        else:\n            bar_thickness = 'a'\n    elif chart.style.use_fractional_gap_spacing:\n        if bar_gap:\n            bar_gap = int((bar_thickness * bar_gap))\n        if group_gap:\n            group_gap = int((bar_thickness * group_gap))\n    spec = [bar_thickness]\n    if (bar_gap is not None):\n        spec.append(bar_gap)\n        if ((group_gap is not None) and (not chart.stacked)):\n            spec.append(group_gap)\n    return util.JoinLists(bar_size=spec)\n", "label": 1}
{"function": "\n\ndef _handle_multipart(self, msg):\n    msgtexts = []\n    subparts = msg.get_payload()\n    if (subparts is None):\n        subparts = []\n    elif isinstance(subparts, str):\n        self.write(subparts)\n        return\n    elif (not isinstance(subparts, list)):\n        subparts = [subparts]\n    for part in subparts:\n        s = self._new_buffer()\n        g = self.clone(s)\n        g.flatten(part, unixfrom=False, linesep=self._NL)\n        msgtexts.append(s.getvalue())\n    boundary = msg.get_boundary()\n    if (not boundary):\n        alltext = self._encoded_NL.join(msgtexts)\n        boundary = self._make_boundary(alltext)\n        msg.set_boundary(boundary)\n    if (msg.preamble is not None):\n        if self._mangle_from_:\n            preamble = fcre.sub('>From ', msg.preamble)\n        else:\n            preamble = msg.preamble\n        self._write_lines(preamble)\n        self.write(self._NL)\n    self.write((('--' + boundary) + self._NL))\n    if msgtexts:\n        self._fp.write(msgtexts.pop(0))\n    for body_part in msgtexts:\n        self.write((((self._NL + '--') + boundary) + self._NL))\n        self._fp.write(body_part)\n    self.write((((self._NL + '--') + boundary) + '--'))\n    if (msg.epilogue is not None):\n        self.write(self._NL)\n        if self._mangle_from_:\n            epilogue = fcre.sub('>From ', msg.epilogue)\n        else:\n            epilogue = msg.epilogue\n        self._write_lines(epilogue)\n", "label": 1}
{"function": "\n\ndef GetArtifacts(self, os_name=None, name_list=None, source_type=None, exclude_dependents=False, provides=None, reload_datastore_artifacts=False):\n    'Retrieve artifact classes with optional filtering.\\n\\n    All filters must match for the artifact to be returned.\\n\\n    Args:\\n      os_name: string to match against supported_os\\n      name_list: list of strings to match against artifact names\\n      source_type: rdf_artifacts.ArtifactSource.SourceType to match against\\n                      source_type\\n      exclude_dependents: if true only artifacts with no dependencies will be\\n                          returned\\n      provides: return the artifacts that provide these dependencies\\n      reload_datastore_artifacts: If true, the data store sources are queried\\n                                  for new artifacts.\\n    Returns:\\n      set of artifacts matching filter criteria\\n    '\n    self._CheckDirty(reload_datastore_artifacts=reload_datastore_artifacts)\n    results = set()\n    for artifact in self._artifacts.itervalues():\n        if (os_name and artifact.supported_os and (os_name not in artifact.supported_os)):\n            continue\n        if (name_list and (artifact.name not in name_list)):\n            continue\n        if source_type:\n            source_types = [c.type for c in artifact.sources]\n            if (source_type not in source_types):\n                continue\n        if (exclude_dependents and artifact.GetArtifactPathDependencies()):\n            continue\n        if provides:\n            for provide_string in artifact.provides:\n                if (provide_string in provides):\n                    results.add(artifact)\n                    continue\n            continue\n        results.add(artifact)\n    return results\n", "label": 1}
{"function": "\n\ndef next(self):\n    response = super(PbufIndexStream, self).next()\n    if (response.done and (not (response.keys or response.results or response.continuation))):\n        raise StopIteration\n    if (self.return_terms and response.results):\n        return [(decode_index_value(self.index, r.key), bytes_to_str(r.value)) for r in response.results]\n    elif response.keys:\n        if PY2:\n            return response.keys[:]\n        else:\n            return [bytes_to_str(key) for key in response.keys]\n    elif response.continuation:\n        return CONTINUATION(bytes_to_str(response.continuation))\n", "label": 1}
{"function": "\n\ndef identifyConceptsUsed(self):\n    self.relationshipSets = [(arcrole, ELR, linkqname, arcqname) for (arcrole, ELR, linkqname, arcqname) in self.modelXbrl.baseSets.keys() if (ELR and (arcrole.startswith('XBRL-') or (linkqname and arcqname)))]\n    conceptsUsed = set((f.qname for f in self.modelXbrl.factsInInstance))\n    for cntx in self.modelXbrl.contexts.values():\n        for dim in cntx.qnameDims.values():\n            conceptsUsed.add(dim.dimensionQname)\n            if dim.isExplicit:\n                conceptsUsed.add(dim.memberQname)\n            else:\n                conceptsUsed.add(dim.typedMember.qname)\n    for (defaultDim, defaultDimMember) in self.modelXbrl.qnameDimensionDefaults.items():\n        conceptsUsed.add(defaultDim)\n        conceptsUsed.add(defaultDimMember)\n    for relationshipSetKey in self.relationshipSets:\n        relationshipSet = self.modelXbrl.relationshipSet(*relationshipSetKey)\n        for rel in relationshipSet.modelRelationships:\n            if isinstance(rel.fromModelObject, ModelConcept):\n                conceptsUsed.add(rel.fromModelObject)\n            if isinstance(rel.toModelObject, ModelConcept):\n                conceptsUsed.add(rel.toModelObject)\n    for qn in (XbrlConst.qnXbrliIdentifier, XbrlConst.qnXbrliPeriod, XbrlConst.qnXbrliUnit):\n        conceptsUsed.add(self.modelXbrl.qnameConcepts[qn])\n    conceptsUsed -= {None}\n    self.conceptsUsed = conceptsUsed\n", "label": 1}
{"function": "\n\ndef _sync_author_detail(self, key='author'):\n    context = self._getContext()\n    detail = context.get(('%ss' % key), [FeedParserDict()])[(- 1)]\n    if detail:\n        name = detail.get('name')\n        email = detail.get('email')\n        if (name and email):\n            context[key] = ('%s (%s)' % (name, email))\n        elif name:\n            context[key] = name\n        elif email:\n            context[key] = email\n    else:\n        (author, email) = (context.get(key), None)\n        if (not author):\n            return\n        emailmatch = re.search('(([a-zA-Z0-9\\\\_\\\\-\\\\.\\\\+]+)@((\\\\[[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.)|(([a-zA-Z0-9\\\\-]+\\\\.)+))([a-zA-Z]{2,4}|[0-9]{1,3})(\\\\]?))(\\\\?subject=\\\\S+)?', author)\n        if emailmatch:\n            email = emailmatch.group(0)\n            author = author.replace(email, '')\n            author = author.replace('()', '')\n            author = author.replace('<>', '')\n            author = author.replace('&lt;&gt;', '')\n            author = author.strip()\n            if (author and (author[0] == '(')):\n                author = author[1:]\n            if (author and (author[(- 1)] == ')')):\n                author = author[:(- 1)]\n            author = author.strip()\n        if (author or email):\n            context.setdefault(('%s_detail' % key), detail)\n        if author:\n            detail['name'] = author\n        if email:\n            detail['email'] = email\n", "label": 1}
{"function": "\n\n@classmethod\ndef _generate_resolve_ivy(cls, jars, excludes, ivyxml, confs, resolve_hash_name, pinned_artifacts=None, jar_dep_manager=None):\n    org = IvyUtils.INTERNAL_ORG_NAME\n    name = resolve_hash_name\n    extra_configurations = [conf for conf in confs if (conf and (conf != 'default'))]\n    jars_by_key = OrderedDict()\n    for jar in jars:\n        jars = jars_by_key.setdefault((jar.org, jar.name), [])\n        jars.append(jar)\n    manager = (jar_dep_manager or JarDependencyManagement.global_instance())\n    artifact_set = PinnedJarArtifactSet(pinned_artifacts)\n    for jars in jars_by_key.values():\n        for (i, dep) in enumerate(jars):\n            direct_coord = M2Coordinate.create(dep)\n            managed_coord = artifact_set[direct_coord]\n            if (direct_coord.rev != managed_coord.rev):\n                coord = manager.resolve_version_conflict(managed_coord, direct_coord, force=dep.force)\n                jars[i] = dep.copy(rev=coord.rev)\n            elif dep.force:\n                artifact_set.put(direct_coord)\n    dependencies = [cls._generate_jar_template(jars) for jars in jars_by_key.values()]\n    overrides = [cls._generate_override_template(_coord) for _coord in artifact_set]\n    excludes = [cls._generate_exclude_template(exclude) for exclude in excludes]\n    template_data = TemplateData(org=org, module=name, extra_configurations=extra_configurations, dependencies=dependencies, excludes=excludes, overrides=overrides)\n    template_relpath = os.path.join('templates', 'ivy_utils', 'ivy.mustache')\n    cls._write_ivy_xml_file(ivyxml, template_data, template_relpath)\n", "label": 1}
{"function": "\n\ndef solve_TLE(self, board):\n    '\\n\\n        :param board: a 9x9 2D array\\n        :return: Boolean\\n        '\n    n = len(board)\n    if all([(board[(i / n)][(i % n)] != '.') for i in xrange((n * n))]):\n        return True\n    for i in xrange(n):\n        for j in xrange(n):\n            if (board[i][j] == '.'):\n                for num in range(1, 10):\n                    num_str = str(num)\n                    condition_row = all([(board[i][col] != num_str) for col in xrange(n)])\n                    condition_col = all([(board[row][j] != num_str) for row in xrange(n)])\n                    condition_square = all([(board[(((i / 3) * 3) + (count / 3))][(((j / 3) * 3) + (count % 3))] != num_str) for count in xrange(n)])\n                    if (condition_col and condition_row and condition_square):\n                        board[i][j] = num_str\n                        if (not self.solve(board)):\n                            board[i][j] = '.'\n                        else:\n                            return True\n    return False\n", "label": 1}
{"function": "\n\ndef generate_extensions(self, extensions, enums, functions):\n    write = set()\n    written = (set((enum.name for enum in enums)) | set((function.proto.name for function in functions)))\n    f = self._f_h\n    self.write_functions(f, write, written, extensions)\n    f = self._f_c\n    if (self.spec.NAME in ('gl', 'glx', 'wgl')):\n        for ext in set((ext.name for ext in extensions)):\n            f.write('int GLAD_{};\\n'.format(ext))\n    written = set()\n    for ext in extensions:\n        if (ext.name == 'GLX_SGIX_video_source'):\n            f.write('#ifdef _VL_H_\\n')\n        if (ext.name == 'GLX_SGIX_dmbuffer'):\n            f.write('#ifdef _DM_BUFFER_H_\\n')\n        for func in ext.functions:\n            if ((func in write) and (func not in written)):\n                self.write_function(f, func)\n                written.add(func)\n        if (ext.name in ('GLX_SGIX_video_source', 'GLX_SGIX_dmbuffer')):\n            f.write('#endif\\n')\n", "label": 1}
{"function": "\n\ndef performAction(self, action):\n    self.steps += 1\n    if (action == self.TurnAround):\n        self._turn()\n    elif (action == self.Forward):\n        self._forward()\n    else:\n        r = random()\n        if (self.env.perseus[1] == 3):\n            if (r < 0.1):\n                self._turn()\n            elif (r < 0.9):\n                self._backup()\n        elif (((self.env.perseus[1] == 2) and (self.env.perseusDir == 3)) or ((self.env.perseus[1] == 4) and (self.env.perseusDir == 1))):\n            if (r < 0.3):\n                self._turn()\n            elif (r < 0.6):\n                self._backup()\n        elif (r < 0.7):\n            self._backup()\n", "label": 1}
{"function": "\n\ndef check_internet_scheme(self, elb_item):\n    '\\n        alert when an ELB has an \"internet-facing\" scheme.\\n        '\n    scheme = elb_item.config.get('scheme', None)\n    vpc = elb_item.config.get('vpc_id', None)\n    if (scheme and (scheme == 'internet-facing') and (not vpc)):\n        self.add_issue(1, 'ELB is Internet accessible.', elb_item)\n    elif (scheme and (scheme == 'internet-facing') and vpc):\n        security_groups = elb_item.config.get('security_groups', [])\n        for sgid in security_groups:\n            sg = Item.query.filter(Item.name.ilike((('%' + sgid) + '%'))).first()\n            if (not sg):\n                continue\n            sg_cidrs = []\n            config = sg.revisions[0].config\n            for rule in config.get('rules', []):\n                cidr = rule.get('cidr_ip', '')\n                if ((rule.get('rule_type', None) == 'ingress') and cidr):\n                    if ((not _check_rfc_1918(cidr)) and (not self._check_inclusion_in_network_whitelist(cidr))):\n                        sg_cidrs.append(cidr)\n            if sg_cidrs:\n                notes = 'SG [{sgname}] via [{cidr}]'.format(sgname=sg.name, cidr=', '.join(sg_cidrs))\n                self.add_issue(1, 'VPC ELB is Internet accessible.', elb_item, notes=notes)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.status = sentry_common_service.ttypes.TSentryResponseStatus()\n                self.status.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.SET):\n                self.privileges = set()\n                (_etype87, _size84) = iprot.readSetBegin()\n                for _i88 in xrange(_size84):\n                    _elem89 = iprot.readString()\n                    self.privileges.add(_elem89)\n                iprot.readSetEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef updateIndels(self, snp, is_negative_strand):\n    contig = snp.chromosome\n    lcontig = self.mFasta.getLength(contig)\n    code = self.mAnnotations.getSequence(contig, '+', snp.pos, (snp.pos + 2))\n    self.mCode = code\n    variants = snp.genotype.split('/')\n    for variant in variants:\n        if (variant[0] == '*'):\n            self.mVariantType.append('W')\n        elif (variant[0] == '+'):\n            toinsert = variant[1:]\n            self.mVariantType.append('I')\n        elif (variant[0] == '-'):\n            todelete = variant[1:]\n            self.mVariantType.append('D')\n        else:\n            raise ValueError((\"unknown variant sign '%s'\" % variant[0]))\n    if (code[0] and (code[1] not in 'abcABC')):\n        return\n    if is_negative_strand:\n        variants = [Genomics.complement(x) for x in variants]\n    for reference_codon in self.mReferenceCodons:\n        variants = snp.genotype.split('/')\n        variants = [x[1:] for x in variants]\n        for variant in variants:\n            if ((len(variant) % 3) != 0):\n                self.mVariantCodons.append('!')\n            else:\n                self.mVariantCodons.append(variant)\n        self.mVariantAAs.extend([Genomics.translate(x) for x in self.mVariantCodons])\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@gof.local_optimizer([AdvancedIncSubtensor1])\ndef local_set_to_inc_subtensor(node):\n    '\\n    AdvancedIncSubtensor1(x, x[ilist]+other, ilist, set_instead_of_inc=True) ->\\n    AdvancedIncSubtensor1(x, other, ilist, set_instead_of_inc=False)\\n\\n    '\n    if (isinstance(node.op, AdvancedIncSubtensor1) and node.op.set_instead_of_inc and node.inputs[1].owner and isinstance(node.inputs[1].owner.op, Elemwise) and isinstance(node.inputs[1].owner.op.scalar_op, scalar.Add)):\n        addn = node.inputs[1].owner\n        subn = None\n        other = None\n        if (addn.inputs[0].owner and isinstance(addn.inputs[0].owner.op, AdvancedSubtensor1)):\n            subn = addn.inputs[0].owner\n            other = addn.inputs[1]\n        elif (addn.inputs[1].owner and isinstance(addn.inputs[1].owner.op, AdvancedSubtensor1)):\n            subn = addn.inputs[1].owner\n            other = addn.inputs[0]\n        else:\n            return\n        if ((subn.inputs[1] != node.inputs[2]) or (subn.inputs[0] != node.inputs[0])):\n            return\n        ret = advanced_inc_subtensor1(node.inputs[0], other, node.inputs[2])\n        copy_stack_trace(node.outputs, ret)\n        return [ret]\n", "label": 1}
{"function": "\n\ndef show_model_changes(new, old=None, fields=None, always=False):\n    'Given a Model object, print a list of changes from its pristine\\n    version stored in the database. Return a boolean indicating whether\\n    any changes were found.\\n\\n    `old` may be the \"original\" object to avoid using the pristine\\n    version from the database. `fields` may be a list of fields to\\n    restrict the detection to. `always` indicates whether the object is\\n    always identified, regardless of whether any changes are present.\\n    '\n    old = (old or new._db._get(type(new), new.id))\n    changes = []\n    for field in old:\n        if ((field == 'mtime') or (fields and (field not in fields))):\n            continue\n        line = _field_diff(field, old, new)\n        if line:\n            changes.append('  {0}: {1}'.format(field, line))\n    for field in (set(new) - set(old)):\n        if (fields and (field not in fields)):\n            continue\n        changes.append('  {0}: {1}'.format(field, colorize('text_highlight', new.formatted()[field])))\n    if (changes or always):\n        print_(format(old))\n    if changes:\n        print_('\\n'.join(changes))\n    return bool(changes)\n", "label": 1}
{"function": "\n\ndef convert_values(self):\n    'Convert datetimes to a comparable value in an expression.\\n        '\n\n    def stringify(value):\n        if (self.encoding is not None):\n            encoder = partial(pprint_thing_encoded, encoding=self.encoding)\n        else:\n            encoder = pprint_thing\n        return encoder(value)\n    (lhs, rhs) = (self.lhs, self.rhs)\n    if (is_term(lhs) and lhs.is_datetime and is_term(rhs) and rhs.isscalar):\n        v = rhs.value\n        if isinstance(v, (int, float)):\n            v = stringify(v)\n        v = pd.Timestamp(_ensure_decoded(v))\n        if (v.tz is not None):\n            v = v.tz_convert('UTC')\n        self.rhs.update(v)\n    if (is_term(rhs) and rhs.is_datetime and is_term(lhs) and lhs.isscalar):\n        v = lhs.value\n        if isinstance(v, (int, float)):\n            v = stringify(v)\n        v = pd.Timestamp(_ensure_decoded(v))\n        if (v.tz is not None):\n            v = v.tz_convert('UTC')\n        self.lhs.update(v)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.end_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.endpoints = []\n                (_etype31, _size28) = iprot.readListBegin()\n                for _i32 in xrange(_size28):\n                    _elem33 = iprot.readString()\n                    self.endpoints.append(_elem33)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef execute(self):\n    'If the training flag is set, train the metamodel. Otherwise,\\n        predict outputs.\\n        '\n    if self._train:\n        input_data = self._param_data\n        if (self.warm_restart is False):\n            input_data = []\n            base = 0\n        else:\n            base = len(input_data)\n        for name in self._surrogate_input_names:\n            train_name = ('params.%s' % name)\n            val = self.get(train_name)\n            num_sample = len(val)\n            for j in xrange(base, (base + num_sample)):\n                if (j > (len(input_data) - 1)):\n                    input_data.append([])\n                input_data[j].append(val[(j - base)])\n        for name in self._surrogate_output_names:\n            train_name = ('responses.%s' % name)\n            output_data = self._response_data[name]\n            if (self.warm_restart is False):\n                output_data = []\n            output_data.extend(self.get(train_name))\n            surrogate = self._get_surrogate(name)\n            if (surrogate is not None):\n                surrogate.train(input_data, output_data)\n        self._train = False\n    inputs = []\n    for name in self._surrogate_input_names:\n        val = self.get(name)\n        inputs.append(val)\n    for name in self._surrogate_output_names:\n        surrogate = self._get_surrogate(name)\n        if (surrogate is not None):\n            setattr(self, name, surrogate.predict(inputs))\n", "label": 1}
{"function": "\n\n@count_calls\ndef check_referenced_versions(self):\n    'Deeply checks all the references in the scene and returns a\\n        dictionary which uses the ids of the Versions as key and the action as\\n        value.\\n\\n        Uses the top level references to get a Stalker Version instance and\\n        then tracks all the changes from these Version instances.\\n\\n        :return: list\\n        '\n    dfs_version_references = []\n    version = self.get_current_version()\n    resolution_dictionary = empty_reference_resolution(root=self.get_referenced_versions())\n    for v in version.walk_hierarchy():\n        dfs_version_references.append(v)\n    dfs_version_references.pop(0)\n    for v in reversed(dfs_version_references):\n        to_be_updated_list = []\n        for ref_v in v.inputs:\n            if (not ref_v.is_latest_published_version()):\n                to_be_updated_list.append(ref_v)\n        if to_be_updated_list:\n            action = 'create'\n            latest_published_version = v.latest_published_version\n            if (latest_published_version and (not v.is_latest_published_version())):\n                if all([(ref_v.latest_published_version in latest_published_version.inputs) for ref_v in to_be_updated_list]):\n                    action = 'update'\n                else:\n                    action = 'create'\n        else:\n            if v.is_latest_published_version():\n                action = 'leave'\n            else:\n                action = 'update'\n            if any((((rev_v in resolution_dictionary['update']) or (rev_v in resolution_dictionary['create'])) for rev_v in v.inputs)):\n                action = 'create'\n        resolution_dictionary[action].append(v)\n    return resolution_dictionary\n", "label": 1}
{"function": "\n\ndef __iter__(self):\n    asset_cache = {\n        \n    }\n    for (dt, series) in self.df.iterrows():\n        if (dt < self.start_date):\n            continue\n        if (dt > self.end_date):\n            return\n        event = FetcherEvent()\n        event.dt = dt\n        for (k, v) in series.iteritems():\n            if isinstance(v, numpy.integer):\n                v = int(v)\n            setattr(event, k, v)\n        if (event.sid in asset_cache):\n            event.sid = asset_cache[event.sid]\n        elif hasattr(event.sid, 'start_date'):\n            asset_cache[event.sid] = event.sid\n        elif (self.finder and isinstance(event.sid, int)):\n            asset = self.finder.retrieve_asset(event.sid, default_none=True)\n            if asset:\n                event.sid = asset_cache[asset] = asset\n            elif self.mask:\n                continue\n            elif (self.symbol is None):\n                event.sid = asset_cache[event.sid] = Equity(event.sid)\n        event.type = DATASOURCE_TYPE.CUSTOM\n        event.source_id = self.namestring\n        (yield event)\n", "label": 1}
{"function": "\n\n@field_names.setter\ndef field_names(self, val):\n    val = [self._unicode(x) for x in val]\n    self._validate_option('field_names', val)\n    if self._field_names:\n        old_names = self._field_names[:]\n    self._field_names = val\n    if (self._align and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._align[new_name] = self._align[old_name]\n        for old_name in old_names:\n            if (old_name not in self._align):\n                self._align.pop(old_name)\n    else:\n        self.align = 'c'\n    if (self._valign and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._valign[new_name] = self._valign[old_name]\n        for old_name in old_names:\n            if (old_name not in self._valign):\n                self._valign.pop(old_name)\n    else:\n        self.valign = 't'\n", "label": 1}
{"function": "\n\ndef List(self, device_path):\n    'Prints a directory listing.\\n\\n  Args:\\n    device_path: Directory to list.\\n  '\n    files = adb_commands.AdbCommands.List(self, device_path)\n    files.sort(key=(lambda x: x.filename))\n    maxname = max((len(f.filename) for f in files))\n    maxsize = max((len(str(f.size)) for f in files))\n    for f in files:\n        mode = (((((((((('d' if stat.S_ISDIR(f.mode) else '-') + ('r' if (f.mode & stat.S_IRUSR) else '-')) + ('w' if (f.mode & stat.S_IWUSR) else '-')) + ('x' if (f.mode & stat.S_IXUSR) else '-')) + ('r' if (f.mode & stat.S_IRGRP) else '-')) + ('w' if (f.mode & stat.S_IWGRP) else '-')) + ('x' if (f.mode & stat.S_IXGRP) else '-')) + ('r' if (f.mode & stat.S_IROTH) else '-')) + ('w' if (f.mode & stat.S_IWOTH) else '-')) + ('x' if (f.mode & stat.S_IXOTH) else '-'))\n        t = time.gmtime(f.mtime)\n        (yield ('%s %*d %04d-%02d-%02d %02d:%02d:%02d %-*s\\n' % (mode, maxsize, f.size, t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec, maxname, f.filename)))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 3):\n            if (ftype == TType.STRING):\n                self.column_family = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.super_column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.STRING):\n                self.column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.value = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.timestamp = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.ttl = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\n@register_uncanonicalize\n@register_specialize\n@gof.local_optimizer(ALL_REDUCE)\ndef local_reduce_broadcastable(node):\n    'Remove reduction over broadcastable dimensions.'\n    if isinstance(node.op, T.CAReduce):\n        (reduced,) = node.inputs\n        odtype = node.outputs[0].dtype\n        if (node.op.axis is None):\n            if all(reduced.broadcastable):\n                return [reduced.dimshuffle().astype(odtype)]\n        else:\n            axis = list(node.op.axis)\n            cuttable = [a for a in axis if reduced.broadcastable[a]]\n            if cuttable:\n                new_axis = []\n                pattern = []\n                ii = 0\n                for p in xrange(reduced.ndim):\n                    if (p not in cuttable):\n                        if (p in axis):\n                            new_axis.append(ii)\n                        pattern.append(p)\n                        ii += 1\n                new_reduced = reduced.dimshuffle(*pattern)\n                if new_axis:\n                    if (type(node.op) == theano.tensor.elemwise.CAReduce):\n                        new_op = node.op.__class__(node.op.scalar_op, axis=new_axis)\n                    else:\n                        new_op = node.op.__class__(axis=new_axis)\n                    return [new_op(new_reduced)]\n                else:\n                    return [new_reduced.astype(odtype)]\n", "label": 1}
{"function": "\n\ndef fuzzy(event, base=None, date_format=None):\n    if (not base):\n        base = datetime.now()\n    if date_format:\n        event = datetime.strptime(event, date_format)\n    elif (type(event) == str):\n        event = datetime.fromtimestamp(int(event))\n    elif (type(event) == int):\n        event = datetime.fromtimestamp(event)\n    elif (type(event) != datetime):\n        raise Exception('Cannot convert object `{}` to fuzzy date string'.format(event))\n    delta = (base - event)\n    if (delta.days == 0):\n        if (delta.seconds < 60):\n            return '{} seconds ago'.format(delta.seconds)\n        elif (delta.seconds < 120):\n            return '1 min and {} secs ago'.format((delta.seconds - 60))\n        elif (delta.seconds < TEN_MINS):\n            return '{} mins and {} secs ago'.format((delta.seconds // 60), (delta.seconds % 60))\n        elif (delta.seconds < ONE_HOUR):\n            return '{} minutes ago'.format((delta.seconds // 60))\n        elif (delta.seconds < TWO_HOURS):\n            return '1 hour and {} mins ago'.format(((delta.seconds % ONE_HOUR) // 60))\n        return 'over {} hours ago'.format((delta.seconds // ONE_HOUR))\n    elif (delta.days < 2):\n        return 'over a day ago'\n    elif (delta.days < 7):\n        return 'over {} days ago'.format(delta.days)\n    return '{date:%b} {date.day}, {date.year}'.format(date=event)\n", "label": 1}
{"function": "\n\ndef _convert_relation(self, prop, kwargs):\n    form_columns = getattr(self.view, 'form_columns', None)\n    if (form_columns and (prop.key not in form_columns)):\n        return None\n    remote_model = prop.mapper.class_\n    column = prop.local_remote_pairs[0][0]\n    if (not column.foreign_keys):\n        column = prop.local_remote_pairs[0][1]\n    kwargs['label'] = self._get_label(prop.key, kwargs)\n    kwargs['description'] = self._get_description(prop.key, kwargs)\n    requirement_options = (validators.Optional, validators.InputRequired)\n    if (not any((isinstance(v, requirement_options) for v in kwargs['validators']))):\n        if (column.nullable or (prop.direction.name != 'MANYTOONE')):\n            kwargs['validators'].append(validators.Optional())\n        else:\n            kwargs['validators'].append(validators.InputRequired())\n    if ('allow_blank' not in kwargs):\n        kwargs['allow_blank'] = column.nullable\n    override = self._get_field_override(prop.key)\n    if override:\n        return override(**kwargs)\n    if ((prop.direction.name == 'MANYTOONE') or (not prop.uselist)):\n        return self._model_select_field(prop, False, remote_model, **kwargs)\n    elif (prop.direction.name == 'ONETOMANY'):\n        return self._model_select_field(prop, True, remote_model, **kwargs)\n    elif (prop.direction.name == 'MANYTOMANY'):\n        return self._model_select_field(prop, True, remote_model, **kwargs)\n", "label": 1}
{"function": "\n\ndef split_command_line(command_line):\n    \"This splits a command line into a list of arguments. It splits arguments\\n    on spaces, but handles embedded quotes, doublequotes, and escaped\\n    characters. It's impossible to do this with a regular expression, so I\\n    wrote a little state machine to parse the command line. \"\n    arg_list = []\n    arg = ''\n    state_basic = 0\n    state_esc = 1\n    state_singlequote = 2\n    state_doublequote = 3\n    state_whitespace = 4\n    state = state_basic\n    for c in command_line:\n        if ((state == state_basic) or (state == state_whitespace)):\n            if (c == '\\\\'):\n                state = state_esc\n            elif (c == \"'\"):\n                state = state_singlequote\n            elif (c == '\"'):\n                state = state_doublequote\n            elif c.isspace():\n                if (state == state_whitespace):\n                    None\n                else:\n                    arg_list.append(arg)\n                    arg = ''\n                    state = state_whitespace\n            else:\n                arg = (arg + c)\n                state = state_basic\n        elif (state == state_esc):\n            arg = (arg + c)\n            state = state_basic\n        elif (state == state_singlequote):\n            if (c == \"'\"):\n                state = state_basic\n            else:\n                arg = (arg + c)\n        elif (state == state_doublequote):\n            if (c == '\"'):\n                state = state_basic\n            else:\n                arg = (arg + c)\n    if (arg != ''):\n        arg_list.append(arg)\n    return arg_list\n", "label": 1}
{"function": "\n\ndef binomial_pdf(x, a, b):\n    'binomial PDF by H. Gene Shin\\n    \\n    '\n    if (a < 1):\n        return 0\n    elif ((x < 0) or (a < x)):\n        return 0\n    elif (b == 0):\n        if (x == 0):\n            return 1\n        else:\n            return 0\n    elif (b == 1):\n        if (x == a):\n            return 1\n        else:\n            return 0\n    else:\n        if (x > (a - x)):\n            p = (1 - b)\n            mn = (a - x)\n            mx = x\n        else:\n            p = b\n            mn = x\n            mx = (a - x)\n        pdf = 1\n        t = 0\n        for q in xrange(1, (mn + 1)):\n            pdf *= ((((a - q) + 1) * p) / ((mn - q) + 1))\n            if (pdf < 1e-100):\n                while (pdf < 0.001):\n                    pdf /= (1 - p)\n                    t -= 1\n            if (pdf > 1e+100):\n                while ((pdf > 1000.0) and (t < mx)):\n                    pdf *= (1 - p)\n                    t += 1\n        for i in xrange((mx - t)):\n            pdf *= (1 - p)\n        pdf = float(('%.10e' % pdf))\n        return pdf\n", "label": 1}
{"function": "\n\ndef read_input(path, stdin=None):\n    \"Stream input the way Hadoop would.\\n\\n    - Resolve globs (``foo_*.gz``).\\n    - Decompress ``.gz`` and ``.bz2`` files.\\n    - If path is ``'-'``, read from stdin\\n    - If path is a directory, recursively read its contents.\\n\\n    You can redefine *stdin* for ease of testing. *stdin* can actually be\\n    any iterable that yields lines (e.g. a list).\\n    \"\n    if (stdin is None):\n        stdin = sys.stdin\n    if (path == '-'):\n        for line in stdin:\n            (yield line)\n        return\n    paths = glob.glob(path)\n    if (not paths):\n        raise IOError(2, ('No such file or directory: %r' % path))\n    elif (len(paths) > 1):\n        for path in paths:\n            for line in read_input(path, stdin=stdin):\n                (yield line)\n        return\n    else:\n        path = paths[0]\n    if os.path.isdir(path):\n        for (dirname, _, filenames) in os.walk(path, followlinks=True):\n            for filename in filenames:\n                for line in read_input(os.path.join(dirname, filename), stdin=stdin):\n                    (yield line)\n        return\n    for line in read_file(path):\n        (yield line)\n", "label": 1}
{"function": "\n\ndef expand_partitions(containers, partitions):\n    '\\n    Validate the partitions of containers. If there are any containers\\n    not in any partition, place them in an new partition.\\n    '\n    all_names = frozenset((c.name for c in containers if (not c.holy)))\n    holy_names = frozenset((c.name for c in containers if c.holy))\n    neutral_names = frozenset((c.name for c in containers if c.neutral))\n    partitions = [frozenset(p) for p in partitions]\n    unknown = set()\n    holy = set()\n    union = set()\n    for partition in partitions:\n        unknown.update(((partition - all_names) - holy_names))\n        holy.update((partition - all_names))\n        union.update(partition)\n    if unknown:\n        raise BlockadeError(('Partitions contain unknown containers: %s' % list(unknown)))\n    if holy:\n        raise BlockadeError(('Partitions contain holy containers: %s' % list(holy)))\n    leftover = all_names.difference(union)\n    if leftover:\n        partitions.append(leftover)\n    if (not neutral_names.issubset(leftover)):\n        partitions.append(neutral_names)\n    return partitions\n", "label": 1}
{"function": "\n\ndef alloc_jacobian(self):\n    '\\n        Creates a jacobian dictionary with the keys pre-populated and correct\\n        array sizes allocated. caches the result in the component, and\\n        returns that cache if it finds it.\\n\\n        Returns\\n        -----------\\n        dict\\n            pre-allocated jacobian dictionary\\n        '\n    if ((self._jacobian_cache is not None) and (len(self._jacobian_cache) > 0)):\n        return self._jacobian_cache\n    self._jacobian_cache = jac = {\n        \n    }\n    u_vec = self.unknowns\n    p_vec = self.params\n    states = self.states\n    p_size_storage = [(n, m['size']) for (n, m) in iteritems(p_vec) if ((not m.get('pass_by_obj')) and (not m.get('remote')))]\n    s_size_storage = []\n    u_size_storage = []\n    for (n, meta) in iteritems(u_vec):\n        if (meta.get('pass_by_obj') or meta.get('remote')):\n            continue\n        if meta.get('state'):\n            s_size_storage.append((n, meta['size']))\n        u_size_storage.append((n, meta['size']))\n    for (u_var, u_size) in u_size_storage:\n        for (p_var, p_size) in p_size_storage:\n            jac[(u_var, p_var)] = np.zeros((u_size, p_size))\n        for (s_var, s_size) in s_size_storage:\n            jac[(u_var, s_var)] = np.zeros((u_size, s_size))\n    return jac\n", "label": 1}
{"function": "\n\ndef parse_time(val):\n    if (not val):\n        return None\n    hr = mi = 0\n    val = val.lower()\n    amflag = ((- 1) != val.find('a'))\n    pmflag = ((- 1) != val.find('p'))\n    for noise in ':amp.':\n        val = val.replace(noise, ' ')\n    val = val.split()\n    if (len(val) > 1):\n        hr = int(val[0])\n        mi = int(val[1])\n    else:\n        val = val[0]\n        if (len(val) < 1):\n            pass\n        elif ('now' == val):\n            tm = localtime()\n            hr = tm[3]\n            mi = tm[4]\n        elif ('noon' == val):\n            hr = 12\n        elif (len(val) < 3):\n            hr = int(val)\n            if ((not amflag) and (not pmflag) and (hr < 7)):\n                hr += 12\n        elif (len(val) < 5):\n            hr = int(val[:(- 2)])\n            mi = int(val[(- 2):])\n        else:\n            hr = int(val[:1])\n    if (amflag and (hr >= 12)):\n        hr = (hr - 12)\n    if (pmflag and (hr < 12)):\n        hr = (hr + 12)\n    return time(hr, mi)\n", "label": 1}
{"function": "\n\ndef resolve_job_references(io_hash, job_outputs, should_resolve=True):\n    '\\n    :param io_hash: an input or output hash in which to resolve any job-based object references possible\\n    :type io_hash: dict\\n    :param job_outputs: a mapping of finished local jobs to their output hashes\\n    :type job_outputs: dict\\n    :param should_resolve: whether it is an error if a job-based object reference in *io_hash* cannot be resolved yet\\n    :type should_resolve: boolean\\n\\n    Modifies *io_hash* in-place.\\n    '\n    q = []\n    for field in io_hash:\n        if is_job_ref(io_hash[field]):\n            io_hash[field] = resolve_job_ref(io_hash[field], job_outputs, should_resolve)\n        elif (isinstance(io_hash[field], list) or isinstance(io_hash[field], dict)):\n            q.append(io_hash[field])\n    while (len(q) > 0):\n        thing = q.pop()\n        if isinstance(thing, list):\n            for i in range(len(thing)):\n                if is_job_ref(thing[i]):\n                    thing[i] = resolve_job_ref(thing[i], job_outputs, should_resolve)\n                elif (isinstance(thing[i], list) or isinstance(thing[i], dict)):\n                    q.append(thing[i])\n        else:\n            for field in thing:\n                if is_job_ref(thing[field]):\n                    thing[field] = resolve_job_ref(thing[field], job_outputs, should_resolve)\n                elif (isinstance(thing[field], list) or isinstance(thing[field], dict)):\n                    q.append(thing[field])\n", "label": 1}
{"function": "\n\ndef keypress(self, size, key):\n    command = self._command_map[key]\n    if (key == ']'):\n        self.shift_order((+ 1))\n        return True\n    elif (key == '['):\n        self.shift_order((- 1))\n        return True\n    elif (key == '>'):\n        self.focus_hotspot(size)\n        return True\n    elif (key == '\\\\'):\n        layout = {\n            FLAT: NESTED,\n            NESTED: FLAT,\n        }[self.layout]\n        self.set_layout(layout)\n        return True\n    command = self._command_map[key]\n    if (command == 'menu'):\n        self.defocus()\n        return True\n    elif (command == urwid.CURSOR_RIGHT):\n        if (self.layout == FLAT):\n            return True\n        (widget, node) = self.tbody.get_focus()\n        if widget.expanded:\n            heavy_widget = widget.first_child()\n            if (heavy_widget is not None):\n                heavy_node = heavy_widget.get_node()\n                self.tbody.change_focus(size, heavy_node)\n            return True\n    elif (command == urwid.CURSOR_LEFT):\n        if (self.layout == FLAT):\n            return True\n        (widget, node) = self.tbody.get_focus()\n        if (not widget.expanded):\n            parent_node = node.get_parent()\n            if (not parent_node.is_root()):\n                self.tbody.change_focus(size, parent_node)\n            return True\n    elif (command == urwid.ACTIVATE):\n        if self.viewer.paused:\n            self.viewer.resume()\n        else:\n            self.viewer.pause()\n        return True\n    return super(StatisticsTable, self).keypress(size, key)\n", "label": 1}
{"function": "\n\ndef _analyze(self):\n    ' works out the updates to be performed '\n    if ((self.value is None) or (self.value == self.previous)):\n        pass\n    elif (self._operation == 'append'):\n        self._append = self.value\n    elif (self._operation == 'prepend'):\n        self._prepend = self.value\n    elif (self.previous is None):\n        self._assignments = self.value\n    elif (len(self.value) < len(self.previous)):\n        self._assignments = self.value\n    elif (len(self.previous) == 0):\n        self._assignments = self.value\n    else:\n        search_space = (len(self.value) - max(0, (len(self.previous) - 1)))\n        search_size = len(self.previous)\n        for i in range(search_space):\n            j = (i + search_size)\n            sub = self.value[i:j]\n            idx_cmp = (lambda idx: (self.previous[idx] == sub[idx]))\n            if (idx_cmp(0) and idx_cmp((- 1)) and (self.previous == sub)):\n                self._prepend = (self.value[:i] or None)\n                self._append = (self.value[j:] or None)\n                break\n        if (self._prepend is self._append is None):\n            self._assignments = self.value\n    self._analyzed = True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef obtainSystemConstants():\n    lines = filter(None, map(str.strip, subprocess.check_output(['qhost']).split('\\n')))\n    line = lines[0]\n    items = line.strip().split()\n    num_columns = len(items)\n    cpu_index = None\n    mem_index = None\n    for i in range(num_columns):\n        if (items[i] == 'NCPU'):\n            cpu_index = i\n        elif (items[i] == 'MEMTOT'):\n            mem_index = i\n    if ((cpu_index is None) or (mem_index is None)):\n        RuntimeError('qhost command does not return NCPU or MEMTOT columns')\n    maxCPU = 0\n    maxMEM = MemoryString('0')\n    for line in lines[2:]:\n        items = line.strip().split()\n        if (len(items) < num_columns):\n            RuntimeError('qhost output has a varying number of columns')\n        if ((items[cpu_index] != '-') and (items[cpu_index] > maxCPU)):\n            maxCPU = items[cpu_index]\n        if ((items[mem_index] != '-') and (MemoryString(items[mem_index]) > maxMEM)):\n            maxMEM = MemoryString(items[mem_index])\n    if ((maxCPU is 0) or (maxMEM is 0)):\n        RuntimeError('qhost returned null NCPU or MEMTOT info')\n    return (maxCPU, maxMEM)\n", "label": 1}
{"function": "\n\ndef _fetch_objects(self, doc_type=None):\n    'Fetch all references and convert to their document objects\\n        '\n    object_map = {\n        \n    }\n    for (collection, dbrefs) in self.reference_map.iteritems():\n        if hasattr(collection, 'objects'):\n            col_name = collection._get_collection_name()\n            refs = [dbref for dbref in dbrefs if ((col_name, dbref) not in object_map)]\n            references = collection.objects.in_bulk(refs)\n            for (key, doc) in references.iteritems():\n                object_map[(col_name, key)] = doc\n        else:\n            if isinstance(doc_type, (ListField, DictField, MapField)):\n                continue\n            refs = [dbref for dbref in dbrefs if ((collection, dbref) not in object_map)]\n            if doc_type:\n                references = doc_type._get_db()[collection].find({\n                    '_id': {\n                        '$in': refs,\n                    },\n                })\n                for ref in references:\n                    doc = doc_type._from_son(ref)\n                    object_map[(collection, doc.id)] = doc\n            else:\n                references = get_db()[collection].find({\n                    '_id': {\n                        '$in': refs,\n                    },\n                })\n                for ref in references:\n                    if ('_cls' in ref):\n                        doc = get_document(ref['_cls'])._from_son(ref)\n                    elif (doc_type is None):\n                        doc = get_document(''.join((x.capitalize() for x in collection.split('_'))))._from_son(ref)\n                    else:\n                        doc = doc_type._from_son(ref)\n                    object_map[(collection, doc.id)] = doc\n    return object_map\n", "label": 1}
{"function": "\n\ndef _build_message(self, deleted, failed, failures):\n    msg = ''\n    deleted_msg = []\n    for (resource, value) in deleted.items():\n        if value:\n            if (not msg):\n                msg = 'Deleted'\n            if (not (value == 1)):\n                resource = self._pluralize(resource)\n            deleted_msg.append((' %d %s' % (value, resource)))\n    if deleted_msg:\n        msg += ','.join(deleted_msg)\n    failed_msg = []\n    if failures:\n        if msg:\n            msg += '. '\n        msg += 'The following resources could not be deleted:'\n        for (resource, value) in failed.items():\n            if value:\n                if (not (value == 1)):\n                    resource = self._pluralize(resource)\n                failed_msg.append((' %d %s' % (value, resource)))\n        msg += ','.join(failed_msg)\n    if msg:\n        msg += '.'\n    else:\n        msg = _('Tenant has no supported resources.')\n    return msg\n", "label": 1}
{"function": "\n\ndef _walk_StatIf(self, node):\n    short_if = (getattr(node, 'short_if', False) and (not self._args.get('ignore_tokens')))\n    first = True\n    for (exp, block) in node.exp_block_pairs:\n        if (exp is not None):\n            if first:\n                (yield self._get_text(node, 'if'))\n                first = False\n            else:\n                (yield self._get_text(node, 'elseif'))\n            if short_if:\n                (yield self._get_text(node, '('))\n                self._indent += 1\n                for t in self._walk(exp):\n                    (yield t)\n                self._indent -= 1\n                (yield self._get_text(node, ')'))\n            else:\n                for t in self._walk(exp):\n                    (yield t)\n                (yield self._get_text(node, 'then'))\n                self._indent += 1\n            for t in self._walk(block):\n                (yield t)\n            if (not short_if):\n                self._indent -= 1\n        else:\n            (yield self._get_text(node, 'else'))\n            self._indent += 1\n            for t in self._walk(block):\n                (yield t)\n            self._indent -= 1\n    if (not short_if):\n        (yield self._get_text(node, 'end'))\n", "label": 1}
{"function": "\n\ndef checkpins(contents, designator, errs):\n    pins = re_pins.findall(contents)\n    nums = []\n    for (name, num, x, y, length, numsize, namesize) in pins:\n        if (((int(x) % 100) != 0) or ((int(y) % 100) != 0)):\n            errs.append(\"Pin '{}' not on 100mil grid\".format(name))\n        if ((designator in ('IC', 'U')) and (int(length) not in (100, 150))):\n            errs.append(\"Pin '{}' not 100 or 150mil long, but part is IC or U\".format(name))\n        if ((int(namesize) != 50) or ((int(numsize) != 50) and num.isdigit())):\n            errs.append(\"Pin '{}' font size not 50mil\".format(name))\n        if num.isdigit():\n            nums.append(int(num))\n    if nums:\n        expected = set(range(min(nums), (max(nums) + 1)))\n        if (set(nums) != expected):\n            missing = [str(x) for x in (set(expected) - set(nums))]\n            errs.append('Missing pins {}'.format(', '.join(missing)))\n        duplicates = set([str(x) for x in nums if (nums.count(x) > 1)])\n        if duplicates:\n            errs.append('Duplicated pins {}'.format(', '.join(duplicates)))\n", "label": 1}
{"function": "\n\ndef __init__(self, kernel, it_space, *args, **kwargs):\n    read_args = [a.data for a in args if (a.access in [READ, RW])]\n    written_args = [a.data for a in args if (a.access in [RW, WRITE, MIN, MAX, INC])]\n    inc_args = [a.data for a in args if (a.access in [INC])]\n    LazyComputation.__init__(self, (set(read_args) | Const._defs), set(written_args), set(inc_args))\n    self._kernel = kernel\n    self._actual_args = args\n    self._it_space = it_space\n    for (i, arg) in enumerate(self._actual_args):\n        arg.position = i\n        arg.indirect_position = i\n    for (i, arg1) in enumerate(self._actual_args):\n        if (arg1._is_dat and arg1._is_indirect):\n            for arg2 in self._actual_args[i:]:\n                if ((arg2.data is arg1.data) and (arg2.map is arg1.map)):\n                    arg2.indirect_position = arg1.indirect_position\n    self._all_args = kwargs.get('all_args', [args])\n    self._inspection = kwargs.get('inspection')\n    self._executor = kwargs.get('executor')\n", "label": 1}
{"function": "\n\ndef visit_statement(self, tokens):\n    tables = set()\n    if self._tables:\n        table_tokens = self._GetDescendants(tokens, 'table', do_not_cross=('exclude',))\n        table_aliases = self._GetTableAliases(tokens)\n        for table in table_tokens:\n            table_name = str(table[0])\n            if (table_name in table_aliases):\n                table_name = str(table_aliases[table_name]['table'][0])\n            tables.add(table_name)\n        if (not (self._tables & tables)):\n            return\n    columns = set()\n    if self._columns:\n        columns = set((str(x[0]).lower() for x in self._GetDescendants(tokens, 'column', do_not_cross=('exclude',))))\n        if (not (self._columns & columns)):\n            return\n    values = set()\n    if self._values:\n        values = set((str(x[0]) for x in self._GetDescendants(tokens, 'val', do_not_cross=('exclude',))))\n        if (not (self._values & values)):\n            return\n    operations = set()\n    if self._operations:\n        for operation in self._operations:\n            if self._GetDescendants(tokens, operation):\n                operations.add(operation)\n        if (not operations):\n            return\n    msg = (self._msg % {\n        'tables': tables,\n        'columns': columns,\n        'values': values,\n        'operations': operations,\n    })\n    self.AddWarning(tokens, msg)\n", "label": 1}
{"function": "\n\ndef get_fields(self):\n    to_update = False\n    doc = inspect.getdoc(self.method)\n    if ((not doc) and issubclass(self.method.im_class.model, models.Model)):\n        fields = None\n        if (self.method.__name__ == 'read'):\n            fields = (self.method.im_class.fields if self.method.im_class.fields else tuple((attr.name for attr in self.method.im_class.model._meta.local_fields)))\n        elif (self.method.__name__ in ('create', 'update')):\n            to_update = True\n            if (hasattr(self.method.im_class, 'form') and hasattr(self.method.im_class.form, '_meta')):\n                fields = self.method.im_class.form._meta.fields\n            else:\n                fields = self.method.im_class.fields\n        if fields:\n            for field in fields:\n                for mfield in self.method.im_class.model._meta.fields:\n                    if (mfield.name == field):\n                        (yield {\n                            'name': mfield.name,\n                            'required': ((not mfield.blank) if to_update else False),\n                            'type': get_field_data_type(mfield),\n                            'verbose': mfield.verbose_name,\n                            'help_text': mfield.help_text,\n                        })\n                        break\n", "label": 1}
{"function": "\n\ndef _parse(stream, ptr=0):\n    i = ptr\n    laststr = None\n    lasttok = None\n    deserialized = {\n        \n    }\n    while (i < len(stream)):\n        c = stream[i]\n        if (c == STRING):\n            (string, i) = _symtostr(stream, i)\n            if (lasttok == STRING):\n                deserialized[laststr] = string\n            laststr = string\n        elif (c == NODE_OPEN):\n            (deserialized[laststr], i) = _parse(stream, (i + 1))\n        elif (c == NODE_CLOSE):\n            return (deserialized, i)\n        elif (c == COMMENT):\n            if (((i + 1) < len(stream)) and (stream[(i + 1)] == '/')):\n                i = stream.find('\\n', i)\n        elif ((c == CR) or (c == LF)):\n            ni = (i + 1)\n            if ((ni < len(stream)) and (stream[ni] == LF)):\n                i = ni\n            if (lasttok != LF):\n                c = LF\n        else:\n            c = lasttok\n        lasttok = c\n        i += 1\n    return (deserialized, i)\n", "label": 1}
{"function": "\n\ndef do_show(self, *args):\n    if 'running-config'.startswith(args[0]):\n        if 'vlan'.startswith(args[1]):\n            self.show_run_vlan()\n        if 'interface'.startswith(args[1]):\n            self.show_run_int(args)\n    elif 'interfaces'.startswith(args[0]):\n        self.show_int(args)\n    elif 'vlan'.startswith(args[0]):\n        if args[1].isdigit():\n            self._show_vlan(int(args[1]))\n        elif 'brief'.startswith(args[1]):\n            self.show_vlan_brief()\n        elif 'ethernet'.startswith(args[1]):\n            self.show_vlan_int(args)\n        else:\n            self.write_line(('Invalid input -> %s' % args[1]))\n            self.write_line('Type ? for a list')\n    elif ('ip'.startswith(args[0]) and 'route'.startswith(args[1]) and 'static'.startswith(args[2])):\n        routes = self.switch_configuration.static_routes\n        if routes:\n            self.write_line('        Destination        Gateway        Port          Cost          Type Uptime src-vrf')\n        for (n, route) in enumerate(routes):\n            self.write_line('{index:<8}{destination:<18} {next_hop:}'.format(index=(n + 1), destination=route.dest, next_hop=route.next_hop))\n        self.write_line('')\n    elif 'version'.startswith(args[0]):\n        self.show_version()\n", "label": 1}
{"function": "\n\ndef _validate_shuffle_split_init(test_size, train_size):\n    'Validation helper to check the test_size and train_size at init\\n\\n    NOTE This does not take into account the number of samples which is known\\n    only at split\\n    '\n    if ((test_size is None) and (train_size is None)):\n        raise ValueError('test_size and train_size can not both be None')\n    if (test_size is not None):\n        if (np.asarray(test_size).dtype.kind == 'f'):\n            if (test_size >= 1.0):\n                raise ValueError(('test_size=%f should be smaller than 1.0 or be an integer' % test_size))\n        elif (np.asarray(test_size).dtype.kind != 'i'):\n            raise ValueError(('Invalid value for test_size: %r' % test_size))\n    if (train_size is not None):\n        if (np.asarray(train_size).dtype.kind == 'f'):\n            if (train_size >= 1.0):\n                raise ValueError(('train_size=%f should be smaller than 1.0 or be an integer' % train_size))\n            elif ((np.asarray(test_size).dtype.kind == 'f') and ((train_size + test_size) > 1.0)):\n                raise ValueError(('The sum of test_size and train_size = %f, should be smaller than 1.0. Reduce test_size and/or train_size.' % (train_size + test_size)))\n        elif (np.asarray(train_size).dtype.kind != 'i'):\n            raise ValueError(('Invalid value for train_size: %r' % train_size))\n", "label": 1}
{"function": "\n\ndef _do_skips(cls):\n    reasons = []\n    all_configs = _possible_configs_for_cls(cls, reasons)\n    if getattr(cls, '__skip_if__', False):\n        for c in getattr(cls, '__skip_if__'):\n            if c():\n                config.skip_test((\"'%s' skipped by %s\" % (cls.__name__, c.__name__)))\n    if (not all_configs):\n        if getattr(cls, '__backend__', False):\n            msg = (\"'%s' unsupported for implementation '%s'\" % (cls.__name__, cls.__only_on__))\n        else:\n            msg = (\"'%s' unsupported on any DB implementation %s%s\" % (cls.__name__, ', '.join(((\"'%s(%s)+%s'\" % (config_obj.db.name, '.'.join((str(dig) for dig in config_obj.db.dialect.server_version_info)), config_obj.db.driver)) for config_obj in config.Config.all_configs())), ', '.join(reasons)))\n        config.skip_test(msg)\n    elif hasattr(cls, '__prefer_backends__'):\n        non_preferred = set()\n        spec = exclusions.db_spec(*util.to_list(cls.__prefer_backends__))\n        for config_obj in all_configs:\n            if (not spec(config_obj)):\n                non_preferred.add(config_obj)\n        if all_configs.difference(non_preferred):\n            all_configs.difference_update(non_preferred)\n    if (config._current not in all_configs):\n        _setup_config(all_configs.pop(), cls)\n", "label": 1}
{"function": "\n\ndef step(self, dependency_states, step_context):\n    dep_product_node = self._dep_product_node()\n    dep_product_state = dependency_states.get(dep_product_node, None)\n    if ((dep_product_state is None) or (type(dep_product_state) == Waiting)):\n        return Waiting([dep_product_node])\n    elif (type(dep_product_state) == Throw):\n        return dep_product_state\n    elif (type(dep_product_state) == Noop):\n        return Noop('Could not compute {} to determine dependencies.'.format(dep_product_node))\n    elif (type(dep_product_state) != Return):\n        State.raise_unrecognized(dep_product_state)\n    dependencies = list(self._dependency_nodes(step_context, dep_product_state.value))\n    for dependency in dependencies:\n        dep_state = dependency_states.get(dependency, None)\n        if ((dep_state is None) or (type(dep_state) == Waiting)):\n            return Waiting(([dep_product_node] + dependencies))\n        elif (type(dep_state) == Throw):\n            return dep_state\n        elif (type(dep_state) == Noop):\n            return Throw(ValueError('No source of explicit dependency {}'.format(dependency)))\n        elif (type(dep_state) != Return):\n            raise State.raise_unrecognized(dep_state)\n    return Return([dependency_states[d].value for d in dependencies])\n", "label": 1}
{"function": "\n\ndef to_lines(self):\n    '\\n        Yields:\\n          Chunks of Lua code.\\n        '\n    for token in self._tokens:\n        if (token.matches(lexer.TokComment) or token.matches(lexer.TokSpace)):\n            continue\n        elif token.matches(lexer.TokNewline):\n            if self._saw_if:\n                self._saw_if = False\n                self._last_was_name_keyword_number = False\n                (yield '\\n')\n            continue\n        elif token.matches(lexer.TokName):\n            if self._last_was_name_keyword_number:\n                (yield ' ')\n            self._last_was_name_keyword_number = True\n            (yield self._name_factory.get_short_name(token.code))\n        elif token.matches(lexer.TokKeyword):\n            if (token.code == 'if'):\n                self._saw_if = True\n            if self._last_was_name_keyword_number:\n                (yield ' ')\n            self._last_was_name_keyword_number = True\n            (yield token.code)\n        elif token.matches(lexer.TokNumber):\n            if self._last_was_name_keyword_number:\n                (yield ' ')\n            self._last_was_name_keyword_number = True\n            (yield token.code)\n        else:\n            self._last_was_name_keyword_number = False\n            (yield token.code)\n", "label": 1}
{"function": "\n\ndef post(self, *args, **kwargs):\n    widget = self.object\n    ordering = self.kwargs.get('ordering')\n    if (int(ordering) == 0):\n        widget.ordering = 0\n        widget.save()\n        widgets = getattr(widget.parent.content, widget.region)\n        widgets = [w for w in widgets if (w.id != widget.id)]\n        for (i, _widget) in enumerate(widgets):\n            _widget.ordering = (i + 1)\n            _widget.save()\n    elif (int(ordering) == (- 1)):\n        next_ordering = (widget.ordering - 1)\n        widgets = getattr(widget.parent.content, widget.region)\n        for w in widgets:\n            if (w.ordering == next_ordering):\n                w.ordering = widget.ordering\n                w.save()\n                widget.ordering = next_ordering\n                widget.save()\n    elif (int(ordering) == 1):\n        next_ordering = (widget.ordering + 1)\n        widgets = getattr(widget.parent.content, widget.region)\n        for w in widgets:\n            if (w.ordering == next_ordering):\n                w.ordering = widget.ordering\n                w.save()\n                widget.ordering = next_ordering\n                widget.save()\n    else:\n        widget.ordering = widget.next_ordering\n        widget.save()\n        widgets = getattr(widget.parent.content, widget.region)\n        widgets = [w for w in widgets if (w.id != widget.id)]\n        widgets.sort(key=(lambda w: w.ordering))\n        for (i, _widget) in enumerate(widgets):\n            _widget.ordering = i\n            _widget.save()\n    messages.success(self.request, _('Widget was successfully moved.'))\n    success_url = self.get_success_url()\n    response = HttpResponseRedirect(success_url)\n    response['X-Horizon-Location'] = success_url\n    return response\n", "label": 1}
{"function": "\n\ndef onColor(self, event=None, item=None):\n    color = hexcolor(event.GetValue())\n    setattr(self.parent.conf, item, color)\n    if (item == 'spectra_color'):\n        self.parent.panel.conf.set_trace_color(color, trace=0)\n    elif (item == 'roi_color'):\n        self.parent.panel.conf.set_trace_color(color, trace=1)\n    elif (item == 'marker_color'):\n        for lmark in self.parent.cursor_markers:\n            if (lmark is not None):\n                lmark.set_color(color)\n    elif ((item == 'roi_fillcolor') and (self.parent.roi_patch is not None)):\n        self.parent.roi_patch.set_color(color)\n    elif (item == 'major_elinecolor'):\n        for l in self.parent.major_markers:\n            l.set_color(color)\n    elif (item == 'minor_elinecolor'):\n        for l in self.parent.minor_markers:\n            l.set_color(color)\n    elif (item == 'hold_elinecolor'):\n        for l in self.parent.hold_markers:\n            l.set_color(color)\n    self.parent.panel.canvas.draw()\n    self.parent.panel.Refresh()\n", "label": 1}
{"function": "\n\ndef get_filters(self):\n    is_public = public_filter()\n    is_active = active_filter()\n    is_datetime = datetime_filter(self.filter_yaml.get_image_date())\n    is_tenant = tenant_filter(self.filter_yaml.get_tenant())\n    images_list = self.filter_yaml.get_image_ids()\n    excluded_images_list = self.filter_yaml.get_excluded_image_ids()\n    if (images_list and excluded_images_list):\n        raise exception.AbortMigrationError(\"In the filter config file specified 'images_list' and 'exclude_images_list'. Must be only one list with images - 'images_list' or 'exclude_images_list'.\")\n    if excluded_images_list:\n        is_image_id = image_id_exclude_filter(excluded_images_list)\n    else:\n        is_image_id = image_id_filter(images_list)\n    is_member = member_filter(self.glance_client, self.filter_yaml.get_tenant())\n    if self.filter_yaml.is_public_and_member_images_filtered():\n        return [(lambda i: (is_active(i) and is_tenant(i) and is_image_id(i) and is_datetime(i)))]\n    else:\n        return [(lambda i: ((is_active(i) and is_public(i)) or (is_active(i) and is_member(i)) or (is_active(i) and is_tenant(i) and is_image_id(i) and is_datetime(i))))]\n", "label": 1}
{"function": "\n\ndef image_vacuum(name):\n    '\\n    Delete images not in use or installed via image_present\\n    '\n    name = name.lower()\n    ret = {\n        'name': name,\n        'changes': {\n            \n        },\n        'result': None,\n        'comment': '',\n    }\n    images = []\n    for state in __salt__['state.show_lowstate']():\n        if ('state' not in state):\n            continue\n        if (state['state'] != __virtualname__):\n            continue\n        if (state['fun'] not in ['image_present']):\n            continue\n        if ('name' in state):\n            images.append(state['name'])\n    for image_uuid in __salt__['vmadm.list'](order='image_uuid'):\n        if (image_uuid in images):\n            continue\n        images.append(image_uuid)\n    ret['result'] = True\n    for image_uuid in __salt__['imgadm.list']():\n        if (image_uuid in images):\n            continue\n        if (image_uuid in __salt__['imgadm.delete'](image_uuid)):\n            ret['changes'][image_uuid] = None\n        else:\n            ret['result'] = False\n            ret['comment'] = 'failed to delete images'\n    if (ret['result'] and (len(ret['changes']) == 0)):\n        ret['comment'] = 'no images deleted'\n    elif (ret['result'] and (len(ret['changes']) > 0)):\n        ret['comment'] = 'images deleted'\n    return ret\n", "label": 1}
{"function": "\n\ndef describe_hosts(self, context, **_kwargs):\n    'Returns status info for all nodes. Includes:\\n            * Hostname\\n            * Compute (up, down, None)\\n            * Instance count\\n            * Volume (up, down, None)\\n            * Volume Count\\n        '\n    services = db.service_get_all(context, False)\n    now = utils.utcnow()\n    hosts = []\n    rv = []\n    for host in [service['host'] for service in services]:\n        if (not (host in hosts)):\n            hosts.append(host)\n    for host in hosts:\n        compute = [s for s in services if ((s['host'] == host) and (s['binary'] == 'nova-compute'))]\n        if compute:\n            compute = compute[0]\n        instances = db.instance_get_all_by_host(context, host)\n        volume = [s for s in services if ((s['host'] == host) and (s['binary'] == 'nova-volume'))]\n        if volume:\n            volume = volume[0]\n        volumes = db.volume_get_all_by_host(context, host)\n        rv.append(host_dict(host, compute, instances, volume, volumes, now))\n    return {\n        'hosts': rv,\n    }\n", "label": 1}
{"function": "\n\ndef on_selection_modified(self, view):\n    if JsLintEventListener.disabled:\n        return\n    if (view.name() != RESULT_VIEW_NAME):\n        return\n    region = view.line(view.sel()[0])\n    s = sublime.load_settings(SETTINGS_FILE)\n    if (self.previous_resion == region):\n        return\n    self.previous_resion = region\n    if s.get('use_node_jslint', False):\n        pattern_position = '\\\\/\\\\/ Line (\\\\d+), Pos (\\\\d+)$'\n        text = view.substr(region)\n        text = re.findall(pattern_position, text)\n        if (len(text) > 0):\n            line = int(text[0][0])\n            col = int(text[0][1])\n    else:\n        text = view.substr(region).split(':')\n        if ((len(text) < 4) or (text[0] != 'jslint') or (re.match('\\\\d+', text[2]) is None) or (re.match('\\\\d+', text[3]) is None)):\n            return\n        line = int(text[2])\n        col = int(text[3])\n    view.add_regions(RESULT_VIEW_NAME, [region], 'comment')\n    file_path = view.settings().get('file_path')\n    window = sublime.active_window()\n    file_view = None\n    for v in window.views():\n        if (v.file_name() == file_path):\n            file_view = v\n            break\n    if (file_view is None):\n        return\n    self.file_view = file_view\n    window.focus_view(file_view)\n    file_view.run_command('goto_line', {\n        'line': line,\n    })\n    file_region = file_view.line(file_view.sel()[0])\n    file_view.add_regions(RESULT_VIEW_NAME, [file_region], 'string')\n", "label": 1}
{"function": "\n\ndef visit_Task(self, node):\n    self.push_module(None)\n    name = node.name\n    _task = task.Task(name)\n    statement = [self.visit(s) for s in node.statement]\n    body = []\n    for s in statement:\n        if isinstance(s, (tuple, list)):\n            for d in s:\n                if isinstance(d, vtypes.Input):\n                    t = _task.Input(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Reg):\n                    t = _task.Reg(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Integer):\n                    t = _task.Integer(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                else:\n                    body.append(s)\n        else:\n            body.append(s)\n    _task.Body(*body)\n    self.pop_module()\n    self.add_object(_task)\n    return _task\n", "label": 1}
{"function": "\n\ndef _update(self, frame_no):\n    if self._signal:\n        start_x = int(((self._screen.width - self._signal.max_width) // 2))\n        start_y = int(((self._screen.height - self._signal.max_height) // 2))\n        (text, colours) = self._signal.rendered_text\n    else:\n        start_x = start_y = 0\n        (text, colours) = ('', [])\n    for y in range(self._screen.height):\n        if (self._strength < 1.0):\n            offset = randint(0, int((6 - (6 * self._strength))))\n        else:\n            offset = 0\n        for x in range(self._screen.width):\n            ix = (x - start_x)\n            iy = (y - start_y)\n            if (self._signal and (random() <= self._strength) and (x >= start_x) and (y >= start_y) and (iy < len(text)) and ((ix + offset) < len(text[iy]))):\n                self._screen.paint(text[iy][(ix + offset)], x, y, colour_map=[colours[iy][ix]])\n            elif (random() < 0.2):\n                self._screen.print_at(chr(randint(33, 126)), x, y)\n    self._strength += self._step\n    if ((self._strength >= 1.25) or (self._strength <= (- 0.5))):\n        self._step = (- self._step)\n", "label": 1}
{"function": "\n\ndef parse_grammar(self, data):\n    if ((self.data is None) or (self.data != data)):\n        self.data = data\n        self.regions = []\n    else:\n        if (self.printer is not None):\n            self.printer(0, 'Already parse')\n        return True\n    starttime = clock()\n    if ('compilation_unit' in self.grammar):\n        if (self.printer is not None):\n            self.printer(0, '== Compilation unit ==')\n        begin = 0\n        if (('before_separator' not in self.grammar['compilation_unit']) or self.grammar['compilation_unit']['before_separator']):\n            separator_output = self.parse_rule(self.grammar['separator'], True, '', 0, 0)\n            if separator_output['successive_match']:\n                begin = separator_output['end']\n                self.regions += separator_output['regions']\n        parse_output = self.parse_rule(self.grammar['compilation_unit'], False, '', 0, begin)\n        if parse_output['successive_match']:\n            self.regions += parse_output['regions']\n        if (('after_separator' not in self.grammar['compilation_unit']) or self.grammar['compilation_unit']['after_separator']):\n            separator_output = self.parse_rule(self.grammar['separator'], True, '', 0, parse_output['end'])\n            if separator_output['successive_match']:\n                parse_output['end'] = separator_output['end']\n                self.regions += separator_output['regions']\n    self.elapse_time = (clock() - starttime)\n    return {\n        'success': parse_output['successive_match'],\n        'begin': parse_output['begin'],\n        'end': parse_output['end'],\n    }\n", "label": 1}
{"function": "\n\ndef check_node_group_template_usage(node_group_template_id, **kwargs):\n    cluster_users = []\n    template_users = []\n    for cluster in api.get_clusters():\n        if (node_group_template_id in [node_group.node_group_template_id for node_group in cluster.node_groups]):\n            cluster_users += [cluster.name]\n    for cluster_template in api.get_cluster_templates():\n        if (node_group_template_id in [node_group.node_group_template_id for node_group in cluster_template.node_groups]):\n            template_users += [cluster_template.name]\n    if (cluster_users or template_users):\n        raise ex.InvalidReferenceException((_('Node group template %(template)s is in use by cluster templates: %(users)s; and clusters: %(clusters)s') % {\n            'template': node_group_template_id,\n            'users': ((template_users and ', '.join(template_users)) or 'N/A'),\n            'clusters': ((cluster_users and ', '.join(cluster_users)) or 'N/A'),\n        }))\n", "label": 1}
{"function": "\n\ndef update(self):\n    '\\n        The function to draw a new frame for the particle system.\\n        '\n    if (self.time_left > 0):\n        self.time_left -= 1\n        for _ in range(self._count):\n            new_particle = self._new_particle()\n            if (new_particle is not None):\n                self.particles.append(new_particle)\n    for particle in self.particles:\n        last = particle.last()\n        if (last is not None):\n            (char, x, y, fg, attr, bg) = last\n            screen_data = self._screen.get_from(x, y)\n            if (self._blend and screen_data):\n                (char2, fg2, attr2, bg2) = screen_data\n                index = 0\n                for (i, colours) in enumerate(particle.colours):\n                    if ((fg2, attr2, bg2) == colours):\n                        index = i\n                        break\n                index -= 1\n                (fg, attr, bg) = particle.colours[max(index, 0)]\n            self._screen.print_at(' ', x, y, fg, attr, bg)\n        if (particle.time < particle.life_time):\n            (char, x, y, fg, attr, bg) = particle.next()\n            screen_data = self._screen.get_from(x, y)\n            if (self._blend and screen_data):\n                (char2, fg2, attr2, bg2) = screen_data\n                index = (- 1)\n                for (i, colours) in enumerate(particle.colours):\n                    if ((fg2, attr2, bg2) == colours):\n                        index = i\n                        break\n                index += 1\n                (fg, attr, bg) = particle.colours[min(index, (len(particle.colours) - 1))]\n            self._screen.print_at(char, x, y, fg, attr, bg)\n        else:\n            self.particles.remove(particle)\n", "label": 1}
{"function": "\n\ndef _skip_instance(self):\n    skip = self._buffer.read_bits(8)\n    if (skip == 0):\n        length = self._vint()\n        for i in xrange(length):\n            self._skip_instance()\n    elif (skip == 1):\n        length = self._vint()\n        self._buffer.read_aligned_bytes(((length + 7) / 8))\n    elif (skip == 2):\n        length = self._vint()\n        self._buffer.read_aligned_bytes(length)\n    elif (skip == 3):\n        tag = self._vint()\n        self._skip_instance()\n    elif (skip == 4):\n        exists = (self._buffer.read_bits(8) != 0)\n        if exists:\n            self._skip_instance()\n    elif (skip == 5):\n        length = self._vint()\n        for i in xrange(length):\n            tag = self._vint()\n            self._skip_instance()\n    elif (skip == 6):\n        self._buffer.read_aligned_bytes(1)\n    elif (skip == 7):\n        self._buffer.read_aligned_bytes(4)\n    elif (skip == 8):\n        self._buffer.read_aligned_bytes(8)\n    elif (skip == 9):\n        self._vint()\n", "label": 1}
{"function": "\n\ndef _non_dominated_front_fast(iterable, key=(lambda x: x), allowequality=True):\n    'Return a subset of items from iterable which are not dominated by any\\n    other item in iterable.\\n\\n    Faster version.\\n    '\n    items = list(iterable)\n    keys = dict(((i, key(i)) for i in items))\n    dim = len(list(keys.values())[0])\n    dominations = {\n        \n    }\n    for i in items:\n        for j in items:\n            good = True\n            if allowequality:\n                for k in range(dim):\n                    if (keys[i][k] >= keys[j][k]):\n                        good = False\n                        break\n            else:\n                for k in range(dim):\n                    if (keys[i][k] > keys[j][k]):\n                        good = False\n                        break\n            if good:\n                dominations[(i, j)] = None\n    res = set()\n    items = set(items)\n    for i in items:\n        res.add(i)\n        for j in list(res):\n            if (i is j):\n                continue\n            if ((j, i) in dominations):\n                res.remove(i)\n                break\n            elif ((i, j) in dominations):\n                res.remove(j)\n    return res\n", "label": 1}
{"function": "\n\ndef transpile_md_to_python(markdown):\n    'A very naive markdown to python converter.'\n    for line in markdown.split('\\n'):\n        line = line.lstrip()\n        if line.startswith('# '):\n            (yield Heading(1, line, 'h1'))\n        elif line.startswith('## '):\n            (yield Heading(2, line, 'h2'))\n        elif line.startswith('### '):\n            (yield Heading(3, line, 'h3'))\n        elif line.startswith('#### '):\n            (yield Heading(4, line, 'h4'))\n        elif line.startswith('##### '):\n            (yield Heading(5, line, 'h5'))\n        elif line.startswith('###### '):\n            (yield Heading(6, line, 'h6'))\n        elif (line.startswith('*') and (not line.endswith('**'))):\n            (yield Tag('emphasis', line, 'em'))\n        elif (line.startswith('**') and line.endswith('**')):\n            (yield Tag('strong', line, 'strong'))\n        elif (line.startswith('[') and line.endswith(')')):\n            (yield Tag('href', line, 'a'))\n", "label": 1}
{"function": "\n\ndef visit_Function(self, node):\n    self.push_module(None)\n    name = node.name\n    width = (self.visit(node.retwidth) if (node.retwidth is not None) else None)\n    func = function.Function(name, width)\n    if (node.retwidth is not None):\n        func._set_raw_width(self.visit(node.retwidth.msb), self.visit(node.retwidth.lsb))\n    statement = [self.visit(s) for s in node.statement]\n    body = []\n    for s in statement:\n        if isinstance(s, (tuple, list)):\n            for d in s:\n                if isinstance(d, vtypes.Input):\n                    t = func.Input(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Reg):\n                    t = func.Reg(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Integer):\n                    t = func.Integer(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                else:\n                    body.append(s)\n        else:\n            body.append(s)\n    func.Body(*body)\n    self.pop_module()\n    self.add_object(func)\n    return func\n", "label": 1}
{"function": "\n\ndef __init__(self, disable_openmp):\n    cc = new_compiler()\n    customize_compiler(cc)\n    self.msvc = (cc.compiler_type == 'msvc')\n    self._print_compiler_version(cc)\n    if disable_openmp:\n        self.openmp_enabled = False\n    else:\n        (self.openmp_enabled, openmp_needs_gomp) = self._detect_openmp()\n    self.sse3_enabled = (self._detect_sse3() if (not self.msvc) else True)\n    self.sse41_enabled = (self._detect_sse41() if (not self.msvc) else True)\n    self.compiler_args_sse2 = (['-msse2'] if (not self.msvc) else ['/arch:SSE2'])\n    self.compiler_args_sse3 = (['-mssse3'] if (self.sse3_enabled and (not self.msvc)) else [])\n    (self.compiler_args_sse41, self.define_macros_sse41) = ([], [])\n    if self.sse41_enabled:\n        self.define_macros_sse41 = [('__SSE4__', 1), ('__SSE4_1__', 1)]\n        if (not self.msvc):\n            self.compiler_args_sse41 = ['-msse4']\n    if self.openmp_enabled:\n        self.compiler_libraries_openmp = []\n        if self.msvc:\n            self.compiler_args_openmp = ['/openmp']\n        else:\n            self.compiler_args_openmp = ['-fopenmp']\n            if openmp_needs_gomp:\n                self.compiler_libraries_openmp = ['gomp']\n    else:\n        self.compiler_libraries_openmp = []\n        self.compiler_args_openmp = []\n    if self.msvc:\n        self.compiler_args_opt = ['/O2']\n    else:\n        self.compiler_args_opt = ['-O3', '-funroll-loops']\n    print()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _unbroadcast(grad_op_tree, x, be):\n    '\\n        Reverse broadcast from shape(grad_op_tree) to shape(x)\\n\\n        Arguments:\\n            grad_op_tree (OpTreeNode or Tensor): The OpTreeNode to broadcast.\\n            x (OpTreeNode or Tensor): Provides the dimension to be broadcasted to.\\n            be: (Backend): The backend to be used.\\n\\n        Returns:\\n            OpTreeNode or Tensor: The broadcasted result.\\n        '\n    if ((not grad_op_tree) or (not x)):\n        return grad_op_tree\n    if (type(x) in _scalar_types):\n        return 0.0\n    in_shape = x.shape\n    out_shape = grad_op_tree.shape\n    if (in_shape == out_shape):\n        return grad_op_tree\n    elif ((len(in_shape) == 2) and (len(out_shape) == 2)):\n        if (in_shape == (1, 1)):\n            return be.sum(grad_op_tree)\n        elif ((in_shape[0] == out_shape[0]) and (in_shape[1] == 1)):\n            return be.sum(grad_op_tree, axis=1)\n        elif ((in_shape[0] == 1) and (in_shape[1] == out_shape[1])):\n            return be.sum(grad_op_tree, axis=0)\n        elif (((out_shape[0] == in_shape[0]) and (out_shape[1] == 1)) or ((out_shape[0] == 1) and (out_shape[1] == in_shape[1]))):\n            return ((0 * x) + grad_op_tree)\n        else:\n            return NotImplemented\n    else:\n        return NotImplemented\n", "label": 1}
{"function": "\n\ndef resetSasl(self):\n    network_config = conf.supybot.networks.get(self.network)\n    self.sasl_authenticated = False\n    self.sasl_username = network_config.sasl.username()\n    self.sasl_password = network_config.sasl.password()\n    self.sasl_ecdsa_key = network_config.sasl.ecdsa_key()\n    self.authenticate_decoder = None\n    self.sasl_next_mechanisms = []\n    self.sasl_current_mechanism = None\n    for mechanism in network_config.sasl.mechanisms():\n        if ((mechanism == 'ecdsa-nist256p-challenge') and ecdsa and self.sasl_username and self.sasl_ecdsa_key):\n            self.sasl_next_mechanisms.append(mechanism)\n        elif ((mechanism == 'external') and (network_config.certfile() or conf.supybot.protocols.irc.certfile())):\n            self.sasl_next_mechanisms.append(mechanism)\n        elif ((mechanism == 'plain') and self.sasl_username and self.sasl_password):\n            self.sasl_next_mechanisms.append(mechanism)\n    if self.sasl_next_mechanisms:\n        self.REQUEST_CAPABILITIES.add('sasl')\n", "label": 1}
{"function": "\n\ndef _find_warnings(filename, source, ast_list):\n    count = 0\n    for node in ast_list:\n        if (isinstance(node, ast.Class) and node.body):\n            class_node = node\n            has_virtuals = False\n            for node in node.body:\n                if (isinstance(node, ast.Class) and node.body):\n                    _find_warnings(filename, source, [node])\n                elif (isinstance(node, ast.Function) and (node.modifiers & ast.FUNCTION_VIRTUAL)):\n                    has_virtuals = True\n                    if (node.modifiers & ast.FUNCTION_DTOR):\n                        break\n            else:\n                if (has_virtuals and (not class_node.bases)):\n                    lines = metrics.Metrics(source)\n                    print(('%s:%d' % (filename, lines.get_line_number(class_node.start))), end=' ')\n                    print(\"'{}' has virtual methods without a virtual dtor\".format(class_node.name))\n                    count += 1\n    return count\n", "label": 1}
{"function": "\n\ndef _set_field_names(self, val):\n    val = [self._unicode(x) for x in val]\n    self._validate_option('field_names', val)\n    if self._field_names:\n        old_names = self._field_names[:]\n    self._field_names = val\n    if (self._align and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._align[new_name] = self._align[old_name]\n        for old_name in old_names:\n            if (old_name not in self._align):\n                self._align.pop(old_name)\n    else:\n        for field in self._field_names:\n            self._align[field] = 'c'\n    if (self._valign and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._valign[new_name] = self._valign[old_name]\n        for old_name in old_names:\n            if (old_name not in self._valign):\n                self._valign.pop(old_name)\n    else:\n        for field in self._field_names:\n            self._valign[field] = 't'\n", "label": 1}
{"function": "\n\ndef match_rating_comparison(s1, s2):\n    codex1 = match_rating_codex(s1)\n    codex2 = match_rating_codex(s2)\n    len1 = len(codex1)\n    len2 = len(codex2)\n    res1 = []\n    res2 = []\n    if (abs((len1 - len2)) >= 3):\n        return None\n    lensum = (len1 + len2)\n    if (lensum <= 4):\n        min_rating = 5\n    elif (lensum <= 7):\n        min_rating = 4\n    elif (lensum <= 11):\n        min_rating = 3\n    else:\n        min_rating = 2\n    for (c1, c2) in _zip_longest(codex1, codex2):\n        if (c1 != c2):\n            if c1:\n                res1.append(c1)\n            if c2:\n                res2.append(c2)\n    unmatched_count1 = unmatched_count2 = 0\n    for (c1, c2) in _zip_longest(reversed(res1), reversed(res2)):\n        if (c1 != c2):\n            if c1:\n                unmatched_count1 += 1\n            if c2:\n                unmatched_count2 += 1\n    return ((6 - max(unmatched_count1, unmatched_count2)) >= min_rating)\n", "label": 1}
{"function": "\n\ndef marshal(self):\n    '\\n        Marshal this object into a raw message for subsequent serialization to bytes.\\n\\n        :returns: list -- The serialized raw message.\\n        '\n    options = {\n        \n    }\n    if (self.timeout is not None):\n        options['timeout'] = self.timeout\n    if (self.receive_progress is not None):\n        options['receive_progress'] = self.receive_progress\n    if (self.caller is not None):\n        options['caller'] = self.caller\n    if (self.caller_authid is not None):\n        options['caller_authid'] = self.caller_authid\n    if (self.caller_authrole is not None):\n        options['caller_authrole'] = self.caller_authrole\n    if (self.procedure is not None):\n        options['procedure'] = self.procedure\n    if self.payload:\n        if (self.enc_algo is not None):\n            options['enc_algo'] = self.enc_algo\n        if (self.enc_key is not None):\n            options['enc_key'] = self.enc_key\n        if (self.enc_serializer is not None):\n            options['enc_serializer'] = self.enc_serializer\n        return [Invocation.MESSAGE_TYPE, self.request, self.registration, options, self.payload]\n    elif self.kwargs:\n        return [Invocation.MESSAGE_TYPE, self.request, self.registration, options, self.args, self.kwargs]\n    elif self.args:\n        return [Invocation.MESSAGE_TYPE, self.request, self.registration, options, self.args]\n    else:\n        return [Invocation.MESSAGE_TYPE, self.request, self.registration, options]\n", "label": 1}
{"function": "\n\n@view_config(renderer='save.mak', route_name='save')\ndef save(request):\n    s = request.session\n    p = request.session['safe_params']\n    u = None\n    op = 'add'\n    vote_dict = {\n        \n    }\n    if (('story_id' in p) and ('logged_in' in s)):\n        dbsession = DBSession()\n        u = users.get_user_by_id(s['users.id'])\n        to_save = submission.get_story_by_id(p['story_id'])\n        if ('op' in p):\n            op = p['op']\n        if (op == 'add'):\n            if (to_save not in u.saved):\n                u.saved.append(to_save)\n                dbsession.add(u)\n            s['message'] = 'Successfully saved {0}'.format(to_save.title)\n        elif (op == 'del'):\n            if (to_save in u.saved):\n                u.saved.remove(to_save)\n                dbsession.add(u)\n            s['message'] = 'Successfully unsaved {0}'.format(to_save.title)\n    elif ('logged_in' in s):\n        u = users.get_user_by_id(s['users.id'])\n    if u:\n        vds = []\n        for i in u.saved:\n            vds.append(users.get_user_votes(s['users.id'], 'on_submission', i.id))\n        for vd in vds:\n            if (type(vd) == dict):\n                vote_dict.update(vd)\n    return {\n        'saved': u.saved,\n        'vote_dict': vote_dict,\n    }\n", "label": 1}
{"function": "\n\ndef filter_input_data(self, input_data, by_name=True):\n    'Filters the keys given in input_data checking against model fields\\n\\n        '\n    if isinstance(input_data, dict):\n        for (key, value) in input_data.items():\n            value = self.normalize(value)\n            if (value is None):\n                del input_data[key]\n        if by_name:\n            input_data = dict([[self.inverted_fields[key], value] for (key, value) in input_data.items() if ((key in self.inverted_fields) and ((self.objective_id is None) or (self.inverted_fields[key] != self.objective_id)))])\n        else:\n            input_data = dict([[key, value] for (key, value) in input_data.items() if ((key in self.fields) and ((self.objective_id is None) or (key != self.objective_id)))])\n        return input_data\n    else:\n        LOGGER.error('Failed to read input data in the expected {field:value} format.')\n        return {\n            \n        }\n", "label": 1}
{"function": "\n\ndef __setitem__(self, key, value):\n    if self._set_slice(key, value, self):\n        return\n    if isinstance(value, tuple):\n        if (not (0 < len(value) <= 2)):\n            raise ValueError('A Header item may be set with either a scalar value, a 1-tuple containing a scalar value, or a 2-tuple containing a scalar value and comment string.')\n        if (len(value) == 1):\n            (value, comment) = (value[0], None)\n            if (value is None):\n                value = ''\n        elif (len(value) == 2):\n            (value, comment) = value\n            if (value is None):\n                value = ''\n            if (comment is None):\n                comment = ''\n    else:\n        comment = None\n    card = None\n    if isinstance(key, int):\n        card = self._cards[key]\n    elif isinstance(key, tuple):\n        card = self._cards[self._cardindex(key)]\n    if card:\n        card.value = value\n        if (comment is not None):\n            card.comment = comment\n        if card._modified:\n            self._modified = True\n    else:\n        self._update((key, value, comment))\n", "label": 1}
{"function": "\n\ndef _get_filesystems_and_globs(datetime_to_task, datetime_to_re):\n    '\\n    Yields a (filesystem, glob) tuple per every output location of task.\\n\\n    The task can have one or several FileSystemTarget outputs.\\n\\n    For convenience, the task can be a luigi.WrapperTask,\\n    in which case outputs of all its dependencies are considered.\\n    '\n    sample_datetimes = [datetime(y, m, d, h) for y in range(2000, 2050, 10) for m in range(1, 4) for d in range(5, 8) for h in range(21, 24)]\n    regexes = [re.compile(datetime_to_re(d)) for d in sample_datetimes]\n    sample_tasks = [datetime_to_task(d) for d in sample_datetimes]\n    sample_outputs = [flatten_output(t) for t in sample_tasks]\n    for (o, t) in zip(sample_outputs, sample_tasks):\n        if (len(o) != len(sample_outputs[0])):\n            raise NotImplementedError(('Outputs must be consistent over time, sorry; was %r for %r and %r for %r' % (o, t, sample_outputs[0], sample_tasks[0])))\n        for target in o:\n            if (not isinstance(target, FileSystemTarget)):\n                raise NotImplementedError(('Output targets must be instances of FileSystemTarget; was %r for %r' % (target, t)))\n    for o in zip(*sample_outputs):\n        glob = _get_per_location_glob(sample_tasks, o, regexes)\n        (yield (o[0].fs, glob))\n", "label": 1}
{"function": "\n\ndef writeBuffer(buff, col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall):\n    for x in buff:\n        printPreceedingCharacter = False\n        altOptions = x[col['alt']].split(',')\n        for y in altOptions:\n            if (((len(x[col['ref']]) != len(y)) or (len(x[col['ref']]) == 0) or (len(y) == 0)) and (not re.search('[^ATGCNatgcn\\\\.-]', y))):\n                printPreceedingCharacter = True\n                break\n        if printPreceedingCharacter:\n            writeRowCheck(x[:], col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall)\n    for x in buff:\n        printPreceedingCharacter = False\n        altOptions = x[col['alt']].split(',')\n        for y in altOptions:\n            if (((len(x[col['ref']]) != len(y)) or (len(x[col['ref']]) == 0) or (len(y) == 0)) and (not re.search('[^ATGCNatgcn\\\\.-]', y))):\n                printPreceedingCharacter = True\n                break\n        if (printPreceedingCharacter == False):\n            writeRowCheck(x[:], col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall)\n", "label": 1}
{"function": "\n\ndef reset_settings(self):\n    for sgroup in self.settings['setting_groups']:\n        for setting in sgroup.values():\n            widget = self.find_child_by_name(setting.name)\n            if (widget is None):\n                continue\n            if ((setting.type == 'string') or (setting.type == 'file') or (setting.type == 'folder')):\n                old_val = ''\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val.replace('\\\\', '\\\\\\\\')\n                widget.setText(old_val)\n            elif (setting.type == 'strings'):\n                old_val = []\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = [v.replace('\\\\', '\\\\\\\\') for v in old_val]\n                widget.setText(','.join(setting.value))\n            elif (setting.type == 'check'):\n                old_val = False\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val\n                widget.setChecked(old_val)\n            elif (setting.type == 'range'):\n                old_val = 0\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val\n                widget.setValue(old_val)\n", "label": 1}
{"function": "\n\ndef sets_cv(self, x, y):\n    totals = ([0] * len(x.columns))\n    if self.min:\n        totals = ([1000] * len(x.columns))\n    i = 0\n    for (train, test) in cross_validation.KFold(n=len(y), k=4):\n        i += 1\n        logging.info('RF selector computing importances for fold {i}'.format(i=i))\n        cls = ensemble.RandomForestRegressor\n        if self.classifier:\n            cls = ensemble.RandomForestClassifier\n        rf = cls(n_estimators=self.n, random_state=self.seed, n_jobs=(- 1))\n        rf.fit(x.values[train], y.values[train])\n        importances = rf.feature_importances_\n        if self.min:\n            totals = [min(imp, t) for (imp, t) in zip(importances, totals)]\n        else:\n            totals = [(imp + t) for (imp, t) in zip(importances, totals)]\n    imps = sorted(zip(totals, x.columns), reverse=True)\n    for (i, x) in enumerate(imps):\n        (imp, f) = x\n        logging.debug(('%d\\t%0.4f\\t%s' % (i, imp, f)))\n    if self.thresh:\n        imps = [t for t in imps if (t[0] > self.thresh)]\n    sets = [[t[1] for t in imps[:(i + 1)]] for i in range(len(imps))]\n    return sets\n", "label": 1}
{"function": "\n\ndef make_uri(base, *args, **kwargs):\n    'Assemble a uri based on a base, any number of path segments,\\n    and query string parameters.\\n\\n    '\n    charset = kwargs.pop('charset', 'utf-8')\n    safe = kwargs.pop('safe', '/:')\n    encode_keys = kwargs.pop('encode_keys', True)\n    base_trailing_slash = False\n    if (base and base.endswith('/')):\n        base_trailing_slash = True\n        base = base[:(- 1)]\n    retval = [base]\n    _path = []\n    trailing_slash = False\n    for s in args:\n        if ((s is not None) and isinstance(s, six.string_types)):\n            if ((len(s) > 1) and s.endswith('/')):\n                trailing_slash = True\n            else:\n                trailing_slash = False\n            _path.append(url_quote(s.strip('/'), charset, safe))\n    path_str = ''\n    if _path:\n        path_str = '/'.join(([''] + _path))\n        if trailing_slash:\n            path_str = (path_str + '/')\n    elif base_trailing_slash:\n        path_str = (path_str + '/')\n    if path_str:\n        retval.append(path_str)\n    params_str = url_encode(kwargs, charset, encode_keys)\n    if params_str:\n        retval.extend(['?', params_str])\n    return ''.join(retval)\n", "label": 1}
{"function": "\n\ndef log_action(self, protocol, action, details):\n    '\\n        Logs various different kinds of requests to the console.\\n        '\n    msg = ('[%s] ' % datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S'))\n    if ((protocol == 'http') and (action == 'complete')):\n        msg += ('HTTP %(method)s %(path)s %(status)s [%(time_taken).2f, %(client)s]\\n' % details)\n        if (200 <= details['status'] < 300):\n            msg = self.style.HTTP_SUCCESS(msg)\n        elif (100 <= details['status'] < 200):\n            msg = self.style.HTTP_INFO(msg)\n        elif (details['status'] == 304):\n            msg = self.style.HTTP_NOT_MODIFIED(msg)\n        elif (300 <= details['status'] < 400):\n            msg = self.style.HTTP_REDIRECT(msg)\n        elif (details['status'] == 404):\n            msg = self.style.HTTP_NOT_FOUND(msg)\n        elif (400 <= details['status'] < 500):\n            msg = self.style.HTTP_BAD_REQUEST(msg)\n        else:\n            msg = self.style.HTTP_SERVER_ERROR(msg)\n    elif ((protocol == 'websocket') and (action == 'connected')):\n        msg += ('WebSocket CONNECT %(path)s [%(client)s]\\n' % details)\n    elif ((protocol == 'websocket') and (action == 'disconnected')):\n        msg += ('WebSocket DISCONNECT %(path)s [%(client)s]\\n' % details)\n    sys.stderr.write(msg)\n", "label": 1}
{"function": "\n\ndef __doStemming(self, word, intact_word):\n    'Perform the actual word stemming\\n        '\n    valid_rule = re.compile('^([a-z]+)(\\\\*?)(\\\\d)([a-z]*)([>\\\\.]?)$')\n    proceed = True\n    while proceed:\n        last_letter_position = self.__getLastLetter(word)\n        if ((last_letter_position < 0) or (word[last_letter_position] not in self.rule_dictionary)):\n            proceed = False\n        else:\n            rule_was_applied = False\n            for rule in self.rule_dictionary[word[last_letter_position]]:\n                rule_match = valid_rule.match(rule)\n                if rule_match:\n                    (ending_string, intact_flag, remove_total, append_string, cont_flag) = rule_match.groups()\n                    remove_total = int(remove_total)\n                    if word.endswith(ending_string[::(- 1)]):\n                        if intact_flag:\n                            if ((word == intact_word) and self.__isAcceptable(word, remove_total)):\n                                word = self.__applyRule(word, remove_total, append_string)\n                                rule_was_applied = True\n                                if (cont_flag == '.'):\n                                    proceed = False\n                                break\n                        elif self.__isAcceptable(word, remove_total):\n                            word = self.__applyRule(word, remove_total, append_string)\n                            rule_was_applied = True\n                            if (cont_flag == '.'):\n                                proceed = False\n                            break\n            if (rule_was_applied == False):\n                proceed = False\n    return word\n", "label": 1}
{"function": "\n\ndef _result__repr__(self):\n    '\\n    This is used as the `__repr__` function for the :class:`Result`\\n    '\n    details = []\n    flags = self.__class__._fldprops\n    rcstr = 'rc=0x{0:X}'.format(self.rc)\n    if (self.rc != 0):\n        rcstr += '[{0}]'.format(self.errstr)\n    details.append(rcstr)\n    if ((flags & C.PYCBC_RESFLD_KEY) and hasattr(self, 'key')):\n        details.append('key={0}'.format(repr(self.key)))\n    if ((flags & C.PYCBC_RESFLD_VALUE) and hasattr(self, 'value')):\n        details.append('value={0}'.format(repr(self.value)))\n    if ((flags & C.PYCBC_RESFLD_CAS) and hasattr(self, 'cas')):\n        details.append('cas=0x{cas:x}'.format(cas=self.cas))\n    if ((flags & C.PYCBC_RESFLD_CAS) and hasattr(self, 'flags')):\n        details.append('flags=0x{flags:x}'.format(flags=self.flags))\n    if ((flags & C.PYCBC_RESFLD_HTCODE) and hasattr(self, 'http_status')):\n        details.append('http_status={0}'.format(self.http_status))\n    if ((flags & C.PYCBC_RESFLD_URL) and hasattr(self, 'url')):\n        details.append('url={0}'.format(self.url))\n    if hasattr(self, '_pycbc_repr_extra'):\n        details += self._pycbc_repr_extra()\n    ret = '{0}<{1}>'.format(self.__class__.__name__, ', '.join(details))\n    return ret\n", "label": 1}
{"function": "\n\ndef validate_kwargs(func, kwargs):\n    'Validate arguments to be supplied to func.'\n    func_name = func.__name__\n    argspec = inspect.getargspec(func)\n    all_args = argspec.args[:]\n    defaults = list((argspec.defaults or []))\n    if (inspect.ismethod(func) and (all_args[:1] == ['self'])):\n        all_args[:1] = []\n    if defaults:\n        required = all_args[:(- len(defaults))]\n    else:\n        required = all_args[:]\n    trans = {arg: ((arg.endswith('_') and arg[:(- 1)]) or arg) for arg in all_args}\n    for key in list(kwargs):\n        key_adj = ('%s_' % key)\n        if (key_adj in all_args):\n            kwargs[key_adj] = kwargs.pop(key)\n    supplied = sorted(kwargs)\n    missing = [trans.get(arg, arg) for arg in required if (arg not in supplied)]\n    if missing:\n        raise MeteorError(400, func.err, ('Missing required arguments to %s: %s' % (func_name, ' '.join(missing))))\n    extra = [arg for arg in supplied if (arg not in all_args)]\n    if extra:\n        raise MeteorError(400, func.err, ('Unknown arguments to %s: %s' % (func_name, ' '.join(extra))))\n", "label": 1}
{"function": "\n\ndef _refresh(self):\n    ' Refresh the enabled/visible state of the action set. '\n    window = self.window\n    if (len(self.enabled_for_perspectives) > 0):\n        self.enabled = ((window is not None) and (window.active_perspective is not None) and (window.active_perspective.id in self.enabled_for_perspectives))\n    if (len(self.visible_for_perspectives) > 0):\n        self.visible = ((window is not None) and (window.active_perspective is not None) and (window.active_perspective.id in self.visible_for_perspectives))\n    if (len(self.enabled_for_views) > 0):\n        self.enabled = ((window is not None) and (window.active_part is not None) and (window.active_part.id in self.enabled_for_views))\n    if (len(self.visible_for_views) > 0):\n        self.visible = ((window is not None) and (window.active_part is not None) and (window.active_part.id in self.visible_for_views))\n    return\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.value = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.description = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef timeago(time=False):\n    \"\\n    Get a datetime object or a int() Epoch timestamp and return a\\n    pretty string like 'an hour ago', 'Yesterday', '3 months ago',\\n    'just now', etc\\n    \"\n    from datetime import datetime\n    now = datetime.now()\n    if (type(time) is int):\n        diff = (now - datetime.fromtimestamp(time))\n    elif isinstance(time, datetime):\n        diff = (now - time)\n    elif (not time):\n        diff = (now - now)\n    second_diff = diff.seconds\n    day_diff = diff.days\n    if (day_diff < 0):\n        return ''\n    if (day_diff == 0):\n        if (second_diff < 10):\n            return 'just now'\n        if (second_diff < 60):\n            return (str(second_diff) + ' seconds ago')\n        if (second_diff < 120):\n            return 'a minute ago'\n        if (second_diff < 3600):\n            return (str((second_diff / 60)) + ' minutes ago')\n        if (second_diff < 7200):\n            return 'an hour ago'\n        if (second_diff < 86400):\n            return (str((second_diff / 3600)) + ' hours ago')\n    if (day_diff == 1):\n        return 'Yesterday'\n    if (day_diff < 7):\n        return (str(day_diff) + ' days ago')\n    if (day_diff < 31):\n        return (str((day_diff / 7)) + ' weeks ago')\n    if (day_diff < 365):\n        return (str((day_diff / 30)) + ' months ago')\n    return (str((day_diff / 365)) + ' years ago')\n", "label": 1}
{"function": "\n\ndef version_sidebar(request, form_data, facets):\n    appver = ''\n    if (('appver' in request.GET) or form_data.get('appver')):\n        appver = form_data.get('appver')\n    app = unicode(request.APP.pretty)\n    exclude_versions = getattr(request.APP, 'exclude_versions', [])\n    rv = [FacetLink(_('Any {0}').format(app), dict(appver='any'), (not appver))]\n    vs = [dict_from_int(f['term']) for f in facets['appversions']]\n    av_dict = version_dict(appver)\n    if (av_dict and (av_dict not in vs) and av_dict['major']):\n        vs.append(av_dict)\n    vs = set(((v['major'], (v['minor1'] if (v['minor1'] not in (None, 99)) else 0)) for v in vs))\n    versions = [('%s.%s' % v) for v in sorted(vs, reverse=True)]\n    for (version, floated) in zip(versions, map(float, versions)):\n        if ((floated not in exclude_versions) and (floated > request.APP.min_display_version)):\n            rv.append(FacetLink(('%s %s' % (app, version)), dict(appver=version), (appver == version)))\n    return rv\n", "label": 1}
{"function": "\n\ndef add_total_row(result, columns):\n    total_row = ([''] * len(columns))\n    has_percent = []\n    for row in result:\n        for (i, col) in enumerate(columns):\n            fieldtype = None\n            if isinstance(col, basestring):\n                col = col.split(':')\n                if (len(col) > 1):\n                    fieldtype = col[1]\n                    if ('/' in fieldtype):\n                        fieldtype = fieldtype.split('/')[0]\n            else:\n                fieldtype = col.get('fieldtype')\n            if ((fieldtype in ['Currency', 'Int', 'Float', 'Percent']) and flt(row[i])):\n                total_row[i] = (flt(total_row[i]) + flt(row[i]))\n            if ((fieldtype == 'Percent') and (i not in has_percent)):\n                has_percent.append(i)\n    for i in has_percent:\n        total_row[i] = (total_row[i] / len(result))\n    first_col_fieldtype = None\n    if isinstance(columns[0], basestring):\n        first_col = columns[0].split(':')\n        if (len(first_col) > 1):\n            first_col_fieldtype = first_col[1].split('/')[0]\n    else:\n        first_col_fieldtype = columns[0].get('fieldtype')\n    if (first_col_fieldtype not in ['Currency', 'Int', 'Float', 'Percent']):\n        if (first_col_fieldtype == 'Link'):\n            total_row[0] = ((\"'\" + _('Total')) + \"'\")\n        else:\n            total_row[0] = _('Total')\n    result.append(total_row)\n    return result\n", "label": 1}
{"function": "\n\ndef _check_fusion(self, root):\n    roots = root.table._root_tables()\n    validator = ExprValidator([root.table])\n    fused_exprs = []\n    can_fuse = False\n    resolved = _maybe_resolve_exprs(root.table, self.input_exprs)\n    if (not resolved):\n        return None\n    for val in resolved:\n        lifted_val = substitute_parents(val)\n        if (isinstance(val, ir.TableExpr) and (self.parent.op().is_ancestor(val) or ((len(roots) == 1) and (val._root_tables()[0] is roots[0])))):\n            can_fuse = True\n            have_root = False\n            for y in root.selections:\n                if y.equals(root.table):\n                    fused_exprs.append(root.table)\n                    have_root = True\n                    continue\n                fused_exprs.append(y)\n            if ((not have_root) and (len(root.selections) == 0)):\n                fused_exprs = ([root.table] + fused_exprs)\n        elif validator.validate(lifted_val):\n            can_fuse = True\n            fused_exprs.append(lifted_val)\n        elif (not validator.validate(val)):\n            can_fuse = False\n            break\n        else:\n            fused_exprs.append(val)\n    if can_fuse:\n        return ops.Selection(root.table, fused_exprs, predicates=root.predicates, sort_keys=root.sort_keys)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef inspect(self, func):\n    ' Return a dict that maps parameter names to injection points for the provided callable. '\n    func = _unwrap(func)\n    if py32:\n        (args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations) = inspect.getfullargspec(func)\n    else:\n        (args, varargs, keywords, defaults) = inspect.getargspec(func)\n        (kwonlyargs, kwonlydefaults, annotations) = ([], {\n            \n        }, {\n            \n        })\n    defaults = (defaults or ())\n    kwonlydefaults = (kwonlydefaults or {\n        \n    })\n    injection_points = {\n        \n    }\n    for arg in args[:(len(args) - len((defaults or [])))]:\n        if (arg not in self._never_inject):\n            injection_points[arg] = _InjectionPoint(arg, implicit=True)\n    for (arg, value) in zip(args[::(- 1)], defaults[::(- 1)]):\n        if isinstance(value, _InjectionPoint):\n            injection_points[arg] = value\n    for (arg, value) in kwonlydefaults.items():\n        if isinstance(value, _InjectionPoint):\n            injection_points[arg] = value\n    for (arg, value) in annotations.items():\n        if isinstance(value, _InjectionPoint):\n            injection_points[arg] = value\n    return injection_points\n", "label": 1}
{"function": "\n\ndef _define_interface(self, plots, allow_mismatch):\n    parameters = [{k: v.precedence for (k, v) in plot.params().items() if ((v.precedence is None) or (v.precedence >= 0))} for plot in plots]\n    param_sets = [set(params.keys()) for params in parameters]\n    if ((not allow_mismatch) and (not all(((pset == param_sets[0]) for pset in param_sets)))):\n        raise Exception('All selectable plot classes must have identical plot options.')\n    styles = [plot.style_opts for plot in plots]\n    if ((not allow_mismatch) and (not all(((style == styles[0]) for style in styles)))):\n        raise Exception('All selectable plot classes must have identical style options.')\n    return (styles[0], parameters[0])\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    outcols = []\n    if self.rgb:\n        outcols.append(self.rgb)\n    if (self.thickEnd or outcols):\n        outcols.append((self.thickEnd if self.thickEnd else self.end))\n    if (self.thickStart or outcols):\n        outcols.append((self.thickStart if self.thickStart else self.start))\n    if (self.strand or outcols):\n        outcols.append(self.strand)\n    if ((self.score_int != '') or outcols):\n        outcols.append(self.score_int)\n    if (self.name or outcols):\n        outcols.append(self.name)\n    outcols.append(self.end)\n    outcols.append(self.start)\n    outcols.append(self.chrom)\n    return '\\t'.join([str(x) for x in outcols[::(- 1)]])\n", "label": 1}
{"function": "\n\ndef assertMessage(self, response, content, level=None, tags=None, limit=None):\n    '\\n        Asserts that the response has a particular message in its context.\\n        \\n        If limit is provided, checks that there are at most the specified\\n        number of messages.\\n        \\n        If level or tags are specified, checks for existence of message having\\n        matching content, level and/or tags. Otherwise, it simply checks for a\\n        message with the content.\\n        \\n        '\n    self.assertTrue((hasattr(response, 'context') and response.context), 'The response must have a non-empty context attribute.')\n    messages = list((response.context['messages'] if ('messages' in response.context) else []))\n    self.assertTrue(bool(messages), \"The response's context must contain at least one message.\")\n    if limit:\n        self.assertGreaterEqual(limit, len(messages), \"The response's context must have at most {limit:d} messages, but it has {actual:d} messages.\".format(limit=limit, actual=len(messages)))\n    self.assertTrue(any((((message.message == content) and ((not level) or (message.level == level)) and ((not tags) or (set((tag.strip() for tag in (message.tags or '').split(' ') if tag)) == set((tag.strip() for tag in (tags or '').split(' ') if tag))))) for message in messages)), \"A message matching the content, level and tags was not found in the response's context.\")\n", "label": 1}
{"function": "\n\ndef randwalk(n, s=1, s3='good'):\n    state = s\n    if (s3 == 'good'):\n        p32 = 0.2\n        p33 = 0.9\n    else:\n        p32 = 0.1\n        p33 = 0.8\n    path = str(state)\n    for i in range(n):\n        if (state == 1):\n            p = random()\n            if (p <= 0.9):\n                path += str(state)\n            else:\n                state = 2\n                path += str(state)\n        elif (state == 2):\n            p = random()\n            if (p <= 0.1):\n                state = 1\n                path += str(state)\n            elif (p <= 0.9):\n                path += str(state)\n            else:\n                state = 3\n                path += str(state)\n        elif (state == 3):\n            p = random()\n            if (p <= p32):\n                state = 2\n                path += str(state)\n            elif (p <= p33):\n                path += str(state)\n            else:\n                state = 4\n                path += str(state)\n        elif (state == 4):\n            p = random()\n            if (p <= 0.6):\n                path += str(state)\n            else:\n                state = 0\n                path += str(state)\n        else:\n            return path\n            break\n    return path\n", "label": 1}
{"function": "\n\ndef find(pattern, path='.', exclude=None, recursive=True):\n    'Find files that match *pattern* in *path*'\n    import fnmatch\n    import os\n    if recursive:\n        for (root, dirnames, filenames) in os.walk(path):\n            for pat in _to_list(pattern):\n                for filename in fnmatch.filter(filenames, pat):\n                    filepath = join(abspath(root), filename)\n                    for excl in _to_list(exclude):\n                        if (excl and fnmatch.fnmatch(filepath, excl)):\n                            break\n                    else:\n                        (yield filepath)\n    else:\n        for pat in _to_list(pattern):\n            for filename in fnmatch.filter(list(path), pat):\n                filepath = join(abspath(path), filename)\n                for excl in _to_list(exclude):\n                    if (excl and fnmatch.fnmatch(filepath, excl)):\n                        break\n                    else:\n                        (yield filepath)\n", "label": 1}
{"function": "\n\n@classmethod\ndef deserialize(cls, buf):\n    'Returns a Match object deserialized from a sequence of bytes.\\n\\n        Args:\\n            buf: A ReceiveBuffer object that contains the bytes that\\n                are the serialized form of the Match object.\\n\\n        Returns:\\n            A new Match object deserialized from the buffer.\\n\\n        Raises:\\n            ValueError: The buffer has an invalid number of available\\n                bytes, or some elements cannot be deserialized.\\n        '\n    (wildcards_ser, in_port, dl_src, dl_dst, dl_vlan, dl_vlan_pcp, dl_type, nw_tos, nw_proto, nw_src, nw_dst, tp_src, tp_dst) = buf.unpack(cls.FORMAT)\n    wildcards = Wildcards.deserialize(wildcards_ser)\n    if (nw_tos & 3):\n        nw_tos &= 252\n    nw_src_prefix_length = (32 - wildcards.nw_src)\n    nw_dst_prefix_length = (32 - wildcards.nw_dst)\n    return Match((None if wildcards.in_port else in_port), (None if wildcards.dl_src else dl_src), (None if wildcards.dl_dst else dl_dst), (None if wildcards.dl_vlan else dl_vlan), (None if wildcards.dl_vlan_pcp else dl_vlan_pcp), (None if wildcards.dl_type else dl_type), (None if wildcards.nw_tos else nw_tos), (None if wildcards.nw_proto else nw_proto), ((nw_src, nw_src_prefix_length) if (nw_src_prefix_length > 0) else None), ((nw_dst, nw_dst_prefix_length) if (nw_dst_prefix_length > 0) else None), (None if wildcards.tp_src else tp_src), (None if wildcards.tp_dst else tp_dst))\n", "label": 1}
{"function": "\n\n@classmethod\ndef deserialize(cls, buf):\n    'Returns a PortStats object deserialized from a sequence of bytes.\\n\\n        Args:\\n            buf: A ReceiveBuffer object that contains the bytes that\\n                are the serialized form of the PortStats object.\\n\\n        Returns:\\n            A new PortStats object deserialized from the buffer.\\n\\n        Raises:\\n            ValueError: The buffer has an invalid number of available\\n                bytes.\\n        '\n    (port_no, rx_packets, tx_packets, rx_bytes, tx_bytes, rx_dropped, tx_dropped, rx_errors, tx_errors, rx_frame_err, rx_over_err, rx_crc_err, collisions) = buf.unpack(cls.FORMAT)\n    return PortStats(port_no, (None if (rx_packets == cls._UNAVAILABLE) else rx_packets), (None if (tx_packets == cls._UNAVAILABLE) else tx_packets), (None if (rx_bytes == cls._UNAVAILABLE) else rx_bytes), (None if (tx_bytes == cls._UNAVAILABLE) else tx_bytes), (None if (rx_dropped == cls._UNAVAILABLE) else rx_dropped), (None if (tx_dropped == cls._UNAVAILABLE) else tx_dropped), (None if (rx_errors == cls._UNAVAILABLE) else rx_errors), (None if (tx_errors == cls._UNAVAILABLE) else tx_errors), (None if (rx_frame_err == cls._UNAVAILABLE) else rx_frame_err), (None if (rx_over_err == cls._UNAVAILABLE) else rx_over_err), (None if (rx_crc_err == cls._UNAVAILABLE) else rx_crc_err), (None if (collisions == cls._UNAVAILABLE) else collisions))\n", "label": 1}
{"function": "\n\ndef _print_Mul(self, expr):\n    prec = precedence(expr)\n    (c, e) = expr.as_coeff_Mul()\n    if (c < 0):\n        expr = _keep_coeff((- c), e)\n        sign = '-'\n    else:\n        sign = ''\n    a = []\n    b = []\n    if (self.order not in ('old', 'none')):\n        args = expr.as_ordered_factors()\n    else:\n        args = Mul.make_args(expr)\n    for item in args:\n        if (item.is_commutative and item.is_Pow and item.exp.is_Rational and item.exp.is_negative):\n            if (item.exp != (- 1)):\n                b.append(Pow(item.base, (- item.exp), evaluate=False))\n            else:\n                b.append(Pow(item.base, (- item.exp)))\n        else:\n            a.append(item)\n    a = (a or [S.One])\n    a_str = [self.parenthesize(x, prec) for x in a]\n    b_str = [self.parenthesize(x, prec) for x in b]\n    if (len(b) == 0):\n        return (sign + '*'.join(a_str))\n    elif (len(b) == 1):\n        return (((sign + '*'.join(a_str)) + '/') + b_str[0])\n    else:\n        return ((sign + '*'.join(a_str)) + ('/(%s)' % '*'.join(b_str)))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef merge(left, right, func):\n    if (left is right):\n        return left\n    if (left is None):\n        (left, right) = (right, left)\n    default = left.default\n    merge = _TreeListSub.merge\n    if (right is None):\n        direct = [func(x, default) for x in left.direct]\n        children = [merge(child, None, func) for child in left.children]\n        if ((direct == left.direct) and (children == left.children)):\n            return left\n        return _TreeListSub(default, direct, children)\n    direct = [func(x, y) for (x, y) in zip(left.direct, right.direct)]\n    children = [merge(c1, c2, func) for (c1, c2) in zip(left.children, right.children)]\n    if ((direct == left.direct) and (children == left.children)):\n        return left\n    if ((direct == right.direct) and (children == right.children)):\n        return right\n    return _TreeListSub(default, direct, children)\n", "label": 1}
{"function": "\n\ndef RemoveNativelySupportedComponents(filters, orders, exists):\n    ' Removes query components that are natively supported by the datastore.\\n\\n  The resulting filters and orders should not be used in an actual query.\\n\\n  Args:\\n    filters: the filters set on the query\\n    orders: the orders set on the query\\n    exists: the names of properties that require an exists filter if\\n      not already specified\\n\\n  Returns:\\n    (filters, orders) the reduced set of filters and orders\\n  '\n    (filters, orders) = Normalize(filters, orders, exists)\n    for f in filters:\n        if (f.op() in EXISTS_OPERATORS):\n            return (filters, orders)\n    has_key_desc_order = False\n    if (orders and (orders[(- 1)].property() == datastore_types.KEY_SPECIAL_PROPERTY)):\n        if (orders[(- 1)].direction() == ASCENDING):\n            orders = orders[:(- 1)]\n        else:\n            has_key_desc_order = True\n    if (not has_key_desc_order):\n        for f in filters:\n            if ((f.op() in INEQUALITY_OPERATORS) and (f.property(0).name() != datastore_types.KEY_SPECIAL_PROPERTY)):\n                break\n        else:\n            filters = [f for f in filters if (f.property(0).name() != datastore_types.KEY_SPECIAL_PROPERTY)]\n    return (filters, orders)\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    if (not [b for b in bases if isinstance(b, HideMetaOpts)]):\n        return super(HideMetaOpts, cls).__new__(cls, name, bases, attrs)\n    else:\n        meta_opts = deepcopy(cls.default_meta_opts)\n        if (('Meta' in attrs) and (attrs['Meta'].__module__ != 'django.db.models.query_utils')):\n            meta = attrs.get('Meta')\n        else:\n            for base in bases:\n                meta = getattr(base, '_meta', None)\n                if meta:\n                    break\n        if meta:\n            for (opt, value) in vars(meta).items():\n                if ((opt not in models.options.DEFAULT_NAMES) and (cls.hide_unknown_opts or (opt in meta_opts))):\n                    meta_opts[opt] = value\n                    delattr(meta, opt)\n        new_class = super(HideMetaOpts, cls).__new__(cls, name, bases, attrs)\n        if meta:\n            for opt in meta_opts:\n                setattr(meta, opt, meta_opts[opt])\n        for opt in meta_opts:\n            setattr(new_class._meta, opt, meta_opts[opt])\n        return new_class\n", "label": 1}
{"function": "\n\ndef cache_response(self, request, response, body=None):\n    '\\n        Algorithm for caching requests.\\n\\n        This assumes a requests Response object.\\n        '\n    if (response.status not in [200, 203, 300, 301]):\n        return\n    response_headers = CaseInsensitiveDict(response.headers)\n    cc_req = self.parse_cache_control(request.headers)\n    cc = self.parse_cache_control(response_headers)\n    cache_url = self.cache_url(request.url)\n    no_store = (cc.get('no-store') or cc_req.get('no-store'))\n    if (no_store and self.cache.get(cache_url)):\n        self.cache.delete(cache_url)\n    if (self.cache_etags and ('etag' in response_headers)):\n        self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n    elif (response.status == 301):\n        self.cache.set(cache_url, self.serializer.dumps(request, response))\n    elif ('date' in response_headers):\n        if (cc and cc.get('max-age')):\n            if (int(cc['max-age']) > 0):\n                self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n        elif ('expires' in response_headers):\n            if response_headers['expires']:\n                self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n", "label": 1}
{"function": "\n\ndef match(self, value=None, name=None):\n    nv = vv = False\n    if value:\n        if re.search(value, self.value):\n            vv = True\n        else:\n            vv = False\n    if name:\n        if re.search(name, self.name):\n            nv = True\n        else:\n            nv = False\n    if (name and value):\n        return (nv and vv)\n    if (name and (not value)):\n        return nv\n    if ((not name) and value):\n        return vv\n    if ((not name) and (not value)):\n        return False\n", "label": 1}
{"function": "\n\n@classmethod\ndef size(cls, val, str_cache=None, object_cache=None, traits_cache=None):\n    if (str_cache is None):\n        str_cache = []\n    if (object_cache is None):\n        object_cache = []\n    if (traits_cache is None):\n        traits_cache = []\n    size = U8.size\n    if (isinstance(val, bool) and (val in (False, True))):\n        pass\n    elif (val is None):\n        pass\n    elif isinstance(val, integer_types):\n        if ((val < AMF3_MIN_INTEGER) or (val > AMF3_MAX_INTEGER)):\n            size += AMF3Double.size\n        else:\n            size += AMF3Integer.size(val)\n    elif isinstance(val, float):\n        size += AMF3Double.size\n    elif isinstance(val, (AMF3Array, list)):\n        size += AMF3ArrayPacker.size(val, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif isinstance(val, string_types):\n        size += AMF3String.size(val, cache=str_cache)\n    elif isinstance(val, AMF3ObjectBase):\n        size += AMF3ObjectPacker.size(val, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif isinstance(val, AMF3Date):\n        size += AMF3DatePacker.size(val, cache=object_cache)\n    else:\n        raise ValueError('Unable to pack value of type {0}'.format(type(val)))\n    return size\n", "label": 1}
{"function": "\n\ndef check_if_doc_is_dynamically_linked(doc, method='Delete'):\n    'Raise `frappe.LinkExistsError` if the document is dynamically linked'\n    for df in get_dynamic_link_map().get(doc.doctype, []):\n        if (df.parent in ('Communication', 'ToDo', 'DocShare', 'Email Unsubscribe')):\n            continue\n        meta = frappe.get_meta(df.parent)\n        if meta.issingle:\n            refdoc = frappe.db.get_singles_dict(df.parent)\n            if ((refdoc.get(df.options) == doc.doctype) and (refdoc.get(df.fieldname) == doc.name) and (((method == 'Delete') and (refdoc.docstatus < 2)) or ((method == 'Cancel') and (refdoc.docstatus == 1)))):\n                frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, df.parent, ''), frappe.LinkExistsError)\n        else:\n            for refdoc in frappe.db.sql('select name, docstatus from `tab{parent}` where\\n\\t\\t\\t\\t{options}=%s and {fieldname}=%s'.format(**df), (doc.doctype, doc.name), as_dict=True):\n                if (((method == 'Delete') and (refdoc.docstatus < 2)) or ((method == 'Cancel') and (refdoc.docstatus == 1))):\n                    frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, df.parent, refdoc.name), frappe.LinkExistsError)\n", "label": 1}
{"function": "\n\ndef _rule_to_post_data(rule):\n    post_data = {\n        \n    }\n    post_data['ruleType'] = rule['ruleType']\n    if ('filter' in rule):\n        filter = rule['filter']\n        if ('httpProtocol' in filter):\n            post_data['filterProtocol'] = filter['httpProtocol']\n        if ('method' in filter):\n            post_data['filterMethod'] = filter['method']\n        if ('url' in filter):\n            post_data['filterUrl'] = filter['url']\n        if ('statusCode' in filter):\n            post_data['filterstatusCode'] = filter['statusCode']\n    if ('action' in rule):\n        action = rule['action']\n        if ('type' in action):\n            post_data['actionType'] = action['type']\n        if ('httpProtocol' in action):\n            post_data['actionProtocol'] = action['httpProtocol']\n        if ('method' in action):\n            post_data['actionMethod'] = action['method']\n        if ('url' in action):\n            post_data['actionUrl'] = action['url']\n        if ('statusCode' in action):\n            post_data['actionStatusCode'] = action['statusCode']\n        if ('statusDescription' in action):\n            post_data['actionStatusDescription'] = action['statusDescription']\n        if ('payload' in action):\n            post_data['actionPayload'] = action['payload']\n        if ('setHeaders' in action):\n            action['headers'] = action.pop('setHeaders')\n    return post_data\n", "label": 1}
{"function": "\n\ndef _ask(self, stage, args, tag):\n    self._report_driver.report_sync(tag, report_type=stage)\n    while True:\n        answer = raw_input('Continue? ([d]etailed/[C]oncise report,[y]es,[n]o,[r]etry): ')\n        if ((not answer) or (answer == 'c') or (answer == 'C')):\n            self._report_driver.report_sync(tag, report_type=stage)\n        elif ((answer == 'd') or (answer == 'D')):\n            self._report_driver.report_sync(tag, report_type=stage, detailed=True)\n        elif ((answer == 'Y') or (answer == 'y')):\n            return True\n        elif ((answer == 'N') or (answer == 'n')):\n            return False\n        elif ((answer == 'R') or (answer == 'r')):\n            if (stage == 'fetch'):\n                self._fetch(args)\n            if (stage == 'checkout'):\n                self._checkout(args)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.privilege = TSentryPrivilege()\n                self.privilege.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef data(self, index, role=Qt.DisplayRole):\n    if ((not index.isValid()) or (not (0 <= index.row() < len(self.packages)))):\n        return to_qvariant()\n    package = self.packages[index.row()]\n    column = index.column()\n    if ((role == Qt.CheckStateRole) and (column == CHECK)):\n        return to_qvariant((package in self.checked))\n    elif (role == Qt.DisplayRole):\n        if (column == NAME):\n            return to_qvariant(package.name)\n        elif (column == VERSION):\n            return to_qvariant(package.version)\n        elif (column == ACTION):\n            action = self.actions.get(package)\n            if (action is not None):\n                return to_qvariant(action)\n        elif (column == DESCRIPTION):\n            return to_qvariant(package.description)\n    elif (role == Qt.TextAlignmentRole):\n        if (column == ACTION):\n            return to_qvariant(int((Qt.AlignRight | Qt.AlignVCenter)))\n        else:\n            return to_qvariant(int((Qt.AlignLeft | Qt.AlignVCenter)))\n    elif (role == Qt.BackgroundColorRole):\n        if (package in self.checked):\n            color = QColor(Qt.darkGreen)\n            color.setAlphaF(0.1)\n            return to_qvariant(color)\n        else:\n            color = QColor(Qt.lightGray)\n            color.setAlphaF(0.3)\n            return to_qvariant(color)\n    return to_qvariant()\n", "label": 1}
{"function": "\n\ndef _get_focus_next(self, focus_dir):\n    current = self\n    walk_tree = ('walk' if (focus_dir is 'focus_next') else 'walk_reverse')\n    while 1:\n        while (getattr(current, focus_dir) is not None):\n            current = getattr(current, focus_dir)\n            if ((current is self) or (current is StopIteration)):\n                return None\n            if (current.is_focusable and (not current.disabled)):\n                return current\n        itr = getattr(current, walk_tree)(loopback=True)\n        if (focus_dir is 'focus_next'):\n            next(itr)\n        for current in itr:\n            if isinstance(current, FocusBehavior):\n                break\n        if isinstance(current, FocusBehavior):\n            if (current is self):\n                return None\n            if (current.is_focusable and (not current.disabled)):\n                return current\n        else:\n            return None\n", "label": 1}
{"function": "\n\ndef _create_result(cmd, params):\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ)\n    result = Result()\n    for line in p.stdout.readlines():\n        if (sys.version_info[0] == 3):\n            line = line.decode(sys.stdout.encoding)\n        result._add_stdout_line(line)\n    for line in p.stderr.readlines():\n        if (sys.version_info[0] == 3):\n            line = line.decode(sys.stderr.encoding)\n        result._add_stderr_line(line)\n    p.wait()\n    if ((_is_param_set(params, _PARAM_PRINT_STDOUT) or config.PRINT_STDOUT_ALWAYS) and (len(result.stdout) > 0)):\n        _print_stdout(result.stdout)\n    if ((_is_param_set(params, _PARAM_PRINT_STDERR) or config.PRINT_STDERR_ALWAYS) and (len(result.stderr) > 0)):\n        if _is_colorama_enabled():\n            _print_stderr(((Fore.RED + result.stderr) + Style.RESET_ALL))\n        else:\n            _print_stderr(result.stderr)\n    result.returncode = p.returncode\n    if ((p.returncode != 0) and (not _is_param_set(params, _PARAM_NO_THROW))):\n        raise NonZeroReturnCodeError(cmd, result)\n    return result\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kw):\n    super(DateConverter, self).__init__(*args, **kw)\n    month_style = (self.month_style or DateConverter.month_style).lower()\n    accept_day = bool(self.accept_day)\n    self.accept_day = self.accept_day\n    if (month_style in ('mdy', 'md', 'mm/dd/yyyy', 'mm/dd', 'us', 'american')):\n        month_style = 'mdy'\n    elif (month_style in ('dmy', 'dm', 'dd/mm/yyyy', 'dd/mm', 'euro', 'european')):\n        month_style = 'dmy'\n    elif (month_style in ('ymd', 'ym', 'yyyy/mm/dd', 'yyyy/mm', 'iso', 'china', 'chinese')):\n        month_style = 'ymd'\n    else:\n        raise TypeError(('Bad month_style: %r' % month_style))\n    self.month_style = month_style\n    separator = self.separator\n    if ((not separator) or (separator == 'auto')):\n        separator = dict(mdy='/', dmy='.', ymd='-')[month_style]\n    elif (separator not in ('-', '.', '/', '\\\\')):\n        raise TypeError(('Bad separator: %r' % separator))\n    self.separator = separator\n    self.format = separator.join((self._formats[part] for part in month_style if ((part != 'd') or accept_day)))\n    self.human_format = separator.join((self._human_formats[part] for part in month_style if ((part != 'd') or accept_day)))\n", "label": 1}
{"function": "\n\ndef _parse_see_also(self, content):\n    '\\n        func_name : Descriptive text\\n            continued text\\n        another_func_name : Descriptive text\\n        func_name1, func_name2, func_name3\\n\\n        '\n    functions = []\n    current_func = None\n    rest = []\n    for line in content:\n        if (not line.strip()):\n            continue\n        if (':' in line):\n            if current_func:\n                functions.append((current_func, rest))\n            r = line.split(':', 1)\n            current_func = r[0].strip()\n            r[1] = r[1].strip()\n            if r[1]:\n                rest = [r[1]]\n            else:\n                rest = []\n        elif (not line.startswith(' ')):\n            if current_func:\n                functions.append((current_func, rest))\n                current_func = None\n                rest = []\n            if (',' in line):\n                for func in line.split(','):\n                    func = func.strip()\n                    if func:\n                        functions.append((func, []))\n            elif line.strip():\n                current_func = line.strip()\n        elif (current_func is not None):\n            rest.append(line.strip())\n    if current_func:\n        functions.append((current_func, rest))\n    return functions\n", "label": 1}
{"function": "\n\ndef _subx(self, template, string, count=0, subn=False):\n    filter = template\n    if ((not callable(template)) and ('\\\\' in template)):\n        import re as sre\n        filter = sre._subx(self, template)\n    state = _State(string, 0, sys.maxsize, self.flags)\n    sublist = []\n    n = last_pos = 0\n    while ((not count) or (n < count)):\n        state.reset()\n        state.string_position = state.start\n        if (not state.search(self._code)):\n            break\n        if (last_pos < state.start):\n            sublist.append(string[last_pos:state.start])\n        if (not ((last_pos == state.start) and (last_pos == state.string_position) and (n > 0))):\n            if callable(filter):\n                sublist.append(filter(SRE_Match(self, state)))\n            else:\n                sublist.append(filter)\n            last_pos = state.string_position\n            n += 1\n        if (state.string_position == state.start):\n            state.start += 1\n        else:\n            state.start = state.string_position\n    if (last_pos < state.end):\n        sublist.append(string[last_pos:state.end])\n    item = ''.join(sublist)\n    if subn:\n        return (item, n)\n    else:\n        return item\n", "label": 1}
{"function": "\n\ndef normpath(path):\n    'Normalize path, eliminating double slashes, etc.'\n    (slash, dot) = (('/', '.') if isinstance(path, unicode) else ('/', '.'))\n    if (path == ''):\n        return dot\n    initial_slashes = path.startswith('/')\n    if (initial_slashes and path.startswith('//') and (not path.startswith('///'))):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if (comp in ('', '.')):\n            continue\n        if ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(- 1)] == '..'))):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = ((slash * initial_slashes) + path)\n    return (path or dot)\n", "label": 1}
{"function": "\n\ndef _validate_policies(self, policies):\n    '\\n        :param policies: list of policies\\n        '\n    for policy in policies:\n        if (int(policy) in self.by_index):\n            raise PolicyError(('Duplicate index %s conflicts with %s' % (policy, self.get_by_index(int(policy)))))\n        for name in policy.alias_list:\n            if (name.upper() in self.by_name):\n                raise PolicyError(('Duplicate name %s conflicts with %s' % (policy, self.get_by_name(name))))\n        if policy.is_default:\n            if (not self.default):\n                self.default = policy\n            else:\n                raise PolicyError(('Duplicate default %s conflicts with %s' % (policy, self.default)))\n        self._add_policy(policy)\n    if (0 not in self.by_index):\n        if (len(self) != 0):\n            raise PolicyError('You must specify a storage policy section for policy index 0 in order to define multiple policies')\n        self._add_policy(StoragePolicy(0, name=LEGACY_POLICY_NAME))\n    enabled_policies = [p for p in self if (not p.is_deprecated)]\n    if (not enabled_policies):\n        raise PolicyError(\"Unable to find policy that's not deprecated!\")\n    if (not self.default):\n        if (len(self) > 1):\n            raise PolicyError('Unable to find default policy')\n        self.default = self[0]\n        self.default.is_default = True\n", "label": 1}
{"function": "\n\ndef _prepare_imports(self, dicts):\n    ' an override for prepare imports that sorts the imports by parent_id dependencies '\n    pseudo_ids = set()\n    pseudo_matches = {\n        \n    }\n    prepared = dict(super(OrganizationImporter, self)._prepare_imports(dicts))\n    for (_, data) in prepared.items():\n        parent_id = (data.get('parent_id', None) or '')\n        if parent_id.startswith('~'):\n            pseudo_ids.add(parent_id)\n    pseudo_ids = [(ppid, get_pseudo_id(ppid)) for ppid in pseudo_ids]\n    for (json_id, data) in prepared.items():\n        for (ppid, spec) in pseudo_ids:\n            match = True\n            for (k, v) in spec.items():\n                if (data[k] != v):\n                    match = False\n                    break\n            if match:\n                if (ppid in pseudo_matches):\n                    raise UnresolvedIdError(('multiple matches for pseudo id: ' + ppid))\n                pseudo_matches[ppid] = json_id\n    network = Network()\n    in_network = set()\n    import_order = []\n    for (json_id, data) in prepared.items():\n        parent_id = data.get('parent_id', None)\n        if (parent_id in pseudo_matches):\n            parent_id = pseudo_matches[parent_id]\n        network.add_node(json_id)\n        if parent_id:\n            network.add_edge(parent_id, json_id)\n    for jid in network.sort():\n        import_order.append((jid, prepared[jid]))\n        in_network.add(jid)\n    if (in_network != set(prepared.keys())):\n        raise PupaInternalError('import is missing nodes in network set')\n    return import_order\n", "label": 1}
{"function": "\n\ndef traverse(obj, *path, **kwargs):\n    '\\n    Traverse the object we receive with the given path. Path\\n    items can be either strings or lists of strings (or any\\n    nested combination thereof). Behavior in given cases is\\n    laid out line by line below.\\n    '\n    if path:\n        if (isinstance(obj, list) or isinstance(obj, tuple)):\n            return [traverse(x, *path) for x in obj]\n        elif isinstance(obj, dict):\n            if (isinstance(path[0], list) or isinstance(path[0], tuple)):\n                for branch in path[0]:\n                    if (not isinstance(branch, basestring)):\n                        raise TraversalError(obj, path[0])\n                return {name: traverse(obj[name], *path[1:], split=True) for name in path[0]}\n            elif (not isinstance(path[0], basestring)):\n                raise TraversalError(obj, path[0])\n            elif (path[0] == '\\\\*'):\n                return {name: traverse(item, *path[1:], split=True) for (name, item) in obj.items()}\n            elif (path[0] in obj):\n                return traverse(obj[path[0]], *path[1:])\n            else:\n                raise TraversalError(obj, path[0])\n        elif kwargs.get('split', False):\n            return obj\n        else:\n            raise TraversalError(obj, path[0])\n    else:\n        return obj\n", "label": 1}
{"function": "\n\ndef _check_command_response(response, reset, msg=None, allowable_errors=None):\n    'Check the response to a command for errors.\\n    '\n    if ('ok' not in response):\n        raise OperationFailure(response.get('$err'), response.get('code'), response)\n    if response.get('wtimeout', False):\n        raise WTimeoutError(response.get('errmsg', response.get('err')), response.get('code'), response)\n    if (not response['ok']):\n        details = response\n        if ('raw' in response):\n            for shard in response['raw'].itervalues():\n                if (not shard.get('ok')):\n                    details = shard\n                    break\n        errmsg = details['errmsg']\n        if ((allowable_errors is None) or (errmsg not in allowable_errors)):\n            if (errmsg.startswith('not master') or errmsg.startswith('node is recovering')):\n                if (reset is not None):\n                    reset()\n                raise AutoReconnect(errmsg)\n            if (errmsg == 'db assertion failure'):\n                errmsg = (\"db assertion failure, assertion: '%s'\" % details.get('assertion', ''))\n                raise OperationFailure(errmsg, details.get('assertionCode'), response)\n            code = details.get('code')\n            if (code in (11000, 11001, 12582)):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif (code == 50):\n                raise ExecutionTimeout(errmsg, code, response)\n            msg = (msg or '%s')\n            raise OperationFailure((msg % errmsg), code, response)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.schema = hive_metastore.ttypes.Schema()\n                self.schema.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.table_dir = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.in_tablename = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.delim = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    self.use_greenlets = False\n    self.auto_start_request = False\n    kwargs.pop('auto_start_request', None)\n    if (('read_preference' not in kwargs) and (kwargs.get('tag_sets') not in (None, [], [{\n        \n    }]))):\n        raise ConfigurationError()\n    if (('read_preference' not in kwargs) and (('slaveok' in kwargs) or ('slave_okay' in kwargs))):\n        secondary = kwargs.pop('slave_okay', kwargs.pop('slaveok', False))\n        kwargs['read_preference'] = (ReadPreference.SECONDARY_PREFERRED if secondary else ReadPreference.PRIMARY)\n    gle_opts = dict([(k, v) for (k, v) in kwargs.items() if (k in SAFE_OPTIONS)])\n    if (gle_opts and ('w' not in gle_opts)):\n        kwargs['w'] = 1\n    if ('safe' in kwargs):\n        safe = kwargs.pop('safe')\n        if (not safe):\n            kwargs.setdefault('w', 0)\n    self.delegate = kwargs.pop('delegate', None)\n    if (not self.delegate):\n        self.delegate = self.__delegate_class__(*args, **kwargs)\n        if kwargs.get('_connect', True):\n            self.synchro_connect()\n", "label": 1}
{"function": "\n\ndef poll(self, timeout):\n    self._lock.acquire()\n    if (timeout == 0):\n        self.poll_timeout = 0\n    elif self._timeouts:\n        self.poll_timeout = (self._timeouts[0][0] - _time())\n        if (self.poll_timeout < 0.0001):\n            self.poll_timeout = 0\n        elif (timeout is not None):\n            self.poll_timeout = min(timeout, self.poll_timeout)\n    elif (timeout is None):\n        self.poll_timeout = _AsyncNotifier._Block\n    else:\n        self.poll_timeout = timeout\n    timeout = self.poll_timeout\n    self._lock.release()\n    if (timeout and (timeout != _AsyncNotifier._Block)):\n        timeout = int((timeout * 1000))\n    (err, n, key, overlap) = win32file.GetQueuedCompletionStatus(self.iocp, timeout)\n    while (err != winerror.WAIT_TIMEOUT):\n        if (overlap and overlap.object):\n            overlap.object(err, n)\n        else:\n            logger.warning('invalid overlap!')\n        (err, n, key, overlap) = win32file.GetQueuedCompletionStatus(self.iocp, 0)\n    self.poll_timeout = 0\n    if (timeout == 0):\n        now = _time()\n        self._lock.acquire()\n        while (self._timeouts and (self._timeouts[0][0] <= now)):\n            (fd_timeout, fd) = self._timeouts.pop(0)\n            if (fd._timeout_id == fd_timeout):\n                fd._timeout_id = None\n                fd._timed_out()\n        self._lock.release()\n", "label": 1}
{"function": "\n\ndef MoveWindow(windowID, xpos=None, ypos=None, width=None, height=None, center=None):\n    (left, top, right, bottom) = win32gui.GetWindowRect(windowID)\n    if ((xpos is None) and (ypos is None)):\n        xpos = left\n        ypos = top\n    if ((width is None) and (height is None)):\n        width = (right - left)\n        height = (bottom - top)\n    if ((xpos is None) and (ypos is not None)):\n        xpos = left\n    if ((ypos is None) and (xpos is not None)):\n        ypos = top\n    if (not width):\n        width = (right - left)\n    if (not height):\n        height = (bottom - top)\n    if center:\n        screenx = win32api.GetSystemMetrics(win32con.SM_CXSCREEN)\n        screeny = win32api.GetSystemMetrics(win32con.SM_CYSCREEN)\n        xpos = int(math.floor(((screenx - width) / 2)))\n        ypos = int(math.floor(((screeny - height) / 2)))\n        if (xpos < 0):\n            xpos = 0\n        if (ypos < 0):\n            ypos = 0\n    win32gui.MoveWindow(windowID, xpos, ypos, width, height, 1)\n", "label": 1}
{"function": "\n\ndef node_clique_number(G, nodes=None, cliques=None):\n    ' Returns the size of the largest maximal clique containing\\n    each given node.\\n\\n    Returns a single or list depending on input nodes.\\n    Optional list of cliques can be input if already computed.\\n    '\n    if (cliques is None):\n        if (nodes is not None):\n            if isinstance(nodes, list):\n                d = {\n                    \n                }\n                for n in nodes:\n                    H = networkx.ego_graph(G, n)\n                    d[n] = max((len(c) for c in find_cliques(H)))\n            else:\n                H = networkx.ego_graph(G, nodes)\n                d = max((len(c) for c in find_cliques(H)))\n            return d\n        cliques = list(find_cliques(G))\n    if (nodes is None):\n        nodes = list(G.nodes())\n    if (not isinstance(nodes, list)):\n        v = nodes\n        d = max([len(c) for c in cliques if (v in c)])\n    else:\n        d = {\n            \n        }\n        for v in nodes:\n            d[v] = max([len(c) for c in cliques if (v in c)])\n    return d\n", "label": 1}
{"function": "\n\ndef __gt__(self, other):\n    if (self.date == 'infinity'):\n        if isinstance(other, Date):\n            return (other.date != 'infinity')\n        else:\n            from .Range import Range\n            if isinstance(other, Range):\n                return (other.end != 'infinity')\n            return (other != 'infinity')\n    elif isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return False\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) > other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date > other.date.replace(tzinfo=self.tz))\n        return (self.date > other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            if (other.end.date == 'infinity'):\n                return False\n            if (other.end.tz and (self.tz is None)):\n                return (self.date.replace(tzinfo=other.end.tz) > other.end.date)\n            elif (self.tz and (other.end.tz is None)):\n                return (self.date > other.end.date.replace(tzinfo=self.tz))\n            return (self.date > other.end.date)\n        else:\n            return self.__gt__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args):\n    ' Construct a Trace object.\\n\\n        Parameters\\n        ==========\\n        args = sympy expression\\n        indices = tuple/list if indices, optional\\n\\n        '\n    if (len(args) == 2):\n        if (not isinstance(args[1], (list, Tuple, tuple))):\n            indices = Tuple(args[1])\n        else:\n            indices = Tuple(*args[1])\n        expr = args[0]\n    elif (len(args) == 1):\n        indices = Tuple()\n        expr = args[0]\n    else:\n        raise ValueError('Arguments to Tr should be of form (expr[, [indices]])')\n    if isinstance(expr, Matrix):\n        return expr.trace()\n    elif (hasattr(expr, 'trace') and callable(expr.trace)):\n        return expr.trace()\n    elif isinstance(expr, Add):\n        return Add(*[Tr(arg, indices) for arg in expr.args])\n    elif isinstance(expr, Mul):\n        (c_part, nc_part) = expr.args_cnc()\n        if (len(nc_part) == 0):\n            return Mul(*c_part)\n        else:\n            obj = Expr.__new__(cls, Mul(*nc_part), indices)\n            return ((Mul(*c_part) * obj) if (len(c_part) > 0) else obj)\n    elif isinstance(expr, Pow):\n        if (_is_scalar(expr.args[0]) and _is_scalar(expr.args[1])):\n            return expr\n        else:\n            return Expr.__new__(cls, expr, indices)\n    else:\n        if _is_scalar(expr):\n            return expr\n        return Expr.__new__(cls, expr, indices)\n", "label": 1}
{"function": "\n\ndef process_pdu(self, pdu):\n    ' Process a PDU by sending a copy to each node as dictated by the\\n            addressing and if a node is promiscuous.\\n        '\n    if _debug:\n        Network._debug('process_pdu %r', pdu)\n    if (self.dropPercent != 0.0):\n        if ((random.random() * 100.0) < self.dropPercent):\n            if _debug:\n                Network._debug('    - packet dropped')\n            return\n    if ((not pdu.pduDestination) or (not isinstance(pdu.pduDestination, Address))):\n        raise RuntimeError('invalid destination address')\n    elif (pdu.pduDestination.addrType == Address.localBroadcastAddr):\n        for n in self.nodes:\n            if (pdu.pduSource != n.address):\n                n.response(deepcopy(pdu))\n    elif (pdu.pduDestination.addrType == Address.localStationAddr):\n        for n in self.nodes:\n            if (n.promiscuous or (pdu.pduDestination == n.address)):\n                n.response(deepcopy(pdu))\n    else:\n        raise RuntimeError('invalid destination address type')\n", "label": 1}
{"function": "\n\ndef all_media(self, from_apps=None):\n    from corehq.apps.hqmedia.models import CommCareMultimedia\n    dom_with_media = (self if (not self.is_snapshot) else self.copied_from)\n    if self.is_snapshot:\n        app_ids = [app.copied_from.get_id for app in self.full_applications()]\n        if from_apps:\n            from_apps = set([a_id for a_id in app_ids if (a_id in from_apps)])\n        else:\n            from_apps = app_ids\n    if from_apps:\n        media = []\n        media_ids = set()\n        apps = [app for app in dom_with_media.full_applications() if (app.get_id in from_apps)]\n        for app in apps:\n            if (app.doc_type != 'Application'):\n                continue\n            for (_, m) in app.get_media_objects():\n                if (m.get_id not in media_ids):\n                    media.append(m)\n                    media_ids.add(m.get_id)\n        return media\n    return CommCareMultimedia.view('hqmedia/by_domain', key=dom_with_media.name, include_docs=True).all()\n", "label": 1}
{"function": "\n\ndef get_linked_doctypes(columns, data):\n    linked_doctypes = {\n        \n    }\n    columns_dict = get_columns_dict(columns)\n    for (idx, col) in enumerate(columns):\n        df = columns_dict[idx]\n        if (df.get('fieldtype') == 'Link'):\n            if isinstance(col, basestring):\n                linked_doctypes[df['options']] = idx\n            else:\n                linked_doctypes[df['options']] = df['fieldname']\n    columns_with_value = []\n    for row in data:\n        if row:\n            if (len(row) != len(columns_with_value)):\n                if isinstance(row, (list, tuple)):\n                    row = enumerate(row)\n                elif isinstance(row, dict):\n                    row = row.items()\n                for (col, val) in row:\n                    if (val and (col not in columns_with_value)):\n                        columns_with_value.append(col)\n    for (doctype, key) in linked_doctypes.items():\n        if (key not in columns_with_value):\n            del linked_doctypes[doctype]\n    return linked_doctypes\n", "label": 1}
{"function": "\n\ndef doWaitForMultipleEvents(self, timeout):\n    log.msg(channel='system', event='iteration', reactor=self)\n    if (timeout is None):\n        timeout = 100\n    ranUserCode = False\n    for reader in self._closedAndReading.keys():\n        ranUserCode = True\n        self._runAction('doRead', reader)\n    for fd in self._writes.keys():\n        ranUserCode = True\n        log.callWithLogger(fd, self._runWrite, fd)\n    if ranUserCode:\n        timeout = 0\n    if (not (self._events or self._writes)):\n        time.sleep(timeout)\n        return\n    handles = (self._events.keys() or [self.dummyEvent])\n    timeout = int((timeout * 1000))\n    val = MsgWaitForMultipleObjects(handles, 0, timeout, QS_ALLINPUT)\n    if (val == WAIT_TIMEOUT):\n        return\n    elif (val == (WAIT_OBJECT_0 + len(handles))):\n        exit = win32gui.PumpWaitingMessages()\n        if exit:\n            self.callLater(0, self.stop)\n            return\n    elif ((val >= WAIT_OBJECT_0) and (val < (WAIT_OBJECT_0 + len(handles)))):\n        event = handles[(val - WAIT_OBJECT_0)]\n        (fd, action) = self._events[event]\n        if (fd in self._reads):\n            fileno = fd.fileno()\n            if (fileno == (- 1)):\n                self._disconnectSelectable(fd, posixbase._NO_FILEDESC, False)\n                return\n            events = WSAEnumNetworkEvents(fileno, event)\n            if (FD_CLOSE in events):\n                self._closedAndReading[fd] = True\n        log.callWithLogger(fd, self._runAction, action, fd)\n", "label": 1}
{"function": "\n\ndef mix_codes(self, agents, actors):\n    '\\n        Combine the actor codes and agent codes addressing duplicates\\n        and removing the general \"~PPL\" if there\\'s a better option.\\n        \\n        Parameters\\n        -----------\\n        agents, actors : Lists of their respective codes\\n        \\n        \\n        Returns\\n        -------\\n        codes: list\\n               [Agent codes] x [Actor codes]\\n        \\n        '\n    codes = set()\n    mix = (lambda a, b: ((a + b) if (not (b in a)) else a))\n    actors = (actors if actors else ['~'])\n    for ag in agents:\n        if ((ag == '~PPL') and (len(agents) > 1)):\n            continue\n        actors = map((lambda a: mix(a, ag[1:])), actors)\n    return filter((lambda a: (a not in ['', '~', '~~', None])), actors)\n    codes = set()\n    print('WTF-1')\n    for act in (actors if actors else ['~']):\n        for ag in (agents if agents else ['~']):\n            if ((ag == '~PPL') and (len(agents) > 1)):\n                continue\n            code = act\n            if (not (ag[1:] in act)):\n                code += ag[1:]\n            if (not (code in ['~', '~~', ''])):\n                codes.add(code)\n    return list(codes)\n", "label": 1}
{"function": "\n\ndef update(self):\n    'Get the latest data from Transmission and updates the state.'\n    self.refresh_transmission_data()\n    if (self.type == 'current_status'):\n        if self.transmission_client.session:\n            upload = self.transmission_client.session.uploadSpeed\n            download = self.transmission_client.session.downloadSpeed\n            if ((upload > 0) and (download > 0)):\n                self._state = 'Up/Down'\n            elif ((upload > 0) and (download == 0)):\n                self._state = 'Seeding'\n            elif ((upload == 0) and (download > 0)):\n                self._state = 'Downloading'\n            else:\n                self._state = 'Idle'\n        else:\n            self._state = 'Unknown'\n    if self.transmission_client.session:\n        if (self.type == 'download_speed'):\n            mb_spd = float(self.transmission_client.session.downloadSpeed)\n            mb_spd = ((mb_spd / 1024) / 1024)\n            self._state = round(mb_spd, (2 if (mb_spd < 0.1) else 1))\n        elif (self.type == 'upload_speed'):\n            mb_spd = float(self.transmission_client.session.uploadSpeed)\n            mb_spd = ((mb_spd / 1024) / 1024)\n            self._state = round(mb_spd, (2 if (mb_spd < 0.1) else 1))\n", "label": 1}
{"function": "\n\ndef parseNode(self):\n    label = None\n    if (self.ttype == PERCENT):\n        self.ttype = self.tokenizer.nextToken()\n        if (self.ttype != ID):\n            return None\n        label = self.tokenizer.sval\n        self.ttype = self.tokenizer.nextToken()\n        if (self.ttype != COLON):\n            return None\n        self.ttype = self.tokenizer.nextToken()\n    if (self.ttype == DOT):\n        self.ttype = self.tokenizer.nextToken()\n        wildcardPayload = CommonToken(0, '.')\n        node = WildcardTreePattern(wildcardPayload)\n        if (label is not None):\n            node.label = label\n        return node\n    if (self.ttype != ID):\n        return None\n    tokenName = self.tokenizer.sval\n    self.ttype = self.tokenizer.nextToken()\n    if (tokenName == 'nil'):\n        return self.adaptor.nil()\n    text = tokenName\n    arg = None\n    if (self.ttype == ARG):\n        arg = self.tokenizer.sval\n        text = arg\n        self.ttype = self.tokenizer.nextToken()\n    treeNodeType = self.wizard.getTokenType(tokenName)\n    if (treeNodeType == INVALID_TOKEN_TYPE):\n        return None\n    node = self.adaptor.createFromType(treeNodeType, text)\n    if ((label is not None) and isinstance(node, TreePattern)):\n        node.label = label\n    if ((arg is not None) and isinstance(node, TreePattern)):\n        node.hasTextArg = True\n    return node\n", "label": 1}
{"function": "\n\ndef _get_veths(net_data):\n    '\\n    Parse the nic setup inside lxc conf tuples back to a dictionary indexed by\\n    network interface\\n    '\n    if isinstance(net_data, dict):\n        net_data = list(net_data.items())\n    nics = salt.utils.odict.OrderedDict()\n    current_nic = salt.utils.odict.OrderedDict()\n    no_names = True\n    for item in net_data:\n        if (item and isinstance(item, dict)):\n            item = list(item.items())[0]\n        elif isinstance(item, six.string_types):\n            sitem = item.strip()\n            if (sitem.startswith('#') or (not sitem)):\n                continue\n            elif ('=' in item):\n                item = tuple([a.strip() for a in item.split('=', 1)])\n        if (item[0] == 'lxc.network.type'):\n            current_nic = salt.utils.odict.OrderedDict()\n        if (item[0] == 'lxc.network.name'):\n            no_names = False\n            nics[item[1].strip()] = current_nic\n        current_nic[item[0].strip()] = item[1].strip()\n    if (no_names and current_nic):\n        nics[DEFAULT_NIC] = current_nic\n    return nics\n", "label": 1}
{"function": "\n\ndef __init__(self, gateway, attributes):\n    if ('next_bill_amount' in attributes):\n        self._next_bill_amount = Decimal(attributes['next_bill_amount'])\n        del attributes['next_bill_amount']\n    Resource.__init__(self, gateway, attributes)\n    if ('price' in attributes):\n        self.price = Decimal(self.price)\n    if ('balance' in attributes):\n        self.balance = Decimal(self.balance)\n    if ('next_billing_period_amount' in attributes):\n        self.next_billing_period_amount = Decimal(self.next_billing_period_amount)\n    if ('add_ons' in attributes):\n        self.add_ons = [AddOn(gateway, add_on) for add_on in self.add_ons]\n    if ('descriptor' in attributes):\n        self.descriptor = Descriptor(gateway, attributes.pop('descriptor'))\n    if ('discounts' in attributes):\n        self.discounts = [Discount(gateway, discount) for discount in self.discounts]\n    if ('status_history' in attributes):\n        self.status_history = [SubscriptionStatusEvent(gateway, status_event) for status_event in self.status_history]\n    if ('transactions' in attributes):\n        self.transactions = [Transaction(gateway, transaction) for transaction in self.transactions]\n", "label": 1}
{"function": "\n\ndef _wait_for_unit_to_become_active(self, unit_name):\n    for attempt in range(0, self._max_tries):\n        state = self._fleet.state(True, unit_name)\n        if self._unit_is_active(unit_name, state):\n            LOGGER.debug('All %s units active', unit_name)\n            return True\n        if (state and all([(s.state == 'failed') for s in state])):\n            LOGGER.warn('All %s units failed', unit_name)\n            LOGGER.debug('State: %r', state)\n            return False\n        for s in [s for s in state if (s.loaded and (s.state == 'activating'))]:\n            LOGGER.debug('Unit %s is activating on %s', unit_name, self._machine_label(s))\n        for s in [s for s in state if (s.loaded and (s.state == 'inactive'))]:\n            LOGGER.debug('Unit %s is inactive on %s', unit_name, self._machine_label(s))\n        LOGGER.debug('Sleeping %i seconds before checking again', self._delay)\n        time.sleep(self._delay)\n    LOGGER.warn('Failed to validate unit state after %i attempts', self._max_tries)\n    return False\n", "label": 1}
{"function": "\n\ndef _node_for(self, object):\n    ' Returns the TreeNode associated with a specified object.\\n        '\n    if ((type(object) is tuple) and (len(object) == 2) and isinstance(object[1], TreeNode)):\n        return object\n    factory = self.factory\n    nodes = [node for node in factory.nodes if node.is_node_for(object)]\n    if (len(nodes) == 1):\n        return (object, nodes[0])\n    if (len(nodes) == 0):\n        return (object, ITreeNodeAdapterBridge(adapter=object))\n    base = nodes[0].node_for\n    nodes = [node for node in nodes if (base == node.node_for)]\n    if (len(nodes) == 1):\n        return (object, nodes[0])\n    root_node = None\n    for (i, node) in enumerate(nodes):\n        if (node.children == ''):\n            root_node = node\n            del nodes[i]\n            break\n    else:\n        root_node = nodes[0]\n    key = ((root_node,) + tuple(nodes))\n    if (key in factory.multi_nodes):\n        return (object, factory.multi_nodes[key])\n    factory.multi_nodes[key] = multi_node = MultiTreeNode(root_node=root_node, nodes=nodes)\n    return (object, multi_node)\n", "label": 1}
{"function": "\n\ndef req_it(self, num=(- 1), ids=None, include_unmangled=False):\n    '\\n        A generator over all the requests in history when the function was called.\\n        Generates deferreds which resolve to requests.\\n        '\n    count = 0\n\n    @defer.inlineCallbacks\n    def def_wrapper(reqid, load=False, num=1):\n        if ((not self.check(reqid)) and load):\n            (yield self.load(reqid, num))\n        req = (yield self.get(reqid))\n        defer.returnValue(req)\n    over = list(self.ordered_ids)\n    for reqid in over:\n        if ((ids is not None) and (reqid not in ids)):\n            continue\n        if ((not include_unmangled) and (reqid in self.unmangled_ids)):\n            continue\n        do_load = True\n        if (reqid in self.all_ids):\n            if ((count % self._preload_limit) == 0):\n                do_load = True\n            if (do_load and (not self.check(reqid))):\n                do_load = False\n                if (((num - count) < self._preload_limit) and (num != (- 1))):\n                    loadnum = (num - count)\n                else:\n                    loadnum = self._preload_limit\n                (yield def_wrapper(reqid, load=True, num=loadnum))\n            else:\n                (yield def_wrapper(reqid))\n            count += 1\n            if ((count >= num) and (num != (- 1))):\n                break\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.ip = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.start_time = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.file = PacketCaptureFile()\n                self.file.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.pid = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef on_modified_async(self, view):\n    if (not is_php_file(view)):\n        return\n    cursor = view.sel()[0].b\n    if (cursor < 1):\n        return\n    while (cursor > 0):\n        curChar = view.substr(sublime.Region((cursor - 1), cursor))\n        if (curChar == '\\\\'):\n            return self.run_completion(view)\n        if (curChar == '$'):\n            return self.run_completion(view)\n        if (curChar == '('):\n            return self.run_completion(view)\n        if (cursor > 1):\n            curChar = view.substr(sublime.Region((cursor - 2), cursor))\n            if (curChar == '->'):\n                return self.run_completion(view)\n            if (curChar == '::'):\n                return self.run_completion(view)\n        if (cursor > 3):\n            curChar = view.substr(sublime.Region((cursor - 4), cursor))\n            if (curChar == 'use '):\n                return self.run_completion(view)\n            if (curChar == 'new '):\n                return self.run_completion(view)\n            if (cursor > 9):\n                curChar = view.substr(sublime.Region((cursor - 10), cursor))\n                if (curChar == 'namespace '):\n                    return self.run_completion(view)\n        cursor -= 1\n", "label": 1}
{"function": "\n\ndef pack_list(from_, pack_type):\n    ' Return the wire packed version of `from_`. `pack_type` should be some\\n    subclass of `xcffib.Struct`, or a string that can be passed to\\n    `struct.pack`. You must pass `size` if `pack_type` is a struct.pack string.\\n    '\n    if (len(from_) == 0):\n        return bytes()\n    if (pack_type == 'c'):\n        if isinstance(from_, bytes):\n            from_ = [six.int2byte(b) for b in six.iterbytes(from_)]\n        elif isinstance(from_, six.string_types):\n            from_ = [six.int2byte(b) for b in bytearray(from_, 'utf-8')]\n        elif isinstance(from_[0], six.integer_types):\n\n            def to_bytes(v):\n                for _ in range(4):\n                    (v, r) = divmod(v, 256)\n                    (yield r)\n            from_ = [six.int2byte(b) for i in from_ for b in to_bytes(i)]\n    if isinstance(pack_type, six.string_types):\n        return struct.pack(('=' + (pack_type * len(from_))), *from_)\n    else:\n        buf = six.BytesIO()\n        for item in from_:\n            if (isinstance(item, Protobj) and hasattr(item, 'pack')):\n                buf.write(item.pack())\n            else:\n                buf.write(item)\n        return buf.getvalue()\n", "label": 1}
{"function": "\n\ndef autodiscover_templates():\n    \"\\n    Autodiscovers cmsplugin_contact_plus templates the way\\n    'django.template.loaders.filesystem.Loader' and\\n    'django.template.loaders.app_directories.Loader' work.\\n    \"\n\n    def sorted_templates(templates):\n        '\\n        Sorts templates\\n        '\n        TEMPLATES = sorted(templates, key=(lambda template: template[1]))\n        return TEMPLATES\n    global TEMPLATES\n    if TEMPLATES:\n        return TEMPLATES\n    override_dir = getattr(settings, 'CMSPLUGIN_CONTACT_PLUS_TEMPLATES', None)\n    if override_dir:\n        return sorted_templates(override_dir)\n    templates = []\n    dirs_to_scan = []\n    if ('django.template.loaders.app_directories.Loader' in settings.TEMPLATE_LOADERS):\n        for app in settings.INSTALLED_APPS:\n            _ = __import__(app)\n            dir = os.path.dirname(_.__file__)\n            if (not (dir in dirs_to_scan)):\n                dirs_to_scan.append(os.path.join(dir, 'templates'))\n    if ('django.template.loaders.filesystem.Loader' in settings.TEMPLATE_LOADERS):\n        for dir in settings.TEMPLATE_DIRS:\n            if (not (dir in dirs_to_scan)):\n                dirs_to_scan.append(dir)\n    for dir in dirs_to_scan:\n        found = glob.glob(os.path.join(dir, 'cmsplugin_contact_plus/*.html'))\n        for file in found:\n            (dir, file) = os.path.split(file)\n            (key, value) = (os.path.join(dir.split('/')[(- 1)], file), file)\n            f = False\n            for (_, template) in templates:\n                if (template == file):\n                    f = True\n            if (not f):\n                templates.append((key, value))\n    return sorted_templates(templates)\n", "label": 1}
{"function": "\n\ndef is_acronym(token, exclude=None):\n    '\\n    Pass single token as a string, return True/False if is/is not valid acronym.\\n\\n    Args:\\n        token (str): single word to check for acronym-ness\\n        exclude (set[str]): if technically valid but not actually good acronyms\\n        are known in advance, pass them in as a set of strings; matching tokens\\n        will return False\\n\\n    Returns:\\n        bool\\n    '\n    if (exclude and (token in exclude)):\n        return False\n    if (not token):\n        return False\n    if (' ' in token):\n        return False\n    if ((len(token) == 2) and (not token.isupper())):\n        return False\n    if token.isdigit():\n        return False\n    if ((not any((char.isupper() for char in token))) and (not (token[0].isdigit() or token[(- 1)].isdigit()))):\n        return False\n    if (not (2 <= sum((1 for char in token if char.isalnum())) <= 10)):\n        return False\n    if (not ACRONYM_REGEX.match(token)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef _add_group_id_fields(self, out_collection, group_func_keys):\n    date_operators = ['dayOfYear', 'dayOfMonth', 'dayOfWeek', 'year', 'month', 'week', 'hour', 'minute', 'second', 'millisecond']\n    clear_group_func_keys = []\n    for key in group_func_keys:\n        out_field = key.split('__')[0]\n        for doc in out_collection:\n            if (key not in doc.keys()):\n                func_field = key.split('__')[1]\n                (func, in_field) = func_field.split('_')\n                out_value = doc.get(in_field)\n                if (func in date_operators):\n                    if (func == 'dayOfYear'):\n                        out_value = out_value.timetuple().tm_yday\n                    elif (func == 'dayOfMonth'):\n                        out_value = out_value.day\n                    elif (func == 'dayOfWeek'):\n                        out_value = out_value.isoweekday()\n                    elif (func == 'year'):\n                        out_value = out_value.year\n                    elif (func == 'month'):\n                        out_value = out_value.month\n                    elif (func == 'week'):\n                        out_value = out_value.isocalendar()[1]\n                    elif (func == 'hour'):\n                        out_value = out_value.hour\n                    elif (func == 'minute'):\n                        out_value = out_value.minute\n                    elif (func == 'second'):\n                        out_value = out_value.second\n                    elif (func == 'millisecond'):\n                        out_value = int((out_value.microsecond / 1000))\n                doc[out_field] = out_value\n        clear_group_func_keys.append(out_field)\n    return (out_collection, clear_group_func_keys)\n", "label": 1}
{"function": "\n\ndef _validate_option(self, option, val):\n    if (option in 'field_names'):\n        self._validate_field_names(val)\n    elif (option in ('start', 'end', 'max_width', 'min_width', 'min_table_width', 'max_table_width', 'padding_width', 'left_padding_width', 'right_padding_width', 'format')):\n        self._validate_nonnegative_int(option, val)\n    elif (option in 'sortby'):\n        self._validate_field_name(option, val)\n    elif (option in 'sort_key'):\n        self._validate_function(option, val)\n    elif (option in 'hrules'):\n        self._validate_hrules(option, val)\n    elif (option in 'vrules'):\n        self._validate_vrules(option, val)\n    elif (option in 'fields'):\n        self._validate_all_field_names(option, val)\n    elif (option in ('header', 'border', 'reversesort', 'xhtml', 'print_empty', 'oldsortslice')):\n        self._validate_true_or_false(option, val)\n    elif (option in 'header_style'):\n        self._validate_header_style(val)\n    elif (option in 'int_format'):\n        self._validate_int_format(option, val)\n    elif (option in 'float_format'):\n        self._validate_float_format(option, val)\n    elif (option in ('vertical_char', 'horizontal_char', 'junction_char')):\n        self._validate_single_char(option, val)\n    elif (option in 'attributes'):\n        self._validate_attributes(option, val)\n", "label": 1}
{"function": "\n\ndef handle_start_block(self, token_text):\n    self.set_mode(MODE.BlockStatement)\n    empty_braces = self.is_next('}')\n    empty_anonymous_function = (empty_braces and (self.flags.last_word == 'function') and (self.last_type == 'TK_END_EXPR'))\n    if (self.opts.brace_style == 'expand'):\n        if ((self.last_type != 'TK_OPERATOR') and (empty_anonymous_function or (self.last_type == 'TK_EQUALS') or (self.is_special_word(self.flags.last_text) and (self.flags.last_text != 'else')))):\n            self.output_space_before_token = True\n        else:\n            self.append_newline()\n    elif (self.last_type not in ['TK_OPERATOR', 'TK_START_EXPR']):\n        if (self.last_type == 'TK_START_BLOCK'):\n            self.append_newline()\n        else:\n            self.output_space_before_token = True\n    elif (self.is_array(self.previous_flags.mode) and (self.flags.last_text == ',')):\n        if (self.last_last_text == '}'):\n            self.output_space_before_token = True\n        else:\n            self.append_newline()\n    self.append_token(token_text)\n    self.indent()\n", "label": 1}
{"function": "\n\ndef _lookup(self, name=None, create=False):\n    'looks up symbol in search path\\n        returns symbol given symbol name,\\n        creating symbol if needed (and create=True)'\n    debug = False\n    if debug:\n        print('====\\nLOOKUP ', name)\n    searchGroups = self._fix_searchGroups()\n    self.__parents = []\n    if (self not in searchGroups):\n        searchGroups.append(self)\n\n    def public_attr(grp, name):\n        return (hasattr(grp, name) and (not ((grp is self) and (name in self._private))))\n    parts = name.split('.')\n    if (len(parts) == 1):\n        for grp in searchGroups:\n            if public_attr(grp, name):\n                self.__parents.append(grp)\n                return getattr(grp, name)\n    parts.reverse()\n    top = parts.pop()\n    out = self.__invalid_name\n    if (top == self.top_group):\n        out = self\n    else:\n        for grp in searchGroups:\n            if public_attr(grp, top):\n                self.__parents.append(grp)\n                out = getattr(grp, top)\n    if (out is self.__invalid_name):\n        raise NameError((\"'%s' is not defined\" % name))\n    if (len(parts) == 0):\n        return out\n    while parts:\n        prt = parts.pop()\n        if hasattr(out, prt):\n            out = getattr(out, prt)\n        elif create:\n            val = None\n            if (len(parts) > 0):\n                val = Group(name=prt)\n            setattr(out, prt, val)\n            out = getattr(out, prt)\n        else:\n            raise LookupError((\"cannot locate member '%s' of '%s'\" % (prt, out)))\n    return out\n", "label": 1}
{"function": "\n\ndef extended_blank_lines(logical_line, blank_lines, indent_level, previous_logical):\n    'Check for missing blank lines after class declaration.'\n    if previous_logical.startswith('class '):\n        if (logical_line.startswith(('def ', 'class ', '@')) or pep8.DOCSTRING_REGEX.match(logical_line)):\n            if (indent_level and (not blank_lines)):\n                (yield (0, 'E309 expected 1 blank line after class declaration'))\n    elif previous_logical.startswith('def '):\n        if (blank_lines and pep8.DOCSTRING_REGEX.match(logical_line)):\n            (yield (0, 'E303 too many blank lines ({0})'.format(blank_lines)))\n    elif pep8.DOCSTRING_REGEX.match(previous_logical):\n        if (indent_level and (not blank_lines) and logical_line.startswith('def ') and ('(self' in logical_line)):\n            (yield (0, 'E301 expected 1 blank line, found 0'))\n", "label": 1}
{"function": "\n\ndef xsString(xc, p, source):\n    if isinstance(source, bool):\n        return ('true' if source else 'false')\n    elif isinstance(source, float):\n        if isnan(source):\n            return 'NaN'\n        elif isinf(source):\n            return ('-INF' if (source < 0) else 'INF')\n        \"\\n        numMagnitude = fabs(source)\\n        if numMagnitude < 1000000 and numMagnitude > .000001:\\n            # don't want floating notation which python does for more than 4 decimal places\\n            s = \\n        \"\n        s = str(source)\n        if s.endswith('.0'):\n            s = s[:(- 2)]\n        return s\n    elif isinstance(source, Decimal):\n        if isnan(source):\n            return 'NaN'\n        elif isinf(source):\n            return ('-INF' if (source < 0) else 'INF')\n        return str(source)\n    elif isinstance(source, ModelValue.DateTime):\n        return ('{0:%Y-%m-%d}' if source.dateOnly else '{0:%Y-%m-%dT%H:%M:%S}').format(source)\n    return str(source)\n", "label": 1}
{"function": "\n\ndef parse_diff(self):\n    sections = []\n    state = None\n    prev_file = None\n    current_file = {\n        \n    }\n    current_hunks = []\n    prev_hunk = None\n    current_hunk = None\n    for line in self.view.lines(sublime.Region(0, self.view.size())):\n        linetext = self.view.substr(line)\n        if linetext.startswith('diff --git'):\n            state = 'header'\n            if (prev_file != line):\n                if (prev_file is not None):\n                    if current_hunk:\n                        current_hunks.append(current_hunk)\n                    sections.append((current_file, current_hunks))\n                prev_file = line\n                prev_hunk = None\n            current_file = line\n            current_hunks = []\n        elif ((state == 'header') and RE_DIFF_HEAD.match(linetext)):\n            current_file = current_file.cover(line)\n        elif linetext.startswith('@@'):\n            state = 'hunk'\n            if (prev_hunk != line):\n                if (prev_hunk is not None):\n                    current_hunks.append(current_hunk)\n                prev_hunk = line\n            current_hunk = line\n        elif ((state == 'hunk') and (linetext[0] in (' ', '-', '+'))):\n            current_hunk = current_hunk.cover(line)\n        elif (state == 'header'):\n            current_file = current_file.cover(line)\n    if (current_file and current_hunk):\n        current_hunks.append(current_hunk)\n        sections.append((current_file, current_hunks))\n    return sections\n", "label": 1}
{"function": "\n\ndef selector(self, output):\n    if isinstance(output, compute_base.Node):\n        return self.parse(output, FieldLists.NODE)\n    elif isinstance(output, compute_base.NodeSize):\n        return self.parse(output, FieldLists.NODE_SIZE)\n    elif isinstance(output, compute_base.NodeImage):\n        return self.parse(output, FieldLists.NODE_IMAGE)\n    elif isinstance(output, compute_base.NodeLocation):\n        return self.parse(output, FieldLists.LOCATION)\n    elif isinstance(output, compute_base.NodeAuthSSHKey):\n        return self.parse(output, FieldLists.NODE_KEY)\n    elif isinstance(output, compute_base.NodeAuthPassword):\n        return self.parse(output, FieldLists.NODE_PASSWORD)\n    elif isinstance(output, compute_base.StorageVolume):\n        return self.parse(output, FieldLists.STORAGE_VOLUME)\n    elif isinstance(output, compute_base.VolumeSnapshot):\n        return self.parse(output, FieldLists.VOLUME_SNAPSHOT)\n    elif isinstance(output, dns_base.Zone):\n        return self.parse(output, FieldLists.ZONE)\n    elif isinstance(output, dns_base.Record):\n        return self.parse(output, FieldLists.RECORD)\n    elif isinstance(output, lb_base.Member):\n        return self.parse(output, FieldLists.MEMBER)\n    elif isinstance(output, lb_base.LoadBalancer):\n        return self.parse(output, FieldLists.BALANCER)\n    elif isinstance(output, container_base.Container):\n        return self.parse(output, FieldLists.CONTAINER)\n    elif isinstance(output, container_base.ContainerImage):\n        return self.parse(output, FieldLists.CONTAINER_IMAGE)\n    elif isinstance(output, container_base.ContainerCluster):\n        return self.parse(output, FieldLists.CONTAINER_CLUSTER)\n    else:\n        return output\n", "label": 1}
{"function": "\n\ndef line_ends_with_close_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)\\\\*/(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(/\\\\*)(.*)\\\\*/(\\\\s*)$', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\"\"\")(.*)\"\"\"(\\\\s*)$', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\->(\\\\s*)$)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)(\\\\s*)$', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=end)(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)(.*)(\\\\=end)(\\\\s*)$', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\})(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)(.*)(%\\\\})(\\\\s*)$', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef parse_challenge(stuff):\n    '\\n    '\n    ret = {\n        \n    }\n    var = b''\n    val = b''\n    in_var = True\n    in_quotes = False\n    new = False\n    escaped = False\n    for c in stuff:\n        if (sys.version_info >= (3, 0)):\n            c = bytes([c])\n        if in_var:\n            if c.isspace():\n                continue\n            if (c == b'='):\n                in_var = False\n                new = True\n            else:\n                var += c\n        elif new:\n            if (c == b'\"'):\n                in_quotes = True\n            else:\n                val += c\n            new = False\n        elif in_quotes:\n            if escaped:\n                escaped = False\n                val += c\n            elif (c == b'\\\\'):\n                escaped = True\n            elif (c == b'\"'):\n                in_quotes = False\n            else:\n                val += c\n        elif (c == b','):\n            if var:\n                ret[var] = val\n            var = b''\n            val = b''\n            in_var = True\n        else:\n            val += c\n    if var:\n        ret[var] = val\n    return ret\n", "label": 1}
{"function": "\n\ndef localStr(self, local_addr=None, local_port=None):\n    l = []\n    w = l.append\n    w('sip:')\n    if (self.username != None):\n        w(self.username)\n        for v in self.userparams:\n            w((';%s' % v))\n        if (self.password != None):\n            w((':%s' % self.password))\n        w('@')\n    if ((local_addr != None) and ('my' in dir(self.host))):\n        w(local_addr)\n    else:\n        w(str(self.host))\n    if (self.port != None):\n        if ((local_port != None) and ('my' in dir(self.port))):\n            w((':%d' % local_port))\n        else:\n            w((':%d' % self.port))\n    if (self.usertype != None):\n        w((';user=%s' % self.usertype))\n    for n in ('transport', 'ttl', 'maddr', 'method', 'tag'):\n        v = getattr(self, n)\n        if (v != None):\n            w((';%s=%s' % (n, v)))\n    if self.lr:\n        w(';lr')\n    for v in self.other:\n        w((';%s' % v))\n    if self.headers:\n        w('?')\n        w('&'.join([('%s=%s' % (h.capitalize(), quote(v))) for (h, v) in self.headers.items()]))\n    return ''.join(l)\n", "label": 1}
{"function": "\n\ndef execute(self, write_concern=None):\n    if (not self.executors):\n        raise InvalidOperation('Bulk operation empty!')\n    if self.done:\n        raise InvalidOperation('Bulk operation already executed!')\n    self.done = True\n    result = {\n        'nModified': 0,\n        'nUpserted': 0,\n        'nMatched': 0,\n        'writeErrors': [],\n        'upserted': [],\n        'writeConcernErrors': [],\n        'nRemoved': 0,\n        'nInserted': 0,\n    }\n    has_update = False\n    has_insert = False\n    broken_nModified_info = False\n    for execute_func in self.executors:\n        exec_name = execute_func.__name__\n        op_result = execute_func()\n        for (key, value) in op_result.items():\n            self.__aggregate_operation_result(result, key, value)\n        if (exec_name == 'exec_update'):\n            has_update = True\n            if ('nModified' not in op_result):\n                broken_nModified_info = True\n        has_insert |= (exec_name == 'exec_insert')\n    if broken_nModified_info:\n        result.pop('nModified')\n    elif (has_insert and self._insert_returns_nModified):\n        pass\n    elif (has_update and self._update_returns_nModified):\n        pass\n    elif (self._update_returns_nModified and self._insert_returns_nModified):\n        pass\n    else:\n        result.pop('nModified')\n    return result\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.caseless:\n        if ((instring[loc:(loc + self.matchLen)].upper() == self.caselessmatch) and ((loc >= (len(instring) - self.matchLen)) or (instring[(loc + self.matchLen)].upper() not in self.identChars)) and ((loc == 0) or (instring[(loc - 1)].upper() not in self.identChars))):\n            return ((loc + self.matchLen), self.match)\n    elif ((instring[loc] == self.firstMatchChar) and ((self.matchLen == 1) or instring.startswith(self.match, loc)) and ((loc >= (len(instring) - self.matchLen)) or (instring[(loc + self.matchLen)] not in self.identChars)) and ((loc == 0) or (instring[(loc - 1)] not in self.identChars))):\n        return ((loc + self.matchLen), self.match)\n    raise ParseException(instring, loc, self.errmsg, self)\n", "label": 1}
{"function": "\n\ndef determine_result(self, returncode, returnsignal, output, isTimeout):\n    if ((returnsignal == 0) and (returncode == 0)):\n        status = None\n        for line in output:\n            line = line.strip()\n            if (line == 'unsat'):\n                status = result.RESULT_UNSAT\n            elif (line == 'sat'):\n                status = result.RESULT_SAT\n            elif ((not status) and line.startswith('(error ')):\n                status = 'ERROR'\n        if (not status):\n            status = result.RESULT_UNKNOWN\n    elif (((returnsignal == 9) or (returnsignal == 15)) and isTimeout):\n        status = 'TIMEOUT'\n    elif (returnsignal == 9):\n        status = 'KILLED BY SIGNAL 9'\n    elif (returnsignal == 6):\n        status = 'ABORTED'\n    elif (returnsignal == 15):\n        status = 'KILLED'\n    else:\n        status = 'ERROR ({0})'.format(returncode)\n    return status\n", "label": 1}
{"function": "\n\ndef getNewHeader(self, requestInfo, token, isCookie):\n    headers = requestInfo.getHeaders()\n    if isCookie:\n        cookieHeader = 'Cookie:'\n        newheader = cookieHeader\n        previousCookies = []\n        for header in requestInfo.getHeaders():\n            if str(header).startswith(cookieHeader):\n                previousCookies = str(header)[len(cookieHeader):].replace(' ', '').split(';')\n                headers.remove(header)\n        newCookies = token.replace(' ', '').split(';')\n        newCookieVariableNames = []\n        for newCookie in newCookies:\n            equalsToken = newCookie.find('=')\n            if (equalsToken >= 0):\n                newCookieVariableNames.append(newCookie[0:(equalsToken + 1)])\n        for previousCookie in previousCookies:\n            equalsToken = previousCookie.find('=')\n            if (equalsToken >= 0):\n                if (previousCookie[0:(equalsToken + 1)] not in newCookieVariableNames):\n                    newCookies.append(previousCookie)\n        newCookies = [x for x in newCookies if x]\n        newheader = ((cookieHeader + ' ') + ';'.join(newCookies))\n    else:\n        newheader = token\n        colon = newheader.find(':')\n        if (colon >= 0):\n            for header in requestInfo.getHeaders():\n                if str(header).startswith(newheader[0:(colon + 1)]):\n                    headers.remove(header)\n    headers.add(newheader)\n    return headers\n", "label": 1}
{"function": "\n\ndef transform_source_batch(self, source, source_name):\n    self.verify_axis_labels(('batch', 'channel', 'height', 'width'), self.data_stream.axis_labels[source_name], source_name)\n    rotation_angles = self.rng.uniform((- self.maximum_rotation), self.maximum_rotation, len(source))\n    if (isinstance(source, list) and all(((isinstance(b, numpy.ndarray) and (b.ndim == 3)) for b in source))):\n        return [self._example_transform(im, angle) for (im, angle) in zip(source, rotation_angles)]\n    elif (isinstance(source, numpy.ndarray) and (source.dtype == object) and all(((isinstance(b, numpy.ndarray) and (b.ndim == 3)) for b in source))):\n        out = numpy.empty(len(source), dtype=object)\n        for (im_idx, (im, angle)) in enumerate(zip(source, rotation_angles)):\n            out[im_idx] = self._example_transform(im, angle)\n        return out\n    elif (isinstance(source, numpy.ndarray) and (source.ndim == 4)):\n        return numpy.array([self._example_transform(im, angle) for (im, angle) in zip(source, rotation_angles)], dtype=source.dtype)\n    else:\n        raise ValueError('uninterpretable batch format; expected a list of arrays with ndim = 3, or an array with ndim = 4')\n", "label": 1}
{"function": "\n\ndef get_features2(self):\n    '\\n        Return all features with its names.\\n\\n        Returns\\n        -------\\n        names : list\\n            Feature names.\\n        values : list\\n            Feature values\\n        '\n    feature_names = []\n    feature_values = []\n    all_vars = vars(self)\n    for name in all_vars.keys():\n        if (not ((name == 'date') or (name == 'mag') or (name == 'err') or (name == 'n_threads') or (name == 'min_period'))):\n            if (not ((name == 'f') or (name == 'f_phase') or (name == 'period_log10FAP') or (name == 'weight') or (name == 'weighted_sum') or (name == 'median') or (name == 'mean') or (name == 'std'))):\n                feature_names.append(name)\n    feature_names.sort()\n    for name in feature_names:\n        feature_values.append(all_vars[name])\n    return (feature_names, feature_values)\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    _get_chronos_service_config(args)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.chronos_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                stop_job(args, hosts[host_id].ip, job_name, instance_id)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.chronos_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                deploy_utils.wait_for_job_stopping('chronos', args.chronos_config.cluster.name, job_name, hosts[host_id].ip, instance_id)\n                start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)\n", "label": 1}
{"function": "\n\ndef _setup_document_fields(self):\n    for f in self.document._fields.values():\n        if (not hasattr(f, 'rel')):\n            if isinstance(f, ReferenceField):\n                f.rel = Relation(f.document_type)\n                f.is_relation = True\n            elif (isinstance(f, ListField) and isinstance(f.field, ReferenceField)):\n                f.field.rel = Relation(f.field.document_type)\n                f.field.is_relation = True\n            else:\n                f.many_to_many = None\n                f.many_to_one = None\n                f.one_to_many = None\n                f.one_to_one = None\n                f.related_model = None\n                f.rel = None\n                f.is_relation = False\n        if ((not hasattr(f, 'verbose_name')) or (f.verbose_name is None)):\n            f.verbose_name = capfirst(create_verbose_name(f.name))\n        if (not hasattr(f, 'flatchoices')):\n            flat = []\n            if (f.choices is not None):\n                for (choice, value) in f.choices:\n                    if isinstance(value, (list, tuple)):\n                        flat.extend(value)\n                    else:\n                        flat.append((choice, value))\n            f.flatchoices = flat\n        if (isinstance(f, ReferenceField) and (not isinstance(f.document_type._meta, (DocumentMetaWrapper, LazyDocumentMetaWrapper))) and (self.document != f.document_type)):\n            f.document_type._meta = LazyDocumentMetaWrapper(f.document_type)\n        if (not hasattr(f, 'auto_created')):\n            f.auto_created = False\n", "label": 1}
{"function": "\n\ndef _read_one_coil_point(fid):\n    'Read coil coordinate information from the hc file'\n    one = '#'\n    while ((len(one) > 0) and (one[0] == '#')):\n        one = fid.readline()\n    if (len(one) == 0):\n        return None\n    one = one.strip().decode('utf-8')\n    if ('Unable' in one):\n        raise RuntimeError('HPI information not available')\n    p = dict()\n    p['valid'] = ('measured' in one)\n    for (key, val) in _coord_dict.items():\n        if (key in one):\n            p['coord_frame'] = val\n            break\n    else:\n        p['coord_frame'] = (- 1)\n    for (key, val) in _kind_dict.items():\n        if (key in one):\n            p['kind'] = val\n            break\n    else:\n        p['kind'] = (- 1)\n    p['r'] = np.empty(3)\n    for (ii, coord) in enumerate('xyz'):\n        sp = fid.readline().decode('utf-8').strip()\n        if (len(sp) == 0):\n            continue\n        sp = sp.split(' ')\n        if ((len(sp) != 3) or (sp[0] != coord) or (sp[1] != '=')):\n            raise RuntimeError(('Bad line: %s' % one))\n        p['r'][ii] = (float(sp[2]) / 100.0)\n    return p\n", "label": 1}
{"function": "\n\ndef asXML(self, doctag=None, namedItemsOnly=False, indent='', formatted=True):\n    'Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.'\n    nl = '\\n'\n    out = []\n    namedItems = dict(((v[1], k) for (k, vlist) in self.__tokdict.items() for v in vlist))\n    nextLevelIndent = (indent + '  ')\n    if (not formatted):\n        indent = ''\n        nextLevelIndent = ''\n        nl = ''\n    selfTag = None\n    if (doctag is not None):\n        selfTag = doctag\n    elif self.__name:\n        selfTag = self.__name\n    if (not selfTag):\n        if namedItemsOnly:\n            return ''\n        else:\n            selfTag = 'ITEM'\n    out += [nl, indent, '<', selfTag, '>']\n    worklist = self.__toklist\n    for (i, res) in enumerate(worklist):\n        if isinstance(res, ParseResults):\n            if (i in namedItems):\n                out += [res.asXML(namedItems[i], (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n            else:\n                out += [res.asXML(None, (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n        else:\n            resTag = None\n            if (i in namedItems):\n                resTag = namedItems[i]\n            if (not resTag):\n                if namedItemsOnly:\n                    continue\n                else:\n                    resTag = 'ITEM'\n            xmlBodyText = _xml_escape(_ustr(res))\n            out += [nl, nextLevelIndent, '<', resTag, '>', xmlBodyText, '</', resTag, '>']\n    out += [nl, indent, '</', selfTag, '>']\n    return ''.join(out)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.validation_class = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.index_type = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.index_name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef on_text_command(self, view, cmd, args):\n    if (isearch_info_for(view) is not None):\n        if (cmd not in ('sbp_inc_search', 'sbp_inc_search_escape')):\n            return ('sbp_inc_search_escape', {\n                'next_cmd': cmd,\n                'next_args': args,\n            })\n        return\n    vs = ViewState.get(view)\n    self.on_anything(view)\n    if (args is None):\n        args = {\n            \n        }\n    if (not cmd.startswith('sbp_')):\n        vs.this_cmd = cmd\n    if (cmd == 'drag_select'):\n        info = isearch_info_for(view)\n        if info:\n            info.done()\n        vs.drag_count = (2 if ('by' in args) else 0)\n    if ((cmd in ('move', 'move_to')) and vs.active_mark and (not args.get('extend', False))):\n        args['extend'] = True\n        return (cmd, args)\n    if (not vs.argument_supplied):\n        return None\n    if (cmd in repeatable_cmds):\n        count = vs.get_count()\n        args.update({\n            'cmd': cmd,\n            '_times': abs(count),\n        })\n        if ((count < 0) and ('forward' in args)):\n            args['forward'] = (not args['forward'])\n        return ('sbp_do_times', args)\n    elif (cmd == 'scroll_lines'):\n        args['amount'] *= vs.get_count()\n        return (cmd, args)\n", "label": 1}
{"function": "\n\ndef _dynamic_init(self, only_fields, include_fields, exclude_fields):\n    \"\\n        Modifies `request_fields` via higher-level dynamic field interfaces.\\n\\n        Arguments:\\n            only_fields: List of field names to render.\\n                All other fields will be deferred (respects sideloads).\\n            include_fields: List of field names to include.\\n                Adds to default field set, (respects sideloads).\\n                `*` means include all fields.\\n            exclude_fields: List of field names to exclude.\\n                Removes from default field set. If set to '*', all fields are\\n                removed, except for ones that are explicitly included.\\n        \"\n    if (not self.dynamic):\n        return\n    if (isinstance(self.request_fields, dict) and (self.request_fields.pop('*', None) is False)):\n        exclude_fields = '*'\n    only_fields = set((only_fields or []))\n    include_fields = (include_fields or [])\n    exclude_fields = (exclude_fields or [])\n    all_fields = set(self.get_all_fields().keys())\n    if only_fields:\n        exclude_fields = '*'\n        include_fields = only_fields\n    if (exclude_fields == '*'):\n        include_fields = set((list(include_fields) + [field for (field, val) in six.iteritems(self.request_fields) if (val or (val == {\n            \n        }))]))\n        exclude_fields = (all_fields - include_fields)\n    elif (include_fields == '*'):\n        include_fields = all_fields\n    for name in exclude_fields:\n        self.request_fields[name] = False\n    for name in include_fields:\n        if (not isinstance(self.request_fields.get(name), dict)):\n            self.request_fields[name] = True\n", "label": 1}
{"function": "\n\ndef Validate(self):\n    'Check the source is well constructed.'\n    if (self.type == 'COMMAND'):\n        args = self.attributes.GetItem('args')\n        if (args and (len(args) == 1) and (' ' in args[0])):\n            raise ArtifactDefinitionError(('Cannot specify a single argument containing a space: %s.' % args))\n    if self.attributes.GetItem('paths'):\n        if (not isinstance(self.attributes.GetItem('paths'), list)):\n            raise ArtifactDefinitionError(\"Arg 'paths' that is not a list.\")\n    if self.attributes.GetItem('path'):\n        if (not isinstance(self.attributes.GetItem('path'), basestring)):\n            raise ArtifactDefinitionError(\"Arg 'path' is not a string.\")\n    if self.returned_types:\n        for rdf_type in self.returned_types:\n            if (rdf_type not in rdfvalue.RDFValue.classes):\n                raise ArtifactDefinitionError(('Invalid return type %s' % rdf_type))\n    src_type = self.TYPE_MAP.get(str(self.type))\n    if (src_type is None):\n        raise ArtifactDefinitionError(('Invalid type %s.' % self.type))\n    required_attributes = src_type.get('required_attributes', [])\n    missing_attributes = set(required_attributes).difference(self.attributes.keys())\n    if missing_attributes:\n        raise ArtifactDefinitionError(('Missing required attributes: %s.' % missing_attributes))\n", "label": 1}
{"function": "\n\ndef extract_chunk(self, nfaces=100, seed=None, auxpts=None):\n    'Extract a chunk of the surface using breadth first search, for testing purposes'\n    node = seed\n    if (seed is None):\n        node = np.random.randint(len(self.pts))\n    ptmap = dict()\n    queue = [node]\n    faces = set()\n    visited = set([node])\n    while ((len(faces) < nfaces) and (len(queue) > 0)):\n        node = queue.pop(0)\n        for face in self.connected[node].indices:\n            if (face not in faces):\n                faces.add(face)\n                for pt in self.polys[face]:\n                    if (pt not in visited):\n                        visited.add(pt)\n                        queue.append(pt)\n    (pts, aux, polys) = ([], [], [])\n    for face in faces:\n        for pt in self.polys[face]:\n            if (pt not in ptmap):\n                ptmap[pt] = len(pts)\n                pts.append(self.pts[pt])\n                if (auxpts is not None):\n                    aux.append(auxpts[pt])\n        polys.append([ptmap[p] for p in self.polys[face]])\n    if (auxpts is not None):\n        return (np.array(pts), np.array(aux), np.array(polys))\n    return (np.array(pts), np.array(polys))\n", "label": 1}
{"function": "\n\ndef gather_candidates(self, context):\n    p = self.__longest_path_that_exists(context['input'])\n    if ((p in (None, [])) or (p == '/') or re.search('//+$', p)):\n        return []\n    complete_str = self.__substitute_path((dirname(p) + '/'))\n    if (not os.path.isdir(complete_str)):\n        return []\n    hidden = (context['complete_str'].find('.') == 0)\n    dirs = [x for x in os.listdir(complete_str) if (os.path.isdir((complete_str + x)) and (hidden or (x[0] != '.')))]\n    files = [x for x in os.listdir(complete_str) if ((not os.path.isdir((complete_str + x))) and (hidden or (x[0] != '.')))]\n    return ([{\n        'word': x,\n        'abbr': (x + '/'),\n    } for x in sorted(dirs)] + [{\n        'word': x,\n    } for x in sorted(files)])\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.roleName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef line_contains_close_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)\\\\*/', line) and (not re.match('^(\\\\+|\\\\-)(.*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(.*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\->)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=end)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\})', line) and (not re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef construct(start_block, vmap, exceptions):\n    bfs_blocks = bfs(start_block)\n    graph = Graph()\n    gen_ret = GenInvokeRetName()\n    block_to_node = {\n        \n    }\n    exceptions_start_block = []\n    for exception in exceptions:\n        for (_, _, block) in exception.exceptions:\n            exceptions_start_block.append(block)\n    for block in bfs_blocks:\n        node = make_node(graph, block, block_to_node, vmap, gen_ret)\n        graph.add_node(node)\n    graph.entry = block_to_node[start_block]\n    del block_to_node, bfs_blocks\n    graph.compute_rpo()\n    graph.number_ins()\n    for node in graph.rpo:\n        preds = [pred for pred in graph.all_preds(node) if (pred.num < node.num)]\n        if (preds and all((pred.in_catch for pred in preds))):\n            node.in_catch = True\n    lexit_nodes = [node for node in graph if node.type.is_return]\n    if (len(lexit_nodes) > 1):\n        logger.error('Multiple exit nodes found !')\n        graph.exit = graph.rpo[(- 1)]\n    elif (len(lexit_nodes) < 1):\n        logger.debug('No exit node found !')\n    else:\n        graph.exit = lexit_nodes[0]\n    return graph\n", "label": 1}
{"function": "\n\ndef normpath(path):\n    'Normalize path, eliminating double slashes, etc.'\n    if (path == ''):\n        return '.'\n    initial_slashes = path.startswith('/')\n    if (initial_slashes and path.startswith('//') and (not path.startswith('///'))):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if (comp in ('', '.')):\n            continue\n        if ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(- 1)] == '..'))):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = '/'.join(comps)\n    if initial_slashes:\n        path = (('/' * initial_slashes) + path)\n    return (path or '.')\n", "label": 1}
{"function": "\n\ndef AAG_heater_algorithm(self, target, last_entry):\n    '\\n        Uses the algorithm described in RainSensorHeaterAlgorithm.pdf to\\n        determine PWM value.\\n\\n        Values are for the default read cycle of 10 seconds.\\n        '\n    deltaT = (last_entry['Rain Sensor Temp (C)'] - target)\n    scaling = 0.5\n    if (deltaT > 8.0):\n        deltaPWM = ((- 40) * scaling)\n    elif (deltaT > 4.0):\n        deltaPWM = ((- 20) * scaling)\n    elif (deltaT > 3.0):\n        deltaPWM = ((- 10) * scaling)\n    elif (deltaT > 2.0):\n        deltaPWM = ((- 6) * scaling)\n    elif (deltaT > 1.0):\n        deltaPWM = ((- 4) * scaling)\n    elif (deltaT > 0.5):\n        deltaPWM = ((- 2) * scaling)\n    elif (deltaT > 0.3):\n        deltaPWM = ((- 1) * scaling)\n    elif (deltaT < (- 0.3)):\n        deltaPWM = (1 * scaling)\n    elif (deltaT < (- 0.5)):\n        deltaPWM = (2 * scaling)\n    elif (deltaT < (- 1.0)):\n        deltaPWM = (4 * scaling)\n    elif (deltaT < (- 2.0)):\n        deltaPWM = (6 * scaling)\n    elif (deltaT < (- 3.0)):\n        deltaPWM = (10 * scaling)\n    elif (deltaT < (- 4.0)):\n        deltaPWM = (20 * scaling)\n    elif (deltaT < (- 8.0)):\n        deltaPWM = (40 * scaling)\n    return int(deltaPWM)\n", "label": 1}
{"function": "\n\ndef filter_results(source, results, aggressive):\n    'Filter out spurious reports from pep8.\\n\\n    If aggressive is True, we allow possibly unsafe fixes (E711, E712).\\n\\n    '\n    non_docstring_string_line_numbers = multiline_string_lines(source, include_docstrings=False)\n    all_string_line_numbers = multiline_string_lines(source, include_docstrings=True)\n    commented_out_code_line_numbers = commented_out_code_lines(source)\n    for r in results:\n        issue_id = r['id'].lower()\n        if (r['line'] in non_docstring_string_line_numbers):\n            if issue_id.startswith(('e1', 'e501', 'w191')):\n                continue\n        if (r['line'] in all_string_line_numbers):\n            if (issue_id in ['e501']):\n                continue\n        if ((not aggressive) and ((r['line'] + 1) in all_string_line_numbers)):\n            if issue_id.startswith(('w29', 'w39')):\n                continue\n        if (aggressive <= 0):\n            if issue_id.startswith(('e711', 'w6')):\n                continue\n        if (aggressive <= 1):\n            if issue_id.startswith(('e712',)):\n                continue\n        if (r['line'] in commented_out_code_line_numbers):\n            if issue_id.startswith(('e26', 'e501')):\n                continue\n        (yield r)\n", "label": 1}
{"function": "\n\ndef update(self):\n    si = self.current\n    if (si is None):\n        return\n    not_in_error = self.not_in_error()\n    flags = (sublime.DRAW_NO_FILL if _ST3 else sublime.DRAW_OUTLINED)\n    self.view.add_regions(REGION_FIND, si.regions, 'text', '', flags)\n    selected = (si.selected or (not_in_error.selected and [not_in_error.selected[(- 1)]]) or [])\n    self.view.add_regions(REGION_SELECTED, selected, 'string', '', sublime.DRAW_NO_OUTLINE)\n    if selected:\n        self.view.show(selected[(- 1)])\n    status = ''\n    if ((si != not_in_error) or si.try_wrapped):\n        status += 'Failing '\n    if self.current.wrapped:\n        status += 'Wrapped '\n    status += ('I-Search ' + ('Forward' if self.current.forward else 'Reverse'))\n    if (si != not_in_error):\n        if (len(self.current.regions) > 0):\n            status += (' %s %s' % (pluralize('match', len(self.current.regions), 'es'), ('above' if self.forward else 'below')))\n    else:\n        n_cursors = min(len(si.selected), len(si.regions))\n        status += (' %s, %s' % (pluralize('match', len(si.regions), 'es'), pluralize('cursor', n_cursors)))\n    self.util.set_status(status)\n", "label": 1}
{"function": "\n\ndef _gather_port_ids_and_networks(self, context, instance, networks=None, port_ids=None):\n    \"Return an instance's complete list of port_ids and networks.\"\n    if (((networks is None) and (port_ids is not None)) or ((port_ids is None) and (networks is not None))):\n        message = _('This method needs to be called with either networks=None and port_ids=None or port_ids and networks as not none.')\n        raise exception.NovaException(message=message)\n    ifaces = compute_utils.get_nw_info_for_instance(instance)\n    if (port_ids is None):\n        port_ids = [iface['id'] for iface in ifaces]\n        net_ids = [iface['network']['id'] for iface in ifaces]\n    if (networks is None):\n        networks = self._get_available_networks(context, instance.project_id, net_ids)\n    else:\n        networks_ids = [network['id'] for network in networks]\n        networks = (networks + [{\n            'id': iface['network']['id'],\n            'name': iface['network']['label'],\n            'tenant_id': iface['network']['meta']['tenant_id'],\n        } for iface in ifaces if _is_not_duplicate(iface['network']['id'], networks_ids, 'networks', instance)])\n        port_ids = ([iface['id'] for iface in ifaces if _is_not_duplicate(iface['id'], port_ids, 'port_ids', instance)] + port_ids)\n    return (networks, port_ids)\n", "label": 1}
{"function": "\n\ndef _quick_drag_menu(self, object):\n    ' Displays the quick drag menu for a specified drag object.\\n        '\n    feature_lists = []\n    if isinstance(object, IFeatureTool):\n        msg = 'Apply to'\n        for dc in self.dock_control.dock_controls:\n            if (dc.visible and (object.feature_can_drop_on(dc.object) or object.feature_can_drop_on_dock_control(dc))):\n                from feature_tool import FeatureTool\n                feature_lists.append([FeatureTool(dock_control=dc)])\n    else:\n        msg = 'Send to'\n        for dc in self.dock_control.dock_controls:\n            if dc.visible:\n                allowed = [f for f in dc.features if ((f.feature_name != '') and f.can_drop(object))]\n                if (len(allowed) > 0):\n                    feature_lists.append(allowed)\n    if (len(feature_lists) > 0):\n        features = []\n        actions = []\n        for list in feature_lists:\n            if (len(list) > 1):\n                sub_actions = []\n                for feature in list:\n                    sub_actions.append(Action(name=('%s Feature' % feature.feature_name), action=('self._drop_on(%d)' % len(features))))\n                    features.append(feature)\n                actions.append(Menu(*sub_actions, name=('%s the %s' % (msg, feature.dock_control.name))))\n            else:\n                actions.append(Action(name=('%s %s' % (msg, list[0].dock_control.name)), action=('self._drop_on(%d)' % len(features))))\n                features.append(list[0])\n        self._object = object\n        self._features = features\n        self.popup_menu(Menu(*actions, name='popup'))\n        self._object = self._features = None\n", "label": 1}
{"function": "\n\ndef merge(self, session, source_state, source_dict, dest_state, dest_dict, load, _recursive):\n    if load:\n        for r in self._reverse_property:\n            if ((source_state, r) in _recursive):\n                return\n    if ('merge' not in self._cascade):\n        return\n    if (self.key not in source_dict):\n        return\n    if self.uselist:\n        instances = source_state.get_impl(self.key).get(source_state, source_dict)\n        if hasattr(instances, '_sa_adapter'):\n            instances = instances._sa_adapter\n        if load:\n            dest_state.get_impl(self.key).get(dest_state, dest_dict)\n        dest_list = []\n        for current in instances:\n            current_state = attributes.instance_state(current)\n            current_dict = attributes.instance_dict(current)\n            _recursive[(current_state, self)] = True\n            obj = session._merge(current_state, current_dict, load=load, _recursive=_recursive)\n            if (obj is not None):\n                dest_list.append(obj)\n        if (not load):\n            coll = attributes.init_state_collection(dest_state, dest_dict, self.key)\n            for c in dest_list:\n                coll.append_without_event(c)\n        else:\n            dest_state.get_impl(self.key)._set_iterable(dest_state, dest_dict, dest_list)\n    else:\n        current = source_dict[self.key]\n        if (current is not None):\n            current_state = attributes.instance_state(current)\n            current_dict = attributes.instance_dict(current)\n            _recursive[(current_state, self)] = True\n            obj = session._merge(current_state, current_dict, load=load, _recursive=_recursive)\n        else:\n            obj = None\n        if (not load):\n            dest_dict[self.key] = obj\n        else:\n            dest_state.get_impl(self.key).set(dest_state, dest_dict, obj, None)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_service_call_name_ != x.has_service_call_name_):\n        return 0\n    if (self.has_service_call_name_ and (self.service_call_name_ != x.service_call_name_)):\n        return 0\n    if (self.has_total_amount_of_calls_ != x.has_total_amount_of_calls_):\n        return 0\n    if (self.has_total_amount_of_calls_ and (self.total_amount_of_calls_ != x.total_amount_of_calls_)):\n        return 0\n    if (self.has_total_cost_of_calls_microdollars_ != x.has_total_cost_of_calls_microdollars_):\n        return 0\n    if (self.has_total_cost_of_calls_microdollars_ and (self.total_cost_of_calls_microdollars_ != x.total_cost_of_calls_microdollars_)):\n        return 0\n    if (len(self.total_billed_ops_) != len(x.total_billed_ops_)):\n        return 0\n    for (e1, e2) in zip(self.total_billed_ops_, x.total_billed_ops_):\n        if (e1 != e2):\n            return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef _process_string(self, token, content):\n    if (self.getting_attrs and (self.current_attr is not None)):\n        if (content.endswith('\"') or content.endswith(\"'\")):\n            if (self.current_attr_value is not None):\n                self.current_attr_value += content\n                if ((self.current_tag == 'script') and (self.current_attr == 'src')):\n                    self.append(self.current_attr_value)\n                self.current_attr = None\n                self.current_attr_value = None\n            elif (len(content) == 1):\n                self.current_attr_value = content\n            else:\n                if ((self.current_tag == 'script') and (self.current_attr == 'src')):\n                    self.append(content)\n                self.current_attr = None\n                self.current_attr_value = None\n        elif (content.startswith('\"') or content.startswith(\"'\")):\n            if (self.current_attr_value is None):\n                self.current_attr_value = content\n            else:\n                self.current_attr_value += content\n", "label": 1}
{"function": "\n\ndef parse(self, chars):\n    token = ''\n    for char in chars:\n        if (char in '<*('):\n            if token:\n                (yield TextNode(token))\n            token = ''\n        if (char == '\\\\'):\n            token += next(chars)\n        elif (char == '<'):\n            tag_content = self.read_until(chars, '>')\n            name = ''\n            regex = None\n            macro = None\n            for char in tag_content:\n                if (char == '='):\n                    (name, regex) = tag_content.split('=', 1)\n                    break\n                if (char == ':'):\n                    (name, macro) = tag_content.split(':', 1)\n                    break\n            if regex:\n                (yield RegexTagNode(name, regex))\n            elif macro:\n                (yield MacroTagNode(name, macro))\n            else:\n                (yield TagNode(tag_content))\n        elif (char == '*'):\n            (yield WildcardNode())\n        elif (char == '('):\n            (yield OptionalNode(list(self.parse(chars))))\n        elif (char == ')'):\n            break\n        else:\n            token += char\n    if token:\n        (yield TextNode(token))\n", "label": 1}
{"function": "\n\ndef deep_deconstruct(self, obj):\n    '\\n        Recursive deconstruction for a field and its arguments.\\n        Used for full comparison for rename/alter; sometimes a single-level\\n        deconstruction will not compare correctly.\\n        '\n    if isinstance(obj, list):\n        return [self.deep_deconstruct(value) for value in obj]\n    elif isinstance(obj, tuple):\n        return tuple((self.deep_deconstruct(value) for value in obj))\n    elif isinstance(obj, dict):\n        return {key: self.deep_deconstruct(value) for (key, value) in obj.items()}\n    elif isinstance(obj, functools.partial):\n        return (obj.func, self.deep_deconstruct(obj.args), self.deep_deconstruct(obj.keywords))\n    elif isinstance(obj, COMPILED_REGEX_TYPE):\n        return RegexObject(obj)\n    elif isinstance(obj, type):\n        return obj\n    elif hasattr(obj, 'deconstruct'):\n        deconstructed = obj.deconstruct()\n        if isinstance(obj, models.Field):\n            deconstructed = deconstructed[1:]\n        (path, args, kwargs) = deconstructed\n        return (path, [self.deep_deconstruct(value) for value in args], {key: self.deep_deconstruct(value) for (key, value) in kwargs.items()})\n    else:\n        return obj\n", "label": 1}
{"function": "\n\ndef _find_and_modify(self, query, projection=None, update=None, upsert=False, sort=None, return_document=ReturnDocument.BEFORE, **kwargs):\n    remove = kwargs.get('remove', False)\n    if (kwargs.get('new', False) and remove):\n        raise OperationFailure(\"remove and returnNew can't co-exist\")\n    if (not (remove or update)):\n        raise ValueError('Must either update or remove')\n    if (remove and update):\n        raise ValueError(\"Can't do both update and remove\")\n    old = self.find_one(query, projection=projection, sort=sort)\n    if ((not old) and (not upsert)):\n        return\n    if (old and ('_id' in old)):\n        query = {\n            '_id': old['_id'],\n        }\n    if remove:\n        self.delete_one(query)\n    else:\n        self._update(query, update, upsert)\n    if ((return_document is ReturnDocument.AFTER) or kwargs.get('new')):\n        return self.find_one(query, projection)\n    return old\n", "label": 1}
{"function": "\n\n@classmethod\ndef _eval(self, n, k):\n    if k.is_Integer:\n        if (n.is_Integer and (n >= 0)):\n            (n, k) = (int(n), int(k))\n            if (k > n):\n                return S.Zero\n            elif (k > (n // 2)):\n                k = (n - k)\n            (M, result) = (int(_sqrt(n)), 1)\n            for prime in sieve.primerange(2, (n + 1)):\n                if (prime > (n - k)):\n                    result *= prime\n                elif (prime > (n // 2)):\n                    continue\n                elif (prime > M):\n                    if ((n % prime) < (k % prime)):\n                        result *= prime\n                else:\n                    (N, K) = (n, k)\n                    exp = a = 0\n                    while (N > 0):\n                        a = int(((N % prime) < ((K % prime) + a)))\n                        (N, K) = ((N // prime), (K // prime))\n                        exp = (a + exp)\n                    if (exp > 0):\n                        result *= (prime ** exp)\n            return Integer(result)\n        else:\n            d = result = ((n - k) + 1)\n            for i in range(2, (k + 1)):\n                d += 1\n                result *= d\n                result /= i\n            return result\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef run(report_name, filters=()):\n    report = get_report_doc(report_name)\n    if (filters and isinstance(filters, basestring)):\n        filters = json.loads(filters)\n    if (not frappe.has_permission(report.ref_doctype, 'report')):\n        frappe.msgprint(_('Must have report permission to access this report.'), raise_exception=True)\n    (columns, result, message) = ([], [], None)\n    if (report.report_type == 'Query Report'):\n        if (not report.query):\n            frappe.msgprint(_('Must specify a Query to run'), raise_exception=True)\n        if (not report.query.lower().startswith('select')):\n            frappe.msgprint(_('Query must be a SELECT'), raise_exception=True)\n        result = [list(t) for t in frappe.db.sql(report.query, filters)]\n        columns = [cstr(c[0]) for c in frappe.db.get_description()]\n    else:\n        module = (report.module or frappe.db.get_value('DocType', report.ref_doctype, 'module'))\n        if (report.is_standard == 'Yes'):\n            method_name = (get_report_module_dotted_path(module, report.name) + '.execute')\n            res = frappe.get_attr(method_name)(frappe._dict(filters))\n            (columns, result) = (res[0], res[1])\n            if (len(res) > 2):\n                message = res[2]\n    if (report.apply_user_permissions and result):\n        result = get_filtered_data(report.ref_doctype, columns, result)\n    if (cint(report.add_total_row) and result):\n        result = add_total_row(result, columns)\n    return {\n        'result': result,\n        'columns': columns,\n        'message': message,\n    }\n", "label": 1}
{"function": "\n\ndef serialize(self):\n    'Serialize this object into a 32-bit unsigned integer.\\n\\n        The returned integer can be passed to deserialize() to\\n        recreate a copy of this object.\\n\\n        Returns:\\n            A 32-bit unsigned integer that is a serialized form of\\n            this object.\\n        '\n    wildcard_bits = ((((((((((OFPFW_IN_PORT if self.in_port else 0) | (OFPFW_DL_VLAN if self.dl_vlan else 0)) | (OFPFW_DL_SRC if self.dl_src else 0)) | (OFPFW_DL_DST if self.dl_dst else 0)) | (OFPFW_DL_TYPE if self.dl_type else 0)) | (OFPFW_NW_PROTO if self.nw_proto else 0)) | (OFPFW_TP_SRC if self.tp_src else 0)) | (OFPFW_TP_DST if self.tp_dst else 0)) | (OFPFW_DL_VLAN_PCP if self.dl_vlan_pcp else 0)) | (OFPFW_NW_TOS if self.nw_tos else 0))\n    if ((self.nw_src < 0) or (self.nw_src > 32)):\n        raise ValueError('invalid nw_src', self.nw_src)\n    wildcard_bits |= (self.nw_src << OFPFW_NW_SRC_SHIFT)\n    if ((self.nw_dst < 0) or (self.nw_dst > 32)):\n        raise ValueError('invalid nw_dst', self.nw_dst)\n    wildcard_bits |= (self.nw_dst << OFPFW_NW_DST_SHIFT)\n    return wildcard_bits\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    for base in bases:\n        for base_name in dir(base):\n            base_value = getattr(base, base_name)\n            if ((not callable(base_value)) or (not base_name.startswith('assert'))):\n                continue\n            if (not (base_name in attrs)):\n                attrs.update({\n                    base_name: base_value,\n                })\n    for (attr_name, attr_value) in attrs.items():\n        if ((not attr_name.startswith('assert')) or (attr_name == 'assert_')):\n            continue\n        if (attr_name[6] == '_'):\n            new_name = underscore_to_camelcase(attr_name)\n        else:\n            new_name = camelcase_to_underscore(attr_name)\n        if (not (new_name in attrs)):\n            attrs[new_name] = attr_value\n    for attr in tools.__all__:\n        attrs.update({\n            attr: getattr(tools, attr),\n        })\n        attrs[attr] = staticmethod(attrs[attr])\n    if datadiff_assert_equal:\n        key = ((attrs.get('use_datadiff', False) and 'assert_equal') or 'datadiff_assert_equal')\n        attrs.update({\n            key: datadiff_assert_equal,\n        })\n        attrs[key] = staticmethod(attrs[key])\n    elif attrs.get('use_datadiff', False):\n        warnings.warn('You enabled ``datadiff.tools.assert_equal``, but looks like you have not ``datadiff`` library installed in your system.')\n    return type.__new__(cls, name, bases, attrs)\n", "label": 1}
{"function": "\n\ndef check(self):\n    if (not THROTTLING_ENABLED):\n        return\n    if (THROTTLING_IGNORE_ADMINS and self.request.user.is_superuser):\n        return\n    (cache_key, timeout) = self.get_cache_key()\n    if (not cache_key):\n        return\n    if (timeout is False):\n        return HttpResponseBadRequest()\n    if ((timeout is None) or (timeout is 0)):\n        return\n    if isinstance(timeout, basestring):\n        if timeout.startswith('/'):\n            return HttpResponseRedirect(timeout)\n        else:\n            callback = get_callable(timeout)\n            maintenance_bundle = {\n                'view_func': self.view_func,\n                'view_args': self.view_args,\n                'view_kwargs': self.view_kwargs,\n            }\n            return callback(self.request, maintenance_bundle)\n    now = int((time.time() * 1000))\n    last_access = cache.get(cache_key, 0)\n    delta = (now - last_access)\n    if (delta >= timeout):\n        cache.set(cache_key, now, THROTTLING_CACHE_EXPIRE)\n        return\n    congestion_view = (self.view_throttling.get('congestion') or THROTTLING.get('congestion'))\n    if congestion_view:\n        if congestion_view.startswith('/'):\n            return HttpResponseRedirect(congestion_view)\n        else:\n            callback = get_callable(congestion_view)\n            congestion_bundle = {\n                'view_func': self.view_func,\n                'view_args': self.view_args,\n                'view_kwargs': self.view_kwargs,\n                'timeout': timeout,\n                'delta': delta,\n            }\n            return callback(self.request, congestion_bundle)\n    return HttpResponseBadRequest()\n", "label": 1}
{"function": "\n\ndef ByteSizePartial(self):\n    n = 0\n    if self.has_service_call_name_:\n        n += 1\n        n += self.lengthString(len(self.service_call_name_))\n    if self.has_request_data_summary_:\n        n += (1 + self.lengthString(len(self.request_data_summary_)))\n    if self.has_response_data_summary_:\n        n += (1 + self.lengthString(len(self.response_data_summary_)))\n    if self.has_api_mcycles_:\n        n += (1 + self.lengthVarInt64(self.api_mcycles_))\n    if self.has_api_milliseconds_:\n        n += (1 + self.lengthVarInt64(self.api_milliseconds_))\n    if self.has_start_offset_milliseconds_:\n        n += 1\n        n += self.lengthVarInt64(self.start_offset_milliseconds_)\n    if self.has_duration_milliseconds_:\n        n += (1 + self.lengthVarInt64(self.duration_milliseconds_))\n    if self.has_namespace_:\n        n += (1 + self.lengthString(len(self.namespace_)))\n    if self.has_was_successful_:\n        n += 2\n    n += (1 * len(self.call_stack_))\n    for i in xrange(len(self.call_stack_)):\n        n += self.lengthString(self.call_stack_[i].ByteSizePartial())\n    if self.has_datastore_details_:\n        n += (1 + self.lengthString(self.datastore_details_.ByteSizePartial()))\n    if self.has_call_cost_microdollars_:\n        n += (1 + self.lengthVarInt64(self.call_cost_microdollars_))\n    n += (1 * len(self.billed_ops_))\n    for i in xrange(len(self.billed_ops_)):\n        n += self.lengthString(self.billed_ops_[i].ByteSizePartial())\n    return n\n", "label": 1}
{"function": "\n\ndef _file_operation(operation, source_path, target_path=None, allow_undo=True, no_confirm=False, rename_on_collision=True, silent=False, hWnd=None):\n    source_path = (source_path or '')\n    if isinstance(source_path, basestring):\n        source_path = os.path.abspath(source_path)\n    else:\n        source_path = [os.path.abspath(i) for i in source_path]\n    target_path = (target_path or '')\n    if isinstance(target_path, basestring):\n        target_path = os.path.abspath(target_path)\n    else:\n        target_path = [os.path.abspath(i) for i in target_path]\n    flags = 0\n    if allow_undo:\n        flags |= shellcon.FOF_ALLOWUNDO\n    if no_confirm:\n        flags |= shellcon.FOF_NOCONFIRMATION\n    if rename_on_collision:\n        flags |= shellcon.FOF_RENAMEONCOLLISION\n    if silent:\n        flags |= shellcon.FOF_SILENT\n    (result, n_aborted) = shell.SHFileOperation(((hWnd or 0), operation, source_path, target_path, flags, None, None))\n    if (result != 0):\n        raise x_winshell(result)\n    elif n_aborted:\n        raise x_winshell(('%d operations were aborted by the user' % n_aborted))\n", "label": 1}
{"function": "\n\ndef _parse_signature(self, args, kw):\n    values = {\n        \n    }\n    (sig_args, var_args, var_kw, defaults) = self._func_signature\n    extra_kw = {\n        \n    }\n    for (name, value) in kw.iteritems():\n        if ((not var_kw) and (name not in sig_args)):\n            raise TypeError(('Unexpected argument %s' % name))\n        if (name in sig_args):\n            values[sig_args] = value\n        else:\n            extra_kw[name] = value\n    args = list(args)\n    sig_args = list(sig_args)\n    while args:\n        while (sig_args and (sig_args[0] in values)):\n            sig_args.pop(0)\n        if sig_args:\n            name = sig_args.pop(0)\n            values[name] = args.pop(0)\n        elif var_args:\n            values[var_args] = tuple(args)\n            break\n        else:\n            raise TypeError(('Extra position arguments: %s' % ', '.join((repr(v) for v in args))))\n    for (name, value_expr) in defaults.iteritems():\n        if (name not in values):\n            values[name] = self._template._eval(value_expr, self._ns, self._pos)\n    for name in sig_args:\n        if (name not in values):\n            raise TypeError(('Missing argument: %s' % name))\n    if var_kw:\n        values[var_kw] = extra_kw\n    return values\n", "label": 1}
{"function": "\n\ndef convert_func(self, lib, opts, args):\n    if (not opts.dest):\n        opts.dest = self.config['dest'].get()\n    if (not opts.dest):\n        raise ui.UserError('no convert destination set')\n    opts.dest = util.bytestring_path(opts.dest)\n    if (not opts.threads):\n        opts.threads = self.config['threads'].get(int)\n    if self.config['paths']:\n        path_formats = ui.get_path_formats(self.config['paths'])\n    else:\n        path_formats = ui.get_path_formats()\n    if (not opts.format):\n        opts.format = self.config['format'].get(unicode).lower()\n    pretend = (opts.pretend if (opts.pretend is not None) else self.config['pretend'].get(bool))\n    if (not pretend):\n        ui.commands.list_items(lib, ui.decargs(args), opts.album)\n        if (not (opts.yes or ui.input_yn('Convert? (Y/n)'))):\n            return\n    if opts.album:\n        albums = lib.albums(ui.decargs(args))\n        items = (i for a in albums for i in a.items())\n        if self.config['copy_album_art']:\n            for album in albums:\n                self.copy_album_art(album, opts.dest, path_formats, pretend)\n    else:\n        items = iter(lib.items(ui.decargs(args)))\n    convert = [self.convert_item(opts.dest, opts.keep_new, path_formats, opts.format, pretend) for _ in range(opts.threads)]\n    pipe = util.pipeline.Pipeline([items, convert])\n    pipe.run_parallel()\n", "label": 1}
{"function": "\n\ndef _get_default_compiler(output):\n    cc = os.environ.get('CC', '')\n    cxx = os.environ.get('CXX', '')\n    if (cc or cxx):\n        output.info(('CC and CXX: %s, %s ' % ((cc or 'None'), (cxx or 'None'))))\n        command = (cc or cxx)\n        if ('gcc' in command):\n            return _gcc_compiler(output, command)\n        if ('clang' in command.lower()):\n            return _clang_compiler(output, command)\n        output.error((\"Not able to automatically detect '%s' version\" % command))\n        return None\n    if (platform.system() == 'Windows'):\n        vs = _visual_compiler_last(output)\n    gcc = _gcc_compiler(output)\n    clang = _clang_compiler(output)\n    if (platform.system() == 'Windows'):\n        return (vs or gcc or clang)\n    elif (platform.system() == 'Darwin'):\n        return (clang or gcc)\n    else:\n        return (gcc or clang)\n", "label": 1}
{"function": "\n\ndef combineMaterials(mesh):\n    material_sets = []\n    for m in mesh.materials:\n        matched = False\n        for s in material_sets:\n            if (s[0].effect == m.effect):\n                s.append(m)\n                matched = True\n                break\n        if (not matched):\n            material_sets.append([m])\n    for s in material_sets:\n        if (len(s) <= 1):\n            continue\n        to_keep = s.pop(0)\n        for scene in mesh.scenes:\n            nodes_to_check = []\n            nodes_to_check.extend(scene.nodes)\n            while (len(nodes_to_check) > 0):\n                curnode = nodes_to_check.pop()\n                for node in curnode.children:\n                    if isinstance(node, collada.scene.Node):\n                        nodes_to_check.append(node)\n                    elif isinstance(node, collada.scene.GeometryNode):\n                        for matnode in node.materials:\n                            if (matnode.target in s):\n                                matnode.target = to_keep\n        for material in s:\n            del mesh.materials[material.id]\n", "label": 1}
{"function": "\n\n@property\ndef tests(self):\n    res = [x for x in self._tests if match_patterns(self.patterns, x['name'], default=True)]\n    res = [x for x in res if (not match_patterns(self.excludes, x['name'], default=False))]\n    res = [x for x in res if (x.get('perf', False) == self.perf)]\n    if (not self.nightly):\n        res = [x for x in res if (x.get('nightly', False) is False)]\n    if self.last_failed:\n        failed_names = self.get_last_failed_names()\n        res = [x for x in res if (x['name'] in failed_names)]\n        if (not res):\n            ui.warning('No failing tests found')\n    return res\n", "label": 1}
{"function": "\n\ndef _scheduleTransmitActual(self, intent):\n    if (intent.targetAddr == self.myAddress):\n        self._processReceivedEnvelope(ReceiveEnvelope(intent.targetAddr, intent.message))\n        return self._finishIntent(intent)\n    if isinstance(intent.targetAddr.addressDetails, RoutedTCPv4ActorAddress):\n        if (not isinstance(intent.message, ForwardMessage)):\n            routing = [(A or self._adminAddr) for A in intent.targetAddr.addressDetails.routing]\n            while (routing and (routing[0] == self.myAddress)):\n                routing = routing[1:]\n            if (self.txOnly and routing and (routing[0] != self._adminAddr)):\n                routing.insert(0, self._adminAddr)\n            if routing:\n                if ((len(routing) != 1) or (routing[0] != intent.targetAddr)):\n                    intent.changeMessage(ForwardMessage(intent.message, intent.targetAddr, self.myAddress, routing))\n                    intent.addCallback((lambda r, i, ta=intent.targetAddr: i.changeTargetAddr(ta)))\n                    intent.changeTargetAddr(intent.message.fwdTargets[0])\n                    self.scheduleTransmit(getattr(self, '_addressMgr', None), intent)\n                    return\n    intent.stage = self._XMITStepSendConnect\n    if self._nextTransmitStep(intent):\n        if hasattr(intent, 'socket'):\n            self._transmitIntents[intent.socket.fileno()] = intent\n        else:\n            self._waitingTransmits.append(intent)\n", "label": 1}
{"function": "\n\ndef _validate_option(self, option, val):\n    if (option in 'field_names'):\n        self._validate_field_names(val)\n    elif (option in ('start', 'end', 'max_width', 'padding_width', 'left_padding_width', 'right_padding_width', 'format')):\n        self._validate_nonnegative_int(option, val)\n    elif (option in 'sortby'):\n        self._validate_field_name(option, val)\n    elif (option in 'sort_key'):\n        self._validate_function(option, val)\n    elif (option in 'hrules'):\n        self._validate_hrules(option, val)\n    elif (option in 'vrules'):\n        self._validate_vrules(option, val)\n    elif (option in 'fields'):\n        self._validate_all_field_names(option, val)\n    elif (option in ('header', 'border', 'reversesort', 'xhtml', 'print_empty')):\n        self._validate_true_or_false(option, val)\n    elif (option in 'header_style'):\n        self._validate_header_style(val)\n    elif (option in 'int_format'):\n        self._validate_int_format(option, val)\n    elif (option in 'float_format'):\n        self._validate_float_format(option, val)\n    elif (option in ('vertical_char', 'horizontal_char', 'junction_char')):\n        self._validate_single_char(option, val)\n    elif (option in 'attributes'):\n        self._validate_attributes(option, val)\n    else:\n        raise Exception(('Unrecognised option: %s!' % option))\n", "label": 1}
{"function": "\n\ndef get_implicit_depends_on(input_hash, depends_on):\n    '\\n    Add DNAnexus links to non-closed data objects in input_hash to depends_on\\n    '\n    q = []\n    for field in input_hash:\n        possible_dep = get_nonclosed_data_obj_link(input_hash[field])\n        if (possible_dep is not None):\n            depends_on.append(possible_dep)\n        elif (isinstance(input_hash[field], list) or isinstance(input_hash[field], dict)):\n            q.append(input_hash[field])\n    while (len(q) > 0):\n        thing = q.pop()\n        if isinstance(thing, list):\n            for i in range(len(thing)):\n                possible_dep = get_nonclosed_data_obj_link(thing[i])\n                if (possible_dep is not None):\n                    depends_on.append(possible_dep)\n                elif (isinstance(thing[i], list) or isinstance(thing[i], dict)):\n                    q.append(thing[i])\n        else:\n            for field in thing:\n                possible_dep = get_nonclosed_data_obj_link(thing[field])\n                if (possible_dep is not None):\n                    depends_on.append(possible_dep)\n                elif (isinstance(thing[field], list) or isinstance(thing[field], dict)):\n                    q.append(thing[field])\n", "label": 1}
{"function": "\n\ndef test_filtered_nested_expression(self):\n    for ct in [0, 1, 2, 4, 8, 16, 32, 64]:\n        self.equivalentEvaluationTest((lambda : sum((((outer * 503) + inner) for outer in xrange(ct) for inner in xrange(outer)))))\n        self.equivalentEvaluationTest((lambda : sum((((outer * 503) + inner) for outer in xrange(ct) if ((outer % 2) == 0) for inner in xrange(outer)))))\n        self.equivalentEvaluationTest((lambda : sum((((outer * 503) + inner) for outer in xrange(ct) if ((outer % 2) == 0) for inner in xrange(outer) if ((inner % 2) == 0)))))\n        self.equivalentEvaluationTest((lambda : sum((((outer * 503) + inner) for outer in xrange(ct) for inner in xrange(outer) if ((inner % 2) == 0)))))\n", "label": 1}
{"function": "\n\ndef set_pubsub_channels(self, request, channels):\n    '\\n        Initialize the channels used for publishing and subscribing messages through the message queue.\\n        '\n    facility = request.path_info.replace(settings.WEBSOCKET_URL, '', 1)\n    audience = {\n        'users': ((('publish-user' in channels) and [SELF]) or []),\n        'groups': ((('publish-group' in channels) and [SELF]) or []),\n        'sessions': ((('publish-session' in channels) and [SELF]) or []),\n        'broadcast': ('publish-broadcast' in channels),\n    }\n    self._publishers = set()\n    for key in self._get_message_channels(request=request, facility=facility, **audience):\n        self._publishers.add(key)\n    audience = {\n        'users': ((('subscribe-user' in channels) and [SELF]) or []),\n        'groups': ((('subscribe-group' in channels) and [SELF]) or []),\n        'sessions': ((('subscribe-session' in channels) and [SELF]) or []),\n        'broadcast': ('subscribe-broadcast' in channels),\n    }\n    self._subscription = self._connection.pubsub()\n    for key in self._get_message_channels(request=request, facility=facility, **audience):\n        self._subscription.subscribe(key)\n", "label": 1}
{"function": "\n\ndef profile_main(func):\n    from cStringIO import StringIO\n    import cProfile\n    import logging\n    import pstats\n    import random\n    only_forced_profile = getattr(settings, 'ONLY_FORCED_PROFILE', False)\n    profile_percentage = getattr(settings, 'PROFILE_PERCENTAGE', None)\n    if ((only_forced_profile and ('profile=forced' not in os.environ.get('QUERY_STRING'))) or ((not only_forced_profile) and profile_percentage and ((float(profile_percentage) / 100.0) <= random.random()))):\n        return func()\n    prof = cProfile.Profile()\n    prof = prof.runctx('func()', globals(), locals())\n    stream = StringIO()\n    stats = pstats.Stats(prof, stream=stream)\n    sort_by = getattr(settings, 'SORT_PROFILE_RESULTS_BY', 'time')\n    if (not isinstance(sort_by, (list, tuple))):\n        sort_by = (sort_by,)\n    stats.sort_stats(*sort_by)\n    restrictions = []\n    profile_pattern = getattr(settings, 'PROFILE_PATTERN', None)\n    if profile_pattern:\n        restrictions.append(profile_pattern)\n    max_results = getattr(settings, 'MAX_PROFILE_RESULTS', 80)\n    if (max_results and (max_results != 'all')):\n        restrictions.append(max_results)\n    stats.print_stats(*restrictions)\n    extra_output = (getattr(settings, 'EXTRA_PROFILE_OUTPUT', None) or ())\n    if (not isinstance(sort_by, (list, tuple))):\n        extra_output = (extra_output,)\n    if ('callees' in extra_output):\n        stats.print_callees()\n    if ('callers' in extra_output):\n        stats.print_callers()\n    logging.info('Profile data:\\n%s', stream.getvalue())\n", "label": 1}
{"function": "\n\ndef get_command_part(parameter, python_proxy_pool=None):\n    'Converts a Python object into a string representation respecting the\\n    Py4J protocol.\\n\\n    For example, the integer `1` is converted to `u\"i1\"`\\n\\n    :param parameter: the object to convert\\n    :rtype: the string representing the command part\\n    '\n    command_part = ''\n    if (parameter is None):\n        command_part = NULL_TYPE\n    elif isinstance(parameter, bool):\n        command_part = (BOOLEAN_TYPE + smart_decode(parameter))\n    elif isinstance(parameter, Decimal):\n        command_part = (DECIMAL_TYPE + smart_decode(parameter))\n    elif (isinstance(parameter, int) and (parameter <= JAVA_MAX_INT)):\n        command_part = (INTEGER_TYPE + smart_decode(parameter))\n    elif (isinstance(parameter, long) or isinstance(parameter, int)):\n        command_part = (LONG_TYPE + smart_decode(parameter))\n    elif isinstance(parameter, float):\n        command_part = (DOUBLE_TYPE + encode_float(parameter))\n    elif isbytearray(parameter):\n        command_part = (BYTES_TYPE + encode_bytearray(parameter))\n    elif ispython3bytestr(parameter):\n        command_part = (BYTES_TYPE + encode_bytearray(parameter))\n    elif isinstance(parameter, basestring):\n        command_part = (STRING_TYPE + escape_new_line(parameter))\n    elif is_python_proxy(parameter):\n        command_part = (PYTHON_PROXY_TYPE + python_proxy_pool.put(parameter))\n        for interface in parameter.Java.implements:\n            command_part += (';' + interface)\n    else:\n        command_part = (REFERENCE_TYPE + parameter._get_object_id())\n    command_part += '\\n'\n    return command_part\n", "label": 1}
{"function": "\n\ndef select_dir(window, index=(- 2), level=0, paths=None, func=None, condition_func=None, is_user=False):\n    if (index == (- 1)):\n        return ''\n    if ((level > 0) and (index == 0)):\n        sel_path = paths[0].split('(')[1][:(- 1)]\n        if func:\n            return_code = func(window, sel_path)\n            if (return_code == 0):\n                return\n        else:\n            return\n    else:\n        if (index == 1):\n            level -= 1\n        elif (index > 1):\n            level += 1\n        if (level <= 0):\n            level = 0\n            dir_path = '.'\n            parent_path = '..'\n            if is_user:\n                paths = pyarduino.base.sys_path.list_user_root_path()\n            else:\n                paths = pyarduino.base.sys_path.list_os_root_path()\n        else:\n            sel_path = paths[index]\n            if (sel_path == pyarduino.base.sys_path.ROOT_PATH):\n                sel_path = '/'\n            dir_path = os.path.abspath(sel_path)\n            if (condition_func and condition_func(dir_path)):\n                func(window, dir_path)\n                return\n            parent_path = os.path.join(dir_path, '..')\n            cur_dir = pyarduino.base.abs_file.Dir(dir_path)\n            sub_dirs = cur_dir.list_dirs()\n            paths = [d.get_path() for d in sub_dirs]\n        paths.insert(0, parent_path)\n        paths.insert(0, ('Select current dir (%s)' % dir_path))\n    sublime.set_timeout((lambda : window.show_quick_panel(paths, (lambda index: select_dir(window, index, level, paths, func, condition_func, is_user)))), 5)\n", "label": 1}
{"function": "\n\ndef pytest_generate_tests(metafunc):\n    'Parametrize tests to run on all backends.'\n    if ('backend' in metafunc.fixturenames):\n        skip_backends = set()\n        if hasattr(metafunc.module, 'skip_backends'):\n            skip_backends = skip_backends.union(set(metafunc.module.skip_backends))\n        if hasattr(metafunc.cls, 'skip_backends'):\n            skip_backends = skip_backends.union(set(metafunc.cls.skip_backends))\n        if metafunc.config.option.backend:\n            backend = set([x.lower() for x in metafunc.config.option.backend])\n        else:\n            backend = set(backends.keys())\n        if hasattr(metafunc.module, 'backends'):\n            backend = backend.intersection(set(metafunc.module.backends))\n        if hasattr(metafunc.cls, 'backends'):\n            backend = backend.intersection(set(metafunc.cls.backends))\n        lazy = []\n        if (not (('skip_greedy' in metafunc.fixturenames) or metafunc.config.option.lazy)):\n            lazy.append('greedy')\n        if (not (('skip_lazy' in metafunc.fixturenames) or metafunc.config.option.greedy)):\n            lazy.append('lazy')\n        backend = [b for b in backend.difference(skip_backends) if (not (('skip_' + b) in metafunc.fixturenames))]\n        params = list(product(backend, lazy))\n        metafunc.parametrize('backend', (params or [(None, None)]), indirect=True, ids=['-'.join(p) for p in params])\n", "label": 1}
{"function": "\n\n@signalcommand\ndef handle(self, *args, **options):\n    if ((len(args) < 1) and (not options['all_applications'])):\n        raise CommandError('need one or more arguments for appname')\n    use_pygraphviz = options.get('pygraphviz', False)\n    use_pydot = options.get('pydot', False)\n    use_json = options.get('json', False)\n    if (use_json and (use_pydot or use_pygraphviz)):\n        raise CommandError('Cannot specify --json with --pydot or --pygraphviz')\n    cli_options = ' '.join(sys.argv[2:])\n    graph_data = generate_graph_data(args, cli_options=cli_options, **options)\n    if use_json:\n        self.render_output_json(graph_data, **options)\n        return\n    dotdata = generate_dot(graph_data)\n    if (not six.PY3):\n        dotdata = dotdata.encode('utf-8')\n    if options['outputfile']:\n        if ((not use_pygraphviz) and (not use_pydot)):\n            if HAS_PYGRAPHVIZ:\n                use_pygraphviz = True\n            elif HAS_PYDOT:\n                use_pydot = True\n        if use_pygraphviz:\n            self.render_output_pygraphviz(dotdata, **options)\n        elif use_pydot:\n            self.render_output_pydot(dotdata, **options)\n        else:\n            raise CommandError('Neither pygraphviz nor pydot could be found to generate the image')\n    else:\n        self.print_output(dotdata)\n", "label": 1}
{"function": "\n\ndef find_loose_thresholds(self, percentile=0.99):\n    results = []\n    for c in range(len(self.sensor.band_names)):\n        (threshold, params) = (self.thresholds[c], self.distributions[c])\n        if (params != None):\n            t = self.__cdf_percentile(params, percentile, self.backscatter_model[c])\n            if self.sensor.log_scale:\n                t = (10 ** t)\n            results.append(t)\n        else:\n            if self.sensor.log_scale:\n                threshold = math.log10(threshold)\n            start = self.histograms[c][0]\n            width = self.histograms[c][1]\n            values = self.histograms[c][2]\n            i = (int(((threshold - start) / width)) + 1)\n            if (self.backscatter_model[c] == RadarHistogram.BACKSCATTER_MODEL_DIP):\n                while (i < (len(values) - 3)):\n                    if ((values[i] > values[(i + 1)]) and (values[i] > values[(i + 2)]) and (values[i] > values[(i + 3)])):\n                        break\n                    i += 1\n            else:\n                while (i < (len(values) - 3)):\n                    if ((values[i] < values[(i + 1)]) and (values[i] < values[(i + 2)]) and (values[i] < values[(i + 3)])):\n                        break\n                    i += 1\n            t = (start + (i * width))\n            if self.sensor.log_scale:\n                t = (10 ** t)\n            results.append(t)\n    return results\n", "label": 1}
{"function": "\n\ndef register(name, service, export_methods=False):\n    _registry[name] = service\n    if (service not in _registered_modules):\n        _registered_modules.append(service)\n    if export_methods:\n        _methods[name] = {\n            \n        }\n        if inspect.ismodule(service):\n            for (attr, value) in service.__dict__.items():\n                if ((not attr.startswith('_')) and (not _registry.get(attr)) and hasattr(value, '__call__')):\n                    _methods[name][attr] = value\n        else:\n            for (attr, value) in service.__class__.__dict__.items():\n                if ((not attr.startswith('_')) and (not _registry.get(attr)) and hasattr(value, '__call__')):\n                    _methods[name][attr] = value.__get__(service, service.__class__)\n    for cls in _consumer_map.get(name, []):\n        _inject(cls, name, service)\n    if (name in _consumer_map):\n        del _consumer_map[name]\n    return service\n", "label": 1}
{"function": "\n\ndef checkReplaceSubstring(self, arguments):\n    allowedFields = {\n        \n    }\n    allowedFields['replace'] = (str, True)\n    allowedFields['with'] = (str, True)\n    for category in arguments.keys():\n        for argumentAttributes in arguments[category]:\n            if ('replace substring' in argumentAttributes):\n                argument = argumentAttributes['long form argument']\n                replace = argumentAttributes['replace substring']\n                for data in replace:\n                    if (not methods.checkIsDictionary(data, self.allowTermination)):\n                        if self.allowTermination:\n                            self.errors.invalidReplaceSubstring(self.name, category, argument)\n                        else:\n                            return False\n                    observedFields = []\n                    toReplace = None\n                    replaceWith = None\n                    for key in data:\n                        value = data[key]\n                        if (key not in allowedFields):\n                            if self.allowTermination:\n                                self.errors.invalidReplaceSubstring(self.name, category, argument)\n                            else:\n                                return False\n                        if (key == 'replace'):\n                            toReplace = value\n                        elif (key == 'with'):\n                            replaceWith = value\n                        observedFields.append(key)\n                    self.arguments[argument].isReplaceSubstring = True\n                    self.arguments[argument].replaceSubstring.append((str(toReplace), str(replaceWith)))\n                    for field in allowedFields.keys():\n                        if (allowedFields[field][1] and (field not in observedFields)):\n                            if self.allowTermination:\n                                self.errors.invalidReplaceSubstring(self.name, category, argument)\n                            else:\n                                return False\n    return True\n", "label": 1}
{"function": "\n\ndef _modified_event(self, dict_, attr, previous, collection=False, force=False):\n    if (not attr.send_modified_events):\n        return\n    if ((attr.key not in self.committed_state) or force):\n        if collection:\n            if (previous is NEVER_SET):\n                if (attr.key in dict_):\n                    previous = dict_[attr.key]\n            if (previous not in (None, NO_VALUE, NEVER_SET)):\n                previous = attr.copy(previous)\n        self.committed_state[attr.key] = previous\n    if ((self.session_id and (self._strong_obj is None)) or (not self.modified)):\n        instance_dict = self._instance_dict()\n        if instance_dict:\n            instance_dict._modified.add(self)\n        inst = self.obj()\n        if self.session_id:\n            self._strong_obj = inst\n        if (inst is None):\n            raise orm_exc.ObjectDereferencedError((\"Can't emit change event for attribute '%s' - parent object of type %s has been garbage collected.\" % (self.manager[attr.key], base.state_class_str(self))))\n        self.modified = True\n", "label": 1}
{"function": "\n\n@register.inclusion_tag('scaffold/admin/submit_line.html', takes_context=True)\ndef submit_row(context):\n    '\\n    Displays the row of buttons for delete and save. \\n    '\n    opts = context['opts']\n    change = context['change']\n    is_popup = context['is_popup']\n    save_as = context['save_as']\n    allow_associated_ordering = context['allow_associated_ordering']\n    model_label = context['model_label']\n    return {\n        'model_label': model_label,\n        'allow_associated_ordering': allow_associated_ordering,\n        'show_move': ((not is_popup) and change),\n        'onclick_attrib': ((opts.get_ordered_objects() and change and 'onclick=\"submitOrderForm();\"') or ''),\n        'show_delete_link': ((not is_popup) and context['has_delete_permission'] and (change or context['show_delete'])),\n        'show_save_as_new': ((not is_popup) and change and save_as),\n        'show_save_and_add_another': (context['has_add_permission'] and (not is_popup) and ((not save_as) or context['add'])),\n        'show_save_and_continue': ((not is_popup) and context['has_change_permission']),\n        'is_popup': is_popup,\n        'show_save': True,\n    }\n", "label": 1}
{"function": "\n\ndef _get_keystone_session(self):\n    cacert = (self.options.os_cacert or None)\n    cert = (self.options.os_cert or None)\n    key = (self.options.os_key or None)\n    insecure = (self.options.insecure or False)\n    ks_session = session.Session.construct(dict(cacert=cacert, cert=cert, key=key, insecure=insecure))\n    (v2_auth_url, v3_auth_url) = self._discover_auth_versions(session=ks_session, auth_url=self.options.os_auth_url)\n    user_domain_name = (self.options.os_user_domain_name or None)\n    user_domain_id = (self.options.os_user_domain_id or None)\n    project_domain_name = (self.options.os_project_domain_name or None)\n    project_domain_id = (self.options.os_project_domain_id or None)\n    domain_info = (user_domain_name or user_domain_id or project_domain_name or project_domain_id)\n    if ((v2_auth_url and (not domain_info)) or (not v3_auth_url)):\n        ks_session.auth = self.get_v2_auth(v2_auth_url)\n    else:\n        ks_session.auth = self.get_v3_auth(v3_auth_url)\n    return ks_session\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.re:\n        result = self.re.match(instring, loc)\n        if (not result):\n            raise ParseException(instring, loc, self.errmsg, self)\n        loc = result.end()\n        return (loc, result.group())\n    if (not (instring[loc] in self.initChars)):\n        raise ParseException(instring, loc, self.errmsg, self)\n    start = loc\n    loc += 1\n    instrlen = len(instring)\n    bodychars = self.bodyChars\n    maxloc = (start + self.maxLen)\n    maxloc = min(maxloc, instrlen)\n    while ((loc < maxloc) and (instring[loc] in bodychars)):\n        loc += 1\n    throwException = False\n    if ((loc - start) < self.minLen):\n        throwException = True\n    if (self.maxSpecified and (loc < instrlen) and (instring[loc] in bodychars)):\n        throwException = True\n    if self.asKeyword:\n        if (((start > 0) and (instring[(start - 1)] in bodychars)) or ((loc < instrlen) and (instring[loc] in bodychars))):\n            throwException = True\n    if throwException:\n        raise ParseException(instring, loc, self.errmsg, self)\n    return (loc, instring[start:loc])\n", "label": 1}
{"function": "\n\ndef cluster_vectorspace(self, vectors, trace=False):\n    if (self._means and (self._repeats > 1)):\n        print('Warning: means will be discarded for subsequent trials')\n    meanss = []\n    for trial in range(self._repeats):\n        if trace:\n            print('k-means trial', trial)\n        if ((not self._means) or (trial > 1)):\n            self._means = self._rng.sample(list(vectors), self._num_means)\n        self._cluster_vectorspace(vectors, trace)\n        meanss.append(self._means)\n    if (len(meanss) > 1):\n        for means in meanss:\n            means.sort(key=sum)\n        min_difference = min_means = None\n        for i in range(len(meanss)):\n            d = 0\n            for j in range(len(meanss)):\n                if (i != j):\n                    d += self._sum_distances(meanss[i], meanss[j])\n            if ((min_difference is None) or (d < min_difference)):\n                (min_difference, min_means) = (d, meanss[i])\n        self._means = min_means\n", "label": 1}
{"function": "\n\ndef __process_escapes(self, s, literal_start):\n    'Convert backlash sequences in raw string literal.\\n\\n        @param s: The raw contents of a raw string literal.\\n\\n        @return: The string with all backlash sequences converted to their chars.'\n    if (s.find('\\\\') < 0):\n        return s\n    slen = len(s)\n    sb = []\n    i = 0\n    while (i < slen):\n        c = s[i]\n        if (c != '\\\\'):\n            sb.append(c)\n            i += 1\n            continue\n        if ((i + 1) >= slen):\n            self.__error('No character after escape', (literal_start + i))\n        i += 1\n        c = s[i]\n        if (c == 't'):\n            sb.append('\\t')\n        elif (c == 'n'):\n            sb.append('\\n')\n        elif (c == 'r'):\n            sb.append('\\r')\n        elif (c == 'b'):\n            sb.append('\\x08')\n        elif (c == 'f'):\n            sb.append('\\x0c')\n        elif (c == '\"'):\n            sb.append('\"')\n        elif (c == '\\\\'):\n            sb.append('\\\\')\n        elif (c == '/'):\n            sb.append('/')\n        elif ((c == 'u') and ((i + 5) <= slen)):\n            hex_string = s[(i + 1):(i + 5)]\n            i += 4\n            sb.append(unichr(int(hex_string, 16)))\n        else:\n            self.__error((('Unexpected backslash escape [' + c) + ']'), (literal_start + i))\n        i += 1\n    return ''.join(sb)\n", "label": 1}
{"function": "\n\ndef fireMouseEvent(listeners, sender, event):\n    x = (DOM.eventGetClientX(event) - DOM.getAbsoluteLeft(sender.getElement()))\n    y = (DOM.eventGetClientY(event) - DOM.getAbsoluteTop(sender.getElement()))\n    etype = DOM.eventGetType(event)\n    if (etype == 'mousedown'):\n        for listener in listeners:\n            listener.onMouseDown(sender, x, y)\n        return True\n    elif (etype == 'mouseup'):\n        for listener in listeners:\n            listener.onMouseUp(sender, x, y)\n        return True\n    elif (etype == 'mousemove'):\n        for listener in listeners:\n            listener.onMouseMove(sender, x, y)\n        return True\n    elif (etype == 'mouseover'):\n        to_element = DOM.eventGetToElement(event)\n        if (to_element and (not DOM.isOrHasChild(sender.getElement(), to_element))):\n            for listener in listeners:\n                listener.onMouseEnter(sender)\n        return True\n    elif (etype == 'mouseout'):\n        to_element = DOM.eventGetToElement(event)\n        if (to_element and (not DOM.isOrHasChild(sender.getElement(), to_element))):\n            for listener in listeners:\n                listener.onMouseLeave(sender)\n        return True\n    return False\n", "label": 1}
{"function": "\n\n@defun\ndef gammainc(ctx, z, a=0, b=None, regularized=False):\n    regularized = bool(regularized)\n    z = ctx.convert(z)\n    if (a is None):\n        a = ctx.zero\n        lower_modified = False\n    else:\n        a = ctx.convert(a)\n        lower_modified = (a != ctx.zero)\n    if (b is None):\n        b = ctx.inf\n        upper_modified = False\n    else:\n        b = ctx.convert(b)\n        upper_modified = (b != ctx.inf)\n    if (not (upper_modified or lower_modified)):\n        if regularized:\n            if (ctx.re(z) < 0):\n                return ctx.inf\n            elif (ctx.re(z) > 0):\n                return ctx.one\n            else:\n                return ctx.nan\n        return ctx.gamma(z)\n    if (a == b):\n        return ctx.zero\n    if (ctx.re(a) > ctx.re(b)):\n        return (- ctx.gammainc(z, b, a, regularized))\n    if (upper_modified and lower_modified):\n        return (+ ctx._gamma3(z, a, b, regularized))\n    elif lower_modified:\n        return ctx._upper_gamma(z, a, regularized)\n    elif upper_modified:\n        return ctx._lower_gamma(z, b, regularized)\n", "label": 1}
{"function": "\n\ndef larch_exec(self, text, debug=True):\n    'execute larch command'\n    self.debug = debug\n    if (not self.initialized):\n        self.initialize_larch()\n    text = text.strip()\n    if (text in ('quit', 'exit', 'EOF')):\n        self.exit()\n    else:\n        ret = None\n        self.input.put(text, lineno=0)\n        while (len(self.input) > 0):\n            (block, fname, lineno) = self.input.get()\n            if (len(block) == 0):\n                continue\n            if self.local_echo:\n                print(block)\n            ret = self.larch.eval(block, fname=fname, lineno=lineno)\n            if self.larch.error:\n                err = self.larch.error.pop(0)\n                (fname, lineno) = (err.fname, err.lineno)\n                self.write(('%s\\n' % err.get_error()[1]))\n                for err in self.larch.error:\n                    if (self.debug or (((err.fname != fname) or (err.lineno != lineno)) and (err.lineno > 0) and (lineno > 0))):\n                        self.write(('%s\\n' % err.get_error()[1]))\n                self.input.clear()\n                break\n            elif (ret is not None):\n                self.write(('%s\\n' % repr(ret)))\n        self.keep_alive_time = (time.time() + self.IDLE_TIME)\n    return (ret is None)\n", "label": 1}
{"function": "\n\ndef _parse_function_type(self, typenode, funcname=None):\n    params = list(getattr(typenode.args, 'params', []))\n    for (i, arg) in enumerate(params):\n        if (not hasattr(arg, 'type')):\n            raise api.CDefError((\"%s arg %d: unknown type '%s' (if you meant to use the old C syntax of giving untyped arguments, it is not supported)\" % ((funcname or 'in expression'), (i + 1), getattr(arg, 'name', '?'))))\n    ellipsis = ((len(params) > 0) and isinstance(params[(- 1)].type, pycparser.c_ast.TypeDecl) and isinstance(params[(- 1)].type.type, pycparser.c_ast.IdentifierType) and (params[(- 1)].type.type.names == ['__dotdotdot__']))\n    if ellipsis:\n        params.pop()\n        if (not params):\n            raise api.CDefError((\"%s: a function with only '(...)' as argument is not correct C\" % (funcname or 'in expression')))\n    args = [self._as_func_arg(*self._get_type_and_quals(argdeclnode.type)) for argdeclnode in params]\n    if ((not ellipsis) and (args == [model.void_type])):\n        args = []\n    (result, quals) = self._get_type_and_quals(typenode.type)\n    abi = None\n    if hasattr(typenode.type, 'quals'):\n        if (typenode.type.quals[(- 3):] == ['volatile', 'volatile', 'const']):\n            abi = '__stdcall'\n    return model.RawFunctionType(tuple(args), result, ellipsis, abi)\n", "label": 1}
{"function": "\n\ndef processConfigurationFile(self, data):\n    self.success = self.checkIsTool(data)\n    if self.success:\n        self.checkTopLevelInformation(data)\n    if self.success:\n        self.checkInputArguments(data['arguments'])\n    if self.success:\n        self.checkOutputArguments(data['arguments'])\n    if self.success:\n        self.checkRemainingArguments(data['arguments'])\n    if self.success:\n        self.success = self.checkWeb()\n    if self.success:\n        self.checkAttributeCombinations()\n    if self.success:\n        self.forceAttributes()\n    if self.success:\n        self.checkAttributeValues()\n    if self.success:\n        self.checkStreamInstructions()\n    if self.success:\n        self.checkConstructionInstructions()\n    if self.success:\n        self.checkReplaceSubstring(data['arguments'])\n    if self.success:\n        self.checkValueCommand()\n    if self.success:\n        self.success = self.checkArgumentOrder()\n    if self.success:\n        self.success = self.parameterSets.checkParameterSets(data['parameter sets'], self.allowTermination, self.name, isTool=True)\n", "label": 1}
{"function": "\n\ndef make_rst(self):\n    (module, class_name) = self.arguments[0].rsplit('.', 1)\n    arguments = self.arguments[1:]\n    obj = import_object(module, class_name)\n    is_configurable = (':no-config:' not in arguments)\n    is_commandable = (':no-commands:' not in arguments)\n    arguments = [i for i in arguments if (i not in (':no-config:', ':no-commands:'))]\n    defaults = {\n        \n    }\n    for klass in reversed(obj.mro()):\n        if (not issubclass(klass, configurable.Configurable)):\n            continue\n        if (not hasattr(klass, 'defaults')):\n            continue\n        klass_defaults = getattr(klass, 'defaults')\n        defaults.update({d[0]: d[1:] for d in klass_defaults})\n    defaults = [(k, v[0], v[1]) for (k, v) in sorted(defaults.items())]\n    context = {\n        'module': module,\n        'class_name': class_name,\n        'class_underline': ('=' * len(class_name)),\n        'obj': obj,\n        'defaults': defaults,\n        'configurable': (is_configurable and issubclass(obj, configurable.Configurable)),\n        'commandable': (is_commandable and issubclass(obj, command.CommandObject)),\n        'is_widget': issubclass(obj, widget.base._Widget),\n        'extra_arguments': arguments,\n    }\n    if context['commandable']:\n        context['commands'] = [attr for attr in dir(obj) if attr.startswith('cmd_')]\n    rst = qtile_class_template.render(**context)\n    for line in rst.splitlines():\n        (yield line)\n", "label": 1}
{"function": "\n\ndef getFirstBracketError(self, view):\n    opener = list('({[')\n    closer = list(')}]')\n    matchingStack = []\n    successResult = BracketResult(True, (- 1), (- 1))\n    codeStr = view.substr(sublime.Region(0, view.size()))\n    for (index, char) in enumerate(codeStr):\n        if ((char not in opener) and (not (char in closer))):\n            continue\n        scopeName = view.scope_name(index)\n        hasScope = (lambda s: (s in scopeName))\n        markdownBracketScopeBegin = 'punctuation.definition.string.begin.markdown'\n        markdownBracketScopeEnd = 'punctuation.definition.string.end.markdown'\n        isMarkdownStringBeginOrEnd = (lambda s: (hasScope(markdownBracketScopeBegin) or hasScope(markdownBracketScopeEnd)))\n        if ((hasScope('string') and (not hasScope('unquoted')) and (not isMarkdownStringBeginOrEnd(markdownBracketScopeBegin))) or hasScope('comment')):\n            continue\n        if (char in opener):\n            matchingStack.append(BracketPosition(index, char))\n        elif (char in closer):\n            matchingOpener = opener[closer.index(char)]\n            if (len(matchingStack) == 0):\n                return BracketResult(False, (- 1), index)\n            poppedOpener = matchingStack.pop()\n            if (matchingOpener != poppedOpener.opener):\n                return BracketResult(False, poppedOpener.position, index)\n    if (len(matchingStack) == 0):\n        return successResult\n    else:\n        poppedOpener = matchingStack.pop()\n        return BracketResult(False, poppedOpener.position, (- 1))\n", "label": 1}
{"function": "\n\ndef translate(pat):\n    'Translate a shell PATTERN to a regular expression.\\n\\n    There is no way to quote meta-characters.\\n    '\n    (i, n) = (0, len(pat))\n    res = ''\n    while (i < n):\n        c = pat[i]\n        i = (i + 1)\n        if (c == '*'):\n            res = (res + '(.*)')\n        elif (c == '?'):\n            res = (res + '(.)')\n        elif (c == '['):\n            j = i\n            if ((j < n) and (pat[j] == '!')):\n                j = (j + 1)\n            if ((j < n) and (pat[j] == ']')):\n                j = (j + 1)\n            while ((j < n) and (pat[j] != ']')):\n                j = (j + 1)\n            if (j >= n):\n                res = (res + '\\\\[')\n            else:\n                stuff = pat[i:j].replace('\\\\', '\\\\\\\\')\n                i = (j + 1)\n                if (stuff[0] == '!'):\n                    stuff = ('^' + stuff[1:])\n                elif (stuff[0] == '^'):\n                    stuff = ('\\\\' + stuff)\n                res = ('%s([%s])' % (res, stuff))\n        else:\n            res = (res + re.escape(c))\n    return (res + '\\\\Z(?ms)')\n", "label": 1}
{"function": "\n\ndef GetItem(self, user, route, has_perm=False, need_perm=False):\n    self.CheckUpdate()\n    if (self.mtitle.find('(BM:') != (- 1)):\n        if (Board.Board.IsBM(user, self.mtitle[4:]) or user.IsSysop()):\n            has_perm = True\n        elif (need_perm and (not has_perm)):\n            return None\n    if ((self.mtitle.find('(BM: BMS)') != (- 1)) or (self.mtitle.find('(BM: SECRET)') != (- 1)) or (self.mtitle.find('(BM: SYSOPS)') != (- 1))):\n        need_perm = True\n    if (len(route) == 0):\n        return self\n    target = (route[0] - 1)\n    _id = target\n    if (_id >= len(self.items)):\n        return None\n    while (self.items[_id].EffectiveId(user) < target):\n        _id += 1\n        if (_id >= len(self.items)):\n            return None\n    item = self.items[_id]\n    item.mtitle = item.title\n    if (len(route) == 1):\n        return item\n    elif item.IsDir():\n        if (not item.CheckUpdate()):\n            return None\n        return item.GetItem(user, route[1:], has_perm, need_perm)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef _get_show_with_qualifiers(show_name, qualifiers):\n    shows = get_show_list(show_name)\n    best_match = (- 1)\n    show_match = None\n    for show in shows:\n        if show.premiered:\n            premiered = show.premiered[:(- 6)].lower()\n        else:\n            premiered = None\n        if (show.network and show.network.get('name')):\n            network = show.network['name'].lower()\n        else:\n            network = None\n        if (show.web_channel and show.web_channel.get('name')):\n            web_channel = show.web_channel['name'].lower()\n        else:\n            web_channel = None\n        if (show.network and show.network.get('country') and show.network['country'].get('code')):\n            country = show.network['country']['code'].lower()\n        elif (show.web_channel and show.web_channel.get('country') and show.web_channel['country'].get('code')):\n            country = show.web_channel['country']['code'].lower()\n        else:\n            country = None\n        if show.language:\n            language = show.language.lower()\n        else:\n            language = None\n        attributes = [premiered, country, network, language, web_channel]\n        show_score = len((set(qualifiers) & set(attributes)))\n        if (show_score > best_match):\n            best_match = show_score\n            show_match = show\n    return show_match\n", "label": 1}
{"function": "\n\ndef check_record(d):\n    'check for mandatory, select options, dates. these should ideally be in doclist'\n    from frappe.utils.dateutils import parse_date\n    doc = frappe.get_doc(d)\n    for key in d:\n        docfield = doc.meta.get_field(key)\n        val = d[key]\n        if docfield:\n            if (docfield.reqd and ((val == '') or (val == None))):\n                frappe.msgprint(_('{0} is required').format(docfield.label), raise_exception=1)\n            if ((docfield.fieldtype == 'Select') and val and docfield.options):\n                if (val not in docfield.options.split('\\n')):\n                    frappe.throw(_('{0} must be one of {1}').format(_(docfield.label), comma_or(docfield.options.split('\\n'))))\n            if (val and (docfield.fieldtype == 'Date')):\n                d[key] = parse_date(val)\n            elif (val and (docfield.fieldtype in ['Int', 'Check'])):\n                d[key] = cint(val)\n            elif (val and (docfield.fieldtype in ['Currency', 'Float', 'Percent'])):\n                d[key] = flt(val)\n", "label": 1}
{"function": "\n\ndef skel_setup(environment, inventory):\n    'Build out the main inventory skeleton as needed.\\n\\n    :param environment: ``dict`` Known environment information\\n    :param inventory: ``dict``  Living dictionary of inventory\\n    '\n    for (key, value) in environment.iteritems():\n        if (key == 'version'):\n            continue\n        for (_key, _value) in value.iteritems():\n            if (_key not in inventory):\n                inventory[_key] = {\n                    \n                }\n                if _key.endswith('container'):\n                    if ('hosts' not in inventory[_key]):\n                        inventory[_key]['hosts'] = []\n                else:\n                    if ('children' not in inventory[_key]):\n                        inventory[_key]['children'] = []\n                    if ('hosts' not in inventory[_key]):\n                        inventory[_key]['hosts'] = []\n            if ('belongs_to' in _value):\n                for assignment in _value['belongs_to']:\n                    if (assignment not in inventory):\n                        inventory[assignment] = {\n                            \n                        }\n                        if ('children' not in inventory[assignment]):\n                            inventory[assignment]['children'] = []\n                        if ('hosts' not in inventory[assignment]):\n                            inventory[assignment]['hosts'] = []\n", "label": 1}
{"function": "\n\ndef validate_ml_unique(self):\n    form_errors = []\n    if (not hasattr(self.instance._meta, 'translation_model')):\n        return\n    for check in self.instance._meta.translation_model._meta.unique_together[:]:\n        lookup_kwargs = {\n            'language_code': GLL.language_code,\n        }\n        for field_name in check:\n            if (self.cleaned_data.get(field_name) is not None):\n                lookup_kwargs[field_name] = self.cleaned_data.get(field_name)\n        if ((len(check) == 2) and ('master' in check) and ('language_code' in check)):\n            continue\n        qs = self.instance._meta.translation_model.objects.filter(**lookup_kwargs)\n        if (self.instance.pk is not None):\n            qs = qs.exclude(master=self.instance.pk)\n        if qs.count():\n            model_name = capfirst(self.instance._meta.verbose_name)\n            field_labels = []\n            for field_name in check:\n                if (field_name == 'language_code'):\n                    field_labels.append(_('language'))\n                elif (field_name == 'master'):\n                    continue\n                else:\n                    field_labels.append(self.instance._meta.translation_model._meta.get_field_by_name(field_name)[0].verbose_name)\n            field_labels = get_text_list(field_labels, _('and'))\n            form_errors.append((_('%(model_name)s with this %(field_label)s already exists.') % {\n                'model_name': unicode(model_name),\n                'field_label': unicode(field_labels),\n            }))\n    if form_errors:\n        raise ValidationError(form_errors)\n", "label": 1}
{"function": "\n\ndef selector(self, output):\n    if isinstance(output, boto.ec2.instance.Reservation):\n        return self.parseReservation(output)\n    elif isinstance(output, boto.ec2.instance.Instance):\n        return self.parseInstance(output)\n    elif isinstance(output, boto.ec2.volume.Volume):\n        return self.parseVolume(output)\n    elif isinstance(output, boto.ec2.blockdevicemapping.BlockDeviceType):\n        return self.parseBlockDeviceType(output)\n    elif isinstance(output, boto.ec2.zone.Zone):\n        return self.parseEC2Zone(output)\n    elif isinstance(output, boto.ec2.address.Address):\n        return self.parseAddress(output)\n    elif isinstance(output, boto.route53.record.Record):\n        return self.parseRecord(output)\n    elif isinstance(output, boto.route53.zone.Zone):\n        return self.parseR53Zone(output)\n    elif isinstance(output, boto.route53.status.Status):\n        return self.parseR53Status(output)\n    elif isinstance(output, boto.ec2.tag.Tag):\n        return self.parseTag(output)\n    elif isinstance(output, boto.ec2.ec2object.EC2Object):\n        return self.parseEC2Object(output)\n    elif isinstance(output, boto.cloudformation.stack.Stack):\n        return self.parseStackObject(output)\n    elif isinstance(output, boto.rds.dbinstance.DBInstance):\n        return self.parseDBInstanceObject(output)\n    else:\n        return output\n", "label": 1}
{"function": "\n\ndef line_starts_with_open_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)/\\\\*', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\n@gof.local_optimizer([T.Elemwise])\ndef local_fill_sink(node):\n    '\\n    f(fill(a, b), fill(c, d), e) -> fill(a, fill(c, f(b, d, e)))\\n\\n    f need to be an elemwise\\n    '\n    if ((not hasattr(node, 'op')) or (not isinstance(node.op, T.Elemwise)) or (node.op == T.fill)):\n        return False\n    models = []\n    inputs = []\n    for input in node.inputs:\n        if (input.owner and (input.owner.op == T.fill)):\n            models.append(input.owner.inputs[0])\n            inputs.append(input.owner.inputs[1])\n        else:\n            inputs.append(input)\n    if (not models):\n        return False\n    c = node.op(*inputs)\n    for model in models:\n        if (model.type != c.type):\n            c = T.fill(model, c)\n    replacements = {\n        node.outputs[0]: c,\n    }\n    for (client, cl_idx) in node.outputs[0].clients:\n        if (hasattr(client, 'op') and isinstance(client.op, T.Elemwise) and (not (client.op == T.fill))):\n            client_inputs = client.inputs[:]\n            client_inputs[cl_idx] = c\n            new_client = client.op(*client_inputs)\n            new_client.owner.outputs[0].clients = client.outputs[0].clients\n            r = local_fill_sink.transform(new_client.owner)\n            if (not r):\n                continue\n            replacements.update(r)\n    return replacements\n", "label": 1}
{"function": "\n\ndef _set_knspace(self, value):\n    if (value is self._knspace):\n        return\n    knspace = (self._knspace or self.__last_knspace)\n    name = self.knsname\n    if (name and knspace and (getattr(knspace, name) == self)):\n        setattr(knspace, name, None)\n    if (value == 'fork'):\n        if (not knspace):\n            knspace = self.knspace\n        if knspace:\n            value = knspace.fork()\n        else:\n            raise ValueError('Cannot fork with no namesapce')\n    for (obj, prop_name, uid) in (self.__callbacks or []):\n        obj.unbind_uid(prop_name, uid)\n    self.__last_knspace = self.__callbacks = None\n    if name:\n        if (value is None):\n            knspace = self.__set_parent_knspace()\n            if knspace:\n                setattr(knspace, name, self)\n            self._knspace = None\n        else:\n            setattr(value, name, self)\n            knspace = self._knspace = value\n        if (not knspace):\n            raise ValueError('Object has name \"{}\", but no namespace'.format(name))\n    else:\n        if (value is None):\n            self.__set_parent_knspace()\n        self._knspace = value\n", "label": 1}
{"function": "\n\ndef _guess_genre_and_host_from_aliases(self):\n    \"Uses available aliases to decide the item's genre\"\n    genre = 'unknown'\n    host = 'unknown'\n    if hasattr(self, 'doi'):\n        joined_doi_string = ''.join(self.doi).lower()\n        if ('10.5061/dryad.' in joined_doi_string):\n            genre = 'dataset'\n            host = 'dryad'\n        elif ('.figshare.' in joined_doi_string):\n            host = 'figshare'\n            genre = 'dataset'\n        else:\n            genre = 'article'\n    elif hasattr(self, 'pmid'):\n        genre = 'article'\n    elif hasattr(self, 'arxiv'):\n        genre = 'article'\n        host = 'arxiv'\n    elif hasattr(self, 'blog'):\n        genre = 'blog'\n        host = 'wordpresscom'\n    elif hasattr(self, 'blog_post'):\n        genre = 'blog'\n        host = 'blog_post'\n    elif hasattr(self, 'url'):\n        joined_url_string = ''.join(self.url).lower()\n        if ('slideshare.net' in joined_url_string):\n            genre = 'slides'\n            host = 'slideshare'\n        elif ('github.com' in joined_url_string):\n            genre = 'software'\n            host = 'github'\n        elif (('youtube.com' in joined_url_string) or ('youtu.be' in joined_url_string)):\n            genre = 'video'\n            host = 'youtube'\n        elif ('vimeo.com' in joined_url_string):\n            genre = 'video'\n            host = 'vimeo'\n        else:\n            genre = 'webpage'\n    return (genre, host)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.query = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.configuration = []\n                (_etype3, _size0) = iprot.readListBegin()\n                for _i4 in xrange(_size0):\n                    _elem5 = iprot.readString()\n                    self.configuration.append(_elem5)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.hadoop_user = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef test_get_case_ids_in_domain_by_owner(self):\n    self.assertEqual(set(get_case_ids_in_domain_by_owner(self.domain, owner_id='XXX')), {case.get_id for case in self.cases if ((case.domain == self.domain) and (case.user_id == 'XXX'))})\n    self.assertEqual(set(get_case_ids_in_domain_by_owner(self.domain, owner_id__in=['XXX'])), {case.get_id for case in self.cases if ((case.domain == self.domain) and (case.user_id == 'XXX'))})\n    self.assertEqual(set(get_case_ids_in_domain_by_owner(self.domain, owner_id='XXX', closed=False)), {case.get_id for case in self.cases if ((case.domain == self.domain) and (case.user_id == 'XXX') and (case.closed is False))})\n    self.assertEqual(set(get_case_ids_in_domain_by_owner(self.domain, owner_id='XXX', closed=True)), {case.get_id for case in self.cases if ((case.domain == self.domain) and (case.user_id == 'XXX') and (case.closed is True))})\n", "label": 1}
{"function": "\n\ndef _transform(self, x, imgsz, roisz, offset, intype, output):\n    xtemp = []\n    if (intype == 'SRC'):\n        xtemp = [(xt / float(imgsz)) for xt in x]\n    elif (intype == 'ROI'):\n        xtemp = [((xt + offset) / float(imgsz)) for xt in x]\n    elif (intype == 'ROI_UNIT'):\n        xtemp = [(((xt * roisz) + offset) / float(imgsz)) for xt in x]\n    elif (intype == 'SRC_UNIT'):\n        xtemp = x\n    else:\n        logger.warning('Bad Parameter to CoordTransformX')\n        return None\n    retVal = []\n    if (output == 'SRC'):\n        retVal = [int((xt * imgsz)) for xt in xtemp]\n    elif (output == 'ROI'):\n        retVal = [int(((xt * imgsz) - offset)) for xt in xtemp]\n    elif (output == 'ROI_UNIT'):\n        retVal = [int((((xt * imgsz) - offset) / float(roisz))) for xt in xtemp]\n    elif (output == 'SRC_UNIT'):\n        retVal = xtemp\n    else:\n        logger.warning('Bad Parameter to CoordTransformX')\n        return None\n    return retVal\n", "label": 1}
{"function": "\n\ndef __init__(self, template):\n    self.template = template\n    self.parts = []\n    parts = re.split('(\\\\{[^\\\\}]*\\\\})', self.template)\n    for part in parts:\n        if part:\n            if (('{' == part[0]) and ('}' == part[(- 1)])):\n                expression = part[1:(- 1)]\n                if re.match('^([a-zA-Z0-9_]|%[0-9a-fA-F][0-9a-fA-F]).*$', expression):\n                    self.parts.append(SimpleExpansion(expression))\n                elif ('+' == part[1]):\n                    self.parts.append(ReservedExpansion(expression))\n                elif ('#' == part[1]):\n                    self.parts.append(FragmentExpansion(expression))\n                elif ('.' == part[1]):\n                    self.parts.append(LabelExpansion(expression))\n                elif ('/' == part[1]):\n                    self.parts.append(PathExpansion(expression))\n                elif (';' == part[1]):\n                    self.parts.append(PathStyleExpansion(expression))\n                elif ('?' == part[1]):\n                    self.parts.append(FormStyleQueryExpansion(expression))\n                elif ('&' == part[1]):\n                    self.parts.append(FormStyleQueryContinuation(expression))\n                elif (part[1] in '=,!@|'):\n                    raise UnsupportedExpression(part)\n                else:\n                    raise BadExpression(part)\n            elif (('{' not in part) and ('}' not in part)):\n                self.parts.append(Literal(part))\n            else:\n                raise BadExpression(part)\n", "label": 1}
{"function": "\n\ndef _validate(self, args, kwargs):\n    'Validate option registration arguments.'\n\n    def error(exception_type, arg_name=None, **msg_kwargs):\n        if (arg_name is None):\n            arg_name = (args[0] if args else '<unknown>')\n        raise exception_type(self.scope, arg_name, **msg_kwargs)\n    if (not args):\n        error(NoOptionNames)\n    for arg in args:\n        if (not arg.startswith('-')):\n            error(OptionNameDash, arg_name=arg)\n        if ((not arg.startswith('--')) and (len(arg) > 2)):\n            error(OptionNameDoubleDash, arg_name=arg)\n    if (('implicit_value' in kwargs) and (kwargs['implicit_value'] is None)):\n        error(ImplicitValIsNone)\n    if (('member_type' in kwargs) and (kwargs.get('type', str) not in [list, list_option])):\n        error(MemberTypeNotAllowed, type_=kwargs.get('type', str).__name__)\n    if (kwargs.get('member_type', str) not in self._allowed_member_types):\n        error(InvalidMemberType, member_type=kwargs.get('member_type', str).__name__)\n    for kwarg in kwargs:\n        if (kwarg not in self._allowed_registration_kwargs):\n            error(InvalidKwarg, kwarg=kwarg)\n    removal_version = kwargs.get('removal_version')\n    if (removal_version is not None):\n        validate_removal_semver(removal_version)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.messageBox = MessageBox()\n                self.messageBox.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.displayName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.contacts = []\n                (_etype49, _size46) = iprot.readListBegin()\n                for _i50 in xrange(_size46):\n                    _elem51 = Contact()\n                    _elem51.read(iprot)\n                    self.contacts.append(_elem51)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.mystery = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.statusCode = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.path = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.listing = []\n                (_etype38, _size35) = iprot.readListBegin()\n                for _i39 in xrange(_size35):\n                    _elem40 = iprot.readString()\n                    self.listing.append(_elem40)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef command(self, mark_success=False, ignore_dependencies=False, ignore_depends_on_past=False, force=False, local=False, pickle_id=None, raw=False, job_id=None, pool=None):\n    '\\n        Returns a command that can be executed anywhere where airflow is\\n        installed. This command is part of the message sent to executors by\\n        the orchestrator.\\n        '\n    dag = self.task.dag\n    iso = self.execution_date.isoformat()\n    cmd = 'airflow run {self.dag_id} {self.task_id} {iso} '\n    cmd += ('--mark_success ' if mark_success else '')\n    cmd += ('--pickle {pickle_id} ' if pickle_id else '')\n    cmd += ('--job_id {job_id} ' if job_id else '')\n    cmd += ('-i ' if ignore_dependencies else '')\n    cmd += ('-I ' if ignore_depends_on_past else '')\n    cmd += ('--force ' if force else '')\n    cmd += ('--local ' if local else '')\n    cmd += ('--pool {pool} ' if pool else '')\n    cmd += ('--raw ' if raw else '')\n    if ((not pickle_id) and dag):\n        if (dag.full_filepath != dag.filepath):\n            cmd += '-sd DAGS_FOLDER/{dag.filepath} '\n        elif dag.full_filepath:\n            cmd += '-sd {dag.full_filepath}'\n    return cmd.format(**locals())\n", "label": 1}
{"function": "\n\n@register.inclusion_tag('zinnia/tags/dummy.html', takes_context=True)\ndef get_calendar_entries(context, year=None, month=None, template='zinnia/tags/entries_calendar.html'):\n    '\\n    Return an HTML calendar of entries.\\n    '\n    if (not (year and month)):\n        day_week_month = (context.get('day') or context.get('week') or context.get('month'))\n        publication_date = getattr(context.get('object'), 'publication_date', None)\n        if day_week_month:\n            current_month = day_week_month\n        elif publication_date:\n            if settings.USE_TZ:\n                publication_date = timezone.localtime(publication_date)\n            current_month = publication_date.date()\n        else:\n            today = timezone.now()\n            if settings.USE_TZ:\n                today = timezone.localtime(today)\n            current_month = today.date()\n        current_month = current_month.replace(day=1)\n    else:\n        current_month = date(year, month, 1)\n    dates = list(map((lambda x: ((settings.USE_TZ and timezone.localtime(x).date()) or x.date())), Entry.published.datetimes('publication_date', 'month')))\n    if (current_month not in dates):\n        dates.append(current_month)\n        dates.sort()\n    index = dates.index(current_month)\n    previous_month = (((index > 0) and dates[(index - 1)]) or None)\n    next_month = (((index != (len(dates) - 1)) and dates[(index + 1)]) or None)\n    calendar = Calendar()\n    return {\n        'template': template,\n        'next_month': next_month,\n        'previous_month': previous_month,\n        'calendar': calendar.formatmonth(current_month.year, current_month.month, previous_month=previous_month, next_month=next_month),\n    }\n", "label": 1}
{"function": "\n\ndef doWrite(self):\n    '\\n        Called when data can be written.\\n\\n        A result that is true (which will be a negative number or an\\n        exception instance) indicates that the connection was lost. A false\\n        result implies the connection is still there; a result of 0\\n        indicates no write was done, and a result of None indicates that a\\n        write was done.\\n        '\n    if ((len(self.dataBuffer) - self.offset) < self.SEND_LIMIT):\n        self.dataBuffer = (buffer(self.dataBuffer, self.offset) + ''.join(self._tempDataBuffer))\n        self.offset = 0\n        self._tempDataBuffer = []\n        self._tempDataLen = 0\n    if self.offset:\n        l = self.writeSomeData(buffer(self.dataBuffer, self.offset))\n    else:\n        l = self.writeSomeData(self.dataBuffer)\n    if ((l < 0) or isinstance(l, Exception)):\n        return l\n    if ((l == 0) and self.dataBuffer):\n        result = 0\n    else:\n        result = None\n    self.offset += l\n    if ((self.offset == len(self.dataBuffer)) and (not self._tempDataLen)):\n        self.dataBuffer = ''\n        self.offset = 0\n        self.stopWriting()\n        if ((self.producer is not None) and ((not self.streamingProducer) or self.producerPaused)):\n            self.producerPaused = 0\n            self.producer.resumeProducing()\n        elif self.disconnecting:\n            return self._postLoseConnection()\n        elif self._writeDisconnecting:\n            self._writeDisconnected = True\n            result = self._closeWriteConnection()\n            return result\n    return result\n", "label": 1}
{"function": "\n\ndef save_or_overwrite_slice(self, args, slc, slice_add_perm, slice_edit_perm):\n    'save or overwrite a slice'\n    slice_name = args.get('slice_name')\n    action = args.get('action')\n    d = args.to_dict(flat=False)\n    del d['action']\n    del d['previous_viz_type']\n    as_list = ('metrics', 'groupby', 'columns')\n    for k in d:\n        v = d.get(k)\n        if ((k in as_list) and (not isinstance(v, list))):\n            d[k] = ([v] if v else [])\n        if ((k not in as_list) and isinstance(v, list)):\n            d[k] = v[0]\n    table_id = druid_datasource_id = None\n    datasource_type = args.get('datasource_type')\n    if (datasource_type in ('datasource', 'druid')):\n        druid_datasource_id = args.get('datasource_id')\n    elif (datasource_type == 'table'):\n        table_id = args.get('datasource_id')\n    if (action == 'save'):\n        slc = models.Slice()\n    slc.params = json.dumps(d, indent=4, sort_keys=True)\n    slc.datasource_name = args.get('datasource_name')\n    slc.viz_type = args.get('viz_type')\n    slc.druid_datasource_id = druid_datasource_id\n    slc.table_id = table_id\n    slc.datasource_type = datasource_type\n    slc.slice_name = slice_name\n    if ((action == 'save') and slice_add_perm):\n        self.save_slice(slc)\n    elif ((action == 'overwrite') and slice_edit_perm):\n        self.overwrite_slice(slc)\n    return redirect(slc.slice_url)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_socket_descriptor_ != x.has_socket_descriptor_):\n        return 0\n    if (self.has_socket_descriptor_ and (self.socket_descriptor_ != x.socket_descriptor_)):\n        return 0\n    if (self.has_data_size_ != x.has_data_size_):\n        return 0\n    if (self.has_data_size_ and (self.data_size_ != x.data_size_)):\n        return 0\n    if (self.has_flags_ != x.has_flags_):\n        return 0\n    if (self.has_flags_ and (self.flags_ != x.flags_)):\n        return 0\n    if (self.has_timeout_seconds_ != x.has_timeout_seconds_):\n        return 0\n    if (self.has_timeout_seconds_ and (self.timeout_seconds_ != x.timeout_seconds_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef visit(self, nodes, root=False):\n    buf = []\n    if (not isinstance(nodes, (list, tuple))):\n        nodes = [nodes]\n    if root:\n        method = getattr(self, '__begin__', None)\n        if method:\n            buf.append(method())\n    for node in nodes:\n        if isinstance(node, string_types):\n            buf.append(node)\n        else:\n            if hasattr(self, 'before_visit'):\n                buf.append(self.before_visit(node))\n            method = getattr(self, (('visit_' + node.__name__) + '_begin'), None)\n            if method:\n                buf.append(method(node))\n            method = getattr(self, ('visit_' + node.__name__), None)\n            if method:\n                buf.append(method(node))\n            elif isinstance(node.what, string_types):\n                buf.append(node.what)\n            else:\n                buf.append(self.visit(node.what))\n            method = getattr(self, (('visit_' + node.__name__) + '_end'), None)\n            if method:\n                buf.append(method(node))\n            if hasattr(self, 'after_visit'):\n                buf.append(self.after_visit(node))\n    if root:\n        method = getattr(self, '__end__', None)\n        if method:\n            buf.append(method())\n    return ''.join(buf)\n", "label": 1}
{"function": "\n\ndef sample_geojson(stream, max_features):\n    ' Read a stream of input GeoJSON and return a string with a limited feature count.\\n    '\n    (data, features) = (ijson.parse(stream), list())\n    for (prefix1, event1, value1) in data:\n        if (event1 != 'start_map'):\n            raise ValueError((prefix1, event1, value1))\n        for (prefix2, event2, value2) in data:\n            if ((event2 == 'map_key') and (value2 == 'type')):\n                (prefix3, event3, value3) = next(data)\n                if ((event3 != 'string') and (value3 != 'FeatureCollection')):\n                    raise ValueError((prefix3, event3, value3))\n            elif ((event2 == 'map_key') and (value2 == 'features')):\n                (prefix4, event4, value4) = next(data)\n                if (event4 != 'start_array'):\n                    raise ValueError((prefix4, event4, value4))\n                for (prefix5, event5, value5) in data:\n                    if ((event5 == 'end_array') or (len(features) == max_features)):\n                        break\n                    _data = chain([(prefix5, event5, value5)], data)\n                    features.append(_build_value(_data))\n                geojson = dict(type='FeatureCollection', features=features)\n                return json.dumps(geojson)\n    raise ValueError()\n", "label": 1}
{"function": "\n\ndef write_render_callable(self, node, name, args, buffered, filtered, cached):\n    'write a top-level render callable.\\n\\n        this could be the main render() method or that of a top-level def.'\n    if self.in_def:\n        decorator = node.decorator\n        if decorator:\n            self.printer.writeline(('@runtime._decorate_toplevel(%s)' % decorator))\n    self.printer.writelines(('def %s(%s):' % (name, ','.join(args))), '__M_caller = context.caller_stack._push_frame()', 'try:')\n    if (buffered or filtered or cached):\n        self.printer.writeline('context._push_buffer()')\n    self.identifier_stack.append(self.compiler.identifiers.branch(self.node))\n    if (((not self.in_def) or self.node.is_block) and ('**pageargs' in args)):\n        self.identifier_stack[(- 1)].argument_declared.add('pageargs')\n    if ((not self.in_def) and ((len(self.identifiers.locally_assigned) > 0) or (len(self.identifiers.argument_declared) > 0))):\n        self.printer.writeline(('__M_locals = __M_dict_builtin(%s)' % ','.join([('%s=%s' % (x, x)) for x in self.identifiers.argument_declared])))\n    self.write_variable_declares(self.identifiers, toplevel=True)\n    for n in self.node.nodes:\n        n.accept_visitor(self)\n    self.write_def_finish(self.node, buffered, filtered, cached)\n    self.printer.writeline(None)\n    self.printer.write('\\n\\n')\n    if cached:\n        self.write_cache_decorator(node, name, args, buffered, self.identifiers, toplevel=True)\n", "label": 1}
{"function": "\n\ndef convert_schema_order(objects, root_type):\n    ref_objects = []\n    cur_objects = []\n    for obj in objects:\n        if (obj.getName() == root_type):\n            ref_objects = [obj]\n            cur_objects = [obj]\n            break\n    if (len(ref_objects) < 1):\n        raise ValueError(('Cannot find root %s' % root_type))\n    while (len(cur_objects) > 0):\n        next_objects = []\n        for obj in cur_objects:\n            for prop in obj.getXMLElements():\n                if (prop.isReference() and (prop.getReferencedObject() not in ref_objects) and (prop.getReferencedObject() not in next_objects)):\n                    next_objects.append(prop.getReferencedObject())\n            for choice in obj.getXMLChoices():\n                for prop in choice.getXMLElements():\n                    if (prop.isReference() and (prop.getReferencedObject() not in ref_objects) and (prop.getReferencedObject() not in next_objects)):\n                        next_objects.append(prop.getReferencedObject())\n        ref_objects.extend(next_objects)\n        cur_objects = next_objects\n    return ref_objects\n", "label": 1}
{"function": "\n\ndef _get_footer(self):\n    name = self.series.name\n    footer = u('')\n    if (getattr(self.series.index, 'freq', None) is not None):\n        footer += ('Freq: %s' % self.series.index.freqstr)\n    if ((self.name is not False) and (name is not None)):\n        if footer:\n            footer += ', '\n        series_name = pprint_thing(name, escape_chars=('\\t', '\\r', '\\n'))\n        footer += (('Name: %s' % series_name) if (name is not None) else '')\n    if self.length:\n        if footer:\n            footer += ', '\n        footer += ('Length: %d' % len(self.series))\n    if ((self.dtype is not False) and (self.dtype is not None)):\n        name = getattr(self.tr_series.dtype, 'name', None)\n        if name:\n            if footer:\n                footer += ', '\n            footer += ('dtype: %s' % pprint_thing(name))\n    if com.is_categorical_dtype(self.tr_series.dtype):\n        level_info = self.tr_series._values._repr_categories_info()\n        if footer:\n            footer += '\\n'\n        footer += level_info\n    return compat.text_type(footer)\n", "label": 1}
{"function": "\n\ndef _round_robin(self, rdesc):\n    \" 'time estimate' using round-robin selection. \"\n    for host in self.cluster:\n        if (host.total_cpus <= 0):\n            (count, criteria) = host.allocator.max_servers({\n                \n            })\n            host.total_cpus = count\n    if self._last_deployed:\n        for (i, host) in enumerate(self.cluster):\n            if (host is self._last_deployed):\n                i = ((i + 1) % len(self.cluster))\n                break\n    else:\n        i = 0\n    for j in range(len(self.cluster)):\n        host = self.cluster[i]\n        if (host.allocated_cpus < host.total_cpus):\n            allocation_host = host\n            break\n        i = ((i + 1) % len(self.cluster))\n    else:\n        return ((- 1), {\n            'min_cpus': 'no idle cpus',\n        })\n    hostnames = []\n    min_cpus = rdesc.get('min_cpus', 1)\n    required = min_cpus\n    host_added = True\n    while (host_added and (required > 0)):\n        host_added = False\n        for j in range(len(self.cluster)):\n            if (host.allocated_cpus < host.total_cpus):\n                hostnames.append(host.netname)\n                host_added = True\n                required -= 1\n                if (required <= 0):\n                    break\n            i = ((i + 1) % len(self.cluster))\n            host = self.cluster[i]\n    if (required > 0):\n        return ((- 1), {\n            'min_cpus': ('want %d, idle %d' % (min_cpus, len(hostnames))),\n        })\n    return (0, {\n        'host': allocation_host,\n        'hostnames': hostnames,\n    })\n", "label": 1}
{"function": "\n\ndef _invert_complex(f, g_ys, symbol):\n    'Helper function for _invert.'\n    if (f == symbol):\n        return (f, g_ys)\n    n = Dummy('n')\n    if f.is_Add:\n        (g, h) = f.as_independent(symbol)\n        if (g is not S.Zero):\n            return _invert_complex(h, imageset(Lambda(n, (n - g)), g_ys), symbol)\n    if f.is_Mul:\n        (g, h) = f.as_independent(symbol)\n        if (g is not S.One):\n            return _invert_complex(h, imageset(Lambda(n, (n / g)), g_ys), symbol)\n    if (hasattr(f, 'inverse') and (not isinstance(f, TrigonometricFunction)) and (not isinstance(f, exp))):\n        if (len(f.args) > 1):\n            raise ValueError('Only functions with one argument are supported.')\n        return _invert_complex(f.args[0], imageset(Lambda(n, f.inverse()(n)), g_ys), symbol)\n    if isinstance(f, exp):\n        if isinstance(g_ys, FiniteSet):\n            exp_invs = Union(*[imageset(Lambda(n, ((I * (((2 * n) * pi) + arg(g_y))) + log(Abs(g_y)))), S.Integers) for g_y in g_ys if (g_y != 0)])\n            return _invert_complex(f.args[0], exp_invs, symbol)\n    return (f, g_ys)\n", "label": 1}
{"function": "\n\ndef _on_after_api_deserialize(self, r, root):\n    if (r is not None):\n        self._activity.set_source_id(self._id())\n        if (('SourceDisk' in r) if isinstance(r, dict) else hasattr(r, 'SourceDisk')):\n            s = (r['SourceDisk'] if ('SourceDisk' in r) else None)\n            if (s is not None):\n                id = (s['ID'] if ('ID' in s) else None)\n                if (id is not None):\n                    self._source = Disk(self._client, s)\n        if (('SourceArchive' in r) if isinstance(r, dict) else hasattr(r, 'SourceArchive')):\n            s = (r['SourceArchive'] if ('SourceArchive' in r) else None)\n            if (s is not None):\n                id = (s['ID'] if ('ID' in s) else None)\n                if (id is not None):\n                    self._source = Resource.create_with('Archive', self._client, s)\n", "label": 1}
{"function": "\n\ndef __init__(self, filename=None, **kwargs):\n    '\\n        :param filename: location of package.xml.  Necessary if\\n          converting ``${prefix}`` in ``<export>`` values, ``str``.\\n        '\n    for attr in self.__slots__:\n        if attr.endswith('s'):\n            value = (list(kwargs[attr]) if (attr in kwargs) else [])\n            setattr(self, attr, value)\n        else:\n            value = (kwargs[attr] if (attr in kwargs) else None)\n            setattr(self, attr, value)\n    if ('depends' in kwargs):\n        for d in kwargs['depends']:\n            for slot in [self.build_depends, self.build_export_depends, self.exec_depends]:\n                if (d not in slot):\n                    slot.append(deepcopy(d))\n        del kwargs['depends']\n    if ('run_depends' in kwargs):\n        for d in kwargs['run_depends']:\n            for slot in [self.build_export_depends, self.exec_depends]:\n                if (d not in slot):\n                    slot.append(deepcopy(d))\n        del kwargs['run_depends']\n    self.filename = filename\n    unknown = set(kwargs.keys()).difference(self.__slots__)\n    if unknown:\n        raise TypeError(('Unknown properties: %s' % ', '.join(unknown)))\n", "label": 1}
{"function": "\n\ndef _serialize(self):\n    data = {\n        \n    }\n    if self.interval:\n        data['interval'] = self.interval\n    else:\n        raise RuntimeError('interval required')\n    if self.time_zone:\n        data['time_zone'] = self.time_zone\n    if self.pre_zone:\n        data['pre_zone'] = self.pre_zone\n    if self.post_zone:\n        data['post_zone'] = self.post_zone\n    if self.factor:\n        data['factor'] = self.factor\n    if self.pre_offset:\n        data['pre_offset'] = self.pre_offset\n    if self.post_offset:\n        data['post_offset'] = self.post_offset\n    if (self.min_doc_count is not None):\n        data['min_doc_count'] = self.min_doc_count\n    if (self.extended_bounds is not None):\n        data['extended_bounds'] = self.extended_bounds\n    if self.field:\n        data['field'] = self.field\n    elif self.key_field:\n        data['key_field'] = self.key_field\n        if self.value_field:\n            data['value_field'] = self.value_field\n        elif self.value_script:\n            data['value_script'] = self.value_script\n            if self.params:\n                data['params'] = self.params\n        else:\n            raise RuntimeError('Invalid key_field: value_field or value_script required')\n    return data\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    a = self.msg\n    b = ''\n    if self.http_scheme:\n        b += ('%s://' % self.http_scheme)\n    if self.http_host:\n        b += self.http_host\n    if self.http_port:\n        b += (':%s' % self.http_port)\n    if self.http_path:\n        b += self.http_path\n    if self.http_query:\n        b += ('?%s' % self.http_query)\n    if self.http_status:\n        if b:\n            b = ('%s %s' % (b, self.http_status))\n        else:\n            b = str(self.http_status)\n    if self.http_reason:\n        if b:\n            b = ('%s %s' % (b, self.http_reason))\n        else:\n            b = ('- %s' % self.http_reason)\n    if self.http_device:\n        if b:\n            b = ('%s: device %s' % (b, self.http_device))\n        else:\n            b = ('device %s' % self.http_device)\n    if self.http_response_content:\n        if (len(self.http_response_content) <= 60):\n            b += ('   %s' % self.http_response_content)\n        else:\n            b += ('  [first 60 chars of response] %s' % self.http_response_content[:60])\n    return ((b and ('%s: %s' % (a, b))) or a)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_name_ != x.has_name_):\n        return 0\n    if (self.has_name_ and (self.name_ != x.name_)):\n        return 0\n    if (self.has_deprecated_multi_ != x.has_deprecated_multi_):\n        return 0\n    if (self.has_deprecated_multi_ and (self.deprecated_multi_ != x.deprecated_multi_)):\n        return 0\n    if (len(self.deprecated_value_) != len(x.deprecated_value_)):\n        return 0\n    for (e1, e2) in zip(self.deprecated_value_, x.deprecated_value_):\n        if (e1 != e2):\n            return 0\n    if (self.has_value_ != x.has_value_):\n        return 0\n    if (self.has_value_ and (self.value_ != x.value_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef getFullName(self, item, partial=''):\n    'Return a syntactically proper name for item.'\n    name = self.GetItemText(item)\n    parent = None\n    obj = None\n    if (item != self.root):\n        parent = self.GetItemParent(item)\n        obj = self.GetPyData(parent)\n    if (((type(obj) is types.DictType) or ((str(type(obj))[17:23] == 'BTrees') and hasattr(obj, 'keys'))) and (((item != self.root) and (parent != self.root)) or ((parent == self.root) and (not self.rootIsNamespace)))):\n        name = (('[' + name) + ']')\n    if partial:\n        if (partial[0] == '['):\n            name += partial\n        else:\n            name += ('.' + partial)\n    if (((item != self.root) and (parent != self.root)) or ((parent == self.root) and (not self.rootIsNamespace))):\n        name = self.getFullName(parent, partial=name)\n    return name\n", "label": 1}
{"function": "\n\ndef almostEqual(self, other):\n    'Checks if this effect is almost equal (within float precision)\\n        to the given effect.\\n\\n        :param collada.material.Effect other:\\n          Effect to compare to\\n\\n        :rtype: bool\\n\\n        '\n    if (self.shadingtype != other.shadingtype):\n        return False\n    if (self.double_sided != other.double_sided):\n        return False\n    for prop in self.supported:\n        thisprop = getattr(self, prop)\n        otherprop = getattr(other, prop)\n        if (type(thisprop) != type(otherprop)):\n            return False\n        elif (type(thisprop) is float):\n            if (not falmostEqual(thisprop, otherprop)):\n                return False\n        elif (type(thisprop) is Map):\n            if ((thisprop.sampler.surface.image.id != otherprop.sampler.surface.image.id) or (thisprop.texcoord != otherprop.texcoord)):\n                return False\n        elif (type(thisprop) is tuple):\n            if (len(thisprop) != len(otherprop)):\n                return False\n            for (valthis, valother) in zip(thisprop, otherprop):\n                if (not falmostEqual(valthis, valother)):\n                    return False\n    return True\n", "label": 1}
{"function": "\n\ndef m44is_identity(m):\n    (m0, m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13, m14, m15) = m\n    return ((m0 == 1) and (m1 == 0) and (m2 == 0) and (m3 == 0) and (m4 == 0) and (m5 == 1) and (m6 == 0) and (m7 == 0) and (m8 == 0) and (m9 == 0) and (m10 == 1) and (m11 == 0) and (m12 == 0) and (m13 == 0) and (m14 == 0) and (m15 == 1))\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_topic_start_ != x.has_topic_start_):\n        return 0\n    if (self.has_topic_start_ and (self.topic_start_ != x.topic_start_)):\n        return 0\n    if (self.has_max_results_ != x.has_max_results_):\n        return 0\n    if (self.has_max_results_ and (self.max_results_ != x.max_results_)):\n        return 0\n    if (self.has_max_results_set_ != x.has_max_results_set_):\n        return 0\n    if (self.has_max_results_set_ and (self.max_results_set_ != x.max_results_set_)):\n        return 0\n    if (self.has_app_id_ != x.has_app_id_):\n        return 0\n    if (self.has_app_id_ and (self.app_id_ != x.app_id_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\n@classmethod\ndef split(cls, line):\n    '\\n        Decode an IRC protocol line as UTF-8 and split into tokens as defined\\n        in RFC 1459 and the IRCv3 message-tags extension.\\n        '\n    line = line.decode('utf-8', 'replace')\n    line = line.rstrip('\\r\\n').split(' ')\n    (i, n) = (0, len(line))\n    parv = []\n    while ((i < n) and (line[i] == '')):\n        i += 1\n    if ((i < n) and line[i].startswith('@')):\n        parv.append(line[i])\n        i += 1\n        while ((i < n) and (line[i] == '')):\n            i += 1\n    if ((i < n) and line[i].startswith(':')):\n        parv.append(line[i])\n        i += 1\n        while ((i < n) and (line[i] == '')):\n            i += 1\n    while (i < n):\n        if line[i].startswith(':'):\n            break\n        elif (line[i] != ''):\n            parv.append(line[i])\n        i += 1\n    if (i < n):\n        trailing = ' '.join(line[i:])\n        parv.append(trailing[1:])\n    return parv\n", "label": 1}
{"function": "\n\ndef trim(c, start, end, trim5=False, trim3=False, both=True):\n    (cstart, cend) = (c.start, c.end)\n    if (((trim5 or both) and (c.strand == '+')) or ((trim3 or both) and (c.strand == '-'))):\n        c.start = max(cstart, start)\n    if (((trim3 or both) and (c.strand == '+')) or ((trim5 or both) and (c.strand == '-'))):\n        c.end = min(cend, end)\n    if ((c.start != cstart) or (c.end != cend)):\n        ((print >> sys.stderr), c.id, 'trimmed [{0}, {1}] => [{2}, {3}]'.format(cstart, cend, c.start, c.end))\n    else:\n        ((print >> sys.stderr), c.id, 'no change')\n", "label": 1}
{"function": "\n\ndef update_user(email, profile='splunk', **kwargs):\n    '\\n    Create a splunk user by email\\n\\n    CLI Example:\\n\\n        salt myminion splunk.update_user example@domain.com roles=[\\'user\\'] realname=\"Test User\"\\n    '\n    client = _get_splunk(profile)\n    email = email.lower()\n    user = list_users(profile).get(email)\n    if (not user):\n        log.error('Failed to retrieve user {0}'.format(email))\n        return False\n    property_map = {\n        \n    }\n    for field in ALLOWED_FIELDS_FOR_MODIFICATION:\n        if kwargs.get(field):\n            property_map[field] = kwargs.get(field)\n    kwargs = {\n        \n    }\n    roles = [role.name for role in user.role_entities]\n    for (k, v) in property_map.items():\n        resource_value = user[k]\n        if (resource_value is not None):\n            if (k.lower() == 'name'):\n                continue\n            if (k.lower() == 'roles'):\n                if isinstance(v, str):\n                    v = v.split(',')\n                if (set(roles) != set(v)):\n                    kwargs['roles'] = list(set(v))\n            elif (resource_value != v):\n                kwargs[k] = v\n    if (len(kwargs) > 0):\n        user.update(**kwargs).refresh()\n        fields_modified = {\n            \n        }\n        for field in ALLOWED_FIELDS_FOR_MODIFICATION:\n            fields_modified[field] = user[field]\n    else:\n        return True\n", "label": 1}
{"function": "\n\ndef check_dict_formatting_in_string(logical_line, tokens):\n    'Check that strings do not use dict-formatting with a single replacement\\n\\n    N352\\n    '\n    if ((not logical_line) or logical_line.startswith('#') or logical_line.endswith('# noqa')):\n        return\n    current_string = ''\n    in_string = False\n    for (token_type, text, start, end, line) in tokens:\n        if (token_type == tokenize.STRING):\n            if (not in_string):\n                current_string = ''\n                in_string = True\n            current_string += text.strip('\"')\n        elif (token_type == tokenize.OP):\n            if (not current_string):\n                continue\n            in_string = False\n            if (text == '%'):\n                format_keys = set()\n                for match in re_str_format.finditer(current_string):\n                    format_keys.add(match.group(1))\n                if (len(format_keys) == 1):\n                    (yield (0, 'N353 Do not use mapping key string formatting with a single key'))\n            if (text != ')'):\n                current_string = ''\n        elif (token_type in (tokenize.NL, tokenize.COMMENT)):\n            continue\n        else:\n            in_string = False\n            if (token_type == tokenize.NEWLINE):\n                current_string = ''\n", "label": 1}
{"function": "\n\ndef _post_commit(self, session, response):\n    signals = []\n    exceptions = []\n    models = session.router\n    for result in (response or ()):\n        if isinstance(result, Exception):\n            exceptions.append(result)\n        if (not isinstance(result, session_result)):\n            continue\n        (meta, result) = result\n        if (not result):\n            continue\n        sm = session.model(meta)\n        (saved, deleted, errors) = sm.post_commit(result)\n        exceptions.extend(errors)\n        if deleted:\n            self.deleted[meta] = deleted\n            if self.signal_delete:\n                signals.append((models.post_delete.fire, sm, deleted))\n        if saved:\n            self.saved[meta] = saved\n            if self.signal_commit:\n                signals.append((models.post_commit.fire, sm, saved))\n    for (fire, sm, instances) in signals:\n        for result in fire(sm.model, instances=instances, session=session, transaction=self):\n            (yield result)\n    if exceptions:\n        nf = len(exceptions)\n        if (nf > 1):\n            error = ('There were %s exceptions during commit.\\n\\n' % nf)\n            error += '\\n\\n'.join((str(e) for e in exceptions))\n        else:\n            error = str(exceptions[0])\n        raise CommitException(error, failures=nf)\n", "label": 1}
{"function": "\n\ndef get_sizes(self, total_width, total_height, xoffset=0, yoffset=0):\n    width = 0\n    height = 0\n    results = []\n    (rows, cols, orientation) = self.calc(self.num_windows, total_width, total_height)\n    if (orientation == ROWCOL):\n        y = 0\n        for (i, row) in enumerate(range(rows)):\n            x = 0\n            width = (total_width // cols)\n            for (j, col) in enumerate(range(cols)):\n                height = (total_height // rows)\n                if ((i == (rows - 1)) and (j == 0)):\n                    remaining = (self.num_windows - len(results))\n                    width = (total_width // remaining)\n                elif ((j == (cols - 1)) or ((len(results) + 1) == self.num_windows)):\n                    width = (total_width - x)\n                results.append(((x + xoffset), (y + yoffset), width, height))\n                if (len(results) == self.num_windows):\n                    return results\n                x += width\n            y += height\n    else:\n        x = 0\n        for (i, col) in enumerate(range(cols)):\n            y = 0\n            height = (total_height // rows)\n            for (j, row) in enumerate(range(rows)):\n                width = (total_width // cols)\n                if ((i == (cols - 1)) and (j == 0)):\n                    remaining = (self.num_windows - len(results))\n                    height = (total_height // remaining)\n                elif ((j == (rows - 1)) or ((len(results) + 1) == self.num_windows)):\n                    height = (total_height - y)\n                results.append(((x + xoffset), (y + yoffset), width, height))\n                if (len(results) == self.num_windows):\n                    return results\n                y += height\n            x += width\n    return results\n", "label": 1}
{"function": "\n\ndef parse(self):\n    (self.encoding, self.text) = self.decode_raw_stream(self.text, (not self.disable_unicode), self.encoding, self.filename)\n    for preproc in self.preprocessor:\n        self.text = preproc(self.text)\n    self.match_reg(self._coding_re)\n    self.textlength = len(self.text)\n    while True:\n        if (self.match_position > self.textlength):\n            break\n        if self.match_end():\n            break\n        if self.match_expression():\n            continue\n        if self.match_control_line():\n            continue\n        if self.match_comment():\n            continue\n        if self.match_tag_start():\n            continue\n        if self.match_tag_end():\n            continue\n        if self.match_python_block():\n            continue\n        if self.match_text():\n            continue\n        if (self.match_position > self.textlength):\n            break\n        raise exceptions.CompileException('assertion failed')\n    if len(self.tag):\n        raise exceptions.SyntaxException(('Unclosed tag: <%%%s>' % self.tag[(- 1)].keyword), **self.exception_kwargs)\n    if len(self.control_line):\n        raise exceptions.SyntaxException((\"Unterminated control keyword: '%s'\" % self.control_line[(- 1)].keyword), self.text, self.control_line[(- 1)].lineno, self.control_line[(- 1)].pos, self.filename)\n    return self.template\n", "label": 1}
{"function": "\n\ndef compare(reference, simulated, display_limit=(- 1)):\n    '\\n    Compare two data files for equivalence.\\n    '\n    time = reference['time']\n    steps = len(time)\n    err = False\n    displayed = 0\n    for i in range(steps):\n        for (n, series) in list(reference.items()):\n            if (n not in simulated):\n                if (n in IGNORABLE_COLS):\n                    continue\n                if ((display_limit >= 0) and (displayed < display_limit)):\n                    log(ERROR, 'missing column %s in second file', n)\n                    displayed += 1\n                break\n            if (len(reference[n]) != len(simulated[n])):\n                if ((display_limit >= 0) and (displayed < display_limit)):\n                    log(ERROR, 'len mismatch for %s (%d vs %d)', n, len(reference[n]), len(simulated[n]))\n                    displayed += 1\n                err = True\n                break\n            ref = float(series[i])\n            sim = float(simulated[n][i])\n            around_zero = (isclose(ref, 0, abs_tol=1e-06) and isclose(sim, 0, abs_tol=1e-06))\n            if ((not around_zero) and (not isclose(ref, sim, rel_tol=0.0001))):\n                if ((display_limit >= 0) and (displayed < display_limit)):\n                    log(ERROR, 'time %s mismatch in %s (%s != %s)', time[i], n, ref, sim)\n                    displayed += 1\n                err = True\n    return err\n", "label": 1}
{"function": "\n\ndef sEqual(dts1, elt1, elt2, equalMode=S_EQUAL, excludeIDs=NO_IDs_EXCLUDED, dts2=None, ns2ns1Tbl=None):\n    if (dts2 is None):\n        dts2 = dts1\n    if (elt1.localName != elt2.localName):\n        return False\n    if (ns2ns1Tbl and (elt2.namespaceURI in ns2ns1Tbl)):\n        if (elt1.namespaceURI != ns2ns1Tbl[elt2.namespaceURI]):\n            return False\n    elif (elt1.namespaceURI != elt2.namespaceURI):\n        return False\n    if (not hasattr(elt1, 'xValid')):\n        xmlValidate(dts1, elt1)\n    if (not hasattr(elt2, 'xValid')):\n        xmlValidate(dts2, elt2)\n    children1 = childElements(elt1)\n    children2 = childElements(elt2)\n    if (len(children1) != len(children2)):\n        return False\n    if ((not xEqual(elt1, elt2, equalMode)) or (attributeDict(dts1, elt1, (), equalMode, excludeIDs) != attributeDict(dts2, elt2, (), equalMode, excludeIDs, ns2ns1Tbl))):\n        return False\n    excludeChildIDs = (excludeIDs if (excludeIDs != TOP_IDs_EXCLUDED) else NO_IDs_EXCLUDED)\n    for i in range(len(children1)):\n        if (not sEqual(dts1, children1[i], children2[i], equalMode, excludeChildIDs, dts2, ns2ns1Tbl)):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef run(command, **kwargs):\n    '\\n    :type command: str\\n    :type capture_output: bool\\n    :type use_shell: bool\\n    :type print_command: bool\\n    :type should_return_returncode: bool\\n    :type should_raise_when_fail: bool\\n    :type should_process_output: bool\\n    :rtype: (str, str, int) | (str, str)\\n    '\n    for (key, value) in six.iteritems(_run_global_settings):\n        kwargs.setdefault(key, value)\n    capture_output = kwargs['capture_output']\n    use_shell = kwargs['use_shell']\n    print_command = kwargs['print_command']\n    should_return_returncode = kwargs['should_return_returncode']\n    should_raise_when_fail = kwargs['should_raise_when_fail']\n    should_process_output = kwargs['should_process_output']\n    use_shell = (('&&' in command) or ('||' in command) or ('|' in command) or use_shell)\n    if print_command:\n        print(command)\n    popen = subprocess.Popen((command if use_shell else shlex.split(command)), stdout=(subprocess.PIPE if capture_output else None), stderr=(subprocess.PIPE if capture_output else None), shell=use_shell)\n    (stdout, stderr) = popen.communicate()\n    return_code = popen.returncode\n    if ((return_code != 0) and should_raise_when_fail):\n        raise RunCommandError('Command execution returns {}'.format(return_code))\n    if capture_output:\n        if should_process_output:\n            stdout = _process_run_command_output(stdout)\n            stderr = _process_run_command_output(stderr)\n        if should_return_returncode:\n            return (stdout, stderr, return_code)\n        else:\n            return (stdout, stderr)\n    elif should_return_returncode:\n        return return_code\n", "label": 1}
{"function": "\n\ndef get_view_index(self, key):\n    \"\\n        Returns the processed view's group and index\\n\\n        @param key: a key to get value\\n        \"\n    group_settings = self.get(key)\n    view_group = (- 1)\n    view_index = (- 1)\n    window = sublime.active_window()\n    view = window.active_view()\n    (current_group, current_index) = window.get_view_index(view)\n    for group_setting in group_settings:\n        if isinstance(group_setting, int):\n            view_group = group_setting\n            view_index = len(window.views_in_group(view_group))\n        elif ((isinstance(group_setting, list) or isinstance(group_setting, tuple)) and group_setting):\n            view_group = group_setting[0]\n            view_index = len(window.views_in_group(view_group))\n            if (len(group_setting) > 1):\n                view_index = group_setting[1]\n        if ((view_group >= window.num_groups()) or (view_group < 0)):\n            view_index = (- 1)\n            continue\n        elif ((view_index > len(window.views_in_group(view_group))) or (view_index < 0)):\n            continue\n        break\n    if ((view_group >= window.num_groups()) or (view_group < 0)):\n        view_group = current_group\n        view_index = len(window.views_in_group(view_group))\n    elif ((view_index >= len(window.views_in_group(view_group))) or (view_index < 0)):\n        view_index = len(window.views_in_group(view_group))\n    return (view_group, view_index)\n", "label": 1}
{"function": "\n\ndef byte_count(byte_count):\n    if isinstance(byte_count, int):\n        return positive_nonzero_integer(byte_count)\n    byte_count = unicode_str(byte_count)\n\n    def _get_byte_count(postfix, base, exponant):\n        char_num = len(postfix)\n        if (byte_count[(- char_num):] == postfix):\n            count = decimal.Decimal(byte_count[:(- char_num)])\n            return positive_nonzero_integer((count * (base ** exponant)))\n        return None\n    if (len(byte_count) > 1):\n        n = None\n        n = (n if (n is not None) else _get_byte_count('K', 1024, 1))\n        n = (n if (n is not None) else _get_byte_count('M', 1024, 2))\n        n = (n if (n is not None) else _get_byte_count('G', 1024, 3))\n        n = (n if (n is not None) else _get_byte_count('T', 1024, 4))\n        n = (n if (n is not None) else _get_byte_count('P', 1024, 5))\n        if (n is not None):\n            return n\n    if (len(byte_count) > 2):\n        n = None\n        n = (n if (n is not None) else _get_byte_count('KB', 1000, 1))\n        n = (n if (n is not None) else _get_byte_count('MB', 1000, 2))\n        n = (n if (n is not None) else _get_byte_count('GB', 1000, 3))\n        n = (n if (n is not None) else _get_byte_count('TB', 1000, 4))\n        n = (n if (n is not None) else _get_byte_count('PB', 1000, 5))\n        if (n is not None):\n            return n\n    return positive_nonzero_integer(byte_count)\n", "label": 1}
{"function": "\n\ndef models_from_model(model, include_related=False, exclude=None):\n    'Generator of all model in model.'\n    if (exclude is None):\n        exclude = set()\n    if (model and (model not in exclude)):\n        exclude.add(model)\n        if (isinstance(model, ModelType) and (not model._meta.abstract)):\n            (yield model)\n            if include_related:\n                exclude.add(model)\n                for field in model._meta.fields:\n                    if hasattr(field, 'relmodel'):\n                        through = getattr(field, 'through', None)\n                        for rmodel in (field.relmodel, field.model, through):\n                            for m in models_from_model(rmodel, include_related=include_related, exclude=exclude):\n                                (yield m)\n                for manytomany in model._meta.manytomany:\n                    related = getattr(model, manytomany)\n                    for m in models_from_model(related.model, include_related=include_related, exclude=exclude):\n                        (yield m)\n        elif ((not isinstance(model, ModelType)) and isclass(model)):\n            (yield model)\n", "label": 1}
{"function": "\n\ndef _reduce_map(self, dimensions, function, reduce_map):\n    if (dimensions and reduce_map):\n        raise Exception('Pass reduced dimensions either as an argument or as part of the kwargs not both.')\n    if (len(set(reduce_map.values())) > 1):\n        raise Exception('Cannot define reduce operations with more than one function at a time.')\n    sanitized_dict = {dimension_sanitizer(kd): kd for kd in self.dimensions('key', True)}\n    if reduce_map:\n        reduce_map = reduce_map.items()\n    if dimensions:\n        reduce_map = [(d, function) for d in dimensions]\n    elif (not reduce_map):\n        reduce_map = [(d, function) for d in self.kdims]\n    reduced = [((d.name if isinstance(d, Dimension) else d), fn) for (d, fn) in reduce_map]\n    sanitized = [(sanitized_dict.get(d, d), fn) for (d, fn) in reduced]\n    grouped = [(fn, [dim for (dim, _) in grp]) for (fn, grp) in groupby(sanitized, (lambda x: x[1]))]\n    return grouped[0]\n", "label": 1}
{"function": "\n\ndef _MarkLinesToFormat(uwlines, lines):\n    \"Skip sections of code that we shouldn't reformat.\"\n    if lines:\n        for uwline in uwlines:\n            uwline.disable = True\n        for (start, end) in sorted(lines):\n            for uwline in uwlines:\n                if (uwline.lineno > end):\n                    break\n                if (uwline.lineno >= start):\n                    uwline.disable = False\n                elif (uwline.last.lineno >= start):\n                    uwline.disable = False\n    index = 0\n    while (index < len(uwlines)):\n        uwline = uwlines[index]\n        if uwline.is_comment:\n            if _DisableYAPF(uwline.first.value.strip()):\n                while (index < len(uwlines)):\n                    uwline = uwlines[index]\n                    uwline.disable = True\n                    if (uwline.is_comment and _EnableYAPF(uwline.first.value.strip())):\n                        break\n                    index += 1\n        elif re.search(DISABLE_PATTERN, uwline.last.value.strip(), re.IGNORECASE):\n            uwline.disable = True\n        index += 1\n", "label": 1}
{"function": "\n\ndef tokens(self, event, next):\n    (kind, data, pos) = event\n    if (kind == START):\n        (tag, attribs) = data\n        name = tag.localname\n        namespace = tag.namespace\n        converted_attribs = {\n            \n        }\n        for (k, v) in attribs:\n            if isinstance(k, QName):\n                converted_attribs[(k.namespace, k.localname)] = v\n            else:\n                converted_attribs[(None, k)] = v\n        if ((namespace == namespaces['html']) and (name in voidElements)):\n            for token in self.emptyTag(namespace, name, converted_attribs, ((not next) or (next[0] != END) or (next[1] != tag))):\n                (yield token)\n        else:\n            (yield self.startTag(namespace, name, converted_attribs))\n    elif (kind == END):\n        name = data.localname\n        namespace = data.namespace\n        if (name not in voidElements):\n            (yield self.endTag(namespace, name))\n    elif (kind == COMMENT):\n        (yield self.comment(data))\n    elif (kind == TEXT):\n        for token in self.text(data):\n            (yield token)\n    elif (kind == DOCTYPE):\n        (yield self.doctype(*data))\n    elif (kind in (XML_NAMESPACE, DOCTYPE, START_NS, END_NS, START_CDATA, END_CDATA, PI)):\n        pass\n    else:\n        (yield self.unknown(kind))\n", "label": 1}
{"function": "\n\ndef merge(self):\n    trans = {\n        \n    }\n    for w in self.words:\n        trans[w] = ''\n    for w1 in self.words:\n        cw = 0\n        lw = len(w1)\n        for i in range(((len(self.doc) - lw) + 1)):\n            if (w1 == self.doc[i:(i + lw)]):\n                cw += 1\n        for w2 in self.words:\n            cnt = 0\n            l2 = (len(w1) + len(w2))\n            for i in range(((len(self.doc) - l2) + 1)):\n                if ((w1 + w2) == self.doc[i:(i + l2)]):\n                    cnt += 1\n            if (cw < (cnt * 2)):\n                trans[w1] = w2\n                break\n    ret = []\n    for w in self.words:\n        if (w not in trans):\n            continue\n        s = ''\n        now = trans[w]\n        while now:\n            s += now\n            if (now not in trans):\n                break\n            tmp = trans[now]\n            del trans[now]\n            now = tmp\n        trans[w] = s\n    for w in self.words:\n        if (w in trans):\n            ret.append((w + trans[w]))\n    return ret\n", "label": 1}
{"function": "\n\ndef softmax_simplifier(numerators, denominators):\n    for numerator in list(numerators):\n        if (not numerator.type.dtype.startswith('float')):\n            continue\n        if (numerator.ndim != 2):\n            continue\n        if (numerator.owner and (numerator.owner.op == tensor.exp)):\n            x = numerator.owner.inputs[0]\n        else:\n            continue\n        matching_denom = None\n        for denominator in denominators:\n            if (denominator.owner and isinstance(denominator.owner.op, tensor.DimShuffle)):\n                if (denominator.owner.op.new_order == (0, 'x')):\n                    z = denominator.owner.inputs[0]\n                    if (z.owner and isinstance(z.owner.op, tensor.Sum)):\n                        if (z.owner.op.axis == (1,)):\n                            if (z.owner.inputs[0] is numerator):\n                                matching_denom = denominator\n                                break\n        if matching_denom:\n            numerators.remove(numerator)\n            denominators.remove(matching_denom)\n            numerators.append(softmax_op(x))\n    return (numerators, denominators)\n", "label": 1}
{"function": "\n\ndef _getAndCheckGeneticProfiles(self, genetic_profile_id=None, study=None):\n    study_id = self._getStudyId(study, None)\n    if (not genetic_profile_id):\n        if (not study_id):\n            raise ValueError('Either genetic_profile_id or study must be specified')\n        if (study_id == self.study):\n            genetic_profile_id = [x['genetic_profile_id'] for x in self.profiles if (x['show_profile_in_analysis_tab'] == 'true')]\n        else:\n            genetic_profile_id = [x['genetic_profile_id'] for x in self.getGeneticProfiles(study_id) if (x['show_profile_in_analysis_tab'] == 'true')]\n        return genetic_profile_id\n    else:\n        if (not study_id):\n            return genetic_profile_id\n        if (study_id == self.study):\n            genetic_profile_id = [x['genetic_profile_id'] for x in self.profiles if (x['genetic_profile_id'] in genetic_profile_id)]\n        else:\n            genetic_profile_id = [x['genetic_profile_id'] for x in self.getGeneticProfiles(study_id) if (x['genetic_profile_id'] in genetic_profile_id)]\n        if (len(genetic_profile_id) == 0):\n            raise ValueError('no valid genetic_profile_ids found')\n        return genetic_profile_id\n", "label": 1}
{"function": "\n\ndef tokensMatch(expectedTokens, receivedTokens, ignoreErrorOrder, ignoreErrors=False):\n    \"Test whether the test has passed or failed\\n\\n    If the ignoreErrorOrder flag is set to true we don't test the relative\\n    positions of parse errors and non parse errors\\n    \"\n    checkSelfClosing = False\n    for token in expectedTokens:\n        if (((token[0] == 'StartTag') and (len(token) == 4)) or ((token[0] == 'EndTag') and (len(token) == 3))):\n            checkSelfClosing = True\n            break\n    if (not checkSelfClosing):\n        for token in receivedTokens:\n            if ((token[0] == 'StartTag') or (token[0] == 'EndTag')):\n                token.pop()\n    if ((not ignoreErrorOrder) and (not ignoreErrors)):\n        return (expectedTokens == receivedTokens)\n    else:\n        tokens = {\n            'expected': [[], []],\n            'received': [[], []],\n        }\n        for (tokenType, tokenList) in zip(tokens.keys(), (expectedTokens, receivedTokens)):\n            for token in tokenList:\n                if (token != 'ParseError'):\n                    tokens[tokenType][0].append(token)\n                elif (not ignoreErrors):\n                    tokens[tokenType][1].append(token)\n        return (tokens['expected'] == tokens['received'])\n", "label": 1}
{"function": "\n\ndef generate_key(func_name, args, dict_args_original, skip_args):\n    args_concat = [v for (key, v) in sorted(dict_args_original.iteritems()) if (key not in skip_args)]\n    args_serialized = '_'.join(sorted([(v.__name__ if hasattr(v, '__call__') else (str(v) if (len(str(v)) < 200) else hashlib.md5(str(v)).hexdigest())) for v in args_concat if (hasattr(v, '__call__') or (str(v).find('0x') == (- 1)))]))\n    logger.info(('Serialized args to ' + args_serialized))\n    key = ((func_name + '_') + ''.join((a for a in args_serialized if (a.isalnum() or (a in '!@#$%^&**_+-')))))\n    full_key = ((func_name + '(') + ''.join([((str(k) + '=') + (str(v) if (len(str(v)) < 200) else hashlib.md5(str(v)).hexdigest())) for (k, v) in sorted(dict_args_original.iteritems()) if (key not in skip_args)]))\n    if (len(key) > 400):\n        key = key[0:400]\n    return (key, full_key)\n", "label": 1}
{"function": "\n\ndef AdjustLabels(self, axis, minimum_label_spacing):\n    if (minimum_label_spacing is None):\n        return\n    if (len(axis.labels) <= 1):\n        return\n    if ((axis.max is not None) and (axis.min is not None)):\n        maximum_possible_spacing = ((axis.max - axis.min) / (len(axis.labels) - 1))\n        if (minimum_label_spacing > maximum_possible_spacing):\n            minimum_label_spacing = maximum_possible_spacing\n    labels = [list(x) for x in zip(axis.label_positions, axis.labels)]\n    labels = sorted(labels, reverse=True)\n    for i in range(1, len(labels)):\n        if ((labels[(i - 1)][0] - labels[i][0]) < minimum_label_spacing):\n            new_position = (labels[(i - 1)][0] - minimum_label_spacing)\n            if ((axis.min is not None) and (new_position < axis.min)):\n                new_position = axis.min\n            labels[i][0] = new_position\n    for i in range((len(labels) - 2), (- 1), (- 1)):\n        if ((labels[i][0] - labels[(i + 1)][0]) < minimum_label_spacing):\n            new_position = (labels[(i + 1)][0] + minimum_label_spacing)\n            if ((axis.max is not None) and (new_position > axis.max)):\n                new_position = axis.max\n            labels[i][0] = new_position\n    (label_positions, labels) = zip(*labels)\n    axis.labels = labels\n    axis.label_positions = label_positions\n", "label": 1}
{"function": "\n\ndef _handlewebError(self, msg):\n    print('')\n    print(('    ERROR: %s' % msg))\n    if (not self.interactive):\n        raise self.failureException(msg)\n    p = '    Show: [B]ody [H]eaders [S]tatus [U]RL; [I]gnore, [R]aise, or sys.e[X]it >> '\n    sys.stdout.write(p)\n    sys.stdout.flush()\n    while True:\n        i = getchar().upper()\n        if (not isinstance(i, type(''))):\n            i = i.decode('ascii')\n        if (i not in 'BHSUIRX'):\n            continue\n        print(i.upper())\n        if (i == 'B'):\n            for (x, line) in enumerate(self.body.splitlines()):\n                if (((x + 1) % self.console_height) == 0):\n                    sys.stdout.write('<-- More -->\\r')\n                    m = getchar().lower()\n                    sys.stdout.write('            \\r')\n                    if (m == 'q'):\n                        break\n                print(line)\n        elif (i == 'H'):\n            pprint.pprint(self.headers)\n        elif (i == 'S'):\n            print(self.status)\n        elif (i == 'U'):\n            print(self.url)\n        elif (i == 'I'):\n            return\n        elif (i == 'R'):\n            raise self.failureException(msg)\n        elif (i == 'X'):\n            self.exit()\n        sys.stdout.write(p)\n        sys.stdout.flush()\n", "label": 1}
{"function": "\n\ndef fix_unicode(self, token):\n    if (token.type == 'identifier'):\n        if ((token.text == 'chr') and (token.next_char == '(')):\n            token.fix = 'unichr'\n        elif ((token.text == 'str') and (token.next_char == '(')):\n            token.fix = 'unicode'\n        elif ((token.text == 'str') and ((token.next_char == ')') and (token.prev_char == '(') and (token.line_tokens[0].text == 'class'))):\n            token.fix = 'unicode'\n        elif ((token.text == 'isinstance') and (token.next_char == '(')):\n            end = token.find_forward(')')\n            t = token.next_token\n            while (t.next_token and (t.next_token.start < end)):\n                t = t.next_token\n                if (t.text == 'str'):\n                    t.fix = 'basestring'\n", "label": 1}
{"function": "\n\n@staticmethod\ndef layout(item):\n    ' Custom Layout Method '\n    if (not item.authorized):\n        enabled = False\n        visible = False\n    elif ((item.enabled is None) or item.enabled):\n        enabled = True\n        visible = True\n    if (enabled and visible):\n        if (item.parent is not None):\n            if (item.enabled and item.authorized):\n                if item.components:\n                    _class = ''\n                    if ((item.parent.parent is None) and item.selected):\n                        _class = 'highlight'\n                    items = item.render_components()\n                    if items:\n                        items = LI(UL(items, _class='menu-extention'))\n                    return [LI(A(item.label, _href=item.url(), _id=item.attr._id, _class=_class)), items]\n                else:\n                    if (item.parent.parent is None):\n                        _class = ((item.selected and 'highlight') or '')\n                    else:\n                        _class = ' '\n                    return LI(A(item.label, _href=item.url(), _id=item.attr._id, _class=_class))\n        else:\n            items = item.render_components()\n            return UL(items, _id='main-sub-menu', _class='sub-menu')\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef _remove_nop_artifact(self, offset):\n    for (src, dst) in self.computed_jump:\n        if ((src < offset < dst) or (dst < offset < src)):\n            old_jmp = self.instrs[src]\n            old_jump_size = len(old_jmp.get_code())\n            if (src < offset < dst):\n                new_jmp = type(old_jmp)(((dst - src) - 1))\n            else:\n                new_jmp = type(old_jmp)(((dst - src) + 1))\n            new_jmp_size = len(new_jmp.get_code())\n            if (new_jmp_size > old_jump_size):\n                raise ValueError('Wtf jump of smaller size is bigger.. ABORT')\n            self.instrs[src] = new_jmp\n            for i in range((old_jump_size - new_jmp_size)):\n                self.instrs[((src + new_jmp_size) + i)] = _NopArtifact()\n    for (name, labeloffset) in self.labels.items():\n        if (labeloffset > offset):\n            self.labels[name] = (labeloffset - 1)\n    new_instr = {\n        \n    }\n    for (instroffset, instr) in self.instrs.items():\n        if (instroffset == offset):\n            continue\n        if (instroffset > offset):\n            instroffset -= 1\n        new_instr[instroffset] = instr\n    self.instrs = new_instr\n    new_computed_jump = []\n    for (src, dst) in self.computed_jump:\n        if (src > offset):\n            src -= 1\n        if (dst > offset):\n            dst -= 1\n        new_computed_jump.append((src, dst))\n    self.computed_jump = new_computed_jump\n    self.size -= 1\n", "label": 1}
{"function": "\n\ndef colored(string, fg=None, bg=None, bold_for_light_color=True):\n    result = ''\n    for (colorstring, is_bg) in ((fg, False), (bg, True)):\n        if colorstring:\n            color = '\\x1b['\n            if (colorstring in COLORS):\n                if (not is_bg):\n                    c = (30 + COLORS[colorstring].index)\n                    if COLORS[colorstring].light:\n                        if bold_for_light_color:\n                            color += '1;'\n                        else:\n                            c += 60\n                else:\n                    c = (40 + COLORS[colorstring].index)\n                    if COLORS[colorstring].light:\n                        if (not bold_for_light_color):\n                            c += 60\n                color += str(c)\n            elif colorstring.isdigit():\n                if (not is_bg):\n                    color += ('38;5;' + colorstring)\n                else:\n                    color += ('48;5;' + colorstring)\n            else:\n                if (len(colorstring) == 4):\n                    r = int((colorstring[1] * 2), 16)\n                    g = int((colorstring[2] * 2), 16)\n                    b = int((colorstring[3] * 2), 16)\n                else:\n                    r = int(colorstring[1:3], 16)\n                    g = int(colorstring[3:5], 16)\n                    b = int(colorstring[5:7], 16)\n                if (not is_bg):\n                    color += '38;2;{!s};{!s};{!s}'.format(r, g, b)\n                else:\n                    color += '48;2;{!s};{!s};{!s}'.format(r, g, b)\n            color += 'm'\n            result += color\n    result += string\n    if (fg or bg):\n        result += RESET\n    return result\n", "label": 1}
{"function": "\n\ndef _update_entry(entry, status, directives):\n    \"Update an entry's attributes using the provided directives\\n\\n    :param entry:\\n        A dict mapping each attribute name to a set of its values\\n    :param status:\\n        A dict holding cross-invocation status (whether delete_others\\n        is True or not, and the set of mentioned attributes)\\n    :param directives:\\n        A dict mapping directive types to directive-specific state\\n    \"\n    for (directive, state) in six.iteritems(directives):\n        if (directive == 'delete_others'):\n            status['delete_others'] = state\n            continue\n        for (attr, vals) in six.iteritems(state):\n            status['mentioned_attributes'].add(attr)\n            vals = _toset(vals)\n            if (directive == 'default'):\n                if (len(vals) and ((attr not in entry) or (not len(entry[attr])))):\n                    entry[attr] = vals\n            elif (directive == 'add'):\n                vals.update(entry.get(attr, ()))\n                if len(vals):\n                    entry[attr] = vals\n            elif (directive == 'delete'):\n                existing_vals = entry.pop(attr, set())\n                if len(vals):\n                    existing_vals -= vals\n                    if len(existing_vals):\n                        entry[attr] = existing_vals\n            elif (directive == 'replace'):\n                entry.pop(attr, None)\n                if len(vals):\n                    entry[attr] = vals\n            else:\n                raise ValueError(('unknown directive: ' + directive))\n", "label": 1}
{"function": "\n\ndef _ProcessNumberFacet(self, facet):\n    'Aggregate a number facet values for manual or auto-discovery facets.'\n    facet_value = float(facet.value().string_value())\n    if (facet.name() in self._manual_facet_map):\n        manual_facet_req = self._manual_facet_map[facet.name()]\n        facet_obj = self._manual_facets[facet.name()]\n        if manual_facet_req.range_list():\n            for range_request in manual_facet_req.range_list():\n                range_pair = ((float(range_request.start()) if range_request.has_start() else None), (float(range_request.end()) if range_request.has_end() else None))\n                if (((range_pair[0] is None) or (facet_value >= range_pair[0])) and ((range_pair[1] is None) or (facet_value < range_pair[1]))):\n                    facet_obj.AddValue(self._GetFacetLabel(range_request), refinement=range_pair)\n        elif manual_facet_req.value_constraint_list():\n            for constraint in manual_facet_req.value_constraint_list():\n                if (facet_value == float(constraint)):\n                    facet_obj.AddValue(constraint)\n        else:\n            facet_obj.AddNumericValue(facet_value)\n    elif self._params.auto_discover_facet_count():\n        if (facet.name() in self._discovered_facets):\n            facet_obj = self._discovered_facets[facet.name()]\n        else:\n            facet_obj = self._discovered_facets[facet.name()] = _Facet(facet.name(), self._params.facet_auto_detect_param().value_limit())\n        facet_obj.AddNumericValue(facet_value)\n", "label": 1}
{"function": "\n\ndef validate_dict(h, *args):\n    for arg in args:\n        if ((type(h).__name__ == 'dict') and (type(arg).__name__ == 'list')):\n            for key in arg:\n                if ((key not in h) or (h[key] is None)):\n                    return False\n        elif ((type(h).__name__ == 'dict') and (type(arg).__name__ == 'dict')):\n            for k in arg:\n                if ((k not in h) or (h[k] is None)):\n                    return False\n                v = arg[k]\n                val = h[k]\n                ans = validate_dict(val, v)\n                if (ans == False):\n                    return False\n        elif (type(arg).__name__ == 'str'):\n            if (arg != h):\n                return False\n        else:\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef _getitem_lowerdim(self, tup):\n    if (self.axis is not None):\n        axis = self.obj._get_axis_number(self.axis)\n        return self._getitem_axis(tup, axis=axis)\n    if self._is_nested_tuple_indexer(tup):\n        return self._getitem_nested_tuple(tup)\n    ax0 = self.obj._get_axis(0)\n    if isinstance(ax0, MultiIndex):\n        result = self._handle_lowerdim_multi_index_axis0(tup)\n        if (result is not None):\n            return result\n    if (len(tup) > self.obj.ndim):\n        raise IndexingError('Too many indexers. handle elsewhere')\n    for (i, key) in enumerate(tup):\n        if (is_label_like(key) or isinstance(key, tuple)):\n            section = self._getitem_axis(key, axis=i)\n            if (not is_list_like_indexer(section)):\n                return section\n            elif (section.ndim == self.ndim):\n                new_key = ((tup[:i] + (_NS,)) + tup[(i + 1):])\n            else:\n                new_key = (tup[:i] + tup[(i + 1):])\n                if (isinstance(section, ABCDataFrame) and (i > 0) and (len(new_key) == 2)):\n                    (a, b) = new_key\n                    new_key = (b, a)\n                if (len(new_key) == 1):\n                    (new_key,) = new_key\n            return getattr(section, self.name)[new_key]\n    raise IndexingError('not applicable')\n", "label": 1}
{"function": "\n\ndef remove_dot_segments(path):\n    r = []\n    while path:\n        if path.startswith('../'):\n            path = path[3:]\n            continue\n        if path.startswith('./'):\n            path = path[2:]\n            continue\n        if path.startswith('/./'):\n            path = path[2:]\n            continue\n        if (path == '/.'):\n            path = '/'\n            continue\n        if path.startswith('/../'):\n            path = path[3:]\n            if r:\n                r.pop()\n            continue\n        if (path == '/..'):\n            path = '/'\n            if r:\n                r.pop()\n            continue\n        if (path == '.'):\n            path = path[1:]\n            continue\n        if (path == '..'):\n            path = path[2:]\n            continue\n        start = 0\n        if path.startswith('/'):\n            start = 1\n        ii = path.find('/', start)\n        if (ii < 0):\n            ii = None\n        r.append(path[:ii])\n        if (ii is None):\n            break\n        path = path[ii:]\n    return ''.join(r)\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    _get_fds_service_config(args)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.fds_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                stop_job(args, hosts[host_id].ip, job_name, instance_id)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.fds_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                deploy_utils.wait_for_job_stopping('fds', args.fds_config.cluster.name, job_name, hosts[host_id].ip, instance_id)\n                start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)\n", "label": 1}
{"function": "\n\ndef should_be_compact_paragraph(self, node):\n    '\\n        Determine if the <p> tags around paragraph ``node`` can be omitted.\\n        '\n    if (isinstance(node.parent, nodes.document) or isinstance(node.parent, nodes.compound)):\n        return False\n    for (key, value) in node.attlist():\n        if (node.is_not_default(key) and (not ((key == 'classes') and (value in ([], ['first'], ['last'], ['first', 'last']))))):\n            return False\n    first = isinstance(node.parent[0], nodes.label)\n    for child in node.parent.children[first:]:\n        if isinstance(child, nodes.Invisible):\n            continue\n        if (child is node):\n            break\n        return False\n    parent_length = len([n for n in node.parent if (not isinstance(n, (nodes.Invisible, nodes.label)))])\n    if (self.compact_simple or self.compact_field_list or (self.compact_p and (parent_length == 1))):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if ((self.email_box_type == 'imap') and (not self.email_box_imap_folder)):\n        self.email_box_imap_folder = 'INBOX'\n    if self.socks_proxy_type:\n        if (not self.socks_proxy_host):\n            self.socks_proxy_host = '127.0.0.1'\n        if (not self.socks_proxy_port):\n            self.socks_proxy_port = 9150\n    else:\n        self.socks_proxy_host = None\n        self.socks_proxy_port = None\n    if (not self.email_box_port):\n        if ((self.email_box_type == 'imap') and self.email_box_ssl):\n            self.email_box_port = 993\n        elif ((self.email_box_type == 'imap') and (not self.email_box_ssl)):\n            self.email_box_port = 143\n        elif ((self.email_box_type == 'pop3') and self.email_box_ssl):\n            self.email_box_port = 995\n        elif ((self.email_box_type == 'pop3') and (not self.email_box_ssl)):\n            self.email_box_port = 110\n    if (not self.id):\n        basename = self.prepare_permission_name()\n        Permission.objects.create(name=(_('Permission for queue: ') + self.title), content_type=ContentType.objects.get(model='queue'), codename=basename)\n    super(Queue, self).save(*args, **kwargs)\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, x, y):\n    I = S.Infinity\n    N = S.NegativeInfinity\n    O = S.Zero\n    if ((x is S.NaN) or (y is S.NaN)):\n        return S.NaN\n    elif (x == y):\n        return S.Zero\n    elif (((x is I) or (x is N) or (x is O)) or ((y is I) or (y is N) or (y is O))):\n        return (erf(y) - erf(x))\n    if ((y.func is erf2inv) and (y.args[0] == x)):\n        return y.args[1]\n    sign_x = x.could_extract_minus_sign()\n    sign_y = y.could_extract_minus_sign()\n    if (sign_x and sign_y):\n        return (- cls((- x), (- y)))\n    elif (sign_x or sign_y):\n        return (erf(y) - erf(x))\n", "label": 1}
{"function": "\n\ndef _process(self):\n    ' Send the first request on the stack. '\n    if (not self._requests):\n        if self._stream:\n            self._want_close = True\n            self._no_process = True\n            self._stream.close(False)\n            self._stream = None\n        self._processing = False\n        return\n    self._processing = True\n    request = self._requests[0]\n    if (not request.response):\n        HTTPResponse(request)\n    if (request.auth and (not isinstance(request.auth, (list, tuple)))):\n        request = request.auth(request)\n    port = _port(request.url)\n    is_secure = (request.url.scheme == 'https')\n    if (not port):\n        port = (443 if is_secure else 80)\n    host = ('%s:%d' % (_hostname(request.url), port))\n    if self._stream:\n        if (not self._stream.connected):\n            self._stream = None\n        elif ((self._ssl_options != request.session.ssl_options) or (not self._stream.can_fetch(host, is_secure))):\n            log.debug(('Closing unusable stream for %r.' % self))\n            self._want_close = True\n            self._no_process = False\n            self._stream.close(False)\n            return\n    log.debug(('Sending HTTP request %r.' % request))\n    self._reset_timer()\n    if (not self._stream):\n        self._stream = _HTTPStream(self, engine=self.engine)\n    if (is_secure and (not self._stream.ssl_enabled)):\n        self._ssl_options = request.session.ssl_options\n        self._stream.startSSL((self._ssl_options or {\n            \n        }))\n    self._stream.connect((_hostname(request.url), port))\n", "label": 1}
{"function": "\n\ndef _validate_port_range(self, rule):\n    'Check that port_range is valid.'\n    if ((rule['port_range_min'] is None) and (rule['port_range_max'] is None)):\n        return\n    if (not rule['protocol']):\n        raise ext_sg.SecurityGroupProtocolRequiredWithPorts()\n    ip_proto = self._get_ip_proto_number(rule['protocol'])\n    if (ip_proto in [constants.PROTO_NUM_TCP, constants.PROTO_NUM_UDP]):\n        if ((rule['port_range_min'] == 0) or (rule['port_range_max'] == 0)):\n            raise ext_sg.SecurityGroupInvalidPortValue(port=0)\n        elif ((rule['port_range_min'] is not None) and (rule['port_range_max'] is not None) and (rule['port_range_min'] <= rule['port_range_max'])):\n            pass\n        else:\n            raise ext_sg.SecurityGroupInvalidPortRange()\n    elif (ip_proto == constants.PROTO_NUM_ICMP):\n        for (attr, field) in [('port_range_min', 'type'), ('port_range_max', 'code')]:\n            if ((rule[attr] is not None) and (not (0 <= rule[attr] <= 255))):\n                raise ext_sg.SecurityGroupInvalidIcmpValue(field=field, attr=attr, value=rule[attr])\n        if ((rule['port_range_min'] is None) and (rule['port_range_max'] is not None)):\n            raise ext_sg.SecurityGroupMissingIcmpType(value=rule['port_range_max'])\n", "label": 1}
{"function": "\n\ndef __new__(cls, start, end, left_open=False, right_open=False):\n    start = _sympify(start)\n    end = _sympify(end)\n    left_open = _sympify(left_open)\n    right_open = _sympify(right_open)\n    if (not all((isinstance(a, (type(true), type(false))) for a in [left_open, right_open]))):\n        raise NotImplementedError(('left_open and right_open can have only true/false values, got %s and %s' % (left_open, right_open)))\n    inftys = [S.Infinity, S.NegativeInfinity]\n    if (not all((((i.is_real is not False) or (i in inftys)) for i in (start, end)))):\n        raise ValueError('Non-real intervals are not supported')\n    if ((end < start) == True):\n        return S.EmptySet\n    elif (end - start).is_negative:\n        return S.EmptySet\n    if ((end == start) and (left_open or right_open)):\n        return S.EmptySet\n    if ((end == start) and (not (left_open or right_open))):\n        return FiniteSet(end)\n    if (start == S.NegativeInfinity):\n        left_open = true\n    if (end == S.Infinity):\n        right_open = true\n    return Basic.__new__(cls, start, end, left_open, right_open)\n", "label": 1}
{"function": "\n\ndef _tree_reduce(x, aggregate, axis, keepdims, dtype, split_every=None, combine=None, name=None):\n    'Perform the tree reduction step of a reduction.\\n\\n    Lower level, users should use ``reduction`` or ``arg_reduction`` directly.\\n    '\n    split_every = (split_every or _globals.get('split_every', 4))\n    if isinstance(split_every, dict):\n        split_every = dict(((k, split_every.get(k, 2)) for k in axis))\n    elif isinstance(split_every, int):\n        n = builtins.max(int((split_every ** (1 / (len(axis) or 1)))), 2)\n        split_every = dict.fromkeys(axis, n)\n    else:\n        split_every = dict(((k, v) for (k, v) in enumerate(x.numblocks) if (k in axis)))\n    depth = 1\n    for (i, n) in enumerate(x.numblocks):\n        if ((i in split_every) and (split_every[i] != 1)):\n            depth = int(builtins.max(depth, ceil(log(n, split_every[i]))))\n    func = compose(partial((combine or aggregate), axis=axis, keepdims=True), partial(_concatenate2, axes=axis))\n    for i in range((depth - 1)):\n        x = partial_reduce(func, x, split_every, True, None, name=((name or funcname((combine or aggregate))) + '-partial'))\n    func = compose(partial(aggregate, axis=axis, keepdims=keepdims), partial(_concatenate2, axes=axis))\n    return partial_reduce(func, x, split_every, keepdims=keepdims, dtype=dtype, name=((name or funcname(aggregate)) + '-aggregate'))\n", "label": 1}
{"function": "\n\ndef write_def_finish(self, node, buffered, filtered, cached, callstack=True):\n    'write the end section of a rendering function, either outermost or\\n        inline.\\n\\n        this takes into account if the rendering function was filtered,\\n        buffered, etc.  and closes the corresponding try: block if any, and\\n        writes code to retrieve captured content, apply filters, send proper\\n        return value.'\n    if ((not buffered) and (not cached) and (not filtered)):\n        self.printer.writeline(\"return ''\")\n        if callstack:\n            self.printer.writelines('finally:', 'context.caller_stack._pop_frame()', None)\n    if (buffered or filtered or cached):\n        if (buffered or cached):\n            self.printer.writelines('finally:', '__M_buf = context._pop_buffer()')\n        else:\n            self.printer.writelines('finally:', '__M_buf, __M_writer = context._pop_buffer_and_writer()')\n        if callstack:\n            self.printer.writeline('context.caller_stack._pop_frame()')\n        s = '__M_buf.getvalue()'\n        if filtered:\n            s = self.create_filter_callable(node.filter_args.args, s, False)\n        self.printer.writeline(None)\n        if (buffered and (not cached)):\n            s = self.create_filter_callable(self.compiler.buffer_filters, s, False)\n        if (buffered or cached):\n            self.printer.writeline(('return %s' % s))\n        else:\n            self.printer.writelines(('__M_writer(%s)' % s), \"return ''\")\n", "label": 1}
{"function": "\n\ndef parse_diff(self, result, stdin=None):\n    lines = result.splitlines()\n    matcher = re.compile('^@@ -([0-9]*),([0-9]*) \\\\+([0-9]*),([0-9]*) @@')\n    diff = []\n    for line_index in range(0, len(lines)):\n        line = lines[line_index]\n        if (not line.startswith('@')):\n            continue\n        match = matcher.match(line)\n        if (not match):\n            continue\n        (line_before, len_before, line_after, len_after) = [int(match.group(x)) for x in [1, 2, 3, 4]]\n        chunk_index = (line_index + 1)\n        tracked_line_index = (line_after - 1)\n        deletion = False\n        insertion = False\n        while True:\n            line = lines[chunk_index]\n            if line.startswith('@'):\n                break\n            elif line.startswith('-'):\n                if (not (line.strip() == '-')):\n                    deletion = True\n                tracked_line_index -= 1\n            elif line.startswith('+'):\n                if (deletion and (not (line.strip() == '+'))):\n                    diff.append(['x', tracked_line_index])\n                    insertion = True\n                elif (not deletion):\n                    insertion = True\n                    diff.append(['+', tracked_line_index])\n            else:\n                if ((not insertion) and deletion):\n                    diff.append(['-', tracked_line_index])\n                insertion = deletion = False\n            tracked_line_index += 1\n            chunk_index += 1\n            if (chunk_index >= len(lines)):\n                break\n    self.annotate(diff)\n", "label": 1}
{"function": "\n\ndef skip(self, ttype):\n    if (ttype == TType.STOP):\n        return\n    elif (ttype == TType.BOOL):\n        self.read_bool()\n    elif (ttype == TType.BYTE):\n        self.read_byte()\n    elif (ttype in (TType.I16, TType.I32, TType.I64)):\n        from_zig_zag(read_varint(self.trans))\n    elif (ttype == TType.DOUBLE):\n        self.read_double()\n    elif (ttype == TType.STRING):\n        self.read_string()\n    elif (ttype == TType.STRUCT):\n        name = self.read_struct_begin()\n        while True:\n            (name, ttype, id) = self.read_field_begin()\n            if (ttype == TType.STOP):\n                break\n            self.skip(ttype)\n            self.read_field_end()\n        self.read_struct_end()\n    elif (ttype == TType.MAP):\n        (ktype, vtype, size) = self.read_map_begin()\n        for i in range(size):\n            self.skip(ktype)\n            self.skip(vtype)\n        self.read_collection_end()\n    elif (ttype == TType.SET):\n        (etype, size) = self.read_collection_begin()\n        for i in range(size):\n            self.skip(etype)\n        self.read_collection_end()\n    elif (ttype == TType.LIST):\n        (etype, size) = self.read_collection_begin()\n        for i in range(size):\n            self.skip(etype)\n        self.read_collection_end()\n", "label": 1}
{"function": "\n\ndef take_action(self, action_name, obj_id=None, obj_ids=None):\n    'Locates the appropriate action and routes the object\\n        data to it. The action should return an HTTP redirect\\n        if successful, or a value which evaluates to ``False``\\n        if unsuccessful.\\n        '\n    obj_ids = (obj_ids or self.request.POST.getlist('object_ids'))\n    action = self.base_actions.get(action_name, None)\n    if ((not action) or (action.method != self.request.method)):\n        return None\n    if ((not action.requires_input) or obj_id or obj_ids):\n        if obj_id:\n            obj_id = self.sanitize_id(obj_id)\n        if obj_ids:\n            obj_ids = [self.sanitize_id(i) for i in obj_ids]\n        if (not action.handles_multiple):\n            response = action.single(self, self.request, obj_id)\n        else:\n            if obj_id:\n                obj_ids = [obj_id]\n            response = action.multiple(self, self.request, obj_ids)\n        return response\n    elif (action and action.requires_input and (not (obj_id or obj_ids))):\n        messages.info(self.request, _('Please select a row before taking that action.'))\n    return None\n", "label": 1}
{"function": "\n\ndef diff_map(self, inconstrs):\n    'Generate SQL to transform existing constraints\\n\\n        :param inconstrs: a YAML map defining the new constraints\\n        :return: list of SQL statements\\n\\n        Compares the existing constraint definitions, as fetched from\\n        the catalogs, to the input map and generates SQL statements to\\n        transform the constraints accordingly.\\n        '\n    stmts = []\n    for turn in (1, 2):\n        for (sch, tbl, cns) in self:\n            constr = self[(sch, tbl, cns)]\n            if getattr(constr, 'inherited', False):\n                continue\n            if isinstance(constr, ForeignKey):\n                if (turn == 1):\n                    continue\n            elif (turn == 2):\n                continue\n            if (((sch, tbl, cns) not in inconstrs) and (not hasattr(constr, 'target'))):\n                stmts.append(constr.drop())\n        for (sch, tbl, cns) in inconstrs:\n            inconstr = inconstrs[(sch, tbl, cns)]\n            if getattr(inconstr, 'inherited', False):\n                continue\n            if hasattr(inconstr, 'target'):\n                continue\n            if isinstance(inconstr, ForeignKey):\n                if (turn == 1):\n                    continue\n            elif (turn == 2):\n                continue\n            if ((sch, tbl, cns) not in self):\n                stmts.append(inconstr.add())\n            else:\n                stmts.append(self[(sch, tbl, cns)].diff_map(inconstr))\n    return stmts\n", "label": 1}
{"function": "\n\ndef process(self, processed, glyphRecords, featureTag):\n    performedAction = False\n    currentRecord = glyphRecords[0]\n    currentGlyph = currentRecord.glyphName\n    if (currentGlyph in self.Coverage):\n        for chainRuleSet in self._ChainRuleSet:\n            for chainRule in chainRuleSet._ChainRule:\n                backtrackCount = chainRule.BacktrackGlyphCount\n                if (not backtrackCount):\n                    backtrackMatch = True\n                else:\n                    (backtrackMatch, backtrack, backtrackMatchIndexes) = self._testContext(reversed(processed), chainRule.Backtrack, backtrackCount)\n                if (not backtrackMatch):\n                    continue\n                inputCount = chainRule.InputGlyphCount\n                if (not inputCount):\n                    inputMatch = True\n                else:\n                    (inputMatch, input, inputMatchIndexes) = self._testContext(glyphRecords[1:], chainRule.Input, (inputCount - 1))\n                if (not inputMatch):\n                    continue\n                input = ([currentRecord] + input)\n                inputMatchIndexes = ([0] + [(i + 1) for i in inputMatchIndexes])\n                lookAheadCount = chainRule.LookAheadGlyphCount\n                if (not lookAheadCount):\n                    lookAheadMatch = True\n                else:\n                    (lookAheadMatch, lookAhead, lookAheadMatchIndexes) = self._testContext(glyphRecords[len(input):], chainRule.LookAhead, lookAheadCount)\n                if (not lookAheadMatch):\n                    continue\n                if (backtrackMatch and inputMatch and lookAheadMatch):\n                    (processed, glyphRecords, performedAction) = self._processMatch(chainRule, processed, glyphRecords, len(input), inputMatchIndexes, featureTag)\n                    if performedAction:\n                        break\n            if performedAction:\n                break\n    return (processed, glyphRecords, performedAction)\n", "label": 1}
{"function": "\n\ndef get_js_from_html(html, root):\n    ' Given an html document provided as a string, extract the\\n    JavaScript.\\n    '\n    parts = []\n    i = 0\n    while True:\n        i = html.find('<script', i)\n        if ((i < 0) or (i > (len(html) - 5))):\n            break\n        i_end1 = html.find('>', (i + 6))\n        i_end2 = html.find('/>', (i + 6))\n        i_end3 = html.find('</script>', (i + 6))\n        i_src = html.find('src=', (i + 6))\n        ends = [j for j in (i_end2, i_end3) if (j > 0)]\n        if (not ends):\n            break\n        i_end = min(ends)\n        i = i_end\n        if ((i_src > 0) and (i_src < i_end1)):\n            i1 = (i_src + 5)\n            quote = html[(i1 - 1)]\n            if (quote not in '\"\\''):\n                continue\n            i2 = html.find(quote, i1)\n            fname = html[i1:i2]\n            for filename in (fname, ((root + '/') + fname)):\n                if filename.startswith('http'):\n                    code = urlopen(filename, timeout=5.0).read().decode()\n                    break\n                elif os.path.isfile(filename):\n                    code = open(filename, 'rb').read().decode()\n                    break\n            else:\n                raise IOError(('Could not get JS for file %r' % fname))\n            parts.append(code)\n        elif ((i_end == i_end3) and (i_end > i_end1)):\n            i1 = (i_end1 + 1)\n            i2 = i_end\n            parts.append(html[i1:i2])\n        else:\n            pass\n    return '\\n'.join(parts)\n", "label": 1}
{"function": "\n\ndef _complement(self, other):\n    if isinstance(other, ProductSet):\n        switch_sets = ProductSet((FiniteSet(o, (o - s)) for (s, o) in zip(self.sets, other.sets)))\n        product_sets = (ProductSet(*set) for set in switch_sets)\n        return Union((p for p in product_sets if (p != other)))\n    elif isinstance(other, Interval):\n        if (isinstance(self, Interval) or isinstance(self, FiniteSet)):\n            return Intersection(other, self.complement(S.Reals))\n    elif isinstance(other, Union):\n        return Union(((o - self) for o in other.args))\n    elif isinstance(other, Complement):\n        return Complement(other.args[0], Union(other.args[1], self), evaluate=False)\n    elif isinstance(other, EmptySet):\n        return S.EmptySet\n    elif isinstance(other, FiniteSet):\n        return FiniteSet(*[el for el in other if (self.contains(el) != True)])\n", "label": 1}
{"function": "\n\ndef do_no_ip(self, *args):\n    if 'address'.startswith(args[0]):\n        if (len(args) == 1):\n            self.port.ips = []\n        else:\n            ip = IPNetwork(('%s/%s' % (args[1], args[2])))\n            is_secondary = ('secondary'.startswith(args[3]) if (len(args) == 4) else False)\n            if is_secondary:\n                self.port.remove_ip(ip)\n            elif (len(self.port.ips) == 1):\n                self.port.remove_ip(ip)\n            else:\n                self.write_line('Must delete secondary before deleting primary')\n    if 'access-group'.startswith(args[0]):\n        direction = args[(- 1)]\n        if 'in'.startswith(direction):\n            self.port.access_group_in = None\n        elif 'out'.startswith(direction):\n            self.port.access_group_out = None\n    if 'vrf'.startswith(args[0]):\n        if 'forwarding'.startswith(args[1]):\n            self.port.vrf = None\n    if 'redirects'.startswith(args[0]):\n        self.port.ip_redirect = False\n    if 'helper-address'.startswith(args[0]):\n        if (len(args) > 2):\n            self.write_line(' ^')\n            self.write_line(\"% Invalid input detected at '^' marker.\")\n            self.write_line('')\n        elif (len(args) == 1):\n            self.port.ip_helpers = []\n        else:\n            ip_address = IPAddress(args[1])\n            if (ip_address in self.port.ip_helpers):\n                self.port.ip_helpers.remove(ip_address)\n", "label": 1}
{"function": "\n\ndef get_closest_mode(self, width, height):\n    'Get the screen mode that best matches a given size.\\n\\n        If no supported mode exactly equals the requested size, a larger one\\n        is returned; or ``None`` if no mode is large enough.\\n\\n        :Parameters:\\n            `width` : int\\n                Requested screen width.\\n            `height` : int\\n                Requested screen height.\\n\\n        :rtype: `ScreenMode`\\n\\n        :since: pyglet 1.2\\n        '\n    current = self.get_mode()\n    best = None\n    for mode in self.get_modes():\n        if ((mode.width < width) or (mode.height < height)):\n            continue\n        if (best is None):\n            best = mode\n        if ((mode.width <= best.width) and (mode.height <= best.height) and ((mode.width < best.width) or (mode.height < best.height))):\n            best = mode\n        if ((mode.width == best.width) and (mode.height == best.height)):\n            points = 0\n            if (mode.rate == current.rate):\n                points += 2\n            if (best.rate == current.rate):\n                points -= 2\n            if (mode.depth == current.depth):\n                points += 1\n            if (best.depth == current.depth):\n                points -= 1\n            if (points > 0):\n                best = mode\n    return best\n", "label": 1}
{"function": "\n\ndef get_download_urls(self, package_name, version='', pkg_type='all'):\n    'Query PyPI for pkg download URI for a packge'\n    if version:\n        versions = [version]\n    else:\n        (package_name, versions) = self.query_versions_pypi(package_name)\n    all_urls = []\n    for ver in versions:\n        metadata = self.release_data(package_name, ver)\n        for urls in self.release_urls(package_name, ver):\n            if ((pkg_type == 'source') and (urls['packagetype'] == 'sdist')):\n                all_urls.append(urls['url'])\n            elif ((pkg_type == 'egg') and urls['packagetype'].startswith('bdist')):\n                all_urls.append(urls['url'])\n            elif (pkg_type == 'all'):\n                all_urls.append(urls['url'])\n        if (metadata and metadata.has_key('download_url') and (metadata['download_url'] != 'UNKNOWN') and (metadata['download_url'] != None)):\n            if (metadata['download_url'] not in all_urls):\n                if (pkg_type != 'all'):\n                    url = filter_url(pkg_type, metadata['download_url'])\n                    if url:\n                        all_urls.append(url)\n    return all_urls\n", "label": 1}
{"function": "\n\ndef _crop_and_border(martix):\n    (t, b, l, r) = (0, 0, 0, 0)\n    for y in xrange(len(martix)):\n        if (sum(martix[y]) == 0):\n            t += 1\n        else:\n            break\n    for y in xrange(len(martix)):\n        if (sum(martix[((- 1) - y)]) == 0):\n            b += 1\n        else:\n            break\n    for x in xrange(len(martix[0])):\n        if (sum(map((lambda row: row[x]), martix)) == 0):\n            l += 1\n        else:\n            break\n    for x in xrange(len(martix[0])):\n        if (sum(map((lambda row: row[((- 1) - x)]), martix)) == 0):\n            r += 1\n        else:\n            break\n    w = len(martix[0])\n    if (t > 0):\n        martix = martix[(t - 1):]\n    else:\n        martix.insert(0, ([0] * w))\n    if (b > 1):\n        martix = martix[:(1 - b)]\n    elif (b == 0):\n        martix.append(([0] * w))\n    for ri in xrange(len(martix)):\n        row = martix[ri]\n        if (l > 0):\n            row = row[(l - 1):]\n        else:\n            row.insert(0, 0)\n        if (r > 1):\n            row = row[:(1 - r)]\n        elif (r == 0):\n            row.append(0)\n        martix[ri] = row\n    return martix\n", "label": 1}
{"function": "\n\ndef _produceNewSample(self):\n    ' returns a new sample, its fitness and its densities '\n    chosenOne = drawIndex(self.alphas, True)\n    mu = self.mus[chosenOne]\n    if self.useAnticipatedMeanShift:\n        if (((len(self.allsamples) % 2) == 1) and (len(self.allsamples) > 1)):\n            if (not (self.elitism and (chosenOne == self.bestChosenCenter))):\n                mu += self.meanShifts[chosenOne]\n    if self.diagonalOnly:\n        sample = normal(mu, self.sigmas[chosenOne])\n    else:\n        sample = multivariate_normal(mu, self.sigmas[chosenOne])\n    if (self.sampleElitism and (len(self.allsamples) > self.windowSize) and ((len(self.allsamples) % self.windowSize) == 0)):\n        sample = self.bestEvaluable.copy()\n    fit = self._oneEvaluation(sample)\n    if (((not self.minimize) and (fit >= self.bestEvaluation)) or (self.minimize and (fit <= self.bestEvaluation)) or (len(self.allsamples) == 0)):\n        self.bestChosenCenter = chosenOne\n        self.bestSigma = self.sigmas[chosenOne].copy()\n    if self.minimize:\n        fit = (- fit)\n    self.allfitnesses.append(fit)\n    self.allsamples.append(sample)\n    return (sample, fit)\n", "label": 1}
{"function": "\n\ndef _validate_select_where(self):\n    ' Checks that a filterset will not create invalid select statement '\n    equal_ops = [self.model._get_column_by_db_name(w.field) for w in self._where if isinstance(w.operator, EqualsOperator)]\n    token_comparison = any([w for w in self._where if isinstance(w.value, Token)])\n    if ((not any(((w.primary_key or w.index) for w in equal_ops))) and (not token_comparison) and (not self._allow_filtering)):\n        raise QueryException('Where clauses require either  =, a IN or a CONTAINS (collection) comparison with either a primary key or indexed field')\n    if (not self._allow_filtering):\n        if (not any((w.index for w in equal_ops))):\n            if ((not any([w.partition_key for w in equal_ops])) and (not token_comparison)):\n                raise QueryException('Filtering on a clustering key without a partition key is not allowed unless allow_filtering() is called on the querset')\n", "label": 1}
{"function": "\n\ndef filter(vulns, exploitsOnly=False, filters={\n    \n}):\n    if (('access' in filters) and (len(filters['access']) != 0)):\n        vulns = [x for x in vulns if (('access' in x) and (x['access']['vector'] in filters['access']))]\n    if (('impact' in filters) and (len(filters['impact']) != 0)):\n        for fil in filters['impact']:\n            vulns = [x for x in vulns if (('impact' in x) and (x['impact'][fil] != 'NONE'))]\n    if exploitsOnly:\n        vulns = [x for x in vulns if (('map_cve_exploitdb' in x) or ('map_cve_msf' in x))]\n    return vulns\n", "label": 1}
{"function": "\n\ndef c2ln(c, l1, l2, n):\n    'char[n] to two unsigned long???'\n    c = (c + n)\n    (l1, l2) = (U32(0), U32(0))\n    f = 0\n    if (n == 8):\n        l2 = (l2 | (U32(c[7]) << 24))\n        f = 1\n    if (f or (n == 7)):\n        l2 = (l2 | (U32(c[6]) << 16))\n        f = 1\n    if (f or (n == 6)):\n        l2 = (l2 | (U32(c[5]) << 8))\n        f = 1\n    if (f or (n == 5)):\n        l2 = (l2 | U32(c[4]))\n        f = 1\n    if (f or (n == 4)):\n        l1 = (l1 | (U32(c[3]) << 24))\n        f = 1\n    if (f or (n == 3)):\n        l1 = (l1 | (U32(c[2]) << 16))\n        f = 1\n    if (f or (n == 2)):\n        l1 = (l1 | (U32(c[1]) << 8))\n        f = 1\n    if (f or (n == 1)):\n        l1 = (l1 | U32(c[0]))\n    return (l1, l2)\n", "label": 1}
{"function": "\n\ndef paint_path(self, gstate, stroke, fill, evenodd, path):\n    shape = ''.join((x[0] for x in path))\n    if (shape == 'ml'):\n        (_, x0, y0) = path[0]\n        (_, x1, y1) = path[1]\n        (x0, y0) = apply_matrix_pt(self.ctm, (x0, y0))\n        (x1, y1) = apply_matrix_pt(self.ctm, (x1, y1))\n        if ((x0 == x1) or (y0 == y1)):\n            self.cur_item.add(LTLine(gstate.linewidth, (x0, y0), (x1, y1)))\n            return\n    if (shape == 'mlllh'):\n        (_, x0, y0) = path[0]\n        (_, x1, y1) = path[1]\n        (_, x2, y2) = path[2]\n        (_, x3, y3) = path[3]\n        (x0, y0) = apply_matrix_pt(self.ctm, (x0, y0))\n        (x1, y1) = apply_matrix_pt(self.ctm, (x1, y1))\n        (x2, y2) = apply_matrix_pt(self.ctm, (x2, y2))\n        (x3, y3) = apply_matrix_pt(self.ctm, (x3, y3))\n        if (((x0 == x1) and (y1 == y2) and (x2 == x3) and (y3 == y0)) or ((y0 == y1) and (x1 == x2) and (y2 == y3) and (x3 == x0))):\n            self.cur_item.add(LTRect(gstate.linewidth, (x0, y0, x2, y2)))\n            return\n    pts = []\n    for p in path:\n        for i in xrange(1, len(p), 2):\n            pts.append(apply_matrix_pt(self.ctm, (p[i], p[(i + 1)])))\n    self.cur_item.add(LTCurve(gstate.linewidth, pts))\n    return\n", "label": 1}
{"function": "\n\ndef process_deletes(self, uowcommit, states):\n    if (self.post_update or (not (self.passive_deletes == 'all'))):\n        children_added = uowcommit.memo(('children_added', self), set)\n        for state in states:\n            history = uowcommit.get_attribute_history(state, self.key, self._passive_delete_flag)\n            if history:\n                for child in history.deleted:\n                    if ((child is not None) and (self.hasparent(child) is False)):\n                        self._synchronize(state, child, None, True, uowcommit, False)\n                        if (self.post_update and child):\n                            self._post_update(child, uowcommit, [state])\n                if (self.post_update or (not self.cascade.delete)):\n                    for child in set(history.unchanged).difference(children_added):\n                        if (child is not None):\n                            self._synchronize(state, child, None, True, uowcommit, False)\n                            if (self.post_update and child):\n                                self._post_update(child, uowcommit, [state])\n", "label": 1}
{"function": "\n\ndef IndexOfNextNonSpace(string_data, current_index):\n    successful = False\n    found_index = current_index\n    string_length = len(string_data)\n    annotation_string = ''\n    while (found_index < string_length):\n        current_char = string_data[found_index]\n        if (IsSpecialWhitespace(current_char) == True):\n            found_index += 1\n            continue\n        if (IsRegularWhitespace(current_char) == True):\n            found_index += 1\n            continue\n        if (current_char == '/'):\n            next_index = (found_index + 1)\n            if (next_index >= string_length):\n                successful = True\n                break\n            else:\n                next_character = string_data[next_index]\n                if (next_character == '/'):\n                    found_index += 1\n                    next_index = found_index\n                    first_pass = True\n                    while (next_index < string_length):\n                        test_char = string_data[next_index]\n                        if (IsEndOfLine(test_char) == True):\n                            break\n                        elif (first_pass != True):\n                            annotation_string += test_char\n                        else:\n                            first_pass = False\n                        next_index += 1\n                    found_index = next_index\n                elif (next_character == '*'):\n                    found_index += 1\n                    next_index = found_index\n                    first_pass = True\n                    while (next_index < string_length):\n                        test_char = string_data[next_index]\n                        if ((test_char == '*') and ((next_index + 1) < string_length) and (string_data[(next_index + 1)] == '/')):\n                            next_index += 2\n                            break\n                        elif (first_pass != True):\n                            annotation_string += test_char\n                        else:\n                            first_pass = False\n                        next_index += 1\n                    found_index = next_index\n                else:\n                    successful = True\n                    break\n        else:\n            successful = True\n            break\n    return (successful, found_index, annotation_string)\n", "label": 1}
{"function": "\n\ndef _bulk_insert(mapper, mappings, session_transaction, isstates, return_defaults):\n    base_mapper = mapper.base_mapper\n    cached_connections = _cached_connection_dict(base_mapper)\n    if session_transaction.session.connection_callable:\n        raise NotImplementedError('connection_callable / per-instance sharding not supported in bulk_insert()')\n    if isstates:\n        if return_defaults:\n            states = [(state, state.dict) for state in mappings]\n            mappings = [dict_ for (state, dict_) in states]\n        else:\n            mappings = [state.dict for state in mappings]\n    else:\n        mappings = list(mappings)\n    connection = session_transaction.connection(base_mapper)\n    for (table, super_mapper) in base_mapper._sorted_tables.items():\n        if (not mapper.isa(super_mapper)):\n            continue\n        records = ((None, state_dict, params, mapper, connection, value_params, has_all_pks, has_all_defaults) for (state, state_dict, params, mp, conn, value_params, has_all_pks, has_all_defaults) in _collect_insert_commands(table, ((None, mapping, mapper, connection) for mapping in mappings), bulk=True, return_defaults=return_defaults))\n        _emit_insert_statements(base_mapper, None, cached_connections, super_mapper, table, records, bookkeeping=return_defaults)\n    if (return_defaults and isstates):\n        identity_cls = mapper._identity_class\n        identity_props = [p.key for p in mapper._identity_key_props]\n        for (state, dict_) in states:\n            state.key = (identity_cls, tuple([dict_[key] for key in identity_props]))\n", "label": 1}
{"function": "\n\n@classmethod\ndef _diff_packets(cls, model_pkt, rcv_pkt):\n    msg = []\n    for rcv_p in rcv_pkt.protocols:\n        if (not isinstance(rcv_p, six.binary_type)):\n            model_protocols = model_pkt.get_protocols(type(rcv_p))\n            if (len(model_protocols) == 1):\n                model_p = model_protocols[0]\n                diff = []\n                for attr in rcv_p.__dict__:\n                    if attr.startswith('_'):\n                        continue\n                    if callable(attr):\n                        continue\n                    if hasattr(rcv_p.__class__, attr):\n                        continue\n                    rcv_attr = repr(getattr(rcv_p, attr))\n                    model_attr = repr(getattr(model_p, attr))\n                    if (rcv_attr != model_attr):\n                        diff.append(('%s=%s' % (attr, rcv_attr)))\n                if diff:\n                    msg.append(('%s(%s)' % (rcv_p.__class__.__name__, ','.join(diff))))\n            elif ((not model_protocols) or (not (str(rcv_p) in str(model_protocols)))):\n                msg.append(str(rcv_p))\n        else:\n            model_p = ''\n            for p in model_pkt.protocols:\n                if isinstance(rcv_p, six.binary_type):\n                    model_p = p\n                    break\n            if (model_p != rcv_p):\n                msg.append(('str(%s)' % repr(rcv_p)))\n    if msg:\n        return '/'.join(msg)\n    else:\n        return 'Encounter an error during packet comparison. it is malformed.'\n", "label": 1}
{"function": "\n\ndef asXML(self, doctag=None, namedItemsOnly=False, indent='', formatted=True):\n    'Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.'\n    nl = '\\n'\n    out = []\n    namedItems = dict([(v[1], k) for (k, vlist) in self.__tokdict.items() for v in vlist])\n    nextLevelIndent = (indent + '  ')\n    if (not formatted):\n        indent = ''\n        nextLevelIndent = ''\n        nl = ''\n    selfTag = None\n    if (doctag is not None):\n        selfTag = doctag\n    elif self.__name:\n        selfTag = self.__name\n    if (not selfTag):\n        if namedItemsOnly:\n            return ''\n        else:\n            selfTag = 'ITEM'\n    out += [nl, indent, '<', selfTag, '>']\n    worklist = self.__toklist\n    for (i, res) in enumerate(worklist):\n        if isinstance(res, ParseResults):\n            if (i in namedItems):\n                out += [res.asXML(namedItems[i], (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n            else:\n                out += [res.asXML(None, (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n        else:\n            resTag = None\n            if (i in namedItems):\n                resTag = namedItems[i]\n            if (not resTag):\n                if namedItemsOnly:\n                    continue\n                else:\n                    resTag = 'ITEM'\n            xmlBodyText = _xml_escape(_ustr(res))\n            out += [nl, nextLevelIndent, '<', resTag, '>', xmlBodyText, '</', resTag, '>']\n    out += [nl, indent, '</', selfTag, '>']\n    return ''.join(out)\n", "label": 1}
{"function": "\n\ndef bounded_stats_per_chunk(chunk, block_data_totals, start, stop):\n    'Given a chunk, return the number of blocks types within the specified selection'\n    (chunk_z, chunk_x) = chunk.get_coords()\n    for z in range(16):\n        world_z = (z + (chunk_z * 16))\n        if (((start != None) and (world_z < int(start[2]))) or ((stop != None) and (world_z > int(stop[2])))):\n            break\n        for x in range(16):\n            world_x = (x + (chunk_x * 16))\n            if (((start != None) and (world_x < int(start[0]))) or ((stop != None) and (world_x > int(stop[0])))):\n                break\n            for y in range(128):\n                if (((start != None) and (y < int(start[1]))) or ((stop != None) and (y > int(stop[1])))):\n                    break\n                (block_id, block_data) = chunk.blocks.get_block_and_data(x, y, z)\n                block_data_totals[block_id][block_data] += 1\n", "label": 1}
{"function": "\n\ndef getdesc(root, host_url=''):\n    methods = {\n        \n    }\n    for (path, funcdef) in root.getapi():\n        method = funcdef.extra_options.get('method', None)\n        name = '_'.join(path)\n        if (method is not None):\n            path = path[:(- 1)]\n        else:\n            method = 'GET'\n            for argdef in funcdef.arguments:\n                if (types.iscomplex(argdef.datatype) or types.isarray(argdef.datatype) or types.isdict(argdef.datatype)):\n                    method = 'POST'\n                    break\n        required_params = []\n        optional_params = []\n        for argdef in funcdef.arguments:\n            if ((method == 'GET') and argdef.mandatory):\n                required_params.append(argdef.name)\n            else:\n                optional_params.append(argdef.name)\n        methods[name] = {\n            'method': method,\n            'path': '/'.join(path),\n        }\n        if required_params:\n            methods[name]['required_params'] = required_params\n        if optional_params:\n            methods[name]['optional_params'] = optional_params\n        if funcdef.doc:\n            methods[name]['documentation'] = funcdef.doc\n    formats = []\n    for p in root.protocols:\n        if (p.name == 'restxml'):\n            formats.append('xml')\n        if (p.name == 'restjson'):\n            formats.append('json')\n    api = {\n        'base_url': (host_url + root._webpath),\n        'version': '0.1',\n        'name': getattr(root, 'name', 'name'),\n        'authority': '',\n        'formats': ['json', 'xml'],\n        'methods': methods,\n    }\n    return json.dumps(api, indent=4)\n", "label": 1}
{"function": "\n\n@classmethod\ndef eval(cls, n, sym=None):\n    if n.is_Number:\n        if (n.is_Integer and n.is_nonnegative):\n            if (n is S.Zero):\n                return S.One\n            elif (n is S.One):\n                if (sym is None):\n                    return (- S.Half)\n                else:\n                    return (sym - S.Half)\n            elif (sym is None):\n                if n.is_odd:\n                    return S.Zero\n                n = int(n)\n                if (n > 500):\n                    (p, q) = bernfrac(n)\n                    return Rational(int(p), int(q))\n                case = (n % 6)\n                highest_cached = cls._highest[case]\n                if (n <= highest_cached):\n                    return cls._cache[n]\n                for i in range((highest_cached + 6), (n + 6), 6):\n                    b = cls._calc_bernoulli(i)\n                    cls._cache[i] = b\n                    cls._highest[case] = i\n                return b\n            else:\n                (n, result) = (int(n), [])\n                for k in range((n + 1)):\n                    result.append(((binomial(n, k) * cls(k)) * (sym ** (n - k))))\n                return Add(*result)\n        else:\n            raise ValueError('Bernoulli numbers are defined only for nonnegative integer indices.')\n    if (sym is None):\n        if (n.is_odd and (n - 1).is_positive):\n            return S.Zero\n", "label": 1}
{"function": "\n\ndef get_correct_sig(args, sigs):\n    'Given a list of args and a collection of possible signatures,\\n    this function returns the most appropriate signature.  This\\n    function is only called by deref_array.  This implies that one of\\n    the signatures has an array type.\\n\\n    '\n    if (sigs is None):\n        return None\n    if (len(sigs) == 1):\n        return sigs[0]\n    else:\n        la = len(args)\n        candidate_sigs = [s for s in sigs if (len(s) == la)]\n        count = len(candidate_sigs)\n        if (count == 0):\n            msg = ('Insufficient number of arguments to method.Valid arguments are:\\n%s' % sigs)\n            raise TypeError(msg)\n        elif (count == 1):\n            return candidate_sigs[0]\n        else:\n            array_idx = [i for (i, a) in enumerate(args) if is_array_or_vtkarray(a)]\n            n_arr = len(array_idx)\n            if (n_arr == 0):\n                return None\n            else:\n                for sig in candidate_sigs:\n                    array_in_sig = [is_array_sig(s) for s in sig]\n                    if (array_in_sig.count(True) != len(array_idx)):\n                        continue\n                    bad = False\n                    for i in array_idx:\n                        if (not array_in_sig[i]):\n                            bad = True\n                    if (not bad):\n                        return sig\n                return None\n", "label": 1}
{"function": "\n\ndef _determine_tests(test_modules):\n    '\\n  Determine a list of tests to run from the provided test modules\\n  '\n    for module in test_modules:\n        attrs = dir(module)\n        if hasattr(module, 'test_phase'):\n            test_phase = module.test_phase\n        else:\n            test_phase = constants.DEFAULT_TEST_PHASE\n        if hasattr(module, 'tests_iteration'):\n            tests_iteration = module.tests_iteration\n        else:\n            tests_iteration = constants.DEFAULT_ITERATION\n        functions = set([fun for fun in attrs if hasattr(getattr(module, fun), '__call__')])\n        tests = dict([(fun.lower(), Test(fun, getattr(module, fun), phase=test_phase, iteration=tests_iteration)) for fun in functions if ('test' in fun.lower())])\n        for fun in functions:\n            if ('validate' in fun.lower()):\n                test_name = fun.lower().replace('validate', 'test')\n                if (test_name in tests):\n                    tests[test_name].validation_function = getattr(module, fun)\n        for test in tests.values():\n            if (test.function.__doc__ is not None):\n                test.description = test.function.__doc__\n            if (test.validation_function is not None):\n                if (test.validation_function.__doc__ is not None):\n                    if (test.description is not None):\n                        test.description = '{0};\\n{1}'.format(test.description, test.validation_function.__doc__)\n                    else:\n                        test.description = test.validation_function.__doc__\n            (yield test)\n", "label": 1}
{"function": "\n\ndef find_prev_lone_bracket(view, start, tags, unbalanced=0):\n    if ((view.substr(start) == tags[0][1]) if (len(tags[0]) > 1) else tags[0]):\n        if ((not unbalanced) and (view.substr((start - 1)) != '\\\\')):\n            return sublime.Region(start, (start + 1))\n    new_start = start\n    for i in range((unbalanced or 1)):\n        prev_opening_bracket = reverse_search_by_pt(view, tags[0], start=0, end=new_start, flags=sublime.IGNORECASE)\n        if (prev_opening_bracket is None):\n            if ((i == 0) and (view.substr(start) == tags[0][(- 1)]) and (view.substr((start - 1)) != '\\\\')):\n                return sublime.Region(start, (start + 1))\n            return\n        while (view.substr((prev_opening_bracket.begin() - 1)) == '\\\\'):\n            prev_opening_bracket = reverse_search_by_pt(view, tags[0], start=0, end=prev_opening_bracket.begin(), flags=sublime.IGNORECASE)\n            if (prev_opening_bracket is None):\n                return\n        new_start = prev_opening_bracket.begin()\n    nested = 0\n    while True:\n        next_closing_bracket = reverse_search_by_pt(view, tags[1], start=prev_opening_bracket.a, end=start, flags=sublime.IGNORECASE)\n        if (not next_closing_bracket):\n            break\n        nested += 1\n        start = next_closing_bracket.begin()\n    if (nested > 0):\n        return find_prev_lone_bracket(view, prev_opening_bracket.begin(), tags, nested)\n    else:\n        return prev_opening_bracket\n", "label": 1}
{"function": "\n\ndef check(data, start_time, end_time, filter_field=None, filter_data=[]):\n    d = []\n    if (data and (len(data) > 0)):\n        for i in data:\n            if (('end_time' in i) and i['end_time'] and (format_timestamp(i['end_time']) < start_time)):\n                continue\n            elif (('start_time' in i) and i['start_time'] and (format_timestamp(i['start_time']) > end_time)):\n                continue\n            elif ((i['deleted'] is True) and (format_timestamp(i['updated_at']) < start_time)):\n                continue\n            elif ((i['paused'] is True) and (format_timestamp(i['updated_at']) < start_time)):\n                continue\n            elif (filter_field and (i[filter_field] not in filter_data)):\n                continue\n            else:\n                d.append(i['id'])\n    return d\n", "label": 1}
{"function": "\n\ndef ProcessFlags(env, flags):\n    for f in flags:\n        if (not f):\n            continue\n        parsed_flags = env.ParseFlags(str(f))\n        for flag in parsed_flags.pop('CPPDEFINES'):\n            if (not isinstance(flag, list)):\n                env.Append(CPPDEFINES=flag)\n                continue\n            if ('\"' in flag[1]):\n                flag[1] = flag[1].replace('\"', '\\\\\"')\n            env.Append(CPPDEFINES=[flag])\n        env.Append(**parsed_flags)\n    for (i, p) in enumerate(env.get('CPPPATH', [])):\n        if isdir(p):\n            env['CPPPATH'][i] = realpath(p)\n    for (i, f) in enumerate(env.get('CCFLAGS', [])):\n        if (isinstance(f, tuple) and (f[0] == '-include')):\n            env['CCFLAGS'][i] = (f[0], env.File(realpath(f[1].get_path())))\n    undefines = [u for u in env.get('CCFLAGS', []) if (isinstance(u, basestring) and u.startswith('-U'))]\n    if undefines:\n        for undef in undefines:\n            env['CCFLAGS'].remove(undef)\n        env.Append(_CPPDEFFLAGS=(' %s' % ' '.join(undefines)))\n", "label": 1}
{"function": "\n\ndef _get_click_probs(self, s, possibleIntents):\n    '\\n            Returns clickProbs list\\n            clickProbs[i][k] = P(C_1, ..., C_k | I=i)\\n            '\n    clickProbs = dict(((i, []) for i in possibleIntents))\n    firstVerticalPos = ((- 1) if (not any(s.layout[:(- 1)])) else [k for (k, l) in enumerate(s.layout) if l][0])\n    prevClick = (- 1)\n    layout = (([False] * len(s.layout)) if self.ignoreLayout else s.layout)\n    for (rank, c) in enumerate(s.clicks):\n        url = s.results[rank]\n        prob = {\n            False: 0.0,\n            True: 0.0,\n        }\n        for i in possibleIntents:\n            a = self.alpha[i][s.query][url]\n            g = self.getGamma(self.gamma, rank, prevClick, layout, i)\n            if (self.explorationBias and any(((s.layout[k] and s.clicks[k]) for k in xrange(rank))) and (not s.layout[rank])):\n                g *= (1 - self.e[firstVerticalPos])\n            prevProb = (1 if (rank == 0) else clickProbs[i][(- 1)])\n            if (c == 0):\n                clickProbs[i].append((prevProb * (1 - (a * g))))\n            else:\n                clickProbs[i].append(((prevProb * a) * g))\n        if (c != 0):\n            prevClick = rank\n    return clickProbs\n", "label": 1}
{"function": "\n\ndef vfs_normpath(path):\n    'Normalize path from posixpath.py, eliminating double slashes, etc.'\n    (slash, dot) = (('/', '.') if isinstance(path, unicode) else ('/', '.'))\n    if (path == ''):\n        return dot\n    initial_slashes = path.startswith('/')\n    if (initial_slashes and path.startswith('//') and (not path.startswith('///'))):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if (comp in ('', '.')):\n            continue\n        if ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(- 1)] == '..'))):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = ((slash * initial_slashes) + path)\n    return (path or dot)\n", "label": 1}
{"function": "\n\ndef _parse_document(data, base_url=None):\n    links = _get_dict(data, '_links')\n    embedded = _get_dict(data, '_embedded')\n    self = _get_dict(links, 'self')\n    url = _get_string(self, 'href')\n    url = urlparse.urljoin(base_url, url)\n    title = _get_string(self, 'title')\n    content = {\n        \n    }\n    for (key, value) in links.items():\n        if (key in ('self', 'curies')):\n            continue\n        key = _map_to_coreapi_key(key)\n        if isinstance(value, list):\n            if (value and ('name' in value[0])):\n                content[key] = {item['name']: _parse_link(item, base_url) for item in value if ('name' in item)}\n            else:\n                content[key] = [_parse_link(item, base_url) for item in value]\n        elif isinstance(value, dict):\n            content[key] = _parse_link(value, base_url)\n    for (key, value) in embedded.items():\n        key = _map_to_coreapi_key(key)\n        if isinstance(value, list):\n            content[key] = [_parse_document(item, base_url=url) for item in value]\n        elif isinstance(value, dict):\n            content[key] = _parse_document(value, base_url=url)\n    for (key, value) in data.items():\n        if (key not in ('_embedded', '_links')):\n            content[key] = value\n    return Document(url, title, content)\n", "label": 1}
{"function": "\n\ndef label_tag(self, contents=None, attrs=None, label_suffix=None):\n    \"\\n        Wraps the given contents in a <label>, if the field has an ID attribute.\\n        contents should be 'mark_safe'd to avoid HTML escaping. If contents\\n        aren't given, uses the field's HTML-escaped label.\\n\\n        If attrs are given, they're used as HTML attributes on the <label> tag.\\n\\n        label_suffix allows overriding the form's label_suffix.\\n        \"\n    contents = (contents or self.label)\n    if (label_suffix is None):\n        label_suffix = (self.field.label_suffix if (self.field.label_suffix is not None) else self.form.label_suffix)\n    if (label_suffix and contents and (contents[(- 1)] not in _(':?.!'))):\n        contents = format_html('{}{}', contents, label_suffix)\n    widget = self.field.widget\n    id_ = (widget.attrs.get('id') or self.auto_id)\n    if id_:\n        id_for_label = widget.id_for_label(id_)\n        if id_for_label:\n            attrs = dict((attrs or {\n                \n            }), **{\n                'for': id_for_label,\n            })\n        if (self.field.required and hasattr(self.form, 'required_css_class')):\n            attrs = (attrs or {\n                \n            })\n            if ('class' in attrs):\n                attrs['class'] += (' ' + self.form.required_css_class)\n            else:\n                attrs['class'] = self.form.required_css_class\n        attrs = (flatatt(attrs) if attrs else '')\n        contents = format_html('<label{}>{}</label>', attrs, contents)\n    else:\n        contents = conditional_escape(contents)\n    return mark_safe(contents)\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    from werkzeug.exceptions import Forbidden\n    cleaned_path = environ.get('PATH_INFO', '').strip('/')\n    for sep in (os.sep, os.altsep):\n        if (sep and (sep != '/')):\n            cleaned_path = cleaned_path.replace(sep, '/')\n    path = '/'.join(([''] + [x for x in cleaned_path.split('/') if (x and (x != '..'))]))\n    file_loader = None\n    flag = False\n    for (search_path, loader) in self.exports.iteritems():\n        if (search_path == path):\n            flag = True\n            (real_filename, file_loader) = loader(None)\n            if (file_loader is not None):\n                break\n        if (not search_path.endswith('/')):\n            search_path += '/'\n        if path.startswith(search_path):\n            flag = True\n            (real_filename, file_loader) = loader(path[len(search_path):])\n            if (file_loader is not None):\n                break\n    if (file_loader is None):\n        if flag:\n            return real_filename(environ, start_response)\n        else:\n            return self.app(environ, start_response)\n    if (not self.is_allowed(real_filename)):\n        return Forbidden(('You can not visit the file %s.' % real_filename))(environ, start_response)\n    res = filedown(environ, real_filename, self.cache, self.cache_timeout)\n    return res(environ, start_response)\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.re:\n        result = self.re.match(instring, loc)\n        if (not result):\n            exc = self.myException\n            exc.loc = loc\n            exc.pstr = instring\n            raise exc\n        loc = result.end()\n        return (loc, result.group())\n    if (not (instring[loc] in self.initChars)):\n        exc = self.myException\n        exc.loc = loc\n        exc.pstr = instring\n        raise exc\n    start = loc\n    loc += 1\n    instrlen = len(instring)\n    bodychars = self.bodyChars\n    maxloc = (start + self.maxLen)\n    maxloc = min(maxloc, instrlen)\n    while ((loc < maxloc) and (instring[loc] in bodychars)):\n        loc += 1\n    throwException = False\n    if ((loc - start) < self.minLen):\n        throwException = True\n    if (self.maxSpecified and (loc < instrlen) and (instring[loc] in bodychars)):\n        throwException = True\n    if self.asKeyword:\n        if (((start > 0) and (instring[(start - 1)] in bodychars)) or ((loc < instrlen) and (instring[loc] in bodychars))):\n            throwException = True\n    if throwException:\n        exc = self.myException\n        exc.loc = loc\n        exc.pstr = instring\n        raise exc\n    return (loc, instring[start:loc])\n", "label": 1}
{"function": "\n\ndef writeline(self, line):\n    'print a line of python, indenting it according to the current\\n        indent level.\\n\\n        this also adjusts the indentation counter according to the\\n        content of the line.\\n\\n        '\n    if (not self.in_indent_lines):\n        self._flush_adjusted_lines()\n        self.in_indent_lines = True\n    if ((line is None) or re.match('^\\\\s*#', line) or re.match('^\\\\s*$', line)):\n        hastext = False\n    else:\n        hastext = True\n    is_comment = (line and len(line) and (line[0] == '#'))\n    if ((not is_comment) and ((not hastext) or self._is_unindentor(line))):\n        if (self.indent > 0):\n            self.indent -= 1\n            if (len(self.indent_detail) == 0):\n                raise exceptions.SyntaxException('Too many whitespace closures')\n            self.indent_detail.pop()\n    if (line is None):\n        return\n    self.stream.write((self._indent_line(line) + '\\n'))\n    if re.search(':[ \\\\t]*(?:#.*)?$', line):\n        match = re.match('^\\\\s*(if|try|elif|while|for|with)', line)\n        if match:\n            indentor = match.group(1)\n            self.indent += 1\n            self.indent_detail.append(indentor)\n        else:\n            indentor = None\n            m2 = re.match('^\\\\s*(def|class|else|elif|except|finally)', line)\n            if m2:\n                self.indent += 1\n                self.indent_detail.append(indentor)\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.re:\n        result = self.re.match(instring, loc)\n        if (not result):\n            raise ParseException(instring, loc, self.errmsg, self)\n        loc = result.end()\n        return (loc, result.group())\n    if (not (instring[loc] in self.initChars)):\n        raise ParseException(instring, loc, self.errmsg, self)\n    start = loc\n    loc += 1\n    instrlen = len(instring)\n    bodychars = self.bodyChars\n    maxloc = (start + self.maxLen)\n    maxloc = min(maxloc, instrlen)\n    while ((loc < maxloc) and (instring[loc] in bodychars)):\n        loc += 1\n    throwException = False\n    if ((loc - start) < self.minLen):\n        throwException = True\n    if (self.maxSpecified and (loc < instrlen) and (instring[loc] in bodychars)):\n        throwException = True\n    if self.asKeyword:\n        if (((start > 0) and (instring[(start - 1)] in bodychars)) or ((loc < instrlen) and (instring[loc] in bodychars))):\n            throwException = True\n    if throwException:\n        raise ParseException(instring, loc, self.errmsg, self)\n    return (loc, instring[start:loc])\n", "label": 1}
{"function": "\n\ndef _get_price_id_for_upgrade(self, package_items, option, value, public=True):\n    'Find the price id for the option and value to upgrade.\\n\\n        :param list package_items: Contains all the items related to an VS\\n        :param string option: Describes type of parameter to be upgraded\\n        :param int value: The value of the parameter to be upgraded\\n        :param bool public: CPU will be in Private/Public Node.\\n        '\n    option_category = {\n        'memory': 'ram',\n        'cpus': 'guest_core',\n        'nic_speed': 'port_speed',\n    }\n    category_code = option_category[option]\n    for item in package_items:\n        is_private = str(item['description']).startswith('Private')\n        for price in item['prices']:\n            if (('locationGroupId' in price) and price['locationGroupId']):\n                continue\n            if ('categories' not in price):\n                continue\n            categories = price['categories']\n            for category in categories:\n                if (not ((category['categoryCode'] == category_code) and (str(item['capacity']) == str(value)))):\n                    continue\n                if (option == 'cpus'):\n                    if (public and (not is_private)):\n                        return price['id']\n                    elif ((not public) and is_private):\n                        return price['id']\n                elif (option == 'nic_speed'):\n                    if ('Public' in item['description']):\n                        return price['id']\n                else:\n                    return price['id']\n", "label": 1}
{"function": "\n\ndef do_privmsg_notice(self, irc, msg):\n    channel = msg.args[0]\n    if (not irc.isChannel(channel)):\n        return\n    if self.registryValue('enable', channel):\n        actions = []\n        results = []\n        for channel in set(map(plugins.getChannel, (channel, 'global'))):\n            db = self.getDb(channel)\n            cursor = db.cursor()\n            cursor.execute('SELECT regexp, action FROM triggers')\n            results.extend([((channel,) + x) for x in cursor.fetchall()])\n        if (len(results) == 0):\n            return\n        max_triggers = self.registryValue('maxTriggers', channel)\n        for (channel, regexp, action) in results:\n            for match in re.finditer(regexp, msg.args[1]):\n                if (match is not None):\n                    thisaction = action\n                    self._updateRank(channel, regexp)\n                    for (i, j) in enumerate(match.groups()):\n                        if match.group((i + 1)):\n                            thisaction = re.sub(('\\\\$' + str((i + 1))), match.group((i + 1)), thisaction)\n                    actions.append(thisaction)\n                    if ((max_triggers != 0) and (max_triggers == len(actions))):\n                        break\n            if ((max_triggers != 0) and (max_triggers == len(actions))):\n                break\n        for action in actions:\n            self._runCommandFunction(irc, msg, action)\n", "label": 1}
{"function": "\n\ndef _add_item(self, dim_vals, data, sort=True, update=True):\n    '\\n        Adds item to the data, applying dimension types and ensuring\\n        key conforms to Dimension type and values.\\n        '\n    if (not isinstance(dim_vals, tuple)):\n        dim_vals = (dim_vals,)\n    self._item_check(dim_vals, data)\n    dim_types = zip(self._cached_index_types, dim_vals)\n    dim_vals = tuple(((v if (None in [t, v]) else t(v)) for (t, v) in dim_types))\n    if self._cached_categorical:\n        valid_vals = zip(self.kdims, dim_vals)\n    else:\n        valid_vals = []\n    for (dim, val) in valid_vals:\n        vals = self._cached_index_values[dim.name]\n        if (vals == 'initial'):\n            self._cached_index_values[dim.name] = []\n        if ((not self._instantiated) and (self.get_dimension(dim).values == 'initial')):\n            if (val not in vals):\n                self._cached_index_values[dim.name].append(val)\n        elif (vals and (val not in vals)):\n            raise KeyError(('%s dimension value %s not in specified dimension values.' % (dim, repr(val))))\n    if (update and (dim_vals in self.data) and isinstance(self.data[dim_vals], (MultiDimensionalMapping, OrderedDict))):\n        self.data[dim_vals].update(data)\n    else:\n        self.data[dim_vals] = data\n    if sort:\n        self._resort()\n", "label": 1}
{"function": "\n\ndef convert_values(self, value, field):\n    \"\\n        Coerce the value returned by the database backend into a consistent\\n        type that is compatible with the field type.\\n\\n        In our case, cater for the fact that SQL Server < 2008 has no\\n        separate Date and Time data types.\\n        TODO: See how we'll handle this for SQL Server >= 2008\\n        \"\n    if (value is None):\n        return None\n    if (field and (field.get_internal_type() == 'DateTimeField')):\n        return value\n    elif (field and (field.get_internal_type() == 'DateField')):\n        value = value.date()\n    elif ((field and (field.get_internal_type() == 'TimeField')) or (isinstance(value, datetime.datetime) and (value.year == 1900) and (value.month == value.day == 1))):\n        value = value.time()\n    elif (isinstance(value, datetime.datetime) and (value.hour == value.minute == value.second == value.microsecond == 0)):\n        value = value.date()\n    elif ((value is not None) and field and (field.get_internal_type() == 'FloatField')):\n        value = float(value)\n    return value\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.mid = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I64):\n                self.createdTime = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 10):\n            if (ftype == TType.LIST):\n                self.contacts = []\n                (_etype35, _size32) = iprot.readListBegin()\n                for _i36 in xrange(_size32):\n                    _elem37 = Contact()\n                    _elem37.read(iprot)\n                    self.contacts.append(_elem37)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 31):\n            if (ftype == TType.BOOL):\n                self.notificationDisabled = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef from_etree(self, data):\n    '\\n        Not the smartest deserializer on the planet. At the request level,\\n        it first tries to output the deserialized subelement called \"object\"\\n        or \"objects\" and falls back to deserializing based on hinted types in\\n        the XML element attribute \"type\".\\n        '\n    if (data.tag == 'request'):\n        elements = data.getchildren()\n        for element in elements:\n            if (element.tag in ('object', 'objects')):\n                return self.from_etree(element)\n        return dict(((element.tag, self.from_etree(element)) for element in elements))\n    elif ((data.tag == 'object') or (data.get('type') == 'hash')):\n        return dict(((element.tag, self.from_etree(element)) for element in data.getchildren()))\n    elif ((data.tag == 'objects') or (data.get('type') == 'list')):\n        return [self.from_etree(element) for element in data.getchildren()]\n    else:\n        type_string = data.get('type')\n        if (type_string in ('string', None)):\n            return data.text\n        elif (type_string == 'integer'):\n            return int(data.text)\n        elif (type_string == 'float'):\n            return float(data.text)\n        elif (type_string == 'boolean'):\n            if (data.text == 'True'):\n                return True\n            else:\n                return False\n        else:\n            return None\n", "label": 1}
{"function": "\n\ndef _prepare_fills(self):\n    fills = {\n        \n    }\n    index = 2\n    fills['0:0:0'] = 0\n    fills['17:0:0'] = 1\n    for xf_format in self.dxf_formats:\n        if (xf_format.pattern or xf_format.bg_color or xf_format.fg_color):\n            xf_format.has_dxf_fill = 1\n            xf_format.dxf_bg_color = xf_format.bg_color\n            xf_format.dxf_fg_color = xf_format.fg_color\n    for xf_format in self.xf_formats:\n        if ((xf_format.pattern == 1) and (xf_format.bg_color != 0) and (xf_format.fg_color != 0)):\n            tmp = xf_format.fg_color\n            xf_format.fg_color = xf_format.bg_color\n            xf_format.bg_color = tmp\n        if ((xf_format.pattern <= 1) and (xf_format.bg_color != 0) and (xf_format.fg_color == 0)):\n            xf_format.fg_color = xf_format.bg_color\n            xf_format.bg_color = 0\n            xf_format.pattern = 1\n        if ((xf_format.pattern <= 1) and (xf_format.bg_color == 0) and (xf_format.fg_color != 0)):\n            xf_format.bg_color = 0\n            xf_format.pattern = 1\n        key = xf_format._get_fill_key()\n        if (key in fills):\n            xf_format.fill_index = fills[key]\n            xf_format.has_fill = 0\n        else:\n            fills[key] = index\n            xf_format.fill_index = index\n            xf_format.has_fill = 1\n            index += 1\n    self.fill_count = index\n", "label": 1}
{"function": "\n\ndef mpf_sum(xs, prec=0, rnd=round_fast, absolute=False):\n    '\\n    Sum a list of mpf values efficiently and accurately\\n    (typically no temporary roundoff occurs). If prec=0,\\n    the final result will not be rounded either.\\n\\n    There may be roundoff error or cancellation if extremely\\n    large exponent differences occur.\\n\\n    With absolute=True, sums the absolute values.\\n    '\n    man = 0\n    exp = 0\n    max_extra_prec = ((prec * 2) or 1000000)\n    special = None\n    for x in xs:\n        (xsign, xman, xexp, xbc) = x\n        if xman:\n            if (xsign and (not absolute)):\n                xman = (- xman)\n            delta = (xexp - exp)\n            if (xexp >= exp):\n                if ((delta > max_extra_prec) and ((not man) or ((delta - bitcount(abs(man))) > max_extra_prec))):\n                    man = xman\n                    exp = xexp\n                else:\n                    man += (xman << delta)\n            else:\n                delta = (- delta)\n                if ((delta - xbc) > max_extra_prec):\n                    if (not man):\n                        (man, exp) = (xman, xexp)\n                else:\n                    man = ((man << delta) + xman)\n                    exp = xexp\n        elif xexp:\n            if absolute:\n                x = mpf_abs(x)\n            special = mpf_add((special or fzero), x, 1)\n    if special:\n        return special\n    return from_man_exp(man, exp, prec, rnd)\n", "label": 1}
{"function": "\n\ndef _pad_2d(subset, side, length):\n    new_set = []\n    max_len = (max([len(x) for (x, _) in subset]) if (length == (- 1)) else length)\n    for (x, y) in subset:\n        if (len(y) > max_len):\n            y = y[:max_len]\n        elif (len(y) < max_len):\n            if (side == 'left'):\n                y = ([0 for _ in range((max_len - len(y)))] + y)\n            elif (side == 'right'):\n                y = (y + [0 for _ in range((max_len - len(y)))])\n        if (len(x) > max_len):\n            x = x[:max_len]\n        elif (len(x) < max_len):\n            if (side == 'left'):\n                x = ([0 for _ in range((max_len - len(x)))] + x)\n            elif (side == 'right'):\n                x = (x + [0 for _ in range((max_len - len(x)))])\n        new_set.append((x, y))\n    return new_set\n", "label": 1}
{"function": "\n\ndef evaluateAtomicValue(self, exprStack, type, contextItem=None):\n    if (exprStack and (len(exprStack) > 0) and isinstance(exprStack[0], ProgHeader)):\n        progHeader = exprStack[0]\n        result = self.atomize(progHeader, self.evaluate(exprStack, contextItem=contextItem))\n        if (isinstance(type, QName) and (type.namespaceURI == XbrlConst.xsd)):\n            type = ('xs:' + type.localName)\n        if isinstance(type, str):\n            (prefix, sep, localName) = type.rpartition(':')\n            if (prefix == 'xs'):\n                if localName.endswith('*'):\n                    localName = localName[:(- 1)]\n                if isinstance(result, (tuple, list, set)):\n                    from arelle import FunctionXs\n                    if type.endswith('*'):\n                        return [FunctionXs.call(self, progHeader, localName, (r,)) for r in result]\n                    elif (len(result) > 0):\n                        return FunctionXs.call(self, progHeader, localName, (result[0],))\n            elif localName.startswith('item()'):\n                return result\n        elif (len(result) == 0):\n            return None\n        elif (len(result) == 1):\n            return result[0]\n        else:\n            return result\n    return None\n", "label": 1}
{"function": "\n\ndef _getarray(self, name=None, correct=True):\n    i = None\n    arr = None\n    for (ip, pname) in enumerate(self.pos_desc):\n        if (name.lower() == pname.lower()):\n            return self.pos[ip]\n    if ('mca' in name):\n        name = name.replace('(', '').replace(')', '')\n        words = name.replace('mca', '@@').split('@@', 2)\n        name = words[0].strip().lower()\n        mca = int(words[1])\n        if (len(self.det_mcas) < 1):\n            self.det_mcas = [None for i in self.det_desc]\n            for (idet, addr) in enumerate(self.det_addr):\n                a = addr.lower().split('.')[0]\n                if ('mca' in a):\n                    w = a.split('mca')[1]\n                    self.det_mcas[idet] = int(w)\n        print('MCAS: ', mca, self.det_mcas)\n        for (idet, nam) in enumerate(self.det_desc):\n            name1 = nam.strip().lower()\n            if ((name == name1) and (mca == self.det_mcas[idet])):\n                i = idet\n                break\n        arr = self.det\n        if correct:\n            arr = self.det_corr\n    elif (name in self.sums_names):\n        i = self.sums_names.index(name)\n        arr = self.sums\n        if correct:\n            arr = self.sums_corr\n    else:\n        i = self.match_detector_name(name)\n        arr = self.det\n        if correct:\n            arr = self.det_corr\n    if (i is not None):\n        return arr[i]\n    return None\n", "label": 1}
{"function": "\n\ndef _find_max_corrs(all_maps, target, threshold):\n    'Compute correlations between template and target components'\n    all_corrs = [compute_corr(target, subj.T) for subj in all_maps]\n    abs_corrs = [np.abs(a) for a in all_corrs]\n    corr_polarities = [np.sign(a) for a in all_corrs]\n    if (threshold <= 1):\n        max_corrs = [list(np.nonzero((s_corr > threshold))[0]) for s_corr in abs_corrs]\n    else:\n        max_corrs = [list(find_outliers(s_corr, threshold=threshold)) for s_corr in abs_corrs]\n    am = [l[i] for (l, i_s) in zip(abs_corrs, max_corrs) for i in i_s]\n    median_corr_with_target = (np.median(am) if (len(am) > 0) else 0)\n    polarities = [l[i] for (l, i_s) in zip(corr_polarities, max_corrs) for i in i_s]\n    maxmaps = [l[i] for (l, i_s) in zip(all_maps, max_corrs) for i in i_s]\n    if (len(maxmaps) == 0):\n        return ([], 0, 0, [])\n    newtarget = np.zeros(maxmaps[0].size)\n    std_of_maps = np.std(np.asarray(maxmaps))\n    mean_of_maps = np.std(np.asarray(maxmaps))\n    for (maxmap, polarity) in zip(maxmaps, polarities):\n        newtarget += (((maxmap / std_of_maps) - mean_of_maps) * polarity)\n    newtarget /= len(maxmaps)\n    newtarget *= std_of_maps\n    sim_i_o = np.abs(np.corrcoef(target, newtarget)[(1, 0)])\n    return (newtarget, median_corr_with_target, sim_i_o, max_corrs)\n", "label": 1}
{"function": "\n\ndef _cliques_heuristic(G, H, k, min_density):\n    h_cnumber = nx.core_number(H)\n    for (i, c_value) in enumerate(sorted(set(h_cnumber.values()), reverse=True)):\n        cands = set((n for (n, c) in h_cnumber.items() if (c == c_value)))\n        if (i == 0):\n            overlap = False\n        else:\n            overlap = set.intersection(*[set((x for x in H[n] if (x not in cands))) for n in cands])\n        if (overlap and (len(overlap) < k)):\n            SH = H.subgraph((cands | overlap))\n        else:\n            SH = H.subgraph(cands)\n        sh_cnumber = nx.core_number(SH)\n        SG = nx.k_core(G.subgraph(SH), k)\n        while (not (_same(sh_cnumber) and (nx.density(SH) >= min_density))):\n            SH = H.subgraph(SG)\n            if (len(SH) <= k):\n                break\n            sh_cnumber = nx.core_number(SH)\n            sh_deg = dict(SH.degree())\n            min_deg = min(sh_deg.values())\n            SH.remove_nodes_from((n for (n, d) in sh_deg.items() if (d == min_deg)))\n            SG = nx.k_core(G.subgraph(SH), k)\n        else:\n            (yield SG)\n", "label": 1}
{"function": "\n\ndef asXML(self, doctag=None, namedItemsOnly=False, indent='', formatted=True):\n    'Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.'\n    nl = '\\n'\n    out = []\n    namedItems = dict(((v[1], k) for (k, vlist) in self.__tokdict.items() for v in vlist))\n    nextLevelIndent = (indent + '  ')\n    if (not formatted):\n        indent = ''\n        nextLevelIndent = ''\n        nl = ''\n    selfTag = None\n    if (doctag is not None):\n        selfTag = doctag\n    elif self.__name:\n        selfTag = self.__name\n    if (not selfTag):\n        if namedItemsOnly:\n            return ''\n        else:\n            selfTag = 'ITEM'\n    out += [nl, indent, '<', selfTag, '>']\n    for (i, res) in enumerate(self.__toklist):\n        if isinstance(res, ParseResults):\n            if (i in namedItems):\n                out += [res.asXML(namedItems[i], (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n            else:\n                out += [res.asXML(None, (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n        else:\n            resTag = None\n            if (i in namedItems):\n                resTag = namedItems[i]\n            if (not resTag):\n                if namedItemsOnly:\n                    continue\n                else:\n                    resTag = 'ITEM'\n            xmlBodyText = _xml_escape(_ustr(res))\n            out += [nl, nextLevelIndent, '<', resTag, '>', xmlBodyText, '</', resTag, '>']\n    out += [nl, indent, '</', selfTag, '>']\n    return ''.join(out)\n", "label": 1}
{"function": "\n\ndef aspectValue(self, xpCtx, aspect, inherit=False):\n    if (aspect == Aspect.DIMENSIONS):\n        dims = set((self.prefixedNameQname(e.get('dimension')) for e in XmlUtil.children(self, XbrlConst.euRend, 'explicitDimCoord')))\n        if (inherit and (self.parentDefinitionNode is not None)):\n            dims |= self.parentDefinitionNode.aspectValue(None, aspect, inherit)\n        return dims\n    if (inherit and (not self.hasAspect(None, aspect))):\n        if (self.parentDefinitionNode is not None):\n            return self.parentDefinitionNode.aspectValue(None, aspect, inherit)\n        return None\n    if (aspect == Aspect.CONCEPT):\n        priItem = XmlUtil.childAttr(self, XbrlConst.euRend, 'primaryItem', 'name')\n        if (priItem is not None):\n            return self.prefixedNameQname(priItem)\n        return None\n    elif (aspect == Aspect.PERIOD_TYPE):\n        if XmlUtil.hasChild(self, XbrlConst.euRend, 'timeReference'):\n            return 'instant'\n    elif (aspect == Aspect.INSTANT):\n        return XmlUtil.datetimeValue(XmlUtil.childAttr(self, XbrlConst.euRend, 'timeReference', 'instant'), addOneDay=True)\n    elif isinstance(aspect, QName):\n        for e in XmlUtil.children(self, XbrlConst.euRend, 'explicitDimCoord'):\n            if (self.prefixedNameQname(e.get('dimension')) == aspect):\n                return self.prefixedNameQname(e.get('value'))\n    return None\n", "label": 1}

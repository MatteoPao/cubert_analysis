{"function": "\n\ndef test_s3_domain_with_default_root_object(self):\n    cmdline = ((self.prefix + '--origin-domain-name foo.s3.amazonaws.com ') + '--default-root-object index.html')\n    result = {\n        'DistributionConfig': {\n            'Origins': {\n                'Quantity': 1,\n                'Items': [{\n                    'S3OriginConfig': mock.ANY,\n                    'DomainName': 'foo.s3.amazonaws.com',\n                    'Id': mock.ANY,\n                    'OriginPath': '',\n                }],\n            },\n            'CallerReference': mock.ANY,\n            'Comment': '',\n            'Enabled': True,\n            'DefaultCacheBehavior': mock.ANY,\n            'DefaultRootObject': 'index.html',\n        },\n    }\n    self.run_cmd(cmdline)\n    self.assertEqual(self.last_kwargs, result)\n", "label": 0}
{"function": "\n\ndef pages_dynamic_tree_menu(context, page, url='/'):\n    '\\n    Render a \"dynamic\" tree menu, with all nodes expanded which are either\\n    ancestors or the current page itself.\\n\\n    Override ``pages/dynamic_tree_menu.html`` if you want to change the\\n    design.\\n\\n    :param page: the current page\\n    :param url: not used anymore\\n    '\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    children = None\n    if (page and ('current_page' in context)):\n        current_page = context['current_page']\n        if ((page.tree_id == current_page.tree_id) and (page.lft <= current_page.lft) and (page.rght >= current_page.rght)):\n            children = page.get_children_for_frontend()\n    context.update({\n        'children': children,\n        'page': page,\n    })\n    return context\n", "label": 0}
{"function": "\n\ndef remove(path, recursive=False, use_sudo=False):\n    '\\n    Remove a file or directory\\n    '\n    func = ((use_sudo and run_as_root) or run)\n    options = ('-r ' if recursive else '')\n    func('/bin/rm {0}{1}'.format(options, quote(path)))\n", "label": 0}
{"function": "\n\ndef marshal_dump(code, f):\n    if isinstance(f, file):\n        marshal.dump(code, f)\n    else:\n        f.write(marshal.dumps(code))\n", "label": 0}
{"function": "\n\ndef test_show_body(self):\n    client = Mock()\n    client.indices.get_settings.return_value = testvars.settings_one\n    client.cluster.state.return_value = testvars.clu_state_one\n    client.indices.stats.return_value = testvars.stats_one\n    ilo = curator.IndexList(client)\n    ao = curator.Alias(name='alias')\n    ao.remove(ilo)\n    ao.add(ilo)\n    body = ao.body()\n    self.assertEqual(testvars.alias_one_body['actions'][0], body['actions'][0])\n    self.assertEqual(testvars.alias_one_body['actions'][1], body['actions'][1])\n", "label": 0}
{"function": "\n\n@mock.patch('SoftLayer.API.BaseClient.iter_call')\ndef test_iterate(self, _iter_call):\n    self.client['SERVICE'].METHOD(iter=True)\n    _iter_call.assert_called_with('SERVICE', 'METHOD')\n", "label": 0}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    try:\n        self._call(*args, **kwargs)\n    except Exception as err:\n        stats.incr('callback-failure', 1)\n        logging.exception('Callback Failed: %s', err)\n", "label": 0}
{"function": "\n\n@register.filter\ndef lookup(h, key):\n    try:\n        return h[key]\n    except KeyError:\n        return ''\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    return self.build('add', self, other)\n", "label": 0}
{"function": "\n\n@classmethod\ndef read(cls, handle):\n    self = cls()\n    self._read_luminosity(handle)\n    self.name = handle.attrs['name'].decode('utf-8')\n    self.peeloff = str2bool(handle.attrs['peeloff'])\n    if (handle.attrs['spectrum'] == b'spectrum'):\n        self.spectrum = Table(np.array(handle['spectrum']))\n    elif (handle.attrs['spectrum'] == b'temperature'):\n        self.temperature = handle.attrs['temperature']\n    elif (handle.attrs['spectrum'] == b'lte'):\n        pass\n    else:\n        raise ValueError(('Unexpected value for `spectrum`: %s' % handle.attrs['spectrum']))\n    return self\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    category = resolve(self.category, context)\n    if isinstance(category, CategoryBase):\n        cat = category\n    else:\n        cat = get_category(category, self.model)\n    try:\n        if (cat is not None):\n            context[self.varname] = drilldown_tree_for_node(cat)\n        else:\n            context[self.varname] = []\n    except:\n        context[self.varname] = []\n    return ''\n", "label": 0}
{"function": "\n\ndef __init__(self, manufacturer, data):\n    self.manufacturer = manufacturer\n    self.data = list(data)\n", "label": 0}
{"function": "\n\ndef process_response(self, request, response):\n    from django.conf import settings\n    from rest_framework.exceptions import PermissionDenied\n    from .access_control import upload_prefix_for_request\n    cookie_name = getattr(settings, 'UPLOAD_PREFIX_COOKIE_NAME', 'upload_prefix')\n    try:\n        response.set_cookie(cookie_name, upload_prefix_for_request(request))\n    except PermissionDenied:\n        response.delete_cookie(cookie_name)\n    return response\n", "label": 0}
{"function": "\n\ndef createLineSet(self, indices, inputlist, materialid):\n    'Create a set of lines for use in this geometry instance.\\n\\n        :param numpy.array indices:\\n          unshaped numpy array that contains the indices for\\n          the inputs referenced in inputlist\\n        :param collada.source.InputList inputlist:\\n          The inputs for this primitive\\n        :param str materialid:\\n          A string containing a symbol that will get used to bind this lineset\\n          to a material when instantiating into a scene\\n\\n        :rtype: :class:`collada.lineset.LineSet`\\n        '\n    inputdict = primitive.Primitive._getInputsFromList(self.collada, self.sourceById, inputlist.getList())\n    return lineset.LineSet(inputdict, materialid, indices)\n", "label": 0}
{"function": "\n\ndef stepSlice(self, offset):\n    ' Move the selected structure one slice up or down\\n        :param offset: +1 or -1\\n        :return:\\n        '\n    volumeId = self.volumeSelector.currentNodeId\n    if (volumeId == ''):\n        self.showUnselectedVolumeWarningMessage()\n        return\n    selectedStructure = self.getCurrentSelectedStructure()\n    if (selectedStructure == self.logic.NONE):\n        self.showUnselectedStructureWarningMessage()\n        return\n    if (selectedStructure == self.logic.BOTH):\n        self.logic.stepSlice(volumeId, self.logic.AORTA, offset)\n        newSlice = self.logic.stepSlice(volumeId, self.logic.PA, offset)\n    else:\n        newSlice = self.logic.stepSlice(volumeId, selectedStructure, offset)\n    self.moveRedWindowToSlice(newSlice)\n", "label": 0}
{"function": "\n\ndef normalize_diff_filename(self, filename):\n    \"Normalize filenames in diffs.\\n\\n        The default behavior of stripping off leading slashes doesn't work for\\n        Perforce (because depot paths start with //), so this overrides it to\\n        just return the filename un-molested.\\n        \"\n    return filename\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/item/shared_item_chair_metal_s1.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef test_content_url_encoding_safe(self):\n    s = Site(self.SITE_PATH, config=self.config)\n    s.load()\n    path = '\".jpg/abc'\n    print(s.content_url(path, ''))\n    print(('/' + quote(path, '')))\n    assert (s.content_url(path, '') == ('/' + quote(path, '')))\n", "label": 0}
{"function": "\n\ndef post_undelete(self, *args, **kwargs):\n    self.post_undelete_called = True\n", "label": 0}
{"function": "\n\ndef timecolon(data):\n    match = re.search('(\\\\d+:\\\\d+:\\\\d+):(\\\\d+)', data)\n    return ('%s,%s' % (match.group(1), match.group(2)))\n", "label": 0}
{"function": "\n\ndef getUpdatedBatchJob(self, maxWait):\n    while True:\n        try:\n            (jobID, status, wallTime) = self.updatedJobsQueue.get(timeout=maxWait)\n        except Empty:\n            return None\n        try:\n            self.runningJobs.remove(jobID)\n        except KeyError:\n            pass\n        else:\n            return (jobID, status, wallTime)\n", "label": 0}
{"function": "\n\ndef package_private_devel_path(self, package):\n    'The path to the linked devel space for a given package.'\n    return os.path.join(self.private_devel_path, package.name)\n", "label": 0}
{"function": "\n\ndef test_getset_owner(self):\n    m = meta.Metadata()\n    o = m.get_owner('files/one')\n    m.set_owner('files/one', *o)\n", "label": 0}
{"function": "\n\ndef indent(string, prefix='    '):\n    '\\n    Indent every line of this string.\\n    '\n    return ''.join((('%s%s\\n' % (prefix, s)) for s in string.split('\\n')))\n", "label": 0}
{"function": "\n\ndef stories(self, task, params={\n    \n}, **options):\n    'Returns a compact representation of all of the stories on the task.\\n\\n        Parameters\\n        ----------\\n        task : {Id} The task containing the stories to get.\\n        [params] : {Object} Parameters for the request\\n        '\n    path = ('/tasks/%s/stories' % task)\n    return self.client.get_collection(path, params, **options)\n", "label": 0}
{"function": "\n\ndef get_context_types_value(context_id, source, filter_func, codebase):\n    query = CodeElementLink.objects.filter(code_element__kind__is_type=True).filter(index=0).filter(code_element__codebase=codebase).filter(code_reference__source=source)\n    query = filter_func(query, context_id)\n    context_types = []\n    pk_set = set()\n    for link in query.all():\n        code_element = link.code_element\n        if (code_element.pk not in pk_set):\n            context_types.append(code_element)\n            pk_set.add(code_element.pk)\n    return context_types\n", "label": 0}
{"function": "\n\ndef _validate_simple_authn(self, username, credentials):\n    '\\n        When the login() method is called, this method is used with the \\n        username and credentials (e.g., password, IP address, etc.). This\\n        method will only check the SimpleAuthn instances.\\n        '\n    try:\n        (login, role_name, user_auths) = self._db.retrieve_role_and_user_auths(username)\n    except DbUserNotFoundError:\n        return self._process_invalid()\n    errors = False\n    for user_auth in user_auths:\n        if user_auth.is_simple_authn():\n            try:\n                authenticated = user_auth.authenticate(login, credentials)\n            except:\n                log.log(LoginManager, log.level.Warning, ('Username: %s with user_auth %s: ERROR' % (login, user_auth)))\n                log.log_exc(LoginManager, log.level.Warning)\n                errors = True\n                traceback.print_exc()\n                continue\n            if authenticated:\n                log.log(LoginManager, log.level.Debug, ('Username: %s with user_auth %s: SUCCESS' % (login, user_auth)))\n                return ValidDatabaseSessionId(login, role_name)\n            else:\n                log.log(LoginManager, log.level.Warning, ('Username: %s with user_auth %s: FAIL' % (login, user_auth)))\n    if errors:\n        raise LoginErrors.LoginError('Error checking credentials. Contact administrators!')\n    return self._process_invalid()\n", "label": 0}
{"function": "\n\ndef test_elemwise_thats_also_a_column():\n    t = symbol('t', 'var * {x: int, time: datetime, y: int}')\n    expr = t[(t.x > 0)].time.truncate(months=1)\n    expected = t[['time', 'x']]\n    result = lean_projection(expr)\n    assert result._child._child._child.isidentical(t[['time', 'x']])\n", "label": 0}
{"function": "\n\ndef test_list(self):\n    lists = {('l%s' % i): list(string.ascii_lowercase[:i]) for i in range(1, 10)}\n    self.collections_common_tests(lists, 'l')\n", "label": 0}
{"function": "\n\ndef fit_transform(self, X, y):\n    'Fit and transform.'\n    self.fit(X, y)\n    return self.transform(X)\n", "label": 0}
{"function": "\n\ndef update_app(self, app_id, app, force=False, minimal=True):\n    'Update an app.\\n\\n        Applies writable settings in `app` to `app_id`\\n        Note: this method can not be used to rename apps.\\n\\n        :param str app_id: target application ID\\n        :param app: application settings\\n        :type app: :class:`marathon.models.app.MarathonApp`\\n        :param bool force: apply even if a deployment is in progress\\n        :param bool minimal: ignore nulls and empty collections\\n\\n        :returns: a dict containing the deployment id and version\\n        :rtype: dict\\n        '\n    app.version = None\n    params = {\n        'force': force,\n    }\n    data = app.to_json(minimal=minimal)\n    response = self._do_request('PUT', '/v2/apps/{app_id}'.format(app_id=app_id), params=params, data=data)\n    return response.json()\n", "label": 0}
{"function": "\n\ndef create_toplevel_ws(self, wsname, width, height, group=2, x=None, y=None):\n    root = self.app.make_window()\n    ws = self.make_ws(wsname, wstype='tabs')\n    vbox = Widgets.VBox()\n    vbox.set_border_width(0)\n    self._add_toolbar(vbox, ws)\n    vbox.add_widget(bnch.widget)\n    root.set_widget(vbox)\n    root.resize(width, height)\n    root.show()\n    self.toplevels.append(root)\n    if (x is not None):\n        root.move(x, y)\n    return bnch\n", "label": 0}
{"function": "\n\ndef get_queryset(self):\n    queryset = self.queryset\n    if isinstance(queryset, (QuerySet, Manager)):\n        queryset = queryset.all()\n    return queryset\n", "label": 0}
{"function": "\n\ndef get_current_editor(self):\n    page = self.notebook.get_current_page()\n    if (page is None):\n        return None\n    return page.get_text_widget()\n", "label": 0}
{"function": "\n\ndef test_error_load_single_field_type(single_schema):\n    (data, errors) = single_schema.load({\n        'child': {\n            'id': 'foo',\n        },\n    })\n    assert (not data)\n    assert (errors == {\n        'child': {\n            'id': [fields.Integer().error_messages['invalid']],\n        },\n    })\n", "label": 0}
{"function": "\n\ndef start(self, fileStore):\n    subprocess.check_call((self.cmd + ' 1'), shell=True)\n", "label": 0}
{"function": "\n\ndef test_reraise(self):\n    self.assertRaises(RuntimeError, reraise, RuntimeError, RuntimeError())\n    try:\n        raise RuntimeError('bla')\n    except Exception:\n        exc_info = sys.exc_info()\n    self.assertRaises(RuntimeError, reraise, *exc_info)\n", "label": 0}
{"function": "\n\ndef check_write_package(self, username, package_reference):\n    '\\n        username: User that request to write the package\\n        package_reference: PackageReference\\n        '\n    self.check_write_conan(username, package_reference.conan)\n", "label": 0}
{"function": "\n\ndef test_it_knows_how_many_total_errors_it_contains(self):\n    errors = [mock.MagicMock() for _ in range(8)]\n    tree = exceptions.ErrorTree(errors)\n    self.assertEqual(tree.total_errors, 8)\n", "label": 0}
{"function": "\n\ndef test_get_lock_multiple_coords(self):\n    member_id2 = self._get_random_uuid()\n    client2 = tooz.coordination.get_coordinator(self.url, member_id2)\n    client2.start()\n    lock_name = self._get_random_uuid()\n    lock = self._coord.get_lock(lock_name)\n    self.assertTrue(lock.acquire())\n    lock2 = client2.get_lock(lock_name)\n    self.assertFalse(lock2.acquire(blocking=False))\n    self.assertTrue(lock.release())\n    self.assertTrue(lock2.acquire(blocking=True))\n    self.assertTrue(lock2.release())\n", "label": 0}
{"function": "\n\ndef test_keys(self):\n    getkeys = self.ts.keys\n    self.assertIs(getkeys(), self.ts.index)\n", "label": 0}
{"function": "\n\ndef user_add_stage(request):\n    if (not request.user.has_perm('auth.change_user')):\n        raise PermissionDenied\n    manipulator = UserCreationForm()\n    if (request.method == 'POST'):\n        new_data = request.POST.copy()\n        errors = manipulator.get_validation_errors(new_data)\n        if (not errors):\n            new_user = manipulator.save(new_data)\n            msg = (_('The %(name)s \"%(obj)s\" was added successfully.') % {\n                'name': 'user',\n                'obj': new_user,\n            })\n            if request.POST.has_key('_addanother'):\n                request.user.message_set.create(message=msg)\n                return HttpResponseRedirect(request.path)\n            else:\n                request.user.message_set.create(message=((msg + ' ') + _('You may edit it again below.')))\n                return HttpResponseRedirect(('../%s/' % new_user.id))\n    else:\n        errors = new_data = {\n            \n        }\n    form = oldforms.FormWrapper(manipulator, new_data, errors)\n    return render_to_response('admin/auth/user/add_form.html', {\n        'title': _('Add user'),\n        'form': form,\n        'is_popup': request.REQUEST.has_key('_popup'),\n        'add': True,\n        'change': False,\n        'has_delete_permission': False,\n        'has_change_permission': True,\n        'has_file_field': False,\n        'has_absolute_url': False,\n        'auto_populated_fields': (),\n        'bound_field_sets': (),\n        'first_form_field_id': 'id_username',\n        'opts': User._meta,\n        'username_help_text': User._meta.get_field('username').help_text,\n    }, context_instance=template.RequestContext(request))\n", "label": 0}
{"function": "\n\ndef generateDictOperationInCode(to_name, expression, emit, context):\n    inverted = expression.isExpressionDictOperationNOTIn()\n    (dict_name, key_name) = generateChildExpressionsCode(expression=expression, emit=emit, context=context)\n    res_name = context.getIntResName()\n    emit(('%s = PyDict_Contains( %s, %s );' % (res_name, key_name, dict_name)))\n    getReleaseCodes(release_names=(dict_name, key_name), emit=emit, context=context)\n    getErrorExitBoolCode(condition=('%s == -1' % res_name), needs_check=expression.mayRaiseException(BaseException), emit=emit, context=context)\n    emit(('%s = BOOL_FROM( %s == %s );' % (to_name, res_name, ('1' if (not inverted) else '0'))))\n", "label": 0}
{"function": "\n\ndef Add(self, node):\n    self.binary('Add', node)\n", "label": 0}
{"function": "\n\ndef random_orthogonal(dim, special=True):\n    if (dim == 1):\n        if (np.random.uniform() < 0.5):\n            return np.ones((1, 1))\n        return (- np.ones((1, 1)))\n    P = np.random.randn(dim, dim)\n    while (np.linalg.matrix_rank(P) != dim):\n        P = np.random.randn(dim, dim)\n    (U, S, V) = np.linalg.svd(P)\n    P = np.dot(U, V)\n    if special:\n        if (np.linalg.det(P) < 0):\n            P[:, [0, 1]] = P[:, [1, 0]]\n    return P\n", "label": 0}
{"function": "\n\ndef register(self, observer):\n    ' Called when an observer wants to be notified\\n        about project changes\\n\\n        '\n    self._observers.append(observer)\n", "label": 0}
{"function": "\n\n@feature('cxx')\n@after('apply_lib_vars')\ndef apply_defines_cxx(self):\n    'after uselib is set for CXXDEFINES'\n    self.defines = getattr(self, 'defines', [])\n    lst = (self.to_list(self.defines) + self.to_list(self.env['CXXDEFINES']))\n    milst = []\n    for defi in lst:\n        if (not (defi in milst)):\n            milst.append(defi)\n    libs = self.to_list(self.uselib)\n    for l in libs:\n        val = self.env[('CXXDEFINES_' + l)]\n        if val:\n            milst += self.to_list(val)\n    self.env['DEFLINES'] = [('%s %s' % (x[0], Utils.trimquotes('='.join(x[1:])))) for x in [y.split('=') for y in milst]]\n    y = self.env['CXXDEFINES_ST']\n    self.env['_CXXDEFFLAGS'] = [(y % x) for x in milst]\n", "label": 0}
{"function": "\n\ndef __iadd__(self, other):\n    self.extend(other)\n    return self\n", "label": 0}
{"function": "\n\n@classmethod\ndef __subclasshook__(cls, other_cls):\n    if (cls is Tombola):\n        interface_names = function_names(cls)\n        found_names = set()\n        for a_cls in other_cls.__mro__:\n            found_names |= function_names(a_cls)\n        if (found_names >= interface_names):\n            return True\n    return NotImplemented\n", "label": 0}
{"function": "\n\ndef destroy(self):\n    ' Destroy the dock manager.\\n\\n        This method will free all of the resources held by the dock\\n        manager. The primary dock area and dock items will not be\\n        destroyed. After the method is called, the dock manager is\\n        invalid and should no longer be used.\\n\\n        '\n    for frame in self._dock_frames:\n        if isinstance(frame, QDockContainer):\n            frame.setDockItem(None)\n            frame.setParent(None, Qt.Widget)\n            frame.hide()\n    for frame in self._dock_frames:\n        if isinstance(frame, QDockWindow):\n            frame.setParent(None, Qt.Widget)\n            frame.hide()\n    for item in self._dock_items:\n        item._manager = None\n    self._dock_area.setCentralWidget(None)\n    self._dock_area.setMaximizedWidget(None)\n    del self._dock_area\n    del self._dock_frames\n    del self._dock_items\n    del self._proximity_handler\n    del self._container_monitor\n    del self._overlay\n", "label": 0}
{"function": "\n\ndef test_incr_sample_rate(self):\n    client = statsd.StatsdClient('localhost', 8125, prefix='', sample_rate=0.999)\n    client.incr('buck.counter', 5)\n    self.assertEqual(client._socket.data, b'buck.counter:5|c|@0.999')\n    if (client._socket.data != 'buck.counter:5|c'):\n        self.assertTrue(client._socket.data.endswith(b'|@0.999'))\n", "label": 0}
{"function": "\n\ndef format_author(self, entry):\n    try:\n        persons = entry.persons['author']\n        if (sys.version_info[0] == 2):\n            authors = [unicode(au) for au in persons]\n        elif (sys.version_info[0] == 3):\n            authors = [str(au) for au in persons]\n    except KeyError:\n        authors = ['']\n    authors = self.strip_chars('; '.join(authors))\n    return authors\n", "label": 0}
{"function": "\n\ndef test_delete_group_inuse_process(self):\n    url = ('/v1/groups/' + GID)\n    req = get_request(url, 'DELETE')\n    self.stubs.Set(db, 'keypair_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'securitygroup_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'network_get_all', fake_not_group_data_not_exists)\n    self.stubs.Set(db, 'process_get_all', fake_not_group_data_exists)\n    res = req.get_response(self.app)\n    self.assertEqual(res.status_code, 409)\n", "label": 0}
{"function": "\n\ndef synchro_connect(self):\n    try:\n        self.synchronize(self.delegate.open)()\n    except AutoReconnect as e:\n        raise ConnectionFailure(str(e))\n", "label": 0}
{"function": "\n\ndef test_validate_configuration_invalid_disk_type(self):\n    raid_config = json.loads(raid_constants.RAID_CONFIG_INVALID_DISK_TYPE)\n    self.assertRaises(exception.InvalidParameterValue, raid.validate_configuration, raid_config, raid_config_schema=self.schema)\n", "label": 0}
{"function": "\n\ndef test_notification_no_pause(self):\n    self.displayer.notification('message', 10)\n    string = self.mock_stdout.write.call_args[0][0]\n    self.assertTrue(('message' in string))\n", "label": 0}
{"function": "\n\ndef _augmented_orthonormal_cols(x, k):\n    (n, m) = x.shape\n    y = np.empty((n, (m + k)), dtype=x.dtype)\n    y[:, :m] = x\n    for i in range(k):\n        v = np.random.randn(n)\n        if np.iscomplexobj(x):\n            v = (v + (1j * np.random.randn(n)))\n        for j in range((m + i)):\n            u = y[:, j]\n            v -= ((np.dot(v, u.conj()) / np.dot(u, u.conj())) * u)\n        v /= np.sqrt(np.dot(v, v.conj()))\n        y[:, (m + i)] = v\n    return y\n", "label": 0}
{"function": "\n\ndef test_call_and_missing_check_with_obj_list(self):\n\n    def yield_hashes(device, partition, policy, suffixes=None, **kwargs):\n        if ((device == 'dev') and (partition == '9') and (suffixes == ['abc']) and (policy == POLICIES.legacy)):\n            (yield ('/srv/node/dev/objects/9/abc/9d41d8cd98f00b204e9800998ecf0abc', '9d41d8cd98f00b204e9800998ecf0abc', {\n                'ts_data': Timestamp(1380144470.0),\n            }))\n        else:\n            raise Exception(('No match for %r %r %r' % (device, partition, suffixes)))\n    job = {\n        'device': 'dev',\n        'partition': '9',\n        'policy': POLICIES.legacy,\n        'frag_index': 0,\n    }\n    self.sender = ssync_sender.Sender(self.daemon, None, job, ['abc'], ['9d41d8cd98f00b204e9800998ecf0abc'])\n    self.sender.connection = FakeConnection()\n    self.sender.response = FakeResponse(chunk_body=':MISSING_CHECK: START\\r\\n:MISSING_CHECK: END\\r\\n')\n    self.sender.daemon._diskfile_mgr.yield_hashes = yield_hashes\n    self.sender.connect = mock.MagicMock()\n    self.sender.updates = mock.MagicMock()\n    self.sender.disconnect = mock.MagicMock()\n    (success, candidates) = self.sender()\n    self.assertTrue(success)\n    self.assertEqual(candidates, dict([('9d41d8cd98f00b204e9800998ecf0abc', {\n        'ts_data': Timestamp(1380144470.0),\n    })]))\n    self.assertEqual(self.sender.failures, 0)\n", "label": 0}
{"function": "\n\ndef test_try_failure_bad_arg(self):\n    rv = self.app.get('/trivial_fn?nothing=1')\n    assert (rv.status_code == 200)\n    data = rv.data.decode('utf8')\n    jsn = json.loads(data)\n    assert (jsn['success'] == False), 'We expect this call failed as it has the wrong argument'\n", "label": 0}
{"function": "\n\ndef test_queryset_deleted_on(self):\n    'qs delete() sets deleted_on to same time as parent on cascade.'\n    p = self.F.ProductFactory.create()\n    s = self.F.SuiteFactory.create(product=p)\n    self.model.Product.objects.all().delete()\n    p = self.refresh(p)\n    s = self.refresh(s)\n    self.assertIsNot(p.deleted_on, None)\n    self.assertEqual(s.deleted_on, p.deleted_on)\n", "label": 0}
{"function": "\n\ndef __init__(self, status):\n    super(RCException, self).__init__(('RAMCloud error ' + str(status)))\n    self.status = status\n", "label": 0}
{"function": "\n\ndef test_delete_all_lines_inversed(self):\n    text = '1\\n22\\n3\\n44\\n5\\n66\\n'\n    self.fillAndClear(text)\n    self.buffer.delete(Range(6, 1))\n    assert (str(self.buffer) == '')\n    assert (self.buffer.lines == [])\n    assert (self.deleted('afterPosition') == Position(1, 1))\n    assert (self.deleted('startPosition') == Position(7, 1))\n", "label": 0}
{"function": "\n\ndef run_osprey(self, config):\n    '\\n        Run osprey-worker.\\n\\n        Parameters\\n        ----------\\n        config : str\\n            Configuration string.\\n        '\n    (fh, filename) = tempfile.mkstemp(dir=self.temp_dir)\n    with open(filename, 'wb') as f:\n        f.write(config)\n    args = Namespace(config=filename, n_iters=1, output='json')\n    execute_worker.execute(args, None)\n    dump = json.loads(execute_dump.execute(args, None))\n    assert (len(dump) == 1)\n    assert (dump[0]['status'] == 'SUCCEEDED'), dump[0]['status']\n", "label": 0}
{"function": "\n\ndef from_text(cls, rdclass, rdtype, tok, origin=None, relativize=True):\n    address = tok.get_string()\n    tok.get_eol()\n    return cls(rdclass, rdtype, address)\n", "label": 0}
{"function": "\n\ndef update_parser_common(self, parser):\n    parser.add_argument('network', metavar='<network>', nargs='+', help='Network(s) to delete (name or ID)')\n    return parser\n", "label": 0}
{"function": "\n\n@process_multiple\ndef to_html(self, values, fields, context):\n    toks = []\n    for value in values:\n        if (value in self.html_map):\n            tok = self.html_map[value]\n        elif (value is None):\n            continue\n        elif (type(value) is float):\n            tok = filters.floatformat(value)\n        else:\n            tok = unicode(value)\n        toks.append(tok)\n    return self.delimiter.join(toks)\n", "label": 0}
{"function": "\n\ndef test_dependency_sorting_4(self):\n    sorted_deps = sort_dependencies([('fixtures_regress', [Store, Person, Book])])\n    self.assertEqual(sorted_deps, [Store, Person, Book])\n", "label": 0}
{"function": "\n\ndef _eval_rewrite_as_gamma(self, x, k):\n    from sympy import gamma\n    return ((((- 1) ** k) * gamma((k - x))) / gamma((- x)))\n", "label": 0}
{"function": "\n\ndef _get_x(self):\n    if (len(self.names) > 1):\n        return ([self.__getattribute__(name) for name in self.names] + list(self.args))\n    return ([self.__getattribute__(self.names[0])] + list(self.args))\n", "label": 0}
{"function": "\n\ndef turn_off(self, **kwargs):\n    'Turn the device off/open the device.'\n    self.action_node.runElse()\n", "label": 0}
{"function": "\n\ndef test_set_rewrite(self):\n    '`LocalMemStorage` set method of existing key'\n    s = LocalMemStorage()\n    s.set('key', 'value')\n    s.set('key', 'value1')\n    self.assertEqual(s.storage['key'], 'value1')\n", "label": 0}
{"function": "\n\ndef do_create(self, max_size=0, dir=None, pre='', suf=''):\n    if (dir is None):\n        dir = tempfile.gettempdir()\n    file = tempfile.SpooledTemporaryFile(max_size=max_size, dir=dir, prefix=pre, suffix=suf)\n    return file\n", "label": 0}
{"function": "\n\ndef mapToJson(self, objects, writer):\n    writer.write(self.header)\n    writer.write('\\n')\n    for (ind, obj) in enumerate(objects):\n        if (ind > 0):\n            writer.write(',\\n')\n        else:\n            writer.write('\\n')\n        writer.write(self.jsonDumpser(self.objConverter(obj)))\n    writer.write(self.footer)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (self is other):\n        return True\n    if (not isinstance(other, collections.Sequence)):\n        raise TypeError('Can only compare repeated scalar fields against sequences.')\n    return (other == self[slice(None, None, None)])\n", "label": 0}
{"function": "\n\ndef test_download_file_proxies_to_transfer_object(self):\n    with mock.patch('boto3.s3.inject.S3Transfer') as transfer:\n        inject.download_file(mock.sentinel.CLIENT, Bucket='bucket', Key='key', Filename='filename')\n        transfer.return_value.download_file.assert_called_with(bucket='bucket', key='key', filename='filename', extra_args=None, callback=None)\n", "label": 0}
{"function": "\n\ndef RemoveMenu(self, menu):\n    ' Remove a wx menu from the Menu.\\n\\n        If the menu does not exist in the menu, this is a no-op.\\n\\n        Parameters\\n        ----------\\n        menu : wxMenu\\n            The wxMenu instance to remove from this menu.\\n\\n        '\n    all_items = self._all_items\n    if (menu in all_items):\n        all_items.remove(menu)\n        menu.Unbind(EVT_MENU_CHANGED, handler=self.OnMenuChanged)\n        menu_item = self._menus_map.pop(menu, None)\n        if (menu_item is not None):\n            self.RemoveItem(menu_item)\n            menu_item.SetSubMenu(None)\n", "label": 0}
{"function": "\n\ndef get_latest_dist():\n    lib = file(os.path.join('petlib', '__init__.py')).read()\n    v = re.findall('VERSION.*=.*[\\'\"](.*)[\\'\"]', lib)[0]\n    return os.path.join('dist', ('petlib-%s.tar.gz' % v))\n", "label": 0}
{"function": "\n\ndef dump(self):\n    out = []\n    for key in self._keys:\n        att_key = self._att_key(key)\n        value = self[att_key]\n        if hasattr(self, ('dump_%s' % att_key)):\n            value = getattr(self, ('dump_%s' % att_key))(value)\n        out.append(('%s: %s' % (key, value)))\n    return '\\n'.join(out)\n", "label": 0}
{"function": "\n\ndef get(self, request):\n    form = bforms.PasswordResetForm()\n    self.payload['form'] = form\n    return render(request, self.payload, 'registration/reset_password.html')\n", "label": 0}
{"function": "\n\ndef test_no_repeats(self):\n    with self.assertNumQueries(2):\n        authors = Author.objects.sql_calc_found_rows().sql_calc_found_rows()[:5]\n        list(authors)\n        assert (authors.found_rows == 10)\n", "label": 0}
{"function": "\n\ndef test_default_theme_is_empty(self):\n    doc = Document()\n    for (class_name, props) in doc.theme._json['attrs'].items():\n        self._compare_dict_to_model_defaults(props, class_name)\n    self.assertEqual(0, len(doc.theme._json['attrs']))\n    self._compare_dict_to_model_class_defaults(doc.theme._fill_defaults, FillProps)\n    self.assertEqual(0, len(doc.theme._fill_defaults))\n    self._compare_dict_to_model_class_defaults(doc.theme._text_defaults, TextProps)\n    self.assertEqual(0, len(doc.theme._text_defaults))\n    self._compare_dict_to_model_class_defaults(doc.theme._line_defaults, LineProps)\n    self.assertEqual(0, len(doc.theme._line_defaults))\n", "label": 0}
{"function": "\n\ndef test_reindex():\n    s = pd.Series([0.5, 1.0, 1.5], index=[2, 1, 3])\n    s2 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n    assert (list(reindex(s, s2).values) == [1.0, 0.5, 1.5])\n", "label": 0}
{"function": "\n\ndef default(self, obj):\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    return super(_JSONEncoder, self).default(obj)\n", "label": 0}
{"function": "\n\ndef _is_numeric(self, value):\n    try:\n        int(value)\n    except (TypeError, ValueError):\n        return False\n    return True\n", "label": 0}
{"function": "\n\ndef set_service_target(context, policy_target_id, relationship):\n    session = context.session\n    with session.begin(subtransactions=True):\n        owner = ServiceTarget(policy_target_id=policy_target_id, servicechain_instance_id=context.instance['id'], servicechain_node_id=context.current_node['id'], position=context.current_position, relationship=relationship)\n        session.add(owner)\n", "label": 0}
{"function": "\n\ndef __init__(self, r, color4):\n    super(ProbeQuad, self).__init__()\n    self.color4 = color4\n    self.vertexes = [(r, 0, 0), (0, r, 0), ((- r), 0, 0), (0, (- r), 0)]\n", "label": 0}
{"function": "\n\ndef format(self, value):\n    if isinstance(value, types.StringTypes):\n        return value\n    else:\n        return str(value)\n", "label": 0}
{"function": "\n\ndef find_item_before(menu, index=0):\n    _items = menu['menu'][:index][:]\n    _items.reverse()\n    for item in _items:\n        if item['enabled']:\n            return menu['menu'].index(item)\n    return find_item_before(menu, index=len(menu['menu']))\n", "label": 0}
{"function": "\n\ndef onWindowResized(self, width, height):\n    shortcutHeight = ((height - self.shortcuts.getAbsoluteTop()) - 8)\n    if (shortcutHeight < 1):\n        shortcutHeight = 1\n    self.shortcuts.setHeight(('%dpx' % shortcutHeight))\n    self.mailDetail.adjustSize(width, height)\n", "label": 0}
{"function": "\n\ndef test_ex_get_node_security_groups(self):\n    node = Node(id='1c01300f-ef97-4937-8f03-ac676d6234be', name=None, state=None, public_ips=None, private_ips=None, driver=self.driver)\n    security_groups = self.driver.ex_get_node_security_groups(node)\n    self.assertEqual(len(security_groups), 2, 'Wrong security groups count')\n    security_group = security_groups[1]\n    self.assertEqual(security_group.id, 4)\n    self.assertEqual(security_group.tenant_id, '68')\n    self.assertEqual(security_group.name, 'ftp')\n    self.assertEqual(security_group.description, 'FTP Client-Server - Open 20-21 ports')\n    self.assertEqual(security_group.rules[0].id, 1)\n    self.assertEqual(security_group.rules[0].parent_group_id, 4)\n    self.assertEqual(security_group.rules[0].ip_protocol, 'tcp')\n    self.assertEqual(security_group.rules[0].from_port, 20)\n    self.assertEqual(security_group.rules[0].to_port, 21)\n    self.assertEqual(security_group.rules[0].ip_range, '0.0.0.0/0')\n", "label": 0}
{"function": "\n\ndef test_call_chooses_correct_handler(self):\n    (sentinel1, sentinel2, sentinel3) = (object(), object(), object())\n    self.commands.add('foo')((lambda context: sentinel1))\n    self.commands.add('bar')((lambda context: sentinel2))\n    self.commands.add('baz')((lambda context: sentinel3))\n    self.assertEqual(sentinel1, self.commands.call(['foo']))\n    self.assertEqual(sentinel2, self.commands.call(['bar']))\n    self.assertEqual(sentinel3, self.commands.call(['baz']))\n", "label": 0}
{"function": "\n\ndef apply_linear(self, params, unknowns, dparams, dunknowns, dresids, mode):\n    \"\\n        Multiplies incoming vector by the Jacobian (fwd mode) or the\\n        transpose Jacobian (rev mode). If the user doesn't provide this\\n        method, then we just multiply by the cached jacobian.\\n\\n        Args\\n        ----\\n        params : `VecWrapper`\\n            `VecWrapper` containing parameters. (p)\\n\\n        unknowns : `VecWrapper`\\n            `VecWrapper` containing outputs and states. (u)\\n\\n        dparams : `VecWrapper`\\n            `VecWrapper` containing either the incoming vector in forward mode\\n            or the outgoing result in reverse mode. (dp)\\n\\n        dunknowns : `VecWrapper`\\n            In forward mode, this `VecWrapper` contains the incoming vector for\\n            the states. In reverse mode, it contains the outgoing vector for\\n            the states. (du)\\n\\n        dresids : `VecWrapper`\\n            `VecWrapper` containing either the outgoing result in forward mode\\n            or the incoming vector in reverse mode. (dr)\\n\\n        mode : string\\n            Derivative mode, can be 'fwd' or 'rev'.\\n        \"\n    self._apply_linear_jac(params, unknowns, dparams, dunknowns, dresids, mode)\n", "label": 0}
{"function": "\n\ndef test_context(self):\n    order = Order(name='Dummy Order')\n    order.save()\n    for i in range(10):\n        item = Item(name=('Item %i' % i), sku=(str(i) * 13), price=D('9.99'), order=order, status=0)\n        item.save()\n    res = self.client.get('/modelformset/simple/')\n    self.assertTrue(('object_list' in res.context))\n    self.assertEqual(len(res.context['object_list']), 10)\n", "label": 0}
{"function": "\n\ndef read_channel(model, channel_name, monitor_name='monitor'):\n    '\\n    Returns the last value recorded in a channel.\\n\\n    Parameters\\n    ----------\\n    model : Model\\n        The model to read the channel from\\n    channel_name : str\\n        The name of the channel to read from\\n    monitor_name : str, optional\\n        The name of the Monitor to read from\\n        (In case you want to read from an old Monitor moved by\\n        `push_monitor`)\\n\\n    Returns\\n    -------\\n    value : float\\n        The last value recorded in this monitoring channel\\n    '\n    return getattr(model, monitor_name).channels[channel_name].val_record[(- 1)]\n", "label": 0}
{"function": "\n\ndef test_add_listener_exception(self):\n    cap = [':candidate']\n    obj = Session(cap)\n    listener = Session(None)\n    with self.assertRaises(SessionError):\n        obj.add_listener(listener)\n", "label": 0}
{"function": "\n\ndef set_h_ffactor(self, *args, **kwargs):\n    return apply(self._cobj.set_h_ffactor, args, kwargs)\n", "label": 0}
{"function": "\n\ndef init_stroke(self, g, touch):\n    l = [touch.x, touch.y]\n    col = g.color\n    new_line = Line(points=l, width=self.line_width, group=g.id)\n    g._strokes[str(touch.uid)] = new_line\n    if self.line_width:\n        canvas_add = self.canvas.add\n        canvas_add(Color(col[0], col[1], col[2], mode='rgb', group=g.id))\n        canvas_add(new_line)\n    g.update_bbox(touch)\n    if self.draw_bbox:\n        self._update_canvas_bbox(g)\n    g.add_stroke(touch, new_line)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/item/shared_item_music_microphone_s2.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef test_scan_clear_product(self):\n    with HTTMock(wechat_api_mock):\n        res = self.client.scan.clear_product('ean13', '6900873042720')\n    self.assertEqual(0, res['errcode'])\n", "label": 0}
{"function": "\n\ndef __init__(self, dateTime, frequency):\n    super(IntraDayRange, self).__init__()\n    assert isinstance(frequency, int)\n    assert (frequency > 1)\n    assert (frequency < bar.Frequency.DAY)\n    ts = int(dt.datetime_to_timestamp(dateTime))\n    slot = int((ts / frequency))\n    slotTs = (slot * frequency)\n    self.__begin = dt.timestamp_to_datetime(slotTs, (not dt.datetime_is_naive(dateTime)))\n    if (not dt.datetime_is_naive(dateTime)):\n        self.__begin = dt.localize(self.__begin, dateTime.tzinfo)\n    self.__end = (self.__begin + datetime.timedelta(seconds=frequency))\n", "label": 0}
{"function": "\n\ndef autodiscover():\n    'Auto-discover INSTALLED_APPS mails.py modules.'\n    for app in settings.INSTALLED_APPS:\n        module = ('%s.mails' % app)\n        try:\n            import_module(module)\n        except:\n            app_module = import_module(app)\n            if module_has_submodule(app_module, 'mails'):\n                raise\n", "label": 0}
{"function": "\n\ndef setup_basic_delete_test(self, user, with_local_site, local_site_name):\n    review_request = self.create_review_request(with_local_site=with_local_site, publish=True)\n    profile = user.get_profile()\n    profile.starred_review_requests.add(review_request)\n    return (get_watched_review_request_item_url(user.username, review_request.display_id, local_site_name), [profile, review_request])\n", "label": 0}
{"function": "\n\ndef unregister_module(self, module):\n    if (module not in self.modules):\n        raise NotRegistered(('The module %s is not registered' % module.__name__))\n    del self.modules[module]\n", "label": 0}
{"function": "\n\ndef sh(cmdline, stdout=subprocess.PIPE, stderr=subprocess.PIPE):\n    'run cmd in a subprocess and return its output.\\n    raises RuntimeError on error.\\n    '\n    p = subprocess.Popen(cmdline, shell=True, stdout=stdout, stderr=stderr)\n    (stdout, stderr) = p.communicate()\n    if (p.returncode != 0):\n        raise RuntimeError(stderr)\n    if stderr:\n        warn(stderr)\n    if PY3:\n        stdout = str(stdout, sys.stdout.encoding)\n    return stdout.strip()\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.content_type == other.content_type) and (_join_b(self.iter_bytes()) == _join_b(other.iter_bytes())))\n", "label": 0}
{"function": "\n\ndef _libname(self, libpath):\n    \"Converts a full library filepath to the library's name.\\n    Ex: /path/to/libhello.a --> hello\\n    \"\n    return os.path.basename(libpath)[3:(- 2)]\n", "label": 0}
{"function": "\n\ndef test_deprecated_simple(self):\n\n    @deprecated()\n    def f(arg):\n        return arg\n    ARG = object()\n    with warnings.catch_warnings(record=True) as recorded:\n        returned = f(ARG)\n    self.assertIs(returned, ARG)\n    self.assertEqual(len(recorded), 1)\n", "label": 0}
{"function": "\n\ndef description(self, around=False):\n    if around:\n        return 'Expand Selection to Quotes'\n    else:\n        return 'Expand Selection to Quoted'\n", "label": 0}
{"function": "\n\ndef get_form(self, request, obj=None, **kwargs):\n    _thread_locals.request = request\n    _thread_locals.obj = obj\n    return super(XOSAdminMixin, self).get_form(request, obj, **kwargs)\n", "label": 0}
{"function": "\n\ndef input(self, data):\n    if ('type' in data):\n        function_name = ('process_' + data['type'])\n        self._dbg('got {}'.format(function_name))\n        for plugin in self.bot_plugins:\n            plugin.register_jobs()\n            plugin.do(function_name, data)\n", "label": 0}
{"function": "\n\ndef db_exists(database_name, **kwargs):\n    \"\\n    Find if a specific database exists on the MS SQL server.\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt minion mssql.db_exists database_name='DBNAME'\\n    \"\n    return (len(tsql_query(\"SELECT database_id FROM sys.databases WHERE NAME='{0}'\".format(database_name), **kwargs)) == 1)\n", "label": 0}
{"function": "\n\ndef load_proposal_rlp(self, blockhash):\n    try:\n        prlp = self.chainservice.db.get(('blockproposal:%s' % blockhash))\n        assert isinstance(prlp, bytes)\n        return prlp\n    except KeyError:\n        return None\n", "label": 0}
{"function": "\n\ndef test_sort_mapping_reverse(self):\n    stream = DataStream(IterableDataset(self.data))\n    transformer = Mapping(stream, SortMapping(operator.itemgetter(0), reverse=True))\n    assert_equal(list(transformer.get_epoch_iterator()), list(zip(([[3, 2, 1]] * 3))))\n", "label": 0}
{"function": "\n\ndef load_train_data(self, input_data_file=''):\n    '\\n        Load train data\\n        Please check dataset/logistic_regression_train.dat to understand the data format\\n        Each feature of data x separated with spaces\\n        And the ground truth y put in the end of line separated by a space\\n        '\n    self.status = 'load_train_data'\n    if (input_data_file == ''):\n        input_data_file = os.path.normpath(os.path.join(os.path.join(os.getcwd(), os.path.dirname(__file__)), 'dataset/logistic_regression_train.dat'))\n    elif (os.path.isfile(input_data_file) is not True):\n        print('Please make sure input_data_file path is correct.')\n        return (self.train_X, self.train_Y)\n    (self.train_X, self.train_Y) = utility.DatasetLoader.load(input_data_file)\n    return (self.train_X, self.train_Y)\n", "label": 0}
{"function": "\n\ndef test_realtime_with_batch_computation(self):\n    with self._get_swap_context():\n        user_id = 'uid'\n        exp_id = 'eid'\n        self.save_new_valid_exploration(exp_id, 'owner')\n        feedback_services.create_thread(exp_id, 'a_state_name', None, 'a subject', 'some text')\n        self.process_and_flush_pending_tasks()\n        ModifiedFeedbackAnalyticsAggregator.start_computation()\n        self.assertEqual(self.count_jobs_in_taskqueue(), 1)\n        self.process_and_flush_pending_tasks()\n        ModifiedFeedbackAnalyticsAggregator.stop_computation(user_id)\n        self.assertEqual(self.count_jobs_in_taskqueue(), 0)\n        self._flush_tasks_and_check_analytics(exp_id, {\n            'num_open_threads': 1,\n            'num_total_threads': 1,\n        })\n        feedback_services.create_thread(exp_id, 'a_state_name', None, 'a subject', 'some text')\n        self._flush_tasks_and_check_analytics(exp_id, {\n            'num_open_threads': 2,\n            'num_total_threads': 2,\n        })\n", "label": 0}
{"function": "\n\ndef register(self, parent, key, create_only=False, **kwargs):\n    '\\n        Add/replace an entry within directory, below a parent node or \"/\".\\n        Note: Replaces (not merges) the attribute values of the entry if existing\\n        @param create_only  If True, does not change an existing entry\\n        @retval  DirEntry if previously existing\\n        '\n    if (not (parent and key)):\n        raise BadRequest('Illegal arguments')\n    if ((not (type(parent) is str)) or (not parent.startswith('/'))):\n        raise BadRequest('Illegal arguments: parent')\n    dn = self._get_path(parent, key)\n    log.debug('Directory.register(%s): %s', dn, kwargs)\n    entry_old = None\n    cur_time = get_ion_ts()\n    direntry = self._read_by_path(dn)\n    if (direntry and create_only):\n        return direntry\n    elif direntry:\n        entry_old = direntry.attributes\n        direntry.attributes = kwargs\n        direntry.ts_updated = cur_time\n        self.dir_store.update(direntry)\n    else:\n        direntry = self._create_dir_entry(parent, key, attributes=kwargs, ts=cur_time)\n        self._ensure_parents_exist([direntry])\n        self.dir_store.create(direntry, create_unique_directory_id())\n    return entry_old\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    s = ('%s' % self._name)\n    if self._location:\n        s = ('%s@%s' % (s, self._location))\n    return s\n", "label": 0}
{"function": "\n\ndef test_dispose1():\n    h = event.HasEvents()\n\n    @h.connect('x1', 'x2')\n    def handler(*events):\n        pass\n    handler_ref = weakref.ref(handler)\n    del handler\n    gc.collect()\n    assert (handler_ref() is not None)\n    handler_ref().dispose()\n    gc.collect()\n    assert (handler_ref() is None)\n", "label": 0}
{"function": "\n\ndef test_add(self):\n    'Test that we can add an image via the s3 backend'\n    expected_image_id = utils.generate_uuid()\n    expected_s3_size = FIVE_KB\n    expected_s3_contents = ('*' * expected_s3_size)\n    expected_checksum = hashlib.md5(expected_s3_contents).hexdigest()\n    expected_location = format_s3_location(S3_CONF['s3_store_access_key'], S3_CONF['s3_store_secret_key'], S3_CONF['s3_store_host'], S3_CONF['s3_store_bucket'], expected_image_id)\n    image_s3 = StringIO.StringIO(expected_s3_contents)\n    (location, size, checksum) = self.store.add(expected_image_id, image_s3, expected_s3_size)\n    self.assertEquals(expected_location, location)\n    self.assertEquals(expected_s3_size, size)\n    self.assertEquals(expected_checksum, checksum)\n    loc = get_location_from_uri(expected_location)\n    (new_image_s3, new_image_size) = self.store.get(loc)\n    new_image_contents = StringIO.StringIO()\n    for chunk in new_image_s3:\n        new_image_contents.write(chunk)\n    new_image_s3_size = new_image_contents.len\n    self.assertEquals(expected_s3_contents, new_image_contents.getvalue())\n    self.assertEquals(expected_s3_size, new_image_s3_size)\n", "label": 0}
{"function": "\n\ndef move_cat(self):\n    speed = random.randint(20, 200)\n    self.cat_body.angle -= random.randint((- 1), 1)\n    direction = Vec2d(1, 0).rotated(self.cat_body.angle)\n    self.cat_body.velocity = (speed * direction)\n", "label": 0}
{"function": "\n\ndef job_status(self, job_id=None):\n    job_id = (job_id or self.lookup_job_id(batch_id))\n    uri = urlparse.urljoin((self.endpoint + '/'), 'job/{0}'.format(job_id))\n    response = requests.get(uri, headers=self.headers())\n    if (response.status_code != 200):\n        self.raise_error(response.content, response.status_code)\n    tree = ET.fromstring(response.content)\n    result = {\n        \n    }\n    for child in tree:\n        result[re.sub('{.*?}', '', child.tag)] = child.text\n    return result\n", "label": 0}
{"function": "\n\ndef test_issue(self):\n    'Show that one can retrieve the associated issue of a PR.'\n    cassette_name = self.cassette_name('issue')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        issue = p.issue()\n        assert isinstance(issue, github3.issues.Issue)\n", "label": 0}
{"function": "\n\ndef __init__(self, view=False):\n    self.view = view\n    if self.view:\n        self.view_map = {\n            0: [0],\n        }\n", "label": 0}
{"function": "\n\ndef replace_body(self, body, payload):\n    if (body is None):\n        return body\n    if (self.fsig in body):\n        return body.replace(self.fsig, urllib.quote_plus(payload))\n    template_sig = self.template_signature(body)\n    if template_sig:\n        tp = TemplateParser()\n        tp.set_payload(payload)\n        new_payload = repr(tp.transform(self.template_signature(body), self.sig))[1:(- 1)]\n        return body.replace(template_sig, new_payload)\n    return body\n", "label": 0}
{"function": "\n\ndef delete(self, image):\n    \"\\n        Delete an image.\\n        \\n        It should go without saying that you can't delete an image \\n        that you didn't create.\\n        \\n        :param image: The :class:`Image` (or its ID) to delete.\\n        \"\n    self._delete(('/images/%s' % base.getid(image)))\n", "label": 0}
{"function": "\n\ndef facettupletrees(table, key, start='start', stop='stop', value=None):\n    '\\n    Construct faceted interval trees for the given table, where each node in\\n    the tree is a row of the table.\\n\\n    '\n    import intervaltree\n    it = iter(table)\n    hdr = next(it)\n    flds = list(map(text_type, hdr))\n    assert (start in flds), 'start field not recognised'\n    assert (stop in flds), 'stop field not recognised'\n    getstart = itemgetter(flds.index(start))\n    getstop = itemgetter(flds.index(stop))\n    if (value is None):\n        getvalue = tuple\n    else:\n        valueindices = asindices(hdr, value)\n        assert (len(valueindices) > 0), 'invalid value field specification'\n        getvalue = itemgetter(*valueindices)\n    keyindices = asindices(hdr, key)\n    assert (len(keyindices) > 0), 'invalid key'\n    getkey = itemgetter(*keyindices)\n    trees = dict()\n    for row in it:\n        k = getkey(row)\n        if (k not in trees):\n            trees[k] = intervaltree.IntervalTree()\n        trees[k].addi(getstart(row), getstop(row), getvalue(row))\n    return trees\n", "label": 0}
{"function": "\n\ndef load_parent(parent_id):\n    parent = Node.load(parent_id)\n    if (parent is None):\n        return None\n    parent_info = {\n        \n    }\n    if ((parent is not None) and parent.is_public):\n        parent_info['title'] = parent.title\n        parent_info['url'] = parent.url\n        parent_info['is_registration'] = parent.is_registration\n        parent_info['id'] = parent._id\n    else:\n        parent_info['title'] = '-- private project --'\n        parent_info['url'] = ''\n        parent_info['is_registration'] = None\n        parent_info['id'] = None\n    return parent_info\n", "label": 0}
{"function": "\n\ndef test_sys_stderr_should_have_no_output_when_no_logger_is_set(memcached):\n    mc = cmemcached.Client([memcached])\n    with patch('sys.stderr') as mock_stderr:\n        mc.get('test_key_with_no_logger')\n        mc.set('test_key_with_no_logger', 'test_value_with_no_logger')\n        assert (not mock_stderr.write.called)\n", "label": 0}
{"function": "\n\ndef write(self, filename, content):\n    create_dirs(self.webd, dirname(filename))\n    buff = BytesIO(content)\n    self.webd.upload_from(buff, b(filename))\n", "label": 0}
{"function": "\n\ndef test_gtkApplicationActivate(self):\n    '\\n        L{Gtk.Application} instances can be registered with a gtk3reactor.\\n        '\n    reactor = gtk3reactor.Gtk3Reactor()\n    self.addCleanup(self.unbuildReactor, reactor)\n    app = Gtk.Application(application_id='com.twistedmatrix.trial.gtk3reactor', flags=Gio.ApplicationFlags.FLAGS_NONE)\n    self.runReactor(app, reactor)\n", "label": 0}
{"function": "\n\ndef test_patch_mocksignature_callable(self):\n    original_something = something\n    something_name = ('%s.something' % __name__)\n\n    @patch(something_name, mocksignature=True)\n    def test(MockSomething):\n        something(3, 4)\n        MockSomething.assert_called_with(3, 4)\n        something(6)\n        MockSomething.assert_called_with(6, 5)\n        self.assertRaises(TypeError, something)\n    test()\n    self.assertIs(something, original_something)\n", "label": 0}
{"function": "\n\ndef __init__(self, allure_helper, title):\n    self.allure_helper = allure_helper\n    self.title = title\n    self.step = None\n", "label": 0}
{"function": "\n\ndef load_xml_config(self, path=None):\n    if (path is not None):\n        self.path = path\n    if (not os.path.isfile(self.path)):\n        raise KaresasnuiServiceConfigParamException(('service.xml not found. path=%s' % str(self.path)))\n    document = XMLParse(self.path)\n    self.services = []\n    service_num = XMLXpathNum(document, '/services/service')\n    for n in xrange(1, (service_num + 1)):\n        system_name = XMLXpath(document, ('/services/service[%i]/system/name/text()' % n))\n        system_command = XMLXpath(document, ('/services/service[%i]/system/command/text()' % n))\n        system_readonly = XMLXpath(document, ('/services/service[%i]/system/readonly/text()' % n))\n        display_name = XMLXpath(document, ('/services/service[%i]/display/name/text()' % n))\n        display_description = XMLXpath(document, ('/services/service[%i]/display/description/text()' % n))\n        self.add_service(str(system_name), str(system_command), str(system_readonly), str(display_name), str(display_description))\n", "label": 0}
{"function": "\n\ndef _should_create_constraint(self, compiler):\n    return (not compiler.dialect.supports_native_boolean)\n", "label": 0}
{"function": "\n\ndef put_object(self, name, fp, metadata):\n    '\\n        Store object into memory\\n\\n        :param name: standard object name\\n        :param fp: `StringIO` in-memory representation object\\n        :param metadata: dictionary of metadata to be written\\n        '\n    self._filesystem[name] = (fp, metadata)\n", "label": 0}
{"function": "\n\ndef _raise_test_exc(self, exc_msg):\n    raise TestException(exc_msg)\n", "label": 0}
{"function": "\n\ndef install_inplace(pkg):\n    'Install scripts of pkg in the current directory.'\n    for (basename, executable) in pkg.executables.items():\n        version_str = '.'.join([str(i) for i in sys.version_info[:2]])\n        scripts_node = root._ctx.srcnode\n        for name in [basename, ('%s-%s' % (basename, version_str))]:\n            nodes = _create_executable(name, executable, scripts_node)\n            installed = ','.join([n.path_from(scripts_node) for n in nodes])\n            pprint('GREEN', ('installing %s in current directory' % installed))\n", "label": 0}
{"function": "\n\ndef testSuccess(self):\n    vor = rapi.testutils.VerifyOpResult\n    vor(opcodes.OpClusterVerify.OP_ID, {\n        constants.JOB_IDS_KEY: [(False, 'error message')],\n    })\n", "label": 0}
{"function": "\n\ndef kilobyte(self, value=None):\n    return self.convertb(value, self.byte)\n", "label": 0}
{"function": "\n\ndef _irfft_out_chunks(a, n, axis):\n    if (n is None):\n        n = (2 * (a.chunks[axis][0] - 1))\n    chunks = list(a.chunks)\n    chunks[axis] = (n,)\n    return chunks\n", "label": 0}
{"function": "\n\n@skipUnless(cache_file_exists('ia', '20100608__ia__primary__adair__precinct.xls'), CACHED_FILE_MISSING_MSG)\ndef test_results(self):\n    filename = '20100608__ia__primary__adair__precinct.xls'\n    mapping = self._get_mapping(filename)\n    self._prep_loader_attrs(mapping)\n    results = self.loader._results(mapping)\n    us_rep_dist_5_rep_results = [r for r in results if ((r.office == 'U.S. REPRESENTATIVE') and (r.district == '5') and (r.primary_party == 'REPUBLICAN'))]\n    self.assertEqual(len(us_rep_dist_5_rep_results), 35)\n    result = us_rep_dist_5_rep_results[0]\n    self.assertEqual(result.source, mapping['generated_filename'])\n    self.assertEqual(result.election_id, mapping['election'])\n    self.assertEqual(result.state, 'IA')\n    self.assertEqual(result.election_type, 'primary')\n    self.assertEqual(result.district, '5')\n    self.assertEqual(result.party, 'REPUBLICAN')\n    self.assertEqual(result.jurisdiction, '1 NW')\n    self.assertEqual(result.reporting_level, 'precinct')\n    self.assertEqual(result.full_name, 'STEVE KING')\n    self.assertEqual(result.votes, 123)\n", "label": 0}
{"function": "\n\ndef get_context_data(self, **kwargs):\n    ctx = super(AddressCreateView, self).get_context_data(**kwargs)\n    ctx['title'] = _('Add a new address')\n    return ctx\n", "label": 0}
{"function": "\n\ndef find_by_selector(self, selector, search_regions=None):\n    search_regions = (search_regions or self.regions)\n    return GrammarParser.filter_by_selector(selector, search_regions)\n", "label": 0}
{"function": "\n\ndef nova(context):\n    global _nova_api_version\n    if (not _nova_api_version):\n        _nova_api_version = _get_nova_api_version(context)\n    clnt = novaclient.Client(_nova_api_version, session=context.session, service_type=CONF.nova_service_type)\n    if (not hasattr(clnt.client, 'last_request_id')):\n        setattr(clnt.client, 'last_request_id', None)\n    return clnt\n", "label": 0}
{"function": "\n\ndef iteritems(self):\n    for tag in self.tags:\n        (yield (tag.name, tag))\n", "label": 0}
{"function": "\n\ndef split_multiline(value):\n    value = [element for element in (line.strip() for line in value.split('\\n')) if element]\n    return value\n", "label": 0}
{"function": "\n\ndef get_tags_count(journal):\n    'Returns a set of tuples (count, tag) for all tags present in the journal.'\n    tags = [tag for entry in journal.entries for tag in set(entry.tags)]\n    tag_counts = set([(tags.count(tag), tag) for tag in tags])\n    return tag_counts\n", "label": 0}
{"function": "\n\ndef vmstats():\n    \"\\n    Return the virtual memory stats for this minion\\n\\n    CLI Example:\\n\\n    .. code-block:: bash\\n\\n        salt '*' status.vmstats\\n    \"\n\n    def linux_vmstats():\n        '\\n        linux specific implementation of vmstats\\n        '\n        procf = '/proc/vmstat'\n        if (not os.path.isfile(procf)):\n            return {\n                \n            }\n        stats = salt.utils.fopen(procf, 'r').read().splitlines()\n        ret = {\n            \n        }\n        for line in stats:\n            if (not line):\n                continue\n            comps = line.split()\n            ret[comps[0]] = _number(comps[1])\n        return ret\n\n    def freebsd_vmstats():\n        '\\n        freebsd specific implementation of vmstats\\n        '\n        ret = {\n            \n        }\n        for line in __salt__['cmd.run']('vmstat -s').splitlines():\n            comps = line.split()\n            if comps[0].isdigit():\n                ret[' '.join(comps[1:])] = _number(comps[0])\n        return ret\n    get_version = {\n        'Linux': linux_vmstats,\n        'FreeBSD': freebsd_vmstats,\n    }\n    errmsg = 'This method is unsupported on the current operating system!'\n    return get_version.get(__grains__['kernel'], (lambda : errmsg))()\n", "label": 0}
{"function": "\n\ndef test_get_children_duplicates(self):\n    from psutil._compat import defaultdict\n    table = defaultdict(int)\n    for p in psutil.process_iter():\n        try:\n            table[p.ppid] += 1\n        except psutil.Error:\n            pass\n    pid = max(sorted(table, key=(lambda x: table[x])))\n    p = psutil.Process(pid)\n    try:\n        c = p.get_children(recursive=True)\n    except psutil.AccessDenied:\n        pass\n    else:\n        self.assertEqual(len(c), len(set(c)))\n", "label": 0}
{"function": "\n\ndef __call__(self, driver):\n    try:\n        return _element_if_visible(_find_element(driver, self.locator))\n    except StaleElementReferenceException:\n        return False\n", "label": 0}
{"function": "\n\ndef _test_update_routing_table(self, is_snat_host=True):\n    router = l3_test_common.prepare_router_data()\n    uuid = router['id']\n    s_netns = ('snat-' + uuid)\n    q_netns = ('qrouter-' + uuid)\n    fake_route1 = {\n        'destination': '135.207.0.0/16',\n        'nexthop': '19.4.4.200',\n    }\n    calls = [mock.call('replace', fake_route1, q_netns)]\n    agent = l3_agent.L3NATAgent(HOSTNAME, self.conf)\n    ri = dvr_router.DvrEdgeRouter(agent, HOSTNAME, uuid, router, **self.ri_kwargs)\n    ri._update_routing_table = mock.Mock()\n    with mock.patch.object(ri, '_is_this_snat_host') as snat_host:\n        snat_host.return_value = is_snat_host\n        ri.update_routing_table('replace', fake_route1)\n        if is_snat_host:\n            ri._update_routing_table('replace', fake_route1, s_netns)\n            calls += [mock.call('replace', fake_route1, s_netns)]\n        ri._update_routing_table.assert_has_calls(calls, any_order=True)\n", "label": 0}
{"function": "\n\ndef install_ssl_certs(instances):\n    certs = []\n    if CONF.object_store_access.public_identity_ca_file:\n        certs.append(CONF.object_store_access.public_identity_ca_file)\n    if CONF.object_store_access.public_object_store_ca_file:\n        certs.append(CONF.object_store_access.public_object_store_ca_file)\n    if (not certs):\n        return\n    with context.ThreadGroup() as tg:\n        for inst in instances:\n            tg.spawn(('configure-ssl-cert-%s' % inst.instance_id), _install_ssl_certs, inst, certs)\n", "label": 0}
{"function": "\n\ndef test_node_site():\n    s = Site(TEST_SITE_ROOT)\n    r = RootNode(TEST_SITE_ROOT.child_folder('content'), s)\n    assert (r.site == s)\n    n = Node(r.source_folder.child_folder('blog'), r)\n    assert (n.site == s)\n", "label": 0}
{"function": "\n\ndef render(self, name, value, attrs=None):\n    value = util.serialize_references(value)\n    return super(ReferencesFieldWidget, self).render(name, value, attrs)\n", "label": 0}
{"function": "\n\ndef on_request(self, context, request):\n    if ('PowerView.ps1' == request.path[1:]):\n        request.send_response(200)\n        request.end_headers()\n        with open('data/PowerSploit/Recon/PowerView.ps1', 'r') as ps_script:\n            ps_script = obfs_ps_script(ps_script.read())\n            request.wfile.write(ps_script)\n    else:\n        request.send_response(404)\n        request.end_headers()\n", "label": 0}
{"function": "\n\ndef test_column_expr(self):\n    c = Column('x', Integer)\n    is_(inspect(c), c)\n    assert (not c.is_selectable)\n    assert (not hasattr(c, 'selectable'))\n", "label": 0}
{"function": "\n\ndef inline_assets(self, base_path, content):\n    for type in self.asset_types:\n        for (statement, path) in self.get_matches(type['pattern'], base_path, content):\n            asset_content = self.get_binary_file_contents(path)\n            encoded_content = urllib.quote(base64.encodestring(asset_content))\n            new_statement = ('url(data:%s;base64,%s)' % (type['mime'], encoded_content))\n            content = content.replace(statement, new_statement)\n    return content\n", "label": 0}
{"function": "\n\ndef test_choice_update(self):\n    self.choice.choice_text = 'third text'\n    self.choice.save()\n    p = Choice.objects.get()\n    self.assertEqual(p.choice_text, 'third text')\n", "label": 0}
{"function": "\n\ndef abort_run(self, drain=False):\n    self._aborting_run = drain\n", "label": 0}
{"function": "\n\ndef test_oldPythonPy3(self):\n    '\\n        L{_checkRequirements} raises L{ImportError} when run on a version of\\n        Python that is too old.\\n        '\n    sys.version_info = self.Py3unsupportedPythonVersion\n    with self.assertRaises(ImportError) as raised:\n        _checkRequirements()\n    self.assertEqual(('Twisted on Python 3 requires Python %d.%d or later.' % self.Py3supportedPythonVersion), str(raised.exception))\n", "label": 0}
{"function": "\n\ndef get_oauth_request(request):\n    ' Converts a Django request object into an `oauth2.Request` object. '\n    headers = {\n        \n    }\n    if ('HTTP_AUTHORIZATION' in request.META):\n        headers['Authorization'] = request.META['HTTP_AUTHORIZATION']\n    return oauth.Request.from_request(request.method, request.build_absolute_uri(request.path), headers, dict(request.REQUEST))\n", "label": 0}
{"function": "\n\ndef _apply_filters(self, query, count_query, joins, count_joins, filters):\n    for (idx, flt_name, value) in filters:\n        flt = self._filters[idx]\n        alias = None\n        count_alias = None\n        if isinstance(flt, sqla_filters.BaseSQLAFilter):\n            path = self._filter_joins.get(flt.column, [])\n            (query, joins, alias) = self._apply_path_joins(query, joins, path, inner_join=False)\n            if (count_query is not None):\n                (count_query, count_joins, count_alias) = self._apply_path_joins(count_query, count_joins, path, inner_join=False)\n        clean_value = flt.clean(value)\n        try:\n            query = flt.apply(query, clean_value, alias)\n        except TypeError:\n            spec = inspect.getargspec(flt.apply)\n            if (len(spec.args) == 3):\n                warnings.warn(('Please update your custom filter %s to include additional `alias` parameter.' % repr(flt)))\n            else:\n                raise\n            query = flt.apply(query, clean_value)\n        if (count_query is not None):\n            try:\n                count_query = flt.apply(count_query, clean_value, count_alias)\n            except TypeError:\n                count_query = flt.apply(count_query, clean_value)\n    return (query, count_query, joins, count_joins)\n", "label": 0}
{"function": "\n\n@property\ndef vcf(self):\n    'serialize to VCARD as specified in RFC2426,\\n        if no UID is specified yet, one will be added (as a UID is mandatory\\n        for carddav as specified in RFC6352\\n        TODO make shure this random uid is unique'\n    import string\n    import random\n\n    def generate_random_uid():\n        \"generate a random uid, when random isn't broken, getting a\\n            random UID from a pool of roughly 10^56 should be good enough\"\n        choice = (string.ascii_uppercase + string.digits)\n        return ''.join([random.choice(choice) for _ in range(36)])\n    if ('UID' not in self.keys()):\n        self['UID'] = [(generate_random_uid(), dict())]\n    collector = list()\n    collector.append('BEGIN:VCARD')\n    collector.append('VERSION:3.0')\n    for key in ['FN', 'N']:\n        try:\n            collector.append(((key + ':') + self[key][0][0]))\n        except IndexError:\n            collector.append((key + ':'))\n    for prop in self.alt_keys():\n        for line in self[prop]:\n            types = self._line_helper(line)\n            collector.append((((prop + types) + ':') + line[0]))\n    collector.append('END:VCARD')\n    return '\\n'.join(collector)\n", "label": 0}
{"function": "\n\ndef close_review_request(server_url, username, password, review_request_id, description):\n    'Closes the specified review request as submitted.'\n    (api_client, api_root) = get_api(server_url, username, password)\n    review_request = get_review_request(review_request_id, api_root)\n    if (review_request.status == SUBMITTED):\n        logging.warning('Review request #%s is already %s.', review_request_id, SUBMITTED)\n        return\n    if description:\n        review_request = review_request.update(status=SUBMITTED, description=description)\n    else:\n        review_request = review_request.update(status=SUBMITTED)\n    print(('Review request #%s is set to %s.' % (review_request_id, review_request.status)))\n", "label": 0}
{"function": "\n\ndef GetLastRequestTimedelta(api_query, from_time=None):\n    'Returns how long since the API Query response was last requested.\\n\\n  Args:\\n    api_query: The API Query from which to retrieve the last request timedelta.\\n    from_time: A DateTime object representing the start time to calculate the\\n               timedelta from.\\n\\n  Returns:\\n    A string that describes how long since the API Query response was last\\n    requested in the form of \"HH hours, MM minutes, ss seconds ago\" or None\\n    if the API Query response has never been requested.\\n  '\n    if (not from_time):\n        from_time = datetime.utcnow()\n    if api_query.last_request:\n        time_delta = (from_time - api_query.last_request)\n        return FormatTimedelta(time_delta)\n    return None\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Static()\n    result.template = 'object/static/particle/shared_particle_test_16.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('obj_n', 'unknown_object')\n    return result\n", "label": 0}
{"function": "\n\ndef __init__(self, name):\n    Exception.__init__(self, (\"Method not found: '%s'\" % name))\n", "label": 0}
{"function": "\n\ndef testUnshareSecondLevelRemoved(self):\n    'Re-share photos, remove the reshared viewpoint, then unshare the source viewpoint.'\n    (child_vp_id, child_ep_ids) = self._tester.ShareNew(self._cookie2, [(self._new_ep_id, self._photo_ids)], [self._user3.user_id], **self._CreateViewpointDict(self._cookie2))\n    self._tester.RemoveViewpoint(self._cookie3, child_vp_id)\n    self._tester.Unshare(self._cookie, self._new_vp_id, [(self._new_ep_id, self._photo_ids[:1])])\n", "label": 0}
{"function": "\n\ndef test_search_comment(self):\n    result = self.search(comment=['fantastic'])\n    self.assertEqual(list(result[0].tracks), self.tracks[3:4])\n    result = self.search(comment=['antasti'])\n    self.assertEqual(list(result[0].tracks), self.tracks[3:4])\n", "label": 0}
{"function": "\n\n@converts('ImageField')\ndef conv_Image(self, model, field, kwargs):\n    return f.FileField(**kwargs)\n", "label": 0}
{"function": "\n\ndef parse(self, text):\n    return [ErrorLine(m) for m in self.regex.finditer(text)]\n", "label": 0}
{"function": "\n\ndef clamp_vect(self, v):\n    'Returns a copy of the vector v clamped to the bounding box'\n    return cpffi.cpBBClampVect(self._bb, v)\n", "label": 0}
{"function": "\n\ndef __exit__(self, *args):\n    self.delegate.disconnect()\n", "label": 0}
{"function": "\n\n@view_config(context='velruse.AuthenticationComplete', renderer='{}:templates/result.mako'.format(__name__))\ndef login_complete_view(request):\n    context = request.context\n    result = {\n        'profile': context.profile,\n        'credentials': context.credentials,\n    }\n    return {\n        'result': json.dumps(result, indent=4),\n    }\n", "label": 0}
{"function": "\n\ndef draw_outlines(context, box, enable_hinting):\n    width = box.style.outline_width\n    color = box.style.get_color('outline_color')\n    style = box.style.outline_style\n    if ((box.style.visibility == 'visible') and (width != 0) and (color.alpha != 0)):\n        outline_box = ((box.border_box_x() - width), (box.border_box_y() - width), (box.border_width() + (2 * width)), (box.border_height() + (2 * width)))\n        for side in SIDES:\n            with stacked(context):\n                clip_border_segment(context, enable_hinting, style, width, side, outline_box)\n                draw_rect_border(context, outline_box, (4 * (width,)), style, styled_color(style, color, side))\n    if isinstance(box, boxes.ParentBox):\n        for child in box.children:\n            if isinstance(child, boxes.Box):\n                draw_outlines(context, child, enable_hinting)\n", "label": 0}
{"function": "\n\ndef emit(self, *args, **kwargs):\n    try:\n        self.__emitting = True\n        for handler in self.__handlers:\n            handler(*args, **kwargs)\n    finally:\n        self.__emitting = False\n        self.__applyChanges()\n", "label": 0}
{"function": "\n\ndef test_existing_spawn(self):\n    child = pexpect.spawnu('bash', timeout=5, echo=False)\n    repl = replwrap.REPLWrapper(child, re.compile('[$#]'), \"PS1='{0}' PS2='{1}' PROMPT_COMMAND=''\")\n    res = repl.run_command('echo $HOME')\n    assert res.startswith('/'), res\n", "label": 0}
{"function": "\n\ndef child_removed(self, child):\n    ' Handle the child removed event for a QtWindow.\\n\\n        '\n    if isinstance(child, WxContainer):\n        self.widget.SetCentralWidget(self.central_widget())\n", "label": 0}
{"function": "\n\ndef test_stubs(self):\n    df = pd.DataFrame([[0, 1, 2, 3, 8], [4, 5, 6, 7, 9]])\n    df.columns = ['id', 'inc1', 'inc2', 'edu1', 'edu2']\n    stubs = ['inc', 'edu']\n    df_long = pd.wide_to_long(df, stubs, i='id', j='age')\n    self.assertEqual(stubs, ['inc', 'edu'])\n", "label": 0}
{"function": "\n\ndef collectstreamuuid(self, streamname):\n    if (not streamname):\n        return\n    shouter.shout(('Get UUID of configured stream ' + streamname))\n    showuuidcommand = ('%s --show-alias n --show-uuid y show attributes -r %s -w %s' % (self.scmcommand, self.repo, streamname))\n    output = shell.getoutput(showuuidcommand)\n    splittedfirstline = output[0].split(' ')\n    streamuuid = splittedfirstline[0].strip()[1:(- 1)]\n    return streamuuid\n", "label": 0}
{"function": "\n\n@classmethod\ndef resource_uri(cls, obj=None):\n    object_id = 'id'\n    if (obj is not None):\n        object_id = obj.id\n    return ('api_events', [object_id])\n", "label": 0}
{"function": "\n\ndef __init__(self, text):\n    if (len(text) >= 10000):\n        text = (text[:9995] + '\\n...')\n    self.text = text\n    self.recipient = None\n", "label": 0}
{"function": "\n\ndef info(self, msg_format, *values):\n    'For progress and other informative messages.'\n    if (len(values) > 0):\n        msg_format = (msg_format % values)\n    print(msg_format)\n", "label": 0}
{"function": "\n\ndef freeze(self, skipSet=None):\n    assert (len(self()) in self.allowedSize)\n    return StringStream.freeze(self, skipSet=skipSet)\n", "label": 0}
{"function": "\n\ndef get_result(self, vlan_range_len):\n    self.intersect()\n    if (vlan_range_len > 1):\n        return self.get_final_available_vlan_range(vlan_range_len)\n    else:\n        return self.get_final_available_vlan()\n", "label": 0}
{"function": "\n\ndef __init__(self, collector, callback=None, *args, **kw):\n    '\\n        Create a pager with a Reference to a remote collector and\\n        an optional callable to invoke upon completion.\\n        '\n    if callable(callback):\n        self.callback = callback\n        self.callbackArgs = args\n        self.callbackKeyword = kw\n    else:\n        self.callback = None\n    self._stillPaging = 1\n    self.collector = collector\n    collector.broker.registerPageProducer(self)\n", "label": 0}
{"function": "\n\ndef __init__(self, name, consumer_key, consumer_secret):\n    self.name = name\n    self.type = 'twitter'\n    self.consumer_key = consumer_key\n    self.consumer_secret = consumer_secret\n    self.login_route = ('velruse.%s-login' % name)\n    self.callback_route = ('velruse.%s-callback' % name)\n", "label": 0}
{"function": "\n\n@mock.patch(('%s.flavors.osclients.Clients' % CTX))\ndef test_cleanup(self, mock_clients):\n    real_context = {\n        'flavors': {\n            'flavor_name': {\n                'flavor_name': 'flavor_name',\n                'id': 'flavor_name',\n            },\n        },\n        'admin': {\n            'credential': mock.MagicMock(),\n        },\n        'task': mock.MagicMock(),\n    }\n    flavors_ctx = flavors.FlavorsGenerator(real_context)\n    flavors_ctx.cleanup()\n    mock_clients.assert_called_with(real_context['admin']['credential'])\n    mock_flavors_delete = mock_clients().nova().flavors.delete\n    mock_flavors_delete.assert_called_with('flavor_name')\n", "label": 0}
{"function": "\n\ndef get_show(self, imdb, tvdb, tvshowtitle, year):\n    try:\n        query = self.search_link\n        post = {\n            'searchquery': tvshowtitle,\n            'searchin': '2',\n        }\n        result = ''\n        links = [self.link_1, self.link_3]\n        for base_link in links:\n            result = client.source(urlparse.urljoin(base_link, query), post=post, headers=self.headers)\n            if ('widget search-page' in str(result)):\n                break\n        result = client.parseDOM(result, 'div', attrs={\n            'class': 'widget search-page',\n        })[0]\n        result = client.parseDOM(result, 'td')\n        tvshowtitle = cleantitle.tv(tvshowtitle)\n        years = [('(%s)' % str(year)), ('(%s)' % str((int(year) + 1))), ('(%s)' % str((int(year) - 1)))]\n        result = [(client.parseDOM(i, 'a', ret='href')[(- 1)], client.parseDOM(i, 'a')[(- 1)]) for i in result]\n        result = [i for i in result if (tvshowtitle == cleantitle.tv(i[1]))]\n        result = [i[0] for i in result if any(((x in i[1]) for x in years))][0]\n        url = client.replaceHTMLCodes(result)\n        try:\n            url = urlparse.parse_qs(urlparse.urlparse(url).query)['u'][0]\n        except:\n            pass\n        url = urlparse.urlparse(url).path\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef get_all_vms(self):\n    '\\n        Returns a generator over all VMs known to this vCenter host.\\n        '\n    for folder in self.get_first_level_of_vm_folders():\n        for vm in get_all_vms_in_folder(folder):\n            (yield vm)\n", "label": 0}
{"function": "\n\ndef wait_for_responses(self, client):\n    'Waits for all responses to come back and resolves the\\n        eventual results.\\n        '\n    assert_open(self)\n    if self.has_pending_requests:\n        raise RuntimeError('Cannot wait for responses if there are pending requests outstanding.  You need to wait for pending requests to be sent first.')\n    pending = self.pending_responses\n    self.pending_responses = []\n    for (command_name, promise) in pending:\n        value = client.parse_response(self.connection, command_name)\n        promise.resolve(value)\n", "label": 0}
{"function": "\n\ndef test_write_job3():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'write': ['test.test', 'test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef __update_copyright(self):\n    'Finds the copyright text and replaces it.'\n    region = self.__find_copyright()\n    self.__replace_copyright(region)\n", "label": 0}
{"function": "\n\ndef __init__(self, idx):\n    self.idx = _uidx()\n    self.isBatch = False\n    self.isSeq = True\n    if isinstance(idx, BaseArray):\n        arr = ct.c_void_p(0)\n        if (idx.type() == Dtype.b8.value):\n            safe_call(backend.get().af_where(ct.pointer(arr), idx.arr))\n        else:\n            safe_call(backend.get().af_retain_array(ct.pointer(arr), idx.arr))\n        self.idx.arr = arr\n        self.isSeq = False\n    elif isinstance(idx, ParallelRange):\n        self.idx.seq = idx\n        self.isBatch = True\n    else:\n        self.idx.seq = Seq(idx)\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/armor/component/shared_deflector_shield_generator_energy_ray.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef leq(levels, int_time=1.0):\n    '\\n    Equivalent level :math:`L_{eq}`.\\n    \\n    :param levels: Levels as function of time.\\n    :param int_time: Integration time. Default value is 1.0 second.\\n    :returns: Equivalent level L_{eq}.\\n    \\n    Sum of levels in dB.\\n    '\n    levels = np.asarray(levels)\n    time = (levels.size * int_time)\n    return _leq(levels, time)\n", "label": 0}
{"function": "\n\ndef get_next_instruction(self):\n    dis = self.disassemble(address=self.program_counter()[1], count=1)\n    return dis.partition('\\n')[0].strip()\n", "label": 0}
{"function": "\n\ndef create(kernel):\n    result = Intangible()\n    result.template = 'object/draft_schematic/munition/shared_detonator_thermal_imperial_issue.iff'\n    result.attribute_template_id = (- 1)\n    result.stfName('string_id_table', '')\n    return result\n", "label": 0}
{"function": "\n\ndef get_dates(self, resource):\n    '\\n        Retrieve dates from mercurial\\n        '\n    try:\n        commits = subprocess.check_output(['hg', 'log', '--template={date|isodatesec}\\n', resource.path]).split('\\n')\n        commits = commits[:(- 1)]\n    except subprocess.CalledProcessError:\n        self.logger.warning(('Unable to get mercurial history for [%s]' % resource))\n        commits = None\n    if (not commits):\n        self.logger.warning(('No mercurial history for [%s]' % resource))\n        return (None, None)\n    created = parse(commits[(- 1)].strip())\n    modified = parse(commits[0].strip())\n    return (created, modified)\n", "label": 0}
{"function": "\n\ndef test_tx_out_bitcoin_address(self):\n    coinbase_bytes = h2b('04ed66471b02c301')\n    tx = Tx.coinbase_tx(COINBASE_PUB_KEY_FROM_80971, int((50 * 100000000.0)), COINBASE_BYTES_FROM_80971)\n    self.assertEqual(tx.txs_out[0].bitcoin_address(), '1DmapcnrJNGeJB13fv9ngRFX1iRvR4zamn')\n", "label": 0}
{"function": "\n\ndef alert_smtp(alert, metric):\n    if ('@' in alert[1]):\n        sender = settings.ALERT_SENDER\n        recipient = alert[1]\n    else:\n        sender = settings.SMTP_OPTS['sender']\n        recipients = settings.SMTP_OPTS['recipients'][alert[0]]\n    if (type(recipients) is str):\n        recipients = [recipients]\n    for recipient in recipients:\n        msg = MIMEMultipart('alternative')\n        msg['Subject'] = ('[skyline alert] ' + metric[1])\n        msg['From'] = sender\n        msg['To'] = recipient\n        link = (settings.GRAPH_URL % metric[1])\n        body = ('Anomalous value: %s <br> Next alert in: %s seconds <a href=\"%s\"><img src=\"%s\"/></a>' % (metric[0], alert[2], link, link))\n        msg.attach(MIMEText(body, 'html'))\n        s = SMTP('127.0.0.1')\n        s.sendmail(sender, recipient, msg.as_string())\n        s.quit()\n", "label": 0}
{"function": "\n\ndef delete_key(self, key):\n    self.server.request('delete', ('/keys/%s' % key))\n", "label": 0}
{"function": "\n\ndef _create_placeholders(self, n_features, n_classes):\n    ' Create the TensorFlow placeholders for the model.\\n        :param n_features: number of features of the first layer\\n        :param n_classes: number of classes\\n        :return: self\\n        '\n    self.keep_prob = tf.placeholder('float')\n    self.hrand = [tf.placeholder('float', [None, self.layers[(l + 1)]]) for l in range((self.n_layers - 1))]\n    self.vrand = [tf.placeholder('float', [None, self.layers[l]]) for l in range((self.n_layers - 1))]\n    self.x = tf.placeholder('float', [None, n_features])\n    self.y_ = tf.placeholder('float', [None, n_classes])\n", "label": 0}
{"function": "\n\ndef __init__(self, name_suggestion):\n    self.name_suggestion = name_suggestion\n", "label": 0}
{"function": "\n\ndef render_datalist(self, list_id):\n    return ''.join([('<datalist id=\"%s\">' % list_id), ''.join([('<option>%s</option>' % color) for color in self.colors]), '</datalist>'])\n", "label": 0}
{"function": "\n\ndef _do_remove(self, section, option):\n    if (not self.config.has_option(section, option)):\n        raise AdminCommandError(_(\"Option '%(option)s' doesn't exist in section '%(section)s'\", option=option, section=section))\n    self.config.remove(section, option)\n    self.config.save()\n    if ((section == 'inherit') and (option == 'file')):\n        self.config.parse_if_needed(force=True)\n", "label": 0}
{"function": "\n\ndef _unit_file(self, name):\n    for extension in ['service', 'yaml']:\n        file_path = '{0}.{1}'.format(name, extension)\n        if path.exists(file_path):\n            with open(file_path) as handle:\n                if (extension == 'service'):\n                    return handle.read()\n                data = yaml.load(handle)\n                if (self._global and ('global' in data)):\n                    return data['global']\n                if (self._name in data):\n                    return data[self._name]\n                raise ValueError('No unit found for {0}'.format(self._name))\n    raise ValueError('No unit file: '.format(name))\n", "label": 0}
{"function": "\n\ndef _create_flavor(self, description=None):\n    flavor = {\n        'flavor': {\n            'name': 'GOLD',\n            'service_type': constants.DUMMY,\n            'description': (description or 'the best flavor'),\n            'enabled': True,\n        },\n    }\n    return (self.plugin.create_flavor(self.ctx, flavor), flavor)\n", "label": 0}
{"function": "\n\ndef test_logout(self):\n    'Tests when logging out with and without continue URL.'\n    host = 'foo.com:1234'\n    path_info = '/_ah/login'\n    cookie_dict = {\n        'dev_appserver_login': ('%s:False:%s' % (EMAIL, USER_ID)),\n    }\n    action = 'Logout'\n    set_email = ''\n    set_admin = False\n    continue_url = ''\n    expected_set = login._clear_user_info_cookie().strip()\n    (status, location, set_cookie, _) = self._run_test(host, path_info, cookie_dict, action, set_email, set_admin, continue_url)\n    self.assertEqual(302, status)\n    self.assertEqual(('http://%s%s' % (host, path_info)), location)\n    self.assertEqual(expected_set, set_cookie)\n    self.assertIsInstance(location, str)\n    self.assertIsInstance(set_cookie, str)\n    continue_url = 'http://foo.com/blah'\n    (status, location, set_cookie, _) = self._run_test(host, path_info, cookie_dict, action, set_email, set_admin, continue_url)\n    self.assertEqual(302, status)\n    self.assertEqual(continue_url, location)\n    self.assertEqual(expected_set, set_cookie)\n    self.assertIsInstance(location, str)\n    self.assertIsInstance(set_cookie, str)\n", "label": 0}
{"function": "\n\ndef test04c__getitem__(self):\n    'Checking cols.__getitem__() with subgroups with a range index with\\n        step.'\n    tbl = self.h5file.create_table('/', 'test', self._TestTDescr, title=self._getMethodName())\n    tbl.append(self._testAData)\n    if self.reopen:\n        self._reopen()\n        tbl = self.h5file.root.test\n    nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)\n    tblcols = tbl.cols._f_col('Info')[0::2]\n    nrarrcols = nrarr['Info'][0::2]\n    if common.verbose:\n        print('Read cols:', tblcols)\n        print('Should look like:', nrarrcols)\n    self.assertTrue(common.areArraysEqual(nrarrcols, tblcols), \"Original array are retrieved doesn't match.\")\n", "label": 0}
{"function": "\n\ndef test_key_has_correct_repr(self):\n    '\\n        Calling repr on a Key instance returns the proper string.\\n        '\n    key = pem.Key(b'test')\n    assert ('<Key({0})>'.format(TEST_DIGEST) == repr(key))\n", "label": 0}
{"function": "\n\ndef calcScale(self, testCount):\n    import math\n    scale = int((self.size / (math.sqrt(testCount) + 1)))\n    return scale\n", "label": 0}
{"function": "\n\ndef get_repository_info(self):\n    '\\n        Find out information about the current Bazaar branch (if any) and\\n        return it.\\n        '\n    if (not check_install(['bzr', 'help'])):\n        logging.debug('Unable to execute \"bzr help\": skipping Bazaar')\n        return None\n    bzr_info = execute(['bzr', 'info'], ignore_errors=True)\n    if ('ERROR: Not a branch:' in bzr_info):\n        repository_info = None\n    else:\n        branch_match = re.search(self.BRANCH_REGEX, bzr_info, re.MULTILINE)\n        path = branch_match.group('branch_path')\n        if (path == '.'):\n            path = os.getcwd()\n        repository_info = RepositoryInfo(path=path, base_path='/', supports_parent_diffs=True)\n    return repository_info\n", "label": 0}
{"function": "\n\ndef do_access_token_response(self, access_token, atinfo, state, refresh_token=None):\n    _tinfo = {\n        'access_token': access_token,\n        'expires_in': atinfo['exp'],\n        'token_type': 'bearer',\n        'state': state,\n    }\n    try:\n        _tinfo['scope'] = atinfo['scope']\n    except KeyError:\n        pass\n    if refresh_token:\n        _tinfo['refresh_token'] = refresh_token\n    return AccessTokenResponse(**by_schema(AccessTokenResponse, **_tinfo))\n", "label": 0}
{"function": "\n\ndef test_set_messages_success(self):\n    author = {\n        'name': 'John Doe',\n        'slug': 'success-msg',\n    }\n    add_url = reverse('add_success_msg')\n    req = self.client.post(add_url, author)\n    self.assertIn((ContactFormViewWithMsg.success_message % author), req.cookies['messages'].value)\n", "label": 0}
{"function": "\n\ndef __init__(self, question, docs):\n    super(Extractor, self).__init__(question, docs, tag=TAG)\n", "label": 0}
{"function": "\n\ndef expect(self, method=None, uri=None, params={\n    \n}):\n    if method:\n        self.assertEqual(method, self.executor.request.method)\n    if uri:\n        self.assertEqual(self.executor.request.uri, ('https://api-ssl.bitly.com/v3' + uri))\n    if params:\n        params.update({\n            'access_token': 'my-access-token',\n        })\n        self.assertEqual(self.executor.request.params, params)\n", "label": 0}
{"function": "\n\ndef test_handle_error_401_sends_challege_default_realm(self):\n    api = restplus.Api(self.app, serve_challenge_on_401=True)\n    exception = HTTPException()\n    exception.code = 401\n    exception.data = {\n        'foo': 'bar',\n    }\n    with self.app.test_request_context('/foo'):\n        resp = api.handle_error(exception)\n        self.assertEqual(resp.status_code, 401)\n        self.assertEqual(resp.headers['WWW-Authenticate'], 'Basic realm=\"flask-restplus\"')\n", "label": 0}
{"function": "\n\ndef __add__(self, other):\n    if isinstance(other, BaseNull):\n        return other\n    return self.map((Q + _unwrap(other)))\n", "label": 0}
{"function": "\n\ndef next_hop(tokeniser):\n    value = tokeniser()\n    if (value.lower() == 'self'):\n        return (IPSelf(tokeniser.afi), NextHopSelf(tokeniser.afi))\n    else:\n        ip = IP.create(value)\n        if (ip.afi == AFI.ipv4):\n            return (ip, NextHop(ip.top()))\n        return (ip, None)\n", "label": 0}
{"function": "\n\ndef test_is_variant(self):\n    expander = GvcfExpander()\n    self.assertTrue(expander.is_variant(json.loads(self.snp_1)))\n    self.assertTrue(expander.is_variant(json.loads(self.snp_2)))\n    self.assertTrue(expander.is_variant(json.loads(self.insertion_1)))\n    self.assertTrue(expander.is_variant(json.loads(self.deletion_1)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_a)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_b)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_c)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_d)))\n    self.assertFalse(expander.is_variant(json.loads(self.ref_ambiguous)))\n    self.assertFalse(expander.is_variant(json.loads(self.no_call_1)))\n", "label": 0}
{"function": "\n\ndef test_coerce_on_select(nyc):\n    t = symbol('t', discover(nyc))\n    t = t[(((((((((t.pickup_latitude >= 40.477399) & (t.pickup_latitude <= 40.917577)) & (t.dropoff_latitude >= 40.477399)) & (t.dropoff_latitude <= 40.917577)) & (t.pickup_longitude >= (- 74.25909))) & (t.pickup_longitude <= (- 73.700272))) & (t.dropoff_longitude >= (- 74.25909))) & (t.dropoff_longitude <= (- 73.700272))) & (t.passenger_count < 6))]\n    t = transform(t, pass_count=(t.passenger_count + 1))\n    result = compute(t.pass_count.coerce('float64'), nyc, return_type='native')\n    s = odo(result, pd.Series)\n    expected = (compute(t, nyc, return_type=pd.DataFrame).passenger_count.astype('float64') + 1.0)\n    assert (list(s) == list(expected))\n", "label": 0}
{"function": "\n\ndef modify_updates(self, updates):\n    '\"\\n        Modifies the parameters before a learning update is applied. Behavior\\n        is defined by subclass\\'s implementation of _modify_updates and any\\n        ModelExtension\\'s implementation of post_modify_updates.\\n\\n        Parameters\\n        ----------\\n        updates : dict\\n            A dictionary mapping shared variables to symbolic values they\\n            will be updated to\\n\\n        Notes\\n        -----\\n        For example, if a given parameter is not meant to be learned, a\\n        subclass or extension\\n        should remove it from the dictionary. If a parameter has a restricted\\n        range, e.g.. if it is the precision of a normal distribution,\\n        a subclass or extension should clip its update to that range. If a\\n        parameter\\n        has any other special properties, its updates should be modified\\n        to respect that here, e.g. a matrix that must be orthogonal should\\n        have its update value modified to be orthogonal here.\\n\\n        This is the main mechanism used to make sure that generic training\\n        algorithms such as those found in pylearn2.training_algorithms\\n        respect the specific properties of the models passed to them.\\n        '\n    self._modify_updates(updates)\n    self._ensure_extensions()\n    for extension in self.extensions:\n        extension.post_modify_updates(updates, self)\n", "label": 0}
{"function": "\n\ndef set_flipped(self, x, y):\n    ' Sets the specified piece as flipped.\\n        '\n    self.pieces[(x + (y * self.width))].set_flipped()\n", "label": 0}
{"function": "\n\ndef on_leave(self, details):\n    self.disconnect()\n", "label": 0}
{"function": "\n\n@needs_mail\n@needs_link\ndef proxy(request, mail, link):\n    return link.get_target(mail)(request, mail.person, mail.job.group_object)\n", "label": 0}
{"function": "\n\ndef TestParallelModify(instances):\n    'PERFORMANCE: Parallel instance modify.\\n\\n  @type instances: list of L{qa_config._QaInstance}\\n  @param instances: list of instances to issue modify commands against\\n\\n  '\n    job_driver = _JobQueueDriver()\n    new_min_mem = qa_config.get(constants.BE_MAXMEM)\n    for instance in instances:\n        cmd = ['gnt-instance', 'modify', '--submit', '-B', ('%s=%s' % (constants.BE_MINMEM, new_min_mem))]\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n        cmd = ['gnt-instance', 'modify', '--submit', '-O', 'fake_os_param=fake_value']\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n        cmd = ['gnt-instance', 'modify', '--submit', '-O', 'fake_os_param=fake_value', '-B', ('%s=%s' % (constants.BE_MINMEM, new_min_mem))]\n        cmd.append(instance.name)\n        job_driver.AddJob(_ExecuteJobSubmittingCmd(cmd))\n    job_driver.WaitForCompletion()\n", "label": 0}
{"function": "\n\ndef get_dir(self, path, dest='', saltenv='base', gzip=None, cachedir=None):\n    '\\n        Get a directory recursively from the salt-master\\n        '\n    ret = []\n    path = self._check_proto(path).rstrip('/')\n    separated = path.rsplit('/', 1)\n    if (len(separated) != 2):\n        prefix = ''\n    else:\n        prefix = separated[0]\n    for fn_ in self.file_list(saltenv, prefix=path):\n        try:\n            if (fn_[len(path)] != '/'):\n                continue\n        except IndexError:\n            continue\n        minion_relpath = fn_[len(prefix):].lstrip('/')\n        ret.append(self.get_file(salt.utils.url.create(fn_), '{0}/{1}'.format(dest, minion_relpath), True, saltenv, gzip))\n    try:\n        for fn_ in self.file_list_emptydirs(saltenv, prefix=path):\n            try:\n                if (fn_[len(path)] != '/'):\n                    continue\n            except IndexError:\n                continue\n            minion_relpath = fn_[len(prefix):].lstrip('/')\n            minion_mkdir = '{0}/{1}'.format(dest, minion_relpath)\n            if (not os.path.isdir(minion_mkdir)):\n                os.makedirs(minion_mkdir)\n            ret.append(minion_mkdir)\n    except TypeError:\n        pass\n    ret.sort()\n    return ret\n", "label": 1}
{"function": "\n\n@require_creds(True)\n@rpcmethod(signature=[SUCCESS_TYPE, URN_TYPE, CREDENTIALS_TYPE], url_name='openflow_gapi')\ndef DeleteSliver(slice_urn, credentials, **kwargs):\n    logger.debug('Called DeleteSliver')\n    try:\n        return gapi.DeleteSliver(slice_urn, kwargs['request'].user)\n    except Slice.DoesNotExist:\n        no_such_slice(slice_urn)\n", "label": 0}
{"function": "\n\ndef _dependencies(self):\n    projects = []\n    for attr in ('install_requires', 'tests_require'):\n        requirements = (getattr(self.distribution, attr, None) or [])\n        for project in requirements:\n            if (not project):\n                continue\n            projects.append(pypi.just_name(project))\n    extras = (getattr(self.distribution, 'extras_require', None) or {\n        \n    })\n    for value in extras.values():\n        projects.extend(map(pypi.just_name, value))\n    return projects\n", "label": 0}
{"function": "\n\ndef identity_provider_create(request, idp_id, description=None, enabled=False, remote_ids=None):\n    manager = keystoneclient(request, admin=True).federation.identity_providers\n    try:\n        return manager.create(id=idp_id, description=description, enabled=enabled, remote_ids=remote_ids)\n    except keystone_exceptions.Conflict:\n        raise exceptions.Conflict()\n", "label": 0}
{"function": "\n\ndef save_graph_db_refs(self, sourceDB=None, targetDB=None, edgeDB=None, simpleKeys=False, unpack_edge=None, edgeDictClass=None, graph=None, **kwargs):\n    'apply kwargs to reference DB objects for this graph'\n    if (sourceDB is not None):\n        self.sourceDB = sourceDB\n    else:\n        simpleKeys = True\n    if (targetDB is not None):\n        self.targetDB = targetDB\n    if (edgeDB is not None):\n        self.edgeDB = edgeDB\n    else:\n        self.pack_edge = self.unpack_edge = (lambda edge: edge)\n    if simpleKeys:\n        self.__class__ = self._IDGraphClass\n    if (unpack_edge is not None):\n        self.unpack_edge = unpack_edge\n    if (graph is not None):\n        self.graph = graph\n    if (edgeDictClass is not None):\n        self.edgeDictClass = edgeDictClass\n", "label": 0}
{"function": "\n\ndef __init__(self, errmsg='You need override this method'):\n    super(NeedOverrideError, self).__init__(self, errmsg)\n", "label": 0}
{"function": "\n\ndef play_rtmpdump_stream(player, url, params={\n    \n}):\n    cmdline = (\"rtmpdump -r '%s' \" % url)\n    for key in params.keys():\n        cmdline += (((key + ' ') + params[key]) if (params[key] != None) else ('' + ' '))\n    cmdline += (' -o - | %s -' % player)\n    print(cmdline)\n    os.system(cmdline)\n    return\n", "label": 0}
{"function": "\n\ndef test_benchmark_variance_06(self):\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['Monthly'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.three_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['3-Month'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.six_month_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['6-month'])\n    np.testing.assert_almost_equal([x.benchmark_variance for x in self.metrics_06.year_periods], ANSWER_KEY.ALGORITHM_PERIOD_BENCHMARK_VARIANCE['year'])\n", "label": 0}
{"function": "\n\n@classmethod\ndef size(cls, val, str_cache, object_cache, traits_cache):\n    if (val in object_cache):\n        index = object_cache.index(val)\n        return AMF3Integer.size((index << 1))\n    else:\n        object_cache.append(val)\n        size = 0\n        traits = type(val)\n        if (traits in traits_cache):\n            index = traits_cache.index(traits)\n            size += AMF3Integer.size(((index << 2) | 1))\n        else:\n            header = 3\n            if traits.__dynamic__:\n                header |= (2 << 2)\n            if traits.__externalizable__:\n                header |= (1 << 2)\n            header |= (len(traits.__members__) << 4)\n            size += AMF3Integer.size(header)\n            if isinstance(val, AMF3Object):\n                size += U8.size\n            else:\n                size += AMF3String.size(traits.__name__, cache=str_cache)\n                traits_cache.append(traits)\n            for member in traits.__members__:\n                size += AMF3String.size(member, cache=str_cache)\n        for member in traits.__members__:\n            value = getattr(val, member)\n            size += AMF3Value.size(value, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n        if traits.__dynamic__:\n            if isinstance(val, AMF3Object):\n                iterator = val.items()\n            else:\n                iterator = val.__dict__.items()\n            for (key, value) in iterator:\n                if (key in traits.__members__):\n                    continue\n                size += AMF3String.size(key, cache=str_cache)\n                size += AMF3Value.size(value, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n            size += U8.size\n        return size\n", "label": 1}
{"function": "\n\ndef all_job_data(jobs, job_type):\n    ' Return an iterator over all job data. Exclude config template dups. '\n    conf_tmpl_ids = []\n    for job in jobs:\n        for (jt, data) in job.iteritems():\n            if (jt == job_type):\n                if (data['config_template_id'] not in conf_tmpl_ids):\n                    conf_tmpl_ids.append(data['config_template_id'])\n                    (yield data)\n", "label": 0}
{"function": "\n\ndef _download_pdf(self, url, base_path):\n    local_file_path = os.path.join(base_path, 'billing-temp-document.pdf')\n    response = requests.get(url, stream=True)\n    should_wipe_bad_headers = True\n    with open(local_file_path, 'wb') as out_file:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                if should_wipe_bad_headers:\n                    pdf_header_pos = chunk.find('%PDF-')\n                    if (pdf_header_pos > 0):\n                        chunk = chunk[pdf_header_pos:]\n                    should_wipe_bad_headers = False\n                out_file.write(chunk)\n                out_file.flush()\n    return local_file_path\n", "label": 0}
{"function": "\n\ndef init_relation(self, models, relation):\n    '\\n        Initialize the relation on a set of models.\\n\\n        :type models: list\\n        :type relation: str\\n        '\n    for model in models:\n        model.set_relation(relation, Result(None, self, model))\n    return models\n", "label": 0}
{"function": "\n\ndef build(self, skip_features=False, managed=False):\n    if hasattr(self.session._internal, 'namespace'):\n        namespace_instance = self.session._internal.namespace['instance']\n        if hasattr(namespace_instance, 'after_request'):\n            getattr(namespace_instance, 'after_request')(self, self.session)\n    if (self.namespace and (not skip_features)):\n        for feature in self.namespace['features']:\n            feature._handle_response(self)\n    if (not isinstance(self.result, UnformattedResponse)):\n        if self.function:\n            self.result = self.function['format'](self.result)\n        if isinstance(self.output_formatter, type):\n            self.output_formatter = self.output_formatter(sapi_request=self.sapi_request, callback=self.callback)\n        if isinstance(self.wrapper, type):\n            self.wrapper = self.wrapper(sapi_request=self.sapi_request)\n        wrapper_result = self.wrapper._build(errors=self.errors, result=self.result)\n        formatter_result = self.output_formatter.build(wrapper_result)\n    else:\n        self.mimetype = self.result.mimetype\n        formatter_result = self.result.content\n    result = {\n        'result': formatter_result,\n        'mimetype': self.mimetype,\n    }\n    if managed:\n        return result\n    else:\n        return self._build_response_obj(self.sapi_request, result)\n", "label": 1}
{"function": "\n\n@classmethod\ndef validate(cls, level):\n    level = int(level)\n    if (level in (cls.NONE, cls.READ, cls.WRITE, cls.ADMIN, cls.SITE_ADMIN)):\n        return level\n    else:\n        raise ValueError(('Invalid AccessType: %d.' % level))\n", "label": 0}
{"function": "\n\n@records.post(validators=record_validator, permission='post_record')\ndef post_record(request):\n    'Saves a single model record.\\n\\n    Posted record attributes will be matched against the related model\\n    definition.\\n\\n    '\n    if (request.headers.get('Validate-Only', 'false') == 'true'):\n        return\n    model_id = request.matchdict['model_id']\n    if request.credentials_id:\n        credentials_id = request.credentials_id\n    else:\n        credentials_id = Everyone\n    record_id = request.db.put_record(model_id, request.data_clean, [credentials_id])\n    request.notify('RecordCreated', model_id, record_id)\n    created = ('%s/models/%s/records/%s' % (request.application_url, model_id, record_id))\n    request.response.status = '201 Created'\n    request.response.headers['location'] = str(created)\n    return {\n        'id': record_id,\n    }\n", "label": 0}
{"function": "\n\ndef on_files_selected(self, paths):\n    \" Handle the 'filesSelected' signal from the dialog.\\n\\n        \"\n    d = self.declaration\n    if (d is not None):\n        d.selected_paths = paths\n", "label": 0}
{"function": "\n\ndef _dispatch(self, inst, kws):\n    assert (self.current_block is not None)\n    fname = ('op_%s' % inst.opname.replace('+', '_'))\n    try:\n        fn = getattr(self, fname)\n    except AttributeError:\n        raise NotImplementedError(inst)\n    else:\n        try:\n            return fn(inst, **kws)\n        except errors.NotDefinedError as e:\n            if (e.loc is None):\n                e.loc = self.loc\n            raise e\n", "label": 0}
{"function": "\n\ndef test_derived(self):\n    import time\n\n    class Local(threading.local):\n\n        def __init__(self):\n            time.sleep(0.01)\n    local = Local()\n\n    def f(i):\n        local.x = i\n        self.assertEqual(local.x, i)\n    threads = []\n    for i in range(10):\n        t = threading.Thread(target=f, args=(i,))\n        t.start()\n        threads.append(t)\n    for t in threads:\n        t.join()\n", "label": 0}
{"function": "\n\ndef test_review_comments(self):\n    \"Show that one can iterate over a PR's review comments.\"\n    cassette_name = self.cassette_name('review_comments')\n    with self.recorder.use_cassette(cassette_name):\n        p = self.get_pull_request()\n        for comment in p.review_comments():\n            assert isinstance(comment, github3.pulls.ReviewComment)\n", "label": 0}
{"function": "\n\ndef update(self, t, dt):\n    if (random.random() < 0.02):\n        self.a += ((random.randint((- 1), 1) * pi) / 8)\n    dx = cos(self.a)\n    dz = sin(self.a)\n    self.x += (dx * dt)\n    self.z += (dz * dt)\n", "label": 0}
{"function": "\n\ndef encryption_oracle(rawInput):\n    key = generateAESKey()\n    iv = generateAESKey()\n    prependAmount = (5 + (getOneRandomByte() % 6))\n    appendAmount = (5 + (getOneRandomByte() % 6))\n    plaintext = (((b'x' * prependAmount) + rawInput) + (b'y' * appendAmount))\n    if (getOneRandomByte() & 1):\n        return aes_ecb_enc(addPKCS7Padding(plaintext, 16), key)\n    else:\n        return aes_cbc_enc(addPKCS7Padding(plaintext, 16), key, iv)\n", "label": 0}
{"function": "\n\ndef parse_status(self, lines):\n    activity = []\n    seen_times = set()\n    for line in lines:\n        (time, fields) = line.split('|')\n        if (time not in seen_times):\n            seen_times.add(time)\n            status_obj = status.Status(int(float(time)), fields)\n            activity.append(status_obj)\n    return activity\n", "label": 0}
{"function": "\n\n@property\ndef directions(self):\n    hashes = []\n    t_query = 'select trip_headsign, trip_short_name, bikes_allowed,\\n        trip_id from trips where route_id=:route_id and _feed=:_feed'\n    t_filter = {\n        'route_id': self.id,\n        '_feed': self.provider.feed_id,\n    }\n    cur = self.provider.conn.cursor()\n    for trip in cur.execute(t_query, t_filter):\n        direction = {\n            \n        }\n        innercur = self.provider.conn.cursor()\n        result = innercur.execute('select s.* from stop_times as st join stops as s\\n                on st.stop_id=s.stop_id and st._feed=s._feed\\n                where st.trip_id=:t_id and st._feed=:_feed', {\n            't_id': trip['trip_id'],\n            '_feed': self.provider.feed_id,\n        })\n        direction['stops'] = [GTFSStop(self.provider, **dict(row)) for row in result]\n        if (trip['trip_headsign'] is not None):\n            direction['headsign'] = trip['trip_headsign']\n        if (trip['trip_short_name'] is not None):\n            direction['short_name'] = trip['trip_short_name']\n        if (trip['bikes_allowed'] is not None):\n            direction['bikes_ok'] = trip['bikes_allowed']\n        h = util.freezehash(direction)\n        if (h not in hashes):\n            hashes.append(h)\n            (yield direction)\n", "label": 0}
{"function": "\n\ndef _partitions_to_src(partitions):\n    return ''.join((part.src for part in partitions))\n", "label": 0}
{"function": "\n\ndef test_input_extra_rewrite(self):\n    self.client_job_description.rewrite_paths = True\n    extra_file = os.path.join(self.input1_files_path, 'moo', 'cow.txt')\n    os.makedirs(os.path.dirname(extra_file))\n    open(extra_file, 'w').write('Hello World!')\n    command_line = ('test.exe %s' % extra_file)\n    self.client_job_description.command_line = command_line\n    self.client.expect_command_line('test.exe /pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt')\n    self.client.expect_put_paths(['/pulsar/staging/1/inputs/dataset_1_files/moo/cow.txt'])\n    self._submit()\n    uploaded_file1 = self.client.put_files[0]\n    assert (uploaded_file1[1] == 'input')\n    assert (uploaded_file1[0] == extra_file)\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.LIST):\n                self.column_names = []\n                (_etype10, _size7) = iprot.readListBegin()\n                for _i11 in xrange(_size7):\n                    _elem12 = iprot.readString()\n                    self.column_names.append(_elem12)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.slice_range = SliceRange()\n                self.slice_range.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_member_addresses(self):\n    addresses = []\n    for member in self.config_members:\n        addresses.append(member.get_server().get_address())\n    return addresses\n", "label": 0}
{"function": "\n\ndef __init__(self, output_type, inplace=False):\n    Op.__init__(self)\n    self.output_type = output_type\n    self.inplace = inplace\n    if inplace:\n        self.destroy_map = {\n            0: [0],\n        }\n    self.warned_numpy_version = False\n", "label": 0}
{"function": "\n\ndef CheckLoggingWorks(self):\n    logger = StringIO.StringIO()\n    expected_output = (';\\n'.join([sqlite.main._BEGIN, 'CREATE TABLE TEST(FOO INTEGER)', 'INSERT INTO TEST(FOO) VALUES (?)', 'ROLLBACK']) + ';\\n')\n    self.cnx = sqlite.connect(self.getfilename(), command_logfile=logger)\n    cu = self.cnx.cursor()\n    cu.execute('CREATE TABLE TEST(FOO INTEGER)')\n    cu.execute('INSERT INTO TEST(FOO) VALUES (?)', (5,))\n    self.cnx.rollback()\n    logger.seek(0)\n    real_output = logger.read()\n    if (expected_output != real_output):\n        self.fail(\"Logging didn't produce expected output.\")\n", "label": 0}
{"function": "\n\ndef load(self, odffile):\n    ' Loads a document into the parser and parses it.\\n            The argument can either be a filename or a document in memory.\\n        '\n    self.lines = []\n    self._wfunc = self._wlines\n    if isinstance(odffile, str):\n        self.document = load(odffile)\n    else:\n        self.document = odffile\n    self._walknode(self.document.topnode)\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n\n    def format_row(row):\n        return ('(%s)' % ', '.join((format_number(value) for value in row)))\n    return ('Matrix44(%s)' % ', '.join((format_row(row) for row in self.rows())))\n", "label": 0}
{"function": "\n\ndef saveXML(self, snode):\n    if (snode is None):\n        snode = self\n    elif (snode.ownerDocument is not self):\n        raise xml.dom.WrongDocumentErr()\n    return snode.toxml()\n", "label": 0}
{"function": "\n\ndef install(self, instance):\n    cfg = self.config\n    if (self.database == self.master_database):\n        template = 'template1'\n    else:\n        template = self.master_database\n    config_opt_map = dict(host='host', user='username', password='password', encoding='encoding', lc_collate='lc-collate', lc_ctype='lc-ctype', tablespace='tablespace')\n    options = [((('--' + opt) + '=') + str(cfg[cfg_name])) for (cfg_name, opt) in config_opt_map.iteritems() if (cfg_name in cfg)]\n    fail = instance.do(((['createdb', '--template', template] + options) + [self.database]))\n    if fail:\n        instance.do(['pg_dump', '-h', str(cfg['host']), '-U', str(cfg['user']), '-E', str(cfg['encoding']), '-f', (('/tmp/' + template) + '.sql'), template])\n        instance.do(((['createdb', '--template', 'template1'] + options) + [self.database]))\n        instance.do(['psql', '-h', str(cfg['host']), '-U', str(cfg['user']), '-f', (('/tmp/' + template) + '.sql'), self.database])\n    info = self.connection_info\n    info['database'] = self.database\n    return info\n", "label": 0}
{"function": "\n\n@contextmanager\ndef trace_ms(statName):\n    if (_instatrace is None):\n        (yield)\n        return\n    now = _instatrace.now_ms()\n    (yield)\n    _instatrace.trace(statName, (_instatrace.now_ms() - now))\n", "label": 0}
{"function": "\n\ndef do(args):\n    ' Main method '\n    if args.name:\n        tc_names = [args.name]\n    else:\n        tc_names = qitoolchain.get_tc_names()\n    for tc_name in tc_names:\n        toolchain = qitoolchain.get_toolchain(tc_name)\n        ui.info(str(toolchain))\n", "label": 0}
{"function": "\n\ndef abort_request(self, stream, ident, parent):\n    'abort a specifig msg by id'\n    msg_ids = parent['content'].get('msg_ids', None)\n    if isinstance(msg_ids, str):\n        msg_ids = [msg_ids]\n    if (not msg_ids):\n        self.abort_queues()\n    for mid in msg_ids:\n        self.aborted.add(str(mid))\n    content = dict(status='ok')\n    reply_msg = self.session.send(stream, 'abort_reply', content=content, parent=parent, ident=ident)\n    self.log.debug(str(reply_msg))\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    ' Returns the materialized path '\n    return ('/'.join([x.value for x in self.parts]) + ('/' if self.is_dir else ''))\n", "label": 0}
{"function": "\n\ndef get_available_user_FIELD_transitions(instance, user, field):\n    '\\n    List of transitions available in current model state\\n    with all conditions met and user have rights on it\\n    '\n    for transition in get_available_FIELD_transitions(instance, field):\n        if transition.has_perm(instance, user):\n            (yield transition)\n", "label": 0}
{"function": "\n\ndef pixelCollision(rect1, rect2, hitmask1, hitmask2):\n    'Checks if two objects collide and not just their rects'\n    rect = rect1.clip(rect2)\n    if ((rect.width == 0) or (rect.height == 0)):\n        return False\n    (x1, y1) = ((rect.x - rect1.x), (rect.y - rect1.y))\n    (x2, y2) = ((rect.x - rect2.x), (rect.y - rect2.y))\n    for x in xrange(rect.width):\n        for y in xrange(rect.height):\n            if (hitmask1[(x1 + x)][(y1 + y)] and hitmask2[(x2 + x)][(y2 + y)]):\n                return True\n    return False\n", "label": 0}
{"function": "\n\ndef our_x2_iterates(n_iters=100):\n    history = []\n    searchspace = SearchSpace()\n    searchspace.add_float('x', (- 10), 10)\n    random = np.random.RandomState(0)\n\n    def fn(params):\n        return (- (params['x'] ** 2))\n    for i in range(n_iters):\n        params = HyperoptTPE(seed=random).suggest(history, searchspace)\n        history.append((params, fn(params), 'SUCCEEDED'))\n    return np.array([h[0]['x'] for h in history])\n", "label": 0}
{"function": "\n\ndef insert(self, window, first_line, *lines):\n    (row, column) = cursor = self.cursors[window]\n    (left, right) = (self[row][:column], self[row][column:])\n    added = len(lines)\n    if lines:\n        last_line = lines[(- 1)]\n        column = len(last_line)\n    else:\n        last_line = first_line\n        column += len(first_line)\n    self[row] = (left + first_line)\n    self[(row + 1):(row + 1)] = lines\n    self[(row + added)] += right\n    for other in self.cursors.itervalues():\n        if (other.row > row):\n            other._row += added\n    cursor.coords = ((row + added), column)\n", "label": 0}
{"function": "\n\ndef clean_message(self):\n    message = self.cleaned_data['message']\n    try:\n        message = message.decode('base64')\n    except TypeError as e:\n        raise ValidationError(('Cannot convert to binary: %r' % e.msg))\n    if (len(message) % 16):\n        raise ValidationError('Wrong block size for message !')\n    if (len(message) <= 16):\n        raise ValidationError('Message too short or missing IV !')\n    return message\n", "label": 0}
{"function": "\n\ndef _bump_version(self, version):\n    try:\n        parts = map(int, version.split('.'))\n    except ValueError:\n        self._fail('Current version is not numeric')\n    parts[(- 1)] += 1\n    return '.'.join(map(str, parts))\n", "label": 0}
{"function": "\n\ndef test_disenroll_with_no_enrollment(self):\n    courses = Course.objects.all()\n    for course in courses:\n        course.delete()\n    client = Client()\n    client.login(username=TEST_USER_USERNAME, password=TEST_USER_PASSWORD)\n    kwargs = {\n        'HTTP_X_REQUESTED_WITH': 'XMLHttpRequest',\n    }\n    response = client.post('/disenroll', {\n        'course_id': 1,\n    }, **kwargs)\n    self.assertEqual(response.status_code, 200)\n    json_string = response.content.decode(encoding='UTF-8')\n    array = json.loads(json_string)\n    self.assertEqual(array['message'], 'record does not exist')\n    self.assertEqual(array['status'], 'failed')\n", "label": 0}
{"function": "\n\ndef get_ud(self, cardinal, user, channel, msg):\n    try:\n        word = msg.split(' ', 1)[1]\n    except IndexError:\n        cardinal.sendMsg(channel, 'Syntax: .ud <word>')\n        return\n    try:\n        url = (URBANDICT_API_PREFIX + word)\n        f = urlopen(url).read()\n        data = json.loads(f)\n        word_def = data['list'][0]['definition']\n        link = data['list'][0]['permalink']\n        response = ('UD for %s: %s (%s)' % (word, word_def, link))\n        cardinal.sendMsg(channel, response.encode('utf-8'))\n    except Exception:\n        cardinal.sendMsg(channel, ('Could not retrieve definition for %s' % word))\n", "label": 0}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    (ishape, fshape) = shapes\n    (igroups, icolors_per_group, irows, icols, icount) = ishape\n    (fmodulesR, fmodulesC, fcolors, frows, fcols) = fshape[:(- 2)]\n    (fgroups, filters_per_group) = fshape[(- 2):]\n    if ((not any_symbolic(irows, icols)) and (irows != icols)):\n        raise ValueError('non-square image argument', (irows, icols))\n    if ((not any_symbolic(frows, fcols)) and (frows != fcols)):\n        raise ValueError('non-square filter shape', (frows, fcols))\n    if ((not any_symbolic(fmodulesR, fmodulesC)) and (fmodulesR != fmodulesC)):\n        raise ValueError('non-square filter grouping', (fmodulesR, fmodulesC))\n    if ((not any_symbolic(icolors_per_group, fcolors)) and (icolors_per_group != fcolors)):\n        raise ValueError(\"color counts don't match\", (icolors_per_group, fcolors))\n    hshape = (fgroups, filters_per_group, fmodulesR, fmodulesC, icount)\n    return [hshape]\n", "label": 0}
{"function": "\n\ndef store_and_use_artifact(self, cache_key, src, results_dir=None):\n    'Read the content of a tarball from an iterator and return an artifact stored in the cache.'\n    with self._tmpfile(cache_key, 'read') as tmp:\n        for chunk in src:\n            tmp.write(chunk)\n        tmp.close()\n        tarball = self._store_tarball(cache_key, tmp.name)\n        artifact = self._artifact(tarball)\n        if (results_dir is not None):\n            safe_rmtree(results_dir)\n        artifact.extract()\n        return True\n", "label": 0}
{"function": "\n\ndef __str__(self):\n    return ('Tuple(%s)' % ', '.join((str(elt) for elt in self.elts)))\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (other is None):\n        return (self._value is None)\n    other = str(other)\n    if (other not in self.values_range):\n        raise ConanException(bad_value_msg(self._name, other, self.values_range))\n    return (other == self.__str__())\n", "label": 0}
{"function": "\n\ndef _set_play_state(self, state):\n    'Helper method for play/pause/toggle.'\n    players = self._get_players()\n    if (len(players) != 0):\n        self._server.Player.PlayPause(players[0]['playerid'], state)\n    self.update_ha_state()\n", "label": 0}
{"function": "\n\ndef _prepare_ivy_xml(self, frozen_resolution, ivyxml, resolve_hash_name_for_report):\n    default_resolution = frozen_resolution.get('default')\n    if (default_resolution is None):\n        raise IvyUtils.IvyError(\"Couldn't find the frozen resolution for the 'default' ivy conf.\")\n    try:\n        jars = default_resolution.jar_dependencies\n        IvyUtils.generate_fetch_ivy(jars, ivyxml, self.confs, resolve_hash_name_for_report)\n    except Exception as e:\n        raise IvyUtils.IvyError('Failed to prepare ivy resolve: {}'.format(e))\n", "label": 0}
{"function": "\n\n@continuation\ndef imp_struct_set_cont(orig_struct, setter, field, app, env, cont, _vals):\n    from pycket.interpreter import check_one_val\n    val = check_one_val(_vals)\n    if (setter is values.w_false):\n        return orig_struct.set_with_extra_info(field, val, app, env, cont)\n    return setter.call_with_extra_info([orig_struct, val], env, cont, app)\n", "label": 0}
{"function": "\n\ndef _do_if_else_condition(self, condition):\n    '\\n        Common logic for evaluating the conditions on #if, #ifdef and\\n        #ifndef lines.\\n        '\n    self.save()\n    d = self.dispatch_table\n    if condition:\n        self.start_handling_includes()\n        d['elif'] = self.stop_handling_includes\n        d['else'] = self.stop_handling_includes\n    else:\n        self.stop_handling_includes()\n        d['elif'] = self.do_elif\n        d['else'] = self.start_handling_includes\n", "label": 0}
{"function": "\n\ndef loadWordFile(self, pre_processor=None):\n    filename = self.getDictionaryPath()\n    with codecs.open(filename, 'r', 'utf-8') as fp:\n        for word in fp.readlines():\n            if pre_processor:\n                self.add(pre_processor(word.strip()))\n            else:\n                self.add(word.strip())\n    return\n", "label": 0}
{"function": "\n\n@sig((((H / ((H / 'a') >> bool)) >> ['a']) >> [int]))\ndef findIndicies(f, xs):\n    '\\n    findIndices :: (a -> Bool) -> [a] -> [Int]\\n\\n    The findIndices function extends findIndex, by returning the indices of all\\n    elements satisfying the predicate, in ascending order.\\n    '\n    return L[(i for (i, x) in enumerate(xs) if f(x))]\n", "label": 0}
{"function": "\n\ndef get_all_active_nodes(self, is_running=None):\n    if self.active_gen_id:\n        return self.get_all_nodes(self.active_gen_id, is_running=is_running)\n    return []\n", "label": 0}
{"function": "\n\ndef _option_required(self, key):\n    conf = S3_CONF.copy()\n    del conf[key]\n    try:\n        self.store = Store(test_utils.TestConfigOpts(conf))\n        return (self.store.add == self.store.add_disabled)\n    except:\n        return False\n    return False\n", "label": 0}
{"function": "\n\ndef _is_verified_address(self, address):\n    if (address in self.addresses):\n        return True\n    (user, host) = address.split('@', 1)\n    return (host in self.domains)\n", "label": 0}
{"function": "\n\ndef deserialize(self, raw_value):\n    if (raw_value.upper() in self.TRUE_RAW_VALUES):\n        return True\n    elif (raw_value.upper() in self.FALSE_RAW_VALUES):\n        return False\n    else:\n        raise DeserializationError('Value \"{}\" must be one of {} for \"{}\"!'.format(raw_value, self.ALLOWED_RAW_VALUES, self.name), raw_value, self.name)\n", "label": 0}
{"function": "\n\ndef extract_info(self, body):\n    '\\n        Extract metadata url\\n        '\n    xhr_url_match = re.search(self._XHR_REQUEST_PATH, body)\n    if (xhr_url_match is not None):\n        xhr_url = xhr_url_match.group(1)\n    else:\n        xhr_url = None\n    if ((xhr_url is not None) and xhr_url.endswith('xml')):\n        default_filename = xhr_url.split('/')[1]\n    else:\n        self.error(ExtractionError, \"ERROR: can't get default_filename.\")\n    return {\n        'default_filename': default_filename,\n        'xhr_url': xhr_url,\n    }\n", "label": 0}
{"function": "\n\ndef WriteScanNode(self, scan_node, indentation=''):\n    'Writes the source scanner node to stdout.\\n\\n    Args:\\n      scan_node: the scan node (instance of SourceScanNode).\\n      indentation: optional indentation string.\\n      scan_step: optional integer indicating the scan step.\\n    '\n    if (not scan_node):\n        return\n    values = []\n    part_index = getattr(scan_node.path_spec, 'part_index', None)\n    if (part_index is not None):\n        values.append('{0:d}'.format(part_index))\n    store_index = getattr(scan_node.path_spec, 'store_index', None)\n    if (store_index is not None):\n        values.append('{0:d}'.format(store_index))\n    start_offset = getattr(scan_node.path_spec, 'start_offset', None)\n    if (start_offset is not None):\n        values.append('start offset: {0:d} (0x{0:08x})'.format(start_offset))\n    location = getattr(scan_node.path_spec, 'location', None)\n    if (location is not None):\n        values.append('location: {0:s}'.format(location))\n    print('{0:s}{1:s}: {2:s}'.format(indentation, scan_node.path_spec.type_indicator, ', '.join(values)))\n    indentation = '  {0:s}'.format(indentation)\n    for sub_scan_node in scan_node.sub_nodes:\n        self.WriteScanNode(sub_scan_node, indentation=indentation)\n", "label": 0}
{"function": "\n\ndef seek(self, offset, whence=os.SEEK_SET):\n    'Seek to the provided location in the file.\\n\\n        :param offset: location to seek to\\n        :type offset: int\\n        :param whence: determines whether `offset` represents a\\n                       location that is absolute, relative to the\\n                       beginning of the file, or relative to the end\\n                       of the file\\n        :type whence: os.SEEK_SET | os.SEEK_CUR | os.SEEK_END\\n        :returns: None\\n        :rtype: None\\n        '\n    if (whence == os.SEEK_SET):\n        self._cursor = (0 + offset)\n    elif (whence == os.SEEK_CUR):\n        self._cursor += offset\n    elif (whence == os.SEEK_END):\n        self._cursor = (self.size() + offset)\n    else:\n        raise ValueError('Unexpected value for `whence`: {}'.format(whence))\n", "label": 0}
{"function": "\n\ndef _eval_is_infinite(self):\n    if any((a.is_infinite for a in self.args)):\n        if any((a.is_zero for a in self.args)):\n            return S.NaN.is_infinite\n        if any(((a.is_zero is None) for a in self.args)):\n            return None\n        return True\n", "label": 0}
{"function": "\n\ndef create_security_groups(self):\n    for hostdef in self.blueprint.host_definitions.all():\n        sg_name = 'stackdio-managed-{0}-stack-{1}'.format(hostdef.slug, self.pk)\n        sg_description = 'stackd.io managed security group'\n        account = hostdef.cloud_image.account\n        if (not account.create_security_groups):\n            logger.debug('Skipping creation of {0} because security group creation is turned off for the account'.format(sg_name))\n            continue\n        driver = account.get_driver()\n        try:\n            sg_id = driver.create_security_group(sg_name, sg_description, delete_if_exists=True)\n        except Exception as e:\n            err_msg = 'Error creating security group: {0}'.format(str(e))\n            self.set_status('create_security_groups', self.ERROR, err_msg, Level.ERROR)\n        logger.debug('Created security group {0}: {1}'.format(sg_name, sg_id))\n        for access_rule in hostdef.access_rules.all():\n            driver.authorize_security_group(sg_id, {\n                'protocol': access_rule.protocol,\n                'from_port': access_rule.from_port,\n                'to_port': access_rule.to_port,\n                'rule': access_rule.rule,\n            })\n        self.security_groups.create(account=account, blueprint_host_definition=hostdef, name=sg_name, description=sg_description, group_id=sg_id, is_managed=True)\n", "label": 0}
{"function": "\n\ndef get_style(self, attribute):\n    \"Get the document's named style at the caret's current position.\\n\\n        If there is a text selection and the style varies over the selection,\\n        `pyglet.text.document.STYLE_INDETERMINATE` is returned.\\n\\n        :Parameters:\\n            `attribute` : str\\n                Name of style attribute to retrieve.  See\\n                `pyglet.text.document` for a list of recognised attribute\\n                names.\\n\\n        :rtype: object\\n        \"\n    if ((self._mark is None) or (self._mark == self._position)):\n        try:\n            return self._next_attributes[attribute]\n        except KeyError:\n            return self._layout.document.get_style(attribute, self._position)\n    start = min(self._position, self._mark)\n    end = max(self._position, self._mark)\n    return self._layout.document.get_style_range(attribute, start, end)\n", "label": 0}
{"function": "\n\ndef __call__(self, feature=None):\n    if (not current_app):\n        log.warn(\"Got a request to check for {feature} but we're outside the request context. Returning False\".format(feature=feature))\n        return False\n    try:\n        return self.model.check(feature)\n    except NoResultFound:\n        raise NoFeatureFlagFound()\n", "label": 0}
{"function": "\n\ndef populate_link(self, finder, upgrade):\n    'Ensure that if a link can be found for this, that it is found.\\n\\n        Note that self.link may still be None - if Upgrade is False and the\\n        requirement is already installed.\\n        '\n    if (self.link is None):\n        self.link = finder.find_requirement(self, upgrade)\n", "label": 0}
{"function": "\n\ndef get_list_display_links(self, request, list_display):\n    '\\n        Return a sequence containing the fields to be displayed as links\\n        on the changelist. The list_display parameter is the list of fields\\n        returned by get_list_display().\\n        '\n    if (self.list_display_links or (self.list_display_links is None) or (not list_display)):\n        return self.list_display_links\n    else:\n        return list(list_display)[:1]\n", "label": 0}
{"function": "\n\ndef merge_bins(distribution, limit):\n    'Merges the bins of a regression distribution to the given limit number\\n\\n    '\n    length = len(distribution)\n    if ((limit < 1) or (length <= limit) or (length < 2)):\n        return distribution\n    index_to_merge = 2\n    shortest = float('inf')\n    for index in range(1, length):\n        distance = (distribution[index][0] - distribution[(index - 1)][0])\n        if (distance < shortest):\n            shortest = distance\n            index_to_merge = index\n    new_distribution = distribution[:(index_to_merge - 1)]\n    left = distribution[(index_to_merge - 1)]\n    right = distribution[index_to_merge]\n    new_bin = [(((left[0] * left[1]) + (right[0] * right[1])) / (left[1] + right[1])), (left[1] + right[1])]\n    new_distribution.append(new_bin)\n    if (index_to_merge < (length - 1)):\n        new_distribution.extend(distribution[(index_to_merge + 1):])\n    return merge_bins(new_distribution, limit)\n", "label": 0}
{"function": "\n\n@defer.inlineCallbacks\ndef test_GetMoreCount(self):\n    counter = _CallCounter(MongoClientProtocol.send_GETMORE)\n    self.patch(MongoClientProtocol, 'send_GETMORE', counter)\n    (yield self.coll.insert([{\n        'x': 42,\n    } for _ in range(20)]))\n    result = (yield self.coll.find({\n        \n    }, limit=10))\n    self.assertEqual(len(result), 10)\n    self.assertEqual(counter.call_count, 0)\n", "label": 0}
{"function": "\n\ndef print_selection(self, *e):\n    if (self._root is None):\n        return\n    if (self._selection is None):\n        tkinter.messagebox.showerror('Print Error', 'No tree selected')\n    else:\n        c = self._cframe.canvas()\n        for widget in self._treewidgets:\n            if (widget is not self._selection):\n                self._cframe.destroy_widget(widget)\n        c.delete(self._selectbox)\n        (x1, y1, x2, y2) = self._selection.bbox()\n        self._selection.move((10 - x1), (10 - y1))\n        c['scrollregion'] = ('0 0 %s %s' % (((x2 - x1) + 20), ((y2 - y1) + 20)))\n        self._cframe.print_to_file()\n        self._treewidgets = [self._selection]\n        self.clear()\n        self.update()\n", "label": 0}
{"function": "\n\ndef smart_split(text):\n    '\\n    Generator that splits a string by spaces, leaving quoted phrases together.\\n    Supports both single and double quotes, and supports escaping quotes with\\n    backslashes. In the output, strings will keep their initial and trailing\\n    quote marks and escaped quotes will remain escaped (the results can then\\n    be further processed with unescape_string_literal()).\\n\\n    >>> list(smart_split(r\\'This is \"a person\\\\\\'s\" test.\\'))\\n    [\\'This\\', \\'is\\', \\'\"a person\\\\\\\\\\\\\\'s\"\\', \\'test.\\']\\n    >>> list(smart_split(r\"Another \\'person\\\\\\'s\\' test.\"))\\n    [\\'Another\\', \"\\'person\\\\\\\\\\'s\\'\", \\'test.\\']\\n    >>> list(smart_split(r\\'A \"\\\\\"funky\\\\\" style\" test.\\'))\\n    [\\'A\\', \\'\"\\\\\\\\\"funky\\\\\\\\\" style\"\\', \\'test.\\']\\n    '\n    text = force_text(text)\n    for bit in smart_split_re.finditer(text):\n        (yield bit.group(0))\n", "label": 0}
{"function": "\n\n@pytest.mark.parametrize('model_class', (ModelWithVanillaMoneyField, ModelWithChoicesMoneyField))\ndef test_currency_querying(self, model_class):\n    model_class.objects.create(money=Money('100.0', moneyed.ZWN))\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.USD)).count() == 0)\n    assert (model_class.objects.filter(money__lt=Money('1000', moneyed.ZWN)).count() == 1)\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    quiet = options.get('quiet', False)\n    codes = self.get_codes()\n    if (not quiet):\n        if codes:\n            self.stdout.write(('Will now delete codes: %s \\n' % codes))\n        else:\n            self.stdout.write('No Object codes to delete. \\n')\n    revisions = self.get_revisions()\n    if (not quiet):\n        if codes:\n            self.stdout.write((\"Will now delete additional doc's revisions in docs: %s \\n\" % [d[0] for d in revisions]))\n        else:\n            self.stdout.write('No additional revision files to delete. \\n')\n    if (codes or revisions):\n        processor = core.document_processor.DocumentProcessor()\n        user = User.objects.filter(is_superuser=True)[0]\n        for code in codes:\n            processor.delete(code, {\n                'user': user,\n            })\n            if (not processor.errors):\n                if (not quiet):\n                    self.stdout.write(('Permanently deleted object with code: %s' % code))\n            else:\n                if (not quiet):\n                    self.stdout.write(processor.errors)\n                raise (Exception, processor.errors)\n        for rev in revisions:\n            processor.delete(rev[0], {\n                'user': user,\n                'delete_revision': rev[1],\n            })\n", "label": 1}
{"function": "\n\ndef put(self, key, value):\n    ' Updates or inserts data for a specified key '\n    url = ((self.base_url + '/') + str(key))\n    headers = {\n        'content-type': 'application/json',\n    }\n    jvalue = jsonpickle.encode(value)\n    data = self.session.put(url, data=jvalue, headers=headers)\n    logging.debug(('Sending request to ' + url))\n    if (data.status_code == 200):\n        logging.debug(((('The value ' + str(value)) + ' was put in the region for the key ') + str(key)))\n        return True\n    else:\n        self.error_response(data)\n", "label": 0}
{"function": "\n\ndef tearDown(self):\n    for fname in os.listdir(self.tempdir):\n        os.remove(os.path.join(self.tempdir, fname))\n    os.rmdir(self.tempdir)\n", "label": 0}
{"function": "\n\n@property\ndef responses(self):\n    return [response for (request, response) in self.data]\n", "label": 0}
{"function": "\n\ndef _process_element(self, element):\n    'Process first level element of the stream.\\n\\n        The element may be stream error or features, StartTLS\\n        request/response, SASL request/response or a stanza.\\n\\n        :Parameters:\\n            - `element`: XML element\\n        :Types:\\n            - `element`: :etree:`ElementTree.Element`\\n        '\n    tag = element.tag\n    if (tag in self._element_handlers):\n        handler = self._element_handlers[tag]\n        logger.debug('Passing element {0!r} to method {1!r}'.format(element, handler))\n        handled = handler(self, element)\n        if handled:\n            return\n    if tag.startswith(self._stanza_namespace_p):\n        stanza = stanza_factory(element, self, self.language)\n        self.uplink_receive(stanza)\n    elif (tag == ERROR_TAG):\n        error = StreamErrorElement(element)\n        self.process_stream_error(error)\n    elif (tag == FEATURES_TAG):\n        logger.debug('Got features element: {0}'.format(serialize(element)))\n        self._got_features(element)\n    else:\n        logger.debug('Unhandled element: {0}'.format(serialize(element)))\n        logger.debug(' known handlers: {0!r}'.format(self._element_handlers))\n", "label": 0}
{"function": "\n\ndef relax():\n    selection = pm.ls(sl=1)\n    if (not selection):\n        return\n    verts = pm.ls(pm.polyListComponentConversion(tv=1))\n    if (not verts):\n        return\n    shape = verts[0].node()\n    dup = shape.duplicate()[0]\n    dup_shape = dup.getShape()\n    pm.polyAverageVertex(verts, i=1, ch=0)\n    ta_node = pm.transferAttributes(dup, verts, transferPositions=True, transferNormals=False, transferUVs=False, transferColors=False, sampleSpace=0, searchMethod=0, flipUVs=False, colorBorders=1)\n    pm.delete(shape, ch=1)\n    pm.delete(dup)\n    pm.select(selection)\n", "label": 0}
{"function": "\n\ndef test_stats(self):\n    (key, stats) = self.memcache.get_stats()[0]\n    self.assertEqual('127.0.0.1:21122 (1)', key)\n    keys = ['bytes', 'pid', 'time', 'limit_maxbytes', 'cmd_get', 'version', 'bytes_written', 'cmd_set', 'get_misses', 'total_connections', 'curr_connections', 'curr_items', 'uptime', 'get_hits', 'total_items', 'rusage_system', 'rusage_user', 'bytes_read']\n    for key in keys:\n        self.assert_(stats.has_key(key), (\"key '%s' is not in stats\" % key))\n", "label": 0}
{"function": "\n\ndef onMouseUp(self, sender, x, y):\n    (x, y) = self.adjustMousePos(x, y)\n    if self.mousehandler:\n        self.mousetarget.onBrowserEvent(DOM.eventGetCurrentEvent())\n    else:\n        self.mousetarget.onMouseUp(sender, x, y)\n", "label": 0}
{"function": "\n\ndef query_lookupd(self):\n    self.logger.debug('querying lookupd...')\n    lookupd = next(self.iterlookupds)\n    try:\n        producers = lookupd.lookup(self.topic)['producers']\n        self.logger.debug(('found %d producers' % len(producers)))\n    except Exception as error:\n        msg = 'Failed to lookup %s on %s (%s)'\n        self.logger.warn((msg % (self.topic, lookupd.address, error)))\n        return\n    for producer in producers:\n        conn = Nsqd((producer.get('broadcast_address') or producer['address']), producer['tcp_port'], producer['http_port'], **self.conn_kwargs)\n        self.connect_to_nsqd(conn)\n", "label": 0}
{"function": "\n\ndef get_infra_name(host_id):\n    'Return DATABASE_INFRA_NAME'\n    from physical.models import Host\n    host = Host.objects.filter(id=host_id).select_related('instance').select_related('databaseinfra')\n    try:\n        host = host[0]\n    except IndexError as e:\n        LOG.warn('Host id does not exists: {}. {}'.format(host_id, e))\n        return None\n    return host.instance_set.all()[0].databaseinfra.name\n", "label": 0}
{"function": "\n\ndef buildIndex(self, l):\n    index = self.mIndex()\n    for (start, end, value) in self.l:\n        index.add(start, end)\n    return index\n", "label": 0}
{"function": "\n\ndef visit_Assign(self, node):\n    left = self.visit(node.left)\n    right = self.visit(node.right)\n    ldelay = (self.visit(node.ldelay.value) if (node.ldelay is not None) else None)\n    rdelay = (self.visit(node.rdelay.value) if (node.rdelay is not None) else None)\n    subst = vtypes.Subst(left, right, ldelay=ldelay, rdelay=rdelay)\n    assign = vtypes.Assign(subst)\n    self.add_object(assign)\n    return assign\n", "label": 0}
{"function": "\n\ndef copy(self):\n    res = LoopType()\n    for (key, value) in self.__dict__.iteritems():\n        setattr(res, key, value)\n    return res\n", "label": 0}
{"function": "\n\ndef full_clean(self):\n    '\\n        Cleans all of self.data and populates self._errors.\\n        '\n    self._errors = []\n    if (not self.is_bound):\n        return\n    for i in range(0, self.total_form_count()):\n        form = self.forms[i]\n        self._errors.append(form.errors)\n    try:\n        self.clean()\n    except ValidationError as e:\n        self._non_form_errors = self.error_class(e.messages)\n", "label": 0}
{"function": "\n\n@retry()\ndef node_list(self):\n    return [item.name() for item in self.conn.listAllDomains()]\n", "label": 0}
{"function": "\n\n@classmethod\ndef convert_json(cls, d, convert):\n    new_d = {\n        \n    }\n    for (k, v) in d.iteritems():\n        new_d[convert(k)] = (cls.convert_json(v, convert) if isinstance(v, dict) else v)\n    return new_d\n", "label": 0}
{"function": "\n\ndef add_dependency_links(self, links):\n    if self.process_dependency_links:\n        warnings.warn('Dependency Links processing has been deprecated and will be removed in a future release.', RemovedInPip8Warning)\n        self.dependency_links.extend(links)\n", "label": 0}
{"function": "\n\ndef test_deepcopy_shared_container(self):\n    (a, x) = T.scalars('ax')\n    h = function([In(a, value=0.0)], a)\n    f = function([x, In(a, value=h.container[a], implicit=True)], (x + a))\n    try:\n        memo = {\n            \n        }\n        ac = copy.deepcopy(a)\n        memo.update({\n            id(a): ac,\n        })\n        hc = copy.deepcopy(h, memo=memo)\n        memo.update({\n            id(h): hc,\n        })\n        fc = copy.deepcopy(f, memo=memo)\n    except NotImplementedError as e:\n        if e[0].startswith('DebugMode is not picklable'):\n            return\n        else:\n            raise\n    h[a] = 1\n    hc[ac] = 2\n    self.assertTrue((f[a] == 1))\n    self.assertTrue((fc[ac] == 2))\n", "label": 0}
{"function": "\n\ndef test_rerun_after_depletion_calls_once(self):\n    'Ensure MessageIterator works when used manually.'\n    from furious.batcher import MessageIterator\n    payload = '[\"test\"]'\n    task = Mock(payload=payload, tag='tag')\n    iterator = MessageIterator('tag', 'qn', 1)\n    with patch.object(iterator, 'queue') as queue:\n        queue.lease_tasks_by_tag.return_value = [task]\n        results = [payload for payload in iterator]\n        self.assertEqual(results, [payload])\n        results = [payload for payload in iterator]\n    queue.lease_tasks_by_tag.assert_called_once_with(60, 1, tag='tag', deadline=10)\n", "label": 0}
{"function": "\n\ndef _untagged_response(self, typ, dat, name):\n    if (typ == 'NO'):\n        return (typ, dat)\n    data = self._get_untagged_response(name)\n    if (not data):\n        return (typ, [None])\n    while True:\n        dat = self._get_untagged_response(name)\n        if (not dat):\n            break\n        data += dat\n    if __debug__:\n        self._log(4, ('_untagged_response(%s, ?, %s) => %.80r' % (typ, name, data)))\n    return (typ, data)\n", "label": 0}
{"function": "\n\ndef get_conf_from_module(mod):\n    'return configuration from module with defaults no worry about None type\\n\\n    '\n    conf = ModuleConfig(CONF_SPEC)\n    mod = _get_correct_module(mod)\n    conf.set_module(mod)\n    if hasattr(mod, 'default'):\n        default = mod.default\n        conf = extract_conf_from(default, conf)\n    else:\n        conf = extract_conf_from(mod, conf)\n    return conf\n", "label": 0}
{"function": "\n\ndef onBeforeTabSelected(self, sender, tabIndex):\n    if (self.fTabs.getWidgetCount() == 6):\n        self.fTabs.add(HTML('2nd Test.<br />Tab should be on right'), '2nd Test', name='test2')\n        return True\n    self.fTabs.remove('test2')\n    return (tabIndex != 6)\n", "label": 0}
{"function": "\n\ndef process(args):\n    conduit = phlsys_makeconduit.make_conduit(args.uri, args.user, args.cert, args.act_as_user)\n    if args.diff_id:\n        diff_id = args.diff_id\n    else:\n        d = {\n            'diff': args.raw_diff_file.read(),\n        }\n        diff_id = conduit('differential.createrawdiff', d)['id']\n    fields = {\n        \n    }\n    d = {\n        'id': args.revision_id,\n        'diffid': diff_id,\n        'fields': fields,\n        'message': args.message,\n    }\n    MessageFields = phlcon_differential.MessageFields\n    args.ccs = _get_set_or_none(args.ccs)\n    args.reviewers = _get_set_or_none(args.reviewers)\n    if args.reviewers:\n        fields[MessageFields.reviewer_phids] = args.reviewers\n    if args.ccs:\n        fields[MessageFields.cc_phids] = args.ccs\n    user_phids = phlcon_user.UserPhidCache(conduit)\n    for users in fields.itervalues():\n        user_phids.add_hint_list(users)\n    for key in fields.iterkeys():\n        fields[key] = [user_phids.get_phid(u) for u in fields[key]]\n    result = conduit('differential.updaterevision', d)\n    if args.format_id:\n        print(result['revisionid'])\n    elif args.format_url:\n        print(result['uri'])\n    else:\n        print(\"Updated revision '{rev_id}', you can view it at this URL:\\n  {url}\".format(rev_id=result['revisionid'], url=result['uri']))\n", "label": 0}
{"function": "\n\ndef update(self, action, action_id):\n    updated_action = {\n        \n    }\n    updated_action['freezer_action'] = utils.create_dict(**action)\n    try:\n        if (action['mandatory'] != ''):\n            updated_action['mandatory'] = action['mandatory']\n    except KeyError:\n        pass\n    try:\n        if (action['max_retries'] != ''):\n            updated_action['max_retries'] = action['max_retries']\n    except KeyError:\n        pass\n    try:\n        if (action['max_retries_interval'] != ''):\n            updated_action['max_retries_interval'] = action['max_retries_interval']\n    except KeyError:\n        pass\n    return self.client.actions.update(action_id, updated_action)\n", "label": 0}
{"function": "\n\ndef test_remove_unpickables_http_exception(self):\n    try:\n        urllib2.urlopen('http://localhost/this.does.not.exist')\n        self.fail('exception expected')\n    except urllib2.URLError as e:\n        pass\n    except urllib2.HTTPError as e:\n        pass\n    removed = mapper.remove_unpickables(e)\n    pickled = pickle.dumps(removed)\n    pickle.loads(pickled)\n", "label": 0}
{"function": "\n\n@property\ndef parents(self):\n    if (self._parents is None):\n        self._parents = [self._odb.get_commit(hash) for hash in self._obj.parents]\n    return list(self._parents)\n", "label": 0}
{"function": "\n\ndef process_request(self, request):\n    if (not getattr(settings, 'MOBILE_DOMAIN', False)):\n        return\n    if ((request.COOKIES.get('ismobile', '0') == '1') or (('HTTP_USER_AGENT' in request.META) and (request.COOKIES.get('isbrowser', '0') != '1') and is_mobile(request.META['HTTP_USER_AGENT']))):\n        redirect = settings.MOBILE_DOMAIN\n        if getattr(settings, 'MOBILE_REDIRECT_PRESERVE_URL', False):\n            redirect = (redirect.rstrip('/') + request.path_info)\n        response = HttpResponseRedirect(redirect)\n        max_age = getattr(settings, 'MOBILE_COOKIE_MAX_AGE', DEFAULT_COOKIE_MAX_AGE)\n        expires_time = (time.time() + max_age)\n        expires = cookie_date(expires_time)\n        response.set_cookie('ismobile', '1', domain=settings.SESSION_COOKIE_DOMAIN, max_age=max_age, expires=expires)\n        return response\n", "label": 0}
{"function": "\n\ndef gen_key(self, prefix=None):\n    if (not prefix):\n        prefix = 'python-couchbase-key_'\n    ret = '{0}{1}'.format(prefix, self._key_counter)\n    self._key_counter += 1\n    return ret\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(CreateFlavorInfoAction, self).clean()\n    name = cleaned_data.get('name')\n    flavor_id = cleaned_data.get('flavor_id')\n    try:\n        flavors = api.nova.flavor_list(self.request, None)\n    except Exception:\n        flavors = []\n        msg = _('Unable to get flavor list')\n        exceptions.check_message(['Connection', 'refused'], msg)\n        raise\n    if (flavors is not None):\n        for flavor in flavors:\n            if (flavor.name == name):\n                raise forms.ValidationError((_('The name \"%s\" is already used by another flavor.') % name))\n            if (flavor.id == flavor_id):\n                raise forms.ValidationError((_('The ID \"%s\" is already used by another flavor.') % flavor_id))\n    return cleaned_data\n", "label": 0}
{"function": "\n\ndef test_start_run(self):\n    assert_false(os.path.exists(self.result_file_path))\n    self.run_results.start_run(self.scenario)\n    assert_equal(len(self.scenario.packb()), self._current_size())\n    assert_greater(self._current_size(), 0)\n    with open(self.result_file_path, 'rb') as f:\n        unpacker = msgpack.Unpacker(file_like=f)\n        got_scenario = Scenario.unpackb(unpacker)\n        for attr in ['name', '_scenario_data', 'user_count', 'operation_count', 'run_seconds', 'container_base', 'container_count', 'containers', 'container_concurrency', 'sizes_by_name', 'version', 'bench_size_thresholds']:\n            assert_equal(getattr(got_scenario, attr), getattr(self.scenario, attr))\n", "label": 0}
{"function": "\n\ndef __init__(self, n):\n    assert (n == 1)\n", "label": 0}
{"function": "\n\ndef _str_allocation_pools(allocation_pools):\n    if isinstance(allocation_pools, str):\n        return allocation_pools\n    return '\\n'.join([('%s,%s' % (pool['start'], pool['end'])) for pool in allocation_pools])\n", "label": 0}
{"function": "\n\ndef _put_n_deployments(self, id_prefix, number_of_deployments, skip_creation=None, add_modification=None):\n    for i in range(0, number_of_deployments):\n        deployment_id = '{0}{1}_{2}'.format(id_prefix, str(i), 'deployment')\n        blueprint_id = '{0}{1}_{2}'.format(id_prefix, str(i), 'blueprint')\n        if (not skip_creation):\n            self.put_deployment(deployment_id=deployment_id, blueprint_id=blueprint_id)\n        if add_modification:\n            response = self._put_deployment_modification(deployment_id=deployment_id)\n            self._mark_deployment_modification_finished(modification_id=response['id'])\n", "label": 0}
{"function": "\n\ndef get_extractor(coarse, fine):\n    log.debug(\"getting fine extractor for '{}: {}'\".format(coarse, fine))\n    try:\n        extractor = importlib.import_module(((__package__ + '.') + question_types[fine]))\n    except (ImportError, KeyError):\n        log.warn(\"Extractor for fine type '{}: {}' not implemented\".format(coarse, fine))\n        raise NoExtractorError(coarse, fine)\n    return extractor.Extractor\n", "label": 0}
{"function": "\n\n@classmethod\ndef _parse_repo(cls, repo, name=None):\n    regexp = '(?P<type>deb(-src)?) (?P<uri>[^\\\\s]+) (?P<suite>[^\\\\s]+)( (?P<section>[\\\\w\\\\s]*))?(,(?P<priority>[\\\\d]+))?'\n    match = re.match(regexp, repo)\n    if (not match):\n        raise errors.IncorrectRepository(\"Couldn't parse repository '{0}'\".format(repo))\n    repo_type = match.group('type')\n    repo_suite = match.group('suite')\n    repo_section = match.group('section')\n    repo_uri = match.group('uri')\n    repo_priority = match.group('priority')\n    return {\n        'name': name,\n        'type': repo_type,\n        'uri': repo_uri,\n        'priority': repo_priority,\n        'suite': repo_suite,\n        'section': (repo_section or ''),\n    }\n", "label": 0}
{"function": "\n\ndef get_formsets_with_inlines(self, request, obj=None):\n    if (request.is_add_view and (obj is not None)):\n        raise Exception(\"'obj' passed to get_formsets_with_inlines wasn't None during add_view\")\n    if ((not request.is_add_view) and (obj is None)):\n        raise Exception(\"'obj' passed to get_formsets_with_inlines was None during change_view\")\n    return super(GetFormsetsArgumentCheckingAdmin, self).get_formsets_with_inlines(request, obj)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    super(RequestPageTypeFormSet, self).clean()\n    cnt_rpts = 0\n    cnt_rpts_mp = 0\n    cnt_rpts_dp = 0\n    for form in self.forms:\n        if (not hasattr(form, 'cleaned_data')):\n            continue\n        data = form.cleaned_data\n        if (('DELETE' in data) and data['DELETE']):\n            continue\n        if (not ('page_type' in data)):\n            continue\n        cnt_rpts += 1\n        pt = data['page_type']\n        if (pt == 'MP'):\n            cnt_rpts_mp += 1\n        else:\n            cnt_rpts_dp += 1\n    if (cnt_rpts_mp == 0):\n        raise ValidationError('For every request page type used for scraper elems definition a RequestPageType object with a corresponding page type has to be added!')\n    if (cnt_rpts_mp > 1):\n        raise ValidationError('Only one RequestPageType object for main page requests allowed!')\n", "label": 0}
{"function": "\n\ndef _get_method(self, request, action, content_type, body):\n    'Look up the action-specific method and its extensions.'\n    try:\n        if (not self.controller):\n            meth = getattr(self, action)\n        else:\n            meth = getattr(self.controller, action)\n    except AttributeError:\n        if ((not self.wsgi_actions) or (action not in (_ROUTES_METHODS + ['action']))):\n            raise\n    else:\n        return (meth, self.wsgi_extensions.get(action, []))\n    if (action == 'action'):\n        action_name = action_peek(body)\n    else:\n        action_name = action\n    return (self.wsgi_actions[action_name], self.wsgi_action_extensions.get(action_name, []))\n", "label": 0}
{"function": "\n\ndef test_id(self):\n    'Each test annotation should be created with a unique ID.'\n    annotation_1 = factories.Annotation()\n    annotation_2 = factories.Annotation()\n    assert annotation_1.get('id')\n    assert annotation_2.get('id')\n    assert (annotation_1['id'] != annotation_2['id'])\n", "label": 0}
{"function": "\n\n@property\ndef primary_key_names(self):\n    'Primary keys of the table\\n        '\n    return [c.name for c in self.columns() if c.primary]\n", "label": 0}
{"function": "\n\ndef __init__(self, activation, dims=None, **kwargs):\n    super(SpeechBottom, self).__init__(**kwargs)\n    self.num_features = self.input_dims['recordings']\n    if (activation is None):\n        activation = Tanh()\n    if dims:\n        child = MLP(([activation] * len(dims)), ([self.num_features] + dims), name='bottom')\n        self.output_dim = child.output_dim\n    else:\n        child = Identity(name='bottom')\n        self.output_dim = self.num_features\n    self.children.append(child)\n    self.mask = tensor.matrix('recordings_mask')\n    self.batch_inputs = {\n        'recordings': tensor.tensor3('recordings'),\n    }\n    self.single_inputs = {\n        'recordings': tensor.matrix('recordings'),\n    }\n", "label": 0}
{"function": "\n\n@defer.deferredGenerator\ndef allapps_action(self, argstr):\n    \"Usage allapps: <method> [args]\\n\\n  dispatch the same command to all application managers.\\n\\n    <method>\\tmethod to invoke on all appmanagers.\\n    [args]\\toptional arguments to pass along.\\n\\n  examples:\\n\\n    ''            #shows help documentation for all applications\\n    'status'      #invoke status assumes there is only one instance\\n    'status all'  #invoke status on all application instances\\n    'status 0'    #invoke status on application instance label '0'\\n\\n  full cli usage:\\n\\n    $ droneblaster allapps\\n    $ droneblaster allapps status\\n    $ droneblaster allapps status all\\n    $ droneblaster allapps status 0\\n\"\n    result = {\n        \n    }\n    descriptions = []\n    code = 0\n    for obj in AppManager.objects:\n        try:\n            action = obj.action\n            if (not action):\n                continue\n            d = action(argstr)\n            wfd = defer.waitForDeferred(d)\n            (yield wfd)\n            foo = wfd.getResult()\n            descriptions.append(foo.get('description', 'None'))\n            code += int(foo.get('code', 0))\n        except:\n            pass\n    result['description'] = '\\n'.join(descriptions)\n    if (not result['description']):\n        result['description'] = 'None'\n    result['code'] = code\n    (yield result)\n", "label": 0}
{"function": "\n\n@test.attr(type='benchmark')\ndef test_002_fill_volume(self):\n    'Fill volume with data'\n    if (self.ctx.ssh is None):\n        raise self.skipException('Booting failed')\n    if (not self.ctx.volume_ready):\n        raise self.skipException('Volume preparation failed')\n    self._start_test()\n    self.ctx.ssh.exec_command('sudo mkdir -m 777 /vol/data')\n    file_lines = (102 * int(self.volume_size))\n    for i in xrange(int(self.volume_fill)):\n        self.ctx.ssh.exec_command((((\"cat /dev/urandom | tr -d -c 'a-zA-Z0-9' | fold -w 1020 | head -n \" + str(file_lines)) + ' > /vol/data/file') + str(i)))\n    self._end_test('Volume filling')\n    self.ctx.volume_filled = True\n    self._check_test()\n", "label": 0}
{"function": "\n\ndef _url_coerce_fn(r):\n    '\\n    :rtype: str\\n    '\n    p = urllib.parse.urlparse(r)\n    if (not p.scheme):\n        raise InvalidInput('Specify an URL scheme (e.g. http://)')\n    if (not p.netloc):\n        raise InvalidInput('Specify a domain (e.g. example.com)')\n    if (p.path and (p.path != '/')):\n        raise InvalidInput('Do not specify a path')\n    if (p.params or p.query or p.fragment):\n        raise InvalidInput('Do not leave trailing elements')\n    if (not p.path):\n        r += '/'\n    r = r.lower()\n    return r\n", "label": 0}
{"function": "\n\ndef _reindent_stats(tokens):\n    \"Return list of (lineno, indentlevel) pairs.\\n\\n    One for each stmt and comment line. indentlevel is -1 for comment lines, as\\n    a signal that tokenize doesn't know what to do about them; indeed, they're\\n    our headache!\\n\\n    \"\n    find_stmt = 1\n    level = 0\n    stats = []\n    for t in tokens:\n        token_type = t[0]\n        sline = t[2][0]\n        line = t[4]\n        if (token_type == tokenize.NEWLINE):\n            find_stmt = 1\n        elif (token_type == tokenize.INDENT):\n            find_stmt = 1\n            level += 1\n        elif (token_type == tokenize.DEDENT):\n            find_stmt = 1\n            level -= 1\n        elif (token_type == tokenize.COMMENT):\n            if find_stmt:\n                stats.append((sline, (- 1)))\n        elif (token_type == tokenize.NL):\n            pass\n        elif find_stmt:\n            find_stmt = 0\n            if line:\n                stats.append((sline, level))\n    return stats\n", "label": 1}
{"function": "\n\ndef __init__(self, mu, var, **kwargs):\n    (self.mu, self.var) = (None, None)\n    if (not isinstance(mu, Layer)):\n        (self.mu, mu) = (mu, None)\n    if (not isinstance(var, Layer)):\n        (self.var, var) = (var, None)\n    input_lst = [i for i in [mu, var] if (not (i is None))]\n    super(GaussianMarginalLogDensityLayer, self).__init__(input_lst, **kwargs)\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.message = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.log_context = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.handle = QueryHandle()\n                self.handle.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.errorCode = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.STRING):\n                self.SQLState = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef confidence(self):\n    \"\\n        Returns a tuple (chi squared, confident) of the experiment. Confident\\n        is simply a boolean specifying whether we're > 95%% sure that the\\n        results are statistically significant.\\n        \"\n    choices = self.choices\n    if (len(choices) >= 2):\n        csq = chi_squared(*choices)\n        confident = (is_confident(csq, len(choices)) if (len(choices) <= 10) else None)\n    else:\n        csq = None\n        confident = False\n    return (csq, confident)\n", "label": 0}
{"function": "\n\ndef compile_function(code, arg_names, local_dict, global_dict, module_dir, compiler='', verbose=1, support_code=None, headers=[], customize=None, type_converters=None, auto_downcast=1, **kw):\n    code = ((ndarray_api_version + '\\n') + code)\n    module_path = function_catalog.unique_module_name(code, module_dir)\n    (storage_dir, module_name) = os.path.split(module_path)\n    mod = inline_ext_module(module_name, compiler)\n    ext_func = inline_ext_function('compiled_func', code, arg_names, local_dict, global_dict, auto_downcast, type_converters=type_converters)\n    mod.add_function(ext_func)\n    if customize:\n        mod.customize = customize\n    if support_code:\n        mod.customize.add_support_code(support_code)\n    for header in headers:\n        mod.customize.add_header(header)\n    if (verbose > 0):\n        print('<weave: compiling>')\n    mod.compile(location=storage_dir, compiler=compiler, verbose=verbose, **kw)\n    try:\n        sys.path.insert(0, storage_dir)\n        exec(('import ' + module_name))\n        func = eval((module_name + '.compiled_func'))\n    finally:\n        del sys.path[0]\n    return func\n", "label": 0}
{"function": "\n\n@testing.requires.threading_with_mock\n@testing.requires.timing_intensive\ndef test_timeout_race(self):\n    dbapi = MockDBAPI()\n    p = pool.QueuePool(creator=(lambda : dbapi.connect(delay=0.05)), pool_size=2, max_overflow=1, use_threadlocal=False, timeout=3)\n    timeouts = []\n\n    def checkout():\n        for x in range(1):\n            now = time.time()\n            try:\n                c1 = p.connect()\n            except tsa.exc.TimeoutError:\n                timeouts.append((time.time() - now))\n                continue\n            time.sleep(4)\n            c1.close()\n    threads = []\n    for i in range(10):\n        th = threading.Thread(target=checkout)\n        th.start()\n        threads.append(th)\n    for th in threads:\n        th.join(join_timeout)\n    assert (len(timeouts) > 0)\n    for t in timeouts:\n        assert (t >= 3), ('Not all timeouts were >= 3 seconds %r' % timeouts)\n        assert (t < 14), ('Not all timeouts were < 14 seconds %r' % timeouts)\n", "label": 0}
{"function": "\n\ndef _load_allowed_remote_addresses(self, app):\n    key = 'PSDASH_ALLOWED_REMOTE_ADDRESSES'\n    addrs = app.config.get(key)\n    if (not addrs):\n        return\n    if isinstance(addrs, (str, unicode)):\n        app.config[key] = [a.strip() for a in addrs.split(',')]\n", "label": 0}
{"function": "\n\ndef validate_config(self):\n    self.config.set('boss', 'data_dir', fs.abspath(self.config.get('boss', 'data_dir')))\n    if (not os.path.exists(self.config.get('boss', 'data_dir'))):\n        os.makedirs(self.config.get('boss', 'data_dir'))\n    pth = os.path.join(self.config.get('boss', 'data_dir'), 'cache')\n    if (not os.path.exists(fs.abspath(pth))):\n        os.makedirs(fs.abspath(pth))\n    self.config.set('boss', 'cache_dir', pth)\n    pth = os.path.join(self.config.get('boss', 'data_dir'), 'boss.db')\n    self.config.set('boss', 'db_path', pth)\n", "label": 0}
{"function": "\n\ndef test_length(session):\n    set_ = session.set(key('test_sortedset_length'), S('abc'), SortedSet)\n    assert (len(set_) == 3)\n    setx = session.set(key('test_sortedsetx_length'), S([1, 2, 3]), IntSet)\n    assert (len(setx) == 3)\n", "label": 0}
{"function": "\n\ndef __init__(self, notifier=None):\n    if (self.__class__.__instance is None):\n        self.__class__.__instance = self\n        if (notifier is None):\n            self._notifier = _AsyncNotifier()\n        else:\n            self._notifier = notifier\n        self._location = None\n        self._name = None\n        self._coros = {\n            \n        }\n        self._scheduled = set()\n        self._suspended = set()\n        self._timeouts = []\n        self._lock = threading.RLock()\n        self._quit = False\n        self._complete = threading.Event()\n        self._daemons = 0\n        self._polling = False\n        self._channels = {\n            \n        }\n        self._atexit = []\n        Coro._asyncoro = Channel._asyncoro = self\n        self._scheduler = threading.Thread(target=self._schedule)\n        self._scheduler.daemon = True\n        self._scheduler.start()\n        atexit.register(self.finish)\n        logger.info('version %s with %s I/O notifier', __version__, self._notifier._poller_name)\n", "label": 0}
{"function": "\n\ndef from_jsobj(jsobj, cls=None):\n    'Create an instance of the given class from a JSON object.\\n\\n    Arguments:\\n      cls: a class that serves as a \"type hint.\"\\n    '\n    if isinstance(jsobj, LIST_TYPES):\n        return [from_jsobj(o, cls=cls) for o in jsobj]\n    if (cls is not None):\n        return cls.from_jsobj(jsobj)\n    if (jsobj is None):\n        return JS_NULL\n    return jsobj\n", "label": 0}
{"function": "\n\ndef contains_subsequence(seq, subseq):\n    for i in range((len(seq) - len(subseq))):\n        if (seq[i:(i + len(subseq))] == subseq):\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef tables(self, db=None):\n    '\\n        Enumerates all tables fro a given database. If not specified, use the\\n        current database.\\n        '\n    if self.has_cap(TABLES_ENUM):\n        if (db is None):\n            if (self.current_db is None):\n                self.database()\n            db = self.current_db\n        n = self.get_nb_tables(db)\n        for i in range(n):\n            (yield TableWrapper(self, self.get_table_name(i, db), db))\n    else:\n        raise Unavailable()\n", "label": 0}
{"function": "\n\ndef _is_us_state(abbr, result):\n    for sep in ('/', '-'):\n        if (result.source_base == 'us{sep}{abbr}'.format(**locals())):\n            return True\n        if result.source_base.startswith('us{sep}{abbr}.'.format(**locals())):\n            return True\n        if result.source_base.startswith('us{sep}{abbr}{sep}'.format(**locals())):\n            return True\n    return False\n", "label": 0}
{"function": "\n\n@util.positional(2)\ndef error(status_code, status_message=None, content_type='text/plain; charset=utf-8', headers=None, content=None):\n    'Create WSGI application that statically serves an error page.\\n\\n  Creates a static error page specifically for non-200 HTTP responses.\\n\\n  Browsers such as Internet Explorer will display their own error pages for\\n  error content responses smaller than 512 bytes.  For this reason all responses\\n  are right-padded up to 512 bytes.\\n\\n  Error pages that are not provided will content will contain the standard HTTP\\n  status message as their content.\\n\\n  Args:\\n    status_code: Integer status code of error.\\n    status_message: Status message.\\n\\n  Returns:\\n    Static WSGI application that sends static error response.\\n  '\n    if (status_message is None):\n        status_message = httplib.responses.get(status_code, 'Unknown Error')\n    if (content is None):\n        content = status_message\n    content = util.pad_string(content)\n    return static_page(content, status=(status_code, status_message), content_type=content_type, headers=headers)\n", "label": 0}
{"function": "\n\ndef _validate_python(self, field_dict, state):\n    try:\n        ref = field_dict[self.field_names[0]]\n    except TypeError:\n        raise Invalid(self.message('notDict', state), field_dict, state)\n    except KeyError:\n        ref = ''\n    errors = {\n        \n    }\n    for name in self.field_names[1:]:\n        if (field_dict.get(name, '') != ref):\n            if self.show_match:\n                errors[name] = self.message('invalid', state, match=ref)\n            else:\n                errors[name] = self.message('invalidNoMatch', state)\n    if errors:\n        error_list = sorted(six.iteritems(errors))\n        error_message = '<br>\\n'.join((('%s: %s' % (name, value)) for (name, value) in error_list))\n        raise Invalid(error_message, field_dict, state, error_dict=errors)\n", "label": 0}
{"function": "\n\ndef get_list(self, *args, **kwargs):\n    (count, data) = super(TweetView, self).get_list(*args, **kwargs)\n    query = {\n        '_id': {\n            '$in': [x['user_id'] for x in data],\n        },\n    }\n    users = db.user.find(query, fields=('name',))\n    users_map = dict(((x['_id'], x['name']) for x in users))\n    for item in data:\n        item['user_name'] = users_map.get(item['user_id'])\n    return (count, data)\n", "label": 0}
{"function": "\n\ndef find_payload_class(payload_type):\n    'Iterate through inherited classes to find a matching class name'\n    subclasses = set()\n    work = [Payload]\n    while work:\n        parent_subclass = work.pop()\n        for child_subclass in parent_subclass.__subclasses__():\n            if (child_subclass not in subclasses):\n                if (hasattr(child_subclass, 'payload_type') and (child_subclass.payload_type == payload_type)):\n                    return child_subclass\n                subclasses.add(child_subclass)\n                work.append(child_subclass)\n    return None\n", "label": 0}
{"function": "\n\ndef parse_inline(text):\n    '\\n    Takes a string of text from a text inline and returns a 3 tuple of\\n    (name, value, **kwargs).\\n    '\n    m = INLINE_SPLITTER.match(text)\n    if (not m):\n        raise InlineUnparsableError\n    args = m.group('args')\n    name = m.group('name')\n    value = ''\n    kwtxt = ''\n    kwargs = {\n        \n    }\n    if args:\n        kwtxt = INLINE_KWARG_PARSER.search(args).group('kwargs')\n        value = re.sub(('%s\\\\Z' % kwtxt), '', args)\n        value = value.strip()\n    if m.group('variant'):\n        kwargs['variant'] = m.group('variant')\n    if kwtxt:\n        for kws in kwtxt.split():\n            (k, v) = kws.split('=')\n            kwargs[str(k)] = v\n    return (name, value, kwargs)\n", "label": 0}
{"function": "\n\ndef _Rotate(self, image, transform):\n    'Use PIL to rotate the given image with the given transform.\\n\\n    Args:\\n      image: PIL.Image.Image object to rotate.\\n      transform: images_service_pb.Transform to use when rotating.\\n\\n    Returns:\\n      PIL.Image.Image with transforms performed on it.\\n\\n    Raises:\\n      BadRequestError if the rotate data given is bad.\\n    '\n    degrees = transform.rotate()\n    if ((degrees < 0) or ((degrees % 90) != 0)):\n        raise apiproxy_errors.ApplicationError(images_service_pb.ImagesServiceError.BAD_TRANSFORM_DATA)\n    degrees %= 360\n    degrees = (360 - degrees)\n    return image.rotate(degrees)\n", "label": 0}
{"function": "\n\ndef iter_keys(self, filename):\n    with open(filename, 'rb') as f:\n        header = f.read(8)\n        self._verify_header(header)\n        current_offset = 8\n        file_size_bytes = os.path.getsize(filename)\n        while True:\n            current_contents = f.read(8)\n            current_offset += 8\n            if (len(current_contents) < 8):\n                if (len(current_contents) > 0):\n                    raise DBMLoadError('Error loading db: partial header read')\n                else:\n                    return\n            (key_size, val_size) = struct.unpack('!ii', current_contents)\n            key = f.read(key_size)\n            if (len(key) != key_size):\n                raise DBMLoadError(('Error loading db: key size does not match (expected %s bytes, got %s instead.' % (key_size, len(key))))\n            value_offset = (current_offset + key_size)\n            if ((value_offset + val_size) > file_size_bytes):\n                return\n            (yield (key, value_offset, val_size))\n            if (val_size == _DELETED):\n                val_size = 0\n            skip_ahead = ((key_size + val_size) + 4)\n            current_offset += skip_ahead\n            if (current_offset > file_size_bytes):\n                raise DBMLoadError('Error loading db: reading past the end of the file (file possibly truncated)')\n            f.seek(current_offset)\n", "label": 0}
{"function": "\n\ndef test_iter_smart_pk_range(self):\n    seen = []\n    for (start_pk, end_pk) in Author.objects.iter_smart_pk_ranges():\n        seen.extend(Author.objects.filter(id__gte=start_pk, id__lt=end_pk).values_list('id', flat=True))\n    all_ids = list(Author.objects.order_by('id').values_list('id', flat=True))\n    assert (seen == all_ids)\n", "label": 0}
{"function": "\n\ndef get_command(self, ctx, name):\n    \"\\n        Get a specific command by looking up the module.\\n\\n        :param ctx: Click context\\n        :param name: Command name\\n        :return: Module's cli function\\n        \"\n    try:\n        if (sys.version_info[0] == 2):\n            name = name.encode('ascii', 'replace')\n        mod = __import__(('cli.commands.cmd_' + name), None, None, ['cli'])\n    except ImportError as e:\n        logging.error('Error importing module {0}:\\n{0}'.format(name, e))\n        exit(1)\n    return mod.cli\n", "label": 0}
{"function": "\n\ndef ensure_role(self, role, dry_run=False):\n    '\\n        Adds the role if it does not already exist, otherwise skips it.\\n        '\n    existing_roles = Role.objects.filter(slug=role.slug)\n    if existing_roles:\n        logger.info('Role already exists: %s', role.name)\n        return existing_roles[0]\n    elif dry_run:\n        logger.info('[DRY RUN] Creating role: %s', role.name)\n    else:\n        if self.verbose:\n            logger.info('Creating role: %s', role.name)\n        role.save()\n", "label": 0}
{"function": "\n\ndef _initPopulation(self, seeds):\n    if (self.parentChildAverage < 1):\n        for s in seeds:\n            s.parent = None\n    self.pop = self._extendPopulation(seeds, self.populationSize)\n", "label": 0}
{"function": "\n\ndef _get_task_with_policy(queue_name, task_id, owner):\n    'Fetches the specified task and enforces ownership policy.\\n\\n    Args:\\n        queue_name: Name of the queue the work item is on.\\n        task_id: ID of the task that is finished.\\n        owner: Who or what has the current lease on the task.\\n\\n    Returns:\\n        The valid WorkQueue task that is currently owned.\\n\\n    Raises:\\n        TaskDoesNotExistError if the task does not exist.\\n        LeaseExpiredError if the lease is no longer active.\\n        NotOwnerError if the specified owner no longer owns the task.\\n    '\n    now = datetime.datetime.utcnow()\n    task = WorkQueue.query.filter_by(queue_name=queue_name, task_id=task_id).with_lockmode('update').first()\n    if (not task):\n        raise TaskDoesNotExistError(('task_id=%r' % task_id))\n    lease_delta = (now - task.eta)\n    if (lease_delta > datetime.timedelta(0)):\n        db.session.rollback()\n        raise LeaseExpiredError(('queue=%r, task_id=%r expired %s' % (task.queue_name, task_id, lease_delta)))\n    if (task.last_owner != owner):\n        db.session.rollback()\n        raise NotOwnerError(('queue=%r, task_id=%r, owner=%r' % (task.queue_name, task_id, task.last_owner)))\n    return task\n", "label": 0}
{"function": "\n\ndef showSublimeContext(self, filename, line):\n    debug(((('showSublimeContext: ' + str(filename)) + ' : ') + str(line)))\n    console_output((((('@@@ Stopped at ' + str(filename.replace((self.projectDir + '/'), ''))) + ':') + str(line)) + ' @@@'))\n    window = sublime.active_window()\n    if window:\n        window.focus_group(0)\n        view = window.active_view()\n        if ((view is not None) and (view.size() >= 0)):\n            filename = os.path.join(self.projectDir, filename)\n            if (view.file_name() != filename):\n                self.activateViewWithFile(filename, line)\n            window.run_command('goto_line', {\n                'line': line,\n            })\n            view = window.active_view()\n            mark = [view.line(view.text_point((line - 1), 0))]\n            view.erase_regions('current_line')\n            view.add_regions('current_line', mark, 'current_line', 'dot', sublime.DRAW_OUTLINED)\n        else:\n            debug('No current view')\n", "label": 0}
{"function": "\n\ndef test_diff_nans(self):\n    'Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/204'\n    arr = np.empty((10, 10), dtype=np.float64)\n    arr[:5] = 1.0\n    arr[5:] = np.nan\n    arr2 = arr.copy()\n    table = np.rec.array([(1.0, 2.0), (3.0, np.nan), (np.nan, np.nan)], names=['cola', 'colb']).view(fits.FITS_rec)\n    table2 = table.copy()\n    assert ImageDataDiff(arr, arr2).identical\n    assert TableDataDiff(table, table2).identical\n    arr2[0][0] = 2.0\n    arr2[5][0] = 2.0\n    table2[0][0] = 2.0\n    table2[1][1] = 2.0\n    diff = ImageDataDiff(arr, arr2)\n    assert (not diff.identical)\n    assert (diff.diff_pixels[0] == ((0, 0), (1.0, 2.0)))\n    assert (diff.diff_pixels[1][0] == (5, 0))\n    assert np.isnan(diff.diff_pixels[1][1][0])\n    assert (diff.diff_pixels[1][1][1] == 2.0)\n    diff = TableDataDiff(table, table2)\n    assert (not diff.identical)\n    assert (diff.diff_values[0] == (('cola', 0), (1.0, 2.0)))\n    assert (diff.diff_values[1][0] == ('colb', 1))\n    assert np.isnan(diff.diff_values[1][1][0])\n    assert (diff.diff_values[1][1][1] == 2.0)\n", "label": 1}
{"function": "\n\ndef do_command(self, verb, args):\n    conn = http_client.HTTPConnection(self.host, self.port, timeout=self.http_timeout)\n    try:\n        body = ('cmd=' + urllib_parse.quote_plus(unicode(verb).encode('utf-8')))\n        for i in range(len(args)):\n            body += ((('&' + unicode((i + 1))) + '=') + urllib_parse.quote_plus(unicode(args[i]).encode('utf-8')))\n        if (None != self.sessionId):\n            body += ('&sessionId=' + unicode(self.sessionId))\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8',\n        }\n        conn.request('POST', '/selenium-server/driver/', body, headers)\n        response = conn.getresponse()\n        data = unicode(response.read(), 'UTF-8')\n        if (not data.startswith('OK')):\n            raise Exception(data)\n        return data\n    finally:\n        conn.close()\n", "label": 0}
{"function": "\n\ndef fdiff(self, argindex=1):\n    (z, m) = self.args\n    fm = sqrt((1 - (m * (sin(z) ** 2))))\n    if (argindex == 1):\n        return (1 / fm)\n    elif (argindex == 2):\n        return (((elliptic_e(z, m) / ((2 * m) * (1 - m))) - (elliptic_f(z, m) / (2 * m))) - (sin((2 * z)) / ((4 * (1 - m)) * fm)))\n    raise ArgumentIndexError(self, argindex)\n", "label": 0}
{"function": "\n\ndef bayesdb_generator_column_stattype(bdb, generator_id, colno):\n    'Return the statistical type of the column `colno` in `generator_id`.'\n    sql = '\\n        SELECT stattype FROM bayesdb_generator_column\\n            WHERE generator_id = ? AND colno = ?\\n    '\n    cursor = bdb.sql_execute(sql, (generator_id, colno))\n    try:\n        row = cursor.next()\n    except StopIteration:\n        generator = bayesdb_generator_name(bdb, generator_id)\n        sql = '\\n            SELECT COUNT(*)\\n                FROM bayesdb_generator AS g, bayesdb_column AS c\\n                WHERE g.id = :generator_id\\n                    AND g.tabname = c.tabname\\n                    AND c.colno = :colno\\n        '\n        cursor = bdb.sql_execute(sql, {\n            'generator_id': generator_id,\n            'colno': colno,\n        })\n        if (cursor_value(cursor) == 0):\n            raise ValueError(('No such column in generator %s: %d' % (generator, colno)))\n        else:\n            raise ValueError(('Column not modelled in generator %s: %d' % (generator, colno)))\n    else:\n        assert (len(row) == 1)\n        return row[0]\n", "label": 0}
{"function": "\n\ndef _mount_shares_to_instance(self, instance):\n    for share in self.shares:\n        share.handler.allow_access_to_instance(instance, share.share_config)\n    with instance.remote() as remote:\n        share_types = set((type(share.handler) for share in self.shares))\n        for share_type in share_types:\n            share_type.setup_instance(remote)\n        for share in self.shares:\n            share.handler.mount_to_instance(remote, share.share_config)\n", "label": 0}
{"function": "\n\ndef get_default_machine(self):\n    ' Reads the default machine from the package configuration\\n        \\n        '\n    server = username = port = password = ''\n    if configuration.check('server'):\n        server = configuration.server\n    if (not server):\n        return None\n    if configuration.check('username'):\n        username = configuration.username\n    if (not username):\n        username = current_user()\n    if (configuration.check('port') is not None):\n        port = configuration.port\n    if configuration.check('password'):\n        password = configuration.password\n    self.annotate({\n        'RemoteQ-server': server,\n        'RemoteQ-username': username,\n        'RemoteQ-port': port,\n    })\n    return (server, port, username, password)\n", "label": 0}
{"function": "\n\ndef get_all_hosts(self):\n    '\\n            Get list of all hosts in cluster\\n            Args:\\n                None\\n            Return:\\n                list of hostnames\\n            Raise:\\n                None\\n       '\n    zook = self.zk_client\n    broker_id_path = self.zk_paths[BROKER_IDS]\n    if zook.exists(broker_id_path):\n        broker_ids = zook.get_children(broker_id_path)\n        brokers = []\n        for broker_id in broker_ids:\n            brokers.append(self.get_host(broker_id))\n        return brokers\n", "label": 0}
{"function": "\n\ndef parse(cls, signed_request, application_secret_key):\n    'Parse a signed request, returning a dictionary describing its payload.'\n\n    def decode(encoded):\n        padding = ('=' * (len(encoded) % 4))\n        return base64.urlsafe_b64decode((encoded + padding))\n    try:\n        (encoded_signature, encoded_payload) = (str(string) for string in signed_request.split('.', 2))\n        signature = decode(encoded_signature)\n        signed_request_data = json.loads(decode(encoded_payload).decode('utf-8'))\n    except (TypeError, ValueError):\n        raise SignedRequestError('Signed request had a corrupt payload')\n    if (signed_request_data.get('algorithm', '').upper() != 'HMAC-SHA256'):\n        raise SignedRequestError('Signed request is using an unknown algorithm')\n    expected_signature = hmac.new(application_secret_key.encode('utf-8'), msg=encoded_payload.encode('utf-8'), digestmod=hashlib.sha256).digest()\n    if (signature != expected_signature):\n        raise SignedRequestError('Signed request signature mismatch')\n    return signed_request_data\n", "label": 0}
{"function": "\n\ndef assemble(self):\n    assembled = {\n        self.type: self.body,\n    }\n    if self.aggregations:\n        assembled['aggs'] = {\n            \n        }\n        for agg in self.aggregations:\n            assembled['aggs'][agg.name] = agg.assemble()\n    return assembled\n", "label": 0}
{"function": "\n\ndef get(self, filepath, version=None, mode='r'):\n    'Returns a bytestring with the file content, but no metadata.'\n    file_stream = self.open(filepath, version=version, mode=mode)\n    if (file_stream is None):\n        raise IOError(('File %s (version %s) not found.' % (filepath, (version if version else 'latest'))))\n    return file_stream.read()\n", "label": 0}
{"function": "\n\ndef upload_form(self):\n    '\\n            Instantiate file upload form and return it.\\n\\n            Override to implement custom behavior.\\n        '\n    upload_form_class = self.get_upload_form()\n    if request.form:\n        formdata = request.form.copy()\n        formdata.update(request.files)\n        return upload_form_class(formdata, admin=self)\n    elif request.files:\n        return upload_form_class(request.files, admin=self)\n    else:\n        return upload_form_class(admin=self)\n", "label": 0}
{"function": "\n\ndef _get_url(self, url):\n    if (self.access == 'public'):\n        url = url.replace('https://', 'http://')\n        req = urllib.request.Request(url)\n        try:\n            return urllib.request.urlopen(req).read()\n        except urllib.error.HTTPError:\n            raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n    else:\n        raise NotImplementedError('Currently, DocumentCloud only allows you to access this resource on public documents.')\n", "label": 0}
{"function": "\n\n@skipUnless(cache_file_exists('ia', '20101102__ia__general__poweshiek__precinct.xls'), CACHED_FILE_MISSING_MSG)\ndef test_results(self):\n    filename = '20101102__ia__general__poweshiek__precinct.xls'\n    mapping = self._get_mapping(filename)\n    self._prep_loader_attrs(mapping)\n    results = self.loader._results(mapping)\n    montezuma_abs_results = [r for r in results if ((r.jurisdiction == 'Montezuma') and (r.votes_type == 'absentee'))]\n    self.assertEqual(len(montezuma_abs_results), 34)\n    result = montezuma_abs_results[0]\n    self.assertEqual(result.office, 'United States Senator')\n    self.assertEqual(result.district, None)\n    self.assertEqual(result.full_name, 'Roxanne Conlin')\n    self.assertEqual(result.party, 'DEM')\n    self.assertEqual(result.write_in, None)\n    self.assertEqual(result.votes, 59)\n    result = montezuma_abs_results[(- 1)]\n    self.assertEqual(result.office, 'State Rep')\n    self.assertEqual(result.district, '75')\n    self.assertEqual(result.full_name, 'Write-In')\n    self.assertEqual(result.party, None)\n    self.assertEqual(result.write_in, 'Write-In')\n    self.assertEqual(result.votes, 0)\n", "label": 0}
{"function": "\n\ndef write(self, bytes):\n    '\\n        Write C{bytes} to the underlying consumer unless\\n        C{_noMoreWritesExpected} has been called or there are/have been too\\n        many bytes.\\n        '\n    if (self._finished is None):\n        self._producer.stopProducing()\n        raise ExcessWrite()\n    if (len(bytes) <= self._length):\n        self._length -= len(bytes)\n        self._consumer.write(bytes)\n    else:\n        _callAppFunction(self._producer.stopProducing)\n        self._finished.errback(WrongBodyLength('too many bytes written'))\n        self._allowNoMoreWrites()\n", "label": 0}
{"function": "\n\ndef LVMPathSpecGetVolumeIndex(path_spec):\n    'Retrieves the volume index from the path specification.\\n\\n  Args:\\n    path_spec: the path specification (instance of PathSpec).\\n  '\n    volume_index = getattr(path_spec, 'volume_index', None)\n    if (volume_index is None):\n        location = getattr(path_spec, 'location', None)\n        if ((location is None) or (not location.startswith('/lvm'))):\n            return\n        volume_index = None\n        try:\n            volume_index = (int(location[4:], 10) - 1)\n        except ValueError:\n            pass\n        if ((volume_index is None) or (volume_index < 0)):\n            return\n    return volume_index\n", "label": 0}
{"function": "\n\ndef update_fpointer(self, nid, mode=ADD):\n    'set _fpointer recursively'\n    if (nid is None):\n        return\n    if (mode is self.ADD):\n        self._fpointer.append(nid)\n    elif (mode is self.DELETE):\n        if (nid in self._fpointer):\n            self._fpointer.remove(nid)\n    elif (mode is self.INSERT):\n        print('WARNNING: INSERT is deprecated to ADD mode')\n        self.update_fpointer(nid)\n", "label": 0}
{"function": "\n\ndef __init__(self):\n    super().__init__()\n    db_host = os.environ.get('MONGO_HOST')\n    db_host = (db_host if db_host else 'localhost')\n    db_port = int(os.environ.get('MONGO_PORT'))\n    db_port = (db_port if db_port else 27017)\n    db_name = os.environ.get('MONGO_DB')\n    db_name = (db_name if db_name else 'default')\n    db_bucket = os.environ.get('MONGO_BUCKET')\n    db_bucket = (db_bucket if db_bucket else 'rxnorm')\n    import pymongo\n    conn = pymongo.MongoClient(host=db_host, port=db_port)\n    db = conn[db_name]\n    db_user = os.environ.get('MONGO_USER')\n    db_pass = os.environ.get('MONGO_PASS')\n    if (db_user and db_pass):\n        db.authenticate(db_user, db_pass)\n    self.mng = db[db_bucket]\n    self.mng.ensure_index('ndc')\n    self.mng.ensure_index('label', text=pymongo.TEXT)\n", "label": 0}
{"function": "\n\n@responses.activate\ndef test_bitly_total_clicks_bad_response():\n    body = '20'\n    params = urlencode(dict(link=shorten, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/link/clicks', params)\n    responses.add(responses.GET, url, body=body, status=400, match_querystring=True)\n    body = shorten\n    params = urlencode(dict(uri=expanded, access_token=token, format='txt'))\n    url = '{0}{1}?{2}'.format(s.api_url, 'v3/shorten', params)\n    responses.add(responses.GET, url, body=body, match_querystring=True)\n    s.short(expanded)\n    assert (s.total_clicks() == 0)\n    assert (s.total_clicks(shorten) == 0)\n", "label": 0}
{"function": "\n\ndef test_RosdepDatabase():\n    from rosdep2.model import RosdepDatabase\n    db = RosdepDatabase()\n    assert (not db.is_loaded('foo'))\n    data = {\n        'a': 1,\n    }\n    db.set_view_data('foo', data, [], 'origin1')\n    assert db.is_loaded('foo')\n    entry = db.get_view_data('foo')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin1')\n    assert (entry.view_dependencies == [])\n    data['a'] = 2\n    assert (entry.rosdep_data != data)\n    data = {\n        'b': 2,\n    }\n    db.set_view_data('bar', data, ['foo'], 'origin2')\n    assert db.is_loaded('bar')\n    entry = db.get_view_data('bar')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin2')\n    assert (entry.view_dependencies == ['foo'])\n    data = {\n        'b': 3,\n    }\n    assert db.is_loaded('bar')\n    db.set_view_data('bar', data, ['baz', 'blah'], 'origin3')\n    assert db.is_loaded('bar')\n    entry = db.get_view_data('bar')\n    assert (entry.rosdep_data == data)\n    assert (entry.origin == 'origin3')\n    assert (set(entry.view_dependencies) == set(['baz', 'blah']))\n", "label": 1}
{"function": "\n\ndef prune_overridden(ansi_string):\n    'Remove color codes that are rendered ineffective by subsequent codes in one escape sequence then sort codes.\\n\\n    :param str ansi_string: Incoming ansi_string with ANSI color codes.\\n\\n    :return: Color string with pruned color sequences.\\n    :rtype: str\\n    '\n    multi_seqs = set((p for p in RE_ANSI.findall(ansi_string) if (';' in p[1])))\n    for (escape, codes) in multi_seqs:\n        r_codes = list(reversed(codes.split(';')))\n        try:\n            r_codes = r_codes[:(r_codes.index('0') + 1)]\n        except ValueError:\n            pass\n        for group in CODE_GROUPS:\n            for pos in reversed([i for (i, n) in enumerate(r_codes) if (n in group)][1:]):\n                r_codes.pop(pos)\n        reduced_codes = ';'.join(sorted(r_codes, key=int))\n        if (codes != reduced_codes):\n            ansi_string = ansi_string.replace(escape, (('\\x1b[' + reduced_codes) + 'm'))\n    return ansi_string\n", "label": 1}
{"function": "\n\ndef __init__(self, **kwargs):\n    super(discreteBarChart, self).__init__(**kwargs)\n    self.model = 'discreteBarChart'\n    height = kwargs.get('height', 450)\n    width = kwargs.get('width', None)\n    if kwargs.get('x_is_date', False):\n        self.set_date_flag(True)\n        self.create_x_axis('xAxis', format=kwargs.get('x_axis_format', '%d %b %Y %H %S'), date=True)\n    else:\n        self.create_x_axis('xAxis', format=None)\n    self.create_y_axis('yAxis', format=kwargs.get('y_axis_format', '.0f'))\n    self.set_custom_tooltip_flag(True)\n    self.set_graph_height(height)\n    if width:\n        self.set_graph_width(width)\n    tooltips = kwargs.get('tooltips', True)\n    if (not tooltips):\n        self.chart_attr = {\n            'tooltips': 'false',\n        }\n", "label": 0}
{"function": "\n\n@access.public\ndef describeResource(self, resource, params):\n    if (resource not in docs.routes):\n        raise RestException(('Invalid resource: %s' % resource))\n    return {\n        'apiVersion': API_VERSION,\n        'swaggerVersion': SWAGGER_VERSION,\n        'basePath': getApiUrl(),\n        'models': dict(docs.models[resource], **docs.models[None]),\n        'apis': [{\n            'path': route,\n            'operations': sorted(op, key=functools.cmp_to_key(self._compareOperations)),\n        } for (route, op) in sorted(six.viewitems(docs.routes[resource]), key=functools.cmp_to_key(self._compareRoutes))],\n    }\n", "label": 0}
{"function": "\n\ndef capture(self, money, authorization, options=None):\n    options = (options or {\n        \n    })\n    params = {\n        'checkout_id': authorization,\n    }\n    token = options.pop('access_token', self.we_pay_settings['ACCESS_TOKEN'])\n    try:\n        response = self.we_pay.call('/checkout/capture', params, token=token)\n    except WePayError as error:\n        transaction_was_unsuccessful.send(sender=self, type='capture', response=error)\n        return {\n            'status': 'FAILURE',\n            'response': error,\n        }\n    transaction_was_successful.send(sender=self, type='capture', response=response)\n    return {\n        'status': 'SUCCESS',\n        'response': response,\n    }\n", "label": 0}
{"function": "\n\ndef reserve_provider_segment(self, session, segment):\n    filters = {\n        \n    }\n    physical_network = segment.get(api.PHYSICAL_NETWORK)\n    if (physical_network is not None):\n        filters['physical_network'] = physical_network\n        vlan_id = segment.get(api.SEGMENTATION_ID)\n        if (vlan_id is not None):\n            filters['vlan_id'] = vlan_id\n    if self.is_partial_segment(segment):\n        alloc = self.allocate_partially_specified_segment(session, **filters)\n        if (not alloc):\n            raise exc.NoNetworkAvailable()\n    else:\n        alloc = self.allocate_fully_specified_segment(session, **filters)\n        if (not alloc):\n            raise exc.VlanIdInUse(**filters)\n    return {\n        api.NETWORK_TYPE: p_const.TYPE_VLAN,\n        api.PHYSICAL_NETWORK: alloc.physical_network,\n        api.SEGMENTATION_ID: alloc.vlan_id,\n        api.MTU: self.get_mtu(alloc.physical_network),\n    }\n", "label": 0}
{"function": "\n\ndef _match_rhs(self, rhs, rightmost_stack):\n    \"\\n        :rtype: bool\\n        :return: true if the right hand side of a CFG production\\n            matches the rightmost elements of the stack.  ``rhs``\\n            matches ``rightmost_stack`` if they are the same length,\\n            and each element of ``rhs`` matches the corresponding\\n            element of ``rightmost_stack``.  A nonterminal element of\\n            ``rhs`` matches any Tree whose node value is equal\\n            to the nonterminal's symbol.  A terminal element of ``rhs``\\n            matches any string whose type is equal to the terminal.\\n        :type rhs: list(terminal and Nonterminal)\\n        :param rhs: The right hand side of a CFG production.\\n        :type rightmost_stack: list(string and Tree)\\n        :param rightmost_stack: The rightmost elements of the parser's\\n            stack.\\n        \"\n    if (len(rightmost_stack) != len(rhs)):\n        return False\n    for i in range(len(rightmost_stack)):\n        if isinstance(rightmost_stack[i], Tree):\n            if (not isinstance(rhs[i], Nonterminal)):\n                return False\n            if (rightmost_stack[i].label() != rhs[i].symbol()):\n                return False\n        else:\n            if isinstance(rhs[i], Nonterminal):\n                return False\n            if (rightmost_stack[i] != rhs[i]):\n                return False\n    return True\n", "label": 0}
{"function": "\n\ndef put(self):\n    pet = self.json_args\n    if (not isinstance(pet['id'], int)):\n        self.set_status(400)\n    if (not self.db.update_(**pet)):\n        self.set_status(404)\n    else:\n        self.set_status(200)\n    self.finish()\n", "label": 0}
{"function": "\n\ndef test_getitem_slice_big():\n    slt = SortedList(range(4))\n    lst = list(range(4))\n    itr = ((start, stop, step) for start in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for stop in [(- 6), (- 4), (- 2), 0, 2, 4, 6] for step in [(- 3), (- 2), (- 1), 1, 2, 3])\n    for (start, stop, step) in itr:\n        assert (slt[start:stop:step] == lst[start:stop:step])\n", "label": 0}
{"function": "\n\ndef get_response(self, cmd, fid, *args):\n    for source in self.select_best_source(fid.decode()):\n        dealer = None\n        try:\n            dealer = self.context.socket(zmq.DEALER)\n            dealer.connect(get_events_uri(self.session, source, 'router'))\n            dealer.send_multipart(((cmd, fid) + args))\n            response = dealer.recv_multipart()\n            if ((not response) or (response[0] == ERROR)):\n                self.logger.debug('Error with source {}', source)\n                continue\n            return response\n        finally:\n            if dealer:\n                dealer.close()\n    self.logger.debug('No more source available.')\n    return [ERROR]\n", "label": 0}
{"function": "\n\ndef httpapi(self, arg, opts):\n    sc = HttpAPIStatsCollector()\n    headers = ['#Item', 'Value']\n    table = []\n    for (k, v) in sc.get().getStats().iteritems():\n        if isinstance(v, dict):\n            v = json.dumps(v)\n        row = []\n        row.append(('#%s' % k))\n        if (k[(- 3):] == '_at'):\n            row.append(formatDateTime(v))\n        else:\n            row.append(v)\n        table.append(row)\n    self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))\n", "label": 0}
{"function": "\n\ndef _find_tab(self, widget):\n    for (key, bnch) in self.tab.items():\n        if (widget == bnch.widget):\n            return bnch\n    return None\n", "label": 0}
{"function": "\n\ndef redraw(self):\n    self.data.sort(self.sortfn)\n    rows = len(self.data)\n    cols = 0\n    if (rows > 0):\n        cols = len(self.data[0])\n    self.grid.resize(rows, cols)\n    self.header.resize(1, cols)\n    cf = self.grid.getCellFormatter()\n    for (nrow, row) in enumerate(self.data):\n        for (ncol, item) in enumerate(row):\n            self.grid.setHTML(nrow, ncol, str(item))\n            cf.setWidth(nrow, ncol, '200px')\n    cf = self.header.getCellFormatter()\n    self.sortbuttons = []\n    for ncol in range(cols):\n        sb = Button(('sort col %d' % ncol))\n        sb.addClickListener(self)\n        self.header.setWidget(0, ncol, sb)\n        cf.setWidth(0, ncol, '200px')\n        self.sortbuttons.append(sb)\n", "label": 0}
{"function": "\n\ndef clear(self):\n    'od.clear() -> None.  Remove all items from od.'\n    try:\n        for node in self.__map.itervalues():\n            del node[:]\n        root = self.__root\n        root[:] = [root, root, None]\n        self.__map.clear()\n    except AttributeError:\n        pass\n    dict.clear(self)\n", "label": 0}
{"function": "\n\n@group_only\ndef seen(self, msg, matches):\n    chat_id = msg.dest.id\n    if (matches.group(2) is not None):\n        return self.seen_by_id(chat_id, matches.group(2))\n    elif (matches.group(3) is not None):\n        return self.seen_by_username(chat_id, matches.group(3))\n    else:\n        return self.seen_by_fullname(chat_id, matches.group(4))\n", "label": 0}
{"function": "\n\ndef updateProperties(self):\n    if (self.modelXbrl is not None):\n        modelXbrl = self.modelXbrl\n        if (modelXbrl.modelDocument.type == ModelDocument.Type.INSTANCE):\n            tbl = self.table\n            coordinates = tbl.getCurrentCellCoordinates()\n            if (coordinates is not None):\n                objId = tbl.getObjectId(coordinates)\n                if ((objId is not None) and (len(objId) > 0)):\n                    if (objId and (objId[0] == 'f')):\n                        viewableObject = self.factPrototypes[int(objId[1:])]\n                    elif (objId[0] != 'a'):\n                        viewableObject = self.modelXbrl.modelObject(objId)\n                    else:\n                        return\n                    modelXbrl.viewModelObject(viewableObject)\n", "label": 0}
{"function": "\n\ndef Validate(self):\n    'Attempt to validate the artifact has been well defined.\\n\\n    This is used to enforce Artifact rules. Since it checks all dependencies are\\n    present, this method can only be called once all artifacts have been loaded\\n    into the registry. Use ValidateSyntax to check syntax for each artifact on\\n    import.\\n\\n    Raises:\\n      ArtifactDefinitionError: If artifact is invalid.\\n    '\n    self.ValidateSyntax()\n    try:\n        for dependency in self.GetArtifactDependencies():\n            dependency_obj = REGISTRY.GetArtifact(dependency)\n            if dependency_obj.error_message:\n                raise ArtifactDefinitionError(('Dependency %s has an error!' % dependency))\n    except ArtifactNotRegisteredError as e:\n        raise ArtifactDefinitionError(e)\n", "label": 0}
{"function": "\n\ndef _destroy_kernel_ramdisk(self, instance, vm_ref):\n    'Three situations can occur:\\n\\n            1. We have neither a ramdisk nor a kernel, in which case we are a\\n               RAW image and can omit this step\\n\\n            2. We have one or the other, in which case, we should flag as an\\n               error\\n\\n            3. We have both, in which case we safely remove both the kernel\\n               and the ramdisk.\\n\\n        '\n    instance_uuid = instance['uuid']\n    if ((not instance['kernel_id']) and (not instance['ramdisk_id'])):\n        LOG.debug('Using RAW or VHD, skipping kernel and ramdisk deletion', instance=instance)\n        return\n    if (not (instance['kernel_id'] and instance['ramdisk_id'])):\n        raise exception.InstanceUnacceptable(instance_id=instance_uuid, reason=_('instance has a kernel or ramdisk but not both'))\n    (kernel, ramdisk) = vm_utils.lookup_kernel_ramdisk(self._session, vm_ref)\n    if (kernel or ramdisk):\n        vm_utils.destroy_kernel_ramdisk(self._session, instance, kernel, ramdisk)\n        LOG.debug('kernel/ramdisk files removed', instance=instance)\n", "label": 0}
{"function": "\n\ndef filter_log_files_for_zipping(log_files):\n    \"Identify unzipped log files that are approporate for zipping.\\n\\n    Each unique log type found should have the most recent log file unzipped\\n    as it's probably still in use.\\n    \"\n    out_files = []\n    for lf in filter_log_files_for_active(log_files):\n        if lf.bzip:\n            continue\n        out_files.append(lf)\n    return out_files\n", "label": 0}
{"function": "\n\ndef http_method_not_allowed(self, request, *args, **kwargs):\n    allowed_methods = [m for m in self.http_method_names if hasattr(self, m)]\n    logger.warning(('Method Not Allowed (%s): %s' % (request.method, request.path)), extra={\n        'status_code': 405,\n        'request': self.request,\n    })\n    return http.HttpResponseNotAllowed(allowed_methods)\n", "label": 0}
{"function": "\n\ndef register(self, name, c):\n    if ((name in self.registered) and (c is not self.registered[name])):\n        raise NameError('{} has been registered by {}'.format(name, self.registered[name]))\n    self.registered[name] = c\n", "label": 0}
{"function": "\n\ndef __init__(self, pattern, flags=0):\n    'The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.'\n    super(Regex, self).__init__()\n    if isinstance(pattern, basestring):\n        if (len(pattern) == 0):\n            warnings.warn('null string passed to Regex; use Empty() instead', SyntaxWarning, stacklevel=2)\n        self.pattern = pattern\n        self.flags = flags\n        try:\n            self.re = re.compile(self.pattern, self.flags)\n            self.reString = self.pattern\n        except sre_constants.error:\n            warnings.warn(('invalid pattern (%s) passed to Regex' % pattern), SyntaxWarning, stacklevel=2)\n            raise\n    elif isinstance(pattern, Regex.compiledREtype):\n        self.re = pattern\n        self.pattern = self.reString = str(pattern)\n        self.flags = flags\n    else:\n        raise ValueError('Regex may only be constructed with a string or a compiled RE object')\n    self.name = _ustr(self)\n    self.errmsg = ('Expected ' + self.name)\n    self.mayIndexError = False\n    self.mayReturnEmpty = True\n", "label": 0}
{"function": "\n\ndef get_cpu_state(self):\n    '\\n        Retrieves CPU state from client\\n        '\n    state = c_int(0)\n    self.library.Cli_GetPlcStatus(self.pointer, byref(state))\n    try:\n        status_string = cpu_statuses[state.value]\n    except KeyError:\n        status_string = None\n    if (not status_string):\n        raise Snap7Exception(('The cpu state (%s) is invalid' % state.value))\n    logging.debug(('CPU state is %s' % status_string))\n    return status_string\n", "label": 0}
{"function": "\n\ndef stackhut_api_call(endpoint, msg, secure=True, return_json=True):\n    url = urllib.parse.urljoin(utils.SERVER_URL, endpoint)\n    log.debug('Calling Stackhut Server at {} with \\n\\t{}'.format(url, json.dumps(msg)))\n    r = requests.post(url, data=json.dumps(msg), headers=json_header)\n    if (r.status_code == requests.codes.ok):\n        return (r.json() if return_json else r.text)\n    else:\n        log.error('Error {} talking to Stackhut Server'.format(r.status_code))\n        log.error(r.text)\n        r.raise_for_status()\n", "label": 0}
{"function": "\n\ndef render(self, context):\n    from pennyblack.models import Link, Newsletter\n    if ('mail' not in context):\n        return '#'\n    mail = context['mail']\n    newsletter = mail.job.newsletter\n    if newsletter.is_workflow():\n        job = newsletter.get_default_job()\n    else:\n        job = mail.job\n    try:\n        link = job.links.get(identifier=self.identifier)\n    except job.links.model.DoesNotExist:\n        link = Newsletter.add_view_link_to_job(self.identifier, job)\n    return (context['base_url'] + reverse('pennyblack.redirect_link', args=(mail.mail_hash, link.link_hash)))\n", "label": 0}
{"function": "\n\ndef __repr__(self):\n    'We want it to print as a Cycle, not as a dict.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.combinatorics import Cycle\\n        >>> Cycle(1, 2)\\n        (1 2)\\n        >>> print(_)\\n        (1 2)\\n        >>> list(Cycle(1, 2).items())\\n        [(1, 2), (2, 1)]\\n        '\n    if (not self):\n        return 'Cycle()'\n    cycles = Permutation(self).cyclic_form\n    s = ''.join((str(tuple(c)) for c in cycles))\n    big = (self.size - 1)\n    if (not any(((i == big) for c in cycles for i in c))):\n        s += ('(%s)' % big)\n    return ('Cycle%s' % s)\n", "label": 0}
{"function": "\n\ndef info2iob(sentence, chunks, informations):\n    info_list = ([], [], [])\n    for information in informations:\n        temp_list = positions(information, sentence)\n        for i in range(3):\n            if (temp_list[i] not in info_list[i]):\n                info_list[i].append(temp_list[i])\n    return tag_sent(chunks, info_list)\n", "label": 0}
{"function": "\n\ndef _setup_units(self, connections, params_dict, unknowns_dict):\n    '\\n        Calculate unit conversion factors for any connected\\n        variables having different units and store them in params_dict.\\n\\n        Args\\n        ----\\n        connections : dict\\n            A dict of target variables (absolute name) mapped\\n            to the absolute name of their source variable and the\\n            relevant indices of that source if applicable.\\n\\n        params_dict : OrderedDict\\n            A dict of parameter metadata for the whole `Problem`.\\n\\n        unknowns_dict : OrderedDict\\n            A dict of unknowns metadata for the whole `Problem`.\\n        '\n    to_prom_name = self.root._sysdata.to_prom_name\n    for (target, (source, idxs)) in iteritems(connections):\n        tmeta = params_dict[target]\n        smeta = unknowns_dict[source]\n        if (('units' not in tmeta) or ('units' not in smeta)):\n            continue\n        src_unit = smeta['units']\n        tgt_unit = tmeta['units']\n        try:\n            (scale, offset) = get_conversion_tuple(src_unit, tgt_unit)\n        except TypeError as err:\n            if (str(err) == 'Incompatible units'):\n                msg = \"Unit '{0}' in source {1} is incompatible with unit '{2}' in target {3}.\".format(src_unit, _both_names(smeta, to_prom_name), tgt_unit, _both_names(tmeta, to_prom_name))\n                self._setup_errors.append(msg)\n                continue\n            else:\n                raise\n        if ((scale != 1.0) or (offset != 0.0)):\n            tmeta['unit_conv'] = (scale, offset)\n", "label": 0}
{"function": "\n\ndef _delete(self, filter, multi=False):\n    if (filter is None):\n        filter = {\n            \n        }\n    if (not isinstance(filter, collections.Mapping)):\n        filter = {\n            '_id': filter,\n        }\n    to_delete = list(self.find(filter))\n    deleted_count = 0\n    for doc in to_delete:\n        doc_id = doc['_id']\n        if isinstance(doc_id, dict):\n            doc_id = helpers.hashdict(doc_id)\n        del self._documents[doc_id]\n        deleted_count += 1\n        if (not multi):\n            break\n    return {\n        'connectionId': self._database.client._id,\n        'n': deleted_count,\n        'ok': 1.0,\n        'err': None,\n    }\n", "label": 0}
{"function": "\n\ndef select_coins(self, colorvalue, use_fee_estimator=None):\n    self._validate_select_coins_parameters(colorvalue, use_fee_estimator)\n    colordef = colorvalue.get_colordef()\n    if (colordef == UNCOLORED_MARKER):\n        return self.select_uncolored_coins(colorvalue, use_fee_estimator)\n    color_id = colordef.get_color_id()\n    if (color_id in self.inputs):\n        total = SimpleColorValue.sum([cv_u[0] for cv_u in self.inputs[color_id]])\n        if (total < colorvalue):\n            msg = 'Not enough coins: %s requested, %s found!'\n            raise InsufficientFundsError((msg % (colorvalue, total)))\n        return ([cv_u[1] for cv_u in self.inputs[color_id]], total)\n    if (colorvalue > self.our_value_limit):\n        raise InsufficientFundsError(('%s requested, %s found!' % (colorvalue, self.our_value_limit)))\n    return super(OperationalETxSpec, self).select_coins(colorvalue)\n", "label": 0}
{"function": "\n\ndef match(self, left, collected=None):\n    collected = ([] if (collected is None) else collected)\n    outcomes = []\n    for p in self.children:\n        (matched, _, _) = outcome = p.match(left, collected)\n        if matched:\n            outcomes.append(outcome)\n    if outcomes:\n        return min(outcomes, key=(lambda outcome: len(outcome[1])))\n    return (False, left, collected)\n", "label": 0}
{"function": "\n\ndef _replace_cdata_list_attribute_values(self, tag_name, attrs):\n    'Replaces class=\"foo bar\" with class=[\"foo\", \"bar\"]\\n\\n        Modifies its input in place.\\n        '\n    if (not attrs):\n        return attrs\n    if self.cdata_list_attributes:\n        universal = self.cdata_list_attributes.get('*', [])\n        tag_specific = self.cdata_list_attributes.get(tag_name.lower(), None)\n        for attr in list(attrs.keys()):\n            if ((attr in universal) or (tag_specific and (attr in tag_specific))):\n                value = attrs[attr]\n                if isinstance(value, str):\n                    values = whitespace_re.split(value)\n                else:\n                    values = value\n                attrs[attr] = values\n    return attrs\n", "label": 0}
{"function": "\n\ndef _sanitize_mod_params(self, other):\n    \"Sanitize the object being modded with this Message.\\n\\n        - Add support for modding 'None' so translation supports it\\n        - Trim the modded object, which can be a large dictionary, to only\\n        those keys that would actually be used in a translation\\n        - Snapshot the object being modded, in case the message is\\n        translated, it will be used as it was when the Message was created\\n        \"\n    if (other is None):\n        params = (other,)\n    elif isinstance(other, dict):\n        params = self._trim_dictionary_parameters(other)\n    else:\n        params = self._copy_param(other)\n    return params\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if (not (type(other) is STP)):\n        return False\n    return ((self.network == other.network) and (self.port == other.port) and (self.label == other.label))\n", "label": 0}
{"function": "\n\ndef setup():\n    '\\n    Create necessary directories for testing.\\n    '\n    train_dir = join(_my_dir, 'train')\n    if (not exists(train_dir)):\n        os.makedirs(train_dir)\n    output_dir = join(_my_dir, 'output')\n    if (not exists(output_dir)):\n        os.makedirs(output_dir)\n", "label": 0}
{"function": "\n\ndef render_result(result_data, test_bundle):\n    result_data = lcase_keys(result_data)\n    result_string = (sublime.expand_variables(RESULTS_TEMPLATES['results'], filter_stats_dict(result_data)) + '\\n')\n    for bundle in result_data['bundlestats']:\n        if (len(test_bundle) and (bundle['path'] != test_bundle)):\n            continue\n        result_string += (('\\n' + sublime.expand_variables(RESULTS_TEMPLATES['bundle'], filter_stats_dict(bundle))) + '\\n')\n        if isinstance(bundle['globalexception'], dict):\n            result_string += (('\\n' + sublime.expand_variables(RESULTS_TEMPLATES['global_exception'], filter_exception_dict(bundle['globalexception']))) + '\\n')\n        for suite in bundle['suitestats']:\n            result_string += ('\\n' + gen_suite_report(suite))\n    result_string += ('\\n' + RESULTS_TEMPLATES['legend'])\n    return result_string\n", "label": 0}
{"function": "\n\ndef _dictify(data, name='input', key_mod=(lambda x: x), value_mod=(lambda x: x)):\n    if data:\n        if isinstance(data, collections.Sequence):\n            return dict(((key_mod(str(v)), value_mod(str(v))) for v in data))\n        elif isinstance(data, collections.Mapping):\n            return dict(((key_mod(str(k)), value_mod(str((v or k)))) for (k, v) in list(data.items())))\n        else:\n            raise BlockadeConfigError(('invalid %s: need list or map' % (name,)))\n    else:\n        return {\n            \n        }\n", "label": 0}
{"function": "\n\ndef login_action(request):\n    username = request.REQUEST['username']\n    password = request.REQUEST['password']\n    user = authenticate(username=username, password=password)\n    if (user is None):\n        return json_failure(_('Invalid username or password'))\n    if (not user.is_active):\n        return json_failure(_('Account disabled.'))\n    login(request, user)\n    return json_response()\n", "label": 0}
{"function": "\n\ndef Check(self):\n    'Assertion verification for options.'\n    try:\n        assert (self.m0 >= 0), 'margin0'\n        assert (self.m1 >= self.m0), 'margin1'\n        assert (self.c0 >= 0), 'cost0'\n        assert (self.c1 >= 0), 'cost1'\n        assert (self.cb >= 0), 'costb'\n        assert (self.ind >= 0), 'indent'\n        assert (self.adj_comment >= 0), 'adj_comment'\n        assert (self.adj_flow >= 0), 'adj_flow'\n        assert (self.adj_call >= 0), 'adj_call'\n        assert (self.adj_arg >= 0), 'adj_arg'\n        assert (self.cpack >= 0), 'cpack'\n    except AssertionError as e:\n        raise Error((\"Illegal option value for '%s'\" % e.args[0]))\n", "label": 1}
{"function": "\n\ndef message_user(self, request, message, level=messages.INFO, extra_tags='', fail_silently=False):\n    '\\n        Send a message to the user. The default implementation\\n        posts a message using the django.contrib.messages backend.\\n\\n        Exposes almost the same API as messages.add_message(), but accepts the\\n        positional arguments in a different order to maintain backwards\\n        compatibility. For convenience, it accepts the `level` argument as\\n        a string rather than the usual level number.\\n        '\n    if (not isinstance(level, int)):\n        try:\n            level = getattr(messages.constants, level.upper())\n        except AttributeError:\n            levels = messages.constants.DEFAULT_TAGS.values()\n            levels_repr = ', '.join((('`%s`' % l) for l in levels))\n            raise ValueError(('Bad message level string: `%s`. Possible values are: %s' % (level, levels_repr)))\n    messages.add_message(request, level, message, extra_tags=extra_tags, fail_silently=fail_silently)\n", "label": 0}
{"function": "\n\ndef add(self, grid):\n    '\\n        Used to add quantities from another grid\\n\\n        Parameters\\n        ----------\\n        grid : 3D Numpy array or SphericalPolarGridView instance\\n            The grid to copy the quantity from\\n        '\n    if (type(self.quantities[self.viewed_quantity]) is list):\n        raise Exception('need to first specify the item to add to')\n    if isinstance(grid, SphericalPolarGridView):\n        if (type(grid.quantities[grid.viewed_quantity]) is list):\n            raise Exception('need to first specify the item to add')\n        self._check_array_dimensions(grid.quantities[grid.viewed_quantity])\n        self.quantities[self.viewed_quantity] += grid.quantities[grid.viewed_quantity]\n    elif isinstance(grid, np.ndarray):\n        self._check_array_dimensions(grid)\n        self.quantities[self.viewed_quantity] += grid\n    else:\n        raise ValueError('grid should be a Numpy array or a SphericalPolarGridView instance')\n", "label": 0}
{"function": "\n\ndef create_junk():\n    fileh = open_file(filename, mode='w')\n    group = fileh.create_group(fileh.root, 'newgroup')\n    for i in range(NLEAVES):\n        table = fileh.create_table(group, ('table' + str(i)), Particle, 'A table', Filters(1))\n        particle = table.row\n        print('Creating table-->', table._v_name)\n        for i in range(NROWS):\n            particle.append()\n        table.flush()\n    fileh.close()\n", "label": 0}
{"function": "\n\ndef getopt(args, shortopts):\n    'getopt(args, options) -> opts, long_opts, args \\nReturns options as list of tuples, long options as entries in a dictionary, and\\nthe remaining arguments.'\n    opts = []\n    longopts = {\n        \n    }\n    while (args and args[0].startswith('-') and (args[0] != '-')):\n        if (args[0] == '--'):\n            args = args[1:]\n            break\n        if args[0].startswith('--'):\n            arg = args.pop(0)\n            _do_longs(longopts, arg)\n        else:\n            (opts, args) = _do_shorts(opts, args[0][1:], shortopts, args[1:])\n    return (opts, longopts, args)\n", "label": 0}
{"function": "\n\n@app.route('/api')\ndef api():\n    \"WebSocket endpoint; Takes a 'topic' GET param.\"\n    ws = request.environ.get('wsgi.websocket')\n    topic = request.args.get('topic')\n    if (None in (ws, topic)):\n        return\n    topic = topic.encode('ascii')\n    for (message, message_topic) in CircusConsumer(topic, endpoint=ZMQ_ENDPOINT):\n        response = json.dumps(dict(message=message, topic=message_topic))\n        ws.send(response)\n", "label": 0}
{"function": "\n\ndef test_active_contributors(self):\n    'Test the active_contributors util method.'\n    start_date = self.start_date\n    en_us_contributors = active_contributors(from_date=start_date, locale='en-US')\n    es_contributors = active_contributors(from_date=start_date, locale='es')\n    all_contributors = active_contributors(from_date=start_date)\n    eq_(3, len(en_us_contributors))\n    assert (self.user in en_us_contributors)\n    assert (self.en_us_old.creator not in en_us_contributors)\n    eq_(4, len(es_contributors))\n    assert (self.user in es_contributors)\n    assert (self.es_old.creator not in es_contributors)\n    eq_(6, len(all_contributors))\n    assert (self.user in all_contributors)\n    assert (self.en_us_old.creator not in all_contributors)\n    assert (self.es_old.creator not in all_contributors)\n", "label": 0}
{"function": "\n\ndef _cleanup_groups(self):\n    exception_list = list()\n    for group in self.cloud.list_groups():\n        if group['name'].startswith(self.group_prefix):\n            try:\n                self.cloud.delete_group(group['id'])\n            except Exception as e:\n                exception_list.append(str(e))\n                continue\n    if exception_list:\n        raise OpenStackCloudException('\\n'.join(exception_list))\n", "label": 0}
{"function": "\n\ndef _expand_glob_path(file_roots):\n    '\\n    Applies shell globbing to a set of directories and returns\\n    the expanded paths\\n    '\n    unglobbed_path = []\n    for path in file_roots:\n        try:\n            if glob.has_magic(path):\n                unglobbed_path.extend(glob.glob(path))\n            else:\n                unglobbed_path.append(path)\n        except Exception:\n            unglobbed_path.append(path)\n    return unglobbed_path\n", "label": 0}
{"function": "\n\ndef listKeysAndSizes(self, bucketName):\n    'Return a list of (name, size) pairs of keys in the bucket'\n    with self.state_.lock:\n        if (bucketName not in self.state_.buckets_):\n            raise S3Interface.BucketNotFound(bucketName)\n        self.state_.validateAccess(bucketName, self.credentials_)\n        return [(key, len(val.value), val.mtime) for (key, val) in self.state_.buckets_[bucketName].iteritems()]\n", "label": 0}
{"function": "\n\ndef cleanup(self, tc_name=''):\n    if (tc_name != ''):\n        self._log.info(('%s: FAILED' % tc_name))\n    for obj in ['ruleset', 'rule', 'classifier', 'action']:\n        self.gbpcfg.gbp_del_all_anyobj(obj)\n", "label": 0}
{"function": "\n\ndef _geo_field(self, field_name=None):\n    \"\\n        Returns the first Geometry field encountered; or specified via the\\n        `field_name` keyword.  The `field_name` may be a string specifying\\n        the geometry field on this GeoQuery's model, or a lookup string\\n        to a geometry field via a ForeignKey relation.\\n        \"\n    if (field_name is None):\n        for fld in self.model._meta.fields:\n            if isinstance(fld, GeometryField):\n                return fld\n        return False\n    else:\n        return GeoWhereNode._check_geo_field(self.model._meta, field_name)\n", "label": 0}
{"function": "\n\ndef compute(self):\n    if self.has_input('foo'):\n        v1 = self.get_input('foo')\n    else:\n        v1 = 0\n    if (v1 != 12):\n        self.change_parameter('foo', (v1 + 1))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef overwrite_attribute(entity_id, attrs, vals):\n    'Overwrite any attribute of an entity.\\n\\n        This function should receive a list of attributes and a\\n        list of values. Set attribute to None to remove any overwritten\\n        value in place.\\n        '\n    for (attr, val) in zip(attrs, vals):\n        if (val is None):\n            _OVERWRITE[entity_id.lower()].pop(attr, None)\n        else:\n            _OVERWRITE[entity_id.lower()][attr] = val\n", "label": 0}
{"function": "\n\n@classmethod\ndef _from_line(cls, remote, line):\n    'Create a new PushInfo instance as parsed from line which is expected to be like\\n            refs/heads/master:refs/heads/master 05d2687..1d0568e as bytes'\n    (control_character, from_to, summary) = line.split('\\t', 3)\n    flags = 0\n    try:\n        flags |= cls._flag_map[control_character]\n    except KeyError:\n        raise ValueError(('Control character %r unknown as parsed from line %r' % (control_character, line)))\n    (from_ref_string, to_ref_string) = from_to.split(':')\n    if (flags & cls.DELETED):\n        from_ref = None\n    else:\n        from_ref = Reference.from_path(remote.repo, from_ref_string)\n    old_commit = None\n    if summary.startswith('['):\n        if ('[rejected]' in summary):\n            flags |= cls.REJECTED\n        elif ('[remote rejected]' in summary):\n            flags |= cls.REMOTE_REJECTED\n        elif ('[remote failure]' in summary):\n            flags |= cls.REMOTE_FAILURE\n        elif ('[no match]' in summary):\n            flags |= cls.ERROR\n        elif ('[new tag]' in summary):\n            flags |= cls.NEW_TAG\n        elif ('[new branch]' in summary):\n            flags |= cls.NEW_HEAD\n    else:\n        split_token = '...'\n        if (control_character == ' '):\n            split_token = '..'\n        (old_sha, new_sha) = summary.split(' ')[0].split(split_token)\n        old_commit = remote.repo.commit(old_sha)\n    return PushInfo(flags, from_ref, to_ref_string, remote, old_commit, summary)\n", "label": 1}
{"function": "\n\ndef init(self, modelDocument):\n    super(ModelRssItem, self).init(modelDocument)\n    try:\n        if (self.modelXbrl.modelManager.rssWatchOptions.latestPubDate and (self.pubDate <= self.modelXbrl.modelManager.rssWatchOptions.latestPubDate)):\n            self.status = _('tested')\n        else:\n            self.status = _('not tested')\n    except AttributeError:\n        self.status = _('not tested')\n    self.results = None\n    self.assertions = None\n", "label": 0}
{"function": "\n\ndef rgb_to_hsv(r, g, b):\n    maxc = max(r, g, b)\n    minc = min(r, g, b)\n    v = maxc\n    if (minc == maxc):\n        return (0.0, 0.0, v)\n    s = ((maxc - minc) / maxc)\n    rc = ((maxc - r) / (maxc - minc))\n    gc = ((maxc - g) / (maxc - minc))\n    bc = ((maxc - b) / (maxc - minc))\n    if (r == maxc):\n        h = (bc - gc)\n    elif (g == maxc):\n        h = ((2.0 + rc) - bc)\n    else:\n        h = ((4.0 + gc) - rc)\n    h = ((h / 6.0) % 1.0)\n    return (h, s, v)\n", "label": 0}
{"function": "\n\ndef build_xform(self):\n    xform = XFormBuilder(self.name)\n    for ig in self.iter_item_groups():\n        data_type = ('repeatGroup' if self.is_repeating else 'group')\n        group = xform.new_group(ig.question_name, ig.question_label, data_type)\n        for item in ig.iter_items():\n            group.new_question(item.question_name, item.question_label, ODK_DATA_TYPES[item.data_type], choices=item.choices)\n    return xform.tostring(pretty_print=True, encoding='utf-8', xml_declaration=True)\n", "label": 0}
{"function": "\n\ndef permutations(xs):\n    if (not xs):\n        (yield [])\n    else:\n        for (y, ys) in selections(xs):\n            for pys in permutations(ys):\n                (yield ([y] + pys))\n", "label": 0}
{"function": "\n\ndef _annotate_local(self):\n    \"Annotate the primaryjoin and secondaryjoin\\n        structures with 'local' annotations.\\n\\n        This annotates all column elements found\\n        simultaneously in the parent table\\n        and the join condition that don't have a\\n        'remote' annotation set up from\\n        _annotate_remote() or user-defined.\\n\\n        \"\n    if self._has_annotation(self.primaryjoin, 'local'):\n        return\n    if self._local_remote_pairs:\n        local_side = util.column_set([l for (l, r) in self._local_remote_pairs])\n    else:\n        local_side = util.column_set(self.parent_selectable.c)\n\n    def locals_(elem):\n        if (('remote' not in elem._annotations) and (elem in local_side)):\n            return elem._annotate({\n                'local': True,\n            })\n    self.primaryjoin = visitors.replacement_traverse(self.primaryjoin, {\n        \n    }, locals_)\n", "label": 0}
{"function": "\n\ndef __cmp__(self, other):\n    'ensure that same seq intervals match in cmp()'\n    if (not isinstance(other, SeqPath)):\n        return (- 1)\n    if (self.path is other.path):\n        return cmp((self.start, self.stop), (other.start, other.stop))\n    else:\n        return NOT_ON_SAME_PATH\n", "label": 0}
{"function": "\n\ndef contribute_to_class(self, cls, name):\n    if ((not self.frozen_by_south) and (name not in [f.name for f in cls._meta.fields])):\n        super(CurrencyField, self).contribute_to_class(cls, name)\n", "label": 0}
{"function": "\n\ndef dapInfo(self, id_):\n    cmd = []\n    cmd.append(COMMAND_ID['DAP_INFO'])\n    cmd.append(ID_INFO[id_])\n    self.interface.write(cmd)\n    resp = self.interface.read()\n    if (resp[0] != COMMAND_ID['DAP_INFO']):\n        raise ValueError('DAP_INFO response error')\n    if (resp[1] == 0):\n        return\n    if (id_ in ('CAPABILITIES', 'PACKET_COUNT', 'PACKET_SIZE')):\n        if (resp[1] == 1):\n            return resp[2]\n        if (resp[1] == 2):\n            return ((resp[3] << 8) | resp[2])\n    x = array.array('B', [i for i in resp[2:(2 + resp[1])]])\n    return x.tostring()\n", "label": 0}
{"function": "\n\ndef test_pos_list_append_with_nonexistent_key(self):\n    '\\n        Invoke list_append() with non-existent key\\n        '\n    charSet = 'abcdefghijklmnopqrstuvwxyz1234567890'\n    minLength = 5\n    maxLength = 30\n    length = random.randint(minLength, maxLength)\n    key = ('test', 'demo', (''.join(map((lambda unused: random.choice(charSet)), range(length))) + '.com'))\n    status = self.as_connection.list_append(key, 'abc', 122)\n    assert (status == 0)\n    (key, _, bins) = self.as_connection.get(key)\n    self.as_connection.remove(key)\n    assert (status == 0)\n    assert (bins == {\n        'abc': [122],\n    })\n", "label": 0}
{"function": "\n\ndef start(self):\n    'Start watching the directory for changes.'\n    with self._inotify_fd_lock:\n        if (self._inotify_fd < 0):\n            return\n        self._inotify_poll.register(self._inotify_fd, select.POLLIN)\n        for directory in self._directories:\n            self._add_watch_for_path(directory)\n", "label": 0}
{"function": "\n\ndef is_lazy_user(user):\n    ' Return True if the passed user is a lazy user. '\n    if user.is_anonymous():\n        return False\n    backend = getattr(user, 'backend', None)\n    if (backend == 'lazysignup.backends.LazySignupBackend'):\n        return True\n    from lazysignup.models import LazyUser\n    return bool((LazyUser.objects.filter(user=user).count() > 0))\n", "label": 0}
{"function": "\n\n@click.command()\n@click.argument('identifier')\n@click.option('--postinstall', '-i', help='Post-install script to download')\n@click.option('--image', help=\"Image ID. The default is to use the current operating system.\\nSee: 'slcli image list' for reference\")\n@helpers.multi_option('--key', '-k', help='SSH keys to add to the root user')\n@environment.pass_env\ndef cli(env, identifier, postinstall, key, image):\n    'Reload operating system on a virtual server.'\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n    keys = []\n    if key:\n        for single_key in key:\n            resolver = SoftLayer.SshKeyManager(env.client).resolve_ids\n            key_id = helpers.resolve_id(resolver, single_key, 'SshKey')\n            keys.append(key_id)\n    if (not (env.skip_confirmations or formatting.no_going_back(vs_id))):\n        raise exceptions.CLIAbort('Aborted')\n    vsi.reload_instance(vs_id, post_uri=postinstall, ssh_keys=keys, image_id=image)\n", "label": 0}
{"function": "\n\ndef findPeak(self, A):\n    '\\n        Binary search\\n        Microsoft Interview, Oct 2014\\n\\n        To reduce the complexity of dealing the edge cases:\\n        * add two anti-peak dummies on the both ends\\n\\n        :param A: An integers list. A[0] and A[-1] are dummies.\\n        :return: return any of peek positions.\\n        '\n    n = len(A)\n    l = 0\n    h = n\n    while (l < h):\n        m = ((l + h) / 2)\n        if (A[(m - 1)] < A[m] > A[(m + 1)]):\n            return m\n        elif (A[(m + 1)] > A[m]):\n            l = (m + 1)\n        else:\n            h = m\n    raise Exception\n", "label": 0}
{"function": "\n\ndef test_invalid_base_fields(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.ForeignKey('testapp.Author'), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E005')\n    assert ('Base field for list must be' in errors[0].msg)\n", "label": 0}
{"function": "\n\ndef _AtNonLeaf(self, attr_value, path):\n    'Called when at a non-leaf value. Should recurse and yield values.'\n    try:\n        if isinstance(attr_value, collections.Mapping):\n            sub_obj = attr_value.get(path[1])\n            if (len(path) > 2):\n                sub_obj = self.Expand(sub_obj, path[2:])\n            if isinstance(sub_obj, basestring):\n                (yield sub_obj)\n            elif isinstance(sub_obj, collections.Mapping):\n                for (k, v) in sub_obj.items():\n                    (yield {\n                        k: v,\n                    })\n            else:\n                for value in sub_obj:\n                    (yield value)\n        else:\n            for sub_obj in attr_value:\n                for value in self.Expand(sub_obj, path[1:]):\n                    (yield value)\n    except TypeError:\n        for value in self.Expand(attr_value, path[1:]):\n            (yield value)\n", "label": 1}
{"function": "\n\ndef log_notifications(self, notifications):\n    main_logger = logging.getLogger(config.main_logger_name)\n    notification_logger = logging.getLogger(config.notifications_logger_name)\n    for notification in notifications:\n        try:\n            notification['content'] = notification['content'].encode('utf-8').replace(',', '\\\\,')\n            keys = ['status', 'login_id', 'content', 'message_id', 'campaign_id', 'sending_id', 'game', 'world_id', 'screen', 'time', 'time_to_live_ts_bigint', 'platform', 'receiver_id']\n            notification_logger.info(','.join([str(notification[key]) for key in keys]))\n        except:\n            main_logger.exception('Error while logging notification to csv log!')\n", "label": 0}
{"function": "\n\ndef write_output(args, powerline, segment_info, write):\n    if args.renderer_arg:\n        segment_info.update(args.renderer_arg)\n    if args.side.startswith('above'):\n        for line in powerline.render_above_lines(width=args.width, segment_info=segment_info, mode=segment_info.get('mode', None)):\n            if line:\n                write((line + '\\n'))\n        args.side = args.side[len('above'):]\n    if args.side:\n        rendered = powerline.render(width=args.width, side=args.side, segment_info=segment_info, mode=segment_info.get('mode', None))\n        write(rendered)\n", "label": 0}
{"function": "\n\ndef validate(self, value):\n    super(Interval, self).validate(value)\n    if (not ((value is None) or (self.interval_type.is_valid(value) and (value >= self.start) and (value <= self.end)))):\n        raise ValueError(('expected a value of type %s in range [%s, %s], got %r' % (self.interval_type, self.start, self.end, value)))\n", "label": 0}
{"function": "\n\ndef test_K4_normalized(self):\n    'Betweenness centrality: K4'\n    G = networkx.complete_graph(4)\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True)\n    b_answer = {\n        0: 0.25,\n        1: 0.25,\n        2: 0.25,\n        3: 0.25,\n    }\n    for n in sorted(G):\n        assert_almost_equal(b[n], b_answer[n])\n    G.add_edge(0, 1, {\n        'weight': 0.5,\n        'other': 0.3,\n    })\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True, weight=None)\n    for n in sorted(G):\n        assert_almost_equal(b[n], b_answer[n])\n    wb_answer = {\n        0: 0.2222222,\n        1: 0.2222222,\n        2: 0.30555555,\n        3: 0.30555555,\n    }\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert_almost_equal(b[n], wb_answer[n])\n    wb_answer = {\n        0: 0.2051282,\n        1: 0.2051282,\n        2: 0.33974358,\n        3: 0.33974358,\n    }\n    b = networkx.current_flow_betweenness_centrality(G, normalized=True, weight='other')\n    for n in sorted(G):\n        assert_almost_equal(b[n], wb_answer[n])\n", "label": 0}
{"function": "\n\ndef data_path(path, createdir=False):\n    'If path is relative, return the given path inside the project data dir,\\n    otherwise return the path unmodified\\n    '\n    if (not isabs(path)):\n        path = join(project_data_dir(), path)\n    if (createdir and (not exists(path))):\n        os.makedirs(path)\n    return path\n", "label": 0}
{"function": "\n\ndef __call__(self, fn):\n\n    def wrapper(*args, **kwargs):\n        that = args[0]\n        that.logger.debug(self.start)\n        ret = fn(*args, **kwargs)\n        that.logger.debug(self.finish)\n        if self.getter:\n            that.logger.debug(pformat(self.getter(ret)))\n        else:\n            that.logger.debug(pformat(ret))\n        return ret\n    wrapper.func_name = fn.func_name\n    if hasattr(fn, '__name__'):\n        wrapper.__name__ = self.name = fn.__name__\n    if hasattr(fn, '__doc__'):\n        wrapper.__doc__ = fn.__doc__\n    if hasattr(fn, '__module__'):\n        wrapper.__module__ = fn.__module__\n    return wrapper\n", "label": 0}
{"function": "\n\ndef configure_uploads(app, upload_sets):\n    \"\\n    Call this after the app has been configured. It will go through all the\\n    upload sets, get their configuration, and store the configuration on the\\n    app. It will also register the uploads module if it hasn't been set. This\\n    can be called multiple times with different upload sets.\\n    \\n    .. versionchanged:: 0.1.3\\n       The uploads module/blueprint will only be registered if it is needed\\n       to serve the upload sets.\\n    \\n    :param app: The `~flask.Flask` instance to get the configuration from.\\n    :param upload_sets: The `UploadSet` instances to configure.\\n    \"\n    if isinstance(upload_sets, UploadSet):\n        upload_sets = (upload_sets,)\n    if (not hasattr(app, 'upload_set_config')):\n        app.upload_set_config = {\n            \n        }\n    set_config = app.upload_set_config\n    defaults = dict(dest=app.config.get('UPLOADS_DEFAULT_DEST'), url=app.config.get('UPLOADS_DEFAULT_URL'))\n    for uset in upload_sets:\n        config = config_for_set(uset, app, defaults)\n        set_config[uset.name] = config\n    should_serve = any(((s.base_url is None) for s in set_config.itervalues()))\n    if using_blueprints:\n        if (('_uploads' not in app.blueprints) and should_serve):\n            app.register_blueprint(uploads_mod)\n    elif (('_uploads' not in app.modules) and should_serve):\n        app.register_module(uploads_mod)\n", "label": 1}
{"function": "\n\ndef configure_host(self):\n    if self.mail.use_ssl:\n        host = smtplib.SMTP_SSL(self.mail.server, self.mail.port)\n    else:\n        host = smtplib.SMTP(self.mail.server, self.mail.port)\n    host.set_debuglevel(int(self.mail.debug))\n    if self.mail.use_tls:\n        host.starttls()\n    if (self.mail.username and self.mail.password):\n        host.login(self.mail.username, self.mail.password)\n    return host\n", "label": 0}
{"function": "\n\ndef host_to_ip(host):\n    '\\n    Returns the IP address of a given hostname\\n    '\n    try:\n        (family, socktype, proto, canonname, sockaddr) = socket.getaddrinfo(host, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0]\n        if (family == socket.AF_INET):\n            (ip, port) = sockaddr\n        elif (family == socket.AF_INET6):\n            (ip, port, flow_info, scope_id) = sockaddr\n    except Exception:\n        ip = None\n    return ip\n", "label": 0}
{"function": "\n\ndef add_all_wordstarts_matching(self, lower_hits, query, max_hits_hint):\n    lower_query = query.lower()\n    if (lower_query in self.basenames_by_wordstarts):\n        for basename in self.basenames_by_wordstarts[lower_query]:\n            lower_hits.add(basename)\n            if (len(lower_hits) >= max_hits_hint):\n                return\n", "label": 0}
{"function": "\n\n@ComputedGraph.Function\ndef extractVectorDataAsPythonArray(self):\n    if (self.computedValueVector.vectorImplVal is None):\n        return None\n    if ((len(self.vectorDataIds) > 0) and (not self.isLoaded)):\n        return None\n    result = ComputedValueGateway.getGateway().extractVectorDataAsPythonArray(self.computedValueVector, self.lowIndex, self.highIndex)\n    if ((result is None) and (not self.vdmThinksIsLoaded())):\n        logging.info('CumulusClient: %s was marked loaded but returned None. reloading', self)\n        self.isLoaded = False\n        ComputedValueGateway.getGateway().reloadVector(self)\n    return result\n", "label": 0}
{"function": "\n\ndef parse_policy(policy):\n    ret = {\n        \n    }\n    ret['name'] = policy['name']\n    ret['type'] = policy['type']\n    attrs = policy['Attributes']\n    if (policy['type'] != 'SSLNegotiationPolicyType'):\n        return ret\n    ret['sslv2'] = bool(attrs.get('Protocol-SSLv2'))\n    ret['sslv3'] = bool(attrs.get('Protocol-SSLv3'))\n    ret['tlsv1'] = bool(attrs.get('Protocol-TLSv1'))\n    ret['tlsv1_1'] = bool(attrs.get('Protocol-TLSv1.1'))\n    ret['tlsv1_2'] = bool(attrs.get('Protocol-TLSv1.2'))\n    ret['server_defined_cipher_order'] = bool(attrs.get('Server-Defined-Cipher-Order'))\n    ret['reference_security_policy'] = attrs.get('Reference-Security-Policy', None)\n    non_ciphers = ['Server-Defined-Cipher-Order', 'Protocol-SSLv2', 'Protocol-SSLv3', 'Protocol-TLSv1', 'Protocol-TLSv1.1', 'Protocol-TLSv1.2', 'Reference-Security-Policy']\n    ciphers = []\n    for cipher in attrs:\n        if (attrs[cipher] and (cipher not in non_ciphers)):\n            ciphers.append(cipher)\n    ciphers.sort()\n    ret['supported_ciphers'] = ciphers\n    return ret\n", "label": 0}
{"function": "\n\ndef conceptsUsed(self):\n    conceptsUsed = set((f.qname for f in self.modelXbrl.factsInInstance))\n    for cntx in self.modelXbrl.contexts.values():\n        for dim in cntx.qnameDims.values():\n            conceptsUsed.add(dim.dimensionQname)\n            if dim.isExplicit:\n                conceptsUsed.add(dim.memberQname)\n            else:\n                conceptsUsed.add(dim.typedMember.qname)\n    for (defaultDim, defaultDimMember) in self.modelXbrl.qnameDimensionDefaults.items():\n        conceptsUsed.add(defaultDim)\n        conceptsUsed.add(defaultDimMember)\n    for roleTypes in (self.modelXbrl.roleTypes, self.modelXbrl.arcroleTypes):\n        for modelRoleTypes in roleTypes.values():\n            for modelRoleType in modelRoleTypes:\n                for qn in modelRoleType.usedOns:\n                    conceptsUsed.add(qn)\n    for relationshipSetKey in self.relationshipSets:\n        relationshipSet = self.modelXbrl.relationshipSet(*relationshipSetKey)\n        for rel in relationshipSet.modelRelationships:\n            if isinstance(rel.fromModelObject, ModelConcept):\n                conceptsUsed.add(rel.fromModelObject)\n            if isinstance(rel.toModelObject, ModelConcept):\n                conceptsUsed.add(rel.toModelObject)\n    for qn in (XbrlConst.qnXbrliIdentifier, XbrlConst.qnXbrliPeriod, XbrlConst.qnXbrliUnit):\n        conceptsUsed.add(qn)\n    conceptsUsed -= {None}\n    return conceptsUsed\n", "label": 1}
{"function": "\n\ndef test_gzip():\n    res = app.get('/', extra_environ=dict(HTTP_ACCEPT_ENCODING='gzip'))\n    assert (int(res.header('content-length')) == len(res.body))\n    assert (res.body != b'this is a test')\n    actual = gzip.GzipFile(fileobj=six.BytesIO(res.body)).read()\n    assert (actual == b'this is a test')\n", "label": 0}
{"function": "\n\ndef make_url(base, filename, rev):\n    'Helper to construct the URL to fetch.\\n\\n  Args:\\n    base: The base property of the Issue to which the Patch belongs.\\n    filename: The filename property of the Patch instance.\\n    rev: Revision number, or None for head revision.\\n\\n  Returns:\\n    A URL referring to the given revision of the file.\\n  '\n    (scheme, netloc, path, _, _, _) = urlparse.urlparse(base)\n    if netloc.endswith('.googlecode.com'):\n        if (rev is None):\n            raise FetchError(\"Can't access googlecode.com without a revision\")\n        if (not path.startswith('/svn/')):\n            raise FetchError(('Malformed googlecode.com URL (%s)' % base))\n        path = path[5:]\n        url = ('%s://%s/svn-history/r%d/%s/%s' % (scheme, netloc, rev, path, filename))\n        return url\n    elif (netloc.endswith('sourceforge.net') and (rev is not None)):\n        if path.strip().endswith('/'):\n            path = path.strip()[:(- 1)]\n        else:\n            path = path.strip()\n        splitted_path = path.split('/')\n        url = ('%s://%s/%s/!svn/bc/%d/%s/%s' % (scheme, netloc, '/'.join(splitted_path[1:3]), rev, '/'.join(splitted_path[3:]), filename))\n        return url\n    url = base\n    if (not url.endswith('/')):\n        url += '/'\n    url += filename\n    if (rev is not None):\n        url += ('?rev=%s' % rev)\n    return url\n", "label": 0}
{"function": "\n\ndef visit_binop(self, obj):\n    lhs = obj.lhs.accept(self)\n    op = obj.op\n    rhs = obj.rhs.accept(self)\n    if (op == '+'):\n        return (lhs + rhs)\n    elif (op == '-'):\n        return (lhs - rhs)\n    elif (op == '*'):\n        return (lhs * rhs)\n    elif (op == '/'):\n        return (lhs / rhs)\n    else:\n        raise ValueError('invalid op', op)\n", "label": 0}
{"function": "\n\ndef StartTransform(self):\n    'Starts CSV transformation on Hadoop cluster.'\n    self._LoadMapper()\n    gcs_dir = self.config['hadoopTmpDir']\n    hadoop_input_filename = ('%s/inputs/input.csv' % gcs_dir)\n    logging.info('Starting Hadoop transform from %s to %s', self.config['sources'][0], self.config['sinks'][0])\n    logging.debug('Hadoop input file: %s', hadoop_input_filename)\n    output_file = self.cloud_storage_client.OpenObject(self.config['sinks'][0], mode='w')\n    input_file = self.cloud_storage_client.OpenObject(self.config['sources'][0])\n    hadoop_input = self.cloud_storage_client.OpenObject(hadoop_input_filename, mode='w')\n    line_count = 0\n    for line in input_file:\n        if (line_count < self.config['skipLeadingRows']):\n            output_file.write(line)\n        else:\n            hadoop_input.write(line)\n        line_count += 1\n    hadoop_input.close()\n    input_file.close()\n    mapreduce_id = self._StartHadoopMapReduce(gcs_dir)\n    self._WaitForMapReduce(mapreduce_id)\n    (bucket, hadoop_dir) = gcs.Gcs.UrlToBucketAndName(gcs_dir)\n    tab_strip_pattern = re.compile('\\t\\r?\\n')\n    for hadoop_result in self.cloud_storage_client.ListBucket(('/%s' % bucket), prefix=('%s/outputs/part-' % hadoop_dir)):\n        logging.debug('Hadoop result file: %s', hadoop_result)\n        hadoop_output = self.cloud_storage_client.OpenObject(hadoop_result)\n        for line in hadoop_output:\n            output_file.write(tab_strip_pattern.sub('\\n', line))\n    output_file.close()\n", "label": 0}
{"function": "\n\ndef file_upload_view_verify(request):\n    '\\n    Use the sha digest hash to verify the uploaded contents.\\n    '\n    form_data = request.POST.copy()\n    form_data.update(request.FILES)\n    for (key, value) in form_data.items():\n        if key.endswith('_hash'):\n            continue\n        if ((key + '_hash') not in form_data):\n            continue\n        submitted_hash = form_data[(key + '_hash')]\n        if isinstance(value, UploadedFile):\n            new_hash = hashlib.sha1(value.read()).hexdigest()\n        else:\n            new_hash = hashlib.sha1(force_bytes(value)).hexdigest()\n        if (new_hash != submitted_hash):\n            return HttpResponseServerError()\n    largefile = request.FILES['file_field2']\n    obj = FileModel()\n    obj.testfile.save(largefile.name, largefile)\n    return HttpResponse('')\n", "label": 0}
{"function": "\n\ndef _limit(self, uri, comment):\n    rv = self.db.execute(['SELECT id FROM comments WHERE remote_addr = ? AND ? - created < 60;'], (comment['remote_addr'], time.time())).fetchall()\n    if (len(rv) >= self.conf.getint('ratelimit')):\n        return (False, '{0}: ratelimit exceeded ({1})'.format(comment['remote_addr'], ', '.join(Guard.ids(rv))))\n    if (comment['parent'] is None):\n        rv = self.db.execute(['SELECT id FROM comments WHERE', '    tid = (SELECT id FROM threads WHERE uri = ?)', 'AND remote_addr = ?', 'AND parent IS NULL;'], (uri, comment['remote_addr'])).fetchall()\n        if (len(rv) >= self.conf.getint('direct-reply')):\n            return (False, ('%i direct responses to %s' % (len(rv), uri)))\n    elif (self.conf.getboolean('reply-to-self') == False):\n        rv = self.db.execute(['SELECT id FROM comments WHERE    remote_addr = ?', 'AND id = ?', 'AND ? - created < ?'], (comment['remote_addr'], comment['parent'], time.time(), self.max_age)).fetchall()\n        if (len(rv) > 0):\n            return (False, 'edit time frame is still open')\n    if (self.conf.getboolean('require-email') and (not comment.get('email'))):\n        return (False, 'email address required but not provided')\n    return (True, '')\n", "label": 0}
{"function": "\n\ndef execute(self, content):\n    command = self.get_link_command()\n    if (not command):\n        sublime.error_message('Could not get link opener command.\\nPlatform not yet supported.')\n        return None\n    if (sys.version_info[0] < 3):\n        content = content.encode(sys.getfilesystemencoding())\n    cmd = (command + [content])\n    arg_list_wrapper = self.settings.get('orgmode.open_link.resolver.abstract.arg_list_wrapper', [])\n    if arg_list_wrapper:\n        cmd = (arg_list_wrapper + [' '.join(cmd)])\n        source_filename = (('\"' + self.view.file_name()) + '\"')\n        cmd += [source_filename]\n        if (sys.platform != 'win32'):\n            cmd += ['--origin', source_filename, '--quiet']\n    print('*****')\n    print(repr(content), content)\n    print(cmd)\n    sublime.status_message(('Executing: %s' % cmd))\n    if (sys.platform != 'win32'):\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    else:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    (stdout, stderr) = process.communicate()\n    if stdout:\n        stdout = str(stdout, sys.getfilesystemencoding())\n        sublime.status_message(stdout)\n    if stderr:\n        stderr = str(stderr, sys.getfilesystemencoding())\n        sublime.error_message(stderr)\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TSentryPrivilegeMap')\n    if (self.privilegeMap is not None):\n        oprot.writeFieldBegin('privilegeMap', TType.MAP, 1)\n        oprot.writeMapBegin(TType.STRING, TType.SET, len(self.privilegeMap))\n        for (kiter104, viter105) in self.privilegeMap.items():\n            oprot.writeString(kiter104)\n            oprot.writeSetBegin(TType.STRUCT, len(viter105))\n            for iter106 in viter105:\n                iter106.write(oprot)\n            oprot.writeSetEnd()\n        oprot.writeMapEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\n@classmethod\ndef handle_token(cls, parser, token):\n    bits = token.split_contents()\n    if ((len(bits) == 3) and (bits[1] == 'as')):\n        return cls(bits[2])\n    elif ((len(bits) == 4) and (bits[2] == 'as')):\n        return cls(bits[3], bits[1])\n    else:\n        raise template.TemplateSyntaxError((\"%r takes 'as var' or 'level as var'\" % bits[0]))\n", "label": 0}
{"function": "\n\ndef depack(self, args):\n    self.is_touch = True\n    self.sx = args['x']\n    self.sy = args['y']\n    self.profile = ['pos']\n    if (('size_w' in args) and ('size_h' in args)):\n        self.shape = ShapeRect()\n        self.shape.width = args['size_w']\n        self.shape.height = args['size_h']\n        self.profile.append('shape')\n    if ('pressure' in args):\n        self.pressure = args['pressure']\n        self.profile.append('pressure')\n    super(MTDMotionEvent, self).depack(args)\n", "label": 0}
{"function": "\n\ndef anno(self, node):\n    if ((node.type is None) and (not getattr(node, 'escapes', False))):\n        return\n    if isinstance(node.type, (types.function, types.Type)):\n        return\n    self.write(' [')\n    if (node.type is not None):\n        self.visit(node.type)\n    if ((node.type is not None) and getattr(node, 'escapes', False)):\n        self.write(':')\n    if getattr(node, 'escapes', False):\n        self.write('E')\n    self.write(']')\n", "label": 0}
{"function": "\n\ndef to_ctype(self, parakeet_type):\n    if isinstance(parakeet_type, (NoneT, ScalarT)):\n        return type_mappings.to_ctype(parakeet_type)\n    elif isinstance(parakeet_type, TupleT):\n        return self.struct_type_from_fields(parakeet_type.elt_types)\n    elif isinstance(parakeet_type, PtrT):\n        return self.ptr_struct_type(parakeet_type.elt_type)\n    elif isinstance(parakeet_type, ArrayT):\n        elt_t = parakeet_type.elt_type\n        rank = parakeet_type.rank\n        return self.array_struct_type(elt_t, rank)\n    elif isinstance(parakeet_type, SliceT):\n        return self.slice_struct_type()\n    elif isinstance(parakeet_type, ClosureT):\n        return self.struct_type_from_fields(parakeet_type.arg_types)\n    elif isinstance(parakeet_type, TypeValueT):\n        return 'int'\n    else:\n        assert False, (\"Don't know how to make C type for %s\" % parakeet_type)\n", "label": 0}
{"function": "\n\n@webob.dec.wsgify\ndef process_request(self, req):\n    if (req.path != self._path):\n        return None\n    results = [ext.obj.healthcheck(req.server_port) for ext in self._backends]\n    healthy = self._are_results_healthy(results)\n    if (req.method == 'HEAD'):\n        functor = self._make_head_response\n        status = self.HEAD_HEALTHY_TO_STATUS_CODES[healthy]\n    else:\n        status = self.HEALTHY_TO_STATUS_CODES[healthy]\n        accept_type = req.accept.best_match(self._accept_order)\n        if (not accept_type):\n            accept_type = self._default_accept\n        functor = self._accept_to_functor[accept_type]\n    (body, content_type) = functor(results, healthy)\n    return webob.response.Response(status=status, body=body, content_type=content_type)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    TestCase.__init__(self, *args, **kwargs)\n    for attr in [x for x in dir(self) if x.startswith('test')]:\n        meth = getattr(self, attr)\n\n        def test_(self):\n            try:\n                meth()\n            except psutil.AccessDenied:\n                pass\n        setattr(self, attr, types.MethodType(test_, self))\n", "label": 0}
{"function": "\n\ndef strip_units(self):\n    '\\n        Strips units from an xypoint structure.\\n\\n        Returns:\\n          A copy of the xypoint with no units\\n          The x-units\\n          the y-units\\n        '\n    xunits = (self.x.unit if isinstance(self.x, u.Quantity) else 1.0)\n    yunits = (self.y.unit if isinstance(self.y, u.Quantity) else 1.0)\n    x = (self.x.value if isinstance(self.x, u.Quantity) else self.x)\n    y = (self.y.value if isinstance(self.y, u.Quantity) else self.y)\n    err = (self.err.value if isinstance(self.err, u.Quantity) else self.err)\n    cont = (self.cont.value if isinstance(self.cont, u.Quantity) else self.cont)\n    return (xypoint(x=x, y=y, cont=cont, err=err), xunits, yunits)\n", "label": 0}
{"function": "\n\ndef map(self, path):\n    'Map `path` through the aliases.\\n\\n        `path` is checked against all of the patterns.  The first pattern to\\n        match is used to replace the root of the path with the result root.\\n        Only one pattern is ever used.  If no patterns match, `path` is\\n        returned unchanged.\\n\\n        The separator style in the result is made to match that of the result\\n        in the alias.\\n\\n        '\n    for (regex, result, pattern_sep, result_sep) in self.aliases:\n        m = regex.match(path)\n        if m:\n            new = path.replace(m.group(0), result)\n            if (pattern_sep != result_sep):\n                new = new.replace(pattern_sep, result_sep)\n            if self.locator:\n                new = self.locator.canonical_filename(new)\n            return new\n    return path\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _filter_pools_for_numa_cells(pools, numa_cells):\n    numa_cells = ([None] + [cell.id for cell in numa_cells])\n    return [pool for pool in pools if any((utils.pci_device_prop_match(pool, [{\n        'numa_node': cell,\n    }]) for cell in numa_cells))]\n", "label": 0}
{"function": "\n\ndef SaveTemporaryFile(fp):\n    'Store incoming database file in a temporary directory.'\n    loc = data_store.DB.Location()\n    if (not os.path.exists(loc)):\n        return False\n    if (not os.path.isdir(loc)):\n        return False\n    filecopy_len_str = fp.read(sutils.SIZE_PACKER.size)\n    filecopy_len = sutils.SIZE_PACKER.unpack(filecopy_len_str)[0]\n    filecopy = rdf_data_server.DataServerFileCopy(fp.read(filecopy_len))\n    rebdir = _CreateDirectory(loc, filecopy.rebalance_id)\n    filedir = utils.JoinPath(rebdir, filecopy.directory)\n    try:\n        os.makedirs(filedir)\n    except OSError:\n        pass\n    filepath = utils.JoinPath(filedir, filecopy.filename)\n    logging.info('Writing to file %s', filepath)\n    with open(filepath, 'wb') as wp:\n        decompressor = zlib.decompressobj()\n        while True:\n            block_len_str = fp.read(sutils.SIZE_PACKER.size)\n            block_len = sutils.SIZE_PACKER.unpack(block_len_str)[0]\n            if (not block_len):\n                break\n            block = fp.read(block_len)\n            to_decompress = (decompressor.unconsumed_tail + block)\n            while to_decompress:\n                decompressed = decompressor.decompress(to_decompress)\n                if decompressed:\n                    wp.write(decompressed)\n                    to_decompress = decompressor.unconsumed_tail\n                else:\n                    to_decompress = ''\n        remaining = decompressor.flush()\n        if remaining:\n            wp.write(remaining)\n    if (os.path.getsize(filepath) != filecopy.size):\n        logging.error('Size of file %s is not %d', filepath, filecopy.size)\n        return False\n    return True\n", "label": 1}
{"function": "\n\n@classmethod\ndef validate(cls, value):\n    if (not value):\n        return value\n    value = os.path.abspath(value)\n    dirname = os.path.dirname(value)\n    if os.path.isfile(value):\n        if (not os.access(value, os.W_OK)):\n            raise config_option.BadValue('You do not have write permissions')\n        if (not os.access(dirname, os.W_OK)):\n            raise config_option.BadValue(('You do not have write permissions for \"%s\"' % dirname))\n        return value\n    elif os.path.isdir(value):\n        raise config_option.BadValue(('\"%s\" is a directory' % value))\n    else:\n        if os.path.isdir(dirname):\n            if (not os.access(dirname, os.W_OK)):\n                raise config_option.BadValue(('You do not have write permissions for \"%s\"' % dirname))\n            return value\n        previous_dir = os.path.dirname(dirname)\n        if (not os.path.isdir(previous_dir)):\n            raise config_option.BadValue(('\"%s\" not found' % value))\n        if (not os.access(previous_dir, os.W_OK)):\n            raise config_option.BadValue(('You do not have write permissions for \"%s\"' % previous_dir))\n        return value\n", "label": 1}
{"function": "\n\ndef update(self, x1, x2, y):\n    self.phase = 'train'\n    for layer in self.layers:\n        x1 = layer.fprop(x1)\n    for layer in self.layers2:\n        x2 = layer.fprop(x2)\n    (grad1, grad2) = self.loss.grad(y, x1, x2)\n    layers = self.layers[self.bprop_until:]\n    for layer in reversed(layers[1:]):\n        grad1 = layer.bprop(grad1)\n    layers[0].bprop(grad1)\n    layers2 = self.layers2[self.bprop_until:]\n    for layer in reversed(layers2[1:]):\n        grad2 = layer.bprop(grad2)\n    layers2[0].bprop(grad2)\n    return self.loss.loss(y, x1, x2)\n", "label": 0}
{"function": "\n\ndef test_can_update_status_via_trigger_on_participant_balance(self):\n    self.db.run(\"UPDATE participants SET balance=10, status_of_1_0_payout='pending-application' WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 10)\n    assert (alice.status_of_1_0_payout == 'pending-application')\n    self.db.run(\"UPDATE participants SET balance=0 WHERE username='alice'\")\n    alice = Participant.from_username('alice')\n    assert (alice.balance == 0)\n    assert (alice.status_of_1_0_payout == 'completed')\n", "label": 0}
{"function": "\n\ndef scaffold_auto_joins(self):\n    '\\n            Return a list of joined tables by going through the\\n            displayed columns.\\n        '\n    if (not self.column_auto_select_related):\n        return []\n    relations = set()\n    for p in self._get_model_iterator():\n        if hasattr(p, 'direction'):\n            if (p.mapper.class_ == self.model):\n                continue\n            if (p.direction.name in ['MANYTOONE', 'MANYTOMANY']):\n                relations.add(p.key)\n    joined = []\n    for (prop, name) in self._list_columns:\n        if (prop in relations):\n            joined.append(getattr(self.model, prop))\n    return joined\n", "label": 0}
{"function": "\n\ndef _trending_for_month(metric=None):\n    this_month_date = month_for_date(datetime.date.today())\n    previous_month_date = get_previous_month(this_month_date)\n    previous_month_year_date = get_previous_year(this_month_date)\n    data = {\n        'month': 0,\n        'previous_month': 0,\n        'previous_month_year': 0,\n    }\n    try:\n        month = MetricMonth.objects.get(metric=metric, created=this_month_date)\n        data['month'] = month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)\n        data['previous_month'] = previous_month.num\n    except ObjectDoesNotExist:\n        pass\n    try:\n        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)\n        data['previous_month_year'] = previous_month_year.num\n    except ObjectDoesNotExist:\n        pass\n    return data\n", "label": 0}
{"function": "\n\ndef onBrowserEvent(self, event):\n    etype = DOM.eventGetType(event)\n    if (etype == 'mousewheel'):\n        if self._mouseWheelPreventDefault:\n            DOM.eventPreventDefault(event)\n        velocity = DOM.eventGetMouseWheelVelocityY(event)\n        for listener in self._mouseWheelListeners:\n            listener.onMouseWheel(self, velocity)\n        return True\n", "label": 0}
{"function": "\n\ndef raw_field_definition_proxy_post_save(sender, instance, raw, **kwargs):\n    \"\\n    When proxy field definitions are loaded from a fixture they're not\\n    passing through the `field_definition_post_save` signal. Make sure they\\n    are.\\n    \"\n    if raw:\n        model_class = instance.content_type.model_class()\n        opts = model_class._meta\n        if (opts.proxy and (opts.concrete_model is sender)):\n            field_definition_post_save(sender=model_class, instance=instance.type_cast(), raw=raw, **kwargs)\n", "label": 0}
{"function": "\n\ndef test_moves_a_block_up_within_a_container(self):\n    for (idx, pos) in [(0, 0), (1, 1), (2, 2)]:\n        block = TextBlock.objects.get(id=self.left_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n    self.app.put(reverse('fp-api:block-move', kwargs={\n        'uuid': self.main_blocks[1].uuid,\n    }), params={\n        'container': self.left_container.uuid,\n        'index': 1,\n    }, user=self.user)\n    moved_block = TextBlock.objects.get(id=self.main_blocks[1].id)\n    self.assertEquals(moved_block.container, self.page.get_container_from_name('left-container'))\n    self.assertEquals(moved_block.display_order, 1)\n    for (idx, pos) in [(0, 0), (1, 2), (2, 3)]:\n        block = TextBlock.objects.get(id=self.left_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n    for (idx, pos) in [(0, 0), (2, 1)]:\n        block = TextBlock.objects.get(id=self.main_blocks[idx].id)\n        self.assertEquals(block.display_order, pos)\n", "label": 0}
{"function": "\n\ndef test_asizer_limit(self):\n    'Test limit setting for Asizer.\\n        '\n    objs = [Foo(42), ThinFoo('spam'), OldFoo(67)]\n    sizer = [asizeof.Asizer() for _ in range(4)]\n    for (limit, asizer) in enumerate(sizer):\n        asizer.asizeof(objs, limit=limit)\n    limit_sizes = [asizer.total for asizer in sizer]\n    self.assertTrue((limit_sizes[0] < limit_sizes[1]), limit_sizes)\n    self.assertTrue((limit_sizes[1] < limit_sizes[2]), limit_sizes)\n    self.assertTrue((limit_sizes[2] < limit_sizes[3]), limit_sizes)\n", "label": 0}
{"function": "\n\ndef decode(self, file):\n    fStart = file.tell()\n    identifier = None\n    try:\n        identifier = self.iEIEncoder.decode(file)\n    except UDHInformationElementIdentifierUnknownError:\n        pass\n    length = self.int8Encoder.decode(file)\n    data = None\n    if (identifier in self.dataEncoders):\n        data = self.dataEncoders[identifier].decode(file)\n    elif (length > 0):\n        data = self.read(file, length)\n    parsed = (file.tell() - fStart)\n    if (parsed != (length + 2)):\n        raise UDHParseError(('Invalid length: expected %d, parsed %d' % ((length + 2), parsed)))\n    if (identifier is None):\n        return None\n    return gsm_types.InformationElement(identifier, data)\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super().clean()\n    actions = Action.objects.in_bulk(cleaned_data['actions'])\n    attachment_counter = 0\n    one_action = False\n    one_action_name = ''\n    actions_items = actions.items()\n    for (k, v) in actions_items:\n        action = getattr(self.model, v.name)\n        if getattr(action, 'return_attachment', False):\n            attachment_counter += 1\n        if getattr(action, 'only_one_action', False):\n            one_action = True\n            one_action_name = getattr(action, 'verbose_name', '')\n    if (attachment_counter > 1):\n        msg = _('Please select at most one action which return attachment.')\n        self.add_error('actions', msg)\n    if (one_action and (len(actions_items) > 1)):\n        msg = (_('You have chosen action: %(name)s can only be selected for transition') % {\n            'name': one_action_name,\n        })\n        self.add_error('actions', msg)\n    return cleaned_data\n", "label": 0}
{"function": "\n\ndef _collapse(intervals):\n    '\\n    Collapse an iterable of intervals sorted by start coord.\\n    \\n    '\n    span = None\n    for (start, stop) in intervals:\n        if (span is None):\n            span = _Interval(start, stop)\n        elif (start <= span.stop < stop):\n            span = _Interval(span.start, stop)\n        elif (start > span.stop):\n            (yield span)\n            span = _Interval(start, stop)\n    if (span is not None):\n        (yield span)\n", "label": 0}
{"function": "\n\ndef claim_invitations(user):\n    \"Claims any pending invitations for the given user's email address.\"\n    invitation_user_id = ('%s:%s' % (models.User.EMAIL_INVITATION, user.email_address))\n    invitation_user = models.User.query.get(invitation_user_id)\n    if invitation_user:\n        invited_build_list = list(invitation_user.builds)\n        if (not invited_build_list):\n            return\n        db.session.add(user)\n        logging.debug('Found %d build admin invitations for id=%r, user=%r', len(invited_build_list), invitation_user_id, user)\n        for build in invited_build_list:\n            build.owners.remove(invitation_user)\n            if (not build.is_owned_by(user.id)):\n                build.owners.append(user)\n                logging.debug('Claiming invitation for build_id=%r', build.id)\n                save_admin_log(build, invite_accepted=True)\n            else:\n                logging.debug('User already owner of build. id=%r, build_id=%r', user.id, build.id)\n            db.session.add(build)\n        db.session.delete(invitation_user)\n        db.session.commit()\n        db.session.add(current_user)\n", "label": 0}
{"function": "\n\ndef process_exception(self, request, exception):\n    if (settings.DEBUG or isinstance(exception, Http404)):\n        return None\n    if isinstance(exception, apiproxy_errors.CapabilityDisabledError):\n        msg = 'Rietveld: App Engine is undergoing maintenance. Please try again in a while.'\n        status = 503\n    elif isinstance(exception, (DeadlineExceededError, MemoryError)):\n        msg = 'Rietveld is too hungry at the moment.Please try again in a while.'\n        status = 503\n    else:\n        msg = 'Unhandled exception.'\n        status = 500\n    logging.exception(('%s: ' % exception.__class__.__name__))\n    technical = ('%s [%s]' % (exception, exception.__class__.__name__))\n    if self._text_requested(request):\n        content = ('%s\\n\\n%s\\n' % (msg, technical))\n        content_type = 'text/plain'\n    else:\n        tpl = loader.get_template('exception.html')\n        ctx = Context({\n            'msg': msg,\n            'technical': technical,\n        })\n        content = tpl.render(ctx)\n        content_type = 'text/html'\n    return HttpResponse(content, status=status, content_type=content_type)\n", "label": 0}
{"function": "\n\ndef get_episode(self, url, imdb, tvdb, title, date, season, episode):\n    try:\n        if (url == None):\n            return\n        url = urlparse.urljoin(self.base_link, url)\n        (season, episode) = (('%01d' % int(season)), ('%01d' % int(episode)))\n        result = client.source(url)\n        if (not (season == '1')):\n            url = client.parseDOM(result, 'a', ret='href', attrs={\n                'class': 'season-.+?',\n            })\n            url = [i for i in url if (('/%s-sezon-' % season) in i)][0]\n            result = client.source(url)\n        result = client.parseDOM(result, 'a', ret='href')\n        result = [i for i in result if (('%s-sezon-%s-bolum-' % (season, episode)) in i)][0]\n        try:\n            url = re.compile('//.+?(/.+)').findall(result)[0]\n        except:\n            url = result\n        url = client.replaceHTMLCodes(url)\n        url = url.encode('utf-8')\n        return url\n    except:\n        return\n", "label": 0}
{"function": "\n\ndef _handle_object_info_reply(self, rep):\n    ' Handle replies for call tips.\\n        '\n    cursor = self._get_cursor()\n    info = self._request_info.get('call_tip')\n    if (info and (info.id == rep['parent_header']['msg_id']) and (info.pos == cursor.position())):\n        content = rep['content']\n        if content.get('ismagic', False):\n            (call_info, doc) = (None, None)\n        else:\n            (call_info, doc) = call_tip(content, format_call=True)\n        if (call_info or doc):\n            self._call_tip_widget.show_call_info(call_info, doc)\n", "label": 0}
{"function": "\n\ndef test_radius_neighbors_classifier_when_no_neighbors():\n    X = np.array([[1.0, 1.0], [2.0, 2.0]])\n    y = np.array([1, 2])\n    radius = 0.1\n    z1 = np.array([[1.01, 1.01], [2.01, 2.01]])\n    z2 = np.array([[1.01, 1.01], [1.4, 1.4]])\n    weight_func = _weight_func\n    for outlier_label in [0, (- 1), None]:\n        for algorithm in ALGORITHMS:\n            for weights in ['uniform', 'distance', weight_func]:\n                rnc = neighbors.RadiusNeighborsClassifier\n                clf = rnc(radius=radius, weights=weights, algorithm=algorithm, outlier_label=outlier_label)\n                clf.fit(X, y)\n                assert_array_equal(np.array([1, 2]), clf.predict(z1))\n                if (outlier_label is None):\n                    assert_raises(ValueError, clf.predict, z2)\n                elif False:\n                    assert_array_equal(np.array([1, outlier_label]), clf.predict(z2))\n", "label": 0}
{"function": "\n\ndef OutputPartial(self, out):\n    if self.has_package_:\n        out.putVarInt32(10)\n        out.putPrefixedString(self.package_)\n    for i in xrange(len(self.capability_)):\n        out.putVarInt32(18)\n        out.putPrefixedString(self.capability_[i])\n    for i in xrange(len(self.call_)):\n        out.putVarInt32(26)\n        out.putPrefixedString(self.call_[i])\n", "label": 0}
{"function": "\n\ndef has_valid_checksum(self, number):\n    (given_number, given_checksum) = (number[:(- 1)], number[(- 1)])\n    calculated_checksum = 0\n    fragment = ''\n    parameter = 7\n    for i in range(len(given_number)):\n        fragment = str((int(given_number[i]) * parameter))\n        if fragment.isalnum():\n            calculated_checksum += int(fragment[(- 1)])\n        if (parameter == 1):\n            parameter = 7\n        elif (parameter == 3):\n            parameter = 1\n        elif (parameter == 7):\n            parameter = 3\n    return (str(calculated_checksum)[(- 1)] == given_checksum)\n", "label": 0}
{"function": "\n\ndef listen(self):\n    'Listen to incoming clients until\\n        self._running is set to False\\n        '\n    l = self.listener\n    self._running = True\n    try:\n        while self._running:\n            log.debug('Accept connection')\n            c = l.accept()\n            try:\n                action = c.recv()\n            except EOFError:\n                c.close()\n                continue\n            if isinstance(action, basestring):\n                args = ()\n                kwargs = {\n                    \n                }\n            else:\n                args = action.get('args', ())\n                kwargs = (action.get('kwargs') or {\n                    \n                })\n                action = action.get('action')\n            log.info(('Dispatch action \"%s\"' % action))\n            method = getattr(self, ('dispatch_%s' % action), None)\n            if method:\n                try:\n                    result = method(*args, **kwargs)\n                except Exception as err:\n                    log.exception(err)\n                    c.send({\n                        'error': True,\n                        'message': ('Exception in action %s - %s' % (action, err)),\n                    })\n                else:\n                    c.send({\n                        'error': False,\n                        'message': 'ok',\n                        'result': result,\n                    })\n            else:\n                log.warn(('No action %s' % action))\n                c.send({\n                    'error': True,\n                    'message': ('No action %s' % action),\n                })\n            c.close()\n    finally:\n        self._listener = None\n        l.close()\n    log.info('Exiting event loop')\n", "label": 0}
{"function": "\n\ndef convert_fragment(self, fragment, fd):\n    mdat = None\n    try:\n        f4v = F4V(fd, raw_payload=True)\n        for box in f4v:\n            if (box.type == 'mdat'):\n                mdat = box.payload.data\n                break\n    except F4VError as err:\n        self.logger.error('Failed to parse fragment {0}-{1}: {2}', fragment.segment, fragment.fragment, err)\n        return\n    if (not mdat):\n        self.logger.error('No MDAT box found in fragment {0}-{1}', fragment.segment, fragment.fragment)\n        return\n    try:\n        for chunk in self.concater.iter_chunks(buf=mdat, skip_header=True):\n            self.reader.buffer.write(chunk)\n            if self.closed:\n                break\n        else:\n            self.logger.debug('Download of fragment {0}-{1} complete', fragment.segment, fragment.fragment)\n    except IOError as err:\n        if ('Unknown tag type' in str(err)):\n            self.logger.error('Unknown tag type found, this stream is probably encrypted')\n            self.close()\n            return\n        self.logger.error('Error reading fragment {0}-{1}: {2}', fragment.segment, fragment.fragment, err)\n", "label": 1}
{"function": "\n\ndef _wait_async_done(self, reservation_id, reqids):\n    \"\\n        _wait_async_done(session_id, reqids)\\n        Helper methods that waits for the specified asynchronous requests to be finished,\\n        and which asserts that they were successful. Note that it doesn't actually return\\n        their responses.\\n        @param reqids Tuple containing the request ids for the commands to check.\\n        @return Nothing\\n        \"\n    reqsl = list(reqids)\n    max_count = 15\n    while (len(reqsl) > 0):\n        time.sleep(0.1)\n        max_count -= 1\n        if (max_count == 0):\n            raise Exception('Maximum time spent waiting async done')\n        requests = self.client.check_async_command_status(reservation_id, tuple(reqsl))\n        self.assertEquals(len(reqsl), len(requests))\n        for (rid, req) in six.iteritems(requests):\n            status = req[0]\n            self.assertTrue((status in ('running', 'ok', 'error')))\n            if (status != 'running'):\n                self.assertEquals('ok', status, ('Contents: ' + req[1]))\n                reqsl.remove(rid)\n", "label": 0}
{"function": "\n\ndef test_os_stat(self):\n    'Test sizing os.stat and os.statvfs objects.\\n        '\n    try:\n        stat = os.stat(__file__)\n    except Exception:\n        pass\n    else:\n        stat_size = asizeof.asizeof(stat)\n        self.assertTrue((stat_size > 0), stat_size)\n        refs = asizeof.named_refs(stat)\n        ref_names = set([name for (name, _) in refs])\n        self.assertTrue((set(['st_mode', 'st_size', 'st_mtime']) <= ref_names), ref_names)\n    try:\n        stat = os.statvfs(__file__)\n    except Exception:\n        pass\n    else:\n        stat_size = asizeof.asizeof(stat)\n        self.assertTrue((stat_size > 0), stat_size)\n        refs = asizeof.named_refs(stat)\n        ref_names = set([name for (name, _) in refs])\n        self.assertTrue((set(['f_bsize', 'f_blocks']) <= ref_names), ref_names)\n", "label": 0}
{"function": "\n\ndef list_names(self, **kwargs):\n    'Get a list of metric names.'\n    url_str = (self.base_url + '/names')\n    newheaders = self.get_headers()\n    if ('dimensions' in kwargs):\n        dimstr = self.get_dimensions_url_string(kwargs['dimensions'])\n        kwargs['dimensions'] = dimstr\n    if kwargs:\n        url_str = (url_str + ('?%s' % urlutils.urlencode(kwargs, True)))\n    (resp, body) = self.client.json_request('GET', url_str, headers=newheaders)\n    return (body['elements'] if (type(body) is dict) else body)\n", "label": 0}
{"function": "\n\ndef mayRaiseException(self, exception_type):\n    if self.tolerant:\n        return False\n    else:\n        if (self.variable_trace is not None):\n            variable = self.getTargetVariableRef().getVariable()\n            if variable.isTempVariable():\n                return False\n            if ((self.previous_trace is not None) and self.previous_trace.mustHaveValue()):\n                return False\n        return True\n", "label": 0}
{"function": "\n\n@ConnectorExist(cid_key='smppc')\ndef smppc(self, arg, opts):\n    sc = SMPPClientStatsCollector()\n    headers = ['#Item', 'Value']\n    table = []\n    for (k, v) in sc.get(opts.smppc).getStats().iteritems():\n        if isinstance(v, dict):\n            v = json.dumps(v)\n        row = []\n        row.append(('#%s' % k))\n        if (k[(- 3):] == '_at'):\n            row.append(formatDateTime(v))\n        else:\n            row.append(v)\n        table.append(row)\n    self.protocol.sendData(tabulate(table, headers, tablefmt='plain', numalign='left').encode('ascii'))\n", "label": 0}
{"function": "\n\n@property\ndef event_description(self):\n    'complete description of this event in text form\\n\\n        :rtype: str\\n        :returns: event description\\n        '\n    location = (('\\nLocation: ' + self.location) if (self.location != '') else '')\n    description = (('\\nDescription: ' + self.description) if (self.description != '') else '')\n    repitition = (('\\nRepeat: ' + self.recurpattern) if (self.recurpattern != '') else '')\n    return '{}: {}{}{}{}'.format(self._rangestr, self.summary, location, repitition, description)\n", "label": 0}
{"function": "\n\ndef create(self):\n    ' Insert Action '\n    token = request.POST.pop('__token', '')\n    if (not self._is_token_match(token)):\n        success = False\n        error_message = 'Invalid Token'\n        data = None\n    else:\n        data = self.__model__()\n        data.set_state_insert()\n        data.assign_from_dict(request.POST)\n        data.save()\n        success = data.success\n        error_message = data.error_message\n    self._setup_view_parameter()\n    self._set_view_parameter(self.__model_name__, data)\n    self._set_view_parameter('success', success)\n    self._set_view_parameter('error_message', error_message)\n    if (request.is_xhr or request.POST.pop('__as_json', False)):\n        if success:\n            token = self._set_token()\n        self._set_view_parameter('__token', token)\n        return self._get_view_parameter_as_json()\n    return self._load_view('create')\n", "label": 0}
{"function": "\n\ndef is_editable(proposal, user):\n    return ((not proposal.scheduled) and (((proposal.proposer == user) and (proposal.status != 'A')) or topiclead(user, proposal.topic)))\n", "label": 0}
{"function": "\n\ndef process_get_results_metadata(self, seqid, iprot, oprot):\n    args = get_results_metadata_args()\n    args.read(iprot)\n    iprot.readMessageEnd()\n    result = get_results_metadata_result()\n    try:\n        result.success = self._handler.get_results_metadata(args.handle)\n        msg_type = TMessageType.REPLY\n    except (TTransport.TTransportException, KeyboardInterrupt, SystemExit):\n        raise\n    except QueryNotFoundException as error:\n        msg_type = TMessageType.REPLY\n        result.error = error\n    except Exception as ex:\n        msg_type = TMessageType.EXCEPTION\n        logging.exception(ex)\n        result = TApplicationException(TApplicationException.INTERNAL_ERROR, 'Internal error')\n    oprot.writeMessageBegin('get_results_metadata', msg_type, seqid)\n    result.write(oprot)\n    oprot.writeMessageEnd()\n    oprot.trans.flush()\n", "label": 0}
{"function": "\n\ndef text(self, value=no_default):\n    \"Get or set the text representation of sub nodes.\\n\\n        Get the text value::\\n\\n            >>> doc = PyQuery('<div><span>toto</span><span>tata</span></div>')\\n            >>> print(doc.text())\\n            toto tata\\n\\n        Set the text value::\\n\\n            >>> doc.text('Youhou !')\\n            [<div>]\\n            >>> print(doc)\\n            <div>Youhou !</div>\\n\\n        \"\n    if (value is no_default):\n        if (not self):\n            return None\n        text = []\n\n        def add_text(tag, no_tail=False):\n            if tag.text:\n                text.append(tag.text)\n            for child in tag.getchildren():\n                add_text(child)\n            if ((not no_tail) and tag.tail):\n                text.append(tag.tail)\n        for tag in self:\n            add_text(tag, no_tail=True)\n        return ' '.join([t.strip() for t in text if t.strip()])\n    for tag in self:\n        for child in tag.getchildren():\n            tag.remove(child)\n        tag.text = value\n    return self\n", "label": 0}
{"function": "\n\ndef shutdown(self):\n    if self.stopped:\n        return\n    self.stopped = True\n    try:\n        for handle in self.map_handles.values():\n            if (handle is not None):\n                handle.close()\n        for handle in self.file_handles.values():\n            if (handle is not None):\n                handle.close()\n    finally:\n        with self.lock:\n            if (self.create_lock_file is True):\n                os.remove(self.lock_file)\n    self.inited = False\n", "label": 0}
{"function": "\n\ndef categories(self, fileids=None, patterns=None):\n    meta = self._get_meta()\n    fileids = make_iterable(fileids, meta.keys())\n    result = sorted(set((cat for cat in itertools.chain(*(meta[str(doc_id)].categories for doc_id in fileids)))))\n    if patterns:\n        patterns = make_iterable(patterns)\n        result = [cat for cat in result if some_items_match([cat], patterns)]\n    return result\n", "label": 0}
{"function": "\n\ndef backward_cpu(self, xs, gys):\n    assert (len(xs) == self.n_in)\n    assert (len(gys) == self.n_out)\n    return tuple((np.zeros_like(xs).astype(np.float32) for _ in six.moves.range(self.n_in)))\n", "label": 0}
{"function": "\n\ndef go_back(self):\n    isdir = is_dir(self.input)\n    input_stripped = self.input.rstrip(os.sep)\n    if (not input_stripped):\n        return\n    input_splitted = input_stripped.split(os.sep)\n    entry_name = input_splitted[(- 1)]\n    if isdir:\n        entry_name += os.sep\n    new_input = os.sep.join(input_splitted[0:(- 1)])\n    if new_input:\n        new_input += os.sep\n    self.set_input(new_input)\n    self.set_selected_entry(entry_name)\n", "label": 0}
{"function": "\n\ndef _iter_cursor_results(self):\n    col_names = [c[0] for c in self._cursor.description]\n    while 1:\n        row = self._cursor.fetchone()\n        if (row is None):\n            break\n        (yield self._make_row(row, col_names))\n", "label": 0}
{"function": "\n\ndef html_tag(self, template, logical_path, debug=False):\n    environment = self.get_environment(current_app)\n    if (debug or self.debug(current_app)):\n        asset = build_asset(environment, logical_path)\n        urls = []\n        for requirement in asset.requirements:\n            logical_path = requirement.attributes.logical_path\n            url = url_for('static', filename=logical_path, body=1)\n            urls.append(url)\n    else:\n        if (logical_path in environment.manifest.files):\n            logical_path = environment.manifest.files[logical_path]\n        urls = (url_for('static', filename=logical_path),)\n    return Markup('\\n'.join((template.format(url=url) for url in urls)))\n", "label": 0}
{"function": "\n\ndef _send_mail(self, handler, trap, is_duplicate):\n    if (is_duplicate and (not handler['mail_on_duplicate'])):\n        return\n    mail = handler['mail']\n    if (not mail):\n        return\n    recipients = handler['mail'].get('recipients')\n    if (not recipients):\n        return\n    subject = (handler['mail']['subject'] % {\n        'trap_oid': trap.oid,\n        'trap_name': ObjectId(trap.oid).name,\n        'ipaddress': trap.host,\n        'hostname': self.resolver.hostname_or_ip(trap.host),\n    })\n    ctxt = dict(trap=trap, dest_host=self.hostname)\n    try:\n        stats.incr('mail_sent_attempted', 1)\n        send_trap_email(recipients, 'trapperkeeper', subject, self.template_env, ctxt)\n        stats.incr('mail_sent_successful', 1)\n    except socket.error as err:\n        stats.incr('mail_sent_failed', 1)\n        logging.warning('Failed to send e-mail for trap: %s', err)\n", "label": 0}
{"function": "\n\ndef testConversion(self):\n    for (expected_value, binary) in self.tests:\n        binary_sid = ''.join([chr(x) for x in binary])\n        if (expected_value is None):\n            self.assertRaises(ValueError, wmi_parser.BinarySIDtoStringSID, binary_sid)\n        else:\n            self.assertEqual(wmi_parser.BinarySIDtoStringSID(binary_sid), expected_value)\n", "label": 0}
{"function": "\n\ndef astar_path_length(G, source, target, heuristic=None, weight='weight'):\n    'Return the length of the shortest path between source and target using\\n    the A* (\"A-star\") algorithm.\\n\\n    Parameters\\n    ----------\\n    G : NetworkX graph\\n\\n    source : node\\n       Starting node for path\\n\\n    target : node\\n       Ending node for path\\n\\n    heuristic : function\\n       A function to evaluate the estimate of the distance\\n       from the a node to the target.  The function takes\\n       two nodes arguments and must return a number.\\n\\n    Raises\\n    ------\\n    NetworkXNoPath\\n        If no path exists between source and target.\\n\\n    See Also\\n    --------\\n    astar_path\\n\\n    '\n    if ((source not in G) or (target not in G)):\n        msg = 'Either source {} or target {} is not in G'\n        raise nx.NodeNotFound(msg.format(source, target))\n    path = astar_path(G, source, target, heuristic, weight)\n    return sum((G[u][v].get(weight, 1) for (u, v) in zip(path[:(- 1)], path[1:])))\n", "label": 0}
{"function": "\n\ndef test_build_graph(self, huang_darwiche_nodes):\n    bbn = build_bbn(huang_darwiche_nodes)\n    nodes = dict([(node.name, node) for node in bbn.nodes])\n    assert (nodes['f_a'].parents == [])\n    assert (nodes['f_b'].parents == [nodes['f_a']])\n    assert (nodes['f_c'].parents == [nodes['f_a']])\n    assert (nodes['f_d'].parents == [nodes['f_b']])\n    assert (nodes['f_e'].parents == [nodes['f_c']])\n    assert (nodes['f_f'].parents == [nodes['f_d'], nodes['f_e']])\n    assert (nodes['f_g'].parents == [nodes['f_c']])\n    assert (nodes['f_h'].parents == [nodes['f_e'], nodes['f_g']])\n", "label": 1}
{"function": "\n\ndef serialize(self):\n    buf = bytearray(struct.pack(self._PACK_STR, self.type_, self.aux_len, self.num, addrconv.ipv6.text_to_bin(self.address)))\n    for src in self.srcs:\n        buf.extend(struct.pack('16s', addrconv.ipv6.text_to_bin(src)))\n    if (0 == self.num):\n        self.num = len(self.srcs)\n        struct.pack_into('!H', buf, 2, self.num)\n    if (self.aux is not None):\n        mod = (len(self.aux) % 4)\n        if mod:\n            self.aux += bytearray((4 - mod))\n            self.aux = six.binary_type(self.aux)\n        buf.extend(self.aux)\n        if (0 == self.aux_len):\n            self.aux_len = (len(self.aux) // 4)\n            struct.pack_into('!B', buf, 1, self.aux_len)\n    return six.binary_type(buf)\n", "label": 0}
{"function": "\n\ndef backup_reports(items):\n    if (not items):\n        return\n    KEEP_MAX_REPORTS = 100\n    tm = app.get_state_item('telemetry', {\n        \n    })\n    if ('backup' not in tm):\n        tm['backup'] = []\n    for params in items:\n        for key in params.keys():\n            if (key in ('v', 'tid', 'cid', 'cd1', 'cd2', 'sr', 'an')):\n                del params[key]\n        if ('qt' not in params):\n            params['qt'] = time()\n        elif (not isinstance(params['qt'], float)):\n            params['qt'] = (time() - (params['qt'] / 1000))\n        tm['backup'].append(params)\n    tm['backup'] = tm['backup'][(KEEP_MAX_REPORTS * (- 1)):]\n    app.set_state_item('telemetry', tm)\n", "label": 0}
{"function": "\n\ndef _bytecode_filenames(self, py_filenames):\n    bytecode_files = []\n    for py_file in py_filenames:\n        if (not py_file.endswith('.py')):\n            continue\n        if self.compile:\n            bytecode_files.append((py_file + 'c'))\n        if (self.optimize > 0):\n            bytecode_files.append((py_file + 'o'))\n    return bytecode_files\n", "label": 0}
{"function": "\n\ndef _faster_to_representation(self, instance):\n    'Modified to_representation with optimizations.\\n\\n        1) Returns a plain old dict as opposed to OrderedDict.\\n            (Constructing ordered dict is ~100x slower than `{}`.)\\n        2) Ensure we use a cached list of fields\\n            (this optimization exists in DRF 3.2 but not 3.1)\\n\\n        Arguments:\\n            instance: a model instance or data object\\n        Returns:\\n            Dict of primitive datatypes.\\n        '\n    ret = {\n        \n    }\n    fields = self._readable_fields\n    for field in fields:\n        try:\n            attribute = field.get_attribute(instance)\n        except SkipField:\n            continue\n        if (attribute is None):\n            ret[field.field_name] = None\n        else:\n            ret[field.field_name] = field.to_representation(attribute)\n    return ret\n", "label": 0}
{"function": "\n\ndef check_func(conf, func, libs=None):\n    if (libs is None):\n        libs = []\n    code = ('\\nchar %(func)s (void);\\n\\n#ifdef _MSC_VER\\n#pragma function(%(func)s)\\n#endif\\n\\nint main (void)\\n{\\n    return %(func)s();\\n}\\n' % {\n        'func': func,\n    })\n    if libs:\n        msg = ('Checking for function %s in %s' % (func, ' '.join([(conf.env['LIB_FMT'] % lib) for lib in libs])))\n    else:\n        msg = ('Checking for function %s' % func)\n    conf.start_message(msg)\n    old_lib = copy.deepcopy(conf.env['LIBS'])\n    try:\n        for lib in libs[::(- 1)]:\n            conf.env['LIBS'].insert(0, lib)\n        ret = conf.builders['ctasks'].try_program('check_func', code, None)\n        if ret:\n            conf.end_message('yes')\n        else:\n            conf.end_message('no !')\n    finally:\n        conf.env['LIBS'] = old_lib\n    conf.conf_results.append({\n        'type': 'func',\n        'value': func,\n        'result': ret,\n    })\n    return ret\n", "label": 0}
{"function": "\n\ndef unlink(self, name):\n    symlinks = self.read_bootstrap().get('symlinks', {\n        \n    })\n    removed_targets = set()\n    found = False\n    for (source_glob, target) in symlinks.items():\n        if self._islinkkey(source_glob, name):\n            found = True\n            for (source, target) in self.expandtargets(source_glob, target):\n                self._remove_link_target(source, target)\n                removed_targets.add(target)\n                shutil.move(source, target)\n                print(tty.progress('Moved {0} -> {1}'.format(collapseuser(source), collapseuser(target))))\n    if (not found):\n        raise StowError('No symlink found with name: {0}'.format(name))\n    try:\n        os.rmdir(os.path.join(self.symlink_dir, name))\n    except OSError as e:\n        if (e.errno != errno.ENOTEMPTY):\n            raise e\n    self.remove_symlink(name)\n    self._update_target_cache((set(self._cached_targets()) - removed_targets))\n", "label": 0}
{"function": "\n\ndef get_memory_amount(builder, installProfile, is_mandatory):\n    if (('hardwareSettings' in builder) and ('memory' in builder['hardwareSettings'])):\n        installProfile.memorySize = builder['hardwareSettings']['memory']\n        return installProfile\n    elif is_mandatory:\n        printer.out((('Error: no hardwareSettings part for builder [' + builder['type']) + ']'), printer.ERROR)\n        return 2\n    else:\n        return installProfile\n", "label": 0}
{"function": "\n\ndef _make_table(self, table, fields):\n    'Set up the schema of the database. `fields` is a mapping\\n        from field names to `Type`s. Columns are added if necessary.\\n        '\n    with self.transaction() as tx:\n        rows = tx.query(('PRAGMA table_info(%s)' % table))\n    current_fields = set([row[1] for row in rows])\n    field_names = set(fields.keys())\n    if current_fields.issuperset(field_names):\n        return\n    if (not current_fields):\n        columns = []\n        for (name, typ) in fields.items():\n            columns.append('{0} {1}'.format(name, typ.sql))\n        setup_sql = 'CREATE TABLE {0} ({1});\\n'.format(table, ', '.join(columns))\n    else:\n        setup_sql = ''\n        for (name, typ) in fields.items():\n            if (name in current_fields):\n                continue\n            setup_sql += 'ALTER TABLE {0} ADD COLUMN {1} {2};\\n'.format(table, name, typ.sql)\n    with self.transaction() as tx:\n        tx.script(setup_sql)\n", "label": 0}
{"function": "\n\ndef __init__(self, raw, buffer_size=DEFAULT_BUFFER_SIZE, max_buffer_size=None):\n    if (not raw.writable()):\n        raise IOError('\"raw\" argument must be writable.')\n    _BufferedIOMixin.__init__(self, raw)\n    if (buffer_size <= 0):\n        raise ValueError('invalid buffer size')\n    if (max_buffer_size is not None):\n        warnings.warn('max_buffer_size is deprecated', DeprecationWarning, self._warning_stack_offset)\n    self.buffer_size = buffer_size\n    self._write_buf = bytearray()\n    self._write_lock = Lock()\n    self._ok = True\n", "label": 0}
{"function": "\n\ndef AnnotateMethod(self, unused_the_api, method, unused_resource):\n    'Annotate a Method with Java Proto specific elements.\\n\\n    Args:\\n      unused_the_api: (Api) The API tree which owns this method.\\n      method: (Method) The method to annotate.\\n      unused_resource: (Resource) The resource which owns this method.\\n\\n    Raises:\\n      ValueError: if missing externalTypeName\\n    '\n    for attr in ('requestType', 'responseType'):\n        schema = method.get(attr)\n        if (schema and (not isinstance(schema, data_types.Void))):\n            name = schema.get('externalTypeName')\n            if (not name):\n                raise ValueError(('missing externalTypeName for %s (%s of method %s)' % (schema['id'], attr, method['rpcMethod'])))\n            java_name = schema.get('javaTypeName')\n            proto_name = (java_name or ('TO_BE_COMPUTED.' + name[(name.rfind('.') + 1):]))\n            schema.SetTemplateValue('protoFullClassName', proto_name)\n", "label": 0}
{"function": "\n\ndef _notification(self, context, method, routers, operation, shuffle_agents):\n    'Notify all or individual Cisco cfg agents.'\n    if utils.is_extension_supported(self._l3plugin, L3AGENT_SCHED):\n        adm_context = ((context.is_admin and context) or context.elevated())\n        self._l3plugin.schedule_routers(adm_context, routers)\n        self._agent_notification(context, method, routers, operation, shuffle_agents)\n    else:\n        cctxt = self.client.prepare(topics=topics.L3_AGENT, fanout=True)\n        cctxt.cast(context, method, routers=[r['id'] for r in routers])\n", "label": 0}
{"function": "\n\ndef validate(self):\n    'Validate arguments, raises UrlArgsValidationError if something is wrong'\n    args = self.request.args\n    if (len(args) == 0):\n        raise UrlArgsValidationError('Mandatory arguments not found, please refer to the HTTPAPI specifications.')\n    for arg in args:\n        if (arg not in self.fields):\n            raise UrlArgsValidationError(('Argument [%s] is unknown.' % arg))\n        for field in self.fields:\n            fieldData = self.fields[field]\n            if (field in args):\n                if (isinstance(args[field][0], int) or isinstance(args[field][0], float)):\n                    value = str(args[field][0])\n                else:\n                    value = args[field][0]\n                if (('pattern' in self.fields[field]) and (self.fields[field]['pattern'].match(value) is None)):\n                    raise UrlArgsValidationError(('Argument [%s] has an invalid value: [%s].' % (field, value)))\n            elif (not fieldData['optional']):\n                raise UrlArgsValidationError(('Mandatory argument [%s] is not found.' % field))\n    return True\n", "label": 1}
{"function": "\n\ndef _get_longname(self, obj):\n    names = [obj.name]\n    parent = obj.parent\n    while (parent is not None):\n        if (isinstance(parent, TestCaseFile) or isinstance(parent, TestDataDirectory) or isinstance(parent, TestCase) or isinstance(parent, UserKeyword)):\n            names.insert(0, parent.name)\n        parent = parent.parent\n    return '.'.join(names)\n", "label": 0}
{"function": "\n\ndef bootstrap_unicel_gateway(apps):\n    currency = (apps.get_model('accounting.Currency') if apps else Currency).objects.get(code='INR')\n    sms_gateway_fee_class = (apps.get_model('smsbillables.SmsGatewayFee') if apps else SmsGatewayFee)\n    sms_gateway_fee_criteria_class = (apps.get_model('smsbillables.SmsGatewayFeeCriteria') if apps else SmsGatewayFeeCriteria)\n    SmsGatewayFee.create_new(SQLUnicelBackend.get_api_id(), INCOMING, 0.5, currency=currency, fee_class=sms_gateway_fee_class, criteria_class=sms_gateway_fee_criteria_class)\n    SmsGatewayFee.create_new(SQLUnicelBackend.get_api_id(), OUTGOING, 0.5, currency=currency, fee_class=sms_gateway_fee_class, criteria_class=sms_gateway_fee_criteria_class)\n    log_smsbillables_info('Updated Unicel gateway fees.')\n", "label": 0}
{"function": "\n\ndef format_delta(delta):\n    days = (delta / ((60 * 60) * 24))\n    hours = (delta / (60 * 60))\n    mins = (delta / 60)\n    if (days == 1):\n        return ('%d day' % days)\n    elif (days > 1):\n        return ('%d days' % days)\n    elif (hours == 1):\n        return ('%d hour' % hours)\n    elif (hours > 1):\n        return ('%d hours' % hours)\n    elif (mins == 1):\n        return ('%d min' % mins)\n    elif (mins > 1):\n        return ('%d mins' % mins)\n    else:\n        return 'just now'\n", "label": 0}
{"function": "\n\ndef lt(x, y):\n    if ((x is None) and (y is not None)):\n        return True\n    elif ((x is not None) and (y is None)):\n        return False\n    else:\n        return (x < y)\n", "label": 0}
{"function": "\n\ndef print_row(data):\n    'print a single db row in chr and str\\n    '\n    index_line = ''\n    pri_line1 = ''\n    chr_line2 = ''\n    asci = re.compile('[a-zA-Z0-9 ]')\n    for (i, xi) in enumerate(data):\n        if (not (i % 5)):\n            diff = (len(pri_line1) - len(index_line))\n            i = str(i)\n            index_line += (diff * ' ')\n            index_line += i\n        str_v = str(xi)\n        pri_line1 += (str(xi) + ',')\n        c = chr(xi)\n        c = (c if asci.match(c) else ' ')\n        w = len(str_v)\n        c = ((c + ((w - 1) * ' ')) + ',')\n        chr_line2 += c\n    print(index_line)\n    print(pri_line1)\n    print(chr_line2)\n", "label": 0}
{"function": "\n\ndef start(self, environment=None, user=None):\n    'Mark this result started.'\n    envs = [environment]\n    try:\n        latest = self.results.get(is_latest=True, tester=user, environment=environment)\n        if (latest.status == Result.STATUS.skipped):\n            envs = self.environments.all()\n    except ObjectDoesNotExist:\n        pass\n    for env in envs:\n        Result.objects.create(runcaseversion=self, tester=user, environment=env, status=Result.STATUS.started, user=user)\n", "label": 0}
{"function": "\n\ndef _update_section_contents(self, contents, section_name, new_values):\n    new_values = new_values.copy()\n    section_start_line_num = self._find_section_start(contents, section_name)\n    last_matching_line = section_start_line_num\n    j = (last_matching_line + 1)\n    while (j < len(contents)):\n        line = contents[j]\n        if (self.SECTION_REGEX.search(line) is not None):\n            self._insert_new_values(line_number=last_matching_line, contents=contents, new_values=new_values)\n            return\n        match = self.OPTION_REGEX.search(line)\n        if (match is not None):\n            last_matching_line = j\n            key_name = match.group(1).strip()\n            if (key_name in new_values):\n                if (not isinstance(new_values[key_name], dict)):\n                    option_value = new_values[key_name]\n                    new_line = ('%s = %s\\n' % (key_name, option_value))\n                    contents[j] = new_line\n                    del new_values[key_name]\n                else:\n                    j = self._update_subattributes(j, contents, new_values[key_name], (len(match.group(1)) - len(match.group(1).lstrip())))\n                    return\n        j += 1\n    if new_values:\n        if (not contents[(- 1)].endswith('\\n')):\n            contents.append('\\n')\n        self._insert_new_values(line_number=(last_matching_line + 1), contents=contents, new_values=new_values)\n", "label": 0}
{"function": "\n\ndef _connect_to_upstream(self):\n    if self.proxy_server:\n        upstream_sock = socks.socksocket()\n        upstream_sock.set_proxy(**self.proxy_server)\n    else:\n        upstream_sock = socket.socket()\n    try:\n        upstream_sock.settimeout(self.proxy_timeout)\n        upstream_sock.connect(self.upstream)\n        if self.use_ssl:\n            upstream_sock = wrap_ssl(upstream_sock)\n    except:\n        drop_socket(upstream_sock)\n        raise\n    self.logger.info(('Connected to upstream %s:%d' % self.upstream))\n    return upstream_sock\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned_data = super(CreateForm, self).clean()\n    source_type = self.cleaned_data.get('volume_source_type')\n    if ((source_type == 'image_source') and (not cleaned_data.get('image_source'))):\n        msg = _('Image source must be specified')\n        self._errors['image_source'] = self.error_class([msg])\n    elif ((source_type == 'snapshot_source') and (not cleaned_data.get('snapshot_source'))):\n        msg = _('Snapshot source must be specified')\n        self._errors['snapshot_source'] = self.error_class([msg])\n    elif ((source_type == 'volume_source') and (not cleaned_data.get('volume_source'))):\n        msg = _('Volume source must be specified')\n        self._errors['volume_source'] = self.error_class([msg])\n    return cleaned_data\n", "label": 0}
{"function": "\n\ndef _regenerate(self, dev_mode=False):\n    if self._compiled:\n        for (module_name, (mtime, content, hash)) in self._compiled.items():\n            if ((module_name not in self._collected) or (not os.path.exists(self._collected[module_name])) or (os.path.getmtime(self._collected[module_name]) != mtime)):\n                self._compiled = {\n                    \n                }\n                break\n        else:\n            return\n    modules = [self.main_module, 'pyjslib']\n    while True:\n        if (not modules):\n            break\n        module_name = modules.pop()\n        path = self._collected[module_name]\n        mtime = os.path.getmtime(path)\n        source = read_text_file(path)\n        try:\n            (content, py_deps, js_deps) = self._compile(module_name, source, dev_mode=dev_mode)\n        except:\n            self._compiled = {\n                \n            }\n            raise\n        hash = sha1(smart_str(content)).hexdigest()\n        self._compiled[module_name] = (mtime, content, hash)\n        for name in py_deps:\n            if (name not in self._collected):\n                if (('.' in name) and (name.rsplit('.', 1)[0] in self._collected)):\n                    name = name.rsplit('.', 1)[0]\n                else:\n                    raise ImportError(('The pyjs module %s could not find the dependency %s' % (module_name, name)))\n            if (name not in self._compiled):\n                modules.append(name)\n", "label": 1}
{"function": "\n\ndef result_fail(self, environment=None, comment='', stepnumber=None, bug='', user=None):\n    'Create a failed result for this case.'\n    result = Result.objects.create(runcaseversion=self, tester=user, environment=environment, status=Result.STATUS.failed, comment=comment, user=user)\n    if (stepnumber is not None):\n        try:\n            step = self.caseversion.steps.get(number=stepnumber)\n        except CaseStep.DoesNotExist:\n            pass\n        else:\n            stepresult = StepResult(result=result, step=step)\n            stepresult.status = StepResult.STATUS.failed\n            stepresult.bug_url = bug\n            stepresult.save(user=user)\n    self.save(force_update=True, user=user)\n", "label": 0}
{"function": "\n\ndef __init__(self, content, url, headers=None, trusted=None):\n    encoding = None\n    if (headers and ('Content-Type' in headers)):\n        (content_type, params) = cgi.parse_header(headers['Content-Type'])\n        if ('charset' in params):\n            encoding = params['charset']\n    self.content = content\n    self.parsed = html5lib.parse(self.content, encoding=encoding, namespaceHTMLElements=False)\n    self.url = url\n    self.headers = headers\n    self.trusted = trusted\n", "label": 0}
{"function": "\n\ndef GetEditMediaLink(self):\n    'The Picasa API mistakenly returns media-edit rather than edit-media, but\\n    this may change soon.\\n    '\n    for a_link in self.link:\n        if (a_link.rel == 'edit-media'):\n            return a_link\n        if (a_link.rel == 'media-edit'):\n            return a_link\n    return None\n", "label": 0}
{"function": "\n\ndef ValidateTree(self, queries, additional_visitors=(), max_alter_rows=100000, allowed_engines=('InnoDB',), fail_fast=False):\n    'Validate a parse tree.\\n\\n    Args:\\n      tokens: pyparsing parse tree\\n\\n    Returns:\\n      Whether the tree validated\\n    '\n    visitors = ([ShardSetChecker(self._schema), AlterChecker(self._schema, max_alter_rows=max_alter_rows), CreateDatabaseChecker(self._schema), DropDatabaseChecker(self._schema), CreateTableChecker(self._schema, allowed_engines=allowed_engines), DropTableChecker(self._schema), ReplaceChecker(self._schema), ColumnChecker(self._schema)] + list(additional_visitors))\n    for query in queries:\n        self._CheckCancelled()\n        assert (query.getName() == 'query'), ('Invalid second-level token: %s' % query.getName())\n        logging.debug('Visiting: %s', query)\n        for visitor in visitors:\n            visitor.visit([query])\n            if (fail_fast and (visitor.Errors() or visitor.Warnings())):\n                self._Finalize(visitors)\n                return False\n        if self._callback:\n            self._callback(self._loc)\n    self._Finalize(visitors)\n    return ((not self._errors) and (not self._warnings))\n", "label": 0}
{"function": "\n\ndef _pd_assert_dibbler_calls(self, expected, actual):\n    \"Check the external process calls for dibbler are expected\\n\\n        in the case of multiple pd-enabled router ports, the exact sequence\\n        of these calls are not deterministic. It's known, though, that each\\n        external_process call is followed with either an enable() or disable()\\n        \"\n    num_ext_calls = (len(expected) // 2)\n    expected_ext_calls = []\n    actual_ext_calls = []\n    expected_action_calls = []\n    actual_action_calls = []\n    for c in range(num_ext_calls):\n        expected_ext_calls.append(expected[(c * 2)])\n        actual_ext_calls.append(actual[(c * 2)])\n        expected_action_calls.append(expected[((c * 2) + 1)])\n        actual_action_calls.append(actual[((c * 2) + 1)])\n    self.assertEqual(expected_action_calls, actual_action_calls)\n    for exp in expected_ext_calls:\n        for act in actual_ext_calls:\n            if (exp == act):\n                break\n        else:\n            msg = 'Unexpected dibbler external process call.'\n            self.fail(msg)\n", "label": 0}
{"function": "\n\ndef do_complete(self, code, cursor_pos):\n    no_complete = {\n        'status': 'ok',\n        'matches': [],\n        'cursor_start': 0,\n        'cursor_end': cursor_pos,\n        'metadata': dict(),\n    }\n    if ((not code) or (code[(- 1)] == ' ')):\n        return no_complete\n    tokens = code.split()\n    if (not tokens):\n        return no_complete\n    token = tokens[(- 1)]\n    start = (cursor_pos - len(token))\n    matches = self.qsh._complete(code, token)\n    return {\n        'status': 'ok',\n        'matches': sorted(matches),\n        'cursor_start': start,\n        'cursor_end': cursor_pos,\n        'metadata': dict(),\n    }\n", "label": 0}
{"function": "\n\ndef _check_expression(self, symbol, myself):\n    for (cre, state, action, next_state) in self._expressions:\n        mo = cre.match(symbol)\n        if ((state is self.current_state) and mo):\n            if (action is not None):\n                action(mo, self)\n            self.current_state = next_state\n", "label": 0}
{"function": "\n\ndef get_connection_mgr():\n    connection_mgr = context.get_context().connection_mgr\n    (server_models, sockpools) = ({\n        \n    }, {\n        \n    })\n    for k in connection_mgr.server_models:\n        server_model = connection_mgr.server_models[k]\n        server_models[repr(k)] = {\n            'info': repr(server_model),\n            'fds': [s.fileno() for s in server_model.active_connections.keys()],\n        }\n    for prot in connection_mgr.sockpools:\n        for sock_type in connection_mgr.sockpools[prot]:\n            sockpool = connection_mgr.sockpools[prot][sock_type]\n            sockpools[repr((prot, sock_type))] = {\n                'info': repr(sockpool),\n                'addresses': map(repr, sockpool.free_socks_by_addr.keys()),\n            }\n    return {\n        'server_models': server_models,\n        'sockpools': sockpools,\n    }\n", "label": 0}
{"function": "\n\ndef handler(self, zh, rc, data, stat):\n    'Handle zookeeper.aget() responses.\\n\\n    This code handles the zookeeper.aget callback. It does not handle watches.\\n\\n    Numeric arguments map to constants. See ``DATA`` in ``help(zookeeper)``\\n    for more information.\\n\\n    Args:\\n      zh Zookeeper handle that made this request.\\n      rc Return code.\\n      data Data stored in the znode.\\n\\n    Does not provide a return value.\\n    '\n    if (zookeeper.OK == rc):\n        logger.debug('This is where your application does work.')\n    else:\n        if (zookeeper.NONODE == rc):\n            logger.info('Node not found. Trying again to set the watch.')\n            time.sleep(1)\n        if (zookeeper.OK != self.aget()):\n            logger.critical('Unable to get znode! Exiting.')\n            sys.exit(1)\n", "label": 0}
{"function": "\n\ndef matchesPredicates(self, elem):\n    if ((self.elementName != None) and (self.elementName != elem.name)):\n        return 0\n    for p in self.predicates:\n        if (not p.value(elem)):\n            return 0\n    return 1\n", "label": 0}
{"function": "\n\ndef start_selection(self, event):\n    if (event.inaxes != self._axes):\n        return False\n    if (event.key == SCRUBBING_KEY):\n        if (not self._roi.defined()):\n            return False\n        elif (not self._roi.contains(event.xdata, event.ydata)):\n            return False\n    self._roi_store()\n    if (event.key == SCRUBBING_KEY):\n        self._scrubbing = True\n        self._dx = (event.xdata - self._roi.center())\n    else:\n        self.reset()\n        self._roi.set_range(event.xdata, event.xdata)\n        self._xi = event.xdata\n    self._mid_selection = True\n    self._sync_patch()\n", "label": 0}
{"function": "\n\ndef __getattr__(self, name):\n    t = type(self)\n    try:\n        i = t.name_to_idx[name]\n    except KeyError:\n        raise AttributeError(name)\n    f = t.fields[i]\n    if (i < len(self.data)):\n        v = self.data[i]\n    else:\n        v = ''\n    if (len(f) >= 3):\n        if (v == ''):\n            return None\n        return f[2](v)\n    else:\n        return v\n", "label": 0}
{"function": "\n\ndef _test_sync_state_helper(self, known_net_ids, active_net_ids):\n    active_networks = set((mock.Mock(id=netid) for netid in active_net_ids))\n    with mock.patch(DHCP_PLUGIN) as plug:\n        mock_plugin = mock.Mock()\n        mock_plugin.get_active_networks_info.return_value = active_networks\n        plug.return_value = mock_plugin\n        dhcp = dhcp_agent.DhcpAgent(HOSTNAME)\n        attrs_to_mock = dict([(a, mock.DEFAULT) for a in ['disable_dhcp_helper', 'cache', 'safe_configure_dhcp_for_network']])\n        with mock.patch.multiple(dhcp, **attrs_to_mock) as mocks:\n            mocks['cache'].get_network_ids.return_value = known_net_ids\n            dhcp.sync_state()\n            diff = (set(known_net_ids) - set(active_net_ids))\n            exp_disable = [mock.call(net_id) for net_id in diff]\n            mocks['cache'].assert_has_calls([mock.call.get_network_ids()])\n            mocks['disable_dhcp_helper'].assert_has_calls(exp_disable)\n", "label": 0}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (node in self.nodes_seen):\n        self.nodes_seen.discard(node)\n        self.process_node(fgraph, node)\n    if (not isinstance(node, string_types)):\n        assert node.inputs\n    if isinstance(new_r, graph.Constant):\n        self.process_constant(fgraph, new_r)\n", "label": 0}
{"function": "\n\ndef _initialize_trunk_interfaces_to_none(self, switch_ip):\n    'Initialize all nexus interfaces to trunk allowed none.'\n    try:\n        switch_ifs = self._mdriver._get_switch_interfaces(switch_ip)\n        if (not switch_ifs):\n            LOG.debug('Skipping switch %s which has no configured interfaces', switch_ip)\n            return\n        self._driver.initialize_all_switch_interfaces(switch_ifs)\n    except Exception:\n        with excutils.save_and_reraise_exception():\n            LOG.warning(_LW('Unable to initialize interfaces to switch %(switch_ip)s'), {\n                'switch_ip': switch_ip,\n            })\n            self._mdriver.register_switch_as_inactive(switch_ip, 'replay init_interface')\n    for (switch_ip, intf_type, port, is_native, ch_grp) in switch_ifs:\n        try:\n            reserved = nxos_db.get_reserved_port_binding(switch_ip, self._mdriver.format_interface_name(intf_type, port))\n        except excep.NexusPortBindingNotFound:\n            continue\n        if (reserved[0].channel_group != ch_grp):\n            self._mdriver._change_baremetal_interfaces(switch_ip, intf_type, port, reserved[0].channel_group, ch_grp)\n    if self._mdriver.is_replay_enabled():\n        return\n    try:\n        mgr = self._driver.nxos_connect(switch_ip)\n        self._driver._close_session(mgr, switch_ip)\n    except Exception:\n        LOG.warning(_LW('Failed to release connection after initialize interfaces for switch %(switch_ip)s'), {\n            'switch_ip': switch_ip,\n        })\n", "label": 0}
{"function": "\n\ndef write_content(self, content, content_type=None):\n    'Helper method to write content bytes to output stream.'\n    if (content_type is not None):\n        self.send_header(HTTP_HEADER_CONTENT_TYPE, content_type)\n    if ('gzip' in self.headers.get(HTTP_HEADER_ACCEPT_ENCODING, '')):\n        content = gzip.compress(content)\n        self.send_header(HTTP_HEADER_CONTENT_ENCODING, 'gzip')\n        self.send_header(HTTP_HEADER_VARY, HTTP_HEADER_ACCEPT_ENCODING)\n    self.send_header(HTTP_HEADER_CONTENT_LENGTH, str(len(content)))\n    self.end_headers()\n    if (self.command == 'HEAD'):\n        return\n    self.wfile.write(content)\n", "label": 0}
{"function": "\n\ndef test_get_vifs_by_ids(self):\n    for i in range(2):\n        self.create_ovs_port()\n    vif_ports = [self.create_ovs_vif_port() for i in range(3)]\n    by_id = self.br.get_vifs_by_ids([v.vif_id for v in vif_ports])\n    by_id = {vid: str(vport) for (vid, vport) in by_id.items()}\n    self.assertEqual({v.vif_id: str(v) for v in vif_ports}, by_id)\n", "label": 0}
{"function": "\n\ndef _format_child_instances(self, children, parent_id):\n    '\\n        The goal of this method is to add an indent at every level. This way the\\n        WF is represented as a tree structure while in a list. For the right visuals\\n        representation the list must be a DF traversal else the idents will end up\\n        looking strange.\\n        '\n    children = format_wf_instances(children)\n    depth = {\n        parent_id: 0,\n    }\n    result = []\n    for child in children:\n        if (child.parent not in depth):\n            parent = None\n            for instance in children:\n                if (WF_PREFIX in instance.id):\n                    instance_id = instance.id[(instance.id.index(WF_PREFIX) + len(WF_PREFIX)):]\n                else:\n                    instance_id = instance.id\n                if (instance_id == child.parent):\n                    parent = instance\n            if (parent and parent.parent and (parent.parent in depth)):\n                depth[child.parent] = (depth[parent.parent] + 1)\n            else:\n                depth[child.parent] = 0\n        child.id = ((INDENT_CHAR * depth[child.parent]) + child.id)\n        result.append(self._format_for_common_representation(child))\n    return result\n", "label": 0}
{"function": "\n\ndef send(self):\n    '\\n        Sends the batch request to the server and returns a list of RpcResponse\\n        objects.  The list will be in the order that the requests were made to\\n        the batch.  Note that the RpcResponse objects may contain an error or a\\n        successful result.  When you iterate through the list, you must test for\\n        response.error.\\n\\n        send() may not be called more than once.\\n        '\n    if self.sent:\n        raise Exception('Batch already sent. Cannot send() again.')\n    else:\n        self.sent = True\n        results = self.client.transport.request(self.req_list)\n        id_to_method = {\n            \n        }\n        by_id = {\n            \n        }\n        for res in results:\n            reqid = res['id']\n            by_id[reqid] = res\n        in_req_order = []\n        for req in self.req_list:\n            reqid = req['id']\n            result = None\n            error = None\n            resp = safe_get(by_id, reqid)\n            if (resp is None):\n                msg = ('Batch response missing result for request id: %s' % reqid)\n                error = RpcException(ERR_INVALID_RESP, msg)\n            else:\n                r_err = safe_get(resp, 'error')\n                if (r_err is None):\n                    result = resp['result']\n                else:\n                    error = RpcException(r_err['code'], r_err['message'], safe_get(r_err, 'data'))\n            in_req_order.append(RpcResponse(req, result, error))\n        return in_req_order\n", "label": 0}
{"function": "\n\ndef find(l, predicate):\n    results = [x for x in l if predicate(x)]\n    return (results[0] if (len(results) > 0) else None)\n", "label": 0}
{"function": "\n\ndef __getitem__(self, position):\n    (i, j) = self._get_indexes(position)\n    if isinstance(i, slice):\n        if (j is None):\n            return Table(self.data[position])\n        else:\n            return Table((cells[j] for cells in self.data[i]))\n    else:\n        try:\n            row = self.data[i]\n        except IndexError:\n            msg = 'no row at index %r of %d-row table'\n            raise IndexError((msg % (position, len(self))))\n        if (j is None):\n            return row\n        else:\n            return row[j]\n", "label": 0}
{"function": "\n\ndef test_exists_many_with_none_keys(self):\n    try:\n        TestExistsMany.client.exists_many(None, {\n            \n        })\n    except e.ParamError as exception:\n        assert (exception.code == (- 2))\n        assert (exception.msg == 'Keys should be specified as a list or tuple.')\n", "label": 0}
{"function": "\n\ndef _handle_message(self, opcode, data):\n    if self.client_terminated:\n        return\n    if (opcode == 1):\n        try:\n            decoded = data.decode('utf-8')\n        except UnicodeDecodeError:\n            self._abort()\n            return\n        self._run_callback(self.handler.on_message, decoded)\n    elif (opcode == 2):\n        self._run_callback(self.handler.on_message, data)\n    elif (opcode == 8):\n        self.client_terminated = True\n        if (len(data) >= 2):\n            self.handler.close_code = struct.unpack('>H', data[:2])[0]\n        if (len(data) > 2):\n            self.handler.close_reason = to_unicode(data[2:])\n        self.close()\n    elif (opcode == 9):\n        self._write_frame(True, 10, data)\n    elif (opcode == 10):\n        self._run_callback(self.handler.on_pong, data)\n    else:\n        self._abort()\n", "label": 1}
{"function": "\n\ndef ack(self, delivery_tag=None, multiple=False):\n    'Acknowledge Message.\\n\\n        :param int/long delivery_tag: Server-assigned delivery tag\\n        :param bool multiple: Acknowledge multiple messages\\n\\n        :raises AMQPInvalidArgument: Invalid Parameters\\n        :raises AMQPChannelError: Raises if the channel encountered an error.\\n        :raises AMQPConnectionError: Raises if the connection\\n                                     encountered an error.\\n\\n        :return:\\n        '\n    if ((delivery_tag is not None) and (not compatibility.is_integer(delivery_tag))):\n        raise AMQPInvalidArgument('delivery_tag should be an integer or None')\n    elif (not isinstance(multiple, bool)):\n        raise AMQPInvalidArgument('multiple should be a boolean')\n    ack_frame = pamqp_spec.Basic.Ack(delivery_tag=delivery_tag, multiple=multiple)\n    self._channel.write_frame(ack_frame)\n", "label": 0}
{"function": "\n\ndef tostring(self, sep='', endcard=True, padding=True):\n    \"\\n        Returns a string representation of the header.\\n\\n        By default this uses no separator between cards, adds the END card, and\\n        pads the string with spaces to the next multiple of 2880 bytes.  That\\n        is, it returns the header exactly as it would appear in a FITS file.\\n\\n        Parameters\\n        ----------\\n        sep : str, optional\\n            The character or string with which to separate cards.  By default\\n            there is no separator, but one could use ``'\\\\\\\\n'``, for example, to\\n            separate each card with a new line\\n\\n        endcard : bool, optional\\n            If True (default) adds the END card to the end of the header\\n            string\\n\\n        padding : bool, optional\\n            If True (default) pads the string with spaces out to the next\\n            multiple of 2880 characters\\n\\n        Returns\\n        -------\\n        s : string\\n            A string representing a FITS header.\\n        \"\n    lines = []\n    for card in self._cards:\n        s = str(card)\n        while s:\n            lines.append(s[:Card.length])\n            s = s[Card.length:]\n    s = sep.join(lines)\n    if endcard:\n        s += (sep + _pad('END'))\n    if padding:\n        s += (' ' * _pad_length(len(s)))\n    return s\n", "label": 0}
{"function": "\n\ndef _handle_message(self, message, buffer, record_fn):\n    'Build a response calling record_fn on all the TlsRecords in message\\n\\n        message: bytes to parse as TlsRecords\\n        record_fn: one of on_tls_request, on_tls_response to handle the record\\n        Returns tuple containing the bytes to send for all the records handled and any remaining unparsed data\\n        '\n    out = ''\n    message = (buffer.buffer + message)\n    buffer.buffer = ''\n    remaining = message\n    while remaining:\n        record = None\n        try:\n            (record, remaining) = tls.parse_tls(remaining, throw_on_incomplete=True)\n        except tls.types.TlsNotEnoughDataError:\n            if buffer.should_buffer:\n                buffer.buffer = remaining\n                if (len(buffer.buffer) >= buffer.MAX_BUFFER):\n                    buffer.buffer = ''\n            return out\n        if (not record):\n            return out\n        record_bytes = record_fn(record)\n        if record_bytes:\n            out += record_bytes\n        if (record.content_type == record.CONTENT_TYPE.CHANGE_CIPHER_SPEC):\n            buffer.should_buffer = False\n    return out\n", "label": 0}
{"function": "\n\ndef run_conv_nnet2_classif(use_gpu, seed, isize, ksize, bsize, n_train=10, check_isfinite=True, pickle=False, verbose=0, version=(- 1)):\n    'Run the train function returned by build_conv_nnet2_classif on one device.\\n    '\n    utt.seed_rng(seed)\n    (train, params, x_shape, y_shape, mode) = build_conv_nnet2_classif(use_gpu=use_gpu, isize=isize, ksize=ksize, n_batch=bsize, verbose=verbose, version=version, check_isfinite=check_isfinite)\n    if use_gpu:\n        device = 'GPU'\n    else:\n        device = 'CPU'\n    xval = my_rand(*x_shape)\n    yval = my_rand(*y_shape)\n    lr = theano._asarray(0.01, dtype='float32')\n    rvals = my_zeros(n_train)\n    t0 = time.time()\n    for i in xrange(n_train):\n        rvals[i] = train(xval, yval, lr)[0]\n    t1 = time.time()\n    print_mode(mode)\n    if (pickle and isinstance(mode, theano.compile.ProfileMode)):\n        import pickle\n        print(('BEGIN %s profile mode dump' % device))\n        print(pickle.dumps(mode))\n        print(('END %s profile mode dump' % device))\n", "label": 0}
{"function": "\n\ndef bPrimary(self, prob):\n    '\\n        The primary magnetic flux density from a magnetic vector potential\\n\\n        :param Problem prob: FDEM problem\\n        :rtype: numpy.ndarray\\n        :return: primary magnetic field\\n        '\n    formulation = prob._formulation\n    if (formulation is 'EB'):\n        gridX = prob.mesh.gridEx\n        gridY = prob.mesh.gridEy\n        gridZ = prob.mesh.gridEz\n        C = prob.mesh.edgeCurl\n    elif (formulation is 'HJ'):\n        gridX = prob.mesh.gridFx\n        gridY = prob.mesh.gridFy\n        gridZ = prob.mesh.gridFz\n        C = prob.mesh.edgeCurl.T\n    if (prob.mesh._meshType is 'CYL'):\n        if (not prob.mesh.isSymmetric):\n            raise NotImplementedError('Non-symmetric cyl mesh not implemented yet!')\n        a = MagneticDipoleVectorPotential(self.loc, gridY, 'y', mu=self.mu, moment=self.moment)\n    else:\n        srcfct = MagneticDipoleVectorPotential\n        ax = srcfct(self.loc, gridX, 'x', mu=self.mu, moment=self.moment)\n        ay = srcfct(self.loc, gridY, 'y', mu=self.mu, moment=self.moment)\n        az = srcfct(self.loc, gridZ, 'z', mu=self.mu, moment=self.moment)\n        a = np.concatenate((ax, ay, az))\n    return (C * a)\n", "label": 0}
{"function": "\n\ndef __init__(self, getter, attribute, new, spec, create, spec_set, autospec, new_callable, kwargs):\n    if (new_callable is not None):\n        if (new is not DEFAULT):\n            raise ValueError(\"Cannot use 'new' and 'new_callable' together\")\n        if (autospec is not None):\n            raise ValueError(\"Cannot use 'autospec' and 'new_callable' together\")\n    self.getter = getter\n    self.attribute = attribute\n    self.new = new\n    self.new_callable = new_callable\n    self.spec = spec\n    self.create = create\n    self.has_local = False\n    self.spec_set = spec_set\n    self.autospec = autospec\n    self.kwargs = kwargs\n    self.additional_patchers = []\n", "label": 0}
{"function": "\n\ndef validate_configurator_version():\n    '\\n    Arch is a rolling release distro, therefore it is important to ensure\\n    the configurator version is current.\\n    '\n    if (settings.CONFIGURATOR_MODULE == 'bootmachine.contrib.configurators.salt'):\n        pkgver = settings.SALT_AUR_PKGVER\n        pkgrel = settings.SALT_AUR_PKGREL\n        response = urllib2.urlopen('https://aur.archlinux.org/packages/sa/salt/PKGBUILD')\n        for line in response:\n            if (line.startswith('pkgver=') and (not (pkgver in line))):\n                abort(\"The requested Salt 'pkgrel={0}' in the AUR was updated to '{1}'.\".format(pkgver, line.strip()))\n            if (line.startswith('pkgrel=') and (not (pkgrel in line))):\n                abort(\"The requested Salt 'pkgrel={0}' in the AUR was updated to '{1}'.\".format(pkgrel, line.strip()))\n", "label": 0}
{"function": "\n\ndef decistmt(s):\n    'Substitute Decimals for floats in a string of statements.\\n\\n    >>> from decimal import Decimal\\n    >>> s = \\'print +21.3e-5*-.1234/81.7\\'\\n    >>> decistmt(s)\\n    \"print +Decimal (\\'21.3e-5\\')*-Decimal (\\'.1234\\')/Decimal (\\'81.7\\')\"\\n\\n    The format of the exponent is inherited from the platform C library.\\n    Known cases are \"e-007\" (Windows) and \"e-07\" (not Windows).  Since\\n    we\\'re only showing 12 digits, and the 13th isn\\'t close to 5, the\\n    rest of the output should be platform-independent.\\n\\n    >>> exec(s) #doctest: +ELLIPSIS\\n    -3.21716034272e-0...7\\n\\n    Output from calculations with Decimal should be identical across all\\n    platforms.\\n\\n    >>> exec(decistmt(s))\\n    -3.217160342717258261933904529E-7\\n    '\n    result = []\n    g = generate_tokens(StringIO(s).readline)\n    for (toknum, tokval, _, _, _) in g:\n        if ((toknum == NUMBER) and ('.' in tokval)):\n            result.extend([(NAME, 'Decimal'), (OP, '('), (STRING, repr(tokval)), (OP, ')')])\n        else:\n            result.append((toknum, tokval))\n    return untokenize(result)\n", "label": 0}
{"function": "\n\ndef onRadioSelect(self, sender, keyCode=None, modifiers=None):\n    if (sender == self.radLevel5):\n        self.level = 5\n    elif (sender == self.radLevel10):\n        self.level = 10\n    elif (sender == self.radLevel15):\n        self.level = 15\n    elif (sender == self.radLevel20):\n        self.level = 20\n", "label": 0}
{"function": "\n\ndef run(self):\n    '\\n        Kill any open Redshift sessions for the given database.\\n        '\n    connection = self.output().connect()\n    query = \"select pg_terminate_backend(process) from STV_SESSIONS where db_name=%s and user_name != 'rdsdb' and process != pg_backend_pid()\"\n    cursor = connection.cursor()\n    logger.info('Killing all open Redshift sessions for database: %s', self.database)\n    try:\n        cursor.execute(query, (self.database,))\n        cursor.close()\n        connection.commit()\n    except psycopg2.DatabaseError as e:\n        if (e.message and ('EOF' in e.message)):\n            connection.close()\n            logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n            time.sleep(self.connection_reset_wait_seconds)\n            logger.info('Reconnecting to Redshift')\n            connection = self.output().connect()\n        else:\n            raise\n    try:\n        self.output().touch(connection)\n        connection.commit()\n    finally:\n        connection.close()\n    logger.info('Done killing all open Redshift sessions for database: %s', self.database)\n", "label": 0}
{"function": "\n\ndef deallocate_for_instance(self, context, instance, **kwargs):\n    'Deallocate all network resources related to the instance.'\n    LOG.debug('deallocate_for_instance()', instance=instance)\n    search_opts = {\n        'device_id': instance.uuid,\n    }\n    neutron = get_client(context)\n    data = neutron.list_ports(**search_opts)\n    ports = [port['id'] for port in data.get('ports', [])]\n    requested_networks = (kwargs.get('requested_networks') or [])\n    if isinstance(requested_networks, objects.NetworkRequestList):\n        requested_networks = requested_networks.as_tuples()\n    ports_to_skip = set([port_id for (nets, fips, port_id, pci_request_id) in requested_networks])\n    ports_to_skip |= set(self._get_preexisting_port_ids(instance))\n    ports = (set(ports) - ports_to_skip)\n    self._unbind_ports(context, ports_to_skip, neutron)\n    self._delete_ports(neutron, instance, ports, raise_if_fail=True)\n    base_api.update_instance_cache_with_nw_info(self, context, instance, network_model.NetworkInfo([]))\n", "label": 0}
{"function": "\n\ndef configure_formatter(self, config):\n    'Configure a formatter from a dictionary.'\n    if ('()' in config):\n        factory = config['()']\n        try:\n            result = self.configure_custom(config)\n        except TypeError as te:\n            if (\"'format'\" not in str(te)):\n                raise\n            config['fmt'] = config.pop('format')\n            config['()'] = factory\n            result = self.configure_custom(config)\n    else:\n        fmt = config.get('format', None)\n        dfmt = config.get('datefmt', None)\n        result = logging.Formatter(fmt, dfmt)\n    return result\n", "label": 0}
{"function": "\n\ndef compile_dir(env, src_path, dst_path, pattern='^.*\\\\.html$', encoding='utf-8', base_dir=None):\n    'Compiles a directory of Jinja2 templates to python code.\\n  \\n  :param env: a Jinja2 Environment instance.\\n  :param src_path: path to the source directory.\\n  :param dst_path: path to the destination directory.\\n  :param encoding: template encoding.\\n  :param base_dir: the base path to be removed from the compiled template filename.\\n  '\n    from os import path, listdir, mkdir\n    file_re = re.compile(pattern)\n    if (base_dir is None):\n        base_dir = src_path\n    for filename in listdir(src_path):\n        src_name = path.join(src_path, filename)\n        dst_name = path.join(dst_path, filename)\n        if path.isdir(src_name):\n            mkdir(dst_name)\n            compile_dir(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n        elif (path.isfile(src_name) and file_re.match(filename)):\n            compile_file(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n", "label": 0}
{"function": "\n\ndef get_project_users_roles(request, project):\n    users_roles = collections.defaultdict(list)\n    if (VERSIONS.active < 3):\n        project_users = user_list(request, project=project)\n        for user in project_users:\n            roles = roles_for_user(request, user.id, project)\n            roles_ids = [role.id for role in roles]\n            users_roles[user.id].extend(roles_ids)\n    else:\n        project_role_assignments = role_assignments_list(request, project=project)\n        for role_assignment in project_role_assignments:\n            if (not hasattr(role_assignment, 'user')):\n                continue\n            user_id = role_assignment.user['id']\n            role_id = role_assignment.role['id']\n            if (('project' in role_assignment.scope) and (role_assignment.scope['project']['id'] == project)):\n                users_roles[user_id].append(role_id)\n    return users_roles\n", "label": 0}
{"function": "\n\ndef create_list_setting(self, name):\n    hlayout = QtGui.QHBoxLayout()\n    setting = self.get_setting(name)\n    button = None\n    if setting.button:\n        button = QtGui.QPushButton(setting.button)\n        button.clicked.connect((lambda : setting.button_callback(button)))\n    combo = QtGui.QComboBox()\n    combo.setObjectName(setting.name)\n    combo.currentIndexChanged.connect(self.call_with_object('setting_changed', combo, setting))\n    combo.editTextChanged.connect(self.call_with_object('setting_changed', combo, setting))\n    combo.setStatusTip(setting.description)\n    combo.setToolTip(setting.description)\n    for val in setting.values:\n        combo.addItem(val)\n    default_index = combo.findText(setting.default_value)\n    if (default_index != (- 1)):\n        combo.setCurrentIndex(default_index)\n    hlayout.addWidget(QtGui.QLabel())\n    hlayout.addWidget(combo)\n    if button:\n        hlayout.addWidget(button)\n    return hlayout\n", "label": 0}
{"function": "\n\ndef onMouseUp(self, sender, x, y):\n    self.dragging = NOT_DRAGGING\n    if self.draggingImage:\n        GlassWidget.hide()\n        if ((self.currentDragOperation == 'none') or (not self.currentTargetElement)):\n            if self.currentTargetElement:\n                self.fireDNDEvent('dragleave', self.currentTargetElement, self.currentDropWidget)\n            else:\n                self.currentDragOperation = 'none'\n            self.returnDrag()\n        else:\n            drop_event = self.fireDNDEvent('drop', self.currentTargetElement, self.currentDropWidget)\n            if isCanceled(drop_event):\n                self.currentDragOperation = drop_event.dataTransfer.dropEffect\n            else:\n                self.currentDragOperation = 'none'\n            self.zapDragImage()\n        self.fireDNDEvent('dragend', None, self.dragWidget)\n", "label": 0}
{"function": "\n\n@blueprint.route('/remove', methods=['POST'])\n@api_wrapper\n@require_login\n@require_team\ndef team_remove_hook():\n    if (api.auth.is_logged_in() and api.user.in_team()):\n        uid = request.form.get('uid')\n        user = api.user.get_user()\n        if (uid == user['uid']):\n            confirm = request.form.get('confirm')\n            team = api.team.get_team()\n            if (confirm != team['teamname']):\n                raise WebException('Please confirm your name.')\n        message = api.team.remove(uid)\n        return {\n            'success': 1,\n            'message': message,\n        }\n    else:\n        raise WebException('Stop. Just stop.')\n", "label": 0}
{"function": "\n\ndef handle_entityref(self, ref):\n    if (not self.elementstack):\n        return\n    if (ref in ('lt', 'gt', 'quot', 'amp', 'apos')):\n        text = ('&%s;' % ref)\n    elif (ref in self.entities):\n        text = self.entities[ref]\n        if (text.startswith('&#') and text.endswith(';')):\n            return self.handle_entityref(text)\n    else:\n        try:\n            name2codepoint[ref]\n        except KeyError:\n            text = ('&%s;' % ref)\n        else:\n            text = chr(name2codepoint[ref]).encode('utf-8')\n    self.elementstack[(- 1)][2].append(text)\n", "label": 0}
{"function": "\n\ndef _plot_mean(self, canvas, helper_data, helper_prediction, levels=20, projection='2d', label=None, **kwargs):\n    (_, free_dims, Xgrid, x, y, _, _, resolution) = helper_data\n    if (len(free_dims) <= 2):\n        (mu, _, _) = helper_prediction\n        if (len(free_dims) == 1):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_1d)\n            plots = dict(gpmean=[pl().plot(canvas, Xgrid[:, free_dims], mu, label=label, **kwargs)])\n        elif (projection == '2d'):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_2d)\n            plots = dict(gpmean=[pl().contour(canvas, x[:, 0], y[0, :], mu.reshape(resolution, resolution).T, levels=levels, label=label, **kwargs)])\n        elif (projection == '3d'):\n            update_not_existing_kwargs(kwargs, pl().defaults.meanplot_3d)\n            plots = dict(gpmean=[pl().surface(canvas, x, y, mu.reshape(resolution, resolution), label=label, **kwargs)])\n    elif (len(free_dims) == 0):\n        pass\n    else:\n        raise RuntimeError('Cannot plot mean in more then 2 input dimensions')\n    return plots\n", "label": 0}
{"function": "\n\ndef user_shipping_address_view(request, checkout):\n    data = (request.POST or None)\n    additional_addresses = request.user.addresses.all()\n    checkout.email = request.user.email\n    shipping_address = checkout.shipping_address\n    if ((shipping_address is not None) and shipping_address.id):\n        address_form = AddressForm(data, autocomplete_type='shipping', initial={\n            'country': request.country,\n        })\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses, initial={\n            'address': shipping_address.id,\n        })\n    elif shipping_address:\n        address_form = AddressForm(data, instance=shipping_address)\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses)\n    else:\n        address_form = AddressForm(data, initial={\n            'country': request.country,\n        })\n        addresses_form = ShippingAddressesForm(data, additional_addresses=additional_addresses)\n    if addresses_form.is_valid():\n        if (addresses_form.cleaned_data['address'] != ShippingAddressesForm.NEW_ADDRESS):\n            address_id = addresses_form.cleaned_data['address']\n            checkout.shipping_address = Address.objects.get(id=address_id)\n            return redirect('checkout:shipping-method')\n        elif address_form.is_valid():\n            checkout.shipping_address = address_form.instance\n            return redirect('checkout:shipping-method')\n    return TemplateResponse(request, 'checkout/shipping_address.html', context={\n        'address_form': address_form,\n        'user_form': addresses_form,\n        'checkout': checkout,\n        'additional_addresses': additional_addresses,\n    })\n", "label": 0}
{"function": "\n\ndef __init__(self, socket_name=None, socket_path=None, config_file=None, colors=None, **kwargs):\n    EnvironmentMixin.__init__(self, '-g')\n    self._windows = []\n    self._panes = []\n    if socket_name:\n        self.socket_name = socket_name\n    if socket_path:\n        self.socket_path = socket_path\n    if config_file:\n        self.config_file = config_file\n    if colors:\n        self.colors = colors\n", "label": 0}
{"function": "\n\ndef check_inline(self, cls, parent_model):\n    \" Validate inline class's fk field is not excluded. \"\n    fk = _get_foreign_key(parent_model, cls.model, fk_name=cls.fk_name, can_fail=True)\n    if (hasattr(cls, 'exclude') and cls.exclude):\n        if (fk and (fk.name in cls.exclude)):\n            raise ImproperlyConfigured((\"%s cannot exclude the field '%s' - this is the foreign key to the parent model %s.%s.\" % (cls.__name__, fk.name, parent_model._meta.app_label, parent_model.__name__)))\n", "label": 0}
{"function": "\n\ndef reexecutable_tasks(self, task_filter):\n    'Keep only reexecutable tasks which match the filter.\\n\\n        Filter is the list of values. If task has reexecute_on key and its\\n        value matches the value from filter then task is not skipped.\\n        :param task_filter: filter (list)\\n        '\n    if (not task_filter):\n        return\n    task_filter = set(task_filter)\n    for task in six.itervalues(self.node):\n        reexecute_on = task.get('reexecute_on')\n        if ((reexecute_on is not None) and task_filter.issubset(reexecute_on)):\n            task['skipped'] = False\n        else:\n            self.make_skipped_task(task)\n", "label": 0}
{"function": "\n\ndef post(self, request):\n    email = request.POST.get('email')\n    api_key = request.POST.get('api_key')\n    if ((not email) or (not api_key)):\n        message = {\n            'success': False,\n            'errors': [],\n        }\n        if (not email):\n            message['errors'].append('Email is mandatory.')\n        if (not api_key):\n            message['errors'].append('API key is mandatory.')\n        return HttpResponseBadRequest(json.dumps(message))\n    member = self.get_member(email, api_key)\n    if (member is None):\n        return HttpResponseForbidden(json.dumps({\n            'success': False,\n            'errors': ['Bad credentials.'],\n        }))\n    return HttpResponse(json.dumps({\n        'success': True,\n        'output': member.get_storage_limit(),\n    }))\n", "label": 0}
{"function": "\n\ndef SetRepo(self, repo):\n    if (self.repo and (self.repo == self.mainRepo)):\n        self.mainRepo = self.repo\n        self.mainRepoSelection = [self.rows[row][0].commit.sha1 for row in self.selection]\n    repo_changed = (self.repo != repo)\n    if repo_changed:\n        self.selection = []\n        self.Scroll(0, 0)\n    if (not repo.parent):\n        self.mainRepo = repo\n    self.repo = repo\n    self.commits = self.repo.get_log(['--topo-order', '--all'])\n    self.CreateLogGraph()\n    if (repo_changed and (self.repo != self.mainRepo)):\n        for version in self.mainRepoSelection:\n            submodule_version = self.repo.parent.get_submodule_version(self.repo.name, version)\n            if submodule_version:\n                rows = [r for r in self.rows if (r[0].commit.sha1 == submodule_version)]\n                if rows:\n                    self.selection.append(self.rows.index(rows[0]))\n    self.SetVirtualSize(((- 1), ((len(self.rows) + 1) * LINH)))\n    self.SetScrollRate(LINH, LINH)\n    self.Refresh()\n", "label": 1}
{"function": "\n\n@property\ndef active_gen_id(self):\n    if self._active_gen_id:\n        return self._active_gen_id\n    active_nodes = InfrastructureNode.objects.filter(deployment_name=self.deployment_name, is_active_generation=1)\n    if (len(active_nodes) == 0):\n        return None\n    first_active_node = active_nodes[0]\n    gen_id = first_active_node.generation_id\n    for active_node in active_nodes:\n        if (active_node.generation_id != gen_id):\n            err_str = 'Inconsistent generation ids in simpledb. %s:%s and %s:%s both marked active'\n            context = (first_active_node.aws_id, first_active_node.generation_id, active_node.aws_id, active_node.generation_id)\n            raise Exception((err_str % context))\n    self._active_gen_id = gen_id\n    return self._active_gen_id\n", "label": 0}
{"function": "\n\ndef _extend_network_dict_address_scope(self, network_res, network_db):\n    network_res[ext_address_scope.IPV4_ADDRESS_SCOPE] = None\n    network_res[ext_address_scope.IPV6_ADDRESS_SCOPE] = None\n    subnetpools = {subnet.subnetpool for subnet in network_db.subnets if subnet.subnetpool}\n    for subnetpool in subnetpools:\n        as_id = subnetpool[ext_address_scope.ADDRESS_SCOPE_ID]\n        if (subnetpool['ip_version'] == constants.IP_VERSION_4):\n            network_res[ext_address_scope.IPV4_ADDRESS_SCOPE] = as_id\n        if (subnetpool['ip_version'] == constants.IP_VERSION_6):\n            network_res[ext_address_scope.IPV6_ADDRESS_SCOPE] = as_id\n    return network_res\n", "label": 0}
{"function": "\n\ndef _read_bytes_from_socket(self, msglen):\n    ' Read bytes from the socket. '\n    chunks = []\n    bytes_recd = 0\n    while (bytes_recd < msglen):\n        if self.stop.is_set():\n            raise InterruptLoop('Stopped while reading from socket')\n        try:\n            chunk = self.socket.recv(min((msglen - bytes_recd), 2048))\n            if (chunk == b''):\n                raise socket.error('socket connection broken')\n            chunks.append(chunk)\n            bytes_recd += len(chunk)\n        except socket.timeout:\n            continue\n        except ssl.SSLError as exc:\n            if _is_ssl_timeout(exc):\n                continue\n            raise\n    return b''.join(chunks)\n", "label": 0}
{"function": "\n\ndef fixup_indent(suite):\n    ' If an INDENT is followed by a thing with a prefix then nuke the prefix\\n        Otherwise we get in trouble when removing __metaclass__ at suite start\\n    '\n    kids = suite.children[::(- 1)]\n    while kids:\n        node = kids.pop()\n        if (node.type == token.INDENT):\n            break\n    while kids:\n        node = kids.pop()\n        if (isinstance(node, Leaf) and (node.type != token.DEDENT)):\n            if node.prefix:\n                node.prefix = ''\n            return\n        else:\n            kids.extend(node.children[::(- 1)])\n", "label": 0}
{"function": "\n\n@dispatch(Expr, MongoQuery)\ndef post_compute(e, q, scope=None):\n    \"\\n    Execute a query using MongoDB's aggregation pipeline\\n\\n    The compute_up functions operate on Mongo Collection / list-of-dict\\n    queries.  Once they're done we need to actually execute the query on\\n    MongoDB.  We do this using the aggregation pipeline framework.\\n\\n    http://docs.mongodb.org/manual/core/aggregation-pipeline/\\n    \"\n    scope = {\n        '$project': toolz.merge({\n            '_id': 0,\n        }, dict(((col, 1) for col in e.fields))),\n    }\n    q = q.append(scope)\n    if (not e.dshape.shape):\n        result = get_result(q.coll.aggregate(list(q.query)))[0]\n        if isscalar(e.dshape.measure):\n            return result[e._name]\n        else:\n            return get(e.fields, result)\n    dicts = get_result(q.coll.aggregate(list(q.query)))\n    if isscalar(e.dshape.measure):\n        return list(pluck(e.fields[0], dicts, default=None))\n    else:\n        return list(pluck(e.fields, dicts, default=None))\n", "label": 0}
{"function": "\n\ndef update(self, other=None, **kwargs):\n    if (other is None):\n        pass\n    elif hasattr(other, 'iteritems'):\n        for (k, v) in other.iteritems():\n            self[k] = v\n    elif hasattr(other, 'keys'):\n        for k in other.keys():\n            self[k] = other[k]\n    else:\n        for (k, v) in other:\n            self[k] = v\n    if kwargs:\n        self.update(kwargs)\n", "label": 0}
{"function": "\n\ndef showStack(self, index):\n    if ((index >= self.getWidgetCount()) or (index == self.visibleStack)):\n        return\n    if (self.visibleStack >= 0):\n        self.setStackVisible(self.visibleStack, False)\n    self.visibleStack = index\n    self.setStackVisible(self.visibleStack, True)\n    for listener in self.stackListeners:\n        listener.onStackChanged(self, index)\n", "label": 0}
{"function": "\n\ndef list_users(order_by='id'):\n    '\\n    Show all users for this company.\\n\\n    CLI Example:\\n\\n        salt myminion bamboohr.list_users\\n\\n    By default, the return data will be keyed by ID. However, it can be ordered\\n    by any other field. Keep in mind that if the field that is chosen contains\\n    duplicate values (i.e., location is used, for a company which only has one\\n    location), then each duplicate value will be overwritten by the previous.\\n    Therefore, it is advisable to only sort by fields that are guaranteed to be\\n    unique.\\n\\n    CLI Examples:\\n\\n        salt myminion bamboohr.list_users order_by=id\\n        salt myminion bamboohr.list_users order_by=email\\n    '\n    ret = {\n        \n    }\n    (status, result) = _query(action='meta', command='users')\n    root = ET.fromstring(result)\n    users = root.getchildren()\n    for user in users:\n        user_id = None\n        user_ret = {\n            \n        }\n        for item in user.items():\n            user_ret[item[0]] = item[1]\n            if (item[0] == 'id'):\n                user_id = item[1]\n        for item in user.getchildren():\n            user_ret[item.tag] = item.text\n        ret[user_ret[order_by]] = user_ret\n    return ret\n", "label": 0}
{"function": "\n\ndef set_display(self, locale_id=None, media_image=None, media_audio=None):\n    text = (Text(locale_id=locale_id) if locale_id else None)\n    if (media_image or media_audio):\n        self.display = Display(text=text, media_image=media_image, media_audio=media_audio)\n    elif text:\n        self.text = text\n", "label": 0}
{"function": "\n\ndef downloadAnimea(self, manga, chapter_start, chapter_end, download_path, download_format):\n    for current_chapter in range(chapter_start, (chapter_end + 1)):\n        manga_chapter_prefix = ((manga.lower().replace('-', '_') + '_') + str(current_chapter).zfill(3))\n        if ((os.path.exists(((download_path + manga_chapter_prefix) + '.cbz')) or os.path.exists(((download_path + manga_chapter_prefix) + '.zip'))) and (overwrite_FLAG == False)):\n            print((('Chapter ' + str(current_chapter)) + ' already downloaded, skipping to next chapter...'))\n            continue\n        url = (((('http://manga.animea.net/' + manga) + '-chapter-') + str(current_chapter)) + '-page-1.html')\n        source = getSourceCode(url)\n        max_pages = int(re.compile('of (.*?)</title>').search(source).group(1))\n        for page in range(1, (max_pages + 1)):\n            url = (((((('http://manga.animea.net/' + manga) + '-chapter-') + str(current_chapter)) + '-page-') + str(page)) + '.html')\n            source = getSourceCode(url)\n            img_url = re.compile('img src=\"(http.*?.[jp][pn]g)\"').search(source).group(1)\n            print((((('Chapter ' + str(current_chapter)) + ' / ') + 'Page ') + str(page)))\n            print(img_url)\n            downloadImage(img_url, os.path.join('mangadl_tmp', ((manga_chapter_prefix + '_') + str(page).zfill(3))))\n        compress(manga_chapter_prefix, download_path, max_pages, download_format)\n", "label": 0}
{"function": "\n\ndef __init__(self, noop=False):\n    if noop:\n        self.func_name = 'posix_fallocate'\n        self.fallocate = noop_libc_function\n        return\n    for func in ('fallocate', 'posix_fallocate'):\n        self.func_name = func\n        self.fallocate = load_libc_function(func, log_error=False)\n        if (self.fallocate is not noop_libc_function):\n            break\n    if (self.fallocate is noop_libc_function):\n        logging.warning(_('Unable to locate fallocate, posix_fallocate in libc.  Leaving as a no-op.'))\n", "label": 0}
{"function": "\n\ndef apply(self):\n    start_tags = []\n    for node in self.document.traverse(docutils.nodes.raw):\n        if (node['format'] != 'html'):\n            continue\n        start_match = self._start_re.match(node.astext())\n        if (not start_match):\n            continue\n        class_match = self._class_re.match(start_match.group(2))\n        if (not class_match):\n            continue\n        admonition_class = class_match.group(1)\n        if (admonition_class == 'info'):\n            admonition_class = 'note'\n        start_tags.append((node, admonition_class))\n    for (node, admonition_class) in reversed(start_tags):\n        content = []\n        for sibling in node.traverse(include_self=False, descend=False, siblings=True, ascend=False):\n            end_tag = (isinstance(sibling, docutils.nodes.raw) and (sibling['format'] == 'html') and self._end_re.match(sibling.astext()))\n            if end_tag:\n                admonition_node = AdmonitionNode(classes=['admonition', admonition_class])\n                admonition_node.extend(content)\n                parent = node.parent\n                parent.replace(node, admonition_node)\n                for n in content:\n                    parent.remove(n)\n                parent.remove(sibling)\n                break\n            else:\n                content.append(sibling)\n", "label": 1}
{"function": "\n\ndef glob_staticfiles(self, item):\n    for finder in finders.get_finders():\n        if hasattr(finder, 'storages'):\n            storages = finder.storages.values()\n        elif hasattr(finder, 'storage'):\n            storages = [finder.storage]\n        else:\n            continue\n        for storage in storages:\n            globber = StorageGlobber(storage)\n            for file in globber.glob(item):\n                (yield storage.path(file))\n", "label": 0}
{"function": "\n\ndef _parse_changelog(self, pkg_name):\n    with open('ChangeLog.md') as f:\n        lineiter = iter(f)\n        for line in lineiter:\n            match = re.search(('^%s\\\\s+(.*)' % pkg_name), line.strip())\n            if (match is None):\n                continue\n            length = len(match.group(1))\n            version = match.group(1).strip()\n            if (lineiter.next().count('-') != len(match.group(0))):\n                continue\n            while 1:\n                change_info = lineiter.next().strip()\n                if change_info:\n                    break\n            match = re.search('released on (\\\\w+\\\\s+\\\\d+\\\\w+\\\\s+\\\\d+)', change_info)\n            if (match is None):\n                continue\n            datestr = match.group(1)\n            return (version, self._parse_date(datestr))\n", "label": 0}
{"function": "\n\ndef _run(self):\n    result = StatusCheckResult(check=self)\n    auth = None\n    if (self.username or self.password):\n        auth = (self.username, self.password)\n    try:\n        resp = requests.get(self.endpoint, timeout=self.timeout, verify=self.verify_ssl_certificate, auth=auth, headers={\n            'User-Agent': settings.HTTP_USER_AGENT,\n        })\n    except requests.RequestException as e:\n        result.error = ('Request error occurred: %s' % (e.message,))\n        result.succeeded = False\n    else:\n        if (self.status_code and (resp.status_code != int(self.status_code))):\n            result.error = ('Wrong code: got %s (expected %s)' % (resp.status_code, int(self.status_code)))\n            result.succeeded = False\n            result.raw_data = resp.content\n        elif self.text_match:\n            if (not re.search(self.text_match, resp.content)):\n                result.error = ('Failed to find match regex /%s/ in response body' % self.text_match)\n                result.raw_data = resp.content\n                result.succeeded = False\n            else:\n                result.succeeded = True\n        else:\n            result.succeeded = True\n    return result\n", "label": 0}
{"function": "\n\n@authorization_required(is_admin=True)\n@threaded\ndef put(self, uid):\n    try:\n        user = Users.get(id=uid)\n    except DoesNotExist:\n        raise HTTPError(404)\n    try:\n        user.login = self.json.get('login', user.login)\n        user.email = self.json.get('email', user.email)\n        user.is_admin = bool(self.json.get('is_admin', user.is_admin))\n        user.password = self.json.get('password', user.password)\n        if (not all((isinstance(user.login, text_type), isinstance(user.email, text_type), (LOGIN_EXP.match(str(user.login)) is not None), (user.password and (len(user.password) > 3)), (EMAIL_EXP.match(str(user.email)) is not None)))):\n            raise HTTPError(400)\n    except:\n        raise HTTPError(400)\n    user.save()\n    self.response({\n        'id': user.id,\n        'login': user.login,\n        'email': user.email,\n        'is_admin': user.is_admin,\n    })\n", "label": 0}
{"function": "\n\ndef test_read_job2():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test.test', 'test1'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef __l2cap_advertise_service(self, name, service_id, service_classes, profiles, provider, description, protocols):\n    if (self._sdpservice is not None):\n        raise BluetoothError('Service already advertised')\n    if (not self.listening):\n        raise BluetoothError('Socket must be listening before advertised')\n    if protocols:\n        raise NotImplementedError('extra protocols not yet supported in Widcomm stack')\n    self._sdpservice = _widcomm._WCSdpService()\n    if service_classes:\n        service_classes = [to_full_uuid(s) for s in service_classes]\n        _sdp_checkraise(self._sdpservice.add_service_class_id_list(service_classes))\n    _sdp_checkraise(self._sdpservice.add_l2cap_protocol_descriptor(self.port))\n    if profiles:\n        for (uuid, version) in profiles:\n            uuid = to_full_uuid(uuid)\n            _sdp_checkraise(self._sdpservice.add_profile_descriptor_list(uuid, version))\n    _sdp_checkraise(self._sdpservice.add_service_name(name))\n    _sdp_checkraise(self._sdpservice.make_public_browseable())\n", "label": 0}
{"function": "\n\ndef listPropsAndMethods(self):\n    res = []\n    if (sublime.platform() == 'windows'):\n        app = 'node'\n        pathToJS = (sublime.packages_path() + '\\\\NPMInfo\\\\npm-info.js')\n        cmd = [app, pathToJS, self.pkgPath]\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, startupinfo=startupinfo)\n    else:\n        pathToJS = (sublime.packages_path() + '/NPMInfo/npm-info.js')\n        cmd = ['/usr/local/bin/node', pathToJS, self.pkgPath]\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    for line in iter(p.stdout.readline, b''):\n        lineStrip = line.rstrip()\n        if ((len(lineStrip) > 0) and (' ' not in lineStrip)):\n            res.append(lineStrip)\n    self.view.window().show_quick_panel(res, self.onSelectPropAndMethod)\n", "label": 0}
{"function": "\n\ndef connect_to_nsqd(self, conn):\n    if (not self.is_running):\n        return\n    if (conn in self.conns):\n        self.logger.debug(('[%s] already connected' % conn))\n        return\n    if (conn in self.pending):\n        self.logger.debug(('[%s] already pending' % conn))\n        return\n    self.logger.debug(('[%s] connecting...' % conn))\n    conn.on_response.connect(self.handle_response)\n    conn.on_error.connect(self.handle_error)\n    conn.on_finish.connect(self.handle_finish)\n    conn.on_requeue.connect(self.handle_requeue)\n    conn.on_auth.connect(self.handle_auth)\n    if self.max_concurrency:\n        conn.on_message.connect(self.queue_message)\n    else:\n        conn.on_message.connect(self.handle_message)\n    self.pending.add(conn)\n    try:\n        conn.connect()\n        conn.identify()\n        if (conn.max_ready_count < self.max_in_flight):\n            msg = ' '.join(['[%s] max RDY count %d < reader max in flight %d,', 'truncation possible'])\n            self.logger.warning((msg % (conn, conn.max_ready_count, self.max_in_flight)))\n        conn.subscribe(self.topic, self.channel)\n        self.send_ready(conn, 1)\n    except NSQException as error:\n        self.logger.warn(('[%s] connection failed (%r)' % (conn, error)))\n        self.handle_connection_failure(conn)\n        return\n    finally:\n        self.pending.remove(conn)\n    if (not self.is_running):\n        conn.close_stream()\n        return\n    self.logger.info(('[%s] connection successful' % conn))\n    self.handle_connection_success(conn)\n", "label": 0}
{"function": "\n\ndef find_by(self, finder, selector, original_find=None, original_query=None):\n    elements = None\n    end_time = (time.time() + self.wait_time)\n    func_name = getattr(getattr(finder, _meth_func), _func_name)\n    find_by = (original_find or func_name[(func_name.rfind('_by_') + 4):])\n    query = (original_query or selector)\n    while (time.time() < end_time):\n        try:\n            elements = finder(selector)\n            if (not isinstance(elements, list)):\n                elements = [elements]\n        except NoSuchElementException:\n            pass\n        if elements:\n            return ElementList([self.element_class(element, self) for element in elements], find_by=find_by, query=query)\n    return ElementList([], find_by=find_by, query=query)\n", "label": 0}
{"function": "\n\ndef get_version():\n    INIT = os.path.abspath(os.path.join(HERE, '../psutil/__init__.py'))\n    with open(INIT, 'r') as f:\n        for line in f:\n            if line.startswith('__version__'):\n                ret = eval(line.strip().split(' = ')[1])\n                assert (ret.count('.') == 2), ret\n                for num in ret.split('.'):\n                    assert num.isdigit(), ret\n                return ret\n        else:\n            raise ValueError(\"couldn't find version string\")\n", "label": 0}
{"function": "\n\ndef visit_Assign(self, node):\n    is_name = (lambda n: isinstance(n, ast.Name))\n    for name in filter(is_name, node.targets):\n        if ((name.id == '__all__') and isinstance(node.value, ast.List)):\n            for subnode in node.value.elts:\n                if isinstance(subnode, ast.Str):\n                    self._tree.add_explicit_export(subnode.s, 1.2)\n        elif (not name.id.startswith('_')):\n            self._tree.add(name.id, 1.1)\n", "label": 0}
{"function": "\n\ndef valid(self, order=None):\n    '\\n        Can do complex validation about whether or not this option is valid.\\n        For example, may check to see if the recipient is in an allowed country\\n        or location.\\n        '\n    if order:\n        try:\n            for item in order.orderitem_set.all():\n                p = item.product\n                price = self.carrier.price(p)\n        except ProductShippingPriceException:\n            return False\n    elif self.cart:\n        try:\n            price = self.cost()\n        except ProductShippingPriceException:\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef test_compute_centroid_quantile(self, empty_tdigest, example_centroids):\n    empty_tdigest.C = example_centroids\n    empty_tdigest.n = 4\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 1.1)]) == (((1 / 2.0) + 0) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[(- 0.5)]) == (((1 / 2.0) + 1) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[0.1]) == (((1 / 2.0) + 2) / 4))\n    assert (empty_tdigest._compute_centroid_quantile(example_centroids[1.5]) == (((1 / 2.0) + 3) / 4))\n", "label": 0}
{"function": "\n\ndef cycle(self):\n    '\\n        Perform full thermostat cycle and return state.\\n        '\n    units = cherrypy.config['units']\n    retry_count = cherrypy.config['retry_count']\n    retry_delay = cherrypy.config['retry_delay']\n    envcontroller = cherrypy.config['envcontroller']\n    thermometer = cherrypy.config['thermometer']\n    thermostat = cherrypy.config['thermostat']\n    (current_heat, current_cool) = envcontroller.get_power_levels()\n    for i in range(retry_count):\n        try:\n            current_temp = thermometer.get_temperature(units=units)\n            break\n        except braubuddy.thermometer.ReadError as err:\n            cherrypy.log.error(err.message)\n            time.sleep(retry_delay)\n    else:\n        cherrypy.request.app.log.error('Unable to collect temperature after {0} tries'.format(retry_count))\n        return False\n    (required_heat, required_cool) = thermostat.get_required_state(current_temp, current_heat, current_cool, units=units)\n    envcontroller.set_heater_level(required_heat)\n    envcontroller.set_cooler_level(required_cool)\n    target = thermostat.target\n    for (name, output) in cherrypy.request.app.config['outputs'].iteritems():\n        try:\n            output.publish_status(target, current_temp, current_heat, current_cool)\n        except braubuddy.output.OutputError as err:\n            cherrypy.log.error(err.message)\n    return True\n", "label": 0}
{"function": "\n\ndef www_authenticate(realm, key, algorithm='MD5', nonce=None, qop=qop_auth, stale=False):\n    'Constructs a WWW-Authenticate header for Digest authentication.'\n    if (qop not in valid_qops):\n        raise ValueError((\"Unsupported value for qop: '%s'\" % qop))\n    if (algorithm not in valid_algorithms):\n        raise ValueError((\"Unsupported value for algorithm: '%s'\" % algorithm))\n    if (nonce is None):\n        nonce = synthesize_nonce(realm, key)\n    s = ('Digest realm=\"%s\", nonce=\"%s\", algorithm=\"%s\", qop=\"%s\"' % (realm, nonce, algorithm, qop))\n    if stale:\n        s += ', stale=\"true\"'\n    return s\n", "label": 0}
{"function": "\n\ndef __init__(self, channel, body_value, properties=None, auto_id=False, opinionated=False):\n    'Create a new instance of the Message object.'\n    super(Message, self).__init__(channel, 'Message')\n    self.properties = (properties or {\n        \n    })\n    if isinstance(body_value, memoryview):\n        self.body = bytes(body_value)\n    else:\n        self.body = self._auto_serialize(body_value)\n    if ((opinionated or auto_id) and ('message_id' not in self.properties)):\n        if auto_id:\n            raise DeprecationWarning('Use opinionated instead of auto_id')\n        self._add_auto_message_id()\n    if opinionated:\n        if ('timestamp' not in self.properties):\n            self._add_timestamp()\n    if ('timestamp' in self.properties):\n        self.properties['timestamp'] = self._as_datetime(self.properties['timestamp'])\n    if self._invalid_properties:\n        msg = ('Invalid property: %s' % self._invalid_properties[0])\n        raise KeyError(msg)\n", "label": 1}
{"function": "\n\ndef delete_color(self, color, palette_type, palette_name):\n    'Delete color.'\n    if (palette_type == '__special__'):\n        if (palette_name == 'Favorites'):\n            favs = util.get_favs()['colors']\n            if (color in favs):\n                favs.remove(color)\n                util.save_palettes(favs, favs=True)\n                self.show_colors(palette_type, palette_name, delete=True, update=False)\n    elif (palette_type in ('__global__', '__project__')):\n        if (palette_type == '__global__'):\n            color_palettes = util.get_palettes()\n        else:\n            color_palettes = util.get_project_palettes(self.view.window())\n        for palette in color_palettes:\n            if (palette_name == palette['name']):\n                if (color in palette['colors']):\n                    palette['colors'].remove(color)\n                    if (palette_type == '__global__'):\n                        util.save_palettes(color_palettes)\n                    else:\n                        util.save_project_palettes(self.view.window(), color_palettes)\n                    self.show_colors(palette_type, palette_name, delete=True, update=False)\n                    break\n", "label": 1}
{"function": "\n\ndef downloadMovie(self, url, path, title, extension):\n    if (not os.path.exists(path)):\n        common.log('Path does not exist')\n        return None\n    if (title == ''):\n        common.log('No title given')\n        return None\n    file_path = xbmc.makeLegalFilename(os.path.join(path, (title + extension)))\n    file_path = urllib.unquote_plus(file_path)\n    if os.path.isfile(file_path):\n        self.pDialog = xbmcgui.Dialog()\n        if (not common.ask(('File already exists. Overwrite?\\n' + os.path.basename(file_path)))):\n            title = common.showOSK(urllib.unquote_plus(title), common.translate(30102))\n            if (not title):\n                return None\n            file_path = xbmc.makeLegalFilename(os.path.join(path, (title + extension)))\n            file_path = urllib.unquote_plus(file_path)\n    success = self.__download(url, file_path)\n    if success:\n        return file_path\n    else:\n        return None\n", "label": 0}
{"function": "\n\n@classmethod\ndef _is_socket(cls, stream):\n    'Check if the given stream is a socket.'\n    try:\n        fd = stream.fileno()\n    except ValueError:\n        return False\n    sock = socket.fromfd(fd, socket.AF_INET, socket.SOCK_RAW)\n    try:\n        sock.getsockopt(socket.SOL_SOCKET, socket.SO_TYPE)\n    except socket.error as ex:\n        if (ex.args[0] != errno.ENOTSOCK):\n            return True\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef set_visible_views(self, indices, data, viewport):\n    view_opts = self.view_opts\n    (new, remaining, old) = self.recycleview.view_adapter.set_visible_views(indices, data, view_opts)\n    remove = self.remove_widget\n    view_indices = self.view_indices\n    for (_, widget) in old:\n        remove(widget)\n        del view_indices[widget]\n    refresh_view_layout = self.refresh_view_layout\n    for (index, widget) in new:\n        opt = view_opts[index]\n        refresh_view_layout(index, opt['pos'], opt['pos_hint'], opt['size'], opt['size_hint'], widget, viewport)\n    add = self.add_widget\n    for (index, widget) in new:\n        view_indices[widget] = index\n        if (widget.parent is None):\n            add(widget)\n    changed = False\n    for (index, widget) in new:\n        opt = view_opts[index]\n        if (changed or ((widget.size == opt['size']) and (widget.size_hint == opt['size_hint']) and (widget.pos_hint == opt['pos_hint']))):\n            continue\n        changed = True\n    if changed:\n        self._size_needs_update = True\n        self.recycleview.refresh_from_layout(view_size=True)\n", "label": 1}
{"function": "\n\ndef _get_parent(self, path, ref, parent_type):\n    ' Return the path of the parent of type \"parent_type\" for the object\\n        in \"path\" with id \"ref\". Returns an empty string if no parent extists.\\n        '\n    parts = path.split('/')\n    if ((parent_type.lower() == 'block') or (parts[(- 4)] == (parent_type.lower() + 's'))):\n        return '/'.join(parts[:(- 2)])\n    object_folder = parts[(- 2)]\n    parent_folder = parts[(- 4)]\n    if (parent_folder in ('recordingchannels', 'units')):\n        block_path = '/'.join(parts[:(- 6)])\n    else:\n        block_path = '/'.join(parts[:(- 4)])\n    if (parent_type.lower() in ('recordingchannel', 'unit')):\n        path = (block_path + '/recordingchannelgroups')\n        for n in self._data.iterNodes(path):\n            if (not ('_type' in n._v_attrs)):\n                continue\n            p = self._search_parent(('%s/%ss' % (n._v_pathname, parent_type.lower())), object_folder, ref)\n            if (p != ''):\n                return p\n        return ''\n    if (parent_type.lower() == 'segment'):\n        path = (block_path + '/segments')\n    elif (parent_type.lower() in ('recordingchannelgroup', 'recordingchannelgroups')):\n        path = (block_path + '/recordingchannelgroups')\n    else:\n        return ''\n    return self._search_parent(path, object_folder, ref)\n", "label": 1}
{"function": "\n\n@app.route('/signin', methods=['GET', 'POST'])\ndef signin():\n    'Sign in'\n    form = SignInForm()\n    if form.validate_on_submit():\n        username = form.username.data.strip()\n        password = form.password.data.strip()\n        if is_valid_login(username, password):\n            session['theme'] = _get_users_theme(username)\n            session['logged_in'] = True\n            session['username'] = username\n            if form.remember_me.data:\n                session.permanent = True\n            else:\n                session.permanent = False\n            if (request.args.get('next') == 'signin'):\n                return redirect('/')\n            else:\n                return redirect((request.args.get('next') or request.referrer or '/'))\n        else:\n            return render_template('signin.html', form=form, error='Failed')\n    return render_template('signin.html', form=form, error=None)\n", "label": 0}
{"function": "\n\ndef format_text_table(table):\n    col_width = [max((len(str(x)) for x in col)) for col in zip(*table)]\n    output = []\n    for row in table:\n        inner = ' | '.join(('{0:{1}}'.format(x, col_width[i]) for (i, x) in enumerate(row)))\n        output.append('| {0} |'.format(inner))\n    return output\n", "label": 0}
{"function": "\n\n@csrf_protect_m\n@filter_hook\ndef get(self, request, *args, **kwargs):\n    \"\\n        The 'change list' admin view for this model.\\n        \"\n    response = self.get_result_list()\n    if response:\n        return response\n    context = self.get_context()\n    context.update((kwargs or {\n        \n    }))\n    response = self.get_response(context, *args, **kwargs)\n    return (response or TemplateResponse(request, (self.object_list_template or self.get_template_list('views/model_list.html')), context))\n", "label": 0}
{"function": "\n\ndef get_mapping(self, cls, no_mapping_ok=False):\n    db = None\n    for candidate_cls in getmro(cls):\n        db = self.mapping.get(candidate_cls, None)\n        if (db is not None):\n            break\n    if (db is None):\n        db = self.default_database\n    if (db is None):\n        if no_mapping_ok:\n            return None\n        raise ValueError(('There is no database mapping for %s' % repr(cls)))\n    return db\n", "label": 0}
{"function": "\n\ndef get_localhost_ip():\n    cmd = ['/sbin/ifconfig']\n    eth_ip = {\n        \n    }\n    try:\n        r = exec_command(cmd)\n    except:\n        current_app.logger.error('[Dial Helpers]: exec_command error: %s:%s', cmd, sys.exc_info()[1])\n        return False\n    if (r['return_code'] == 0):\n        r_data = r['stdout'].split('\\n')\n        for (index, line) in enumerate(r_data):\n            if line.startswith('inet addr:'):\n                eth_ip[r_data[(index - 1)].split()[0]] = line.split().split(':')[1]\n    else:\n        current_app.logger.error('[Dial Helpers]: exec_command return: %s:%s:%s', cmd, r['return_code'], r['stderr'])\n        return False\n    return eth_ip\n", "label": 0}
{"function": "\n\ndef windows_shell(chan):\n    import threading\n    stdout.write('*** Emulating terminal on Windows; press F6 or Ctrl+Z then enter to send EOF,\\r\\nor at the end of the execution.\\r\\n')\n    stdout.flush()\n    out_lock = threading.RLock()\n\n    def write(recv, std):\n        while True:\n            data = recv(256)\n            if (not data):\n                if std:\n                    with out_lock:\n                        stdout.write('\\r\\n*** EOF reached; (press F6 or ^Z then enter to end)\\r\\n')\n                        stdout.flush()\n                break\n            stream = [stderr_bytes, stdout_bytes][std]\n            with out_lock:\n                stream.write(data)\n                stream.flush()\n    threading.Thread(target=write, args=(chan.recv, True)).start()\n    threading.Thread(target=write, args=(chan.recv_stderr, False)).start()\n    try:\n        while True:\n            d = stdin_bytes.read(1)\n            if (not d):\n                chan.shutdown_write()\n                break\n            try:\n                chan.send(d)\n            except socket.error:\n                break\n    except EOFError:\n        pass\n", "label": 0}
{"function": "\n\ndef test_push_selects_groupby(self):\n    'Test pushing selections through groupby.'\n    lp = StoreTemp('OUTPUT', Select(expression.LTEQ(AttRef('c'), AttRef('a')), Select(expression.LTEQ(AttRef('b'), AttRef('c')), GroupBy([AttIndex(1), AttIndex(2), AttIndex(0)], [expression.COUNTALL()], Scan(self.x_key, self.x_scheme)))))\n    expected = collections.Counter([(b, c, a) for (a, b, c) in self.x_data if ((c <= a) and (b <= c))])\n    expected = collections.Counter(((k + (v,)) for (k, v) in expected.items()))\n    self.assertEquals(self.get_count(lp, Select), 2)\n    self.assertEquals(self.get_count(lp, Scan), 1)\n    self.assertIsInstance(lp.input, Select)\n    pp = self.logical_to_physical(lp)\n    self.assertIsInstance(pp.input, MyriaSplitConsumer)\n    self.assertIsInstance(pp.input.input.input, GroupBy)\n    self.assertEquals(self.get_count(pp, Select), 1)\n    self.db.evaluate(pp)\n    result = self.db.get_temp_table('OUTPUT')\n    self.assertEquals(result, expected)\n", "label": 0}
{"function": "\n\ndef _get_project_attribute_data(row, project_attribute_key_data):\n    name = project_attribute_key_data['name']\n    data_type = project_attribute_key_data['data_type']\n    project_attribute_data = {\n        'key': project_attribute_key_data['id'],\n    }\n    if (data_type == 'BOOLEAN'):\n        project_attribute_data['boolean_value'] = (row[name] == 'True')\n    elif (data_type == 'CHAR'):\n        project_attribute_data['char_value'] = row[name]\n    elif (data_type == 'DATE'):\n        project_attribute_data['date_value'] = row[name]\n    elif (data_type == 'DATETIME'):\n        dt = datetime.strptime(row[name], '%Y-%m-%dT%H:%M:%S%z')\n        project_attribute_data['datetime_value'] = row[name]\n    elif (data_type == 'FLOAT'):\n        project_attribute_data['float_value'] = float(row[name])\n    elif (data_type == 'INTEGER'):\n        project_attribute_data['integer_value'] = int(row[name])\n    else:\n        raise NotImplementedError\n    return project_attribute_data\n", "label": 0}
{"function": "\n\ndef get_tokens_unprocessed(self, text):\n    bashlexer = BashLexer(**self.options)\n    pos = 0\n    curcode = ''\n    insertions = []\n    for match in line_re.finditer(text):\n        line = match.group()\n        m = re.match('^((?:\\\\(\\\\S+\\\\))?(?:|sh\\\\S*?|\\\\w+\\\\S+[@:]\\\\S+(?:\\\\s+\\\\S+)?|\\\\[\\\\S+[@:][^\\\\n]+\\\\].+)[$#%])(.*\\\\n?)', line)\n        if m:\n            if (not insertions):\n                pos = match.start()\n            insertions.append((len(curcode), [(0, Generic.Prompt, m.group(1))]))\n            curcode += m.group(2)\n        elif line.startswith('>'):\n            insertions.append((len(curcode), [(0, Generic.Prompt, line[:1])]))\n            curcode += line[1:]\n        else:\n            if insertions:\n                toks = bashlexer.get_tokens_unprocessed(curcode)\n                for (i, t, v) in do_insertions(insertions, toks):\n                    (yield ((pos + i), t, v))\n            (yield (match.start(), Generic.Output, line))\n            insertions = []\n            curcode = ''\n    if insertions:\n        for (i, t, v) in do_insertions(insertions, bashlexer.get_tokens_unprocessed(curcode)):\n            (yield ((pos + i), t, v))\n", "label": 0}
{"function": "\n\ndef indication(self, server, pdu):\n    if _debug:\n        UDPMultiplexer._debug('indication %r %r', server, pdu)\n    if (pdu.pduDestination.addrType == Address.localBroadcastAddr):\n        dest = self.addrBroadcastTuple\n        if _debug:\n            UDPMultiplexer._debug('    - requesting local broadcast: %r', dest)\n    elif (pdu.pduDestination.addrType == Address.localStationAddr):\n        dest = unpack_ip_addr(pdu.pduDestination.addrAddr)\n        if _debug:\n            UDPMultiplexer._debug('    - requesting local station: %r', dest)\n    else:\n        raise RuntimeError('invalid destination address type')\n    self.directPort.indication(PDU(pdu, destination=dest))\n", "label": 0}
{"function": "\n\ndef _choose(old_style, new_style):\n    family = distrib_family()\n    if (family == 'debian'):\n        distrib = distrib_id()\n        at_least_trusty = ((distrib == 'Ubuntu') and (V(distrib_release()) >= V('14.04')))\n        at_least_jessie = ((distrib == 'Debian') and (V(distrib_release()) >= V('8.0')))\n        if (at_least_trusty or at_least_jessie):\n            return new_style\n        else:\n            return old_style\n    else:\n        raise UnsupportedFamily(supported=['debian'])\n", "label": 0}
{"function": "\n\ndef startTag(self, namespace, name, attrs):\n    assert ((namespace is None) or isinstance(namespace, string_types)), type(namespace)\n    assert isinstance(name, string_types), type(name)\n    assert all(((((namespace is None) or isinstance(namespace, string_types)) and isinstance(name, string_types) and isinstance(value, string_types)) for ((namespace, name), value) in attrs.items()))\n    return {\n        'type': 'StartTag',\n        'name': text_type(name),\n        'namespace': to_text(namespace),\n        'data': dict((((to_text(namespace, False), to_text(name)), to_text(value, False)) for ((namespace, name), value) in attrs.items())),\n    }\n", "label": 0}
{"function": "\n\n@lower_cast(types.BaseTuple, types.BaseTuple)\ndef tuple_to_tuple(context, builder, fromty, toty, val):\n    if (isinstance(fromty, types.BaseNamedTuple) or isinstance(toty, types.BaseNamedTuple)):\n        raise NotImplementedError\n    if (len(fromty) != len(toty)):\n        raise NotImplementedError\n    olditems = cgutils.unpack_tuple(builder, val, len(fromty))\n    items = [context.cast(builder, v, f, t) for (v, f, t) in zip(olditems, fromty, toty)]\n    return context.make_tuple(builder, toty, items)\n", "label": 0}
{"function": "\n\ndef stderr_to_parser_error(parse_args, *args, **kwargs):\n    if (isinstance(sys.stderr, StdIOBuffer) or isinstance(sys.stdout, StdIOBuffer)):\n        return parse_args(*args, **kwargs)\n    old_stdout = sys.stdout\n    old_stderr = sys.stderr\n    sys.stdout = StdIOBuffer()\n    sys.stderr = StdIOBuffer()\n    try:\n        try:\n            result = parse_args(*args, **kwargs)\n            for key in list(vars(result)):\n                if (getattr(result, key) is sys.stdout):\n                    setattr(result, key, old_stdout)\n                if (getattr(result, key) is sys.stderr):\n                    setattr(result, key, old_stderr)\n            return result\n        except SystemExit:\n            code = sys.exc_info()[1].code\n            stdout = sys.stdout.getvalue()\n            stderr = sys.stderr.getvalue()\n            raise ArgumentParserError('SystemExit', stdout, stderr, code)\n    finally:\n        sys.stdout = old_stdout\n        sys.stderr = old_stderr\n", "label": 0}
{"function": "\n\ndef get(self, name, default=None):\n    '\\n            Get a Python-attribute of this item instance, falls back\\n            to the same attribute in the parent item if not set in\\n            this instance, used to inherit attributes to components\\n\\n            @param name: the attribute name\\n        '\n    if (name in self.__dict__):\n        value = self.__dict__[name]\n    else:\n        value = None\n    if (value is not None):\n        return value\n    if (name[:2] == '__'):\n        raise AttributeError\n    parent = self.parent\n    if (parent is not None):\n        return parent.get(name)\n    return default\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    try:\n        message = record.getMessage()\n        assert isinstance(message, basestring_type)\n        record.message = _safe_unicode(message)\n    except Exception as e:\n        record.message = ('Bad message (%r): %r' % (e, record.__dict__))\n    record.asctime = self.formatTime(record, self.datefmt)\n    if (record.levelno in self._colors):\n        record.color = self._colors[record.levelno]\n        record.end_color = self._normal\n    else:\n        record.color = record.end_color = ''\n    formatted = (self._fmt % record.__dict__)\n    if record.exc_info:\n        if (not record.exc_text):\n            record.exc_text = self.formatException(record.exc_info)\n    if record.exc_text:\n        lines = [formatted.rstrip()]\n        lines.extend((_safe_unicode(ln) for ln in record.exc_text.split('\\n')))\n        formatted = '\\n'.join(lines)\n    return formatted.replace('\\n', '\\n    ')\n", "label": 0}
{"function": "\n\ndef next3Fixtures(self, type_return='string'):\n    now = datetime.datetime.now()\n    url = (('http://www.premierleague.com/en-gb/matchday/league-table.html?season=2015-2016&month=' + months[now.month]) + '&timelineView=date&toDate=1451433599999&tableView=NEXT_3_FIXTURES')\n    team_names = soup(template='.next3FixturesTable')\n    for i in range(len(team_names)):\n        team_names[i] = str(team_names[i].text)\n    next_3_fixtures = soup.select('.club-row .col-fixture')\n    for i in range(len(next_3_fixtures)):\n        next_3_fixtures[i] = str(next_3_fixtures[i].text)\n    return_dict = {\n        \n    }\n    for i in range(len(team_names)):\n        return_dict[team_names[i]] = next_3_fixtures[i]\n    if (type_return == 'dict'):\n        return return_dict\n    return str(return_dict)\n", "label": 0}
{"function": "\n\n@defer.inlineCallbacks\ndef test_SpecifiedFields(self):\n    (yield self.coll.insert([dict(((k, v) for k in 'abcdefg')) for v in range(5)], safe=True))\n    res = (yield self.coll.find(fields={\n        'a': 1,\n        'c': 1,\n    }))\n    (yield self.coll.count(fields={\n        'a': 1,\n        'c': 1,\n    }))\n    self.assertTrue(all(((x in ['a', 'c', '_id']) for x in res[0].keys())))\n    res = (yield self.coll.find(fields=['a', 'c']))\n    (yield self.coll.count(fields=['a', 'c']))\n    self.assertTrue(all(((x in ['a', 'c', '_id']) for x in res[0].keys())))\n    res = (yield self.coll.find(fields=[]))\n    (yield self.coll.count(fields=[]))\n    self.assertTrue(all(((x in ['_id']) for x in res[0].keys())))\n    (yield self.assertFailure(self.coll.find({\n        \n    }, fields=[1]), TypeError))\n", "label": 0}
{"function": "\n\ndef handle_noargs(self, *args, **options):\n    r = get_r()\n    try:\n        keys = r.r.smembers(r._metric_slugs_key)\n        slugs = set((s.split(':')[1] for s in keys))\n        r.r.srem(r._metric_slugs_key, *keys)\n        r.r.sadd(r._metric_slugs_key, *slugs)\n        p = '\\nMetrics: Converted {0} Keys to {1} Slugs\\n'\n        self.stdout.write(p.format(len(keys), len(slugs)))\n    except (IndexError, ResponseError):\n        pass\n    try:\n        keys = r.r.smembers(r._gauge_slugs_key)\n        slugs = set((s.split(':')[1] for s in keys))\n        r.r.srem(r._gauge_slugs_key, *keys)\n        r.r.sadd(r._gauge_slugs_key, *slugs)\n        p = 'Gauges: Converted {0} Keys to {1} Slugs\\n'\n        self.stdout.write(p.format(len(keys), len(slugs)))\n    except (IndexError, ResponseError):\n        pass\n    i = 0\n    categories = r.categories()\n    for category in categories:\n        try:\n            k = r._category_key(category)\n            data = r.r.get(k)\n            if data:\n                data = json.loads(data)\n                r.r.delete(k)\n                r.r.sadd(k, *set(data))\n                i += 1\n        except ResponseError:\n            pass\n    if (i > 0):\n        p = 'Converted {0} Categories from JSON -> Redis Sets\\n'\n        self.stdout.write(p.format(i))\n", "label": 0}
{"function": "\n\ndef __init__(self, env, name=None, version=None):\n    'Create a new page object or retrieves an existing page.\\n\\n        :param env: an `Environment` object.\\n        :param name: the page name or a `Resource` object.\\n        :param version: the page version. The value takes precedence over the\\n                        `Resource` version when both are specified.\\n        '\n    self.env = env\n    if version:\n        try:\n            version = int(version)\n        except ValueError:\n            version = None\n    if isinstance(name, Resource):\n        resource = name\n        name = resource.id\n        if ((version is None) and (resource.version is not None)):\n            try:\n                version = int(resource.version)\n            except ValueError:\n                version = None\n    self.name = name\n    self._resource_version = version\n    if name:\n        self._fetch(name, version)\n    else:\n        self.version = 0\n        self.text = self.comment = self.author = ''\n        self.time = None\n        self.readonly = 0\n    self.old_text = self.text\n    self.old_readonly = self.readonly\n", "label": 0}
{"function": "\n\ndef __call__(self, value):\n    if (value is None):\n        return None\n    try:\n        p = value.split(':', 2)\n        _60 = Duration._60\n        _unsigned = Duration._unsigned\n        if (len(p) == 1):\n            result = _unsigned(p[0])\n        if (len(p) == 2):\n            result = ((60 * _unsigned(p[0])) + _60(p[1]))\n        if (len(p) == 3):\n            result = (((3600 * _unsigned(p[0])) + (60 * _60(p[1]))) + _60(p[2]))\n    except ValueError:\n        raise ValueError('Invalid duration value: %s', value)\n    return result\n", "label": 0}
{"function": "\n\ndef returner(ret):\n    '\\n    Return data to a mongodb server\\n    '\n    (conn, mdb) = _get_conn(ret)\n    col = mdb[ret['id']]\n    if isinstance(ret['return'], dict):\n        back = _remove_dots(ret['return'])\n    else:\n        back = ret['return']\n    if isinstance(ret, dict):\n        full_ret = _remove_dots(ret)\n    else:\n        full_ret = ret\n    log.debug(back)\n    sdata = {\n        'minion': ret['id'],\n        'jid': ret['jid'],\n        'return': back,\n        'fun': ret['fun'],\n        'full_ret': full_ret,\n    }\n    if ('out' in ret):\n        sdata['out'] = ret['out']\n    if (float(version) > 2.3):\n        mdb.saltReturns.insert_one(sdata.copy())\n    else:\n        mdb.saltReturns.insert(sdata.copy())\n", "label": 0}
{"function": "\n\ndef _get_position_ref_node(self, instance):\n    if self.is_sorted:\n        position = 'sorted-child'\n        node_parent = instance.get_parent()\n        if node_parent:\n            ref_node_id = node_parent.pk\n        else:\n            ref_node_id = ''\n    else:\n        prev_sibling = instance.get_prev_sibling()\n        if prev_sibling:\n            position = 'right'\n            ref_node_id = prev_sibling.pk\n        else:\n            position = 'first-child'\n            if instance.is_root():\n                ref_node_id = ''\n            else:\n                ref_node_id = instance.get_parent().pk\n    return {\n        '_ref_node_id': ref_node_id,\n        '_position': position,\n    }\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _parse_rule_sections(parser, config):\n    sections = [section for section in parser.sections() if (section != 'general')]\n    for rule_name in sections:\n        for (option_name, option_value) in parser.items(rule_name):\n            config.set_rule_option(rule_name, option_name, option_value)\n", "label": 0}
{"function": "\n\ndef DownloadActivity(self, serviceRecord, activity):\n    workoutID = activity.ServiceData['WorkoutID']\n    logger.debug(('DownloadActivity for %s' % workoutID))\n    session = self._get_session(record=serviceRecord)\n    resp = session.get((self._urlRoot + ('/api/workout/%d' % workoutID)))\n    try:\n        res = resp.json()\n    except ValueError:\n        raise APIException(('Parse failure in Motivato activity (%d) download: %s' % (workoutID, res.text)))\n    lap = Lap(stats=activity.Stats, startTime=activity.StartTime, endTime=activity.EndTime)\n    activity.Laps = [lap]\n    activity.GPS = False\n    if (('track' in res) and ('points' in res['track'])):\n        for pt in res['track']['points']:\n            wp = Waypoint()\n            if ('moment' not in pt):\n                continue\n            wp.Timestamp = self._parseDateTime(pt['moment'])\n            if ((('lat' in pt) and ('lon' in pt)) or ('ele' in pt)):\n                wp.Location = Location()\n                if (('lat' in pt) and ('lon' in pt)):\n                    wp.Location.Latitude = pt['lat']\n                    wp.Location.Longitude = pt['lon']\n                    activity.GPS = True\n                if ('ele' in pt):\n                    wp.Location.Altitude = float(pt['ele'])\n            if ('bpm' in pt):\n                wp.HR = pt['bpm']\n            lap.Waypoints.append(wp)\n    activity.Stationary = (len(lap.Waypoints) == 0)\n    return activity\n", "label": 1}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_content():\n        self.set_content(x.content())\n    if x.has_statuscode():\n        self.set_statuscode(x.statuscode())\n    for i in xrange(x.header_size()):\n        self.add_header().CopyFrom(x.header(i))\n    if x.has_contentwastruncated():\n        self.set_contentwastruncated(x.contentwastruncated())\n    if x.has_externalbytessent():\n        self.set_externalbytessent(x.externalbytessent())\n    if x.has_externalbytesreceived():\n        self.set_externalbytesreceived(x.externalbytesreceived())\n    if x.has_finalurl():\n        self.set_finalurl(x.finalurl())\n", "label": 0}
{"function": "\n\ndef _parse_request_line(self, line):\n    request_line = _read_request_line_dict(line)\n    if (not request_line):\n        return\n    uri = urlparse(request_line['uri'])\n    if uri.scheme:\n        self.request.protocol = uri.scheme\n    if uri.netloc:\n        if (':' in uri.netloc):\n            (self.request.host, self.request.port) = uri.netloc.split(':')\n        else:\n            self.request.host = uri.netloc\n    if uri.port:\n        self.request.port = uri.port\n    if uri.path:\n        self.request.path = uri.path\n    if uri.query:\n        query = parse_qs(uri.query)\n        for key in query:\n            self.request.query[key] = query[key]\n    if ('method' in request_line):\n        self.request.method = request_line['method']\n", "label": 1}
{"function": "\n\ndef snipt_page(request, user, snipt_id):\n    try:\n        snipt = Snippet.objects.get(slug=snipt_id)\n        if ('c' in request.GET):\n            return HttpResponseRedirect(((snipt.get_absolute_url() + '#comment-') + request.GET['c']))\n    except:\n        return HttpResponseRedirect(((('/' + user) + '/tag/') + snipt_id))\n    context_user = User.objects.get(id=snipt.user.id)\n    if (request.user.id == context_user.id):\n        mine = True\n    else:\n        mine = False\n    if ((not snipt.public) and (not mine)):\n        try:\n            if (request.GET['key'] != snipt.key):\n                raise Http404()\n            else:\n                key = True\n        except:\n            raise Http404()\n    disable_wrap = request.session.get('disable_wrap')\n    disable_message = request.session.get('disable_message')\n    return render_to_response('snipt.html', locals(), context_instance=RequestContext(request))\n", "label": 0}
{"function": "\n\ndef test_field_checks(self):\n\n    class InvalidListCharModel(TemporaryModel):\n        field = ListCharField(models.CharField(), max_length=32)\n    errors = InvalidListCharModel.check(actually_check=True)\n    assert (len(errors) == 1)\n    assert (errors[0].id == 'django_mysql.E004')\n    assert ('Base field for list has errors' in errors[0].msg)\n    assert ('max_length' in errors[0].msg)\n", "label": 0}
{"function": "\n\ndef load_certificate_request(type, buffer):\n    '\\n    Load a certificate request from a buffer\\n\\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\\n    :param buffer: The buffer the certificate request is stored in\\n    :return: The X509Req object\\n    '\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if (type == FILETYPE_PEM):\n        req = _lib.PEM_read_bio_X509_REQ(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif (type == FILETYPE_ASN1):\n        req = _lib.d2i_X509_REQ_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if (req == _ffi.NULL):\n        _raise_current_error()\n    x509req = X509Req.__new__(X509Req)\n    x509req._req = _ffi.gc(req, _lib.X509_REQ_free)\n    return x509req\n", "label": 0}
{"function": "\n\ndef test_export_xls():\n    headers = ['x', 'y']\n    data = [['1', '2'], ['3', '4'], ['5,6', '7'], [None, None]]\n    sheet = ([headers] + data)\n    generator = create_generator(content_generator(headers, data), 'xls')\n    response = make_response(generator, 'xls', 'foo')\n    assert_equal('application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', response['content-type'])\n    expected_data = [[(((cell is not None) and cell) or 'NULL') for cell in row] for row in sheet]\n    sheet_data = _read_xls_sheet_data(response)\n    assert_equal(expected_data, sheet_data)\n    assert_equal('attachment; filename=foo.xlsx', response['content-disposition'])\n", "label": 0}
{"function": "\n\ndef createVM(request_id, hostname, recipe, updateProgress):\n    configParser = configparser.RawConfigParser()\n    configFilePath = '/opt/chef-tools/createvm/createvm.config'\n    configParser.read(configFilePath)\n    subdomain = configParser.get('cocreate_config', 'subdomain')\n    progress = 0\n    validHostname = checkHostname(hostname)\n    if (validHostname == False):\n        return (None, None, 'Invalid hostname', progress)\n    fqdn = ((hostname + '.') + subdomain)\n    validRecipe = checkRecipe(configParser, recipe)\n    if (validRecipe == False):\n        return (None, None, 'Unsupported template', progress)\n    updateProgress(request_id, progress, 'Beginning VM template cloning')\n    try:\n        cloneVM(configParser, hostname)\n    except subprocess.CalledProcessError:\n        print('A cloning error occurred')\n        return (None, None, 'VM cloning failed', progress)\n    progress = 33\n    updateProgress(request_id, progress, 'Waiting for new VM IP address')\n    ipAddress = None\n    try:\n        ipAddress = getIP(configParser, hostname)\n    except:\n        print('Could not get IP Address')\n        return (None, None, 'Could not obtain VM IP address', progress)\n    progress = 67\n    updateProgress(request_id, progress, 'Beginning VM bootstrap')\n    try:\n        bootstrapVM(configParser, ipAddress, hostname, recipe)\n    except subprocess.CalledProcessError:\n        print('An error occurred during bootstrap')\n        return (None, None, 'VM bootstrap failed', progress)\n    url = ((('http://' + fqdn) + '/') + recipe)\n    progress = 100\n    updateProgress(request_id, progress, 'VM creation complete', url)\n    return (ipAddress, fqdn, None, progress)\n", "label": 0}
{"function": "\n\ndef _walknode(self, node):\n    if (node.nodeType == Node.ELEMENT_NODE):\n        self.startElementNS(node.qname, node.tagName, node.attributes)\n        for c in node.childNodes:\n            self._walknode(c)\n        self.endElementNS(node.qname, node.tagName)\n    if ((node.nodeType == Node.TEXT_NODE) or (node.nodeType == Node.CDATA_SECTION_NODE)):\n        self.characters(str(node))\n", "label": 0}
{"function": "\n\ndef wait_till_stopped(self, conf, container_id, timeout=10, message=None, waiting=True):\n    'Wait till a container is stopped'\n    stopped = False\n    inspection = None\n    for _ in until(timeout=timeout, action=message):\n        try:\n            inspection = conf.harpoon.docker_context.inspect_container(container_id)\n            if (not isinstance(inspection, dict)):\n                log.error('Weird response from inspecting the container\\tresponse=%s', inspection)\n            elif (not inspection['State']['Running']):\n                stopped = True\n                conf.container_id = None\n                break\n            else:\n                break\n        except (socket.timeout, ValueError):\n            log.warning('Failed to inspect the container\\tcontainer_id=%s', container_id)\n        except DockerAPIError as error:\n            if (error.response.status_code != 404):\n                raise\n            else:\n                break\n    if (not inspection):\n        log.warning('Failed to inspect the container!')\n        stopped = True\n        exit_code = 1\n    else:\n        exit_code = inspection['State']['ExitCode']\n    return (stopped, exit_code)\n", "label": 0}
{"function": "\n\ndef post(self, *args, **kwargs):\n    if (self.request.POST.get('action') == 'invite'):\n        if (not self.domain_object.sms_mobile_worker_registration_enabled):\n            return self.get(*args, **kwargs)\n        if self.invitations_form.is_valid():\n            phone_numbers = self.invitations_form.cleaned_data.get('phone_numbers')\n            app_id = self.invitations_form.cleaned_data.get('app_id')\n            result = SelfRegistrationInvitation.initiate_workflow(self.domain, phone_numbers, app_id=app_id)\n            (success_numbers, invalid_format_numbers, numbers_in_use) = result\n            if success_numbers:\n                messages.success(self.request, (_('Invitations sent to: %(phone_numbers)s') % {\n                    'phone_numbers': ','.join(success_numbers),\n                }))\n            if invalid_format_numbers:\n                messages.error(self.request, (_('Invitations could not be sent to: %(phone_numbers)s. These number(s) are in an invalid format.') % {\n                    'phone_numbers': ','.join(invalid_format_numbers),\n                }))\n            if numbers_in_use:\n                messages.error(self.request, (_('Invitations could not be sent to: %(phone_numbers)s. These number(s) are already in use.') % {\n                    'phone_numbers': ','.join(numbers_in_use),\n                }))\n        return self.get(*args, **kwargs)\n    else:\n        if (not self.domain_object.sms_mobile_worker_registration_enabled):\n            raise Http404()\n        return self.paginate_crud_response\n", "label": 0}
{"function": "\n\ndef discover_affected_files(include_test_sources, include_scripts, project):\n    source_dir = project.get_property('dir_source_main_python')\n    files = discover_python_files(source_dir)\n    if include_test_sources:\n        if project.get_property('dir_source_unittest_python'):\n            unittest_dir = project.get_property('dir_source_unittest_python')\n            files = itertools.chain(files, discover_python_files(unittest_dir))\n        if project.get_property('dir_source_integrationtest_python'):\n            integrationtest_dir = project.get_property('dir_source_integrationtest_python')\n            files = itertools.chain(files, discover_python_files(integrationtest_dir))\n    if (include_scripts and project.get_property('dir_source_main_scripts')):\n        scripts_dir = project.get_property('dir_source_main_scripts')\n        files = itertools.chain(files, discover_files_matching(scripts_dir, '*'))\n    return files\n", "label": 0}
{"function": "\n\ndef AddLine(self, line):\n    'Adds a line of text to the block. Paragraph type is auto-determined.'\n    if self.paragraphs.IsType(paragraph.CodeBlock):\n        if line.startswith('<'):\n            self.paragraphs.Close()\n            line = line[1:].lstrip()\n            if line:\n                self.AddLine(line)\n            return\n        if (line[:1] not in ' \\t'):\n            self.paragraphs.Close()\n            self.AddLine(line)\n            return\n        self.paragraphs.AddLine(line)\n        return\n    self._ParseArgs(line)\n    if (not line.strip()):\n        self.paragraphs.SetType(paragraph.BlankLine)\n        return\n    match = regex.list_item.match((line or ''))\n    if match:\n        leader = match.group(1)\n        self.paragraphs.Close()\n        line = regex.list_item.sub('', line)\n        self.paragraphs.SetType(paragraph.ListItem, leader)\n        self.paragraphs.AddLine(line)\n        return\n    if (line and (line[:1] in ' \\t')):\n        if self.paragraphs.IsType(paragraph.ListItem):\n            self.paragraphs.AddLine(line.lstrip())\n            return\n    elif self.paragraphs.IsType(paragraph.ListItem):\n        self.paragraphs.Close()\n    self.paragraphs.SetType(paragraph.TextParagraph)\n    if ((line == '>') or line.endswith(' >')):\n        line = line[:(- 1)].rstrip()\n        if line:\n            self.paragraphs.AddLine(line)\n        self.paragraphs.SetType(paragraph.CodeBlock)\n        return\n    self.paragraphs.AddLine(line)\n", "label": 1}
{"function": "\n\ndef __init__(self, *args):\n    if (len(args) == 1):\n        (start, stop, step) = (0, args[0], 1)\n    elif (len(args) == 2):\n        (start, stop, step) = (args[0], args[1], 1)\n    elif (len(args) == 3):\n        (start, stop, step) = args\n    else:\n        raise TypeError('range() requires 1-3 int arguments')\n    try:\n        (start, stop, step) = (int(start), int(stop), int(step))\n    except ValueError:\n        raise TypeError('an integer is required')\n    if (step == 0):\n        raise ValueError('range() arg 3 must not be zero')\n    elif (step < 0):\n        stop = min(stop, start)\n    else:\n        stop = max(stop, start)\n    self._start = start\n    self._stop = stop\n    self._step = step\n    self._len = (((stop - start) // step) + bool(((stop - start) % step)))\n", "label": 0}
{"function": "\n\ndef doCapNew(self, msg):\n    if (len(msg.args) != 3):\n        log.warning('Bad CAP NEW from server: %r', msg)\n        return\n    caps = msg.args[2].split()\n    assert caps, 'Empty list of capabilities'\n    self._addCapabilities(msg.args[2])\n    if ((not self.sasl_authenticated) and ('sasl' in self.state.capabilities_ls)):\n        self.resetSasl()\n        s = self.state.capabilities_ls['sasl']\n        if (s is not None):\n            self.filterSaslMechanisms(set(s.split(',')))\n    common_supported_unrequested_capabilities = (set(self.state.capabilities_ls) & (self.REQUEST_CAPABILITIES - self.state.capabilities_ack))\n    if common_supported_unrequested_capabilities:\n        caps = ' '.join(sorted(common_supported_unrequested_capabilities))\n        self.sendMsg(ircmsgs.IrcMsg(command='CAP', args=('REQ', caps)))\n", "label": 0}
{"function": "\n\ndef _remove_client_present(self, client):\n    id_ = client.id_\n    if ((id_ is None) or (id_ not in self.present)):\n        return\n    clients = self.present[id_]\n    if (client not in clients):\n        return\n    clients.remove(client)\n    if (len(clients) == 0):\n        del self.present[id_]\n        if self.presence_events:\n            data = {\n                'new': [],\n                'lost': [id_],\n            }\n            self.event.fire_event(data, salt.utils.event.tagify('change', 'presence'))\n            data = {\n                'present': list(self.present.keys()),\n            }\n            self.event.fire_event(data, salt.utils.event.tagify('present', 'presence'))\n", "label": 0}
{"function": "\n\ndef init(allocator=drv.mem_alloc):\n    '\\n    Initialize libraries used by scikit-cuda.\\n\\n    Initialize the CUBLAS, CUSOLVER, and CULA libraries used by \\n    high-level functions provided by scikit-cuda.\\n\\n    Parameters\\n    ----------\\n    allocator : an allocator used internally by some of the high-level\\n        functions.\\n\\n    Notes\\n    -----\\n    This function does not initialize PyCUDA; it uses whatever device\\n    and context were initialized in the current host thread.\\n    '\n    global _global_cublas_handle, _global_cublas_allocator\n    if (not _global_cublas_handle):\n        from . import cublas\n        _global_cublas_handle = cublas.cublasCreate()\n    if (_global_cublas_allocator is None):\n        _global_cublas_allocator = allocator\n    global _global_cusolver_handle\n    if (not _global_cusolver_handle):\n        from . import cusolver\n        _global_cusolver_handle = cusolver.cusolverDnCreate()\n    if _has_cula:\n        cula.culaInitialize()\n    if _has_magma:\n        magma.magma_init()\n", "label": 0}
{"function": "\n\ndef init_ipython_session(argv=[], auto_symbols=False, auto_int_to_Integer=False):\n    'Construct new IPython session. '\n    import IPython\n    if (V(IPython.__version__) >= '0.11'):\n        if (V(IPython.__version__) >= '1.0'):\n            from IPython.terminal import ipapp\n        else:\n            from IPython.frontend.terminal import ipapp\n        app = ipapp.TerminalIPythonApp()\n        app.display_banner = False\n        app.initialize(argv)\n        if auto_symbols:\n            readline = import_module('readline')\n            if readline:\n                enable_automatic_symbols(app)\n        if auto_int_to_Integer:\n            enable_automatic_int_sympification(app)\n        return app.shell\n    else:\n        from IPython.Shell import make_IPython\n        return make_IPython(argv)\n", "label": 0}
{"function": "\n\ndef _add_xtra_pore_data(self):\n    xpdata = self._xtra_pore_data\n    if (xpdata is not None):\n        if isinstance(xpdata, type([])):\n            for pdata in xpdata:\n                try:\n                    self[('pore.' + pdata)] = self._dictionary[('p' + pdata)][self._pore_map]\n                except:\n                    logger.warning((('Could not add pore data: ' + pdata) + ' to network'))\n                    pass\n        else:\n            try:\n                self[('pore.' + xpdata)] = self._dictionary[('p' + xpdata)][self._pore_map]\n            except:\n                logger.warning((('Could not add pore data: ' + xpdata) + ' to network'))\n                pass\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    return ((self.version == other.version) and (self.vrid == other.vrid) and (self.priority == other.priority) and (self.ip_addresses == other.ip_addresses) and (self.advertisement_interval == other.advertisement_interval) and (self.preempt_mode == other.preempt_mode) and (self.preempt_delay == other.preempt_delay) and (self.accept_mode == other.accept_mode) and (self.is_ipv6 == other.is_ipv6))\n", "label": 0}
{"function": "\n\ndef mark_invalid_input_sequences(self):\n    \"Fill in domain knowledge about valid input\\n    sequences (e.g. don't prune failure without pruning recovery.)\\n    Only do so if this isn't a view of a previously computed DAG\"\n    fingerprint2previousfailure = {\n        \n    }\n    for event in self._events_list:\n        if hasattr(event, 'fingerprint'):\n            fingerprint = event.fingerprint[1:]\n            if (type(event) in self._failure_types):\n                fingerprint2previousfailure[fingerprint] = event\n            elif (type(event) in self._recovery_types):\n                if (fingerprint in fingerprint2previousfailure):\n                    failure = fingerprint2previousfailure[fingerprint]\n                    failure.dependent_labels.append(event.label)\n", "label": 0}
{"function": "\n\ndef unquote(string):\n    if (not string):\n        return b''\n    res = string.split(b'%')\n    if (len(res) != 1):\n        string = res[0]\n        for item in res[1:]:\n            try:\n                string += (bytes([int(item[:2], 16)]) + item[2:])\n            except ValueError:\n                string += (b'%' + item)\n    return string\n", "label": 0}
{"function": "\n\ndef uncommented_lines(filename, use_sudo=False):\n    '\\n    Get the lines of a remote file, ignoring empty or commented ones\\n    '\n    func = (run_as_root if use_sudo else run)\n    res = func(('cat %s' % quote(filename)), quiet=True)\n    if res.succeeded:\n        return [line for line in res.splitlines() if (line and (not line.startswith('#')))]\n    else:\n        return []\n", "label": 0}
{"function": "\n\ndef create(self, req, body):\n    'Create or import keypair.\\n\\n        Sending name will generate a key and return private_key\\n        and fingerprint.\\n\\n        You can send a public_key to add an existing ssh key\\n\\n        params: keypair object with:\\n            name (required) - string\\n            public_key (optional) - string\\n        '\n    context = req.environ['nova.context']\n    authorize(context, action='create')\n    try:\n        params = body['keypair']\n        name = params['name']\n    except KeyError:\n        msg = _('Invalid request body')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        if ('public_key' in params):\n            keypair = self.api.import_key_pair(context, context.user_id, name, params['public_key'])\n            keypair = self._filter_keypair(keypair, user_id=True)\n        else:\n            (keypair, private_key) = self.api.create_key_pair(context, context.user_id, name)\n            keypair = self._filter_keypair(keypair, user_id=True)\n            keypair['private_key'] = private_key\n        return {\n            'keypair': keypair,\n        }\n    except exception.KeypairLimitExceeded:\n        msg = _('Quota exceeded, too many key pairs.')\n        raise webob.exc.HTTPForbidden(explanation=msg)\n    except exception.InvalidKeypair as exc:\n        raise webob.exc.HTTPBadRequest(explanation=exc.format_message())\n    except exception.KeyPairExists as exc:\n        raise webob.exc.HTTPConflict(explanation=exc.format_message())\n", "label": 0}
{"function": "\n\ndef convertPyClassOrFunctionDefinitionToForaFunctionExpression(self, classOrFunctionDefinition, objectIdToObjectDefinition):\n    pyAst = self.convertClassOrFunctionDefinitionToNativePyAst(classOrFunctionDefinition, objectIdToObjectDefinition)\n    assert (pyAst is not None)\n    sourcePath = objectIdToObjectDefinition[classOrFunctionDefinition.sourceFileId].path\n    tr = None\n    if isinstance(classOrFunctionDefinition, TypeDescription.FunctionDefinition):\n        if (isinstance(pyAst, ForaNative.PythonAstStatement) and pyAst.isFunctionDef()):\n            tr = self.nativeConverter.convertPythonAstFunctionDefToForaOrParseError(pyAst.asFunctionDef, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]))\n        else:\n            assert pyAst.isLambda()\n            tr = self.nativeConverter.convertPythonAstLambdaToForaOrParseError(pyAst.asLambda, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]))\n    elif isinstance(classOrFunctionDefinition, TypeDescription.ClassDefinition):\n        objectIdToFreeVar = {v: k for (k, v) in classOrFunctionDefinition.freeVariableMemberAccessChainsToId.iteritems()}\n        baseClasses = [objectIdToFreeVar[baseId].split('.') for baseId in classOrFunctionDefinition.baseClassIds]\n        tr = self.nativeConverter.convertPythonAstClassDefToForaOrParseError(pyAst.asClassDef, pyAst.extent, ForaNative.CodeDefinitionPoint.ExternalFromStringList([sourcePath]), baseClasses)\n    else:\n        assert False\n    if isinstance(tr, ForaNative.PythonToForaConversionError):\n        raise convertNativePythonToForaConversionError(tr, sourcePath)\n    return tr\n", "label": 1}
{"function": "\n\ndef _walk(self, expr):\n    node = expr.op()\n    if isinstance(node, ops.TableColumn):\n        is_valid = self._validate_column(expr)\n        self.valid = (self.valid and is_valid)\n    for arg in node.flat_args():\n        if isinstance(arg, ir.ValueExpr):\n            self._walk(arg)\n", "label": 0}
{"function": "\n\n@property\ndef line_print(self):\n    inv_types = {v: k for (k, v) in defines.Types.iteritems()}\n    if (self._code is None):\n        self._code = defines.Codes.EMPTY.number\n    msg = 'From {source}, To {destination}, {type}-{mid}, {code}-{token}, ['.format(source=self._source, destination=self._destination, type=inv_types[self._type], mid=self._mid, code=defines.Codes.LIST[self._code].name, token=self._token)\n    for opt in self._options:\n        msg += '{name}: {value}, '.format(name=opt.name, value=opt.value)\n    msg += ']'\n    if (self.payload is not None):\n        msg += ' {payload}...{length} bytes'.format(payload=self.payload[0:20], length=len(self.payload))\n    else:\n        msg += ' No payload'\n    return msg\n", "label": 0}
{"function": "\n\ndef get_profile(self):\n    '\\n        Returns site-specific profile for this user. Raises\\n        SiteProfileNotAvailable if this site does not allow profiles.\\n        '\n    warnings.warn('The use of AUTH_PROFILE_MODULE to define user profiles has been deprecated.', PendingDeprecationWarning)\n    if (not hasattr(self, '_profile_cache')):\n        from django.conf import settings\n        if (not getattr(settings, 'AUTH_PROFILE_MODULE', False)):\n            raise SiteProfileNotAvailable('You need to set AUTH_PROFILE_MODULE in your project settings')\n        try:\n            (app_label, model_name) = settings.AUTH_PROFILE_MODULE.split('.')\n        except ValueError:\n            raise SiteProfileNotAvailable('app_label and model_name should be separated by a dot in the AUTH_PROFILE_MODULE setting')\n        try:\n            model = models.get_model(app_label, model_name)\n            if (model is None):\n                raise SiteProfileNotAvailable('Unable to load the profile model, check AUTH_PROFILE_MODULE in your project settings')\n            self._profile_cache = model._default_manager.using(self._state.db).get(user__id__exact=self.id)\n            self._profile_cache.user = self\n        except (ImportError, ImproperlyConfigured):\n            raise SiteProfileNotAvailable\n    return self._profile_cache\n", "label": 0}
{"function": "\n\ndef register(self, fid, event):\n    if event:\n        if (event & _AsyncPoller._Read):\n            self.rset.add(fid)\n        if (event & _AsyncPoller._Write):\n            self.wset.add(fid)\n        if (event & _AsyncPoller._Error):\n            self.xset.add(fid)\n", "label": 0}
{"function": "\n\ndef _insert_index(self, data):\n    data = data.copy()\n    idx_nlevels = data.index.nlevels\n    if (idx_nlevels == 1):\n        data.insert(0, 'Index', data.index)\n    else:\n        for i in range(idx_nlevels):\n            data.insert(i, 'Index{0}'.format(i), data.index.get_level_values(i))\n    col_nlevels = data.columns.nlevels\n    if (col_nlevels > 1):\n        col = data.columns.get_level_values(0)\n        values = [data.columns.get_level_values(i).values for i in range(1, col_nlevels)]\n        col_df = pd.DataFrame(values)\n        data.columns = col_df.columns\n        data = pd.concat([col_df, data])\n        data.columns = col\n    return data\n", "label": 0}
{"function": "\n\ndef handle(self, request, data):\n    project = self._update_project(request, data)\n    if (not project):\n        return False\n    project_id = data['project_id']\n    domain_id = getattr(project, 'domain_id', '')\n    ret = self._update_project_members(request, data, project_id)\n    if (not ret):\n        return False\n    if PROJECT_GROUP_ENABLED:\n        ret = self._update_project_groups(request, data, project_id, domain_id)\n        if (not ret):\n            return False\n    if api.keystone.is_cloud_admin(request):\n        ret = self._update_project_quota(request, data, project_id)\n        if (not ret):\n            return False\n    return True\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **kwargs):\n    valid_domains = ['@stud.ntnu.no', '@ntnu.no']\n    for campain_id in args:\n        try:\n            campain = Campaign.objects.filter(pk=int(campain_id))\n        except Campaign.DoesNotExist:\n            raise CommandError(('Campaign ID %s does not exits' % campain_id))\n        signatures_qs = Signature.objects.filter(campaign__pk=int(campain_id))\n        for domain in valid_domains:\n            signatures_qs = signatures_qs.exclude(email__endswith=domain)\n        for signature in signatures_qs:\n            signature.delete()\n", "label": 0}
{"function": "\n\ndef getFieldExtractorForReadMessage(self, jsonMessage, objectToRead):\n    if ('field' not in jsonMessage):\n        raise MalformedMessageException(\"missing 'field'\")\n    field = jsonMessage['field']\n    if (not isinstance(field, str)):\n        raise MalformedMessageException('fieldname not a string')\n    field = intern(field)\n    try:\n        fieldDef = getattr(getObjectClass(objectToRead), field)\n    except:\n        raise InvalidFieldException()\n    if (not Decorators.isPropertyToExpose(fieldDef)):\n        raise InvalidFieldException()\n\n    def getter(x):\n        return getattr(x, field)\n    return getter\n", "label": 0}
{"function": "\n\ndef _parse_settings_bond_6(opts, iface, bond_def):\n    '\\n    Filters given options and outputs valid settings for bond6.\\n    If an option has a value that is not expected, this\\n    function will log what the Interface, Setting and what it was\\n    expecting.\\n    '\n    bond = {\n        'mode': '6',\n    }\n    for binding in ['miimon', 'downdelay', 'updelay']:\n        if (binding in opts):\n            try:\n                int(opts[binding])\n                bond.update({\n                    binding: opts[binding],\n                })\n            except ValueError:\n                _raise_error_iface(iface, binding, ['integer'])\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({\n                binding: bond_def[binding],\n            })\n    if ('use_carrier' in opts):\n        if (opts['use_carrier'] in _CONFIG_TRUE):\n            bond.update({\n                'use_carrier': '1',\n            })\n        elif (opts['use_carrier'] in _CONFIG_FALSE):\n            bond.update({\n                'use_carrier': '0',\n            })\n        else:\n            valid = (_CONFIG_TRUE + _CONFIG_FALSE)\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({\n            'use_carrier': bond_def['use_carrier'],\n        })\n    return bond\n", "label": 0}
{"function": "\n\ndef _get_axis_class(axis_type, range_input):\n    if (axis_type is None):\n        return None\n    elif (axis_type == 'linear'):\n        return LinearAxis\n    elif (axis_type == 'log'):\n        return LogAxis\n    elif (axis_type == 'datetime'):\n        return DatetimeAxis\n    elif (axis_type == 'auto'):\n        if isinstance(range_input, FactorRange):\n            return CategoricalAxis\n        elif isinstance(range_input, Range1d):\n            try:\n                Datetime.validate(Datetime(), range_input.start)\n                return DatetimeAxis\n            except ValueError:\n                pass\n        return LinearAxis\n    else:\n        raise ValueError((\"Unrecognized axis_type: '%r'\" % axis_type))\n", "label": 0}
{"function": "\n\ndef _parse_metadata(context, repos, record):\n    'parse metadata formats'\n    if isinstance(record, str):\n        exml = etree.fromstring(record, context.parser)\n    elif hasattr(record, 'getroot'):\n        exml = record.getroot()\n    else:\n        exml = record\n    root = exml.tag\n    LOGGER.debug('Serialized metadata, parsing content model')\n    if (root == ('{%s}MD_Metadata' % context.namespaces['gmd'])):\n        return [_parse_iso(context, repos, exml)]\n    elif (root == '{http://www.isotc211.org/2005/gmi}MI_Metadata'):\n        return [_parse_iso(context, repos, exml)]\n    elif (root == 'metadata'):\n        return [_parse_fgdc(context, repos, exml)]\n    elif (root == ('{%s}TRANSFER' % context.namespaces['gm03'])):\n        return [_parse_gm03(context, repos, exml)]\n    elif (root == ('{%s}Record' % context.namespaces['csw'])):\n        return [_parse_dc(context, repos, exml)]\n    elif (root == ('{%s}RDF' % context.namespaces['rdf'])):\n        return [_parse_dc(context, repos, exml)]\n    elif (root == ('{%s}DIF' % context.namespaces['dif'])):\n        pass\n    else:\n        raise RuntimeError('Unsupported metadata format')\n", "label": 1}
{"function": "\n\ndef __init__(self, identifier, buckets=None, **kwargs):\n    super(SongProxy, self).__init__()\n    buckets = (buckets or [])\n    self.id = identifier\n    self._object_type = 'song'\n    kwargs = dict(((str(k), v) for (k, v) in kwargs.iteritems()))\n    if kwargs.has_key('track_id'):\n        self.track_id = kwargs['track_id']\n    if kwargs.has_key('tag'):\n        self.tag = kwargs['tag']\n    if kwargs.has_key('score'):\n        self.score = kwargs['score']\n    if kwargs.has_key('audio'):\n        self.audio = kwargs['audio']\n    if kwargs.has_key('release_image'):\n        self.release_image = kwargs['release_image']\n    core_attrs = ['title', 'artist_name', 'artist_id']\n    if (not all(((ca in kwargs) for ca in core_attrs))):\n        profile = self.get_attribute('profile', **{\n            'id': self.id,\n            'bucket': buckets,\n        })\n        kwargs.update(profile.get('songs')[0])\n    [self.__dict__.update({\n        ca: kwargs.pop(ca),\n    }) for ca in core_attrs]\n    self.cache.update(kwargs)\n", "label": 1}
{"function": "\n\ndef test_disk_usage(self):\n    usage = psutil.disk_usage(os.getcwd())\n    assert (usage.total > 0), usage\n    assert (usage.used > 0), usage\n    assert (usage.free > 0), usage\n    assert (usage.total > usage.used), usage\n    assert (usage.total > usage.free), usage\n    assert (0 <= usage.percent <= 100), usage.percent\n    fname = tempfile.mktemp()\n    try:\n        psutil.disk_usage(fname)\n    except OSError:\n        err = sys.exc_info()[1]\n        if (err.args[0] != errno.ENOENT):\n            raise\n    else:\n        self.fail('OSError not raised')\n", "label": 1}
{"function": "\n\ndef beacon(config):\n    '\\n    Emit the status of a connected display to the minion\\n\\n    Mainly this is used to detect when the display fails to connect for whatever reason.\\n\\n    .. code-block:: yaml\\n\\n        beacons:\\n          glxinfo:\\n            user: frank\\n            screen_event: True\\n\\n    '\n    log.trace('glxinfo beacon starting')\n    ret = []\n    _validate = validate(config)\n    if (not _validate[0]):\n        return ret\n    retcode = __salt__['cmd.retcode']('DISPLAY=:0 glxinfo', runas=config['user'], python_shell=True)\n    if (('screen_event' in config) and config['screen_event']):\n        last_value = last_state.get('screen_available', False)\n        screen_available = (retcode == 0)\n        if ((last_value != screen_available) or ('screen_available' not in last_state)):\n            ret.append({\n                'tag': 'screen_event',\n                'screen_available': screen_available,\n            })\n        last_state['screen_available'] = screen_available\n    return ret\n", "label": 0}
{"function": "\n\ndef _convert_to_python(self, value_dict, state):\n    is_empty = self.field_is_empty\n    if ((self.field in value_dict) and (value_dict.get(self.field) == self.expected_value)):\n        for required_field in self.required_fields:\n            if ((required_field not in value_dict) or is_empty(value_dict.get(required_field))):\n                raise Invalid((_('You must give a value for %s') % required_field), value_dict, state, error_dict={\n                    required_field: Invalid(self.message('empty', state), value_dict.get(required_field), state),\n                })\n    return value_dict\n", "label": 0}
{"function": "\n\ndef exchange(self, pdu, timeout):\n    if ((not isinstance(pdu, Symmetry)) or (self.cfg.get('symm-log') is True)):\n        log.debug('SEND {0}'.format(pdu))\n    data = (pdu.to_string() if pdu else None)\n    try:\n        data = self.mac.exchange(data, timeout)\n        if (data is None):\n            return None\n    except nfc.clf.DigitalProtocolError as error:\n        log.debug('{0!r}'.format(error))\n        return None\n    pdu = ProtocolDataUnit.from_string(data)\n    if ((not isinstance(pdu, Symmetry)) or (self.cfg.get('symm-log') is True)):\n        log.debug('RECV {0}'.format(pdu))\n    return pdu\n", "label": 0}
{"function": "\n\ndef __init__(self, thread):\n    self.ident = 0\n    try:\n        self.ident = thread.ident\n    except AttributeError:\n        pass\n    if (not self.ident):\n        for (tid, athread) in threading._active.items():\n            if (athread is thread):\n                self.ident = tid\n                break\n    try:\n        self.name = thread.name\n    except AttributeError:\n        self.name = thread.getName()\n    try:\n        self.daemon = thread.daemon\n    except AttributeError:\n        self.daemon = thread.isDaemon()\n", "label": 0}
{"function": "\n\ndef blockCombine(l):\n    ' Produce a matrix from a list of lists of its components. '\n    l = [list(map(mat, row)) for row in l]\n    hdims = [m.shape[1] for m in l[0]]\n    hs = sum(hdims)\n    vdims = [row[0].shape[0] for row in l]\n    vs = sum(vdims)\n    res = zeros((hs, vs))\n    vindex = 0\n    for (i, row) in enumerate(l):\n        hindex = 0\n        for (j, m) in enumerate(row):\n            res[vindex:(vindex + vdims[i]), hindex:(hindex + hdims[j])] = m\n            hindex += hdims[j]\n        vindex += vdims[i]\n    return res\n", "label": 0}
{"function": "\n\n@classmethod\ndef from_obj(cls, obj, return_obj=None, contained_type=None, binding_var=None):\n    if (not obj):\n        return None\n    if (return_obj is None):\n        return_obj = cls()\n    if (not contained_type):\n        contained_type = cls._contained_type\n    if (not binding_var):\n        binding_var = cls._binding_var\n    for item in getattr(obj, binding_var):\n        return_obj.append(contained_type.from_obj(item))\n    return return_obj\n", "label": 0}
{"function": "\n\ndef get_artist(self, artists):\n    'Returns an artist string (all artists) and an artist_id (the main\\n        artist) for a list of discogs album or track artists.\\n        '\n    artist_id = None\n    bits = []\n    for (i, artist) in enumerate(artists):\n        if (not artist_id):\n            artist_id = artist['id']\n        name = artist['name']\n        name = re.sub(' \\\\(\\\\d+\\\\)$', '', name)\n        name = re.sub('(?i)^(.*?), (a|an|the)$', '\\\\2 \\\\1', name)\n        bits.append(name)\n        if (artist['join'] and (i < (len(artists) - 1))):\n            bits.append(artist['join'])\n    artist = (' '.join(bits).replace(' ,', ',') or None)\n    return (artist, artist_id)\n", "label": 0}
{"function": "\n\ndef test_manage_job():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == False)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef _setup_extensions(self):\n    self.extensions = []\n    if try_murmur3:\n        self.extensions.append(murmur3_ext)\n    if try_libev:\n        self.extensions.append(libev_ext)\n    if try_cython:\n        try:\n            from Cython.Build import cythonize\n            cython_candidates = ['cluster', 'concurrent', 'connection', 'cqltypes', 'metadata', 'pool', 'protocol', 'query', 'util']\n            compile_args = ([] if is_windows else ['-Wno-unused-function'])\n            self.extensions.extend(cythonize([Extension(('cassandra.%s' % m), [('cassandra/%s.py' % m)], extra_compile_args=compile_args) for m in cython_candidates], nthreads=build_concurrency, exclude_failures=True))\n            self.extensions.extend(cythonize(NoPatchExtension('*', ['cassandra/*.pyx'], extra_compile_args=compile_args), nthreads=build_concurrency))\n        except Exception:\n            sys.stderr.write('Failed to cythonize one or more modules. These will not be compiled as extensions (optional).\\n')\n", "label": 0}
{"function": "\n\ndef make_revision_with_plugins(obj, user=None, message=None):\n    '\\n    Only add to revision if it is a draft.\\n    '\n    from cms.models.pluginmodel import CMSPlugin\n    from cms.utils.reversion_hacks import revision_context, revision_manager\n    cls = obj.__class__\n    if hasattr(revision_manager, '_registration_key_for_model'):\n        model_key = revision_manager._registration_key_for_model(cls)\n    else:\n        model_key = cls\n    if (model_key in revision_manager._registered_models):\n        placeholder_relation = find_placeholder_relation(obj)\n        if revision_context.is_active():\n            if user:\n                revision_context.set_user(user)\n            if message:\n                revision_context.set_comment(message)\n            adapter = revision_manager.get_adapter(obj.__class__)\n            revision_context.add_to_context(revision_manager, obj, adapter.get_version_data(obj))\n            for ph in obj.get_placeholders():\n                phadapter = revision_manager.get_adapter(ph.__class__)\n                revision_context.add_to_context(revision_manager, ph, phadapter.get_version_data(ph))\n            filters = {\n                ('placeholder__%s' % placeholder_relation): obj,\n            }\n            for plugin in CMSPlugin.objects.filter(**filters):\n                (plugin_instance, admin) = plugin.get_plugin_instance()\n                if plugin_instance:\n                    padapter = revision_manager.get_adapter(plugin_instance.__class__)\n                    revision_context.add_to_context(revision_manager, plugin_instance, padapter.get_version_data(plugin_instance))\n                bpadapter = revision_manager.get_adapter(plugin.__class__)\n                revision_context.add_to_context(revision_manager, plugin, bpadapter.get_version_data(plugin))\n", "label": 0}
{"function": "\n\ndef _emit(self, cond_br, target=None, reg=None, absolute=False, link=REGISTERS['null']):\n    if (target is None):\n        imm = 0\n    elif isinstance(target, Label):\n        target.pinned = False\n        self.asm._add_backpatch_item(target.name)\n        imm = 0\n    elif isinstance(target, int):\n        imm = target\n    else:\n        raise AssembleError('Invalid branch target: {}'.format(target))\n    if reg:\n        if ((not (reg.spec & _REG_AR)) or (reg.name not in GENERAL_PURPOSE_REGISTERS)):\n            raise AssembleError('Must be general purpose regfile A register {}'.format(reg))\n        assert (reg.addr < 32)\n        raddr_a = reg.addr\n        use_reg = True\n    else:\n        raddr_a = 0\n        use_reg = False\n    (waddr_add, waddr_mul, write_swap, pack) = self._encode_write_operands(link)\n    if pack:\n        raise AssembleError('Packing is not available for link register')\n    insn = BranchInsn(sig=15, cond_br=cond_br, rel=(not absolute), reg=use_reg, raddr_a=raddr_a, ws=write_swap, waddr_add=waddr_add, waddr_mul=waddr_mul, immediate=imm)\n    self.asm._emit(insn)\n", "label": 0}
{"function": "\n\ndef scan_quoted_string(self, length=None):\n    self.skip_char(b'\"')\n    out = b''\n    while ((length is None) or (len(out) <= length)):\n        if (not self.char):\n            raise ValueError('quoted string is missing closing quote')\n        elif (self.char == b'\"'):\n            if ((length is None) or (len(out) == length)):\n                self.skip_char(b'\"')\n                break\n            else:\n                raise ValueError(('quoted string ended too early (expected %d)' % length))\n        elif (self.char == b'\\\\'):\n            c = self.advance()\n            if (c in b'\\r\\n'):\n                continue\n            elif (c in b'0123'):\n                s = ((c + self.advance()) + self.advance())\n                val = int(s, 8)\n                out += chr(val)\n            elif (c == b'b'):\n                out += b'\\x08'\n            elif (c == b'f'):\n                out += b'\\x0c'\n            elif (c == b'n'):\n                out += b'\\n'\n            elif (c == b'r'):\n                out += b'\\r'\n            elif (c == b't'):\n                out += b'\\t'\n            elif (c == b'v'):\n                out += b'\\x0b'\n            elif (c == b'x'):\n                s = (self.advance() + self.advance())\n                val = int(s, 16)\n                out += chr(val)\n            else:\n                raise ValueError(('unknown escape character \\\\%s at %d' % (c, self.pos)))\n        else:\n            out += self.char\n        self.advance()\n    return out\n", "label": 1}
{"function": "\n\ndef __init__(self, sliceDB, seqDB, annotationType=None, itemClass=AnnotationSeq, itemSliceClass=AnnotationSlice, itemAttrDict=None, sliceAttrDict=None, maxCache=None, autoGC=True, checkFirstID=True, **kwargs):\n    'sliceDB must map identifier to a sliceInfo object;\\n        sliceInfo must have attributes: id, start, stop, orientation;\\n        seqDB must map sequence ID to a sliceable sequence object;\\n        sliceAttrDict gives optional dict of item attributes that\\n        should be mapped to sliceDB item attributes.\\n        maxCache specfies the maximum number of annotation objects\\n        to keep in the cache.'\n    if autoGC:\n        self._weakValueDict = classutil.RecentValueDictionary(autoGC)\n    else:\n        self._weakValueDict = {\n            \n        }\n    self.autoGC = autoGC\n    if (sliceAttrDict is None):\n        sliceAttrDict = {\n            \n        }\n    if (sliceDB is not None):\n        self.sliceDB = sliceDB\n    else:\n        self.sliceDB = classutil.get_shelve_or_dict(**kwargs)\n    self.seqDB = seqDB\n    self.annotationType = annotationType\n    self.itemClass = itemClass\n    self.itemSliceClass = itemSliceClass\n    self.sliceAttrDict = sliceAttrDict\n    if (maxCache is not None):\n        self.maxCache = maxCache\n    if checkFirstID:\n        try:\n            k = iter(self).next()\n            self.get_annot_obj(k, self.sliceDB[k])\n        except KeyError:\n            raise KeyError(('cannot create annotation object %s; sequence database %s may not be correct' % (k, repr(seqDB))))\n        except StopIteration:\n            pass\n", "label": 0}
{"function": "\n\ndef init_app(self, app, dsn=None, logging=None, level=None, logging_exclusions=None, wrap_wsgi=None, register_signal=None):\n    if (dsn is not None):\n        self.dsn = dsn\n    if (level is not None):\n        self.level = level\n    if (wrap_wsgi is not None):\n        self.wrap_wsgi = wrap_wsgi\n    elif (self.wrap_wsgi is None):\n        if (app and app.debug):\n            self.wrap_wsgi = False\n        else:\n            self.wrap_wsgi = True\n    if (register_signal is not None):\n        self.register_signal = register_signal\n    if (logging is not None):\n        self.logging = logging\n    if (logging_exclusions is not None):\n        self.logging_exclusions = logging_exclusions\n    if (not self.client):\n        self.client = make_client(self.client_cls, app, self.dsn)\n    if self.logging:\n        kwargs = {\n            \n        }\n        if (self.logging_exclusions is not None):\n            kwargs['exclude'] = self.logging_exclusions\n        setup_logging(SentryHandler(self.client, level=self.level), **kwargs)\n    if self.wrap_wsgi:\n        app.wsgi_app = SentryMiddleware(app.wsgi_app, self.client)\n    app.before_request(self.before_request)\n    if self.register_signal:\n        got_request_exception.connect(self.handle_exception, sender=app)\n        request_finished.connect(self.after_request, sender=app)\n    if (not hasattr(app, 'extensions')):\n        app.extensions = {\n            \n        }\n    app.extensions['sentry'] = self\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    from sympy.combinatorics.permutations import Permutation, Cycle\n    if Permutation.print_cyclic:\n        if (not self.size):\n            return 'Permutation()'\n        s = Cycle(self)((self.size - 1)).__repr__()[len('Cycle'):]\n        last = s.rfind('(')\n        if ((not (last == 0)) and (',' not in s[last:])):\n            s = (s[last:] + s[:last])\n        return ('Permutation%s' % s)\n    else:\n        s = self.support()\n        if (not s):\n            if (self.size < 5):\n                return ('Permutation(%s)' % str(self.array_form))\n            return ('Permutation([], size=%s)' % self.size)\n        trim = (str(self.array_form[:(s[(- 1)] + 1)]) + (', size=%s' % self.size))\n        use = full = str(self.array_form)\n        if (len(trim) < len(full)):\n            use = trim\n        return ('Permutation%s' % use)\n", "label": 0}
{"function": "\n\ndef set_serial_number(self, serial):\n    '\\n        Set the serial number of the certificate.\\n\\n        :param serial: The new serial number.\\n        :type serial: :py:class:`int`\\n\\n        :return: :py:data`None`\\n        '\n    if (not isinstance(serial, _integer_types)):\n        raise TypeError('serial must be an integer')\n    hex_serial = hex(serial)[2:]\n    if (not isinstance(hex_serial, bytes)):\n        hex_serial = hex_serial.encode('ascii')\n    bignum_serial = _ffi.new('BIGNUM**')\n    small_serial = _lib.BN_hex2bn(bignum_serial, hex_serial)\n    if (bignum_serial[0] == _ffi.NULL):\n        set_result = _lib.ASN1_INTEGER_set(_lib.X509_get_serialNumber(self._x509), small_serial)\n        if set_result:\n            _raise_current_error()\n    else:\n        asn1_serial = _lib.BN_to_ASN1_INTEGER(bignum_serial[0], _ffi.NULL)\n        _lib.BN_free(bignum_serial[0])\n        if (asn1_serial == _ffi.NULL):\n            _raise_current_error()\n        asn1_serial = _ffi.gc(asn1_serial, _lib.ASN1_INTEGER_free)\n        set_result = _lib.X509_set_serialNumber(self._x509, asn1_serial)\n        if (not set_result):\n            _raise_current_error()\n", "label": 0}
{"function": "\n\ndef fileUpdated(self, file):\n    '\\n        On file update, if the name or the MIME type changed, we must update\\n        them accordingly on the S3 key so that the file downloads with the\\n        correct name and content type.\\n        '\n    if file.get('imported'):\n        return\n    bucket = self._getBucket()\n    key = bucket.get_key(file['s3Key'], validate=True)\n    if (not key):\n        return\n    disp = ('attachment; filename=\"%s\"' % file['name'])\n    mime = (file.get('mimeType') or '')\n    if ((key.content_type != mime) or (key.content_disposition != disp)):\n        key.set_remote_metadata(metadata_plus={\n            'Content-Type': mime,\n            'Content-Disposition': disp.encode('utf8'),\n        }, metadata_minus=[], preserve_acl=True)\n", "label": 0}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    argvals = list(args)\n    ops = []\n    tapes = set()\n    for (i, arg) in enumerate(args):\n        if isinstance(arg, Node):\n            argvals[i] = arg.value\n            if (i in self.zero_grads):\n                continue\n            for (tape, parent_rnode) in iteritems(arg.tapes):\n                if (not tape.complete):\n                    ops.append((tape, i, parent_rnode))\n                    tapes.add(tape)\n    (result, aux) = self.fun(*argvals, **kwargs)\n    if (result is NotImplemented):\n        return result\n    if ops:\n        result = new_node(result, tapes)\n        for (tape, argnum, parent) in ops:\n            gradfun = self.gradmaker(argnum, aux, result, args, kwargs)\n            rnode = result.tapes[tape]\n            rnode.parent_grad_ops.append((gradfun, parent))\n    return result\n", "label": 0}
{"function": "\n\ndef crossdomain(origin=None, methods=None, headers=None, max_age=21600, attach_to_all=True, automatic_options=True):\n    if (methods is not None):\n        methods = ', '.join(sorted((x.upper() for x in methods)))\n    if ((headers is not None) and (not isinstance(headers, str))):\n        headers = ', '.join((x.upper() for x in headers))\n    if (not isinstance(origin, str)):\n        origin = ', '.join(origin)\n    if isinstance(max_age, timedelta):\n        max_age = max_age.total_seconds()\n\n    def get_methods():\n        if (methods is not None):\n            return methods\n        options_resp = current_app.make_default_options_response()\n        return options_resp.headers['allow']\n\n    def decorator(f):\n\n        def wrapped_function(*args, **kwargs):\n            if (automatic_options and (request.method == 'OPTIONS')):\n                resp = current_app.make_default_options_response()\n            else:\n                resp = make_response(f(*args, **kwargs))\n            if ((not attach_to_all) and (request.method != 'OPTIONS')):\n                return resp\n            h = resp.headers\n            h['Access-Control-Allow-Origin'] = origin\n            h['Access-Control-Allow-Methods'] = get_methods()\n            h['Access-Control-Max-Age'] = str(max_age)\n            if (headers is not None):\n                h['Access-Control-Allow-Headers'] = headers\n            return resp\n        f.provide_automatic_options = False\n        return update_wrapper(wrapped_function, f)\n    return decorator\n", "label": 0}
{"function": "\n\ndef test_number_to_string(self):\n    ' Numbers are turned into strings.\\n        '\n    cleaner = Cleaners()\n    in_int = 85\n    in_float = 82.12\n    in_string = 'big frame, small spirit!'\n    in_list = ['hands', 'by', 'the', 'halyards']\n    in_none = None\n    assert (cleaner.number_to_string(in_int) == str(in_int))\n    assert (cleaner.number_to_string(in_float) == str(in_float))\n    assert (cleaner.number_to_string(in_string) == in_string)\n    assert (cleaner.number_to_string(in_list) == in_list)\n    assert (cleaner.number_to_string(in_none) is None)\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.stdout.write('Collecting duplicates...')\n    duplicates = sum([self.get_quota_duplicate_versions(quota) for quota in Quota.objects.all()], [])\n    self.stdout.write('...Done')\n    if (not duplicates):\n        self.stdout.write('No duplicates were found. Congratulations!')\n    else:\n        self.stdout.write(('There are %s duplicates for quotas versions.' % len(duplicates)))\n        while True:\n            delete = (raw_input('  Do you want to delete them? [Y/n]:') or 'y')\n            if (delete.lower() not in ('y', 'n')):\n                self.stdout.write('  Please enter letter \"y\" or \"n\"')\n            else:\n                delete = (delete.lower() == 'y')\n                break\n        if delete:\n            for duplicate in duplicates:\n                duplicate.delete()\n            self.stdout.write('All duplicates were deleted.')\n        else:\n            self.stdout.write('Duplicates were not deleted.')\n", "label": 0}
{"function": "\n\ndef get_submitted_exams(course, student):\n    try:\n        exams = Exam.objects.filter(course=course).order_by('exam_num')\n    except Exam.DoesNotExist:\n        exams = None\n    try:\n        submitted_exams = ExamSubmission.objects.filter(exam__course=course, student=student)\n    except ExamSubmission.DoesNotExist:\n        submitted_exams = None\n    if (len(exams) != len(submitted_exams)):\n        for exam in exams:\n            found_exam = False\n            for submitted_exam in submitted_exams:\n                if (exam.id == submitted_exam.exam_id):\n                    found_exam = True\n            if (not found_exam):\n                submission = ExamSubmission.objects.create(student=student, exam=exam)\n                submission.save()\n        submitted_exams = ExamSubmission.objects.filter(exam__course=course, student=student)\n    return submitted_exams\n", "label": 0}
{"function": "\n\ndef execute(self, cmd):\n    if (len(cmd) > 237):\n        print_error('Your command must be at most 237 characters long. Longer strings might crash the server.')\n        return\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    sock.bind(('0.0.0.0', 9999))\n    sock.settimeout(2)\n    packet = ((((b'\\x0c\\x153\\x00' + os.urandom(4)) + (b'\\x00' * 38)) + struct.pack('<H', len(cmd))) + cmd).ljust(512, b'\\x00')\n    try:\n        sock.sendto(packet, (self.target, 9999))\n    except socket.error:\n        return ''\n    while True:\n        try:\n            (data, addr) = sock.recvfrom(512)\n        except socket.timeout:\n            sock.close()\n            raise\n        if ((len(data) == 512) and (data[1] == '\\x16')):\n            break\n    length = struct.unpack('<H', data[14:16])[0]\n    output = data[16:(16 + length)]\n    sock.close()\n    return output\n", "label": 0}
{"function": "\n\n@log\ndef getattr(self, path, fh=None):\n    (uid, gid, pid) = fuse_get_context()\n    working_path = path\n    if is_special_file(path):\n        (working_path, special) = explode_special_file(working_path)\n    record = self._get_record(working_path)\n    if record.is_directory():\n        mode = (stat.S_IFDIR | PERMISSION_ALL_READ)\n        nlink = 2\n    else:\n        mode = (stat.S_IFREG | PERMISSION_ALL_READ)\n        nlink = 1\n    if is_special_file(path):\n        size = 0\n        (working_path, special) = explode_special_file(path)\n        if (special == 'meta'):\n            node = self._get_node(working_path)\n            record_buf = self._enumerator.get_record_buf(node.get_record_number())\n            size = len(get_meta_for_file(record, working_path))\n    else:\n        data_attribute = record.data_attribute()\n        if (data_attribute is not None):\n            if (data_attribute.non_resident() == 0):\n                size = len(data_attribute.value())\n            else:\n                size = data_attribute.data_size()\n        else:\n            size = record.filename_information().logical_size()\n    return {\n        'st_atime': unixtimestamp(record.standard_information().accessed_time()),\n        'st_ctime': unixtimestamp(record.standard_information().changed_time()),\n        'st_mtime': unixtimestamp(record.standard_information().modified_time()),\n        'st_size': size,\n        'st_uid': uid,\n        'st_gid': gid,\n        'st_mode': mode,\n        'st_nlink': nlink,\n    }\n", "label": 0}
{"function": "\n\ndef _update(self, url, data, kwargs, coerce):\n    params = {\n        \n    }\n    for k in data.keys():\n        if (data[k] is None):\n            del data[k]\n    if (('who' in kwargs) and (kwargs['who'] is not None)):\n        params['_who'] = kwargs['who']\n    if (('why' in kwargs) and (kwargs['why'] is not None)):\n        params['_why'] = kwargs['why']\n    headers = kwargs.get('headers', {\n        \n    })\n    resp = self.connection.request(url, method='PUT', params=params, data=data, headers=headers)\n    if (resp.status == httplib.NO_CONTENT):\n        location = resp.headers.get('location')\n        if (not location):\n            raise LibcloudError('Missing location header')\n        obj_ids = self._url_to_obj_ids(location)\n        return coerce(**obj_ids)\n    else:\n        raise LibcloudError(('Unexpected status code: %s' % resp.status))\n", "label": 0}
{"function": "\n\ndef viewRssFeed(self, modelDocument, parentNode):\n    self.id = 1\n    for rssItem in modelDocument.rssItems:\n        node = self.treeView.insert(parentNode, 'end', rssItem.objectId(), text=(rssItem.companyName or ''), tags=(('odd' if (self.id & 1) else 'even'),))\n        self.treeView.set(node, 'form', rssItem.formType)\n        self.treeView.set(node, 'filingDate', rssItem.filingDate)\n        self.treeView.set(node, 'cik', rssItem.cikNumber)\n        self.treeView.set(node, 'status', rssItem.status)\n        self.treeView.set(node, 'period', rssItem.period)\n        self.treeView.set(node, 'fiscalYrEnd', rssItem.fiscalYearEnd)\n        self.treeView.set(node, 'results', (' '.join((str(result) for result in (rssItem.results or []))) + ((' ' + str(rssItem.assertions)) if rssItem.assertions else '')))\n        self.id += 1\n    else:\n        pass\n", "label": 0}
{"function": "\n\n@contextfunction\ndef htform(context, form):\n    'Set time zone'\n    request = context['request']\n    user = None\n    if request.user.username:\n        try:\n            user = request.user.profile\n        except Exception:\n            pass\n    default_timezone = settings.HARDTREE_SERVER_DEFAULT_TIMEZONE\n    try:\n        conf = ModuleSetting.get('default_timezone')[0]\n        default_timezone = conf.value\n    except:\n        pass\n    try:\n        conf = ModuleSetting.get('default_timezone', user=user)[0]\n        default_timezone = conf.value\n    except Exception:\n        default_timezone = getattr(settings, 'HARDTREE_SERVER_TIMEZONE')[default_timezone][0]\n    all_timezones = getattr(settings, 'HARDTREE_SERVER_TIMEZONE', [(1, '(GMT-11:00) International Date Line West')])\n    title = all_timezones[int(default_timezone)][1]\n    GMT = title[4:10]\n    sign = GMT[0:1]\n    hours = int(GMT[1:3])\n    mins = int(GMT[4:6])\n    if (not form.errors):\n        for field in form:\n            try:\n                date = datetime.strptime(str(field.form.initial[field.name]), '%Y-%m-%d %H:%M:%S')\n                if date:\n                    if (sign == '-'):\n                        field.form.initial[field.name] = (date - timedelta(hours=hours, minutes=mins))\n                    else:\n                        field.form.initial[field.name] = (date + timedelta(hours=hours, minutes=mins))\n            except:\n                pass\n    return form\n", "label": 1}
{"function": "\n\ndef _retry_reboot(self, context, instance):\n    current_power_state = self._get_power_state(context, instance)\n    current_task_state = instance.task_state\n    retry_reboot = False\n    reboot_type = compute_utils.get_reboot_type(current_task_state, current_power_state)\n    pending_soft = ((current_task_state == task_states.REBOOT_PENDING) and (instance.vm_state in vm_states.ALLOW_SOFT_REBOOT))\n    pending_hard = ((current_task_state == task_states.REBOOT_PENDING_HARD) and (instance.vm_state in vm_states.ALLOW_HARD_REBOOT))\n    started_not_running = ((current_task_state in [task_states.REBOOT_STARTED, task_states.REBOOT_STARTED_HARD]) and (current_power_state != power_state.RUNNING))\n    if (pending_soft or pending_hard or started_not_running):\n        retry_reboot = True\n    return (retry_reboot, reboot_type)\n", "label": 0}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    super(UserSettingsForm, self).__init__(*args, **kwargs)\n\n    def get_language_display_name(code, desc):\n        try:\n            desc = translation.get_language_info(code)['name_local']\n            desc = string.capwords(desc)\n        except KeyError:\n            pass\n        return ('%s (%s)' % (desc, code))\n    languages = [(k, get_language_display_name(k, v)) for (k, v) in settings.LANGUAGES]\n    self.fields['language'].choices = languages\n    timezones = []\n    language = translation.get_language()\n    current_locale = translation.to_locale(language)\n    babel_locale = babel.Locale.parse(current_locale)\n    for (tz, offset) in self._sorted_zones():\n        try:\n            utc_offset = (_('UTC %(hour)s:%(min)s') % {\n                'hour': offset[:3],\n                'min': offset[3:],\n            })\n        except Exception:\n            utc_offset = ''\n        if (tz == 'UTC'):\n            tz_name = _('UTC')\n        elif (tz == 'GMT'):\n            tz_name = _('GMT')\n        else:\n            tz_label = babel.dates.get_timezone_location(tz, locale=babel_locale)\n            tz_name = (_('%(offset)s: %(label)s') % {\n                'offset': utc_offset,\n                'label': tz_label,\n            })\n        timezones.append((tz, tz_name))\n    self.fields['timezone'].choices = timezones\n", "label": 0}
{"function": "\n\ndef solve(self, rhs_mat, system, mode):\n    \" Solves the linear system for the problem in self.system. The\\n        full solution vector is returned.\\n\\n        Args\\n        ----\\n        rhs_mat : dict of ndarray\\n            Dictionary containing one ndarry per top level quantity of\\n            interest. Each array contains the right-hand side for the linear\\n            solve.\\n\\n        system : `System`\\n            Parent `System` object.\\n\\n        mode : string\\n            Derivative mode, can be 'fwd' or 'rev'.\\n\\n        Returns\\n        -------\\n        dict of ndarray : Solution vectors\\n        \"\n    self.system = system\n    if (self.mode is None):\n        self.mode = mode\n    sol_buf = OrderedDict()\n    for (voi, rhs) in rhs_mat.items():\n        self.voi = None\n        if system._jacobian_changed:\n            self.jacobian = self._assemble_jacobian(rhs, mode)\n            system._jacobian_changed = False\n            if (self.options['solve_method'] == 'LU'):\n                self.lup = lu_factor(self.jacobian)\n        if (self.options['solve_method'] == 'LU'):\n            deriv = lu_solve(self.lup, rhs)\n        else:\n            deriv = np.linalg.solve(self.jacobian, rhs)\n        self.system = None\n        sol_buf[voi] = deriv\n    return sol_buf\n", "label": 0}
{"function": "\n\ndef test_numpy_geometric(self):\n    geom = jit_unary('np.random.geometric')\n    self.assertRaises(ValueError, geom, (- 1.0))\n    self.assertRaises(ValueError, geom, 0.0)\n    self.assertRaises(ValueError, geom, 1.001)\n    N = 200\n    r = [geom(1.0) for i in range(N)]\n    self.assertPreciseEqual(r, ([1] * N))\n    r = [geom(0.9) for i in range(N)]\n    n = r.count(1)\n    self.assertGreaterEqual(n, (N // 2))\n    self.assertLess(n, N)\n    self.assertFalse([i for i in r if (i > 1000)])\n    r = [geom(0.4) for i in range(N)]\n    self.assertTrue([i for i in r if (i > 4)])\n    r = [geom(0.01) for i in range(N)]\n    self.assertTrue([i for i in r if (i > 50)])\n    r = [geom(1e-15) for i in range(N)]\n    self.assertTrue([i for i in r if (i > (2 ** 32))])\n", "label": 1}
{"function": "\n\ndef value__get(self):\n    if (self.selectedIndex is not None):\n        return self.options[self.selectedIndex][0]\n    else:\n        for (option, checked) in self.options:\n            if checked:\n                return option\n        else:\n            if self.options:\n                return self.options[0][0]\n            else:\n                return None\n", "label": 0}
{"function": "\n\ndef get_available_screens():\n    '\\n    Gets the available screens in this package for dynamic instantiation.\\n    '\n    ignore_list = ['__init__.py']\n    screens = []\n    for module in os.listdir(os.path.join(os.path.dirname(__file__))):\n        if ((module in ignore_list) or (module[(- 3):] != '.py')):\n            continue\n        module_name = module[:(- 3)]\n        m = __import__(module_name, globals(), locals())\n        for (name, obj) in inspect.getmembers(m):\n            if (inspect.isclass(obj) and issubclass(obj, base.ScreenBase) and (not name.endswith('Base'))):\n                screens.append(obj)\n    screens.extend(get_available_plugin_screens())\n    return screens\n", "label": 0}
{"function": "\n\n@synchronizedDeferred(busy)\n@deferredAsThread\ndef garbageCheck(self):\n    'Check for file patterns that are removeable'\n    watchDict = copy.deepcopy(self.watchDict)\n    for (directory, garbageList) in watchDict.iteritems():\n        if (not os.path.exists(directory)):\n            continue\n        for (pattern, limit) in garbageList:\n            self.cleanupLinks(directory)\n            files = [os.path.join(directory, f) for f in os.listdir(directory) if re.search(pattern, f)]\n            files = sorted(files)\n            if (len(files) > int(limit)):\n                log(('These files matched:\\n\\t%s' % '\\n\\t'.join(files)))\n            while (len(files) > int(limit)):\n                oldfile = files.pop(0)\n                log(('Deleting %s' % oldfile))\n                if os.path.islink(oldfile):\n                    continue\n                if os.path.isdir(oldfile):\n                    for (base, dirs, myfiles) in os.walk(oldfile, topdown=False):\n                        for name in myfiles:\n                            os.remove(os.path.join(base, name))\n                        for name in dirs:\n                            os.rmdir(os.path.join(base, name))\n                    os.rmdir(oldfile)\n                else:\n                    os.unlink(oldfile)\n        self.cleanupLinks(directory)\n", "label": 1}
{"function": "\n\ndef _update_project_members(self, request, data, project_id):\n    users_to_add = 0\n    try:\n        available_roles = api.keystone.role_list(request)\n        member_step = self.get_step(PROJECT_USER_MEMBER_SLUG)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_to_add += len(role_list)\n        for role in available_roles:\n            field_name = member_step.get_member_field_name(role.id)\n            role_list = data[field_name]\n            users_added = 0\n            for user in role_list:\n                api.keystone.add_tenant_user_role(request, project=project_id, user=user, role=role.id)\n                users_added += 1\n            users_to_add -= users_added\n    except Exception:\n        if PROJECT_GROUP_ENABLED:\n            group_msg = _(', add project groups')\n        else:\n            group_msg = ''\n        exceptions.handle(request, (_('Failed to add %(users_to_add)s project members%(group_msg)s and set project quotas.') % {\n            'users_to_add': users_to_add,\n            'group_msg': group_msg,\n        }))\n", "label": 0}
{"function": "\n\ndef _execute_multi_task(self, gen, executor, task):\n    if task.unordered:\n        results_gen = self._execute_multi_gen_task(gen, executor, task)\n        return gen.send(results_gen)\n    future_tasks = [executor.submit(t) for t in task.tasks]\n    while True:\n        if (not task.wait(executor, future_tasks, self.engine.pool_timeout)):\n            self.engine.update_gui()\n        else:\n            break\n    if task.skip_errors:\n        results = []\n        for f in future_tasks:\n            try:\n                results.append(f.result())\n            except Exception:\n                pass\n    else:\n        try:\n            results = [f.result() for f in future_tasks]\n        except Exception:\n            return gen.throw(*sys.exc_info())\n    return gen.send(results)\n", "label": 1}
{"function": "\n\ndef _insert(self, data):\n    if isinstance(data, list):\n        return [self._insert(item) for item in data]\n    if (not all((isinstance(k, string_types) for k in data))):\n        raise ValueError('Document keys must be strings')\n    if ('_id' not in data):\n        data['_id'] = ObjectId()\n    object_id = data['_id']\n    if isinstance(object_id, dict):\n        object_id = helpers.hashdict(object_id)\n    if (object_id in self._documents):\n        raise DuplicateKeyError('Duplicate Key Error', 11000)\n    for unique in self._uniques:\n        find_kwargs = {\n            \n        }\n        for (key, direction) in unique:\n            if (key in data):\n                find_kwargs[key] = data[key]\n        answer = self.find(find_kwargs)\n        if (answer.count() > 0):\n            raise DuplicateKeyError('Duplicate Key Error', 11000)\n    self._documents[object_id] = self._internalize_dict(data)\n    return data['_id']\n", "label": 1}
{"function": "\n\ndef fix_file(filename, options=None, output=None):\n    if (not options):\n        options = parse_args([filename])\n    original_source = readlines_from_file(filename)\n    fixed_source = original_source\n    if (options.in_place or output):\n        encoding = detect_encoding(filename)\n    if output:\n        output = codecs.getwriter(encoding)((output.buffer if hasattr(output, 'buffer') else output))\n        output = LineEndingWrapper(output)\n    fixed_source = fix_lines(fixed_source, options, filename=filename)\n    if options.diff:\n        new = io.StringIO(fixed_source)\n        new = new.readlines()\n        diff = get_diff_text(original_source, new, filename)\n        if output:\n            output.write(diff)\n            output.flush()\n        else:\n            return diff\n    elif options.in_place:\n        fp = open_with_encoding(filename, encoding=encoding, mode='w')\n        fp.write(fixed_source)\n        fp.close()\n    elif output:\n        output.write(fixed_source)\n        output.flush()\n    else:\n        return fixed_source\n", "label": 1}
{"function": "\n\ndef _check_var(self, doc):\n    '\\n        Run checks on the variable whose documentation is C{var} and\\n        whose name is C{name}.\\n        \\n        @param doc: The documentation for the variable to check.\\n        @type doc: L{APIDoc}\\n        @rtype: C{None}\\n        '\n    if (self._checks & DocChecker.VAR):\n        if ((self._checks & (DocChecker.DESCR | DocChecker.TYPE)) and (doc.descr in (None, UNKNOWN)) and (doc.type_descr in (None, UNKNOWN)) and (doc.docstring in (None, UNKNOWN))):\n            self.warning('Undocumented', doc)\n        else:\n            if ((self._checks & DocChecker.DESCR) and (doc.descr in (None, UNKNOWN))):\n                self.warning('No description', doc)\n            if ((self._checks & DocChecker.TYPE) and (doc.type_descr in (None, UNKNOWN))):\n                self.warning('No type information', doc)\n", "label": 1}
{"function": "\n\ndef main(argv=None):\n    argv = sys.argv\n    path = os.path.abspath(os.path.dirname(__file__))\n    if ((len(argv) == 1) or (argv[1] == '--help') or (argv[1] == '-h')):\n        print(globals()['__doc__'])\n        map_keyword2script = mapKeyword2Script(path)\n        if (len(argv) <= 2):\n            print('CGAT tools are grouped by keywords. The following keywords')\n            print('are defined:\\n')\n            print(('%s\\n' % printListInColumns(map_keyword2script.keys(), 3)))\n        if ('all' in argv[2:]):\n            print('The list of all available commands is:\\n')\n            print(('%s\\n' % printListInColumns(sorted([os.path.basename(x)[:(- 3)] for x in glob.glob(os.path.join(path, '*.py'))]), 3)))\n        else:\n            for arg in argv[2:]:\n                if (arg in map_keyword2script):\n                    print((\"Tools matching the keyword '%s':\\n\" % arg))\n                    print(('%s\\n' % printListInColumns(sorted(map_keyword2script[arg]), 3)))\n        return\n    command = argv[1]\n    (file, pathname, description) = imp.find_module(command, [path])\n    module = imp.load_module(command, file, pathname, description)\n    del sys.argv[0]\n    module.main(sys.argv)\n", "label": 0}
{"function": "\n\ndef _LineContainsI18n(uwline):\n    'Return true if there are i18n comments or function calls in the line.\\n\\n  I18n comments and pseudo-function calls are closely related. They cannot\\n  be moved apart without breaking i18n.\\n\\n  Arguments:\\n    uwline: (unwrapped_line.UnwrappedLine) The line currently being formatted.\\n\\n  Returns:\\n    True if the line contains i18n comments or function calls. False otherwise.\\n  '\n    if style.Get('I18N_COMMENT'):\n        for tok in uwline.tokens:\n            if (tok.is_comment and re.match(style.Get('I18N_COMMENT'), tok.value)):\n                return True\n    if style.Get('I18N_FUNCTION_CALL'):\n        length = len(uwline.tokens)\n        index = 0\n        while (index < (length - 1)):\n            if ((uwline.tokens[(index + 1)].value == '(') and (uwline.tokens[index].value in style.Get('I18N_FUNCTION_CALL'))):\n                return True\n            index += 1\n    return False\n", "label": 0}
{"function": "\n\ndef suggest(self):\n    pattern = QtCore.QRegExp('\\\\w+$')\n    cursor = self._editor.textCursor()\n    block = cursor.block()\n    text = block.text()\n    if ((not self._room) or (not self._room.users) or (not text) or cursor.hasSelection()):\n        return False\n    blockText = QtCore.QString(text[:(cursor.position() - block.position())])\n    matchPosition = blockText.indexOf(pattern)\n    if (matchPosition < 0):\n        return False\n    word = blockText[matchPosition:]\n    if word.trimmed().isEmpty():\n        return False\n    matchingUserNames = []\n    for user in self._room.users:\n        if QtCore.QString(user['name']).startsWith(word, QtCore.Qt.CaseInsensitive):\n            matchingUserNames.append(user['name'])\n    if (len(matchingUserNames) == 1):\n        self._replace(cursor, word, (matchingUserNames[0] + (': ' if (matchPosition == 0) else ' ')))\n    else:\n        menu = QtGui.QMenu('Suggestions', self._editor)\n        for userName in matchingUserNames:\n            action = QtGui.QAction(userName, menu)\n            action.setData((cursor, word, userName, matchPosition))\n            self.connect(action, QtCore.SIGNAL('triggered()'), self._userSelected)\n            menu.addAction(action)\n        menu.popup(self._editor.mapToGlobal(self._editor.cursorRect().center()))\n", "label": 1}
{"function": "\n\n@classmethod\ndef invalidate(cls, region):\n    'Invalidate an entire region\\n\\n        .. note::\\n\\n            This does not actually *clear* the region of data, but\\n            just sets the value to expire on next access.\\n\\n        :param region: Region name\\n        :type region: string\\n\\n        '\n    redis = global_connection.redis\n    namespaces = redis.smembers(('retools:%s:namespaces' % region))\n    if (not namespaces):\n        return None\n    longest_expire = max([x['expires'] for x in CacheRegion.regions.values()])\n    new_created = ((time.time() - longest_expire) - 3600)\n    for ns in namespaces:\n        cache_keyset_key = ('retools:%s:%s:keys' % (region, ns))\n        keys = (set(['']) | redis.smembers(cache_keyset_key))\n        for key in keys:\n            cache_key = ('retools:%s:%s:%s' % (region, ns, key))\n            if (not redis.exists(cache_key)):\n                redis.srem(cache_keyset_key, key)\n            else:\n                redis.hset(cache_key, 'created', new_created)\n", "label": 0}
{"function": "\n\ndef test_option():\n    kernel = get_kernel()\n    d = Dummy(kernel)\n    assert ('Options:' in d.line_dummy.__doc__)\n    assert ('--size' in d.line_dummy.__doc__)\n    ret = d.call_magic('line', 'dummy', '', 'hey -s400,200')\n    assert (ret == d)\n    assert (d.foo == 'hey'), d.foo\n    assert (d.size == (400, 200))\n    ret = d.call_magic('line', 'dummy', '', 'hey there')\n    assert (d.foo == 'hey there')\n    ret = d.call_magic('line', 'dummy', '', 'range(1, 10)')\n    assert (d.foo == range(1, 10))\n    ret = d.call_magic('line', 'dummy', '', '[1, 2, 3]')\n    assert (d.foo == [1, 2, 3])\n    ret = d.call_magic('line', 'dummy', '', 'hey -l -s400,200')\n    assert (d.size == (400, 200))\n    assert (d.foo == 'hey -l')\n    ret = d.call_magic('line', 'dummy', '', 'hey -s -- -s400,200')\n    assert (d.size == (400, 200))\n    assert (d.foo == 'hey -s')\n", "label": 1}
{"function": "\n\ndef print_ctypes_struct(struct, name='', ident=0, hexa=False):\n    if isinstance(struct, _ctypes._Pointer):\n        if (ctypes.cast(struct, ctypes.c_void_p).value is None):\n            print('{0} -> NULL'.format(name))\n            return\n        return print_ctypes_struct(struct[0], (name + '<deref>'), hexa=hexa)\n    if (not hasattr(struct, '_fields_')):\n        value = struct\n        if hasattr(struct, 'value'):\n            value = struct.value\n        if isinstance(value, basestring):\n            value = repr(value)\n        if hexa:\n            try:\n                print('{0} -> {1}'.format(name, hex(value)))\n                return\n            except TypeError:\n                pass\n        print('{0} -> {1}'.format(name, value))\n        return\n    for (fname, ftype) in struct._fields_:\n        value = getattr(struct, fname)\n        print_ctypes_struct(value, '{0}.{1}'.format(name, fname), hexa=hexa)\n", "label": 0}
{"function": "\n\ndef test_column_optimizations_with_bcolz_and_rewrite():\n    try:\n        import bcolz\n    except ImportError:\n        return\n    bc = bcolz.ctable([[1, 2, 3], [10, 20, 30]], names=['a', 'b'])\n    func = (lambda x: x)\n    for cols in [None, 'abc', ['abc']]:\n        dsk2 = merge(dict(((('x', i), (dataframe_from_ctable, bc, slice(0, 2), cols, {\n            \n        })) for i in [1, 2, 3])), dict(((('y', i), (getitem, ('x', i), (list, ['a', 'b']))) for i in [1, 2, 3])))\n        expected = dict(((('y', i), (dataframe_from_ctable, bc, slice(0, 2), (list, ['a', 'b']), {\n            \n        })) for i in [1, 2, 3]))\n        result = dd.optimize(dsk2, [('y', i) for i in [1, 2, 3]])\n        assert (result == expected)\n", "label": 0}
{"function": "\n\ndef __protect__(self, key, value=sentinel):\n    'Protected keys add its parents, not sure if useful'\n    if (not isinstance(key, list)):\n        key = (key.split('.') if isinstance(key, basestring) else [key])\n    (key, path) = (key.pop(0), key)\n    if (len(path) > 0):\n        self.get(key).protect(path, value)\n    elif (value is not sentinel):\n        self[key] = value\n    if (key not in self):\n        raise KeyError(('key %s has no value to protect' % key))\n    self.__PROTECTED__.add(key)\n", "label": 0}
{"function": "\n\ndef compute(self):\n    im = self.get_input('Input Image')\n    if self.has_input('Input PixelType'):\n        inPixelType = self.get_input('Input PixelType')\n    else:\n        inPixelType = im.getPixelType()\n    if self.has_input('Dimension'):\n        dim = self.get_input('Dimension')\n    else:\n        dim = im.getDim()\n    inImgType = itk.Image[(inPixelType._type, dim)]\n    try:\n        self.filter_ = itk.CurvatureAnisotropicDiffusionImageFilter[(inImgType, inImgType)].New(im.getImg())\n    except:\n        raise ModuleError(self, 'Filter requires a decimal PixelType')\n    if self.has_input('Iterations'):\n        iterations = self.get_input('Iterations')\n    else:\n        iterations = 5\n    if self.has_input('TimeStep'):\n        timestep = self.get_input('TimeStep')\n    elif (dim == 2):\n        timestep = 0.125\n    else:\n        timestep = 0.0625\n    if self.has_input('Conductance'):\n        conductance = self.get_input('Conductance')\n    else:\n        conductance = 3.0\n    self.filter_.SetNumberOfIterations(iterations)\n    self.filter_.SetTimeStep(timestep)\n    self.filter_.SetConductanceParameter(conductance)\n    self.filter_.Update()\n    outIm = Image()\n    outIm.setImg(self.filter_.GetOutput())\n    outIm.setPixelType(inPixelType)\n    outIm.setDim(dim)\n    self.set_output('Output Image', outIm)\n    self.set_output('Filter', self)\n", "label": 0}
{"function": "\n\ndef __init__(self, dictionary):\n    self.championId = dictionary.get('championId', 0)\n    self.createDate = dictionary.get('createDate', 0)\n    self.fellowPlayers = [(Player(player) if (not isinstance(player, Player)) else player) for player in dictionary.get('fellowPlayers', []) if player]\n    self.gameId = dictionary.get('gameId', 0)\n    self.gameMode = dictionary.get('gameMode', '')\n    self.gameType = dictionary.get('gameType', '')\n    self.invalid = dictionary.get('invalid', False)\n    self.ipEarned = dictionary.get('ipEarned', 0)\n    self.level = dictionary.get('level', 0)\n    self.mapId = dictionary.get('mapId', 0)\n    self.spell1 = dictionary.get('spell1', 0)\n    self.spell2 = dictionary.get('spell2', 0)\n    val = dictionary.get('stats', None)\n    self.stats = (RawStats(val) if (val and (not isinstance(val, RawStats))) else val)\n    self.subType = dictionary.get('subType', '')\n    self.teamId = dictionary.get('teamId', 0)\n", "label": 0}
{"function": "\n\n@staticmethod\ndef from_line(line):\n    line = line.rstrip()\n    fields = line.split('\\t')\n    (contig, start, stop) = (fields[0], int(fields[1]), int(fields[2]))\n    n = len(fields)\n    name = ((fields[3] or None) if (n >= 4) else None)\n    score = ((fields[4] or None) if (n >= 5) else None)\n    strand = ((fields[5] or None) if (n >= 6) else None)\n    thick_start = ((fields[6] or None) if (n >= 7) else None)\n    thick_end = ((fields[7] or None) if (n >= 8) else None)\n    item_rgb = ((fields[8] or None) if (n >= 9) else None)\n    return BedRecord(contig, start, stop, name, score, strand, thick_start, thick_end, item_rgb)\n", "label": 1}
{"function": "\n\ndef _plot_timeseries(ax, ch_idx, tmin, tmax, vmin, vmax, ylim, data, color, times, vline=None, x_label=None, y_label=None, colorbar=False, hline=None):\n    'Aux function to show time series on topo split across multiple axes'\n    import matplotlib.pyplot as plt\n    picker_flag = False\n    for (data_, color_) in zip(data, color):\n        if (not picker_flag):\n            ax.plot(times, data_[ch_idx], color_, picker=1000000000.0)\n            picker_flag = True\n        else:\n            ax.plot(times, data_[ch_idx], color_)\n    if vline:\n        for x in vline:\n            plt.axvline(x, color='w', linewidth=0.5)\n    if hline:\n        for y in hline:\n            plt.axhline(y, color='w', linewidth=0.5)\n    if (x_label is not None):\n        plt.xlabel(x_label)\n    if (y_label is not None):\n        if isinstance(y_label, list):\n            plt.ylabel(y_label[ch_idx])\n        else:\n            plt.ylabel(y_label)\n    if colorbar:\n        plt.colorbar()\n", "label": 1}
{"function": "\n\ndef updateDragOperation(self, event):\n    '\\n        http://dev.w3.org/html5/spec/dnd.html\\n        '\n    dataTransfer = event.dataTransfer\n    ea = dataTransfer.effectAllowed\n    de = dataTransfer.dropEffect\n    if ((de == 'copy') and (ea in ['uninitialized', 'copy', 'copyLink', 'copyMove', 'all'])):\n        self.currentDragOperation = 'copy'\n    elif ((de == 'link') and (ea in ['uninitialized', 'link', 'copyLink', 'linkMove', 'all'])):\n        self.currentDragOperation = 'link'\n    elif ((de == 'move') and (ea in ['uninitialized', 'move', 'copyMove', 'linkMove', 'all'])):\n        self.currentDragOperation = 'move'\n    else:\n        self.currentDragOperation = 'none'\n", "label": 0}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_freq():\n    ' reading/writing of 3D RNMRTK frequency domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_3d', 'freq_3d.sec'))\n    assert (data.shape == (128, 128, 4096))\n    assert (np.abs((data[(0, 1, 2)] - 3.23)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)] - 1.16)) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef _digest(self, alg, password, salt=None):\n    \"\\n        Helper method to perform the password digest.\\n\\n        :param alg: The hash algorithm to use.\\n        :type alg: str - 'sha512' | 'bcrypt'\\n        :param password: The password to digest.\\n        :type password: str\\n        :param salt: The salt to use. In the case of bcrypt,\\n                     when storing the password, pass None;\\n                     when testing the password, pass the hashed value.\\n        :type salt: None or str\\n        :returns: The hashed value as a string.\\n        \"\n    cur_config = config.getConfig()\n    if (alg == 'sha512'):\n        return hashlib.sha512((password + salt).encode('utf8')).hexdigest()\n    elif (alg == 'bcrypt'):\n        try:\n            import bcrypt\n        except ImportError:\n            raise Exception('Bcrypt module is not installed. See girder.local.cfg.')\n        password = password.encode('utf8')\n        if (salt is None):\n            rounds = int(cur_config['auth']['bcrypt_rounds'])\n            return bcrypt.hashpw(password, bcrypt.gensalt(rounds))\n        else:\n            if isinstance(salt, six.text_type):\n                salt = salt.encode('utf8')\n            return bcrypt.hashpw(password, salt)\n    else:\n        raise Exception(('Unsupported hash algorithm: %s' % alg))\n", "label": 0}
{"function": "\n\ndef read_data(self, offset, length):\n    end = (offset + length)\n    eof = self['size']\n    if (end > eof):\n        end = eof\n        length = (end - offset)\n        if (length <= 0):\n            return ''\n    first_block = (offset / self.blocksize)\n    last_block = (end / self.blocksize)\n    output = StringIO()\n    for n_block in range(first_block, (last_block + 1)):\n        block_offset = (n_block * self.blocksize)\n        fragment_offset = 0\n        if (n_block == first_block):\n            fragment_offset = (offset - block_offset)\n        fragment_end = self.blocksize\n        if (n_block == last_block):\n            fragment_end = (end - block_offset)\n        block_data = self.read_block(n_block)\n        fragment = block_data[fragment_offset:fragment_end]\n        assert (len(fragment) == (fragment_end - fragment_offset))\n        output.write(fragment)\n    output = output.getvalue()\n    assert (len(output) == length)\n    return output\n", "label": 0}
{"function": "\n\ndef test_login_logout_save():\n    (auth, app, user) = _get_flask_app()\n    auth.session = Session()\n    client = app.test_client()\n    auth.session.saved = 0\n    resp = client.get('/protected/')\n    print(resp.data)\n    assert (auth.session_key not in auth.session)\n    assert (auth.redirect_key in auth.session)\n    assert (auth.session.saved == 1)\n    data = {\n        'login': user.login,\n        'password': 'foobar',\n        '_csrf_token': auth.get_csrf_token(),\n    }\n    client.post(auth.url_sign_in, data=data)\n    assert (auth.session_key in auth.session)\n    assert (auth.session.saved == 4)\n    data = {\n        '_csrf_token': auth.get_csrf_token(),\n    }\n    client.post(auth.url_sign_out, data=data)\n    assert (auth.session_key not in auth.session)\n    assert (auth.session.saved == 5)\n", "label": 0}
{"function": "\n\ndef _index_document(index_list):\n    'Helper to generate an index specifying document.\\n\\n    Takes a list of (key, direction) pairs.\\n    '\n    if isinstance(index_list, collections.Mapping):\n        raise TypeError(('passing a dict to sort/create_index/hint is not allowed - use a list of tuples instead. did you mean %r?' % list(iteritems(index_list))))\n    elif (not isinstance(index_list, (list, tuple))):\n        raise TypeError(('must use a list of (key, direction) pairs, not: ' + repr(index_list)))\n    if (not len(index_list)):\n        raise ValueError('key_or_list must not be the empty list')\n    index = SON()\n    for (key, value) in index_list:\n        if (not isinstance(key, string_type)):\n            raise TypeError('first item in each key pair must be a string')\n        if (not isinstance(value, (string_type, int, collections.Mapping))):\n            raise TypeError(\"second item in each key pair must be 1, -1, '2d', 'geoHaystack', or another valid MongoDB index specifier.\")\n        index[key] = value\n    return index\n", "label": 0}
{"function": "\n\ndef _build_spanning_datetimes(self, d1, d2):\n    businessdays = list(self.iterbusinessdays(d1, d2))\n    if (len(businessdays) == 0):\n        return businessdays\n    businessdays = [datetime.datetime.combine(d, self.business_hours[0]) for d in businessdays]\n    if (d1 > businessdays[0]):\n        businessdays[0] = d1\n    if (self.isbusinessday(d2) and (d2 >= datetime.datetime.combine(d2, self.business_hours[0]))):\n        businessdays.append(datetime.datetime.combine(d2, self.business_hours[1]))\n        if (d2 < businessdays[(- 1)]):\n            businessdays[(- 1)] = datetime.datetime.combine(businessdays[(- 1)], d2.time())\n    elif (len(businessdays) == 1):\n        businessdays.append(datetime.datetime.combine(businessdays[0], self.business_hours[1]))\n    else:\n        businessdays[(- 1)] = datetime.datetime.combine(businessdays[(- 1)], self.business_hours[1])\n    return businessdays\n", "label": 0}
{"function": "\n\ndef write(self, data, escape=True):\n    if (not escape):\n        self.body.write(str(data))\n    elif (hasattr(data, 'xml') and callable(data.xml)):\n        self.body.write(data.xml())\n    else:\n        if (not isinstance(data, (str, unicode))):\n            data = str(data)\n        elif isinstance(data, unicode):\n            data = data.encode('utf8', 'xmlcharrefreplace')\n        data = cgi.escape(data, True).replace(\"'\", '&#x27;')\n        self.body.write(data)\n", "label": 0}
{"function": "\n\ndef get_fields_to_translatable_models(model):\n    if (model in _F2TM_CACHE):\n        return _F2TM_CACHE[model]\n    results = []\n    if NEW_META_API:\n        for f in model._meta.get_fields():\n            if (f.is_relation and f.related_model):\n                if (get_translatable_fields_for_model(f.related_model) is not None):\n                    results.append((f.name, f.related_model))\n    else:\n        for field_name in model._meta.get_all_field_names():\n            (field_object, modelclass, direct, m2m) = model._meta.get_field_by_name(field_name)\n            if (direct and isinstance(field_object, RelatedField)):\n                if (get_translatable_fields_for_model(field_object.related.parent_model) is not None):\n                    results.append((field_name, field_object.related.parent_model))\n            if isinstance(field_object, RelatedObject):\n                if (get_translatable_fields_for_model(field_object.model) is not None):\n                    results.append((field_name, field_object.model))\n    _F2TM_CACHE[model] = dict(results)\n    return _F2TM_CACHE[model]\n", "label": 1}
{"function": "\n\ndef match_or_trust(self, host, der_encoded_certificate):\n    base64_encoded_certificate = b64encode(der_encoded_certificate)\n    if isfile(self.path):\n        with open(self.path) as f_in:\n            for line in f_in:\n                (known_host, _, known_cert) = line.strip().partition(':')\n                known_cert = known_cert.encode('utf-8')\n                if (host == known_host):\n                    return (base64_encoded_certificate == known_cert)\n    try:\n        makedirs(dirname(self.path))\n    except OSError:\n        pass\n    f_out = os_open(self.path, ((O_CREAT | O_APPEND) | O_WRONLY), 384)\n    if isinstance(host, bytes):\n        os_write(f_out, host)\n    else:\n        os_write(f_out, host.encode('utf-8'))\n    os_write(f_out, b':')\n    os_write(f_out, base64_encoded_certificate)\n    os_write(f_out, b'\\n')\n    os_close(f_out)\n    return True\n", "label": 0}
{"function": "\n\ndef merge(self, follower):\n    if ((follower not in self.followers) and (follower != self.creator)):\n        logger.error('user {0} is not a follower of {1}@{2}'.format(follower, self.creator, self.name))\n        return False\n    self.mergeQueue.add(follower)\n    if (len(self.mergeQueue) >= min(len(self.followers), self.pPartialBarrier)):\n        for follower in self.mergeQueue:\n            [panda.merge(follower) for panda in self.pandas]\n        self.mergeQueue.clear()\n        [panda.update_fields({\n            panda.FCONSENSUS: panda.z.generic(),\n        }) for panda in self.pandas]\n        self.pMergeClock += 1\n        self.update_fields({\n            self.FMERGECLOCK: self.pMergeClock,\n        })\n        logger.debug('merge clock {0}'.format(self.pMergeClock))\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef __init__(self, idl_parsed):\n    '\\n        Creates a new Contract from the parsed IDL JSON\\n\\n        :Parameters:\\n          idl_parsed\\n            Barrister parsed IDL as a list of dicts\\n        '\n    self.idl_parsed = idl_parsed\n    self.interfaces = {\n        \n    }\n    self.structs = {\n        \n    }\n    self.enums = {\n        \n    }\n    self.meta = {\n        \n    }\n    for e in idl_parsed:\n        if (e['type'] == 'struct'):\n            self.structs[e['name']] = Struct(e, self)\n        elif (e['type'] == 'enum'):\n            self.enums[e['name']] = Enum(e)\n        elif (e['type'] == 'interface'):\n            self.interfaces[e['name']] = Interface(e, self)\n        elif (e['type'] == 'meta'):\n            for (k, v) in list(e.items()):\n                if (k != 'type'):\n                    self.meta[k] = v\n", "label": 0}
{"function": "\n\ndef _live_receivers(self, senderkey):\n    '\\n        Filter sequence of receivers to get resolved, live receivers.\\n\\n        This checks for weak references and resolves them, then returning only\\n        live receivers.\\n        '\n    none_senderkey = _make_id(None)\n    receivers = []\n    for ((receiverkey, r_senderkey), receiver) in self.receivers:\n        if ((r_senderkey == none_senderkey) or (r_senderkey == senderkey)):\n            if isinstance(receiver, WEAKREF_TYPES):\n                receiver = receiver()\n                if (receiver is not None):\n                    receivers.append(receiver)\n            else:\n                receivers.append(receiver)\n    return receivers\n", "label": 0}
{"function": "\n\ndef _build_doc(self):\n    '\\n        Raises\\n        ------\\n        ValueError\\n            * If a URL that lxml cannot parse is passed.\\n\\n        Exception\\n            * Any other ``Exception`` thrown. For example, trying to parse a\\n              URL that is syntactically correct on a machine with no internet\\n              connection will fail.\\n\\n        See Also\\n        --------\\n        pandas.io.html._HtmlFrameParser._build_doc\\n        '\n    from lxml.html import parse, fromstring, HTMLParser\n    from lxml.etree import XMLSyntaxError\n    parser = HTMLParser(recover=False, encoding=self.encoding)\n    try:\n        r = parse(self.io, parser=parser)\n        try:\n            r = r.getroot()\n        except AttributeError:\n            pass\n    except (UnicodeDecodeError, IOError):\n        if (not _is_url(self.io)):\n            r = fromstring(self.io, parser=parser)\n            try:\n                r = r.getroot()\n            except AttributeError:\n                pass\n        else:\n            scheme = parse_url(self.io).scheme\n            if (scheme not in _valid_schemes):\n                msg = ('%r is not a valid url scheme, valid schemes are %s' % (scheme, _valid_schemes))\n                raise ValueError(msg)\n            else:\n                raise\n    else:\n        if (not hasattr(r, 'text_content')):\n            raise XMLSyntaxError('no text parsed from document', 0, 0, 0)\n    return r\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    if (not self.pk):\n        try:\n            self.position = (self.siblings(include_self=True).only('position').order_by('-position')[0].position + 1)\n        except IndexError:\n            if self.parent:\n                self.position = (self.parent.position + 1)\n            else:\n                try:\n                    self.position = (Forum.objects.only('position').order_by('-position')[0].position + 1)\n                except IndexError:\n                    self.position = 1\n        if (self.position != 1):\n            Forum.objects.update_position(self.position)\n        if self.parent:\n            self.level = (self.parent.level + 1)\n    super(Forum, self).save(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef level_matches(self, level, consumer_level):\n    '\\n        >>> l = Logger([])\\n        >>> l.level_matches(3, 4)\\n        False\\n        >>> l.level_matches(3, 2)\\n        True\\n        >>> l.level_matches(slice(None, 3), 3)\\n        False\\n        >>> l.level_matches(slice(None, 3), 2)\\n        True\\n        >>> l.level_matches(slice(1, 3), 1)\\n        True\\n        >>> l.level_matches(slice(2, 3), 1)\\n        False\\n        '\n    if isinstance(level, slice):\n        (start, stop) = (level.start, level.stop)\n        if ((start is not None) and (start > consumer_level)):\n            return False\n        if ((stop is not None) and (stop <= consumer_level)):\n            return False\n        return True\n    else:\n        return (level >= consumer_level)\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = Archive()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = InvalidOperation()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef download_set(set_id, get_filename, size_label=None):\n    \"\\n    Download the set with 'set_id' to the current directory.\\n\\n    @param set_id: str, id of the photo set\\n    @param get_filename: Function, function that creates a filename for the photo\\n    @param size_label: str|None, size to download (or None for largest available)\\n    \"\n    suffix = (' ({})'.format(size_label) if size_label else '')\n    pset = Flickr.Photoset(id=set_id)\n    photos = pset.getPhotos()\n    pagenum = 2\n    while True:\n        try:\n            page = pset.getPhotos(page=pagenum)\n            photos.extend(page)\n            pagenum += 1\n        except FlickrAPIError as ex:\n            if (ex.code == 1):\n                break\n            raise\n    dirname = pset.title.replace(os.sep, '_')\n    if (not os.path.exists(dirname)):\n        os.mkdir(dirname)\n    for photo in photos:\n        fname = get_full_path(dirname, get_filename(pset, photo, suffix))\n        if os.path.exists(fname):\n            print('Skipping {0}, as it exists already'.format(fname))\n            continue\n        print('Saving: {0}'.format(fname))\n        photo.save(fname, size_label)\n        info = photo.getInfo()\n        taken = parser.parse(info['taken'])\n        taken_unix = time.mktime(taken.timetuple())\n        os.utime(fname, (taken_unix, taken_unix))\n", "label": 0}
{"function": "\n\ndef boundary_edges(polys):\n    'Returns the edges that are on the boundary of a mesh, as defined by belonging to only 1 face'\n    edges = dict()\n    for (i, poly) in enumerate(np.sort(polys)):\n        for (a, b) in [(0, 1), (1, 2), (0, 2)]:\n            key = (poly[a], poly[b])\n            if (key not in edges):\n                edges[key] = []\n            edges[key].append(i)\n    epts = []\n    for (edge, faces) in edges.items():\n        if (len(faces) == 1):\n            epts.append(edge)\n    return np.array(epts)\n", "label": 0}
{"function": "\n\ndef serialize(self, previous):\n    params = {\n        \n    }\n    unsaved_keys = (self._unsaved_values or set())\n    previous = (previous or self._previous or {\n        \n    })\n    for (k, v) in self.items():\n        if ((k == 'id') or (isinstance(k, str) and k.startswith('_'))):\n            continue\n        elif isinstance(v, APIResource):\n            continue\n        elif hasattr(v, 'serialize'):\n            params[k] = v.serialize(previous.get(k, None))\n        elif (k in unsaved_keys):\n            params[k] = _compute_diff(v, previous.get(k, None))\n        elif ((k == 'additional_owners') and (v is not None)):\n            params[k] = _serialize_list(v, previous.get(k, None))\n    return params\n", "label": 1}
{"function": "\n\ndef has_changes(self):\n    if self.is_dirty:\n        return True\n    for child in self._db_actions:\n        if child.has_changes():\n            return True\n    for child in self._db_tags:\n        if child.has_changes():\n            return True\n    return False\n", "label": 0}
{"function": "\n\ndef unpack_args(self, arg, kwarg_name, kwargs):\n    try:\n        new_args = kwargs[kwarg_name]\n        if (not isinstance(new_args, list)):\n            new_args = [new_args]\n        for i in new_args:\n            if (not isinstance(i, str)):\n                raise MesonException('html_args values must be strings.')\n    except KeyError:\n        return []\n    if (len(new_args) > 0):\n        return [(arg + '@@'.join(new_args))]\n    return []\n", "label": 0}
{"function": "\n\ndef ntime(*args, **kwargs):\n    if args:\n        if (args[0] is None):\n            return None\n    elif kwargs:\n        if (None in [v for (k, v) in kwargs.iteritems() if (k != 'tz')]):\n            return None\n    return SaneTime(*args, **kwargs)\n", "label": 0}
{"function": "\n\ndef str_replace(arr, pat, repl, n=(- 1), case=True, flags=0):\n    '\\n    Replace occurrences of pattern/regex in the Series/Index with\\n    some other string. Equivalent to :meth:`str.replace` or\\n    :func:`re.sub`.\\n\\n    Parameters\\n    ----------\\n    pat : string\\n        Character sequence or regular expression\\n    repl : string\\n        Replacement sequence\\n    n : int, default -1 (all)\\n        Number of replacements to make from start\\n    case : boolean, default True\\n        If True, case sensitive\\n    flags : int, default 0 (no flags)\\n        re module flags, e.g. re.IGNORECASE\\n\\n    Returns\\n    -------\\n    replaced : Series/Index of objects\\n    '\n    use_re = ((not case) or (len(pat) > 1) or flags)\n    if use_re:\n        if (not case):\n            flags |= re.IGNORECASE\n        regex = re.compile(pat, flags=flags)\n        n = (n if (n >= 0) else 0)\n\n        def f(x):\n            return regex.sub(repl, x, count=n)\n    else:\n        f = (lambda x: x.replace(pat, repl, n))\n    return _na_map(f, arr)\n", "label": 0}
{"function": "\n\ndef _AddToHostData(self, host_data, artifact, data, parser):\n    'Parse raw data collected for an artifact into the host_data table.'\n    if (type(data) != dict):\n        raise test_lib.Error(('Data for %s is not of type dictionary.' % artifact))\n    rdfs = []\n    stats = []\n    files = []\n    for (path, lines) in data.items():\n        stat = self.CreateStat(path)\n        stats.append(stat)\n        file_obj = StringIO.StringIO(lines)\n        files.append(file_obj)\n        if (not parser.process_together):\n            rdfs.extend(list(parser.Parse(stat, file_obj, None)))\n    if parser.process_together:\n        rdfs = list(parser.ParseMultiple(stats, files, None))\n    host_data[artifact] = self.SetArtifactData(anomaly=[a for a in rdfs if isinstance(a, rdf_anomaly.Anomaly)], parsed=[r for r in rdfs if (not isinstance(r, rdf_anomaly.Anomaly))], raw=stats, results=host_data.get(artifact))\n    return host_data\n", "label": 0}
{"function": "\n\ndef login(username, password):\n    verify_to_schema(UserLoginSchema, {\n        'username': username,\n        'password': password,\n    })\n    user = api.user.get_user(username_lower=username.lower())\n    if (user is None):\n        raise WebException('No user with that username exists!')\n    if user.get('disabled', False):\n        raise WebException('This account is disabled.')\n    if confirm_password(password, user['password']):\n        if (user['uid'] is not None):\n            session['uid'] = user['uid']\n            if (user['type'] == 0):\n                session['admin'] = True\n            session.permanent = True\n        else:\n            raise WebException('Login error. Error code: 1.')\n    else:\n        raise WebException('Wrong password.')\n", "label": 0}
{"function": "\n\ndef apply_theme(self, property_values):\n    ' Apply a set of theme values which will be used rather than\\n        defaults, but will not override application-set values.\\n\\n        The passed-in dictionary may be kept around as-is and shared with\\n        other instances to save memory (so neither the caller nor the\\n        |HasProps| instance should modify it).\\n\\n        .. |HasProps| replace:: :class:`~bokeh.properties.HasProps`\\n\\n        '\n    old_dict = None\n    if hasattr(self, '__themed_values__'):\n        old_dict = getattr(self, '__themed_values__')\n    if (old_dict is property_values):\n        return\n    removed = set()\n    if (old_dict is not None):\n        removed.update(set(old_dict.keys()))\n    added = set(property_values.keys())\n    old_values = dict()\n    for k in added.union(removed):\n        old_values[k] = getattr(self, k)\n    if (len(property_values) > 0):\n        setattr(self, '__themed_values__', property_values)\n    elif hasattr(self, '__themed_values__'):\n        delattr(self, '__themed_values__')\n    for (k, v) in old_values.items():\n        prop = self.lookup(k)\n        prop.trigger_if_changed(self, v)\n", "label": 0}
{"function": "\n\ndef random_interface(dut, exclude=None):\n    exclude = ([] if (exclude is None) else exclude)\n    interfaces = dut.api('interfaces')\n    names = [name for name in list(interfaces.keys()) if name.startswith('Et')]\n    exclude_interfaces = dut.settings.get('exclude_interfaces', [])\n    if exclude_interfaces:\n        exclude_interfaces = exclude_interfaces.split(',')\n    exclude_interfaces.extend(exclude)\n    if (sorted(exclude_interfaces) == sorted(names)):\n        raise TypeError('unable to allocate interface from dut')\n    choices = set(names).difference(exclude)\n    return random.choice(list(choices))\n", "label": 0}
{"function": "\n\ndef save(self, *args, **kwargs):\n    '\\n        A custom save that publishes or unpublishes the object where\\n        appropriate.\\n\\n        Save with keyword argument obj.save(publish=False) to skip the process.\\n        '\n    from bakery import tasks\n    from django.contrib.contenttypes.models import ContentType\n    if (not kwargs.pop('publish', True)):\n        super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n    else:\n        try:\n            preexisting = self.__class__.objects.get(pk=self.pk)\n        except self.__class__.DoesNotExist:\n            preexisting = None\n        if (not preexisting):\n            if self.get_publication_status():\n                action = 'publish'\n            else:\n                action = None\n        elif ((not self.get_publication_status()) and preexisting.get_publication_status()):\n            action = 'unpublish'\n        elif self.get_publication_status():\n            action = 'publish'\n        else:\n            action = None\n        with transaction.atomic():\n            super(AutoPublishingBuildableModel, self).save(*args, **kwargs)\n        ct = ContentType.objects.get_for_model(self.__class__)\n        if (action == 'publish'):\n            tasks.publish_object.delay(ct.pk, self.pk)\n        elif (action == 'unpublish'):\n            tasks.unpublish_object.delay(ct.pk, self.pk)\n", "label": 1}
{"function": "\n\ndef test_independent_generators(self):\n    N = 10\n    random_seed(1)\n    py_numbers = [random_random() for i in range(N)]\n    numpy_seed(2)\n    np_numbers = [numpy_random() for i in range(N)]\n    random_seed(1)\n    numpy_seed(2)\n    pairs = [(random_random(), numpy_random()) for i in range(N)]\n    self.assertPreciseEqual([p[0] for p in pairs], py_numbers)\n    self.assertPreciseEqual([p[1] for p in pairs], np_numbers)\n", "label": 0}
{"function": "\n\ndef write_image(results, output_filename=None):\n    print(('Gathered %s results that represents a mandelbrot image (using %s chunks that are computed jointly by %s workers).' % (len(results), CHUNK_COUNT, WORKERS)))\n    if (not output_filename):\n        return\n    try:\n        from PIL import Image\n    except ImportError as e:\n        raise RuntimeError(('Pillow is required to write image files: %s' % e))\n    color_max = 0\n    for (_point, color) in results:\n        color_max = max(color, color_max)\n    img = Image.new('L', IMAGE_SIZE, 'black')\n    pixels = img.load()\n    for ((x, y), color) in results:\n        if (color_max == 0):\n            color = 0\n        else:\n            color = int(((float(color) / color_max) * 255.0))\n        pixels[(x, y)] = color\n    img.save(output_filename)\n", "label": 0}
{"function": "\n\ndef Validate(self, value, unused_key=None):\n    'Validates a subnet.'\n    if (value is None):\n        raise validation.MissingAttribute('subnet must be specified')\n    if (not isinstance(value, basestring)):\n        raise validation.ValidationError((\"subnet must be a string, not '%r'\" % type(value)))\n    try:\n        ipaddr.IPNetwork(value)\n    except ValueError:\n        raise validation.ValidationError(('%s is not a valid IPv4 or IPv6 subnet' % value))\n    parts = value.split('/')\n    if ((len(parts) == 2) and (not re.match('^[0-9]+$', parts[1]))):\n        raise validation.ValidationError(('Prefix length of subnet %s must be an integer (quad-dotted masks are not supported)' % value))\n    return value\n", "label": 0}
{"function": "\n\ndef _extract_monitor_data(self, monitor):\n    if (self._monitor_type is not None):\n        raise TypeError('Your result `%s` already extracted data from a `%s` monitor. Please use a new empty result for a new monitor.')\n    self._monitor_type = monitor.__class__.__name__\n    if isinstance(monitor, SpikeCounter):\n        self._extract_spike_counter(monitor)\n    elif isinstance(monitor, VanRossumMetric):\n        self._extract_van_rossum_metric(monitor)\n    elif isinstance(monitor, PopulationSpikeCounter):\n        self._extract_population_spike_counter(monitor)\n    elif isinstance(monitor, StateSpikeMonitor):\n        self._extract_state_spike_monitor(monitor)\n    elif isinstance(monitor, PopulationRateMonitor):\n        self._extract_population_rate_monitor(monitor)\n    elif isinstance(monitor, ISIHistogramMonitor):\n        self._extract_isi_hist_monitor(monitor)\n    elif isinstance(monitor, SpikeMonitor):\n        self._extract_spike_monitor(monitor)\n    elif isinstance(monitor, MultiStateMonitor):\n        self._extract_multi_state_monitor(monitor)\n    elif isinstance(monitor, StateMonitor):\n        self._extract_state_monitor(monitor)\n    else:\n        raise ValueError(('Monitor Type %s is not supported (yet)' % str(type(monitor))))\n", "label": 1}
{"function": "\n\ndef test_cache(self):\n    mocked_repo = MagicMock()\n    mocked_commit = MagicMock()\n    mocked_repo.lookup_reference().resolve().target = 'head'\n    mocked_repo.walk.return_value = [mocked_commit]\n    mocked_commit.commit_time = 1411135000\n    mocked_commit.hex = '1111111111'\n    cache = CommitCache(mocked_repo)\n    cache.update()\n    cache['2014-09-20'] = Commit(1, 1, '1111111111')\n    assert (sorted(cache.keys()) == ['2014-09-19', '2014-09-20'])\n    asserted_time = datetime.fromtimestamp(mocked_commit.commit_time)\n    asserted_time = '{}-{}-{}'.format(asserted_time.hour, asserted_time.minute, asserted_time.second)\n    assert (repr(cache['2014-09-19']) == ('[%s-1111111111]' % asserted_time))\n    del cache['2014-09-20']\n    for commit_date in cache:\n        assert (commit_date == '2014-09-19')\n    mocked_repo.lookup_reference.has_calls([call('HEAD')])\n    mocked_repo.walk.assert_called_once_with('head', GIT_SORT_TIME)\n    assert (mocked_repo.lookup_reference().resolve.call_count == 2)\n", "label": 0}
{"function": "\n\ndef __deleteData__(self, uri, query, auth=True):\n    if self.ssl:\n        url = ('%s://%s:%s%s' % ('https', self.hostname, self.port, uri))\n    else:\n        url = ('%s://%s:%s%s' % ('http', self.hostname, self.port, uri))\n    full_uri = ('%s' % uri)\n    if (query != None):\n        full_uri = ('%s?%s' % (full_uri, urllib.urlencode(query)))\n    if self.token:\n        if auth:\n            headers = {\n                'Content-Type': 'application/json',\n                'X-Auth-Token': self.token,\n            }\n        else:\n            headers = {\n                'Content-Type': 'application/json',\n            }\n    else:\n        date = utils.formatdate()\n        if auth:\n            sig = self.__signRequest__('DELETE', full_uri, date, 'application/json')\n            headers = {\n                'Content-Type': 'application/json',\n                'Date': date,\n                'Authorization': sig,\n            }\n        else:\n            headers = {\n                'Content-Type': 'application/json',\n                'Date': date,\n            }\n    r = requests.delete(url, params=query, headers=headers)\n    return self.__processResponse__({\n        'status': r.status_code,\n        'data': r.text,\n    })\n", "label": 0}
{"function": "\n\ndef candidates(self, items, artist, album, va_likely):\n    'Returns a list of AlbumInfo objects for discogs search results\\n        matching an album and artist (if not various).\\n        '\n    if (not self.discogs_client):\n        return\n    if va_likely:\n        query = album\n    else:\n        query = ('%s %s' % (artist, album))\n    try:\n        return self.get_albums(query)\n    except DiscogsAPIError as e:\n        self._log.debug('API Error: {0} (query: {1})', e, query)\n        if (e.status_code == 401):\n            self.reset_auth()\n            return self.candidates(items, artist, album, va_likely)\n        else:\n            return []\n    except CONNECTION_ERRORS:\n        self._log.debug('Connection error in album search', exc_info=True)\n        return []\n", "label": 0}
{"function": "\n\ndef check_sizes(size, width, height):\n    'Check that these arguments, in supplied, are consistent.\\n    Return a (width, height) pair.\\n    '\n    if (not size):\n        return (width, height)\n    if (len(size) != 2):\n        raise ValueError('size argument should be a pair (width, height)')\n    if ((width is not None) and (width != size[0])):\n        raise ValueError(('size[0] (%r) and width (%r) should match when both are used.' % (size[0], width)))\n    if ((height is not None) and (height != size[1])):\n        raise ValueError(('size[1] (%r) and height (%r) should match when both are used.' % (size[1], height)))\n    return size\n", "label": 0}
{"function": "\n\ndef Execute(self, opt, args):\n    self.opt = opt\n    project_list = self.GetProjects(args)\n    pending = []\n    branch = None\n    if opt.branch:\n        branch = opt.branch\n    for project in project_list:\n        if opt.current_branch:\n            cbr = project.CurrentBranch\n            up_branch = project.GetUploadableBranch(cbr)\n            if up_branch:\n                avail = [up_branch]\n            else:\n                avail = None\n                print(('ERROR: Current branch (%s) not pushable. You may be able to type \"git branch --set-upstream-to m/master\" to fix your branch.' % str(cbr)), file=sys.stderr)\n        else:\n            avail = project.GetUploadableBranches(branch)\n        if avail:\n            pending.append((project, avail))\n    if (pending and (not opt.bypass_hooks)):\n        hook = RepoHook('pre-push', self.manifest.repo_hooks_project, self.manifest.topdir, abort_if_user_denies=True)\n        pending_proj_names = [project.name for (project, avail) in pending]\n        pending_worktrees = [project.worktree for (project, avail) in pending]\n        try:\n            hook.Run(opt.allow_all_hooks, project_list=pending_proj_names, worktree_list=pending_worktrees)\n        except HookError as e:\n            print(('ERROR: %s' % str(e)), file=sys.stderr)\n            return\n    if (not pending):\n        print('no branches ready for push', file=sys.stderr)\n    elif ((len(pending) == 1) and (len(pending[0][1]) == 1)):\n        self._SingleBranch(opt, pending[0][1][0])\n    else:\n        self._MultipleBranches(opt, pending)\n", "label": 1}
{"function": "\n\ndef to_json(self, user):\n    ret = super(GitHubNodeSettings, self).to_json(user)\n    user_settings = user.get_addon('github')\n    ret.update({\n        'user_has_auth': (user_settings and user_settings.has_auth),\n        'is_registration': self.owner.is_registration,\n    })\n    if (self.user_settings and self.user_settings.has_auth):\n        valid_credentials = False\n        owner = self.user_settings.owner\n        connection = GitHubClient(external_account=self.external_account)\n        valid_credentials = True\n        try:\n            repos = itertools.chain.from_iterable((connection.repos(), connection.my_org_repos()))\n            repo_names = ['{0} / {1}'.format(repo.owner.login, repo.name) for repo in repos]\n        except GitHubError:\n            repo_names = []\n            valid_credentials = False\n        if (owner == user):\n            ret.update({\n                'repo_names': repo_names,\n            })\n        ret.update({\n            'node_has_auth': True,\n            'github_user': (self.user or ''),\n            'github_repo': (self.repo or ''),\n            'github_repo_full_name': ('{0} / {1}'.format(self.user, self.repo) if (self.user and self.repo) else ''),\n            'auth_osf_name': owner.fullname,\n            'auth_osf_url': owner.url,\n            'auth_osf_id': owner._id,\n            'github_user_name': self.external_account.display_name,\n            'github_user_url': self.external_account.profile_url,\n            'is_owner': (owner == user),\n            'valid_credentials': valid_credentials,\n            'addons_url': web_url_for('user_addons'),\n            'files_url': self.owner.web_url_for('collect_file_trees'),\n        })\n    return ret\n", "label": 1}
{"function": "\n\ndef iter_source_code(paths, config, skipped):\n    'Iterate over all Python source files defined in paths.'\n    for path in paths:\n        if os.path.isdir(path):\n            if should_skip(path, config, os.getcwd()):\n                skipped.append(path)\n                continue\n            for (dirpath, dirnames, filenames) in os.walk(path, topdown=True):\n                for dirname in list(dirnames):\n                    if should_skip(dirname, config, dirpath):\n                        skipped.append(dirname)\n                        dirnames.remove(dirname)\n                for filename in filenames:\n                    if filename.endswith('.py'):\n                        if should_skip(filename, config, dirpath):\n                            skipped.append(filename)\n                        else:\n                            (yield os.path.join(dirpath, filename))\n        else:\n            (yield path)\n", "label": 1}
{"function": "\n\ndef _parse(self, opt, fg, bg, attr):\n    if (not opt):\n        return _Color(fg, bg, attr)\n    v = self._config.GetString(('%s.%s' % (self._section, opt)))\n    if (v is None):\n        return _Color(fg, bg, attr)\n    v = v.strip().lower()\n    if (v == 'reset'):\n        return RESET\n    elif (v == ''):\n        return _Color(fg, bg, attr)\n    have_fg = False\n    for a in v.split(' '):\n        if is_color(a):\n            if have_fg:\n                bg = a\n            else:\n                fg = a\n        elif is_attr(a):\n            attr = a\n    return _Color(fg, bg, attr)\n", "label": 0}
{"function": "\n\ndef isnpint(ctx, x):\n    '\\n        Determine if *x* is a nonpositive integer.\\n        '\n    if (not x):\n        return True\n    if hasattr(x, '_mpf_'):\n        (sign, man, exp, bc) = x._mpf_\n        return (sign and (exp >= 0))\n    if hasattr(x, '_mpc_'):\n        return ((not x.imag) and ctx.isnpint(x.real))\n    if (type(x) in int_types):\n        return (x <= 0)\n    if isinstance(x, ctx.mpq):\n        (p, q) = x._mpq_\n        if (not p):\n            return True\n        return ((q == 1) and (p <= 0))\n    return ctx.isnpint(ctx.convert(x))\n", "label": 1}
{"function": "\n\ndef __init__(self, default, *items):\n    DiagramItem.__init__(self, 'g')\n    assert (default < len(items))\n    self.default = default\n    self.items = [wrapString(item) for item in items]\n    self.width = ((ARC_RADIUS * 4) + max((item.width for item in self.items)))\n    self.up = 0\n    self.down = 0\n    self.yAdvance = self.items[self.default].yAdvance\n    for (i, item) in enumerate(self.items):\n        if (i < default):\n            self.up += max(ARC_RADIUS, ((item.up + item.down) + VERTICAL_SEPARATION))\n        elif (i == default):\n            self.up += max(ARC_RADIUS, item.up)\n            self.down += max(ARC_RADIUS, item.down)\n        else:\n            assert (i > default)\n            self.down += max(ARC_RADIUS, ((VERTICAL_SEPARATION + item.up) + item.down))\n    if DEBUG:\n        self.attrs['data-updown'] = '{0} {1}'.format(self.up, self.down)\n        self.attrs['data-type'] = 'choice'\n", "label": 0}
{"function": "\n\ndef _children_updated(self, object, name, event):\n    ' Handles the children of a node being changed.\\n        '\n    name = name[:(- 6)]\n    self.log_change(self._get_undo_item, object, name, event)\n    start = event.index\n    n = len(event.added)\n    end = (start + len(event.removed))\n    tree = self._tree\n    for (expanded, node, nid) in self._object_info_for(object, name):\n        children = node.get_children(object)\n        if expanded:\n            for cnid in self._nodes_for(nid)[start:end]:\n                self._delete_node(cnid)\n            remaining = (len(children) - len(event.removed))\n            child_index = 0\n            for child in event.added:\n                (child, child_node) = self._node_for(child)\n                if (child_node is not None):\n                    insert_index = ((start + child_index) if (start <= remaining) else None)\n                    self._insert_node(nid, insert_index, child_node, child)\n                    child_index += 1\n        else:\n            dummy = getattr(nid, '_dummy', None)\n            if ((dummy is None) and (len(children) > 0)):\n                nid._dummy = QtGui.QTreeWidgetItem(nid)\n            elif ((dummy is not None) and (len(children) == 0)):\n                nid.removeChild(dummy)\n                del nid._dummy\n        if node.can_auto_open(object):\n            nid.setExpanded(True)\n", "label": 1}
{"function": "\n\ndef _dump_json(data):\n    options = getattr(settings, 'JSON_OPTIONS', {\n        \n    })\n    if ('cls' in options):\n        if isinstance(options['cls'], six.string_types):\n            options['cls'] = import_string(options['cls'])\n    else:\n        try:\n            use_django = getattr(settings, 'JSON_USE_DJANGO_SERIALIZER')\n        except AttributeError:\n            use_django = True\n        else:\n            warnings.warn(\"JSON_USE_DJANGO_SERIALIZER is deprecated and will be removed. Please use JSON_OPTIONS['cls'] instead.\", DeprecationWarning)\n        if use_django:\n            options['cls'] = DjangoJSONEncoder\n    return json.dumps(data, **options)\n", "label": 0}
{"function": "\n\n@gen.engine\ndef Transform(self, client, device_photo, callback):\n    from device import Device\n    from user_photo import UserPhoto\n    device_id = device_photo.device_id\n    if (device_id not in MoveDevicePhoto._device_to_user_cache):\n        query_expr = ('device.device_id={t}', {\n            't': device_id,\n        })\n        devices = (yield gen.Task(Device.IndexQuery, client, query_expr, None))\n        assert (len(devices) == 1)\n        MoveDevicePhoto._device_to_user_cache[device_id] = devices[0].user_id\n    user_id = MoveDevicePhoto._device_to_user_cache[device_id]\n    existing = (yield gen.Task(UserPhoto.Query, client, user_id, device_photo.photo_id, None, must_exist=False))\n    if (existing is None):\n        logging.info('Creating user photo for photo %s, device %s, user %s', device_photo.photo_id, device_id, user_id)\n        user_photo = UserPhoto.CreateFromKeywords(photo_id=device_photo.photo_id, user_id=user_id, asset_keys=device_photo.asset_keys)\n    else:\n        logging.info('Photo %s, device %s, user %s already has user photo', device_photo.photo_id, device_id, user_id)\n        user_photo = None\n    if (user_photo is not None):\n        self._LogUpdate(user_photo)\n    if (Version._mutate_items and (user_photo is not None)):\n        (yield gen.Task(user_photo.Update, client))\n    callback(device_photo)\n", "label": 0}
{"function": "\n\n@setup_cache\ndef test_set_get_delete(cache):\n    for value in range(100):\n        cache.set(value, value)\n    cache.check()\n    for value in range(100):\n        assert (cache.get(value) == value)\n    cache.check()\n    for value in range(100):\n        assert (value in cache)\n    cache.check()\n    for value in range(100):\n        assert cache.delete(value)\n    assert (cache.delete(100) == False)\n    cache.check()\n    for value in range(100):\n        cache[value] = value\n    cache.check()\n    for value in range(100):\n        assert (cache[value] == value)\n    cache.check()\n    cache.clear()\n    assert (len(cache) == 0)\n    cache.check()\n", "label": 1}
{"function": "\n\ndef assertErrorPage(self, status, message=None, pattern=''):\n    'Compare the response body with a built in error page.\\n        \\n        The function will optionally look for the regexp pattern,\\n        within the exception embedded in the error page.'\n    page = cherrypy._cperror.get_error_page(status, message=message)\n    esc = re.escape\n    epage = esc(page)\n    epage = epage.replace(esc('<pre id=\"traceback\"></pre>'), ((esc('<pre id=\"traceback\">') + '(.*)') + esc('</pre>')))\n    m = re.match(ntob(epage, self.encoding), self.body, re.DOTALL)\n    if (not m):\n        self._handlewebError(('Error page does not match; expected:\\n' + page))\n        return\n    if (pattern is None):\n        if (m and m.group(1)):\n            self._handlewebError('Error page contains traceback')\n    elif ((m is None) or (not re.search(ntob(re.escape(pattern), self.encoding), m.group(1)))):\n        msg = 'Error page does not contain %s in traceback'\n        self._handlewebError((msg % repr(pattern)))\n", "label": 0}
{"function": "\n\ndef pre_validate(self, form):\n    if self._invalid_formdata:\n        raise ValidationError(self.gettext('Not a valid choice'))\n    elif self.data:\n        obj_list = list((x[1] for x in self._get_object_list()))\n        for v in self.data:\n            if (v not in obj_list):\n                raise ValidationError(self.gettext('Not a valid choice'))\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('SlicePredicate')\n    if (self.column_names != None):\n        oprot.writeFieldBegin('column_names', TType.LIST, 1)\n        oprot.writeListBegin(TType.STRING, len(self.column_names))\n        for iter13 in self.column_names:\n            oprot.writeString(iter13)\n        oprot.writeListEnd()\n        oprot.writeFieldEnd()\n    if (self.slice_range != None):\n        oprot.writeFieldBegin('slice_range', TType.STRUCT, 2)\n        self.slice_range.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n\n    def validate(self):\n        return\n", "label": 0}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('IndexExpression')\n    if (self.column_name != None):\n        oprot.writeFieldBegin('column_name', TType.STRING, 1)\n        oprot.writeString(self.column_name)\n        oprot.writeFieldEnd()\n    if (self.op != None):\n        oprot.writeFieldBegin('op', TType.I32, 2)\n        oprot.writeI32(self.op)\n        oprot.writeFieldEnd()\n    if (self.value != None):\n        oprot.writeFieldBegin('value', TType.STRING, 3)\n        oprot.writeString(self.value)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n\n    def validate(self):\n        if (self.column_name is None):\n            raise TProtocol.TProtocolException(message='Required field column_name is unset!')\n        if (self.op is None):\n            raise TProtocol.TProtocolException(message='Required field op is unset!')\n        if (self.value is None):\n            raise TProtocol.TProtocolException(message='Required field value is unset!')\n        return\n", "label": 0}
{"function": "\n\ndef get_search_results(self, req, terms, filters):\n    if (not ('milestone' in filters)):\n        return\n    term_regexps = search_to_regexps(terms)\n    milestone_realm = Resource(self.realm)\n    for (name, due, completed, description) in MilestoneCache(self.env).milestones.itervalues():\n        if all(((r.search(description) or r.search(name)) for r in term_regexps)):\n            milestone = milestone_realm(id=name)\n            if ('MILESTONE_VIEW' in req.perm(milestone)):\n                dt = (completed if completed else (due if due else datetime_now(utc)))\n                (yield (get_resource_url(self.env, milestone, req.href), get_resource_name(self.env, milestone), dt, '', shorten_result(description, terms)))\n    for result in AttachmentModule(self.env).get_search_results(req, milestone_realm, terms):\n        (yield result)\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('query_result')\n    if (self.success is not None):\n        oprot.writeFieldBegin('success', TType.STRUCT, 0)\n        self.success.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.error is not None):\n        oprot.writeFieldBegin('error', TType.STRUCT, 1)\n        self.error.write(oprot)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\ndef __exit__(self, ex_type, ex_value, ex_traceback):\n    if (not ex_value):\n        return True\n    if isinstance(ex_value, exception.Forbidden):\n        raise Fault(webob.exc.HTTPForbidden(explanation=ex_value.format_message()))\n    elif isinstance(ex_value, exception.VersionNotFoundForAPIMethod):\n        raise\n    elif isinstance(ex_value, exception.Invalid):\n        raise Fault(exception.ConvertedException(code=ex_value.code, explanation=ex_value.format_message()))\n    elif isinstance(ex_value, TypeError):\n        exc_info = (ex_type, ex_value, ex_traceback)\n        LOG.error(_LE('Exception handling resource: %s'), ex_value, exc_info=exc_info)\n        raise Fault(webob.exc.HTTPBadRequest())\n    elif isinstance(ex_value, Fault):\n        LOG.info(_LI('Fault thrown: %s'), ex_value)\n        raise ex_value\n    elif isinstance(ex_value, webob.exc.HTTPException):\n        LOG.info(_LI('HTTP exception thrown: %s'), ex_value)\n        raise Fault(ex_value)\n    return False\n", "label": 0}
{"function": "\n\ndef check_args(self, f, methods_args):\n    to_check = []\n    args = inspect.getargspec(f)\n    for arg in self.check:\n        if (arg in args[0]):\n            to_check.append(args[0].index(arg))\n    for index in to_check:\n        arg = methods_args[(index - 1)]\n        if self.look_at.cache.get(arg, False):\n            raise FuseOSError(errno.EACCES)\n        if self.look_at.check_key(arg):\n            self.look_at.cache[arg] = True\n            raise FuseOSError(errno.ENOENT)\n        self.look_at.cache[arg] = False\n", "label": 0}
{"function": "\n\ndef _eval_is_irrational(self):\n    for t in self.args:\n        a = t.is_irrational\n        if a:\n            others = list(self.args)\n            others.remove(t)\n            if all((((x.is_rational and fuzzy_not(x.is_zero)) is True) for x in others)):\n                return True\n            return\n        if (a is None):\n            return\n    return False\n", "label": 0}
{"function": "\n\n@contextfunction\ndef core_logo_content(context, gif=False):\n    'Return current logo encoded as base64'\n    staticpath = getattr(settings, 'STATIC_DOC_ROOT', './static')\n    logopath = (staticpath + '/logo')\n    if gif:\n        logopath += '.gif'\n        mimetype = 'image/gif'\n    else:\n        logopath += '.png'\n        mimetype = 'image/png'\n    customlogo = ''\n    try:\n        conf = ModuleSetting.get_for_module('treeio.core', 'logopath')[0]\n        customlogo = (getattr(settings, 'MEDIA_ROOT', './static/media') + conf.value)\n    except:\n        pass\n    logofile = ''\n    if customlogo:\n        try:\n            logofile = open(customlogo, 'r')\n        except:\n            pass\n    if (not logofile):\n        try:\n            logofile = open(logopath, 'r')\n        except:\n            pass\n    result = ((('data:' + mimetype) + ';base64,') + base64.b64encode(logofile.read()))\n    return Markup(result)\n", "label": 0}
{"function": "\n\n@public\ndef gcd(f, g=None, *gens, **args):\n    '\\n    Compute GCD of ``f`` and ``g``.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy import gcd\\n    >>> from sympy.abc import x\\n\\n    >>> gcd(x**2 - 1, x**2 - 3*x + 2)\\n    x - 1\\n\\n    '\n    if hasattr(f, '__iter__'):\n        if (g is not None):\n            gens = ((g,) + gens)\n        return gcd_list(f, *gens, **args)\n    elif (g is None):\n        raise TypeError('gcd() takes 2 arguments or a sequence of arguments')\n    options.allowed_flags(args, ['polys'])\n    try:\n        ((F, G), opt) = parallel_poly_from_expr((f, g), *gens, **args)\n    except PolificationFailed as exc:\n        (domain, (a, b)) = construct_domain(exc.exprs)\n        try:\n            return domain.to_sympy(domain.gcd(a, b))\n        except NotImplementedError:\n            raise ComputationFailed('gcd', 2, exc)\n    result = F.gcd(G)\n    if (not opt.polys):\n        return result.as_expr()\n    else:\n        return result\n", "label": 0}
{"function": "\n\ndef _showmodule(module_name):\n    module = sys.modules[module_name]\n    if (not hasattr(module, '__file__')):\n        raise ValueError('cannot display module {0} (no __file__)'.format(module_name))\n    if module.__file__.endswith('.py'):\n        fname = module.__file__\n    else:\n        if (not module.__file__.endswith('.pyc')):\n            raise ValueError('cannot display module file {0} for {1}'.format(module.__file__, module_name))\n        fname = module.__file__[:(- 1)]\n    if (not os.path.exists(fname)):\n        raise ValueError('could not find file {0} for {1}'.format(fname, module_name))\n    (leaf_count, branch_count) = _get_samples_by_line(fname)\n    lines = []\n    with open(fname) as f:\n        for (i, line) in enumerate(f):\n            lines.append(((i + 1), cgi.escape(line), leaf_count[(i + 1)], branch_count[(i + 1)]))\n    return lines\n", "label": 0}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.settings = BootstrapSettings()\n                self.settings.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef plugins(self, *plugins):\n    import json\n    ret = []\n    available_plugins = self.get('system/plugins')\n    self.message['debug']['available_plugins'] = available_plugins\n    plugins = set(plugins)\n    enabled_plugins = set(available_plugins['enabled'])\n    if ('*' in plugins):\n        plugins = set(available_plugins['all'].keys())\n    if (not (plugins <= set(available_plugins['all'].keys()))):\n        self.fail('{}, not available!'.format(','.join(list((plugins - set(available_plugins['all'].keys()))))))\n    if (self.module.params['state'] == 'present'):\n        if (not (plugins <= enabled_plugins)):\n            ret = self.put('system/plugins', {\n                'plugins': json.dumps(list((plugins | enabled_plugins))),\n            })\n            self.changed = True\n    elif (self.module.params['state'] == 'absent'):\n        if len((enabled_plugins & plugins)):\n            ret = self.put('system/plugins', {\n                'plugins': json.dumps(list((enabled_plugins - plugins))),\n            })\n            self.changed = True\n    return ret\n", "label": 0}
{"function": "\n\ndef _compress_hextets(self, hextets):\n    'Compresses a list of hextets.\\n\\n        Compresses a list of strings, replacing the longest continuous\\n        sequence of \"0\" in the list with \"\" and adding empty strings at\\n        the beginning or at the end of the string such that subsequently\\n        calling \":\".join(hextets) will produce the compressed version of\\n        the IPv6 address.\\n\\n        Args:\\n            hextets: A list of strings, the hextets to compress.\\n\\n        Returns:\\n            A list of strings.\\n\\n        '\n    best_doublecolon_start = (- 1)\n    best_doublecolon_len = 0\n    doublecolon_start = (- 1)\n    doublecolon_len = 0\n    for (index, hextet) in enumerate(hextets):\n        if (hextet == '0'):\n            doublecolon_len += 1\n            if (doublecolon_start == (- 1)):\n                doublecolon_start = index\n            if (doublecolon_len > best_doublecolon_len):\n                best_doublecolon_len = doublecolon_len\n                best_doublecolon_start = doublecolon_start\n        else:\n            doublecolon_len = 0\n            doublecolon_start = (- 1)\n    if (best_doublecolon_len > 1):\n        best_doublecolon_end = (best_doublecolon_start + best_doublecolon_len)\n        if (best_doublecolon_end == len(hextets)):\n            hextets += ['']\n        hextets[best_doublecolon_start:best_doublecolon_end] = ['']\n        if (best_doublecolon_start == 0):\n            hextets = ([''] + hextets)\n    return hextets\n", "label": 0}
{"function": "\n\ndef handle(self, *args, **options):\n    self.sync_tenant = options.get('tenant')\n    self.sync_public = options.get('shared')\n    self.schema_name = options.get('schema_name')\n    self.executor = options.get('executor')\n    self.installed_apps = settings.INSTALLED_APPS\n    self.args = args\n    self.options = options\n    if self.schema_name:\n        if self.sync_public:\n            raise CommandError('schema should only be used with the --tenant switch.')\n        elif (self.schema_name == get_public_schema_name()):\n            self.sync_public = True\n        else:\n            self.sync_tenant = True\n    elif ((not self.sync_public) and (not self.sync_tenant)):\n        self.sync_tenant = True\n        self.sync_public = True\n    if hasattr(settings, 'TENANT_APPS'):\n        self.tenant_apps = settings.TENANT_APPS\n    if hasattr(settings, 'SHARED_APPS'):\n        self.shared_apps = settings.SHARED_APPS\n", "label": 0}
{"function": "\n\ndef delete_access_list_items(self, loadbalancer, item_ids):\n    \"\\n        Removes the item(s) from the load balancer's access list\\n        that match the provided IDs. 'item_ids' should be one or\\n        more access list item IDs.\\n        \"\n    if (not isinstance(item_ids, (list, tuple))):\n        item_ids = [item_ids]\n    valid_ids = [itm['id'] for itm in self.get_access_list(loadbalancer)]\n    bad_ids = [str(itm) for itm in item_ids if (itm not in valid_ids)]\n    if bad_ids:\n        raise exc.AccessListIDNotFound(('The following ID(s) are not valid Access List items: %s' % ', '.join(bad_ids)))\n    items = '&'.join([('id=%s' % item_id) for item_id in item_ids])\n    uri = ('/loadbalancers/%s/accesslist?%s' % (utils.get_id(loadbalancer), items))\n    (resp, body) = self.api.method_delete(uri)\n    return body\n", "label": 0}
{"function": "\n\n@expose(help='Change directory to site webroot')\ndef cd(self):\n    if (not self.app.pargs.site_name):\n        try:\n            while (not self.app.pargs.site_name):\n                self.app.pargs.site_name = input('Enter site name : ').strip()\n        except IOError as e:\n            Log.error(self, 'Unable to read input, please try again')\n    self.app.pargs.site_name = self.app.pargs.site_name.strip()\n    (ee_domain, ee_www_domain) = ValidateDomain(self.app.pargs.site_name)\n    if (not check_domain_exists(self, ee_domain)):\n        Log.error(self, 'site {0} does not exist'.format(ee_domain))\n    ee_site_webroot = getSiteInfo(self, ee_domain).site_path\n    EEFileUtils.chdir(self, ee_site_webroot)\n    try:\n        subprocess.call(['bash'])\n    except OSError as e:\n        Log.debug(self, '{0}{1}'.format(e.errno, e.strerror))\n        Log.error(self, 'unable to change directory')\n", "label": 0}
{"function": "\n\ndef gcd(self, other):\n    'Return Factors of ``gcd(self, other)``. The keys are\\n        the intersection of factors with the minimum exponent for\\n        each factor.\\n\\n        Examples\\n        ========\\n\\n        >>> from sympy.core.exprtools import Factors\\n        >>> from sympy.abc import x, y, z\\n        >>> a = Factors((x*y**2).as_powers_dict())\\n        >>> b = Factors((x*y/z).as_powers_dict())\\n        >>> a.gcd(b)\\n        Factors({x: 1, y: 1})\\n        '\n    if (not isinstance(other, Factors)):\n        other = Factors(other)\n        if other.is_zero:\n            return Factors(self.factors)\n    factors = {\n        \n    }\n    for (factor, exp) in self.factors.items():\n        (factor, exp) = (sympify(factor), sympify(exp))\n        if (factor in other.factors):\n            lt = (exp - other.factors[factor]).is_negative\n            if (lt == True):\n                factors[factor] = exp\n            elif (lt == False):\n                factors[factor] = other.factors[factor]\n    return Factors(factors)\n", "label": 0}
{"function": "\n\ndef get_args_without_osxims():\n    new_argv = []\n    i = 1\n    while (i < len(sys.argv)):\n        arg = sys.argv[i]\n        next_arg = (sys.argv[(i + 1)] if ((i + 1) < len(sys.argv)) else None)\n        if ((arg == '-isysroot') and ('SDKs/MacOSX' in next_arg)):\n            i = (i + 2)\n        elif ('-mmacosx-version-min' in arg):\n            i = (i + 1)\n        elif (arg == '-F/Applications/Xcode.app/Contents/Developer/Library/Frameworks'):\n            i = (i + 1)\n        else:\n            i = (i + 1)\n            new_argv.append(arg)\n    return new_argv\n", "label": 0}
{"function": "\n\ndef GroupPositionSetpointGet(self, socketId, GroupName, nbElement):\n    command = (('GroupPositionSetpointGet(' + GroupName) + ',')\n    for i in range(nbElement):\n        if (i > 0):\n            command += ','\n        command += 'double *'\n    command += ')'\n    (error, returnedString) = self.Send(socketId, command)\n    if (error != 0):\n        return [error, returnedString]\n    (i, j, retList) = (0, 0, [error])\n    for paramNb in range(nbElement):\n        while (((i + j) < len(returnedString)) and (returnedString[(i + j)] != ',')):\n            j += 1\n        retList.append(eval(returnedString[i:(i + j)]))\n        (i, j) = (((i + j) + 1), 0)\n    return retList\n", "label": 0}
{"function": "\n\ndef setup_app(app, config):\n    \"Configure flask app.\\n\\n    :param app: Flask app object\\n    :param config: api's config\\n    :type  config: dict\\n    \"\n    flask_config = (config.get('flask') or {\n        \n    })\n    for (key, value) in flask_config.items():\n        if (key == 'PERMANENT_SESSION_LIFETIME'):\n            app.config[key] = datetime.timedelta(days=int(value))\n        else:\n            app.config[key] = value\n    app_config = (config.get('app') or {\n        \n    })\n    for (key, value) in app_config.items():\n        app.config[key] = value\n    app.config['plugins'] = (app.config.get('plugins') or {\n        \n    })\n    for (name, config) in app.config['plugins'].items():\n        plugin_class = get_cls_with_path(config.pop('class'))\n        plugin = plugin_class(config)\n        app.config['plugins'][name] = plugin\n    default_class_sign = ''\n    app.config['responses'] = (app.config.get('responses') or {\n        \n    })\n    for (name, class_path) in app.config['responses'].items():\n        if (name == 'default'):\n            default_class_sign = class_path\n        else:\n            response_class = get_cls_with_path(class_path)\n            app.config['responses'][name] = response_class\n    app.config['responses']['default'] = app.config['responses'][default_class_sign]\n", "label": 1}
{"function": "\n\ndef connectionLost(self, reason):\n    if (self.bodyDecoder is not None):\n        try:\n            try:\n                self.bodyDecoder.noMoreData()\n            except PotentialDataLoss:\n                self.response._bodyDataFinished(Failure())\n            except _DataLoss:\n                self.response._bodyDataFinished(Failure(ResponseFailed([reason, Failure()], self.response)))\n            else:\n                self.response._bodyDataFinished()\n        except:\n            log.err()\n    elif (self.state != DONE):\n        if self._everReceivedData:\n            exceptionClass = ResponseFailed\n        else:\n            exceptionClass = ResponseNeverReceived\n        self._responseDeferred.errback(Failure(exceptionClass([reason])))\n        del self._responseDeferred\n", "label": 0}
{"function": "\n\ndef __init__(self, generator, name=None, storage=None, cachefile_backend=None, cachefile_strategy=None):\n    '\\n        :param generator: The object responsible for generating a new image.\\n        :param name: The filename\\n        :param storage: A Django storage object that will be used to save the\\n            file.\\n        :param cachefile_backend: The object responsible for managing the\\n            state of the file.\\n        :param cachefile_strategy: The object responsible for handling events\\n            for this file.\\n\\n        '\n    self.generator = generator\n    if (not name):\n        try:\n            name = generator.cachefile_name\n        except AttributeError:\n            fn = get_by_qname(settings.IMAGEKIT_CACHEFILE_NAMER, 'namer')\n            name = fn(generator)\n    self.name = name\n    storage = (storage or getattr(generator, 'cachefile_storage', None) or get_singleton(settings.IMAGEKIT_DEFAULT_FILE_STORAGE, 'file storage backend'))\n    self.cachefile_backend = (cachefile_backend or getattr(generator, 'cachefile_backend', None) or get_singleton(settings.IMAGEKIT_DEFAULT_CACHEFILE_BACKEND, 'cache file backend'))\n    self.cachefile_strategy = (cachefile_strategy or getattr(generator, 'cachefile_strategy', None) or get_singleton(settings.IMAGEKIT_DEFAULT_CACHEFILE_STRATEGY, 'cache file strategy'))\n    super(ImageCacheFile, self).__init__(storage=storage)\n", "label": 0}
{"function": "\n\ndef __new__(cls, j, m):\n    j = sympify(j)\n    m = sympify(m)\n    if j.is_number:\n        if ((2 * j) != int((2 * j))):\n            raise ValueError(('j must be integer or half-integer, got: %s' % j))\n        if (j < 0):\n            raise ValueError(('j must be >= 0, got: %s' % j))\n    if m.is_number:\n        if ((2 * m) != int((2 * m))):\n            raise ValueError(('m must be integer or half-integer, got: %s' % m))\n    if (j.is_number and m.is_number):\n        if (abs(m) > j):\n            raise ValueError(('Allowed values for m are -j <= m <= j, got j, m: %s, %s' % (j, m)))\n        if (int((j - m)) != (j - m)):\n            raise ValueError(('Both j and m must be integer or half-integer, got j, m: %s, %s' % (j, m)))\n    return State.__new__(cls, j, m)\n", "label": 1}
{"function": "\n\ndef test_profiler():\n    with prof:\n        out = get(dsk, 'e')\n    assert (out == 6)\n    prof_data = sorted(prof.results, key=(lambda d: d.key))\n    keys = [i.key for i in prof_data]\n    assert (keys == ['c', 'd', 'e'])\n    tasks = [i.task for i in prof_data]\n    assert (tasks == [(add, 'a', 'b'), (mul, 'a', 'b'), (mul, 'c', 'd')])\n    prof.clear()\n    assert (prof.results == [])\n", "label": 0}
{"function": "\n\n@classmethod\n@unguarded\ndef validate_url(cls, url):\n    '\\n        Return a boolean indicating whether the URL has a protocol and hostname.\\n        If a port is specified, ensure it is an integer.\\n\\n        Arguments:\\n            url (str): The URL to check.\\n\\n        Returns:\\n            Boolean indicating whether the URL has a protocol and hostname.\\n        '\n    result = urlparse.urlsplit(url)\n    if ((not result.scheme) or (not result.netloc)):\n        return False\n    try:\n        if (result.port is not None):\n            int(result.port)\n        elif result.netloc.endswith(':'):\n            return False\n    except ValueError:\n        return False\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef get_previous_link(self):\n    if (not self.has_previous):\n        return None\n    if (self.cursor and (not self.cursor.reverse) and (self.cursor.offset != 0)):\n        compare = self._get_position_from_instance(self.page[0], self.ordering)\n    else:\n        compare = self.previous_position\n    offset = 0\n    for item in self.page:\n        position = self._get_position_from_instance(item, self.ordering)\n        if (position != compare):\n            break\n        compare = position\n        offset += 1\n    else:\n        if (not self.has_next):\n            offset = self.page_size\n            position = None\n        elif self.cursor.reverse:\n            offset = (self.cursor.offset + self.page_size)\n            position = self.next_position\n        else:\n            offset = 0\n            position = self.next_position\n    cursor = Cursor(offset=offset, reverse=True, position=position)\n    return self.encode_cursor(cursor)\n", "label": 1}
{"function": "\n\ndef envs(ignore_cache=False):\n    '\\n    Return a list of refs that can be used as environments\\n    '\n    if (not ignore_cache):\n        env_cache = os.path.join(__opts__['cachedir'], 'hgfs/envs.p')\n        cache_match = salt.fileserver.check_env_cache(__opts__, env_cache)\n        if (cache_match is not None):\n            return cache_match\n    ret = set()\n    for repo in init():\n        repo['repo'].open()\n        if (repo['branch_method'] in ('branches', 'mixed')):\n            for branch in _all_branches(repo['repo']):\n                branch_name = branch[0]\n                if (branch_name == repo['base']):\n                    branch_name = 'base'\n                ret.add(branch_name)\n        if (repo['branch_method'] in ('bookmarks', 'mixed')):\n            for bookmark in _all_bookmarks(repo['repo']):\n                bookmark_name = bookmark[0]\n                if (bookmark_name == repo['base']):\n                    bookmark_name = 'base'\n                ret.add(bookmark_name)\n        ret.update([x[0] for x in _all_tags(repo['repo'])])\n        repo['repo'].close()\n    return [x for x in sorted(ret) if _env_is_exposed(x)]\n", "label": 1}
{"function": "\n\ndef __init__(self, typename=None, critical=None, value=None, subject=None, issuer=None, _ext=None):\n    if (_ext is not None):\n        ext = _ext\n    elif ((subject is None) and (issuer is None)):\n        ext = crypto.X509Extension(typename, critical, value)\n    elif ((subject is not None) and (issuer is None)):\n        subject = subject._cert\n        ext = crypto.X509Extension(typename, critical, value, subject=subject)\n    elif ((subject is None) and (issuer is not None)):\n        issuer = issuer._cert\n        ext = crypto.X509Extension(typename, critical, value, issuer=issuer)\n    elif ((subject is not None) and (issuer is not None)):\n        issuer = issuer._cert\n        ext = crypto.X509Extension(typename, critical, value, subject=subject, issuer=issuer)\n    self._ext = ext\n", "label": 1}
{"function": "\n\ndef scan(self, result):\n    if result.valid:\n        if result.description.startswith('ASCII cpio archive'):\n            self.consecutive_hits += 1\n            if ((not self.found_archive) or (self.found_archive_in_file != result.file.name)):\n                self.found_archive_in_file = result.file.name\n                self.found_archive = True\n                result.extract = True\n            elif ('TRAILER!!!' in result.description):\n                self.found_archive = False\n                result.extract = False\n                self.consecutive_hits = 0\n            else:\n                result.extract = False\n        elif (self.consecutive_hits < 4):\n            self.found_archive = False\n            self.found_archive_in_file = None\n            self.consecutive_hits = 0\n        elif (self.consecutive_hits >= 4):\n            result.valid = False\n", "label": 0}
{"function": "\n\ndef get_stack(message=None, stack_first_frame=1, max_frames=15):\n    try:\n        stack = inspect.stack()\n        frame_num = stack_first_frame\n        context = []\n        while ((len(stack) > frame_num) and (frame_num < (max_frames + stack_first_frame))):\n            exec_line = ('%s:%s:%s' % (stack[frame_num][1], stack[frame_num][2], stack[frame_num][3]))\n            context.insert(0, exec_line)\n            if (exec_line.endswith('_control_flow') or exec_line.endswith('load_ion') or exec_line.endswith('spawn_process') or exec_line.endswith(':main') or exec_line.endswith(':dispatch_request')):\n                break\n            frame_num += 1\n        stack_str = '\\n '.join(context)\n        if message:\n            stack_str = ((message + '\\n') + stack_str)\n        return stack_str\n    except Exception as ex:\n        stack_str = ('ERROR: ' + str(ex))\n        if message:\n            stack_str = ((message + '\\n') + stack_str)\n        return stack_str\n", "label": 1}
{"function": "\n\ndef _check_workers(self):\n    while (not self.stopped.is_set()):\n        for (worker, info) in self.worker_tracker.workers.iteritems():\n            if ((int(time.time()) - info.last_update) > HEARTBEAT_CHECK_INTERVAL):\n                info.continous_register = 0\n                if (info.status == RUNNING):\n                    info.status = HANGUP\n                elif (info.status == HANGUP):\n                    info.status = STOPPED\n                    self.black_list.append(worker)\n                    for job in self.job_tracker.running_jobs:\n                        self.job_tracker.remove_worker(job, worker)\n            elif (info.continous_register >= CONTINOUS_HEARTBEAT):\n                if (info.status != RUNNING):\n                    info.status = RUNNING\n                if (worker in self.black_list):\n                    self.black_list.remove(worker)\n                for job in self.job_tracker.running_jobs:\n                    if (not client_call(worker, 'has_job')):\n                        client_call(worker, 'prepare', job)\n                        client_call(worker, 'run_job', job)\n                    self.job_tracker.add_worker(job, worker)\n        self.stopped.wait(HEARTBEAT_CHECK_INTERVAL)\n", "label": 1}
{"function": "\n\ndef perform_search(self, dir, s=None, start=None, update_search_start=False):\n    self.cancel_highlight()\n    if (s is None):\n        s = self.last_search_string\n        if (s is None):\n            self.ui.message('No previous search term.')\n            return False\n    else:\n        self.last_search_string = s\n    if (start is None):\n        start = self.search_start\n    case_insensitive = (s.lower() == s)\n    if (start > len(self.ui.source)):\n        start = 0\n    i = ((start + dir) % len(self.ui.source))\n    if (i >= len(self.ui.source)):\n        i = 0\n    while (i != start):\n        sline = self.ui.source[i].text\n        if case_insensitive:\n            sline = sline.lower()\n        if (s in sline):\n            sl = self.ui.source[i]\n            sl.set_highlight(True)\n            self.highlight_line = sl\n            self.ui.source.set_focus(i)\n            if update_search_start:\n                self.search_start = i\n            return True\n        i = ((i + dir) % len(self.ui.source))\n    return False\n", "label": 1}
{"function": "\n\ndef run(self):\n    super(FakeDeletionThread, self).run()\n    receiver = NailgunReceiver\n    kwargs = {\n        'task_uuid': self.task_uuid,\n        'nodes': self.data['args']['nodes'],\n        'status': 'ready',\n    }\n    nodes_to_restore = copy.deepcopy(self.data['args'].get('nodes_to_restore', []))\n    resp_method = getattr(receiver, self.respond_to)\n    try:\n        resp_method(**kwargs)\n        db().commit()\n    except Exception:\n        db().rollback()\n        raise\n    recover_nodes = self.params.get('recover_nodes', True)\n    recover_offline_nodes = self.params.get('recover_offline_nodes', True)\n    if (not recover_nodes):\n        db().commit()\n        return\n    for node_data in nodes_to_restore:\n        is_offline = (('online' in node_data) and (not node_data['online']))\n        if (is_offline and (not recover_offline_nodes)):\n            continue\n        node_data['status'] = 'discover'\n        objects.Node.create(node_data)\n    db().commit()\n", "label": 0}
{"function": "\n\ndef createAES(key, IV, implList=None):\n    'Create a new AES object.\\n\\n    @type key: str\\n    @param key: A 16, 24, or 32 byte string.\\n\\n    @type IV: str\\n    @param IV: A 16 byte string\\n\\n    @rtype: L{tlslite.utils.AES}\\n    @return: An AES object.\\n    '\n    if (implList == None):\n        implList = ['openssl', 'pycrypto', 'python']\n    for impl in implList:\n        if ((impl == 'openssl') and cryptomath.m2cryptoLoaded):\n            return openssl_aes.new(key, 2, IV)\n        elif ((impl == 'pycrypto') and cryptomath.pycryptoLoaded):\n            return pycrypto_aes.new(key, 2, IV)\n        elif (impl == 'python'):\n            return python_aes.new(key, 2, IV)\n    raise NotImplementedError()\n", "label": 0}
{"function": "\n\ndef profile_head_single(benchmark):\n    import gc\n    results = []\n    gc.collect()\n    try:\n        from ctypes import cdll, CDLL\n        cdll.LoadLibrary('libc.so.6')\n        libc = CDLL('libc.so.6')\n        libc.malloc_trim(0)\n    except:\n        pass\n    N = (args.hrepeats + args.burnin)\n    results = []\n    try:\n        for i in range(N):\n            gc.disable()\n            d = dict()\n            try:\n                d = benchmark.run()\n            except KeyboardInterrupt:\n                raise\n            except Exception as e:\n                err = ''\n                try:\n                    err = d.get('traceback')\n                    if (err is None):\n                        err = str(e)\n                except:\n                    pass\n                print(('%s died with:\\n%s\\nSkipping...\\n' % (benchmark.name, err)))\n            results.append(d.get('timing', np.nan))\n            gc.enable()\n            gc.collect()\n    finally:\n        gc.enable()\n    if results:\n        results = results[args.burnin:]\n    sys.stdout.write('.')\n    sys.stdout.flush()\n    return Series(results, name=benchmark.name)\n", "label": 0}
{"function": "\n\ndef replace(self, new):\n    'Replaces this node with a new one in the parent.'\n    assert (self.parent is not None), str(self)\n    assert (new is not None)\n    if (not isinstance(new, list)):\n        new = [new]\n    l_children = []\n    found = False\n    for ch in self.parent.children:\n        if (ch is self):\n            assert (not found), (self.parent.children, self, new)\n            if (new is not None):\n                l_children.extend(new)\n            found = True\n        else:\n            l_children.append(ch)\n    assert found, (self.children, self, new)\n    self.parent.changed()\n    self.parent.children = l_children\n    for x in new:\n        x.parent = self.parent\n    self.parent = None\n", "label": 1}
{"function": "\n\ndef get_flinalg_funcs(names, arrays=(), debug=0):\n    'Return optimal available _flinalg function objects with\\n    names. arrays are used to determine optimal prefix.'\n    ordering = []\n    for i in range(len(arrays)):\n        t = arrays[i].dtype.char\n        if (t not in _type_conv):\n            t = 'd'\n        ordering.append((t, i))\n    if ordering:\n        ordering.sort()\n        required_prefix = _type_conv[ordering[0][0]]\n    else:\n        required_prefix = 'd'\n    if (ordering and has_column_major_storage(arrays[ordering[0][1]])):\n        (suffix1, suffix2) = ('_c', '_r')\n    else:\n        (suffix1, suffix2) = ('_r', '_c')\n    funcs = []\n    for name in names:\n        func_name = (required_prefix + name)\n        func = getattr(_flinalg, (func_name + suffix1), getattr(_flinalg, (func_name + suffix2), None))\n        funcs.append(func)\n    return tuple(funcs)\n", "label": 0}
{"function": "\n\ndef maximalSquare(self, matrix):\n    if (not matrix):\n        return 0\n    (m, n) = (len(matrix), len(matrix[0]))\n    size = [[0 for j in xrange(n)] for i in xrange(2)]\n    max_size = 0\n    for j in xrange(n):\n        if (matrix[0][j] == '1'):\n            size[0][j] = 1\n        max_size = max(max_size, size[0][j])\n    for i in xrange(1, m):\n        if (matrix[i][0] == '1'):\n            size[(i % 2)][0] = 1\n        else:\n            size[(i % 2)][0] = 0\n        for j in xrange(1, n):\n            if (matrix[i][j] == '1'):\n                size[(i % 2)][j] = (min(size[(i % 2)][(j - 1)], size[((i - 1) % 2)][j], size[((i - 1) % 2)][(j - 1)]) + 1)\n                max_size = max(max_size, size[(i % 2)][j])\n            else:\n                size[(i % 2)][j] = 0\n    return (max_size * max_size)\n", "label": 1}
{"function": "\n\ndef sql_flush(self, style, tables, sequences):\n    if tables:\n        if ((self.postgres_version[0] >= 8) and (self.postgres_version[1] >= 1)):\n            sql = [('%s %s;' % (style.SQL_KEYWORD('TRUNCATE'), style.SQL_FIELD(', '.join([self.quote_name(table) for table in tables]))))]\n        else:\n            sql = [('%s %s %s;' % (style.SQL_KEYWORD('DELETE'), style.SQL_KEYWORD('FROM'), style.SQL_FIELD(self.quote_name(table)))) for table in tables]\n        for sequence_info in sequences:\n            table_name = sequence_info['table']\n            column_name = sequence_info['column']\n            if (column_name and (len(column_name) > 0)):\n                sequence_name = ('%s_%s_seq' % (table_name, column_name))\n            else:\n                sequence_name = ('%s_id_seq' % table_name)\n            sql.append((\"%s setval('%s', 1, false);\" % (style.SQL_KEYWORD('SELECT'), style.SQL_FIELD(self.quote_name(sequence_name)))))\n        return sql\n    else:\n        return []\n", "label": 0}
{"function": "\n\ndef _topological_sort(self, goal_info_by_goal):\n    dependees_by_goal = OrderedDict()\n\n    def add_dependee(goal, dependee=None):\n        dependees = dependees_by_goal.get(goal)\n        if (dependees is None):\n            dependees = set()\n            dependees_by_goal[goal] = dependees\n        if dependee:\n            dependees.add(dependee)\n    for (goal, goal_info) in goal_info_by_goal.items():\n        add_dependee(goal)\n        for dependency in goal_info.goal_dependencies:\n            add_dependee(dependency, goal)\n    satisfied = set()\n    while dependees_by_goal:\n        count = len(dependees_by_goal)\n        for (goal, dependees) in dependees_by_goal.items():\n            unsatisfied = len((dependees - satisfied))\n            if (unsatisfied == 0):\n                satisfied.add(goal)\n                dependees_by_goal.pop(goal)\n                (yield goal_info_by_goal[goal])\n                break\n        if (len(dependees_by_goal) == count):\n            for dependees in dependees_by_goal.values():\n                dependees.difference_update(satisfied)\n            raise self.GoalCycleError('Cycle detected in goal dependencies:\\n\\t{0}'.format('\\n\\t'.join(('{0} <- {1}'.format(goal, list(dependees)) for (goal, dependees) in dependees_by_goal.items()))))\n", "label": 0}
{"function": "\n\ndef overrules(self, other):\n    '\\n        Detects if the other index is a non-unique, non primary index\\n        that can be overwritten by this one.\\n\\n        :param other: The other index\\n        :type other: Index\\n\\n        :rtype: bool\\n        '\n    if other.is_primary():\n        return False\n    elif (self.is_simple_index() and other.is_unique()):\n        return False\n    same_columns = self.spans_columns(other.get_columns())\n    if (same_columns and (self.is_primary() or self.is_unique()) and self.same_partial_index(other)):\n        return True\n    return False\n", "label": 0}
{"function": "\n\ndef resolve_columns(self, row, fields=()):\n    '\\n        This routine is necessary so that distances and geometries returned\\n        from extra selection SQL get resolved appropriately into Python\\n        objects.\\n        '\n    values = []\n    aliases = self.extra_select.keys()\n    if self.aggregates:\n        aliases.extend([None for i in xrange(len(self.aggregates))])\n    rn_offset = 0\n    if SpatialBackend.oracle:\n        if ((self.high_mark is not None) or self.low_mark):\n            rn_offset = 1\n    index_start = (rn_offset + len(aliases))\n    values = [self.convert_values(v, self.extra_select_fields.get(a, None)) for (v, a) in izip(row[rn_offset:index_start], aliases)]\n    if (SpatialBackend.oracle or getattr(self, 'geo_values', False)):\n        for (value, field) in izip(row[index_start:], fields):\n            values.append(self.convert_values(value, field))\n    else:\n        values.extend(row[index_start:])\n    return tuple(values)\n", "label": 1}
{"function": "\n\n@utils.synchronized('dhcp-agent')\ndef port_update_end(self, context, payload):\n    'Handle the port.update.end notification event.'\n    updated_port = dhcp.DictModel(payload['port'])\n    network = self.cache.get_network_by_id(updated_port.network_id)\n    if network:\n        LOG.info(_LI('Trigger reload_allocations for port %s'), updated_port)\n        driver_action = 'reload_allocations'\n        if self._is_port_on_this_agent(updated_port):\n            orig = self.cache.get_port_by_id(updated_port['id'])\n            old_ips = {i['ip_address'] for i in (orig['fixed_ips'] or [])}\n            new_ips = {i['ip_address'] for i in updated_port['fixed_ips']}\n            if (old_ips != new_ips):\n                driver_action = 'restart'\n        self.cache.put_port(updated_port)\n        self.call_driver(driver_action, network)\n", "label": 0}
{"function": "\n\ndef addAndFixActions(startDict, actions):\n    curDict = copy.copy(startDict)\n    for action in actions:\n        new_ops = []\n        for op in action.db_operations:\n            if (op.vtType == 'add'):\n                if ((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId))):\n                    curDict[(op.db_what, op.db_objectId)] = op\n                    new_ops.append(op)\n            elif (op.vtType == 'change'):\n                if (curDict.has_key((op.db_what, op.db_oldObjId)) and ((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId)))):\n                    del curDict[(op.db_what, op.db_oldObjId)]\n                    curDict[(op.db_what, op.db_newObjId)] = op\n                    new_ops.append(op)\n            elif (op.vtType == 'delete'):\n                if (((op.db_parentObjId is None) or curDict.has_key((op.db_parentObjType, op.db_parentObjId))) and curDict.has_key((op.db_what, op.db_objectId))):\n                    del curDict[(op.db_what, op.db_objectId)]\n                    new_ops.append(op)\n        action.db_operations = new_ops\n    return curDict\n", "label": 1}
{"function": "\n\ndef test_time_dep_bra():\n    b = TimeDepBra(0, t)\n    assert isinstance(b, TimeDepBra)\n    assert isinstance(b, BraBase)\n    assert isinstance(b, StateBase)\n    assert isinstance(b, QExpr)\n    assert (b.label == (Integer(0),))\n    assert (b.args == (Integer(0), t))\n    assert (b.time == t)\n    assert (b.dual_class() == TimeDepKet)\n    assert (b.dual == TimeDepKet(0, t))\n    k = TimeDepBra(x, 0.5)\n    assert (k.label == (x,))\n    assert (k.args == (x, sympify(0.5)))\n    assert (TimeDepBra() == TimeDepBra('psi', 't'))\n", "label": 1}
{"function": "\n\ndef get_mtv(self):\n    pos = (self.pos + self.vec)\n    pos = collision.uncenter_position(pos, self.col.bbox)\n    q = collections.deque((Vector3(),))\n    while q:\n        current_vector = q.popleft()\n        transform_vectors = self.col.check_collision(pos, current_vector)\n        if (not all(transform_vectors)):\n            break\n        for vector in transform_vectors:\n            test_vec = ((self.vec + current_vector) + vector)\n            if (test_vec.dist_sq() <= (self.vec.dist_sq() + FP_MAGIC)):\n                q.append((current_vector + vector))\n    else:\n        logger.debug('Physics failed to generate an MTV, bailing out')\n        self.vec.zero()\n        return Vector3()\n    possible_mtv = [current_vector]\n    while q:\n        current_vector = q.popleft()\n        transform_vectors = self.col.check_collision(pos, current_vector)\n        if (not all(transform_vectors)):\n            possible_mtv.append(current_vector)\n    return min(possible_mtv)\n", "label": 0}
{"function": "\n\ndef __call__(self, request, *args, **kwargs):\n    '\\n        In case of the wrapped view being determined as a class (that it has\\n        a dispatch attribute) we return a new instance of the class with all\\n        view arguments passed to the dispatch method. The case of a view\\n        function things are much simpler, we just call the view function with\\n        the view arguments.\\n\\n        For debugging purposes we insert some additional information that is\\n        useful for view classes in the raised exception.\\n        '\n    try:\n        if isclass(self.view):\n            view = self.view()\n            if hasattr(view, 'dispatch'):\n                return view.dispatch(request, *args, **kwargs)\n        else:\n            view = self.view\n        if callable(view):\n            return view(request, *args, **kwargs)\n    except (Http404, PermissionDenied, SystemExit):\n        raise\n    except Exception as ex:\n        try:\n            (cls, e, trace) = sys.exc_info()\n            msg = ('%s in %s.%s: %s' % (cls.__name__, self.view.__module__, self.view.__name__, e))\n        except Exception:\n            raise ex\n        else:\n            raise UtkikException(msg).with_traceback(trace)\n    raise ImproperlyConfigured(('%s.%s does not define a view function or class view.' % (self.view.__module__, self.view.__name__)))\n", "label": 0}
{"function": "\n\ndef _safeio(self, callback):\n    if self.closed:\n        raise ValueError('I/O operation on closed file')\n    for i in range(self.retries):\n        try:\n            if (not self.stream):\n                self.stream = self._reconnect()\n            return callback(self.stream)\n        except (EOFError, IOError, OSError, socket.error):\n            if (i >= (self.retries - 1)):\n                raise\n            if self.stream:\n                self.stream.close()\n            self.stream = None\n            time.sleep(0.5)\n", "label": 0}
{"function": "\n\ndef download_file(url, chunk_size=(100 * 1024)):\n    ' Helper method to download a file displaying a progress bar '\n    print('Fetching:', url)\n    file_content = None\n    progressbar = None\n    if (sys.version_info.major <= 2):\n        from rplibs.progressbar import FileTransferSpeed, ETA, ProgressBar, Percentage\n        from rplibs.progressbar import Bar\n        widgets = ['\\tDownloading: ', FileTransferSpeed(), ' ', Bar(), Percentage(), '   ', ETA()]\n        file_content = []\n        bytes_read = 0\n        try:\n            usock = urllib.request.urlopen(url)\n            file_size = int(usock.headers.get('Content-Length', 10000000000.0))\n            print('File size is', round((file_size / (1024 ** 2)), 2), 'MB')\n            progressbar = ProgressBar(widgets=widgets, maxval=file_size).start()\n            while True:\n                data = usock.read(chunk_size)\n                file_content.append(data)\n                bytes_read += len(data)\n                progressbar.update(bytes_read)\n                if (not data):\n                    break\n            usock.close()\n        except Exception:\n            print('ERROR: Could not fetch', url, '!', file=sys.stderr)\n            raise\n    else:\n        print('Downloading .. (progressbar disabled due to python 3 build)')\n        try:\n            usock = urllib.request.urlopen(url)\n            file_content = []\n            while True:\n                data = usock.read(chunk_size)\n                file_content.append(data)\n                if (not data):\n                    break\n            usock.close()\n        except Exception:\n            print('ERROR: Could not fetch', url, '!', file=sys.stderr)\n            raise\n    if progressbar:\n        progressbar.finish()\n    return binary_type().join(file_content)\n", "label": 0}
{"function": "\n\ndef bitcount(self, key, start=None, end=None):\n    '\\n        Returns the count of set bits in the value of ``key``.  Optional\\n        ``start`` and ``end`` paramaters indicate which bytes to consider\\n        '\n    params = [key]\n    if ((start is not None) and (end is not None)):\n        params.append(start)\n        params.append(end)\n    elif (((start is not None) and (end is None)) or ((end is not None) and (start is None))):\n        raise RedisError('Both start and end must be specified')\n    return self.execute_command('BITCOUNT', *params)\n", "label": 0}
{"function": "\n\ndef _format_action(self, action):\n    help_position = min((self._action_max_length + 2), self._max_help_position)\n    help_width = (self._width - help_position)\n    action_width = ((help_position - self._current_indent) - 2)\n    action_header = self._format_action_invocation(action)\n    if (not action.help):\n        tup = (self._current_indent, '', action_header)\n        action_header = ('%*s%s\\n' % tup)\n    elif (len(action_header) <= action_width):\n        tup = (self._current_indent, '', action_width, action_header)\n        action_header = ('%*s%-*s  ' % tup)\n        indent_first = 0\n    else:\n        tup = (self._current_indent, '', action_header)\n        action_header = ('%*s%s\\n' % tup)\n        indent_first = help_position\n    parts = [action_header]\n    if action.help:\n        help_text = self._expand_help(action)\n        help_lines = self._split_lines(help_text, help_width)\n        parts.append(('%*s%s\\n' % (indent_first, '', help_lines[0])))\n        for line in help_lines[1:]:\n            parts.append(('%*s%s\\n' % (help_position, '', line)))\n    elif (not action_header.endswith('\\n')):\n        parts.append('\\n')\n    for subaction in self._iter_indented_subactions(action):\n        parts.append(self._format_action(subaction))\n    return self._join_parts(parts)\n", "label": 0}
{"function": "\n\ndef __wrap(self, value):\n    if isinstance(value, (tuple, set, frozenset)):\n        return type(value)([self.__wrap(v) for v in value])\n    elif (isinstance(value, list) and (not isinstance(value, Collection))):\n        return Collection(value, self.__class__)\n    elif isinstance(value, Object):\n        return value\n    elif isinstance(value, Raw):\n        return value.value\n    elif isinstance(value, dict):\n        if isinstance(self, CaseInsensitiveObject):\n            return CaseInsensitiveObject(value)\n        else:\n            return Object(value)\n    else:\n        return value\n", "label": 0}
{"function": "\n\ndef compute(self):\n    port_object = None\n    if self.has_input('SetInputConnection0'):\n        ic = self.get_input('SetInputConnection0')\n        if hasattr(ic, 'vtkInstance'):\n            ic = ic.vtkInstance\n        producer = ic.GetProducer()\n        try:\n            port_object = producer.GetOutput()\n        except AttributeError:\n            raise ModuleError(self, 'expected a module that supports GetOutput')\n    elif self.has_input('SetInput'):\n        port_object = self.get_input('SetInput')\n        if hasattr(port_object, 'vtkInstance'):\n            port_object = port_object.vtkInstance\n    if port_object:\n        self.auto_set_results(port_object)\n", "label": 0}
{"function": "\n\ndef generate_ipv4list(num_items=100, include_hosts=False):\n    '\\n    Generate a list of unique IPv4 addresses. This is a total hack.\\n\\n    :param num_items:\\n        Number of items to generate\\n\\n    :param include_hosts:\\n        Whether to include /32 addresses\\n    '\n    ipset = set()\n    while (len(ipset) < num_items):\n        ip = generate_ipv4()\n        if ip.startswith('0'):\n            continue\n        if ip.endswith('.0.0.0'):\n            prefix = '/8'\n        elif ip.endswith('.0.0'):\n            prefix = '/16'\n        elif ip.endswith('.0'):\n            prefix = '/24'\n        elif include_hosts:\n            prefix = '/32'\n        else:\n            continue\n        ip += prefix\n        ipset.add(ip)\n    return sorted(ipset)\n", "label": 0}
{"function": "\n\ndef handle_text(self):\n    '\\n        Takes care of converting body text to unicode, if its text at all.\\n        Sets self.original_encoding to original char encoding, and converts body\\n        to unicode if possible. Must come after handle_compression, and after\\n        self.mediaType is valid.\\n        '\n    self.encoding = None\n    if (self.mediaType and ((self.mediaType.type == 'text') or ((self.mediaType.type == 'application') and ('xml' in self.mediaType.subtype)))):\n        if ('charset' in self.mediaType.params):\n            override_encodings = [self.mediaType.params['charset']]\n        else:\n            override_encodings = []\n        if (self.body != ''):\n            if UnicodeDammit:\n                dammit = UnicodeDammit(self.body, override_encodings)\n                if dammit.unicode:\n                    self.text = dammit.unicode\n                    self.originalEncoding = dammit.originalEncoding\n                else:\n                    pass\n            else:\n                u = None\n                for e in (override_encodings + ['utf8', 'iso-8859-1']):\n                    try:\n                        u = self.body.decode(e, 'strict')\n                        self.originalEncoding = e\n                        break\n                    except UnicodeError:\n                        pass\n                if (not u):\n                    u = self.body.decode('utf8', 'replace')\n                    self.originalEncoding = None\n                self.text = (u or None)\n    else:\n        self.text = b64encode(self.body)\n        self.encoding = 'base64'\n", "label": 1}
{"function": "\n\ndef __init__(self, request, **kwargs):\n    \"\\n            Matching the following example jsonpath:\\n            \\n                /path/[?(@.field1='searchterm*'&@.field2='*search*')][/@['field1'],/@['field2']][0:24]\\n                \\n            The last part of the URL will contain a JSONPath-query:\\n            \\n                [filter][sort][start:end:step]\\n        \"\n    path = request.path\n    if (not path.endswith('/')):\n        path = (path + '/')\n    match = re.match('^/.*/(\\\\[.*\\\\])/$', path)\n    if match:\n        self.jsonpath = match.groups()[0]\n    if self.jsonpath:\n        parts = self.jsonpath[1:(- 1)].split('][')\n        for part in parts:\n            if part.startswith('?'):\n                self.jsonpath_filters = part\n            elif re.match('^[/\\\\\\\\].*$', part):\n                self.jsonpath_sorting = part\n            elif re.match('^\\\\d*:\\\\d*:{0,1}\\\\d*$', part):\n                self.jsonpath_paging = part\n    super(JsonQueryRestStoreInfo, self).__init__(request, **kwargs)\n", "label": 0}
{"function": "\n\ndef cwd_for_window(window):\n    \"\\n    Return the working directory in which the window's commands should run.\\n\\n    In the common case when the user has one folder open, return that.\\n    Otherwise, return one of the following (in order of preference):\\n        1) One of the open folders, preferring a folder containing the active\\n           file.\\n        2) The directory containing the active file.\\n        3) The user's home directory.\\n    \"\n    folders = window.folders()\n    if (len(folders) == 1):\n        return folders[0]\n    else:\n        active_view = window.active_view()\n        active_file_name = (active_view.file_name() if active_view else None)\n        if (not active_file_name):\n            return (folders[0] if len(folders) else os.path.expanduser('~'))\n        for folder in folders:\n            if active_file_name.startswith(folder):\n                return folder\n        return os.path.dirname(active_file_name)\n", "label": 0}
{"function": "\n\ndef GetRealPath(file=None, encodeURL=False):\n    if (file is None):\n        file = ''\n    if ((file.find('/') != 0) and (file.find('\\\\') != 0) and (not re.search('^[a-zA-Z]+:[/\\\\\\\\]?', file))):\n        if hasattr(sys, 'frozen'):\n            path = os.path.dirname(sys.executable)\n        elif ('__file__' in globals()):\n            path = os.path.dirname(os.path.realpath(__file__))\n        else:\n            path = os.getcwd()\n        path = ((path + os.sep) + file)\n        path = re.sub('[/\\\\\\\\]+', re.escape(os.sep), path)\n        path = re.sub('[/\\\\\\\\]+$', '', path)\n        if encodeURL:\n            return urllib_pathname2url(path)\n        else:\n            return path\n    return file\n", "label": 0}
{"function": "\n\ndef _check_boolean(self, row, col):\n    from .base import isspmatrix\n    if (isspmatrix(row) or isspmatrix(col)):\n        raise IndexError('Indexing with sparse matrices is not supported except boolean indexing where matrix and index are equal shapes.')\n    if (isinstance(row, np.ndarray) and (row.dtype.kind == 'b')):\n        row = self._boolean_index_to_array(row)\n    if (isinstance(col, np.ndarray) and (col.dtype.kind == 'b')):\n        col = self._boolean_index_to_array(col)\n    return (row, col)\n", "label": 0}
{"function": "\n\ndef can_fetch(self, useragent, url):\n    'using the parsed robots.txt decide if useragent can fetch url'\n    if self.disallow_all:\n        return False\n    if self.allow_all:\n        return True\n    parsed_url = urlparse.urlparse(urllib.unquote(url))\n    url = urlparse.urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment))\n    url = urllib.quote(url)\n    if (not url):\n        url = '/'\n    for entry in self.entries:\n        if entry.applies_to(useragent):\n            return entry.allowance(url)\n    if self.default_entry:\n        return self.default_entry.allowance(url)\n    return True\n", "label": 0}
{"function": "\n\ndef generate_query_string(self, otp, nonce, timestamp=False, sl=None, timeout=None):\n    '\\n        Returns a query string which is sent to the validation servers.\\n        '\n    data = [('id', self.client_id), ('otp', otp), ('nonce', nonce)]\n    if timestamp:\n        data.append(('timestamp', '1'))\n    if (sl is not None):\n        if ((sl not in range(0, 101)) and (sl not in ['fast', 'secure'])):\n            raise Exception('sl parameter value must be between 0 and 100 or string \"fast\" or \"secure\"')\n        data.append(('sl', sl))\n    if timeout:\n        data.append(('timeout', timeout))\n    query_string = urlencode(data)\n    if self.key:\n        hmac_signature = self.generate_message_signature(query_string)\n        hmac_signature = hmac_signature\n        query_string += ('&h=%s' % hmac_signature.replace('+', '%2B'))\n    return query_string\n", "label": 0}
{"function": "\n\ndef _eval_is_prime(self):\n    '\\n        If product is a positive integer, multiplication\\n        will never result in a prime number.\\n        '\n    if self.is_number:\n        '\\n            If input is a number that is not completely simplified.\\n            e.g. Mul(sqrt(3), sqrt(3), evaluate=False)\\n            So we manually evaluate it and return whether that is prime or not.\\n            '\n        r = S.One\n        for arg in self.args:\n            r *= arg\n        return r.is_prime\n    if (self.is_integer and self.is_positive):\n        '\\n            Here we count the number of arguments that have a minimum value\\n            greater than two.\\n            If there are more than one of such a symbol then the result is not prime.\\n            Else, the result cannot be determined.\\n            '\n        number_of_args = 0\n        for arg in self.args:\n            if (arg - 1).is_positive:\n                number_of_args += 1\n        if (number_of_args > 1):\n            return False\n", "label": 0}
{"function": "\n\ndef GetHandlerType(self):\n    'Get handler type of mapping.\\n\\n    Returns:\\n      Handler type determined by which handler id attribute is set.\\n\\n    Raises:\\n      UnknownHandlerType: when none of the no handler id attributes are set.\\n\\n      UnexpectedHandlerAttribute: when an unexpected attribute is set for the\\n        discovered handler type.\\n\\n      HandlerTypeMissingAttribute: when the handler is missing a\\n        required attribute for its handler type.\\n\\n      MissingHandlerAttribute: when a URL handler is missing an attribute\\n    '\n    if (getattr(self, HANDLER_API_ENDPOINT) is not None):\n        mapping_type = HANDLER_API_ENDPOINT\n    else:\n        for id_field in URLMap.ALLOWED_FIELDS.iterkeys():\n            if (getattr(self, id_field) is not None):\n                mapping_type = id_field\n                break\n        else:\n            raise appinfo_errors.UnknownHandlerType(('Unknown url handler type.\\n%s' % str(self)))\n    allowed_fields = URLMap.ALLOWED_FIELDS[mapping_type]\n    for attribute in self.ATTRIBUTES.iterkeys():\n        if ((getattr(self, attribute) is not None) and (not ((attribute in allowed_fields) or (attribute in URLMap.COMMON_FIELDS) or (attribute == mapping_type)))):\n            raise appinfo_errors.UnexpectedHandlerAttribute(('Unexpected attribute \"%s\" for mapping type %s.' % (attribute, mapping_type)))\n    if ((mapping_type == HANDLER_STATIC_FILES) and (not self.upload)):\n        raise appinfo_errors.MissingHandlerAttribute(('Missing \"%s\" attribute for URL \"%s\".' % (UPLOAD, self.url)))\n    return mapping_type\n", "label": 1}
{"function": "\n\ndef pptable(table):\n    head = table['head']\n    body = table['body']\n    pretty_head = [(('+' + '+'.join(['{:->24}'.format('') for n in head])) + '+'), (('|' + '|'.join(['{:^24}'.format(str(n)[(- 24):]) for n in head])) + '|'), (('+' + '+'.join(['{:=>24}'.format('') for n in head])) + '+')]\n    pretty_body = []\n    for row in body:\n        pretty_body.append((('|' + '|'.join(['{:>24}'.format(str(n)[(- 24):]) for n in row])) + '|'))\n        pretty_body.append((('+' + '+'.join(['{:->24}'.format('') for n in head])) + '+'))\n    prettytable = '\\n'.join((pretty_head + pretty_body))\n    print(prettytable)\n", "label": 0}
{"function": "\n\ndef test_include_1():\n    sub_ffi = FFI()\n    sub_ffi.cdef('static const int k2 = 121212;')\n    sub_ffi.include(original_ffi)\n    assert ('macro FOOBAR' in original_ffi._parser._declarations)\n    assert ('macro FOOBAZ' in original_ffi._parser._declarations)\n    sub_ffi.set_source('re_python_pysrc', None)\n    sub_ffi.emit_python_code(str(tmpdir.join('_re_include_1.py')))\n    if (sys.version_info[:2] >= (3, 3)):\n        import importlib\n        importlib.invalidate_caches()\n    from _re_include_1 import ffi\n    assert (ffi.integer_const('FOOBAR') == (- 42))\n    assert (ffi.integer_const('FOOBAZ') == (- 43))\n    assert (ffi.integer_const('k2') == 121212)\n    lib = ffi.dlopen(extmod)\n    assert (lib.FOOBAR == (- 42))\n    assert (lib.FOOBAZ == (- 43))\n    assert (lib.k2 == 121212)\n    p = ffi.new('bar_t *', [5, b'foobar'])\n    assert (p.a[4] == ord('a'))\n", "label": 1}
{"function": "\n\ndef unique_list(seq, hashfunc=None):\n    seen = {\n        \n    }\n    if (not hashfunc):\n        return [x for x in seq if ((x not in seen) and (not seen.__setitem__(x, True)))]\n    else:\n        return [x for x in seq if ((hashfunc(x) not in seen) and (not seen.__setitem__(hashfunc(x), True)))]\n", "label": 0}
{"function": "\n\ndef GetFont(self):\n    '\\n        Return Font that should be used to draw text in this block\\n        '\n    font = None\n    if (self.engine.reportFormat.UseListCtrlTextFormat and self.GetListCtrl()):\n        listCtrl = self.GetListCtrl()\n        if listCtrl.IsVirtual():\n            attr = listCtrl.OnGetItemAttr(self.rowIndex)\n            if (attr and attr.HasFont()):\n                font = attr.GetFont()\n        else:\n            font = listCtrl.GetItemFont(self.rowIndex)\n    if (font and font.IsOk()):\n        return font\n    else:\n        return self.GetFormat().GetFont()\n", "label": 0}
{"function": "\n\ndef parse_long(tokens, options):\n    (raw, eq, value) = tokens.move().partition('=')\n    value = (None if (eq == value == '') else value)\n    opt = [o for o in options if (o.long and o.long.startswith(raw))]\n    if (len(opt) < 1):\n        if (tokens.error is DocoptExit):\n            raise tokens.error(('%s is not recognized' % raw))\n        else:\n            o = Option(None, raw, (1 if (eq == '=') else 0))\n            options.append(o)\n            return [o]\n    if (len(opt) > 1):\n        raise tokens.error(('%s is not a unique prefix: %s?' % (raw, ', '.join((('%s' % o.long) for o in opt)))))\n    opt = copy(opt[0])\n    if (opt.argcount == 1):\n        if (value is None):\n            if (tokens.current() is None):\n                raise tokens.error(('%s requires argument' % opt.name))\n            value = tokens.move()\n    elif (value is not None):\n        raise tokens.error(('%s must not have an argument' % opt.name))\n    opt.value = (value or True)\n    return [opt]\n", "label": 1}
{"function": "\n\ndef initialize_locksets(self):\n    log.debug('initializing locksets')\n    v = self.sign(VoteBlock(0, 0, self.chainservice.chain.genesis.hash))\n    self.add_vote(v)\n    head_proposal = self.load_proposal(self.head.hash)\n    if head_proposal:\n        assert (head_proposal.blockhash == self.head.hash)\n        for v in head_proposal.signing_lockset:\n            self.add_vote(v)\n        assert self.heights[(self.head.header.number - 1)].has_quorum\n    last_committing_lockset = self.load_last_committing_lockset()\n    if last_committing_lockset:\n        assert (last_committing_lockset.has_quorum == self.head.hash)\n        for v in last_committing_lockset.votes:\n            self.add_vote(v)\n        assert self.heights[self.head.header.number].has_quorum\n    else:\n        assert (self.head.header.number == 0)\n    assert self.highest_committing_lockset\n    assert self.last_committing_lockset\n    assert self.last_valid_lockset\n", "label": 1}
{"function": "\n\ndef _unpack_index(self, index):\n    ' Parse index. Always return a tuple of the form (row, col).\\n        Where row/col is a integer, slice, or array of integers.\\n        '\n    from .base import spmatrix\n    if (isinstance(index, (spmatrix, np.ndarray)) and (index.ndim == 2) and (index.dtype.kind == 'b')):\n        return index.nonzero()\n    index = self._check_ellipsis(index)\n    if isinstance(index, tuple):\n        if (len(index) == 2):\n            (row, col) = index\n        elif (len(index) == 1):\n            (row, col) = (index[0], slice(None))\n        else:\n            raise IndexError('invalid number of indices')\n    else:\n        (row, col) = (index, slice(None))\n    (row, col) = self._check_boolean(row, col)\n    return (row, col)\n", "label": 0}
{"function": "\n\ndef distrib_id():\n    '\\n    Get the OS distribution ID.\\n\\n    Returns a string such as ``\"Debian\"``, ``\"Ubuntu\"``, ``\"RHEL\"``,\\n    ``\"CentOS\"``, ``\"SLES\"``, ``\"Fedora\"``, ``\"Arch\"``, ``\"Gentoo\"``,\\n    ``\"SunOS\"``...\\n\\n    Example::\\n\\n        from fabtools.system import distrib_id\\n\\n        if distrib_id() != \\'Debian\\':\\n            abort(u\"Distribution is not supported\")\\n\\n    '\n    with settings(hide('running', 'stdout')):\n        kernel = run('uname -s')\n        if (kernel == 'Linux'):\n            if is_file('/usr/bin/lsb_release'):\n                id_ = run('lsb_release --id --short')\n                if (id in ['arch', 'Archlinux']):\n                    id_ = 'Arch'\n                return id_\n            elif is_file('/etc/debian_version'):\n                return 'Debian'\n            elif is_file('/etc/fedora-release'):\n                return 'Fedora'\n            elif is_file('/etc/arch-release'):\n                return 'Arch'\n            elif is_file('/etc/redhat-release'):\n                release = run('cat /etc/redhat-release')\n                if release.startswith('Red Hat Enterprise Linux'):\n                    return 'RHEL'\n                elif release.startswith('CentOS'):\n                    return 'CentOS'\n                elif release.startswith('Scientific Linux'):\n                    return 'SLES'\n            elif is_file('/etc/gentoo-release'):\n                return 'Gentoo'\n        elif (kernel == 'SunOS'):\n            return 'SunOS'\n", "label": 1}
{"function": "\n\ndef populate(self):\n    self.page = get_page_draft(self.request.current_page)\n    if (not self.page):\n        return\n    if get_cms_setting('PERMISSION'):\n        has_global_current_page_change_permission = has_page_change_permission(self.request)\n    else:\n        has_global_current_page_change_permission = False\n    can_change = (self.request.current_page and self.request.current_page.has_change_permission(self.request))\n    if (has_global_current_page_change_permission or can_change):\n        try:\n            mypageextension = MyPageExtension.objects.get(extended_object_id=self.page.id)\n        except MyPageExtension.DoesNotExist:\n            mypageextension = None\n        try:\n            if mypageextension:\n                url = admin_reverse('extensionapp_mypageextension_change', args=(mypageextension.pk,))\n            else:\n                url = (admin_reverse('extensionapp_mypageextension_add') + ('?extended_object=%s' % self.page.pk))\n        except NoReverseMatch:\n            pass\n        else:\n            not_edit_mode = (not self.toolbar.edit_mode)\n            current_page_menu = self.toolbar.get_or_create_menu('page')\n            current_page_menu.add_modal_item(_('Page Extension'), url=url, disabled=not_edit_mode)\n", "label": 1}
{"function": "\n\ndef __init__(self, value=None, feature=None, last_supported_version=None, useinstead=None, issue=None, deprecated_since_version=None):\n    self.fullMessage = ''\n    if feature:\n        if deprecated_since_version:\n            self.fullMessage = ('%s has been deprecated since SymPy %s. ' % (feature, deprecated_since_version))\n        else:\n            self.fullMessage = ('%s has been deprecated. ' % feature)\n    if last_supported_version:\n        self.fullMessage += ('It will be last supported in SymPy version %s. ' % last_supported_version)\n    if useinstead:\n        self.fullMessage += ('Use %s instead. ' % useinstead)\n    if issue:\n        self.fullMessage += ('See https://github.com/sympy/sympy/issues/%d for more info. ' % issue)\n    if value:\n        if (not isinstance(value, str)):\n            value = ('(%s)' % repr(value))\n        value = (' ' + value)\n    else:\n        value = ''\n    self.fullMessage += value\n", "label": 0}
{"function": "\n\ndef handle_msg(self, proto, msg):\n    if (msg[0:3] not in ['GET', 'SET', 'ADD', 'RES', 'UPD']):\n        return\n    if (msg[0:3] in ['GET']):\n        (_, key) = msg.split(' ', 1)\n        key = json.loads(key)\n        if (key in self.dict):\n            item = self.dict[key]\n            proto.send(('RES ' + json.dumps([key, item])))\n        else:\n            proto.send(('RES ' + json.dumps([key, None])))\n    if (msg[0:3] in ['SET']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        self.dict[key] = value\n    if (msg[0:3] in ['ADD']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if (key not in self.dict):\n            self.dict[key] = []\n        self.dict[key].append(value)\n    if (msg[0:3] in ['RES']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if (key in self.queue):\n            self.queue[key].put(value)\n    if (msg[0:3] in ['UPD']):\n        (_, item) = msg.split(' ', 1)\n        (key, value) = json.loads(item)\n        if ((key not in self.dict) or (self.dict[key] is not value)):\n            self.dict[key] = value\n", "label": 1}
{"function": "\n\ndef _exc_info_to_string(self, err, test):\n    '\\n        Converts a sys.exc_info()-style tuple of values into a string.\\n        '\n    (exctype, value, tb) = err\n    while (tb and self._is_relevant_tb_level(tb)):\n        tb = tb.tb_next\n    if (exctype is test.failureException):\n        length = self._count_relevant_tb_levels(tb)\n        msgLines = traceback.format_exception(exctype, value, tb, length)\n    else:\n        msgLines = traceback.format_exception(exctype, value, tb)\n    if self.buffer:\n        output = self._stdout_buffer.getvalue()\n        error = self._stderr_buffer.getvalue()\n        if output:\n            if (not output.endswith('\\n')):\n                output += '\\n'\n            msgLines.append((STDOUT_LINE % output))\n        if error:\n            if (not error.endswith('\\n')):\n                error += '\\n'\n            msgLines.append((STDERR_LINE % error))\n    return ''.join(msgLines)\n", "label": 0}
{"function": "\n\ndef __init__(self, api, name=None, public_key=None, private_key=None, private_key_passphrase=None, consumers=[], container_ref=None, created=None, updated=None, status=None, public_key_ref=None, private_key_ref=None, private_key_passphrase_ref=None):\n    secret_refs = {\n        \n    }\n    if public_key_ref:\n        secret_refs['public_key'] = public_key_ref\n    if private_key_ref:\n        secret_refs['private_key'] = private_key_ref\n    if private_key_passphrase_ref:\n        secret_refs['private_key_passphrase'] = private_key_passphrase_ref\n    super(RSAContainer, self).__init__(api=api, name=name, consumers=consumers, container_ref=container_ref, created=created, updated=updated, status=status, secret_refs=secret_refs)\n    if public_key:\n        self.public_key = public_key\n    if private_key:\n        self.private_key = private_key\n    if private_key_passphrase:\n        self.private_key_passphrase = private_key_passphrase\n", "label": 0}
{"function": "\n\ndef teardown_method(self, method):\n    '\\n        Default teardown method for all scripts. If run through Sauce Labs OnDemand, the job name, status and tags\\n        are updated. Also the video and server log are downloaded if so configured.\\n        '\n    if hasattr(self, 'config'):\n        if (('sauce labs' in self.cf['browsers'][self.cf['saunter']['default_browser']]) and (not self.cf['browsers'][self.cf['saunter']['default_browser']]['sauce labs']['ondemand']) and self.cf['saunter']['screenshots']['on_finish']):\n            self.take_named_screenshot('final')\n    if hasattr(self, 'driver'):\n        self.driver.quit()\n    if hasattr(self.browser, 'proxy'):\n        self.config['saunter']['proxies'].append(self.proxy)\n", "label": 0}
{"function": "\n\ndef update(self, req, id, body):\n    context = req.environ['nova.context']\n    authorize(context)\n    try:\n        utils.check_string_length(id, 'quota_class_name', min_length=1, max_length=255)\n    except exception.InvalidInput as e:\n        raise webob.exc.HTTPBadRequest(explanation=e.format_message())\n    quota_class = id\n    bad_keys = []\n    if (not self.is_valid_body(body, 'quota_class_set')):\n        msg = _('quota_class_set not specified')\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    quota_class_set = body['quota_class_set']\n    for key in quota_class_set.keys():\n        if (key not in self.supported_quotas):\n            bad_keys.append(key)\n            continue\n        try:\n            body['quota_class_set'][key] = utils.validate_integer(body['quota_class_set'][key], key, max_value=db.MAX_INT)\n        except exception.InvalidInput as e:\n            raise webob.exc.HTTPBadRequest(explanation=e.format_message())\n    if bad_keys:\n        msg = (_('Bad key(s) %s in quota_set') % ','.join(bad_keys))\n        raise webob.exc.HTTPBadRequest(explanation=msg)\n    try:\n        nova.context.require_admin_context(context)\n    except exception.AdminRequired:\n        raise webob.exc.HTTPForbidden()\n    for (key, value) in quota_class_set.items():\n        try:\n            db.quota_class_update(context, quota_class, key, value)\n        except exception.QuotaClassNotFound:\n            db.quota_class_create(context, quota_class, key, value)\n    values = QUOTAS.get_class_quotas(context, quota_class)\n    return self._format_quota_set(None, values)\n", "label": 1}
{"function": "\n\n@render_to('race/awards.html')\ndef awards(request):\n    if request.user.is_authenticated():\n        user_badges = set(((slug, level) for (slug, level) in BadgeAward.objects.filter(user=request.user).values_list('slug', 'level')))\n    else:\n        user_badges = []\n    badges_awarded = BadgeAward.objects.values('slug', 'level').annotate(num=Count('pk'))\n    badges_counts = {'{0}_{1}'.format(k['slug'], k['level']): k['num'] for k in badges_awarded}\n    user_count = UserProfile.objects.count()\n    badges_list = list()\n    for badge_cls in badges._registry.values():\n        for (level, badge) in enumerate(badge_cls.levels):\n            badge_count = badges_counts.get('{0}_{1}'.format(badge_cls.slug, level), 0)\n            badges_list.append({\n                'slug': badge_cls.slug,\n                'level': level,\n                'name': badge.name,\n                'description': badge.description,\n                'count': badge_count,\n                'percentage': (((badge_count / float(user_count)) * 100) if user_count else 0),\n                'user_has': ((badge_cls.slug, level) in user_badges),\n            })\n    return {\n        'badges_list': badges_list,\n        'user_count': user_count,\n    }\n", "label": 0}
{"function": "\n\ndef OnKeyPressed(self, event):\n    if self.CallTipActive():\n        self.CallTipCancel()\n    key = event.GetKeyCode()\n    if ((key == 32) and event.ControlDown()):\n        pos = self.GetCurrentPos()\n        if event.ShiftDown():\n            self.CallTipSetBackground('yellow')\n            self.CallTipShow(pos, 'lots of of text: blah, blah, blah\\n\\nshow some suff, maybe parameters..\\n\\nfubar(param1, param2)')\n        else:\n            kw = keyword.kwlist[:]\n            kw.sort()\n            self.AutoCompSetIgnoreCase(False)\n            for i in range(len(kw)):\n                if (kw[i] in keyword.kwlist):\n                    kw[i] = kw[i]\n            self.AutoCompShow(0, ' '.join(kw))\n    else:\n        event.Skip()\n", "label": 0}
{"function": "\n\n@staticmethod\ndef _from_db_object(context, fixedip, db_fixedip, expected_attrs=None):\n    if (expected_attrs is None):\n        expected_attrs = []\n    for field in fixedip.fields:\n        if (field == 'default_route'):\n            continue\n        if (field not in FIXED_IP_OPTIONAL_ATTRS):\n            fixedip[field] = db_fixedip[field]\n    if ('instance' in expected_attrs):\n        fixedip.instance = (objects.Instance._from_db_object(context, objects.Instance(context), db_fixedip['instance']) if db_fixedip['instance'] else None)\n    if ('network' in expected_attrs):\n        fixedip.network = (objects.Network._from_db_object(context, objects.Network(context), db_fixedip['network']) if db_fixedip['network'] else None)\n    if ('virtual_interface' in expected_attrs):\n        db_vif = db_fixedip['virtual_interface']\n        vif = (objects.VirtualInterface._from_db_object(context, objects.VirtualInterface(context), db_fixedip['virtual_interface']) if db_vif else None)\n        fixedip.virtual_interface = vif\n    if ('floating_ips' in expected_attrs):\n        fixedip.floating_ips = obj_base.obj_make_list(context, objects.FloatingIPList(context), objects.FloatingIP, db_fixedip['floating_ips'])\n    fixedip._context = context\n    fixedip.obj_reset_changes()\n    return fixedip\n", "label": 1}
{"function": "\n\ndef __call__(self, parser, namespace, value, option_string=None):\n    try:\n        assert (self.dest == 'spam'), ('dest: %s' % self.dest)\n        assert (option_string == '-s'), ('flag: %s' % option_string)\n        expected_ns = NS(spam=0.25)\n        if (value in [0.125, 0.625]):\n            expected_ns.badger = 2\n        elif (value in [2.0]):\n            expected_ns.badger = 84\n        else:\n            raise AssertionError(('value: %s' % value))\n        assert (expected_ns == namespace), ('expected %s, got %s' % (expected_ns, namespace))\n    except AssertionError:\n        e = sys.exc_info()[1]\n        raise ArgumentParserError(('opt_action failed: %s' % e))\n    setattr(namespace, 'spam', value)\n", "label": 0}
{"function": "\n\ndef _LoadArtifactsFromDatastore(self, source_urns=None, token=None, overwrite_if_exists=True):\n    'Load artifacts from the data store.'\n    if (token is None):\n        token = access_control.ACLToken(username='GRRArtifactRegistry', reason='Managing Artifacts.')\n    loaded_artifacts = []\n    for artifact_coll_urn in (source_urns or []):\n        with aff4.FACTORY.Create(artifact_coll_urn, aff4_type='RDFValueCollection', token=token, mode='rw') as artifact_coll:\n            for artifact_value in artifact_coll:\n                self.RegisterArtifact(artifact_value, source=('datastore:%s' % artifact_coll_urn), overwrite_if_exists=overwrite_if_exists)\n                loaded_artifacts.append(artifact_value)\n                logging.debug('Loaded artifact %s from %s', artifact_value.name, artifact_coll_urn)\n    revalidate = True\n    while revalidate:\n        revalidate = False\n        for artifact_obj in loaded_artifacts[:]:\n            try:\n                artifact_obj.Validate()\n            except ArtifactDefinitionError as e:\n                logging.error('Artifact %s did not validate: %s', artifact_obj.name, e)\n                artifact_obj.error_message = utils.SmartStr(e)\n                loaded_artifacts.remove(artifact_obj)\n                revalidate = True\n", "label": 0}
{"function": "\n\ndef kmeans_init(X, Y, n_labels, n_hidden_states, latent_node_features=False):\n    all_feats = []\n    for (x, y) in zip(X, Y):\n        if (len(x) == 3):\n            (features, edges, n_hidden) = x\n        elif (len(x) == 4):\n            (features, edges, _, n_hidden) = x\n        else:\n            raise ValueError('Something is fishy!')\n        n_visible = features.shape[0]\n        if latent_node_features:\n            n_visible -= n_hidden\n        if (np.max(edges) != ((n_hidden + n_visible) - 1)):\n            raise ValueError(\"Edges don't add up\")\n        labels_one_hot = np.zeros((n_visible, n_labels), dtype=np.int)\n        y = y.ravel()\n        gx = np.ogrid[:n_visible]\n        labels_one_hot[(gx, y)] = 1\n        graph = sparse.coo_matrix((np.ones(edges.shape[0]), edges.T), ((n_visible + n_hidden), (n_visible + n_hidden)))\n        graph = (graph + graph.T)[(- n_hidden):, :n_visible]\n        neighbors = (graph * labels_one_hot.reshape(n_visible, (- 1)))\n        neighbors /= np.maximum(neighbors.sum(axis=1)[:, np.newaxis], 1)\n        all_feats.append(neighbors)\n    all_feats_stacked = np.vstack(all_feats)\n    try:\n        km = KMeans(n_clusters=n_hidden_states)\n    except TypeError:\n        km = KMeans(k=n_hidden_states)\n    km.fit(all_feats_stacked)\n    H = []\n    for (y, feats) in zip(Y, all_feats):\n        H.append(np.hstack([y, (km.predict(feats) + n_labels)]))\n    return H\n", "label": 0}
{"function": "\n\ndef clean(self):\n    cleaned = super(AuthorizeRequestTokenForm, self).clean()\n    t = Token.objects.get(id=cleaned.get('obj_id'))\n    default_scopes = t.scope.split(' ')\n    scopes = cleaned.get('scopes')\n    if (not scopes):\n        raise forms.ValidationError('You need to select permissions for the client')\n    if (('statements/read/mine' in scopes) and ('statements/read' in scopes)):\n        raise forms.ValidationError(\"'statements/read/mine' and 'statements/read' are conflicting scope values. choose one.\")\n    if ('all' in default_scopes):\n        return cleaned\n    elif ('all' in scopes):\n        raise forms.ValidationError(\"Can't raise permissions beyond what the consumer registered.\")\n    if (set(scopes) != set(default_scopes)):\n        nomatch = [k for k in scopes if (k not in default_scopes)]\n        if (not (('all/read' in nomatch) or (('statements/read' in nomatch) and ('all/read' in default_scopes)) or (('statements/read/mine' in nomatch) and (('all/read' in default_scopes) or ('statements/read' in default_scopes))))):\n            raise forms.ValidationError(\"Can't raise permissions beyond what the consumer registered.\")\n    return cleaned\n", "label": 1}
{"function": "\n\ndef to_terminal(self):\n    'Yield lines to be printed in a terminal.'\n    average_cc = 0.0\n    analyzed = 0\n    for (name, blocks) in self.results:\n        if ('error' in blocks):\n            (yield (name, (blocks['error'],), {\n                'error': True,\n            }))\n            continue\n        (res, cc, n) = cc_to_terminal(blocks, self.config.show_complexity, self.config.min, self.config.max, self.config.total_average)\n        average_cc += cc\n        analyzed += n\n        if res:\n            (yield (name, (), {\n                \n            }))\n            (yield (res, (), {\n                'indent': 1,\n            }))\n    if ((self.config.average or self.config.total_average) and analyzed):\n        cc = (average_cc / analyzed)\n        ranked_cc = cc_rank(cc)\n        (yield ('\\n{0} blocks (classes, functions, methods) analyzed.', (analyzed,), {\n            \n        }))\n        (yield ('Average complexity: {0}{1} ({2}){3}', (RANKS_COLORS[ranked_cc], ranked_cc, cc, RESET), {\n            \n        }))\n", "label": 0}
{"function": "\n\ndef elemwise(op, *args, **kwargs):\n    ' Elementwise operation for dask.Dataframes '\n    columns = kwargs.pop('columns', no_default)\n    _name = ('elemwise-' + tokenize(op, kwargs, *args))\n    args = _maybe_from_pandas(args)\n    from .multi import _maybe_align_partitions\n    args = _maybe_align_partitions(args)\n    dasks = [arg for arg in args if isinstance(arg, (_Frame, Scalar))]\n    dfs = [df for df in dasks if isinstance(df, _Frame)]\n    divisions = dfs[0].divisions\n    n = (len(divisions) - 1)\n    other = [(i, arg) for (i, arg) in enumerate(args) if (not isinstance(arg, (_Frame, Scalar)))]\n    if other:\n        op2 = partial_by_order(op, other)\n    else:\n        op2 = op\n    keys = [((d._keys() * n) if isinstance(d, Scalar) else d._keys()) for d in dasks]\n    dsk = dict((((_name, i), ((op2,) + frs)) for (i, frs) in enumerate(zip(*keys))))\n    dsk = merge(dsk, *[d.dask for d in dasks])\n    if (columns is no_default):\n        if ((len(dfs) >= 2) and (len(dasks) != len(dfs))):\n            msg = 'elemwise with 2 or more DataFrames and Scalar is not supported'\n            raise NotImplementedError(msg)\n        columns = _emulate(op, *args, **kwargs)\n    return _Frame(dsk, _name, columns, divisions)\n", "label": 1}
{"function": "\n\ndef send_status(test_result, status_url, repo_token, pending=False):\n    if (status_url and repo_token):\n        commit_id = status_url.rstrip('/').split('/')[(- 1)]\n        log_url = (cfg.CONF.worker.log_url_prefix + commit_id)\n        headers = {\n            'Authorization': ('token ' + repo_token),\n            'Content-Type': 'application/json',\n        }\n        if pending:\n            data = {\n                'state': 'pending',\n                'description': 'Solum says: Testing in progress',\n                'target_url': log_url,\n            }\n        elif (test_result == 0):\n            data = {\n                'state': 'success',\n                'description': 'Solum says: Tests passed',\n                'target_url': log_url,\n            }\n        else:\n            data = {\n                'state': 'failure',\n                'description': 'Solum says: Tests failed',\n                'target_url': log_url,\n            }\n        try:\n            conn = get_http_connection()\n            (resp, _) = conn.request(status_url, 'POST', headers=headers, body=json.dumps(data))\n            if (resp['status'] != '201'):\n                LOG.debug(('Failed to send back status. Error code %s,status_url %s, repo_token %s' % (resp['status'], status_url, repo_token)))\n        except (httplib2.HttpLib2Error, socket.error) as ex:\n            LOG.warning(('Error in sending status, status url: %s, repo token: %s, error: %s' % (status_url, repo_token, ex)))\n    else:\n        LOG.debug('No url or token available to send back status')\n", "label": 0}
{"function": "\n\ndef _swap_generator(self, coro):\n    'Internal use only.\\n        '\n    self._lock.acquire()\n    cid = coro._id\n    coro = self._coros.get(cid, None)\n    if (coro is None):\n        logger.warning('invalid coroutine %s to swap', cid)\n        self._lock.release()\n        return (- 1)\n    if (coro._callers or (not coro._hot_swappable)):\n        logger.debug('postponing hot swapping of %s', str(coro))\n        self._lock.release()\n        return 0\n    else:\n        coro._timeout = None\n        if (coro._state is None):\n            coro._generator = coro._swap_generator\n            coro._value = None\n            if (coro._complete == 0):\n                coro._complete = None\n            elif isinstance(coro._complete, Event):\n                coro._complete.clear()\n            self._scheduled.add(cid)\n            coro._state = AsynCoro._Scheduled\n            coro._hot_swappable = False\n        else:\n            coro._exceptions.append((HotSwapException, HotSwapException(coro._swap_generator)))\n            if (coro._state in (AsynCoro._Suspended, AsynCoro._AwaitMsg_)):\n                self._suspended.discard(cid)\n                self._scheduled.add(cid)\n                coro._state = AsynCoro._Scheduled\n        coro._swap_generator = None\n        if (self._polling and (len(self._scheduled) == 1)):\n            self._notifier.interrupt()\n    self._lock.release()\n    return 0\n", "label": 1}
{"function": "\n\ndef __init__(self, parameters=None, return_annotation=_empty, __validate_parameters__=True):\n    \"Constructs Signature from the given list of Parameter\\n        objects and 'return_annotation'.  All arguments are optional.\\n        \"\n    if (parameters is None):\n        params = OrderedDict()\n    elif __validate_parameters__:\n        params = OrderedDict()\n        top_kind = _POSITIONAL_ONLY\n        for (idx, param) in enumerate(parameters):\n            kind = param.kind\n            if (kind < top_kind):\n                msg = 'wrong parameter order: {0} before {1}'\n                msg = msg.format(top_kind, param.kind)\n                raise ValueError(msg)\n            else:\n                top_kind = kind\n            name = param.name\n            if (name is None):\n                name = str(idx)\n                param = param.replace(name=name)\n            if (name in params):\n                msg = 'duplicate parameter name: {0!r}'.format(name)\n                raise ValueError(msg)\n            params[name] = param\n    else:\n        params = OrderedDict(((param.name, param) for param in parameters))\n    self._parameters = params\n    self._return_annotation = return_annotation\n", "label": 0}
{"function": "\n\ndef format(self, record):\n    'Uses contextstring if request_id is set, otherwise default.'\n    record.project = self.project\n    record.version = self.version\n    context = getattr(local.store, 'context', None)\n    if context:\n        d = _dictify_context(context)\n        for (k, v) in d.items():\n            setattr(record, k, v)\n    for key in ('instance', 'color'):\n        if (key not in record.__dict__):\n            record.__dict__[key] = ''\n    if record.__dict__.get('request_id', None):\n        self._fmt = CONF.logging_context_format_string\n    else:\n        self._fmt = CONF.logging_default_format_string\n    if ((record.levelno == logging.DEBUG) and CONF.logging_debug_format_suffix):\n        self._fmt += (' ' + CONF.logging_debug_format_suffix)\n    if record.exc_info:\n        record.exc_text = self.formatException(record.exc_info, record)\n    return logging.Formatter.format(self, record)\n", "label": 0}
{"function": "\n\ndef handle_class(val, class_name):\n    cls_errors = []\n    docstring = inspect.getdoc(val)\n    if (docstring is None):\n        cls_errors.append((class_name, '**missing** class-level docstring'))\n    else:\n        cls_errors = [(e,) for e in NumpyClassDocString(docstring, class_name, val).get_errors()]\n        methods = dict(((name, func) for (name, func) in inspect.getmembers(val) if ((not name.startswith('_')) and callable(func) and (type(func) is not type))))\n        for (m_name, method) in six.iteritems(methods):\n            if (inspect.getmodule(method) is not None):\n                continue\n            cls_errors.extend(handle_method(method, m_name, class_name))\n    return cls_errors\n", "label": 0}
{"function": "\n\ndef get_features(self, context_obj):\n    if (('source_token' not in context_obj) or (len(context_obj['source_token']) == 0)):\n        return [0.0 for i in range((len(self.thresholds) * 2))]\n    (translations, translations_weighted) = ([], [])\n    for thr in self.thresholds:\n        (all_words, all_words_weighted) = ([], [])\n        for word in context_obj['source_token']:\n            trans = [fl for fl in self.lex_prob[word] if (fl >= thr)]\n            all_words.append(len(trans))\n            all_words_weighted.append((len(trans) * self.corpus_freq.freq(word)))\n        translations.append(np.average(all_words))\n        translations_weighted.append(np.average(all_words_weighted))\n    return (translations + translations_weighted)\n", "label": 0}
{"function": "\n\ndef safely_reserve_a_username(cursor, gen_usernames=gen_random_usernames, reserve=insert_into_participants):\n    'Safely reserve a username.\\n\\n    :param cursor: a :py:class:`psycopg2.cursor` managed as a :py:mod:`postgres`\\n        transaction\\n    :param gen_usernames: a generator of usernames to try\\n    :param reserve: a function that takes the cursor and does the SQL\\n        stuff\\n    :database: one ``INSERT`` on average\\n    :returns: a 12-hex-digit unicode\\n    :raises: :py:class:`FailedToReserveUsername` if no acceptable username is found\\n        within 100 attempts, or :py:class:`RanOutOfUsernameAttempts` if the username\\n        generator runs out first\\n\\n    The returned value is guaranteed to have been reserved in the database.\\n\\n    '\n    cursor.execute('SAVEPOINT safely_reserve_a_username')\n    seatbelt = 0\n    for username in gen_usernames():\n        seatbelt += 1\n        if (seatbelt > 100):\n            raise FailedToReserveUsername\n        try:\n            check = reserve(cursor, username)\n        except IntegrityError:\n            cursor.execute('ROLLBACK TO safely_reserve_a_username')\n            continue\n        else:\n            assert (check == username)\n            break\n    else:\n        raise RanOutOfUsernameAttempts\n    cursor.execute('RELEASE safely_reserve_a_username')\n    return username\n", "label": 0}
{"function": "\n\ndef save_font(self, name, family, generic):\n    \" It is possible that the HTML browser doesn't know how to\\n            show a particular font. Fortunately ODF provides generic fallbacks.\\n            Unfortunately they are not the same as CSS2.\\n            CSS2: serif, sans-serif, cursive, fantasy, monospace\\n            ODF: roman, swiss, modern, decorative, script, system\\n            This method put the font and fallback into a dictionary\\n        \"\n    htmlgeneric = 'sans-serif'\n    if (generic == 'roman'):\n        htmlgeneric = 'serif'\n    elif (generic == 'swiss'):\n        htmlgeneric = 'sans-serif'\n    elif (generic == 'modern'):\n        htmlgeneric = 'monospace'\n    elif (generic == 'decorative'):\n        htmlgeneric = 'sans-serif'\n    elif (generic == 'script'):\n        htmlgeneric = 'monospace'\n    elif (generic == 'system'):\n        htmlgeneric = 'serif'\n    self.fontdict[name] = (family, htmlgeneric)\n", "label": 0}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01, gamma=10, cof=4000):\n    ''\n    samplerate = self.audio.samplerate\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for y in super(Y4, self).walk(N, freq_base, freq_max, start, end, each=False, combine=False, gamma=gamma, cof=cof):\n        y = np.maximum(0, y)\n        if (not combine):\n            if each:\n                for v in y:\n                    (yield v)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\ndef send_data(sock, data, address=None):\n    if (address is None):\n        _rep = []\n    else:\n        _rep = list(address)\n    for a in data:\n        if isinstance(a, str):\n            _rep.append(a.encode('utf-8'))\n        else:\n            _rep.append(a)\n    while True:\n        try:\n            sock.send_multipart(_rep, zmq.NOBLOCK)\n        except zmq.ZMQError as e:\n            if (e.errno == errno.EAGAIN):\n                gethub().do_write(sock)\n                continue\n            elif (e.errno == errno.EINTR):\n                continue\n            else:\n                raise\n        else:\n            break\n", "label": 0}
{"function": "\n\ndef data(self, index, role=QtCore.Qt.DisplayRole):\n    if (not index.isValid()):\n        return QtCore.QVariant()\n    obj = index.internalPointer()\n    obj_type = obj.get_type()\n    if (role == QtCore.Qt.DisplayRole):\n        if (obj.get_type() in (Object.GROUP, Object.USER)):\n            return QtCore.QVariant(obj.get_display_name())\n    elif (role == QtCore.Qt.UserRole):\n        if (obj_type == Object.GROUP):\n            return QtCore.QVariant(obj.gid)\n        elif (obj_type == Object.USER):\n            return QtCore.QVariant(obj.uri)\n    elif (role == QtCore.Qt.ToolTipRole):\n        if (obj_type == Object.USER):\n            tool_tip = ('%s (URI: %s)' % (obj.get_display_name(), obj.uri))\n            return QtCore.QVariant(tool_tip)\n    elif (role == QtCore.Qt.DecorationRole):\n        if (obj_type == Object.USER):\n            return QtGui.QPixmap('../icons/exit.png')\n    return QtCore.QVariant()\n", "label": 1}
{"function": "\n\ndef url_for_static(filename=None, **kwargs):\n    from uliweb import settings, application\n    from uliweb.core.SimpleFrame import get_url_adapter\n    from urlparse import urlparse, urlunparse, urljoin\n    import urllib\n    domain = application.domains.get('static', {\n        \n    })\n    ver = settings.GLOBAL.STATIC_VER\n    if ver:\n        kwargs['ver'] = ver\n    external = kwargs.pop('_external', False)\n    if (not external):\n        external = domain.get('display', False)\n    if filename.startswith('/'):\n        if filename.endswith('/'):\n            filename = filename[:(- 1)]\n        if kwargs:\n            filename += ('?' + urllib.urlencode(kwargs))\n        if external:\n            return urljoin(domain.get('domain', ''), filename)\n        return filename\n    r = urlparse(filename)\n    if (r.scheme or r.netloc):\n        x = list(r)\n        if kwargs:\n            x[4] = urllib.urlencode(kwargs)\n            return urlunparse(x)\n        else:\n            return filename\n    kwargs['filename'] = filename\n    url_adapter = get_url_adapter('static')\n    return url_adapter.build('uliweb.contrib.staticfiles.static', kwargs, force_external=external)\n", "label": 1}
{"function": "\n\ndef filter_items(value, startswith=None, strip_prefix=False):\n    'Jinja2 filter used to filter a dictionary\\'s keys by specifying a\\n    required prefix.\\n\\n    Returns a list of key/value tuples.\\n\\n    Usage:\\n        {{ my_dict|filter_items }}\\n        {{ my_dict|filter_items(\"MY_PREFIX_\") }}\\n        {{ my_dict|filter_items(\"MY_PREFIX_\", True) }}\\n    '\n    if (startswith is not None):\n        value = [x for x in value.items() if x[0].startswith(startswith)]\n    else:\n        value = value.items()\n    if ((startswith is not None) and strip_prefix):\n        value = [(x[0].replace(startswith, '', 1), x[1]) for x in value]\n    return value\n", "label": 0}
{"function": "\n\ndef _ch_neighbor_connectivity(ch_names, neighbors):\n    'Compute sensor connectivity matrix\\n\\n    Parameters\\n    ----------\\n    ch_names : list of str\\n        The channel names.\\n    neighbors : list of list\\n        A list of list of channel names. The neighbors to\\n        which the channels in ch_names are connected with.\\n        Must be of the same length as ch_names.\\n\\n    Returns\\n    -------\\n    ch_connectivity : scipy.sparse matrix\\n        The connectivity matrix.\\n    '\n    if (len(ch_names) != len(neighbors)):\n        raise ValueError('`ch_names` and `neighbors` must have the same length')\n    set_neighbors = set([c for d in neighbors for c in d])\n    rest = (set(ch_names) - set_neighbors)\n    if (len(rest) > 0):\n        raise ValueError('Some of your neighbors are not present in the list of channel names')\n    for neigh in neighbors:\n        if ((not isinstance(neigh, list)) and (not all((isinstance(c, string_types) for c in neigh)))):\n            raise ValueError('`neighbors` must be a list of lists of str')\n    ch_connectivity = np.eye(len(ch_names), dtype=bool)\n    for (ii, neigbs) in enumerate(neighbors):\n        ch_connectivity[(ii, [ch_names.index(i) for i in neigbs])] = True\n    ch_connectivity = sparse.csr_matrix(ch_connectivity)\n    return ch_connectivity\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('TListSentryRolesRequest')\n    if (self.protocol_version is not None):\n        oprot.writeFieldBegin('protocol_version', TType.I32, 1)\n        oprot.writeI32(self.protocol_version)\n        oprot.writeFieldEnd()\n    if (self.requestorUserName is not None):\n        oprot.writeFieldBegin('requestorUserName', TType.STRING, 2)\n        oprot.writeString(self.requestorUserName)\n        oprot.writeFieldEnd()\n    if (self.groupName is not None):\n        oprot.writeFieldBegin('groupName', TType.STRING, 3)\n        oprot.writeString(self.groupName)\n        oprot.writeFieldEnd()\n    if (self.component is not None):\n        oprot.writeFieldBegin('component', TType.STRING, 4)\n        oprot.writeString(self.component)\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 0}
{"function": "\n\ndef test_indexSearch(self, dataFrame):\n    datasearch = DataSearch('Test', dataFrame=dataFrame)\n    filterString = 'indexSearch([0])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 1)\n    filterString = 'indexSearch([0, 2])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 2)\n    filterString = 'indexSearch([0, 1, 2])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 3)\n    filterString = 'indexSearch([99])'\n    datasearch.setFilterString(filterString)\n    (ret, valid) = datasearch.search()\n    assert valid\n    assert (sum(ret) == 0)\n", "label": 0}
{"function": "\n\ndef _set_item(self, item_id, value):\n    'Set the current value of an item in the dialog\\n        '\n    item_hwnd = wrapped(win32gui.GetDlgItem, self.hwnd, item_id)\n    class_name = wrapped(win32gui.GetClassName, item_hwnd)\n    styles = wrapped(win32gui.GetWindowLong, self.hwnd, win32con.GWL_STYLE)\n    if (class_name == 'Edit'):\n        if isinstance(value, datetime.date):\n            value = value.strftime('%d %b %Y')\n        value = unicode(value).replace('\\r\\n', '\\n').replace('\\n', '\\r\\n')\n        wrapped(win32gui.SetDlgItemText, self.hwnd, item_id, value)\n    elif (class_name == 'Button'):\n        SendMessage(item_hwnd, win32con.BM_SETCHECK, int(value), 0)\n    elif (class_name == 'ComboBox'):\n        for item in value:\n            if isinstance(item, tuple):\n                item = item[0]\n            SendMessage(item_hwnd, win32con.CB_ADDSTRING, 0, utils.string_as_pointer(str(item)))\n        SendMessage(item_hwnd, win32con.CB_SETCURSEL, 0, 0)\n    elif (class_name == 'Static'):\n        wrapped(win32gui.SetDlgItemText, self.hwnd, item_id, unicode(value))\n    else:\n        raise RuntimeError(('Unknown class: %s' % class_name))\n", "label": 0}
{"function": "\n\ndef pre_build_check():\n    '\\n    Try to verify build tools\\n    '\n    if os.environ.get('CASS_DRIVER_NO_PRE_BUILD_CHECK'):\n        return True\n    try:\n        from distutils.ccompiler import new_compiler\n        from distutils.sysconfig import customize_compiler\n        from distutils.dist import Distribution\n        be = build_ext(Distribution())\n        be.initialize_options()\n        be.finalize_options()\n        have_python_include = any((os.path.isfile(os.path.join(p, 'Python.h')) for p in be.include_dirs))\n        if (not have_python_include):\n            sys.stderr.write((\"Did not find 'Python.h' in %s.\\n\" % (be.include_dirs,)))\n            return False\n        compiler = new_compiler(compiler=be.compiler)\n        customize_compiler(compiler)\n        executables = []\n        if (compiler.compiler_type in ('unix', 'cygwin')):\n            executables = [compiler.executables[exe][0] for exe in ('compiler_so', 'linker_so')]\n        elif (compiler.compiler_type == 'nt'):\n            executables = [getattr(compiler, exe) for exe in ('cc', 'linker')]\n        if executables:\n            from distutils.spawn import find_executable\n            for exe in executables:\n                if (not find_executable(exe)):\n                    sys.stderr.write(('Failed to find %s for compiler type %s.\\n' % (exe, compiler.compiler_type)))\n                    return False\n    except Exception as exc:\n        sys.stderr.write(('%s\\n' % str(exc)))\n        sys.stderr.write('Failed pre-build check. Attempting anyway.\\n')\n    return True\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    print(('Querying database at %s' % CURRENCY_API_URL))\n    currencies = []\n    if len(args):\n        currencies = list(args)\n    api = urlopen(CURRENCY_API_URL)\n    d = json.loads(api.read())\n    i = 0\n    for currency in sorted(d.keys()):\n        if ((not currencies) or (currency in currencies)):\n            if (not Currency.objects.filter(code=currency)):\n                print(('Creating %r (%s)' % (d[currency], currency)))\n                Currency(code=currency, name=d[currency], factor=1.0, is_active=False).save()\n                i += 1\n    if (i == 1):\n        print(('%i new currency' % i))\n    else:\n        print(('%i new currencies' % i))\n", "label": 0}
{"function": "\n\ndef doNick(self, msg):\n    'Handles NICK messages.'\n    if (msg.nick == self.nick):\n        newNick = msg.args[0]\n        self.nick = newNick\n        (nick, user, domain) = ircutils.splitHostmask(msg.prefix)\n        self.prefix = ircutils.joinHostmask(self.nick, user, domain)\n    elif conf.supybot.followIdentificationThroughNickChanges():\n        try:\n            id = ircdb.users.getUserId(msg.prefix)\n            u = ircdb.users.getUser(id)\n        except KeyError:\n            return\n        if u.auth:\n            (_, user, host) = ircutils.splitHostmask(msg.prefix)\n            newhostmask = ircutils.joinHostmask(msg.args[0], user, host)\n            for (i, (when, authmask)) in enumerate(u.auth[:]):\n                if ircutils.strEqual(msg.prefix, authmask):\n                    log.info('Following identification for %s: %s -> %s', u.name, authmask, newhostmask)\n                    u.auth[i] = (u.auth[i][0], newhostmask)\n                    ircdb.users.setUser(u)\n", "label": 0}
{"function": "\n\ndef ToPrimitiveDict(self):\n    'Handle dict generation specifically for Artifacts.'\n    artifact_dict = super(Artifact, self).ToPrimitiveDict()\n    artifact_dict['name'] = utils.SmartStr(self.name)\n    for source in artifact_dict['sources']:\n        if ('type' in source):\n            source['type'] = str(source['type'])\n        if ('key_value_pairs' in source['attributes']):\n            outarray = []\n            for indict in source['attributes']['key_value_pairs']:\n                outarray.append(dict(indict.items()))\n            source['attributes']['key_value_pairs'] = outarray\n    for field in self.required_repeated_fields:\n        if (field not in artifact_dict):\n            artifact_dict[field] = []\n    return artifact_dict\n", "label": 0}
{"function": "\n\n@login_required\ndef new(request, form_class=BlogForm, template_name='blog/new.html'):\n    if (request.method == 'POST'):\n        if (request.POST['action'] == 'create'):\n            blog_form = form_class(request.user, request.POST)\n            if blog_form.is_valid():\n                blog = blog_form.save(commit=False)\n                blog.author = request.user\n                if getattr(settings, 'BEHIND_PROXY', False):\n                    blog.creator_ip = request.META['HTTP_X_FORWARDED_FOR']\n                else:\n                    blog.creator_ip = request.META['REMOTE_ADDR']\n                blog.save()\n                messages.add_message(request, messages.SUCCESS, (ugettext(\"Successfully saved post '%s'\") % blog.title))\n                if notification:\n                    if (blog.status == 2):\n                        if friends:\n                            notification.send((x['friend'] for x in Friendship.objects.friends_for_user(blog.author)), 'blog_friend_post', {\n                                'post': blog,\n                            })\n                return HttpResponseRedirect(reverse('blog_list_yours'))\n        else:\n            blog_form = form_class()\n    else:\n        blog_form = form_class()\n    return render_to_response(template_name, {\n        'blog_form': blog_form,\n    }, context_instance=RequestContext(request))\n", "label": 0}
{"function": "\n\n@command\ndef module(self, input='', search_type='prefix', project=None, file=None, module=None, deps=None, sandbox=None, cabal=False, db=None, package=None, source=False, standalone=False):\n    q = {\n        'input': input,\n        'type': search_type,\n    }\n    fs = []\n    if project:\n        fs.append({\n            'project': project,\n        })\n    if file:\n        fs.append({\n            'file': file,\n        })\n    if module:\n        fs.append({\n            'module': module,\n        })\n    if deps:\n        fs.append({\n            'deps': deps,\n        })\n    if sandbox:\n        fs.append({\n            'cabal': {\n                'sandbox': sandbox,\n            },\n        })\n    if cabal:\n        fs.append({\n            'cabal': 'cabal',\n        })\n    if db:\n        fs.append({\n            'db': encode_package_db(db),\n        })\n    if package:\n        fs.append({\n            'package': package,\n        })\n    if source:\n        fs.append('sourced')\n    if standalone:\n        fs.append('standalone')\n    return cmd('module', {\n        'query': q,\n        'filters': fs,\n    }, parse_modules)\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if (not isinstance(other, Vulnerability)):\n        raise TypeError((\"Expected Vulnerability, got '%s' instead\" % type(other)))\n    if (other.cves != self.cves):\n        return False\n    if (other.threat != self.threat):\n        return False\n    if (other.name != self.name):\n        return False\n    if (other.cvss != self.cvss):\n        return False\n    if (other.description != self.description):\n        return False\n    if (other.id != self.id):\n        return False\n    if (other.level != self.level):\n        return False\n    if (other.references != self.references):\n        return False\n    for (host, port) in self.hosts:\n        for (o_host, o_port) in other.hosts:\n            if ((o_host == host) and (o_port == port)):\n                break\n        else:\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef MergeFlags(self, args, unique=1, dict=None):\n    '\\n        Merge the dict in args into the construction variables of this\\n        env, or the passed-in dict.  If args is not a dict, it is\\n        converted into a dict using ParseFlags.  If unique is not set,\\n        the flags are appended rather than merged.\\n        '\n    if (dict is None):\n        dict = self\n    if (not SCons.Util.is_Dict(args)):\n        args = self.ParseFlags(args)\n    if (not unique):\n        self.Append(**args)\n        return self\n    for (key, value) in args.items():\n        if (not value):\n            continue\n        try:\n            orig = self[key]\n        except KeyError:\n            orig = value\n        else:\n            if (not orig):\n                orig = value\n            elif value:\n                try:\n                    orig = (orig + value)\n                except (KeyError, TypeError):\n                    try:\n                        add_to_orig = orig.append\n                    except AttributeError:\n                        value.insert(0, orig)\n                        orig = value\n                    else:\n                        add_to_orig(value)\n        t = []\n        if (key[(- 4):] == 'PATH'):\n            for v in orig:\n                if (v not in t):\n                    t.append(v)\n        else:\n            orig.reverse()\n            for v in orig:\n                if (v not in t):\n                    t.insert(0, v)\n        self[key] = t\n    return self\n", "label": 1}
{"function": "\n\ndef resolve_promise(o):\n    if isinstance(o, dict):\n        for (k, v) in o.items():\n            o[k] = resolve_promise(v)\n    elif isinstance(o, (list, tuple)):\n        o = [resolve_promise(x) for x in o]\n    elif isinstance(o, Promise):\n        try:\n            o = force_unicode(o)\n        except:\n            try:\n                o = [resolve_promise(x) for x in o]\n            except:\n                raise Exception(('Unable to resolve lazy object %s' % o))\n    elif callable(o):\n        o = o()\n    return o\n", "label": 1}
{"function": "\n\ndef check_char_lookup(self, lookup):\n    lname = ('field__' + lookup)\n    mymodel = CharListModel.objects.create(field=['mouldy', 'rotten'])\n    mouldy = CharListModel.objects.filter(**{\n        lname: 'mouldy',\n    })\n    assert (mouldy.count() == 1)\n    assert (mouldy[0] == mymodel)\n    rotten = CharListModel.objects.filter(**{\n        lname: 'rotten',\n    })\n    assert (rotten.count() == 1)\n    assert (rotten[0] == mymodel)\n    clean = CharListModel.objects.filter(**{\n        lname: 'clean',\n    })\n    assert (clean.count() == 0)\n    with pytest.raises(ValueError):\n        list(CharListModel.objects.filter(**{\n            lname: ['a', 'b'],\n        }))\n    both = CharListModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) & Q(**{\n        lname: 'rotten',\n    })))\n    assert (both.count() == 1)\n    assert (both[0] == mymodel)\n    either = CharListModel.objects.filter((Q(**{\n        lname: 'mouldy',\n    }) | Q(**{\n        lname: 'clean',\n    })))\n    assert (either.count() == 1)\n    not_clean = CharListModel.objects.exclude(**{\n        lname: 'clean',\n    })\n    assert (not_clean.count() == 1)\n    not_mouldy = CharListModel.objects.exclude(**{\n        lname: 'mouldy',\n    })\n    assert (not_mouldy.count() == 0)\n", "label": 1}
{"function": "\n\ndef enqueue(self, pdu):\n    self.log('enqueue {pdu.name} PDU'.format(pdu=pdu))\n    if (not (pdu.type in connection_mode_pdu_types)):\n        self.err('non connection mode pdu on data link connection')\n        pdu = FrameReject.from_pdu(pdu, flags='W', dlc=self)\n        self.close()\n        self.send_queue.append(pdu)\n        return\n    if self.state.CLOSED:\n        pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=1)\n        self.send_queue.append(pdu)\n    if self.state.LISTEN:\n        if isinstance(pdu, Connect):\n            if (super(DataLinkConnection, self).enqueue(pdu) == False):\n                log.warn('full backlog on listening socket')\n                pdu = DisconnectedMode(pdu.ssap, pdu.dsap, reason=32)\n                self.send_queue.append(pdu)\n                return False\n            return True\n    if self.state.CONNECT:\n        if (isinstance(pdu, ConnectionComplete) or isinstance(pdu, DisconnectedMode)):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.DISCONNECT:\n        if isinstance(pdu, DisconnectedMode):\n            with self.lock:\n                self.recv_queue.append(pdu)\n                self.recv_ready.notify()\n    if self.state.ESTABLISHED:\n        return self._enqueue_state_established(pdu)\n", "label": 1}
{"function": "\n\n@verbose\ndef save(self, fname, ftype='stc', verbose=None):\n    'Save the source estimates to a file\\n\\n        Parameters\\n        ----------\\n        fname : string\\n            The stem of the file name. The stem is extended with \"-vl.stc\"\\n            or \"-vl.w\".\\n        ftype : string\\n            File format to use. Allowed values are \"stc\" (default) and \"w\".\\n            The \"w\" format only supports a single time point.\\n        verbose : bool, str, int, or None\\n            If not None, override default verbose level (see mne.verbose).\\n            Defaults to self.verbose.\\n        '\n    if (ftype not in ['stc', 'w']):\n        raise ValueError(('ftype must be \"stc\" or \"w\", not \"%s\"' % ftype))\n    if (ftype == 'stc'):\n        logger.info('Writing STC to disk...')\n        if (not (fname.endswith('-vl.stc') or fname.endswith('-vol.stc'))):\n            fname += '-vl.stc'\n        _write_stc(fname, tmin=self.tmin, tstep=self.tstep, vertices=self.vertices, data=self.data)\n    elif (ftype == 'w'):\n        logger.info('Writing STC to disk (w format)...')\n        if (not (fname.endswith('-vl.w') or fname.endswith('-vol.w'))):\n            fname += '-vl.w'\n        _write_w(fname, vertices=self.vertices, data=self.data)\n    logger.info('[done]')\n", "label": 0}
{"function": "\n\ndef add_image_info_cb(self, viewer, channel, info):\n    save_thumb = self.settings.get('cache_thumbs', False)\n    chname = channel.name\n    thumbkey = self.get_thumb_key(chname, info.name, info.path)\n    thumbpath = self.get_thumbpath(info.path)\n    with self.thmblock:\n        try:\n            bnch = self.thumbDict[thumbkey]\n            if (bnch.thumbpath == thumbpath):\n                return\n        except KeyError:\n            pass\n    image = None\n    if ((thumbpath is not None) and os.path.exists(thumbpath)):\n        save_thumb = False\n        try:\n            image = self.fv.load_image(thumbpath)\n        except Exception as e:\n            pass\n    try:\n        if (image is None):\n            image = info.image_loader(info.path)\n        image.set(name=info.name)\n        self.fv.gui_do(self._make_thumb, chname, image, info.name, info.path, thumbkey, info.image_future, save_thumb=save_thumb, thumbpath=thumbpath)\n    except Exception as e:\n        self.logger.error((\"Error generating thumbnail for '%s': %s\" % (info.path, str(e))))\n", "label": 0}
{"function": "\n\ndef decrypt(self, encBytes):\n    'Decrypt the passed-in bytes.\\n\\n        This requires the key to have a private component.  It performs\\n        PKCS1 decryption of the passed-in data.\\n\\n        @type encBytes: L{bytearray} of unsigned bytes\\n        @param encBytes: The value which will be decrypted.\\n\\n        @rtype: L{bytearray} of unsigned bytes or None.\\n        @return: A PKCS1 decryption of the passed-in data or None if\\n        the data is not properly formatted.\\n        '\n    if (not self.hasPrivateKey()):\n        raise AssertionError()\n    if (len(encBytes) != numBytes(self.n)):\n        return None\n    c = bytesToNumber(encBytes)\n    if (c >= self.n):\n        return None\n    m = self._rawPrivateKeyOp(c)\n    decBytes = numberToBytes(m, numBytes(self.n))\n    if ((decBytes[0] != 0) or (decBytes[1] != 2)):\n        return None\n    for x in range(1, (len(decBytes) - 1)):\n        if (decBytes[x] == 0):\n            break\n    else:\n        return None\n    return decBytes[(x + 1):]\n", "label": 0}
{"function": "\n\ndef run(self):\n    isIdle = False\n    while (not self._finish):\n        (state, nextCheck, idle) = self._tracker.check_idle()\n        if ((state == 'idle') and (not isIdle) and (idle >= (self._idleSeconds * 1000))):\n            self.emit(QtCore.SIGNAL('idle()'))\n        elif ((state is not None) and isIdle):\n            self.emit(QtCore.SIGNAL('active()'))\n        if (state is not None):\n            isIdle = (state == 'idle')\n        self.usleep((nextCheck * 1000))\n", "label": 0}
{"function": "\n\ndef prepareQsub(self, cpu, mem, jobID):\n    qsubline = ['qsub', '-b', 'y', '-terse', '-j', 'y', '-cwd', '-o', '/dev/null', '-e', '/dev/null', '-N', ('toil_job_' + str(jobID))]\n    if self.boss.environment:\n        qsubline.append('-v')\n        qsubline.append(','.join((((k + '=') + quote((os.environ[k] if (v is None) else v))) for (k, v) in self.boss.environment.iteritems())))\n    reqline = list()\n    if (mem is not None):\n        memStr = (str((mem / 1024)) + 'K')\n        reqline += [('vf=' + memStr), ('h_vmem=' + memStr)]\n    if (len(reqline) > 0):\n        qsubline.extend(['-hard', '-l', ','.join(reqline)])\n    if ((cpu is not None) and (math.ceil(cpu) > 1)):\n        peConfig = (os.getenv('TOIL_GRIDENGINE_PE') or 'shm')\n        qsubline.extend(['-pe', peConfig, str(int(math.ceil(cpu)))])\n    return qsubline\n", "label": 0}
{"function": "\n\ndef receive(self, category=None, timeout=None, alarm_value=None):\n    \"Similar to 'receive' of Coro, except it retrieves (waiting,\\n        if necessary) messages in given 'category'.\\n        \"\n    c = self._categories.get(category, None)\n    if c:\n        msg = c.popleft()\n        raise StopIteration(msg)\n    if timeout:\n        start = _time()\n    while 1:\n        msg = (yield self._coro.receive(timeout=timeout, alarm_value=alarm_value))\n        if (msg == alarm_value):\n            raise StopIteration(msg)\n        for categorize in reversed(self._categorize):\n            c = categorize(msg)\n            if (c == category):\n                raise StopIteration(msg)\n            if (c is not None):\n                bucket = self._categories.get(c, None)\n                if (not bucket):\n                    bucket = self._categories[c] = collections.deque()\n                bucket.append(msg)\n                break\n        else:\n            self._categories[None].append(msg)\n        if timeout:\n            now = _time()\n            timeout -= (now - start)\n            start = now\n", "label": 1}
{"function": "\n\n@classmethod\ndef email_url_config(cls, url, backend=None):\n    'Parses an email URL.'\n    config = {\n        \n    }\n    url = (urlparse.urlparse(url) if (not isinstance(url, cls.URL_CLASS)) else url)\n    path = url.path[1:]\n    path = path.split('?', 2)[0]\n    config.update({\n        'EMAIL_FILE_PATH': path,\n        'EMAIL_HOST_USER': url.username,\n        'EMAIL_HOST_PASSWORD': url.password,\n        'EMAIL_HOST': url.hostname,\n        'EMAIL_PORT': _cast_int(url.port),\n    })\n    if backend:\n        config['EMAIL_BACKEND'] = backend\n    elif (url.scheme not in cls.EMAIL_SCHEMES):\n        raise ImproperlyConfigured(('Invalid email schema %s' % url.scheme))\n    elif (url.scheme in cls.EMAIL_SCHEMES):\n        config['EMAIL_BACKEND'] = cls.EMAIL_SCHEMES[url.scheme]\n    if (url.scheme in ('smtps', 'smtp+tls')):\n        config['EMAIL_USE_TLS'] = True\n    elif (url.scheme == 'smtp+ssl'):\n        config['EMAIL_USE_SSL'] = True\n    if url.query:\n        config_options = {\n            \n        }\n        for (k, v) in urlparse.parse_qs(url.query).items():\n            opt = {\n                k.upper(): _cast_int(v[0]),\n            }\n            if (k.upper() in cls._EMAIL_BASE_OPTIONS):\n                config.update(opt)\n            else:\n                config_options.update(opt)\n        config['OPTIONS'] = config_options\n    return config\n", "label": 1}
{"function": "\n\ndef onEnterNode(self, node):\n    for variable in node.getClosureVariables():\n        assert (not variable.isModuleVariable())\n        current = node\n        while (current is not variable.getOwner()):\n            if current.isParentVariableProvider():\n                if (variable not in current.getClosureVariables()):\n                    current.addClosureVariable(variable)\n            assert (current.getParentVariableProvider() is not current)\n            current = current.getParentVariableProvider()\n            assert (current is not None), variable\n", "label": 0}
{"function": "\n\n@register(_dump_registry, bytes)\ndef _dump_bytes(obj, stream):\n    l = len(obj)\n    if (l == 0):\n        stream.append(TAG_EMPTY_STR)\n    elif (l == 1):\n        stream.append((TAG_STR1 + obj))\n    elif (l == 2):\n        stream.append((TAG_STR2 + obj))\n    elif (l == 3):\n        stream.append((TAG_STR3 + obj))\n    elif (l == 4):\n        stream.append((TAG_STR4 + obj))\n    elif (l < 256):\n        stream.append(((TAG_STR_L1 + I1.pack(l)) + obj))\n    else:\n        stream.append(((TAG_STR_L4 + I4.pack(l)) + obj))\n", "label": 0}
{"function": "\n\ndef on_widget(self, instance, value):\n    stack = self.ids.stack\n    prefs = [btn.widget_ref() for btn in self.parents]\n    if (value in prefs):\n        index = prefs.index(value)\n        for btn in self.parents:\n            btn.state = 'normal'\n        self.parents[index].state = 'down'\n        return\n    stack.clear_widgets()\n    if (not value):\n        return\n    widget = value\n    parents = []\n    while True:\n        btn = ConsoleButton(text=widget.__class__.__name__)\n        btn.widget_ref = weakref.ref(widget)\n        btn.bind(on_release=self.highlight_widget)\n        parents.append(btn)\n        if (widget == widget.parent):\n            break\n        widget = widget.parent\n    for btn in reversed(parents):\n        stack.add_widget(btn)\n    self.ids.sv.scroll_x = 1\n    self.parents = parents\n    btn.state = 'down'\n", "label": 0}
{"function": "\n\ndef print_scoreboard(self):\n    'Print object as a scoreboard.'\n    output = ''\n    innings = []\n    away = []\n    home = []\n    for x in self:\n        innings.append(x['inning'])\n        away.append(x['away'])\n        home.append(x['home'])\n    output += 'Inning\\t'\n    for x in innings:\n        output += (str(x) + ' ')\n    output += '\\n'\n    for x in innings:\n        output += '---'\n    output += '\\nAway\\t'\n    for (y, x) in enumerate(away, start=1):\n        if (y >= 10):\n            output += (str(x) + '  ')\n        else:\n            output += (str(x) + ' ')\n    output += '\\nHome\\t'\n    for (y, x) in enumerate(home, start=1):\n        if (y >= 10):\n            output += (str(x) + '  ')\n        else:\n            output += (str(x) + ' ')\n    return output\n", "label": 0}
{"function": "\n\ndef test_upload_image(client, mocker):\n    today = datetime.date.today()\n    mocker.patch('requests.get')\n    mocker.patch('redwind.tasks.create_queue')\n    rv = client.post('/save_new', data={\n        'photo': (open('tests/image.jpg', 'rb'), 'image.jpg', 'image/jpeg'),\n        'post_type': 'photo',\n        'content': 'High score',\n        'action': 'publish_quietly',\n    })\n    assert (rv.status_code == 302)\n    assert (rv.location == 'http://example.com/{}/{:02d}/high-score'.format(today.year, today.month))\n    permalink = rv.location\n    rv = client.get(permalink)\n    assert (rv.status_code == 200)\n    content = rv.get_data(as_text=True)\n    assert ('High score' in content)\n    assert ('<img' in content)\n    rv = client.get((permalink + '/files/image.jpg'))\n    assert (rv.status_code == 200)\n", "label": 0}
{"function": "\n\ndef _check_failure_put_connections(self, conns, req, nodes, min_conns):\n    '\\n        Identify any failed connections and check minimum connection count.\\n        '\n    if ((req.if_none_match is not None) and ('*' in req.if_none_match)):\n        statuses = [conn.resp.status for conn in conns if conn.resp]\n        if (HTTP_PRECONDITION_FAILED in statuses):\n            self.app.logger.debug(_('Object PUT returning 412, %(statuses)r'), {\n                'statuses': statuses,\n            })\n            raise HTTPPreconditionFailed(request=req)\n    if any((conn for conn in conns if (conn.resp and (conn.resp.status == HTTP_CONFLICT)))):\n        status_times = [('%(status)s (%(timestamp)s)' % {\n            'status': conn.resp.status,\n            'timestamp': HeaderKeyDict(conn.resp.getheaders()).get('X-Backend-Timestamp', 'unknown'),\n        }) for conn in conns if conn.resp]\n        self.app.logger.debug(_('Object PUT returning 202 for 409: %(req_timestamp)s <= %(timestamps)r'), {\n            'req_timestamp': req.timestamp.internal,\n            'timestamps': ', '.join(status_times),\n        })\n        raise HTTPAccepted(request=req)\n    self._check_min_conn(req, conns, min_conns)\n", "label": 1}
{"function": "\n\ndef infer_shape(self, node, shapes):\n    (images, hidacts, frows, fcols) = node.inputs\n    (ishape, hshape, frowshp, fcolshp) = shapes\n    (igroups, icolors_per_group, irows, icols, icount) = ishape\n    (hgroups, hcolors_per_group, hrows, hcols, hcount) = hshape\n    fmodulesR = hrows\n    fmodulesC = hcols\n    fcolors = icolors_per_group\n    fgroups = hgroups\n    filters_per_group = hcolors_per_group\n    fshape = (fmodulesR, fmodulesC, fcolors, frows, fcols, fgroups, filters_per_group)\n    if (not_symbolic(irows, icols) and (irows != icols)):\n        raise NotImplementedError('non-square image argument', (irows, icols))\n    if (not_symbolic(hrows, hcols) and (hrows != hcols)):\n        raise NotImplementedError('non-square filter shape', (hrows, hcols))\n    if (not_symbolic(icount, hcount) and (icount != hcount)):\n        raise NotImplementedError('different number of images', (icount, hcount))\n    if (not_symbolic(igroups, hgroups) and (igroups != hgroups)):\n        raise ValueError('hgroups must match igroups', igroups, hgroups)\n    return [fshape]\n", "label": 0}
{"function": "\n\ndef _get_text(self, encoding):\n    lines = self.editor.toPlainText().splitlines()\n    if self.clean_trailing_whitespaces:\n        lines = [l.rstrip() for l in lines]\n    try:\n        last_line = lines[(- 1)]\n    except IndexError:\n        pass\n    else:\n        while (last_line == ''):\n            try:\n                lines.pop()\n                last_line = lines[(- 1)]\n            except IndexError:\n                last_line = None\n    text = (self._eol.join(lines) + self._eol)\n    return text.encode(encoding)\n", "label": 0}
{"function": "\n\ndef mon_status(conn, logger, hostname, args, silent=False):\n    '\\n    run ``ceph daemon mon.`hostname` mon_status`` on the remote end and provide\\n    not only the output, but be able to return a boolean status of what is\\n    going on.\\n    ``False`` represents a monitor that is not doing OK even if it is up and\\n    running, while ``True`` would mean the monitor is up and running correctly.\\n    '\n    mon = ('mon.%s' % hostname)\n    try:\n        out = mon_status_check(conn, logger, hostname, args)\n        if (not out):\n            logger.warning(('monitor: %s, might not be running yet' % mon))\n            return False\n        if (not silent):\n            logger.debug(('*' * 80))\n            logger.debug(('status for monitor: %s' % mon))\n            for line in json.dumps(out, indent=2, sort_keys=True).split('\\n'):\n                logger.debug(line)\n            logger.debug(('*' * 80))\n        if (out['rank'] >= 0):\n            logger.info(('monitor: %s is running' % mon))\n            return True\n        if ((out['rank'] == (- 1)) and out['state']):\n            logger.info(('monitor: %s is currently at the state of %s' % (mon, out['state'])))\n            return True\n        logger.info(('monitor: %s is not running' % mon))\n        return False\n    except RuntimeError:\n        logger.info(('monitor: %s is not running' % mon))\n        return False\n", "label": 0}
{"function": "\n\ndef execute(self, cluster, commands):\n    pipes = {\n        \n    }\n    pool = self.get_pool(commands)\n    for (db_num, command_list) in commands.iteritems():\n        pipes[db_num] = cluster[db_num].get_pipeline()\n        for command in command_list:\n            pipes[db_num].add(command.clone())\n    for (db_num, pipe) in pipes.iteritems():\n        pool.add(db_num, pipe.execute, (), {\n            \n        })\n    db_result_map = pool.join()\n    results = defaultdict(list)\n    for (db_num, db_results) in db_result_map.iteritems():\n        assert (len(db_results) == 1)\n        db_results = db_results[0]\n        if isinstance(db_results, Exception):\n            for command in commands[db_num]:\n                results[command].append(db_results)\n            continue\n        for (command, result) in db_results.iteritems():\n            results[command].append(result)\n    return results\n", "label": 0}
{"function": "\n\ndef _HasHeaders(self, args):\n    'Look at the args and decided if we expect headers or not.'\n    headers = True\n    for arg in args:\n        if (arg in ['--no-headers', 'h', '--no-heading']):\n            headers = False\n        elif (arg in ['--headers']):\n            headers = True\n        elif (('h' in arg) and (not arg.startswith('-')) and (',' not in arg)):\n            headers = False\n    return headers\n", "label": 0}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 10):\n            self.set_package(d.getPrefixedString())\n            continue\n        if (tt == 18):\n            self.set_capability(d.getPrefixedString())\n            continue\n        if (tt == 24):\n            self.set_status(d.getVarInt32())\n            continue\n        if (tt == 34):\n            self.set_internal_message(d.getPrefixedString())\n            continue\n        if (tt == 42):\n            self.set_admin_message(d.getPrefixedString())\n            continue\n        if (tt == 50):\n            self.set_error_message(d.getPrefixedString())\n            continue\n        if (tt == 58):\n            self.set_scheduled_time(d.getPrefixedString())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef delete_buf(self, path):\n    if (not utils.is_shared(path)):\n        msg.error(('Skipping deleting %s because it is not in shared path %s.' % (path, G.PROJECT_PATH)))\n        return\n    if os.path.isdir(path):\n        for (dirpath, dirnames, filenames) in os.walk(path):\n            dirnames[:] = [d for d in dirnames if (d[0] != '.')]\n            for f in filenames:\n                f_path = os.path.join(dirpath, f)\n                if (f[0] == '.'):\n                    msg.log(('Not deleting buf for hidden file %s' % f_path))\n                else:\n                    self.delete_buf(f_path)\n        return\n    buf_to_delete = self.get_buf_by_path(path)\n    if (buf_to_delete is None):\n        msg.error(('%s is not in this workspace' % path))\n        return\n    msg.log('deleting buffer ', utils.to_rel_path(path))\n    event = {\n        'name': 'delete_buf',\n        'id': buf_to_delete['id'],\n    }\n    G.AGENT.send(event)\n", "label": 0}
{"function": "\n\ndef _cleanse_tags(self, message):\n    'Using BeautifulSoup, remove or modify improper tags & attributes'\n    soup = BeautifulSoup(message, 'lxml')\n    for tag in soup.findAll():\n        if (tag.name not in self.VALID_TAGS):\n            tag.hidden = True\n        for (attr, value) in dict(tag.attrs).iteritems():\n            if (attr not in self.VALID_ATTRS):\n                del tag.attrs[attr]\n            if (attr == 'src'):\n                parsed_src = urlparse(value)\n                valid_netlocs = settings.ALLOWED_HOSTS\n                valid_netlocs.append('')\n                if (parsed_src.netloc not in valid_netlocs):\n                    tag.hidden = True\n            if (attr == 'href'):\n                parsed_src = urlparse(value)\n                if (parsed_src.scheme not in self.VALID_SCHEMES):\n                    tag.hidden = True\n        if (self.ADD_MAX_WIDTH and (tag.name == 'img')):\n            tag.attrs['style'] = 'max-width: 100%;'\n    return unicode(soup).encode('utf-8', errors='ignore')\n", "label": 1}
{"function": "\n\ndef api_serialize_impl(self, withClean=False):\n    Util.validate_type(withClean, 'bool')\n    missing = []\n    ret = {\n        \n    }\n    if (withClean or self.n_id):\n        Util.set_by_path(ret, 'ID', self.m_id)\n    if (withClean or self.n_name):\n        Util.set_by_path(ret, 'Name', self.m_name)\n    elif self.is_new:\n        missing.append('name')\n    if (withClean or self.n_description):\n        Util.set_by_path(ret, 'Description', self.m_description)\n    if (withClean or self.n_network_mask_len):\n        Util.set_by_path(ret, 'NetworkMaskLen', self.m_network_mask_len)\n    elif self.is_new:\n        missing.append('network_mask_len')\n    if (withClean or self.n_band_width_mbps):\n        Util.set_by_path(ret, 'BandWidthMbps', self.m_band_width_mbps)\n    elif self.is_new:\n        missing.append('band_width_mbps')\n    if (withClean or self.n_swytch_id):\n        Util.set_by_path(ret, 'Switch.ID', self.m_swytch_id)\n    if (len(missing) > 0):\n        raise SaklientException('required_field', ('Required fields must be set before the Router creation: ' + ', '.join(missing)))\n    return ret\n", "label": 1}
{"function": "\n\ndef finalize(self):\n    if (self.ct == 0):\n        return\n    elif (self.ct == 1):\n        return 0\n    prev = min_diff = None\n    while self.heap:\n        if (min_diff is None):\n            if (prev is None):\n                prev = heapq.heappop(self.heap)\n                continue\n        curr = heapq.heappop(self.heap)\n        diff = (curr - prev)\n        if ((min_diff is None) or (min_diff > diff)):\n            min_diff = diff\n        prev = curr\n    return min_diff\n", "label": 0}
{"function": "\n\ndef load_ctfs(saved_data, volume_property):\n    ' Given the saved data produced via `save_ctfs`, this sets the\\n    state of the passed volume_property appropriately.\\n\\n    It returns the new color transfer function and piecewise function.\\n    '\n    rgb = saved_data['rgb']\n    a = saved_data['alpha']\n    new_ctf = True\n    ctf = volume_property.rgb_transfer_function\n    if isinstance(ctf, ColorTransferFunction):\n        new_ctf = False\n        ctf.remove_all_points()\n    else:\n        ctf = ColorTransferFunction()\n    nc = len(rgb)\n    for i in range(nc):\n        ctf.add_rgb_point(rgb[i][0], *rgb[i][1:])\n    if new_ctf:\n        volume_property.set_color(ctf)\n    try:\n        ctf.range = saved_data['range']\n    except Exception:\n        pass\n    na = len(a)\n    new_otf = True\n    otf = volume_property.get_scalar_opacity()\n    if isinstance(otf, PiecewiseFunction):\n        new_otf = False\n        otf.remove_all_points()\n    else:\n        otf = PiecewiseFunction()\n    for i in range(na):\n        otf.add_point(a[i][0], a[i][1])\n    if new_otf:\n        volume_property.set_scalar_opacity(otf)\n    return (ctf, otf)\n", "label": 0}
{"function": "\n\ndef __rebuildAsAssignment(node, firstVarStatement):\n    'Rebuilds the items of a var statement into a assignment list and moves declarations to the given var statement'\n    assignment = Node.Node(node.tokenizer, 'semicolon')\n    assignmentList = Node.Node(node.tokenizer, 'comma')\n    assignment.append(assignmentList, 'expression')\n    for child in list(node):\n        if hasattr(child, 'name'):\n            if hasattr(child, 'initializer'):\n                assign = __createSimpleAssignment(child.name, child.initializer)\n                assignmentList.append(assign)\n            firstVarStatement.append(child)\n        else:\n            for identifier in child.names:\n                firstVarStatement.append(__createDeclaration(identifier.value))\n            if hasattr(child, 'initializer'):\n                assign = __createMultiAssignment(child.names, child.initializer)\n                assignmentList.append(assign)\n            node.remove(child)\n    if (len(assignmentList) > 0):\n        node.parent.replace(node, assignment)\n    elif (getattr(node, 'rel', None) == 'iterator'):\n        if hasattr(child, 'name'):\n            node.parent.replace(node, __createIdentifier(child.name))\n        else:\n            node.parent.replace(node, child.names)\n    else:\n        if hasattr(node, 'rel'):\n            Console.warn(('Remove related node (%s) from parent: %s' % (node.rel, node)))\n        node.parent.remove(node)\n    if (len(assignmentList) == 1):\n        assignment.replace(assignmentList, assignmentList[0])\n", "label": 1}
{"function": "\n\ndef _post_parse(self):\n    for test_id in self._unknown_entities:\n        matcher = (lambda i: ((i == test_id) or i.startswith(('%s.' % test_id))))\n        known_ids = filter(matcher, self._tests)\n        for id_ in known_ids:\n            if (self._tests[id_]['status'] == 'init'):\n                self._tests[id_]['status'] = self._unknown_entities[test_id]['status']\n            if self._unknown_entities[test_id].get('reason'):\n                self._tests[id_]['reason'] = self._unknown_entities[test_id]['reason']\n            elif self._unknown_entities[test_id].get('traceback'):\n                self._tests[id_]['traceback'] = self._unknown_entities[test_id]['traceback']\n    for test_id in self._expected_failures:\n        if self._tests.get(test_id):\n            if (self._tests[test_id]['status'] == 'fail'):\n                self._tests[test_id]['status'] = 'xfail'\n                if self._expected_failures[test_id]:\n                    self._tests[test_id]['reason'] = self._expected_failures[test_id]\n            elif (self._tests[test_id]['status'] == 'success'):\n                self._tests[test_id]['status'] = 'uxsuccess'\n    for test_id in self._tests:\n        for file_name in ['traceback', 'reason']:\n            if (file_name in self._tests[test_id]):\n                self._tests[test_id][file_name] = encodeutils.safe_decode(self._tests[test_id][file_name])\n    self._is_parsed = True\n", "label": 1}
{"function": "\n\ndef build_context(context):\n    'get_context method of doc or module is supposed to render content templates and push it into context'\n    context = frappe._dict(context)\n    if (not ('url_prefix' in context)):\n        context.url_prefix = ''\n    context.update(get_website_settings())\n    context.update((frappe.local.conf.get('website_context') or {\n        \n    }))\n    if context.doc:\n        context.update(context.doc.as_dict())\n        context.update(context.doc.website)\n        if hasattr(context.doc, 'get_context'):\n            ret = context.doc.get_context(context)\n            if ret:\n                context.update(ret)\n        for prop in ('no_cache', 'no_sitemap'):\n            if (not (prop in context)):\n                context[prop] = getattr(context.doc, prop, False)\n    elif context.controller:\n        module = frappe.get_module(context.controller)\n        if module:\n            for prop in ('base_template_path', 'template', 'no_cache', 'no_sitemap', 'condition_field'):\n                if hasattr(module, prop):\n                    context[prop] = getattr(module, prop)\n            if hasattr(module, 'get_context'):\n                ret = module.get_context(context)\n                if ret:\n                    context.update(ret)\n            if hasattr(module, 'get_children'):\n                context.children = module.get_children(context)\n    if context.show_sidebar:\n        add_sidebar_data(context)\n    add_metatags(context)\n    if (not context.base_template_path):\n        app_base = frappe.get_hooks('base_template')\n        context.base_template_path = (app_base[0] if app_base else 'templates/base.html')\n    return context\n", "label": 1}
{"function": "\n\ndef initialize(self, taskParent, override=None):\n    \"This method initializes a task for (re)use.  taskParent is the\\n        object instance of the parent task, or a 'task environment' (something\\n        that runs tasks).\\n        If subclass overrides this method, it should call the superclass\\n        method at some point.\\n\\n        - Copy shared data from taskParent, overriding items from _override_\\n          if they are present there ('contagion' of task values).\\n        - Generate a unique tag, to be used with the Gen2 Monitor.\\n        - Clear done event, initialize times and result.\\n        \"\n    if (taskParent and hasattr(taskParent, 'shares')):\n        for var in taskParent.shares:\n            if (override and (var in override)):\n                self.__dict__[var] = override[var]\n            else:\n                self.__dict__[var] = taskParent.__dict__[var]\n    else:\n        pass\n    if (not self.tag):\n        try:\n            self.tag = ((str(taskParent) + '.') + self.tagger.get_tag(self))\n        except:\n            self.tag = get_tag(taskParent)\n    self.ev_done.clear()\n    self.starttime = time.time()\n    self.endtime = 0\n    self.totaltime = 0\n    self.result = None\n    return self.tag\n", "label": 0}
{"function": "\n\ndef _restore(self, obj):\n    if has_tag(obj, tags.B64):\n        restore = self._restore_base64\n    elif has_tag(obj, tags.BYTES):\n        restore = self._restore_quopri\n    elif has_tag(obj, tags.ID):\n        restore = self._restore_id\n    elif has_tag(obj, tags.REF):\n        restore = self._restore_ref\n    elif has_tag(obj, tags.ITERATOR):\n        restore = self._restore_iterator\n    elif has_tag(obj, tags.TYPE):\n        restore = self._restore_type\n    elif has_tag(obj, tags.REPR):\n        restore = self._restore_repr\n    elif has_tag(obj, tags.REDUCE):\n        restore = self._restore_reduce\n    elif has_tag(obj, tags.OBJECT):\n        restore = self._restore_object\n    elif has_tag(obj, tags.FUNCTION):\n        restore = self._restore_function\n    elif util.is_list(obj):\n        restore = self._restore_list\n    elif has_tag(obj, tags.TUPLE):\n        restore = self._restore_tuple\n    elif has_tag(obj, tags.SET):\n        restore = self._restore_set\n    elif util.is_dictionary(obj):\n        restore = self._restore_dict\n    else:\n        restore = (lambda x: x)\n    return restore(obj)\n", "label": 1}
{"function": "\n\ndef _hostmaskPatternEqual(pattern, hostmask):\n    try:\n        return (_patternCache[pattern](hostmask) is not None)\n    except KeyError:\n        fd = minisix.io.StringIO()\n        for c in pattern:\n            if (c == '*'):\n                fd.write('.*')\n            elif (c == '?'):\n                fd.write('.')\n            elif (c in '[{'):\n                fd.write('[[{]')\n            elif (c in '}]'):\n                fd.write('[}\\\\]]')\n            elif (c in '|\\\\'):\n                fd.write('[|\\\\\\\\]')\n            elif (c in '^~'):\n                fd.write('[~^]')\n            else:\n                fd.write(re.escape(c))\n        fd.write('$')\n        f = re.compile(fd.getvalue(), re.I).match\n        _patternCache[pattern] = f\n        return (f(hostmask) is not None)\n", "label": 0}
{"function": "\n\ndef arg_opts(self, name, default, taken_names):\n    opts = {\n        \n    }\n    opts['positional'] = (name in self.positional)\n    opts['optional'] = (name in self.optional)\n    if ('_' in name):\n        opts['attr_name'] = name\n        name = translate_underscores(name)\n    names = [name]\n    if self.auto_shortflags:\n        for char in name:\n            if (not ((char == name) or (char in taken_names))):\n                names.append(char)\n                break\n    opts['names'] = names\n    if (default not in (None, NO_DEFAULT)):\n        opts['kind'] = type(default)\n        opts['default'] = default\n    if (name in self.help):\n        opts['help'] = self.help[name]\n    return opts\n", "label": 0}
{"function": "\n\ndef __init__(self, config_files=None, refresh=False, private=False, config_key=None, config_defaults=None, cloud=None):\n    if (config_files is None):\n        config_files = []\n    config = os_client_config.config.OpenStackConfig(config_files=(os_client_config.config.CONFIG_FILES + config_files))\n    self.extra_config = config.get_extra_config(config_key, config_defaults)\n    if (cloud is None):\n        self.clouds = [shade.OpenStackCloud(cloud_config=cloud_config) for cloud_config in config.get_all_clouds()]\n    else:\n        try:\n            self.clouds = [shade.OpenStackCloud(cloud_config=config.get_one_cloud(cloud))]\n        except os_client_config.exceptions.OpenStackConfigException as e:\n            raise shade.OpenStackCloudException(e)\n    if private:\n        for cloud in self.clouds:\n            cloud.private = True\n    if refresh:\n        for cloud in self.clouds:\n            cloud._cache.invalidate()\n", "label": 0}
{"function": "\n\ndef __init__(self, uri, **config):\n    self.uri = uri\n    prefix = config.pop('prefix', '')\n    log.info(\"Connecting MongoEngine to '%s'.\", _safe_uri_replace.sub('\\\\1://\\\\2@', uri))\n    connection = self.connection = dict(tz_aware=True)\n    (scheme, parts) = uri.split('://', 1)\n    (parts, self.db) = parts.split('/', 1)\n    (auth, host) = (parts.split('@', 1) if ('@' in parts) else (None, parts))\n    if (scheme != 'mongodb'):\n        raise Exception(\"The URL must begin with 'mongodb://'!\")\n    (connection['host'], connection['port']) = (host.split(':') if (':' in host) else (host, '27017'))\n    connection['port'] = int(connection['port'])\n    if auth:\n        (connection['username'], _, connection['password']) = auth.partition(':')\n    for (k, v) in items(config):\n        (pfx, _, k) = k.rpartition('.')\n        if ((pfx != prefix) or (k in ('engine', 'model', 'ready'))):\n            continue\n        connection[k] = (int(v) if v.isdigit() else v)\n    self.cb = config.get('ready', None)\n", "label": 0}
{"function": "\n\ndef checkArguments(self, category, arguments, allowedAttributes, isInput, isOutput):\n    for argumentInformation in arguments:\n        helpInfo = (self.name, 'arguments', category)\n        attributes = argumentAttributes()\n        (self.success, attributes) = methods.checkAttributes(argumentInformation, allowedAttributes, attributes, self.allowTermination, helpInfo)\n        if (attributes.longFormArgument in self.arguments):\n            if self.allowTermination:\n                self.errors.repeatedLongFormArgument(helpInfo, attributes.longFormArgument)\n            else:\n                self.success = False\n                return\n        if (attributes.shortFormArgument in self.shortFormArguments):\n            if self.allowTermination:\n                self.errors.repeatedShortFormArgument(helpInfo, attributes.longFormArgument, attributes.shortFormArgument)\n            else:\n                self.success = False\n                return\n        attributes.category = category\n        if isInput:\n            attributes.isInput = True\n        elif isOutput:\n            attributes.isOutput = True\n        self.arguments[attributes.longFormArgument] = attributes\n        self.shortFormArguments.append(attributes.shortFormArgument)\n", "label": 0}
{"function": "\n\ndef val(self, env):\n    'Returns the value of this variable'\n    if self.name:\n        return env[self.name]\n    if (self.constuction[0] == 'Gen+'):\n        gather = [v.val(env) for v in self.constuction[1:]]\n        Sum = None\n        for v in gather:\n            if (Sum is None):\n                Sum = v\n            else:\n                Sum = (v + Sum)\n        return Sum\n    if (self.constuction[0] == 'Gen*'):\n        base = self.constuction[1].val(env)\n        exps = [v.val(env) for v in self.constuction[2:]]\n        Prod = 1\n        for v in exps:\n            Prod = (v * Prod)\n        return (Prod * base)\n    raise Exception('Unknown case')\n", "label": 0}
{"function": "\n\ndef add_data(self, group, name, data):\n    if self.isgroup(data):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'group',\n            'class': data.__class__.__name__,\n        })\n        for comp in dir(data):\n            self.add_data(g, comp, getattr(data, comp))\n    elif isinstance(data, (list, tuple)):\n        dtype = 'list'\n        if isinstance(data, tuple):\n            dtype = 'tuple'\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': dtype,\n        })\n        for (ix, comp) in enumerate(data):\n            iname = ('item%i' % ix)\n            self.add_data(g, iname, comp)\n    elif isinstance(data, dict):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'dict',\n        })\n        for (key, val) in data.items():\n            self.add_data(g, key, val)\n    elif isParameter(data):\n        g = self.add_h5group(group, name, attrs={\n            'larchtype': 'parameter',\n        })\n        self.add_h5dataset(g, 'json', data.asjson())\n    else:\n        d = self.add_h5dataset(group, name, data)\n", "label": 0}
{"function": "\n\n@patch('nefertari.view.BaseView._run_init_actions')\ndef test_init(self, run):\n    request = Mock(content_type='application/json', json={\n        'param1.foo': 'val1',\n        'param3': 'val3',\n    }, method='POST', accept=[''])\n    request.params.mixed.return_value = {\n        'param2.foo': 'val2',\n    }\n    view = DummyBaseView(context={\n        'foo': 'bar',\n    }, request=request)\n    run.assert_called_once_with()\n    assert (request.override_renderer == 'nefertari_json')\n    assert (list(sorted(view._params.keys())) == ['param1', 'param2', 'param3'])\n    assert (view._params['param1'] == {\n        'foo': 'val1',\n    })\n    assert (view._params['param2'] == {\n        'foo': 'val2',\n    })\n    assert (view._params['param3'] == 'val3')\n    assert (view.request == request)\n    assert (view.context == {\n        'foo': 'bar',\n    })\n    assert (view._before_calls == {\n        \n    })\n    assert (view._after_calls == {\n        \n    })\n", "label": 1}
{"function": "\n\ndef __init__(self, *fields, **attributes):\n    if self.abstract:\n        raise TypeError('abstract nodes are not instanciable')\n    if fields:\n        if (len(fields) != len(self.fields)):\n            if (not self.fields):\n                raise TypeError(('%r takes 0 arguments' % self.__class__.__name__))\n            raise TypeError(('%r takes 0 or %d argument%s' % (self.__class__.__name__, len(self.fields), (((len(self.fields) != 1) and 's') or ''))))\n        for (name, arg) in izip(self.fields, fields):\n            setattr(self, name, arg)\n    for attr in self.attributes:\n        setattr(self, attr, attributes.pop(attr, None))\n    if attributes:\n        raise TypeError(('unknown attribute %r' % iter(attributes).next()))\n", "label": 1}
{"function": "\n\ndef listdir(self, path, start_time=None, end_time=None):\n    '\\n        Get an iterable with S3 folder contents.\\n        Iterable contains paths relative to queried path.\\n\\n        :param start_time: Optional argument to copy files with modified dates after start_time\\n        :param end_time: Optional argument to copy files with modified dates before end_time\\n        '\n    (bucket, key) = self._path_to_bucket_and_key(path)\n    s3_bucket = self.s3.get_bucket(bucket, validate=True)\n    key_path = self._add_path_delimiter(key)\n    key_path_len = len(key_path)\n    for item in s3_bucket.list(prefix=key_path):\n        last_modified_date = time.strptime(item.last_modified, '%Y-%m-%dT%H:%M:%S.%fZ')\n        if (((not start_time) and (not end_time)) or (start_time and (not end_time) and (start_time < last_modified_date)) or ((not start_time) and end_time and (last_modified_date < end_time)) or (start_time and end_time and (start_time < last_modified_date < end_time))):\n            (yield (self._add_path_delimiter(path) + item.key[key_path_len:]))\n", "label": 1}
{"function": "\n\ndef Parse(self, cmd, args, stdout, stderr, return_val, time_taken, knowledge_base):\n    'Parse the dpkg output.'\n    _ = (stderr, time_taken, args, knowledge_base)\n    self.CheckReturn(cmd, return_val)\n    column_lengths = []\n    i = 0\n    for (i, line) in enumerate(stdout.splitlines()):\n        if line.startswith('+++-'):\n            for col in line.split('-')[1:]:\n                if (not re.match('=*', col)):\n                    raise parsers.ParseError(('Invalid header parsing for %s at line %s' % (cmd, i)))\n                column_lengths.append(len(col))\n            break\n    if column_lengths:\n        remaining_lines = stdout.splitlines()[(i + 1):]\n        for (i, line) in enumerate(remaining_lines):\n            cols = line.split(None, len(column_lengths))\n            (status, name, version, arch, desc) = cols\n            if (status[1] == 'i'):\n                status = rdf_client.SoftwarePackage.InstallState.INSTALLED\n            else:\n                status = rdf_client.SoftwarePackage.InstallState.UNKNOWN\n            (yield rdf_client.SoftwarePackage(name=name, description=desc, version=version, architecture=arch, install_state=status))\n", "label": 0}
{"function": "\n\ndef main(args=None):\n    'Run the main command-line interface for beets. Includes top-level\\n    exception handlers that print friendly error messages.\\n    '\n    try:\n        _raw_main(args)\n    except UserError as exc:\n        message = (exc.args[0] if exc.args else None)\n        log.error('error: {0}', message)\n        sys.exit(1)\n    except util.HumanReadableException as exc:\n        exc.log(log)\n        sys.exit(1)\n    except library.FileOperationError as exc:\n        log.debug('{}', traceback.format_exc())\n        log.error('{}', exc)\n        sys.exit(1)\n    except confit.ConfigError as exc:\n        log.error('configuration error: {0}', exc)\n        sys.exit(1)\n    except db_query.InvalidQueryError as exc:\n        log.error('invalid query: {0}', exc)\n        sys.exit(1)\n    except IOError as exc:\n        if (exc.errno == errno.EPIPE):\n            pass\n        else:\n            raise\n    except KeyboardInterrupt:\n        log.debug('{}', traceback.format_exc())\n", "label": 1}
{"function": "\n\ndef test_graph_equality_attributes(self):\n    '\\n        Graph equality test. This one checks node equality. \\n        '\n    gr = graph()\n    gr.add_nodes([0, 1, 2])\n    gr.add_edge((0, 1))\n    gr.add_node_attribute(1, ('a', 'x'))\n    gr.add_node_attribute(2, ('b', 'y'))\n    gr.add_edge_attribute((0, 1), ('c', 'z'))\n    gr2 = deepcopy(gr)\n    gr3 = deepcopy(gr)\n    gr3.del_edge((0, 1))\n    gr3.add_edge((0, 1))\n    gr4 = deepcopy(gr)\n    gr4.del_edge((0, 1))\n    gr4.add_edge((0, 1))\n    gr4.add_edge_attribute((0, 1), ('d', 'k'))\n    gr5 = deepcopy(gr)\n    gr5.del_node(2)\n    gr5.add_node(2)\n    gr5.add_node_attribute(0, ('d', 'k'))\n    assert (gr == gr2)\n    assert (gr2 == gr)\n    assert (gr != gr3)\n    assert (gr3 != gr)\n    assert (gr != gr4)\n    assert (gr4 != gr)\n    assert (gr != gr5)\n    assert (gr5 != gr)\n", "label": 0}
{"function": "\n\ndef get_function_signature(function, method=True):\n    signature = inspect.getargspec(function)\n    defaults = signature.defaults\n    if method:\n        args = signature.args[1:]\n    else:\n        args = signature.args\n    if defaults:\n        kwargs = zip(args[(- len(defaults)):], defaults)\n        args = args[:(- len(defaults))]\n    else:\n        kwargs = []\n    st = ('%s.%s(' % (function.__module__, function.__name__))\n    for a in args:\n        st += (str(a) + ', ')\n    for (a, v) in kwargs:\n        if (type(v) == str):\n            v = ((\"'\" + v) + \"'\")\n        st += (((str(a) + '=') + str(v)) + ', ')\n    if (kwargs or args):\n        return (st[:(- 2)] + ')')\n    else:\n        return (st + ')')\n", "label": 0}
{"function": "\n\ndef write(self, data):\n    if (self.paused or ((not self.iAmStreaming) and (not self.outstandingPull))):\n        self._buffer.append(data)\n    elif (self.consumer is not None):\n        assert (not self._buffer), 'Writing fresh data to consumer before my buffer is empty!'\n        bytesSent = self._writeSomeData(data)\n        self.outstandingPull = False\n        if (not (bytesSent == len(data))):\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer.append(data[bytesSent:])\n    if ((self.producer is not None) and self.producerIsStreaming):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (bytesBuffered >= self.bufferSize):\n            self.producer.pauseProducing()\n            self.producerPaused = True\n", "label": 1}
{"function": "\n\ndef check_hbase_cluster_status(self, cluster):\n    job = cluster.jobs['master']\n    if (job.running_tasks_count < 1):\n        job.last_status = Status.ERROR\n        job.last_message = 'No running masters!'\n    else:\n        active = 0\n        for task in job.running_tasks.itervalues():\n            if self.is_master_active(task):\n                cluster.entry = ('%s:%d' % (task.host, task.port))\n                version = self.get_latest_metric(task, 'hadoop:service=HBase,name=Info', 'version')\n                revision = self.get_latest_metric(task, 'hadoop:service=HBase,name=Info', 'revision')\n                cluster.version = ('%s, r%s' % (version, revision))\n                active += 1\n        if (active > 1):\n            job.last_status = Status.ERROR\n            job.last_message = 'Too many active masters!'\n        elif (active < 1):\n            job.last_status = Status.ERROR\n            job.last_message = 'No active masters!'\n        elif (job.running_tasks_count < 2):\n            pass\n    job = cluster.jobs['regionserver']\n    if (job.running_tasks_count < 3):\n        job.last_status = Status.ERROR\n        job.last_message = 'Too few running regionservers!'\n    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])\n", "label": 0}
{"function": "\n\ndef status_done(self, result):\n    if (result is False):\n        self.view.set_status('git-status-index', '')\n        self.view.set_status('git-status-working', '')\n    else:\n        lines = [line for line in result.splitlines() if re.match('^[ MADRCU?!]{1,2}\\\\s+.*', line)]\n        index = [line[0] for line in lines if (not line[0].isspace())]\n        working = [line[1] for line in lines if (not line[1].isspace())]\n        self.view.set_status('git-status-index', ('index: ' + self.status_string(index)))\n        self.view.set_status('git-status-working', ('working: ' + self.status_string(working)))\n", "label": 0}
{"function": "\n\ndef test_as_coeff_exponent():\n    assert ((3 * (x ** 4)).as_coeff_exponent(x) == (3, 4))\n    assert ((2 * (x ** 3)).as_coeff_exponent(x) == (2, 3))\n    assert ((4 * (x ** 2)).as_coeff_exponent(x) == (4, 2))\n    assert ((6 * (x ** 1)).as_coeff_exponent(x) == (6, 1))\n    assert ((3 * (x ** 0)).as_coeff_exponent(x) == (3, 0))\n    assert ((2 * (x ** 0)).as_coeff_exponent(x) == (2, 0))\n    assert ((1 * (x ** 0)).as_coeff_exponent(x) == (1, 0))\n    assert ((0 * (x ** 0)).as_coeff_exponent(x) == (0, 0))\n    assert (((- 1) * (x ** 0)).as_coeff_exponent(x) == ((- 1), 0))\n    assert (((- 2) * (x ** 0)).as_coeff_exponent(x) == ((- 2), 0))\n    assert (((2 * (x ** 3)) + (pi * (x ** 3))).as_coeff_exponent(x) == ((2 + pi), 3))\n    assert (((x * log(2)) / ((2 * x) + (pi * x))).as_coeff_exponent(x) == ((log(2) / (2 + pi)), 0))\n    D = Derivative\n    f = Function('f')\n    fx = D(f(x), x)\n    assert (fx.as_coeff_exponent(f(x)) == (fx, 0))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 3):\n            if (ftype == TType.STRING):\n                self.column_family = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.super_column = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _process_dependent_arguments(self):\n    'Convert incoming configuration arguments to their\\n        proper form.\\n\\n        Callables are resolved, ORM annotations removed.\\n\\n        '\n    for attr in ('order_by', 'primaryjoin', 'secondaryjoin', 'secondary', '_user_defined_foreign_keys', 'remote_side'):\n        attr_value = getattr(self, attr)\n        if util.callable(attr_value):\n            setattr(self, attr, attr_value())\n    for attr in ('primaryjoin', 'secondaryjoin'):\n        val = getattr(self, attr)\n        if (val is not None):\n            setattr(self, attr, _orm_deannotate(expression._only_column_elements(val, attr)))\n    if ((self.order_by is not False) and (self.order_by is not None)):\n        self.order_by = [expression._only_column_elements(x, 'order_by') for x in util.to_list(self.order_by)]\n    self._user_defined_foreign_keys = util.column_set((expression._only_column_elements(x, 'foreign_keys') for x in util.to_column_set(self._user_defined_foreign_keys)))\n    self.remote_side = util.column_set((expression._only_column_elements(x, 'remote_side') for x in util.to_column_set(self.remote_side)))\n    self.target = self.mapper.mapped_table\n", "label": 1}
{"function": "\n\ndef get_task(self, taskname):\n    task = getattr(self.pavement, taskname, None)\n    if (not task):\n        for finder in self.task_finders:\n            task = finder.get_task(taskname)\n            if task:\n                break\n    if (not task):\n        task = _import_task(taskname)\n    if (not task):\n        all_tasks = self.get_tasks()\n        matches = [t for t in all_tasks if (t.shortname == taskname)]\n        if (len(matches) > 1):\n            matched_names = [t.name for t in matches]\n            raise BuildFailure(('Ambiguous task name %s (%s)' % (taskname, matched_names)))\n        elif matches:\n            task = matches[0]\n    return task\n", "label": 1}
{"function": "\n\ndef test_caching(self):\n    changed = False\n\n    class TestLoader(loaders.BaseLoader):\n\n        def get_source(self, environment, template):\n            return ('foo', None, (lambda : (not changed)))\n    env = Environment(loader=TestLoader(), cache_size=(- 1))\n    tmpl = env.get_template('template')\n    assert (tmpl is env.get_template('template'))\n    changed = True\n    assert (tmpl is not env.get_template('template'))\n    changed = False\n    env = Environment(loader=TestLoader(), cache_size=0)\n    assert (env.get_template('template') is not env.get_template('template'))\n    env = Environment(loader=TestLoader(), cache_size=2)\n    t1 = env.get_template('one')\n    t2 = env.get_template('two')\n    assert (t2 is env.get_template('two'))\n    assert (t1 is env.get_template('one'))\n    t3 = env.get_template('three')\n    assert ('one' in env.cache)\n    assert ('two' not in env.cache)\n    assert ('three' in env.cache)\n", "label": 0}
{"function": "\n\ndef fragment_count(self):\n    table = self.fragmentruntable.payload.fragment_run_entry_table\n    (first_fragment, end_fragment) = (None, None)\n    for (i, fragmentrun) in enumerate(table):\n        if (fragmentrun.discontinuity_indicator is not None):\n            if (fragmentrun.discontinuity_indicator == 0):\n                break\n            elif (fragmentrun.discontinuity_indicator > 0):\n                continue\n        if (first_fragment is None):\n            first_fragment = fragmentrun.first_fragment\n        end_fragment = fragmentrun.first_fragment\n        fragment_duration = (fragmentrun.first_fragment_timestamp + fragmentrun.fragment_duration)\n        if (self.timestamp > fragment_duration):\n            offset = ((self.timestamp - fragment_duration) / fragmentrun.fragment_duration)\n            end_fragment += int(offset)\n    if (first_fragment is None):\n        first_fragment = 1\n    if (end_fragment is None):\n        end_fragment = 1\n    return (first_fragment, end_fragment)\n", "label": 0}
{"function": "\n\ndef __eq__(self, other):\n    if isinstance(other, datetime):\n        other = Date(other)\n    if isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return (self.date == 'infinity')\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) == other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date == other.date.replace(tzinfo=self.tz))\n        return (self.date == other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            return False\n        else:\n            return self.__eq__(Date(other, tz=self.tz))\n", "label": 0}
{"function": "\n\ndef __init__(self, cgroups, kill_process_fn, hardtimelimit, softtimelimit, walltimelimit, pid_to_kill, cores, callbackFn=(lambda reason: None)):\n    super(_TimelimitThread, self).__init__()\n    if (hardtimelimit or softtimelimit):\n        assert (CPUACCT in cgroups)\n    assert (walltimelimit is not None)\n    if cores:\n        self.cpuCount = len(cores)\n    else:\n        try:\n            self.cpuCount = multiprocessing.cpu_count()\n        except NotImplementedError:\n            self.cpuCount = 1\n    self.daemon = True\n    self.cgroups = cgroups\n    self.timelimit = (hardtimelimit or ((((60 * 60) * 24) * 365) * 100))\n    self.softtimelimit = (softtimelimit or ((((60 * 60) * 24) * 365) * 100))\n    self.latestKillTime = (util.read_monotonic_time() + walltimelimit)\n    self.pid_to_kill = pid_to_kill\n    self.callback = callbackFn\n    self.kill_process = kill_process_fn\n    self.finished = threading.Event()\n", "label": 0}
{"function": "\n\ndef doAuthenticate(self, msg):\n    if (not self.authenticate_decoder):\n        self.authenticate_decoder = ircutils.AuthenticateDecoder()\n    self.authenticate_decoder.feed(msg)\n    if (not self.authenticate_decoder.ready):\n        return\n    string = self.authenticate_decoder.get()\n    self.authenticate_decoder = None\n    mechanism = self.sasl_current_mechanism\n    if (mechanism == 'ecdsa-nist256p-challenge'):\n        if (string == b''):\n            self.sendSaslString(self.sasl_username.encode('utf-8'))\n            return\n        try:\n            with open(self.sasl_ecdsa_key) as fd:\n                private_key = SigningKey.from_pem(fd.read())\n            authstring = private_key.sign(base64.b64decode(msg.args[0].encode()))\n            self.sendSaslString(authstring)\n        except (BadDigestError, OSError, ValueError):\n            self.sendMsg(ircmsgs.IrcMsg(command='AUTHENTICATE', args=('*',)))\n            self.tryNextSaslMechanism()\n    elif (mechanism == 'external'):\n        self.sendSaslString(b'')\n    elif (mechanism == 'plain'):\n        authstring = b'\\x00'.join([self.sasl_username.encode('utf-8'), self.sasl_username.encode('utf-8'), self.sasl_password.encode('utf-8')])\n        self.sendSaslString(authstring)\n", "label": 0}
{"function": "\n\ndef render_image(self, rgbobj, dst_x, dst_y):\n    for ax in self.figure.axes:\n        if (not (ax in (self.ax_img, self.ax_util))):\n            if hasattr(ax, 'lines'):\n                for line in ax.lines:\n                    try:\n                        line._transformed_path.invalidate()\n                    except AttributeError:\n                        pass\n    if self.in_axes:\n        self.render_image2(rgbobj, dst_x, dst_y)\n    else:\n        self.render_image1(rgbobj, dst_x, dst_y)\n    self.ax_util.cla()\n    if self.t_['show_pan_position']:\n        self.ax_util.add_line(self.cross1)\n        self.ax_util.add_line(self.cross2)\n    if self.message:\n        self.draw_message(self.message)\n    self.figure.canvas.draw()\n", "label": 0}
{"function": "\n\n@setup_cache\ndef test_stats(cache):\n    for value in range(100):\n        cache[value] = value\n    assert (cache.stats(enable=True) == (0, 0))\n    for value in range(100):\n        cache[value]\n    for value in range(100, 110):\n        cache.get(value)\n    assert (cache.stats(reset=True) == (100, 10))\n    assert (cache.stats(enable=False) == (0, 0))\n    for value in range(100):\n        cache[value]\n    for value in range(100, 110):\n        cache.get(value)\n    assert (cache.stats() == (0, 0))\n    assert (len(cache.check()) == 0)\n", "label": 1}
{"function": "\n\ndef test_path_in_several_ways(self):\n    alice = Node(name='Alice')\n    bob = Node(name='Bob')\n    carol = Node(name='Carol')\n    dave = Node(name='Dave')\n    path = Path(alice, 'LOVES', bob, Relationship(carol, 'HATES', bob), carol, 'KNOWS', dave)\n    assert path.__bool__()\n    assert path.__nonzero__()\n    assert (path[0] == Relationship(alice, 'LOVES', bob))\n    assert (path[1] == Relationship(carol, 'HATES', bob))\n    assert (path[2] == Relationship(carol, 'KNOWS', dave))\n    assert (path[(- 1)] == Relationship(carol, 'KNOWS', dave))\n    assert (path[0:1] == Path(alice, 'LOVES', bob))\n    assert (path[0:2] == Path(alice, 'LOVES', bob, Relationship(carol, 'HATES', bob), carol))\n    try:\n        _ = path[7]\n    except IndexError:\n        assert True\n    else:\n        assert False\n", "label": 1}
{"function": "\n\ndef process_request(self, request):\n    if (':' in request.get_host()):\n        (domain, port) = request.get_host().split(':')\n        if (int(port) not in (80, 443)):\n            domain = request.get_host()\n    else:\n        domain = request.get_host().split(':')[0]\n    domain = domain.lower()\n    cache_key = ('Site:domain:%s' % domain)\n    site = cache.get(cache_key)\n    if site:\n        SITE_ID.value = site\n    else:\n        try:\n            site = Site.objects.get(domain=domain)\n        except Site.DoesNotExist:\n            site = None\n        if (not site):\n            if domain.startswith('www.'):\n                fallback_domain = domain[4:]\n            else:\n                fallback_domain = ('www.' + domain)\n            try:\n                site = Site.objects.get(domain=fallback_domain)\n            except Site.DoesNotExist:\n                site = None\n        if ((not site) and getattr(settings, 'CREATE_SITES_AUTOMATICALLY', True)):\n            site = Site(domain=domain, name=domain)\n            site.save()\n        if site:\n            SITE_ID.value = site.pk\n        else:\n            SITE_ID.value = _default_site_id\n        cache.set(cache_key, SITE_ID.value, (5 * 60))\n", "label": 1}
{"function": "\n\ndef _create_test_db(self, verbosity, autoclobber, keepdb=False):\n    '\\n        Internal implementation - creates the test db tables.\\n        '\n    suffix = self.sql_table_creation_suffix()\n    test_database_name = self._get_test_db_name()\n    qn = self.connection.ops.quote_name\n    with self._nodb_connection.cursor() as cursor:\n        try:\n            cursor.execute(('CREATE DATABASE %s %s' % (qn(test_database_name), suffix)))\n        except Exception as e:\n            if keepdb:\n                return test_database_name\n            sys.stderr.write(('Got an error creating the test database: %s\\n' % e))\n            if (not autoclobber):\n                confirm = input((\"Type 'yes' if you would like to try deleting the test database '%s', or 'no' to cancel: \" % test_database_name))\n            if (autoclobber or (confirm == 'yes')):\n                try:\n                    if (verbosity >= 1):\n                        print(('Destroying old test database for alias %s...' % (self._get_database_display_str(verbosity, test_database_name),)))\n                    cursor.execute(('DROP DATABASE %s' % qn(test_database_name)))\n                    cursor.execute(('CREATE DATABASE %s %s' % (qn(test_database_name), suffix)))\n                except Exception as e:\n                    sys.stderr.write(('Got an error recreating the test database: %s\\n' % e))\n                    sys.exit(2)\n            else:\n                print('Tests cancelled.')\n                sys.exit(1)\n    return test_database_name\n", "label": 0}
{"function": "\n\ndef get_variable_name(self, abbreviated_xpath):\n    '\\n        If the abbreviated_xpath has been renamed in\\n        self.variable_names_json return that new name, otherwise\\n        return the original abbreviated_xpath.\\n        '\n    if (not hasattr(self, '_keys')):\n        self._keys = self.get_keys()\n    if (not hasattr(self, '_headers')):\n        self._headers = self.get_headers()\n    assert (abbreviated_xpath in self._keys), abbreviated_xpath\n    i = self._keys.index(abbreviated_xpath)\n    header = self._headers[i]\n    if (not hasattr(self, '_variable_names')):\n        self._variable_names = ColumnRename.get_dict()\n        assert (type(self._variable_names) == dict)\n    if ((header in self._variable_names) and self._variable_names[header]):\n        return self._variable_names[header]\n    return header\n", "label": 0}
{"function": "\n\ndef _update_document_fields_positional(self, doc, fields, spec, updater, subdocument=None):\n    'Implements the $set behavior on an existing document'\n    for (k, v) in iteritems(fields):\n        if ('$' in k):\n            field_name_parts = k.split('.')\n            if (not subdocument):\n                current_doc = doc\n                subspec = spec\n                for part in field_name_parts[:(- 1)]:\n                    if (part == '$'):\n                        subspec = subspec.get('$elemMatch', subspec)\n                        for item in current_doc:\n                            if filter_applies(subspec, item):\n                                current_doc = item\n                                break\n                        continue\n                    new_spec = {\n                        \n                    }\n                    for el in subspec:\n                        if el.startswith(part):\n                            if (len(el.split('.')) > 1):\n                                new_spec['.'.join(el.split('.')[1:])] = subspec[el]\n                            else:\n                                new_spec = subspec[el]\n                    subspec = new_spec\n                    current_doc = current_doc[part]\n                subdocument = current_doc\n                if ((field_name_parts[(- 1)] == '$') and isinstance(subdocument, list)):\n                    for (i, doc) in enumerate(subdocument):\n                        if filter_applies(subspec, doc):\n                            subdocument[i] = v\n                            break\n                    continue\n            updater(subdocument, field_name_parts[(- 1)], v)\n            continue\n        self._update_document_single_field(doc, k, v, updater)\n    return subdocument\n", "label": 1}
{"function": "\n\ndef data(self):\n    if (not hasattr(self, '_data')):\n        request = URLRequest(self.url)\n        if (self.env and self.env.cache):\n            headers = self.env.cache.get(('url', 'headers', self.url))\n            if headers:\n                (etag, lmod) = headers\n                if etag:\n                    request.add_header('If-None-Match', etag)\n                if lmod:\n                    request.add_header('If-Modified-Since', lmod)\n        try:\n            response = urlopen(request)\n        except HTTPError as e:\n            if (e.code != 304):\n                raise\n            self._data = self.env.cache.get(('url', 'contents', self.url))\n        else:\n            with contextlib.closing(response):\n                self._data = response.read()\n            if (self.env and self.env.cache):\n                self.env.cache.set(('url', 'headers', self.url), (response.headers.get('ETag'), response.headers.get('Last-Modified')))\n                self.env.cache.set(('url', 'contents', self.url), self._data)\n    return self._data\n", "label": 1}
{"function": "\n\ndef on_query_context(self, view, key, operator, operand, match_all):\n\n    def test(a):\n        if (operator == sublime.OP_EQUAL):\n            return (a == operand)\n        if (operator == sublime.OP_NOT_EQUAL):\n            return (a != operand)\n        return False\n    if (key == 'i_search_active'):\n        return test((isearch_info_for(view) is not None))\n    if (key == 'sbp_has_visible_mark'):\n        if (not SettingsManager.get('sbp_cancel_mark_enabled', False)):\n            return False\n        return (CmdUtil(view).state.mark_ring.has_visible_mark() == operand)\n    if (key == 'sbp_use_alt_bindings'):\n        return test(SettingsManager.get('sbp_use_alt_bindings'))\n    if (key == 'sbp_use_super_bindings'):\n        return test(SettingsManager.get('sbp_use_super_bindings'))\n    if (key == 'sbp_alt+digit_inserts'):\n        return test((SettingsManager.get('sbp_alt+digit_inserts') or (not SettingsManager.get('sbp_use_alt_bindings'))))\n    if (key == 'sbp_has_prefix_argument'):\n        return test(CmdUtil(view).has_prefix_arg())\n", "label": 0}
{"function": "\n\ndef call_for_each_element(data, function, args=[], data_type='sequential'):\n    if (data_type == 'plain'):\n        return function(data, *args)\n    elif (data_type == 'sequential'):\n        assert list_of_lists(data)\n        return [function(d, *args) for d in data]\n    elif (data_type == 'token'):\n        assert (type(data) == dict)\n    return {token: function(contexts, *args) for (token, contexts) in data.items()}\n", "label": 0}
{"function": "\n\ndef get_recent(thing1=None, thing2=None):\n    ctx = context.get_context()\n    if (thing1 is None):\n        return [k for k in ctx.recent.keys()]\n    if (thing2 is None):\n        if isinstance(ctx.recent[thing1], (dict, cache.Cache)):\n            return dict([(repr(k), list(v)) for (k, v) in ctx.recent[thing1].items()])\n        else:\n            return list(ctx.recent[thing1])\n    elif isinstance(ctx.recent[thing1], (dict, cache.Cache)):\n        return dict([(repr(k), list(v)) for (k, v) in ctx.recent[thing1].items() if (thing2 in str(k))])\n    else:\n        return (('{\"error\":\"' + thing1) + ' recent data is not dict.\"}')\n", "label": 0}
{"function": "\n\ndef read(self, expressions):\n    data = {\n        \n    }\n    for i in expressions.split('|'):\n        (name, type) = i.split(':')\n        if (type == 'byte'):\n            data[name] = self.read_byte()\n        if (type == 'ubyte'):\n            data[name] = self.read_ubyte()\n        if (type == 'short'):\n            data[name] = self.read_short()\n        if (type == 'ushort'):\n            data[name] = self.read_ushort()\n        if (type == 'double'):\n            data[name] = self.read_double()\n        if (type == 'position'):\n            data[name] = self.read_position()\n    return data\n", "label": 0}
{"function": "\n\ndef update(self, keys):\n    '\\n        Update arrow position.\\n        '\n    if self.allow_input:\n        if (keys[pg.K_DOWN] and (not keys[pg.K_UP]) and (self.index == 0)):\n            self.index = 1\n            self.allow_input = False\n            self.notify(c.CLICK)\n        elif (keys[pg.K_UP] and (not keys[pg.K_DOWN]) and (self.index == 1)):\n            self.index = 0\n            self.allow_input = False\n            self.notify(c.CLICK)\n        self.rect.y = self.pos_list[self.index]\n    if ((not keys[pg.K_DOWN]) and (not keys[pg.K_UP])):\n        self.allow_input = True\n", "label": 1}
{"function": "\n\ndef heuristic_search(graph, start, goal, heuristic):\n    '\\n    A* search algorithm.\\n    \\n    A set of heuristics is available under C{graph.algorithms.heuristics}. User-created heuristics\\n    are allowed too.\\n    \\n    @type graph: graph, digraph\\n    @param graph: Graph\\n    \\n    @type start: node\\n    @param start: Start node\\n    \\n    @type goal: node\\n    @param goal: Goal node\\n    \\n    @type heuristic: function\\n    @param heuristic: Heuristic function\\n    \\n    @rtype: list\\n    @return: Optimized path from start to goal node \\n    '\n    queue = [(0, start, 0, None)]\n    g = {\n        \n    }\n    explored = {\n        \n    }\n    while queue:\n        (_, current, dist, parent) = heappop(queue)\n        if (current == goal):\n            path = ([current] + [n for n in _reconstruct_path(parent, explored)])\n            path.reverse()\n            return path\n        if (current in explored):\n            continue\n        explored[current] = parent\n        for neighbor in graph[current]:\n            if (neighbor in explored):\n                continue\n            ncost = (dist + graph.edge_weight((current, neighbor)))\n            if (neighbor in g):\n                (qcost, h) = g[neighbor]\n                if (qcost <= ncost):\n                    continue\n            else:\n                h = heuristic(neighbor, goal)\n            g[neighbor] = (ncost, h)\n            heappush(queue, ((ncost + h), neighbor, ncost, current))\n    raise NodeUnreachable(start, goal)\n", "label": 0}
{"function": "\n\ndef parse_test(self, node):\n    token = next(self.stream)\n    if self.stream.current.test('name:not'):\n        next(self.stream)\n        negated = True\n    else:\n        negated = False\n    name = self.stream.expect('name').value\n    while (self.stream.current.type == 'dot'):\n        next(self.stream)\n        name += ('.' + self.stream.expect('name').value)\n    dyn_args = dyn_kwargs = None\n    kwargs = []\n    if (self.stream.current.type == 'lparen'):\n        (args, kwargs, dyn_args, dyn_kwargs) = self.parse_call(None)\n    elif ((self.stream.current.type in ('name', 'string', 'integer', 'float', 'lparen', 'lbracket', 'lbrace')) and (not self.stream.current.test_any('name:else', 'name:or', 'name:and'))):\n        if self.stream.current.test('name:is'):\n            self.fail('You cannot chain multiple tests with is')\n        args = [self.parse_expression()]\n    else:\n        args = []\n    node = nodes.Test(node, name, args, kwargs, dyn_args, dyn_kwargs, lineno=token.lineno)\n    if negated:\n        node = nodes.Not(node, lineno=token.lineno)\n    return node\n", "label": 0}
{"function": "\n\ndef get_field_parameters(self):\n    params = {\n        \n    }\n    if self.nullable:\n        params['null'] = True\n    if ((self.field_class is ForeignKeyField) or (self.name != self.db_column)):\n        params['db_column'] = (\"'%s'\" % self.db_column)\n    if (self.primary_key and (self.field_class is not PrimaryKeyField)):\n        params['primary_key'] = True\n    if self.is_foreign_key():\n        params['rel_model'] = self.rel_model\n        if self.to_field:\n            params['to_field'] = (\"'%s'\" % self.to_field)\n        if self.related_name:\n            params['related_name'] = (\"'%s'\" % self.related_name)\n    if (not self.is_primary_key()):\n        if self.unique:\n            params['unique'] = 'True'\n        elif (self.index and (not self.is_foreign_key())):\n            params['index'] = 'True'\n    return params\n", "label": 1}
{"function": "\n\ndef rectangle(self, rect=None, duration=1.0, block=True, color=(1, 1, 1, 1), parent=None, depth=0):\n    'Draw a single-colored rectangle.'\n    if (duration == 0):\n        block = False\n    l = rect[0]\n    r = rect[1]\n    t = rect[2]\n    b = rect[3]\n    obj = self._engine.direct.gui.OnscreenImage.OnscreenImage(image='blank.tga', pos=(((l + r) / 2), depth, ((b + t) / 2)), scale=(((r - l) / 2), 1, ((b - t) / 2)), color=color, parent=parent)\n    self._to_destroy.append(obj)\n    obj.setTransparency(self._engine.pandac.TransparencyAttrib.MAlpha)\n    if self.implicit_markers:\n        self.marker(250)\n    if block:\n        if ((type(duration) == list) or (type(duration) == tuple)):\n            self.sleep(duration[0])\n            self.waitfor(duration[1])\n        elif (type(duration) == str):\n            self.waitfor(duration)\n        else:\n            self.sleep(duration)\n        self._destroy_object(obj, 251)\n    else:\n        if (duration > 0):\n            self._engine.base.taskMgr.doMethodLater(duration, self._destroy_object, 'ConvenienceFunctions, remove_rect', extraArgs=[obj, 251])\n        return obj\n", "label": 0}
{"function": "\n\ndef get_field_value(self, field_name, field_type, value):\n    if (value is None):\n        return None\n    values_map = self._import_config.get_value_mapping(field_name)\n    if isinstance(value, list):\n        return [self.get_field_value(field_name, field_type, v) for v in value]\n    if field_type.startswith('user'):\n        return self._to_yt_user(value)\n    if (field_type.lower() == 'date'):\n        return self.to_unix_date(value)\n    if isinstance(value, basestring):\n        return values_map.get(value, value)\n    if isinstance(value, int):\n        return values_map.get(value, str(value))\n", "label": 0}
{"function": "\n\ndef test_postgres_search_path_parsing(self):\n    url = 'postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn?currentSchema=otherschema'\n    url = dj_database_url.parse(url)\n    assert (url['ENGINE'] == 'django.db.backends.postgresql_psycopg2')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 5431)\n    assert (url['OPTIONS']['options'] == '-c search_path=otherschema')\n", "label": 0}
{"function": "\n\ndef build_suite(self, test_labels=None, extra_tests=None, **kwargs):\n    suite = TestSuite()\n    test_labels = (test_labels or ['.'])\n    extra_tests = (extra_tests or [])\n    discover_kwargs = {\n        \n    }\n    if (self.pattern is not None):\n        discover_kwargs['pattern'] = self.pattern\n    if (self.top_level is not None):\n        discover_kwargs['top_level_dir'] = self.top_level\n    for label in test_labels:\n        kwargs = discover_kwargs.copy()\n        tests = None\n        label_as_path = os.path.abspath(label)\n        if (not os.path.exists(label_as_path)):\n            tests = self.test_loader.loadTestsFromName(label)\n        elif (os.path.isdir(label_as_path) and (not self.top_level)):\n            top_level = label_as_path\n            while True:\n                init_py = os.path.join(top_level, '__init__.py')\n                if os.path.exists(init_py):\n                    try_next = os.path.dirname(top_level)\n                    if (try_next == top_level):\n                        break\n                    top_level = try_next\n                    continue\n                break\n            kwargs['top_level_dir'] = top_level\n        if (not (tests and tests.countTestCases())):\n            tests = self.test_loader.discover(start_dir=label, **kwargs)\n            self.test_loader._top_level_dir = None\n        suite.addTests(tests)\n    for test in extra_tests:\n        suite.addTest(test)\n    return reorder_suite(suite, self.reorder_by)\n", "label": 1}
{"function": "\n\ndef mpf_exp(x, prec, rnd=round_fast):\n    (sign, man, exp, bc) = x\n    if man:\n        mag = (bc + exp)\n        wp = (prec + 14)\n        if sign:\n            man = (- man)\n        if ((prec > 600) and (exp >= 0)):\n            e = mpf_e((wp + int((1.45 * mag))))\n            return mpf_pow_int(e, (man << exp), prec, rnd)\n        if (mag < (- wp)):\n            return mpf_perturb(fone, sign, prec, rnd)\n        if (mag > 1):\n            wpmod = (wp + mag)\n            offset = (exp + wpmod)\n            if (offset >= 0):\n                t = (man << offset)\n            else:\n                t = (man >> (- offset))\n            lg2 = ln2_fixed(wpmod)\n            (n, t) = divmod(t, lg2)\n            n = int(n)\n            t >>= mag\n        else:\n            offset = (exp + wp)\n            if (offset >= 0):\n                t = (man << offset)\n            else:\n                t = (man >> (- offset))\n            n = 0\n        man = exp_basecase(t, wp)\n        return from_man_exp(man, (n - wp), prec, rnd)\n    if (not exp):\n        return fone\n    if (x == fninf):\n        return fzero\n    return x\n", "label": 1}
{"function": "\n\ndef AddAll(self, rdf_values, callback=None):\n    'Adds a list of rdfvalues to the collection.'\n    for rdf_value in rdf_values:\n        if (rdf_value is None):\n            raise ValueError(\"Can't add None to the collection via AddAll.\")\n        if (self._rdf_type and (not isinstance(rdf_value, self._rdf_type))):\n            raise ValueError(('This collection only accepts values of type %s' % self._rdf_type.__name__))\n        if (not rdf_value.age):\n            rdf_value.age.Now()\n    buf = cStringIO.StringIO()\n    for (index, rdf_value) in enumerate(rdf_values):\n        data = rdf_protodict.EmbeddedRDFValue(payload=rdf_value).SerializeToString()\n        buf.write(struct.pack('<i', len(data)))\n        buf.write(data)\n        self.size += 1\n        if callback:\n            callback(index, rdf_value)\n    self.fd.Seek(0, 2)\n    self.fd.Write(buf.getvalue())\n    self.stream_dirty = True\n", "label": 0}
{"function": "\n\n@frappe.whitelist()\ndef get_permissions(doctype=None, role=None):\n    frappe.only_for('System Manager')\n    out = frappe.db.sql(('select * from tabDocPerm\\n\\t\\twhere %s%s order by parent, permlevel, role' % (((doctype and (\" parent='%s'\" % frappe.db.escape(doctype))) or ''), ((role and (((doctype and ' and ') or '') + (\" role='%s'\" % frappe.db.escape(role)))) or ''))), as_dict=True)\n    linked_doctypes = {\n        \n    }\n    for d in out:\n        d.linked_doctypes = linked_doctypes.setdefault(d.parent, get_linked_doctypes(d.parent))\n    return out\n", "label": 0}
{"function": "\n\ndef login(request, template=None, next_url=None):\n    feedback = ''\n    action_url = '.'\n    if ('user' in request.session.keys()):\n        if next_url:\n            return HttpResponseRedirect(next_url)\n        else:\n            return HttpResponseRedirect('/')\n    try:\n        if request.REQUEST['next_url']:\n            next_url = request.REQUEST['next_url']\n            action_url = ('%s?next_url=%s' % (action_url, next_url))\n    except:\n        pass\n    if (request.method == 'POST'):\n        form = LoginForm(request.POST)\n        if form.is_valid():\n            email = form.cleaned_data['email']\n            password = form.cleaned_data['password']\n            query = db.Query(User)\n            user = query.filter('email =', email).get()\n            if user:\n                success = user.login(email=email, password=password, request=request)\n                if success:\n                    if (next_url is not None):\n                        return HttpResponseRedirect(next_url)\n                    else:\n                        return HttpResponseRedirect('/')\n                else:\n                    feedback = 'Bad username and password.  Please try again.'\n            else:\n                feedback = 'Bad username and password.  Please try again.'\n    else:\n        form = LoginForm()\n    c = RequestContext(request, {\n        'form': form,\n        'feedback': feedback,\n        'action_url': action_url,\n    })\n    if (template is None):\n        template = 'login.html'\n    return render_to_response(template, c)\n", "label": 1}
{"function": "\n\ndef inet_ntop(af, packed_ip):\n    'Convert an packed IP address of the given family to string format.'\n    if (af == AF_INET):\n        return inet_ntoa(packed_ip)\n    elif (af == AF_INET6):\n        if ((len(packed_ip) != 16) or (not _is_str(packed_ip))):\n            raise ValueError('invalid length of packed IP address string')\n        tokens = [('%x' % i) for i in _unpack('>8H', packed_ip)]\n        words = list(_unpack('>8H', packed_ip))\n        int_val = 0\n        for (i, num) in enumerate(reversed(words)):\n            word = num\n            word = (word << (16 * i))\n            int_val = (int_val | word)\n        if ((65535 < int_val <= 4294967295) or ((int_val >> 32) == 65535)):\n            packed_ipv4 = _pack('>2H', *[int(i, 16) for i in tokens[(- 2):]])\n            ipv4_str = inet_ntoa(packed_ipv4)\n            tokens = (tokens[0:(- 2)] + [ipv4_str])\n        return ':'.join(_compact_ipv6_tokens(tokens))\n    else:\n        raise ValueError(('unknown address family %d' % af))\n", "label": 1}
{"function": "\n\ndef _setupTypeInfo(self, test, maskPattern):\n    data = ((self.errorCorrectLevel << 3) | maskPattern)\n    bits = QRUtil.getBCHTypeInfo(data)\n    for i in range(15):\n        mod = ((not test) and (((bits >> i) & 1) == 1))\n        if (i < 6):\n            self.modules[i][8] = mod\n        elif (i < 8):\n            self.modules[(i + 1)][8] = mod\n        else:\n            self.modules[((self.moduleCount - 15) + i)][8] = mod\n    for i in range(15):\n        mod = ((not test) and (((bits >> i) & 1) == 1))\n        if (i < 8):\n            self.modules[8][((self.moduleCount - i) - 1)] = mod\n        elif (i < 9):\n            self.modules[8][(((15 - i) - 1) + 1)] = mod\n        else:\n            self.modules[8][((15 - i) - 1)] = mod\n    self.modules[(self.moduleCount - 8)][8] = (not test)\n", "label": 0}
{"function": "\n\ndef sync_ldap_groups(self, ldap_groups):\n    '\\n        Synchronize LDAP groups with local group database.\\n        '\n    attributes = getattr(settings, 'LDAP_SYNC_GROUP_ATTRIBUTES', None)\n    groupname_field = 'name'\n    if (groupname_field not in attributes.values()):\n        error_msg = (\"LDAP_SYNC_GROUP_ATTRIBUTES must contain the group name field '%s'\" % groupname_field)\n        raise ImproperlyConfigured(error_msg)\n    for (cname, attrs) in ldap_groups:\n        try:\n            items = attrs.items()\n        except AttributeError:\n            continue\n        group_attr = {\n            \n        }\n        for (name, attr) in items:\n            group_attr[attributes[name]] = attr[0].decode('utf-8')\n        try:\n            groupname = group_attr[groupname_field]\n            group_attr[groupname_field] = groupname.lower()\n        except KeyError:\n            logger.warning((\"Group is missing a required attribute '%s'\" % groupname_field))\n            continue\n        kwargs = {\n            (groupname_field + '__iexact'): groupname,\n            'defaults': group_attr,\n        }\n        try:\n            (group, created) = Group.objects.get_or_create(**kwargs)\n        except IntegrityError as e:\n            logger.error(('Error creating group %s' % e))\n        else:\n            if created:\n                logger.debug(('Created group %s' % groupname))\n    logger.info('Groups are synchronized')\n", "label": 0}
{"function": "\n\ndef test_apply(self):\n    '\\n        Tests marking migrations as applied/unapplied.\\n        '\n    recorder = MigrationRecorder(connection)\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), set())\n    recorder.record_applied('myapp', '0432_ponies')\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), {('myapp', '0432_ponies')})\n    recorder_other = MigrationRecorder(connections['other'])\n    self.assertEqual(set(((x, y) for (x, y) in recorder_other.applied_migrations() if (x == 'myapp'))), set())\n    recorder.record_unapplied('myapp', '0432_ponies')\n    self.assertEqual(set(((x, y) for (x, y) in recorder.applied_migrations() if (x == 'myapp'))), set())\n", "label": 0}
{"function": "\n\ndef _camel_case_to_underscores(self, text):\n    result = []\n    pos = 0\n    while (pos < len(text)):\n        if text[pos].isupper():\n            if ((((pos - 1) > 0) and text[(pos - 1)].islower()) or (((pos - 1) > 0) and ((pos + 1) < len(text)) and text[(pos + 1)].islower())):\n                result.append(('_%s' % text[pos].lower()))\n            else:\n                result.append(text[pos].lower())\n        else:\n            result.append(text[pos])\n        pos += 1\n    return ''.join(result)\n", "label": 0}
{"function": "\n\ndef gpx_fields_to_xml(instance, tag, version, custom_attributes=None):\n    fields = instance.gpx_10_fields\n    if (version == '1.1'):\n        fields = instance.gpx_11_fields\n    tag_open = bool(tag)\n    body = ''\n    if tag:\n        body = ('\\n<' + tag)\n        if custom_attributes:\n            for (key, value) in custom_attributes.items():\n                body += (' %s=\"%s\"' % (key, mod_utils.make_str(value)))\n    for gpx_field in fields:\n        if isinstance(gpx_field, str):\n            if tag_open:\n                body += '>'\n                tag_open = False\n            if (gpx_field[0] == '/'):\n                body += ('<%s>' % gpx_field)\n            else:\n                body += ('\\n<%s' % gpx_field)\n                tag_open = True\n        else:\n            value = getattr(instance, gpx_field.name)\n            if gpx_field.attribute:\n                body += (' ' + gpx_field.to_xml(value, version))\n            elif value:\n                if tag_open:\n                    body += '>'\n                    tag_open = False\n                xml_value = gpx_field.to_xml(value, version)\n                if xml_value:\n                    body += xml_value\n    if tag:\n        if tag_open:\n            body += '>'\n        body += (('</' + tag) + '>')\n    return body\n", "label": 1}
{"function": "\n\ndef load_directives(self, inherits):\n    \"there has got to be a better way\\n           plugin pattern away!!!\\n           \\n           #1 this could be a comprehension but it'd be hard\\n              to read.\\n           #2 maybe we should put a dummy sys.modules in place\\n              while doing this to limit junk in the name space.\\n        \"\n    m = self.__class__.__module__\n    exclude = ['__init__.py']\n    relpath = sys.modules[m].__file__\n    reldir = os.path.split(relpath)[0]\n    fulldir = os.path.realpath(reldir)\n    pyfiles = []\n    for f in os.listdir(fulldir):\n        ff = os.path.join(fulldir, f)\n        if os.path.isfile(ff):\n            pyfiles.append(f)\n    pyfiles = [f for f in pyfiles if f.endswith('.py')]\n    pyfiles = [f for f in pyfiles if (f not in exclude)]\n    pyfiles = [f[:(- 3)] for f in pyfiles]\n    candidates = []\n    for py in pyfiles:\n        m = __import__(py, globals(), locals(), 'romeo.directives')\n        for i in dir(m):\n            attr = getattr(m, i)\n            if (not (type(attr) == types.TypeType)):\n                continue\n            matches = [cls for cls in inherits if issubclass(attr, cls) if (attr.__name__ != cls.__name__)]\n            if (len(matches) == len(inherits)):\n                candidates.append(attr)\n    for c in candidates:\n        cinst = c(c.name, self.root, *c.init_args(self), **c.init_kwargs(self))\n        self.directives.append(cinst)\n", "label": 1}
{"function": "\n\ndef filter(self, iterable, prereleases=None):\n    if (prereleases is None):\n        prereleases = self.prereleases\n    if self._specs:\n        for spec in self._specs:\n            iterable = spec.filter(iterable, prereleases=bool(prereleases))\n        return iterable\n    else:\n        filtered = []\n        found_prereleases = []\n        for item in iterable:\n            if (not isinstance(item, (LegacyVersion, Version))):\n                parsed_version = parse(item)\n            else:\n                parsed_version = item\n            if isinstance(parsed_version, LegacyVersion):\n                continue\n            if (parsed_version.is_prerelease and (not prereleases)):\n                if (not filtered):\n                    found_prereleases.append(item)\n            else:\n                filtered.append(item)\n        if ((not filtered) and found_prereleases and (prereleases is None)):\n            return found_prereleases\n        return filtered\n", "label": 1}
{"function": "\n\ndef validate(self, object, name, value):\n    ' Validates that the value is a valid font descriptor string.\\n        '\n    try:\n        point_size = family = style = weight = underline = ''\n        facename = ['']\n        for word in value.split():\n            lword = word.lower()\n            if (lword in font_families):\n                family = (' ' + lword)\n            elif (lword in font_styles):\n                style = (' ' + lword)\n            elif (lword in font_weights):\n                weight = (' ' + lword)\n            elif (lword == 'underline'):\n                underline = (' ' + lword)\n            elif (lword not in font_noise):\n                try:\n                    int(lword)\n                    point_size = (lword + ' pt')\n                except:\n                    facename.append(word)\n        fontstr = ('%s%s%s%s%s%s' % (point_size, family, style, weight, underline, ' '.join(facename))).strip()\n        return fontstr\n    except Exception:\n        pass\n    raise TraitError(object, name, 'a font descriptor string', repr(value))\n", "label": 0}
{"function": "\n\ndef _update_no_improvement_count(self, early_stopping, X_val, y_val):\n    if early_stopping:\n        self.validation_scores_.append(self.score(X_val, y_val))\n        if self.verbose:\n            print(('Validation score: %f' % self.validation_scores_[(- 1)]))\n        last_valid_score = self.validation_scores_[(- 1)]\n        if (last_valid_score < (self.best_validation_score_ + self.tol)):\n            self._no_improvement_count += 1\n        else:\n            self._no_improvement_count = 0\n        if (last_valid_score > self.best_validation_score_):\n            self.best_validation_score_ = last_valid_score\n            self._best_coefs = [c.copy() for c in self.coefs_]\n            self._best_intercepts = [i.copy() for i in self.intercepts_]\n    else:\n        if (self.loss_curve_[(- 1)] > (self.best_loss_ - self.tol)):\n            self._no_improvement_count += 1\n        else:\n            self._no_improvement_count = 0\n        if (self.loss_curve_[(- 1)] < self.best_loss_):\n            self.best_loss_ = self.loss_curve_[(- 1)]\n", "label": 0}
{"function": "\n\ndef read_events(self, timeout=None):\n    timeout_ms = 2147483647\n    if (timeout is not None):\n        timeout_ms = int((timeout * 1000))\n        if ((timeout_ms < 0) or (timeout_ms >= 2147483647)):\n            raise ValueError('Timeout value out of range')\n    try:\n        events = []\n        (rc, num, key, _) = win32file.GetQueuedCompletionStatus(self.__cphandle, timeout_ms)\n        if (rc == 0):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled and (not watch._removed)):\n                    events.extend(process_events(watch, num))\n        elif (rc == 5):\n            with self.__lock:\n                watch = self.__key_to_watch.get(key)\n                if ((watch is not None) and watch.enabled):\n                    close_watch(watch)\n                    del self.__key_to_watch[key]\n                    events.append(FSEvent(watch, FSEvent.DeleteSelf))\n        return events\n    except pywintypes.error as e:\n        raise FSMonitorWindowsError(*e.args)\n", "label": 1}
{"function": "\n\ndef on_status(self, status):\n    if (status.in_reply_to_status_id == tweetID):\n        parsedNumbers = ''.join(status.text.split(' ')[1:]).replace(' ', '').replace('[', '').replace(']', '').replace('(', '').replace(')', '').split(',')\n        givenNumbers = list(map(int, parsedNumbers))\n        areSorted = True\n        if (len(givenNumbers) != len(numbers)):\n            areSorted = False\n        for n in givenNumbers:\n            if (givenNumbers.count(n) != numbers.count(n)):\n                areSorted = False\n                break\n        for i in range(len(givenNumbers)):\n            if (i > 0):\n                if (not (givenNumbers[i] >= givenNumbers[(i - 1)])):\n                    areSorted = False\n                    break\n        if areSorted:\n            print(givenNumbers)\n            api.update_status((('@' + status.author.screen_name) + ' Awesome! Thanks!'), in_reply_to_status_id=status.id)\n            return False\n        else:\n            api.update_status((('@' + status.author.screen_name) + \" Those numbers aren't sorted!\"), in_reply_to_status_id=status.id)\n            return True\n    else:\n        return True\n", "label": 0}
{"function": "\n\ndef reset(self, **traits):\n    'Creates the dataset afresh or resets existing data source.'\n    self.set(trait_change_notify=False, **traits)\n    points = self.points\n    scalars = self.scalars\n    (x, y, z) = (self.x, self.y, self.z)\n    points = np.c_[(x.ravel(), y.ravel(), z.ravel())].ravel()\n    points.shape = ((- 1), 3)\n    self.set(points=points, trait_change_notify=False)\n    triangles = self.triangles\n    assert (triangles.shape[1] == 3), 'The shape of the triangles array must be (X, 3)'\n    assert (triangles.max() < len(points)), 'The triangles indices must be smaller that the number of points'\n    assert (triangles.min() >= 0), 'The triangles indices must be positive or null'\n    if (self.dataset is None):\n        pd = tvtk.PolyData()\n    else:\n        pd = self.dataset\n    pd.set(points=points)\n    pd.set(polys=triangles)\n    if ((not ('scalars' in traits)) and (scalars is not None) and (scalars.shape != x.shape)):\n        scalars = z\n    if ((scalars is not None) and (len(scalars) > 0)):\n        if (not scalars.flags.contiguous):\n            scalars = scalars.copy()\n            self.set(scalars=scalars, trait_change_notify=False)\n        assert (x.shape == scalars.shape)\n        pd.point_data.scalars = scalars.ravel()\n        pd.point_data.scalars.name = 'scalars'\n    self.dataset = pd\n", "label": 1}
{"function": "\n\n@login_required\ndef editor(request, id):\n    '\\n    Display the article editor.\\n    '\n    article = (get_object_or_404(Article, pk=id) if id else None)\n    if (article and (not article.can_edit(request))):\n        raise Http404\n    if (request.method == 'POST'):\n        form = EditorForm(instance=article, data=request.POST)\n        if form.is_valid():\n            creating = (article is None)\n            article = form.save(commit=False)\n            if creating:\n                article.author = request.user\n            article.save()\n            messages.info(request, ('The article has been saved.' if creating else 'Your changes to the article have been saved.'))\n            if (('action' in request.POST) and (request.POST['action'] == 'continue')):\n                return redirect('articles:editor', article.id)\n            else:\n                return redirect(article)\n    else:\n        form = EditorForm(instance=article)\n    return render(request, 'articles/editor.html', {\n        'title': (('Edit \"%s\"' % article) if article else 'New Article'),\n        'form': form,\n        'description': ('Use the form below to %s.' % ('edit the article' if article else 'create an article')),\n    })\n", "label": 1}
{"function": "\n\ndef _update_an_article(self, postid):\n    (afile, aname) = self.conf.get_article(postid, self.args.type)\n    if self.args.output:\n        self._write_html_file(afile)\n        return\n    (html, meta, txt, medias) = self._get_and_update_article_content(afile)\n    if (not html):\n        return\n    resultclass = WordPressPost\n    if (self.args.type == 'page'):\n        postid = meta.postid\n        resultclass = WordPressPage\n    post = self.wpcall(GetPost(postid, result_class=resultclass))\n    if (not post):\n        slog.warning(('No post \"%s\"!' % postid))\n        return\n    slog.info('Old article:')\n    self.print_results(post)\n    post.title = meta.title\n    post.user = meta.author\n    post.slug = meta.nicename\n    post.date = meta.date\n    post.content = html\n    post.post_status = meta.poststatus\n    if meta.modified:\n        post.date_modified = meta.modified\n    terms = self.cache.get_terms_from_meta(meta.category, meta.tags)\n    if terms:\n        post.terms = terms\n    elif (self.args.type == 'post'):\n        slog.warning('Please provide some terms.')\n        return\n    succ = self.wpcall(EditPost(postid, post))\n    if (succ == None):\n        return\n    if succ:\n        slog.info(('Update %s successfully!' % postid))\n    else:\n        slog.info(('Update %s fail!' % postid))\n", "label": 1}
{"function": "\n\ndef _filter_requirements(lines_iter, filter_names=None, filter_sys_version=False):\n    for line in lines_iter:\n        line = line.strip()\n        if ((not line) or line.startswith('#')):\n            continue\n        match = REQ_PATTERN.match(line)\n        if (match is None):\n            raise AssertionError((\"Could not parse requirement: '%s'\" % line))\n        name = match.group('name')\n        if ((filter_names is not None) and (name not in filter_names)):\n            continue\n        if (filter_sys_version and match.group('pyspec')):\n            (pycomp, pyspec) = match.group('pycomp', 'pyspec')\n            comp = STR_TO_CMP[pycomp]\n            pyver_spec = StrictVersion(pyspec)\n            if comp(SYS_VERSION, pyver_spec):\n                (yield line.split(';')[0])\n            continue\n        (yield line)\n", "label": 1}
{"function": "\n\ndef parse(self, node):\n    '\\n        Get tag name, indentantion and correct attributes of a node according\\n        to its class\\n        '\n    node_class_name = node.__class__.__name__\n    spec = self.rst_terms[node_class_name]\n    tag_name = (spec[0] or node_class_name)\n    use_name_in_class = ((len(spec) > 3) and spec[3])\n    indent = (spec[4] if (len(spec) > 4) else True)\n    if use_name_in_class:\n        node['classes'].insert(0, node_class_name)\n    attributes = {\n        \n    }\n    replacements = {\n        'refuri': 'href',\n        'uri': 'src',\n        'refid': 'href',\n        'morerows': 'rowspan',\n        'morecols': 'colspan',\n        'classes': 'class',\n        'ids': 'id',\n    }\n    ignores = ('names', 'dupnames', 'bullet', 'enumtype', 'colwidth', 'stub', 'backrefs', 'auto', 'anonymous')\n    for (k, v) in node.attributes.items():\n        if ((k in ignores) or (not v)):\n            continue\n        if (k in replacements):\n            k = replacements[k]\n        if isinstance(v, list):\n            v = ' '.join(v)\n        attributes[k] = v\n    if getattr(self, 'next_elem_attr', None):\n        attributes.update(self.next_elem_attr)\n        del self.next_elem_attr\n    return (tag_name, indent, attributes)\n", "label": 1}
{"function": "\n\ndef find_referenced_templates(ast):\n    'Finds all the referenced templates from the AST.  This will return an\\n    iterator over all the hardcoded template extensions, inclusions and\\n    imports.  If dynamic inheritance or inclusion is used, `None` will be\\n    yielded.\\n\\n    >>> from jinja2 import Environment, meta\\n    >>> env = Environment()\\n    >>> ast = env.parse(\\'{% extends \"layout.html\" %}{% include helper %}\\')\\n    >>> list(meta.find_referenced_templates(ast))\\n    [\\'layout.html\\', None]\\n\\n    This function is useful for dependency tracking.  For example if you want\\n    to rebuild parts of the website after a layout template has changed.\\n    '\n    for node in ast.find_all((nodes.Extends, nodes.FromImport, nodes.Import, nodes.Include)):\n        if (not isinstance(node.template, nodes.Const)):\n            if isinstance(node.template, (nodes.Tuple, nodes.List)):\n                for template_name in node.template.items:\n                    if isinstance(template_name, nodes.Const):\n                        if isinstance(template_name.value, string_types):\n                            (yield template_name.value)\n                    else:\n                        (yield None)\n            else:\n                (yield None)\n            continue\n        if isinstance(node.template.value, string_types):\n            (yield node.template.value)\n        elif (isinstance(node, nodes.Include) and isinstance(node.template.value, (tuple, list))):\n            for template_name in node.template.value:\n                if isinstance(template_name, string_types):\n                    (yield template_name)\n        else:\n            (yield None)\n", "label": 1}
{"function": "\n\ndef test_mysql_database_url_with_sslca_options(self):\n    os.environ['DATABASE_URL'] = 'mysql://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:3306/d8r82722r2kuvn?ssl-ca=rds-combined-ca-bundle.pem'\n    url = dj_database_url.config()\n    assert (url['ENGINE'] == 'django.db.backends.mysql')\n    assert (url['NAME'] == 'd8r82722r2kuvn')\n    assert (url['HOST'] == 'ec2-107-21-253-135.compute-1.amazonaws.com')\n    assert (url['USER'] == 'uf07k1i6d8ia0v')\n    assert (url['PASSWORD'] == 'wegauwhgeuioweg')\n    assert (url['PORT'] == 3306)\n    assert (url['OPTIONS'] == {\n        'ssl': {\n            'ca': 'rds-combined-ca-bundle.pem',\n        },\n    })\n    os.environ['DATABASE_URL'] = 'mysql://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:3306/d8r82722r2kuvn?'\n    url = dj_database_url.config()\n    assert ('OPTIONS' not in url)\n", "label": 0}
{"function": "\n\ndef parse_data(infile):\n    'Parse data from `infile`.'\n    blocks = re.compile(' '.join(([('=' * 9)] * 8)))\n    dashes = re.compile('^-{79}$')\n    title = re.compile('^Timings for (.*)$')\n    row = re.compile((' '.join((['(.{9})'] * 7)) + ' (.{8,9})'))\n    lines = infile.readlines()\n    data = co.OrderedDict()\n    index = 0\n    while (index < len(lines)):\n        line = lines[index]\n        if blocks.match(line):\n            try:\n                name = title.match(lines[(index + 1)]).group(1)\n            except:\n                index += 1\n                continue\n            data[name] = {\n                \n            }\n            assert dashes.match(lines[(index + 2)])\n            cols = parse_row(row, lines[(index + 3)])\n            assert blocks.match(lines[(index + 4)])\n            get_row = parse_row(row, lines[(index + 5)])\n            assert (get_row[0] == 'get')\n            set_row = parse_row(row, lines[(index + 6)])\n            assert (set_row[0] == 'set')\n            delete_row = parse_row(row, lines[(index + 7)])\n            assert (delete_row[0] == 'delete')\n            assert blocks.match(lines[(index + 9)])\n            data[name]['get'] = dict(zip(cols, get_row))\n            data[name]['set'] = dict(zip(cols, set_row))\n            data[name]['delete'] = dict(zip(cols, delete_row))\n            index += 10\n        else:\n            index += 1\n    return data\n", "label": 1}
{"function": "\n\ndef pp_options_list(keys, width=80, _print=False):\n    ' Builds a concise listing of available options, grouped by prefix '\n    from textwrap import wrap\n    from itertools import groupby\n\n    def pp(name, ks):\n        pfx = ((('- ' + name) + '.[') if name else '')\n        ls = wrap(', '.join(ks), width, initial_indent=pfx, subsequent_indent='  ', break_long_words=False)\n        if (ls and ls[(- 1)] and name):\n            ls[(- 1)] = (ls[(- 1)] + ']')\n        return ls\n    ls = []\n    singles = [x for x in sorted(keys) if (x.find('.') < 0)]\n    if singles:\n        ls += pp('', singles)\n    keys = [x for x in keys if (x.find('.') >= 0)]\n    for (k, g) in groupby(sorted(keys), (lambda x: x[:x.rfind('.')])):\n        ks = [x[(len(k) + 1):] for x in list(g)]\n        ls += pp(k, ks)\n    s = '\\n'.join(ls)\n    if _print:\n        print(s)\n    else:\n        return s\n", "label": 0}
{"function": "\n\ndef _add_container_actions(self, container):\n    title_group_map = {\n        \n    }\n    for group in self._action_groups:\n        if (group.title in title_group_map):\n            msg = _('cannot merge actions - two groups are named %r')\n            raise ValueError((msg % group.title))\n        title_group_map[group.title] = group\n    group_map = {\n        \n    }\n    for group in container._action_groups:\n        if (group.title not in title_group_map):\n            title_group_map[group.title] = self.add_argument_group(title=group.title, description=group.description, conflict_handler=group.conflict_handler)\n        for action in group._group_actions:\n            group_map[action] = title_group_map[group.title]\n    for group in container._mutually_exclusive_groups:\n        mutex_group = self.add_mutually_exclusive_group(required=group.required)\n        for action in group._group_actions:\n            group_map[action] = mutex_group\n    for action in container._actions:\n        group_map.get(action, self)._add_action(action)\n", "label": 0}
{"function": "\n\ndef __init__(self, op_tree, ad):\n    '\\n        op_tree: the op_tree at this grad_node\\n        ad: the autodiff object with global op_tree, next_error and dicts\\n        '\n    assert (op_tree is not None)\n    self.op_tree = op_tree\n    self.grad_op_tree = None\n    self.ad = ad\n    self.left = None\n    self.right = None\n    if isinstance(op_tree, Tensor):\n        if (op_tree._original_base not in ad.map_tensor_grad_node):\n            ad.map_tensor_grad_node[op_tree._original_base] = self\n    elif (type(op_tree) == OpTreeNode):\n        if (op_tree[1] is not None):\n            if (isinstance(op_tree[1], Tensor) and (op_tree[1]._original_base in ad.map_tensor_grad_node)):\n                self.left = ad.map_tensor_grad_node[op_tree[1]._original_base]\n            else:\n                self.left = GradNode(op_tree[1], ad)\n        if (op_tree[2] is not None):\n            if (isinstance(op_tree[2], Tensor) and (op_tree[2]._original_base in ad.map_tensor_grad_node)):\n                self.right = ad.map_tensor_grad_node[op_tree[2]._original_base]\n            else:\n                self.right = GradNode(op_tree[2], ad)\n", "label": 1}
{"function": "\n\ndef test_04_ore_init_with_statement(self):\n    s = Ore_Sword_Statement(ORE_TEST_STATEMENT)\n    assert (len(s.states) == 1)\n    assert (len(s.original_deposits) == 1)\n    assert (len(s.resources) == 1)\n    assert (s.xml_document != None)\n    assert (s.dom != None)\n    assert s.parsed\n    assert s.valid\n    (uri, description) = s.states[0]\n    assert (uri == 'http://purl.org/net/sword/terms/state/Testing')\n    assert (description == 'The work has passed through review and is now in the archive')\n    t = datetime.strptime('2011-03-02T20:50:06Z', '%Y-%m-%dT%H:%M:%SZ')\n    entry = s.resources[0]\n    assert (len(entry.packaging) == 1)\n    assert (entry.deposited_by == 'sword')\n    assert (entry.deposited_on_behalf_of == 'jbloggs')\n    assert (entry.deposited_on == t)\n    assert (entry.uri == 'http://localhost:8080/part-IRI/43/my_deposit/example.zip')\n    assert (entry.packaging[0] == 'http://purl.org/net/sword/package/SimpleZip')\n", "label": 1}
{"function": "\n\n@requires_admin\ndef post(self, project_id):\n    project = Project.get(project_id)\n    if (project is None):\n        return ('', 404)\n    args = self.post_parser.parse_args()\n    if args.name:\n        project.name = args.name\n    if args.slug:\n        match = Project.query.filter((Project.slug == args.slug), (Project.id != project.id)).first()\n        if match:\n            return (('{\"error\": \"Project with slug %r already exists\"}' % (args.slug,)), 400)\n        project.slug = args.slug\n    if args.repository:\n        repository = Repository.get(args.repository)\n        if (repository is None):\n            return (('{\"error\": \"Repository with url %r does not exist\"}' % (args.repository,)), 400)\n        project.repository = repository\n    if (args.status == 'inactive'):\n        project.status = ProjectStatus.inactive\n    elif (args.status == 'active'):\n        project.status = ProjectStatus.active\n    db.session.add(project)\n    data = self.serialize(project)\n    data['repository'] = self.serialize(project.repository)\n    return self.respond(data, serialize=False)\n", "label": 0}
{"function": "\n\ndef make_node(self, *inputs):\n    assert (self.nout == 1)\n    assert (len(inputs) == 2)\n    _inputs = [gpu_contiguous(as_cuda_ndarray_variable(i)) for i in inputs]\n    if ((self.nin > 0) and (len(_inputs) != self.nin)):\n        raise TypeError('Wrong argument count', (self.nin, len(_inputs)))\n    for i in _inputs[1:]:\n        if (i.type.ndim != inputs[0].type.ndim):\n            raise TypeError('different ranks among inputs')\n    if any([any(i.type.broadcastable) for i in inputs]):\n        raise Exception(\"pycuda don't support broadcasted dimensions\")\n    otype = CudaNdarrayType(broadcastable=([False] * _inputs[0].type.ndim))\n    out_node = Apply(self, _inputs, [otype() for o in xrange(self.nout)])\n    return out_node\n", "label": 1}
{"function": "\n\ndef sufficient_options(self):\n    'Check if all required options are present.\\n\\n        :raises: AuthPluginOptionsMissing\\n        '\n    has_token = (self.opts.get('token') or self.opts.get('auth_token'))\n    no_auth = (has_token and self.opts.get('endpoint'))\n    has_tenant = (self.opts.get('tenant_id') or self.opts.get('tenant_name'))\n    has_credential = (self.opts.get('username') and has_tenant and self.opts.get('password') and self.opts.get('auth_url'))\n    missing = (not (no_auth or has_credential))\n    if missing:\n        missing_opts = []\n        opts = ['token', 'endpoint', 'username', 'password', 'auth_url', 'tenant_id', 'tenant_name']\n        for opt in opts:\n            if (not self.opts.get(opt)):\n                missing_opts.append(opt)\n        raise exceptions.AuthPluginOptionsMissing(missing_opts)\n", "label": 1}
{"function": "\n\ndef test_result_generation_when_one_test_has_two_cases():\n    jobstep = JobStep(id=uuid.uuid4(), project_id=uuid.uuid4(), job_id=uuid.uuid4())\n    fp = StringIO(SAMPLE_XUNIT_DOUBLE_CASES)\n    handler = XunitHandler(jobstep)\n    results = handler.get_tests(fp)\n    assert (len(results) == 2)\n    r1 = results[0]\n    assert (type(r1) is TestResult)\n    assert (r1.step == jobstep)\n    assert (r1.package is None)\n    assert (r1.name == 'test_simple.SampleTest.test_falsehood')\n    assert (r1.duration == 750.0)\n    assert (r1.result == Result.failed)\n    assert (r1.message == 'test_simple.py:8: in test_falsehood\\n    assert False\\nE   AssertionError: assert False\\n\\ntest_simple.py:4: in tearDown\\n    1/0\\nE   ZeroDivisionError: integer division or modulo by zero')\n    assert (r1.reruns == 3)\n    r2 = results[1]\n    assert (type(r2) is TestResult)\n    assert (r2.step == jobstep)\n    assert (r2.package is None)\n    assert (r2.name == 'test_simple.SampleTest.test_truth')\n    assert (r2.duration == 1250.0)\n    assert (r2.result == Result.failed)\n    assert (r2.message == 'test_simple.py:4: in tearDown\\n    1/0\\nE   ZeroDivisionError: integer division or modulo by zero')\n    assert (r2.reruns == 0)\n", "label": 1}
{"function": "\n\ndef optimizeJumps(irdata):\n    instrs = irdata.flat_instructions\n    jump_instrs = [ins for ins in instrs if isinstance(ins, ir.LazyJumpBase)]\n    while 1:\n        done = True\n        (posd, _) = _calcMinimumPositions(instrs)\n        for ins in jump_instrs:\n            if ((ins.min < ins.max) and ins.widenIfNecessary(irdata.labels, posd)):\n                done = False\n        if done:\n            break\n    for ins in jump_instrs:\n        assert (ins.min <= ins.max)\n        ins.max = ins.min\n", "label": 1}
{"function": "\n\ndef handle(self):\n    message_format = '%s     %3d%%     [ %d / %d ]'\n    last_status_message_len = 0\n    status_message = ''\n    message_sent = False\n    self.server.binwalk.status.running = True\n    while True:\n        time.sleep(0.1)\n        try:\n            self.request.send(binwalk.core.compat.str2bytes(('\\x08' * last_status_message_len)))\n            self.request.send(binwalk.core.compat.str2bytes((' ' * last_status_message_len)))\n            self.request.send(binwalk.core.compat.str2bytes(('\\x08' * last_status_message_len)))\n            if self.server.binwalk.status.shutdown:\n                self.server.binwalk.status.finished = True\n                break\n            if (self.server.binwalk.status.total != 0):\n                percentage = ((float(self.server.binwalk.status.completed) / float(self.server.binwalk.status.total)) * 100)\n                status_message = (message_format % (self.server.binwalk.status.fp.path, percentage, self.server.binwalk.status.completed, self.server.binwalk.status.total))\n            elif (not message_sent):\n                status_message = 'No status information available at this time!'\n            else:\n                continue\n            last_status_message_len = len(status_message)\n            self.request.send(binwalk.core.compat.str2bytes(status_message))\n            message_sent = True\n        except IOError as e:\n            if (e.errno == errno.EPIPE):\n                break\n        except Exception as e:\n            binwalk.core.common.debug((('StatusRequestHandler exception: ' + str(e)) + '\\n'))\n        except KeyboardInterrupt as e:\n            raise e\n    self.server.binwalk.status.running = False\n    return\n", "label": 0}
{"function": "\n\ndef _get_constraint_matrix(self, hosts, filter_properties):\n    num_hosts = len(hosts)\n    num_instances = filter_properties.get('num_instances', 1)\n    solver_cache = filter_properties['solver_cache']\n    constraint_matrix = [[True for j in xrange((num_instances + 1))] for i in xrange(num_hosts)]\n    solver_cache['constraint_matrix'] = constraint_matrix\n    constraint_objects = [cons() for cons in self.constraint_classes]\n    constraint_objects.sort(key=(lambda cons: cons.precedence))\n    precedence_level = 0\n    for constraint_object in constraint_objects:\n        if (constraint_object.precedence > precedence_level):\n            solver_cache['constraint_matrix'] = constraint_matrix\n            precedence_level = constraint_object.precedence\n        this_cons_mat = constraint_object.get_constraint_matrix(hosts, filter_properties)\n        if (not this_cons_mat):\n            continue\n        for i in xrange(num_hosts):\n            constraint_matrix[i][1:] = [(constraint_matrix[i][(j + 1)] & this_cons_mat[i][j]) for j in xrange(num_instances)]\n    solver_cache['constraint_matrix'] = constraint_matrix\n    return constraint_matrix\n", "label": 0}
{"function": "\n\ndef clean(self):\n    username = self.cleaned_data.get('username')\n    password = self.cleaned_data.get('password')\n    message = ERROR_MESSAGE\n    if (username and password):\n        self.user_cache = authenticate(username=username, password=password)\n        if (self.user_cache is None):\n            if ('@' in username):\n                try:\n                    user = User.objects.get(email=username)\n                except (User.DoesNotExist, User.MultipleObjectsReturned):\n                    pass\n                else:\n                    if user.check_password(password):\n                        message = (_(\"Your e-mail address is not your username. Try '%s' instead.\") % user.username)\n            raise forms.ValidationError(message)\n        elif ((not self.user_cache.is_active) or (not self.user_cache.is_staff)):\n            raise forms.ValidationError(message)\n    self.check_for_test_cookie()\n    return self.cleaned_data\n", "label": 1}
{"function": "\n\ndef dialect_of(data, **kwargs):\n    ' CSV dialect of a CSV file stored in SSH, HDFS, or a Directory. '\n    keys = set(['delimiter', 'doublequote', 'escapechar', 'lineterminator', 'quotechar', 'quoting', 'skipinitialspace', 'strict', 'has_header'])\n    if isinstance(data, (HDFS(CSV), SSH(CSV))):\n        with sample(data) as fn:\n            d = dialect_of(CSV(fn, **data.dialect))\n    elif isinstance(data, (HDFS(Directory(CSV)), SSH(Directory(CSV)))):\n        with sample(data) as fn:\n            d = dialect_of(CSV(fn, **data.kwargs))\n    elif isinstance(data, Directory(CSV)):\n        d = dialect_of(next(data))\n    else:\n        assert isinstance(data, CSV)\n        with open(data.path, 'r') as f:\n            text = f.read()\n        result = dict()\n        d = sniffer.sniff(text)\n        d = dict(((k, getattr(d, k)) for k in keys if hasattr(d, k)))\n        if (data.has_header is None):\n            d['has_header'] = sniffer.has_header(text)\n        else:\n            d['has_header'] = data.has_header\n        d.update(data.dialect)\n    d.update(kwargs)\n    d = dict(((k, v) for (k, v) in d.items() if (k in keys)))\n    return d\n", "label": 1}
{"function": "\n\ndef test_posts_atom(client, silly_posts):\n    rv = client.get('/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('Probably a &lt;i&gt;dumb&lt;/i&gt; joke' in content)\n    assert ('First interesting article' in content)\n    rv = client.get('/everything/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('mal.colm/reynolds' in content)\n    assert ('First interesting article' in content)\n    rv = client.get('/notes/', query_string={\n        'feed': 'atom',\n    })\n    assert (200 == rv.status_code)\n    assert rv.content_type.startswith('application/atom+xml')\n    content = rv.get_data(as_text=True)\n    assert ('Probably a &lt;i&gt;dumb&lt;/i&gt; joke' in content)\n    assert ('First interesting article' not in content)\n", "label": 1}
{"function": "\n\ndef addFile(self, relPath, fullPath, distname, override=False):\n    fileName = os.path.basename(relPath)\n    fileExtension = os.path.splitext(fileName)[1]\n    if self.__package:\n        fileId = ('%s/' % self.__package)\n    else:\n        fileId = ''\n    if ((fileExtension in classExtensions) and (distname == 'classes')):\n        fileId += os.path.splitext(relPath)[0]\n        construct = jasy.item.Class.ClassItem\n        dist = self.classes\n    elif ((fileExtension in translationExtensions) and (distname == 'translations')):\n        fileId += os.path.splitext(relPath)[0]\n        construct = jasy.item.Translation.TranslationItem\n        dist = self.translations\n    elif (fileName in docFiles):\n        fileId += os.path.dirname(relPath)\n        fileId = fileId.strip('/')\n        construct = jasy.item.Doc.DocItem\n        dist = self.docs\n    else:\n        fileId += relPath\n        construct = jasy.item.Asset.AssetItem\n        dist = self.assets\n    if (construct != jasy.item.Asset.AssetItem):\n        fileId = fileId.replace('/', '.')\n    if ((fileId in dist) and (not override)):\n        raise UserError(('Item ID was registered before: %s' % fileId))\n    item = construct(self, fileId).attach(fullPath)\n    Console.debug(('Registering %s %s' % (item.kind, fileId)))\n    dist[fileId] = item\n", "label": 1}
{"function": "\n\n@utils.synchronized('dhcp-agent')\ndef sync_state(self, networks=None):\n    \"Sync the local DHCP state with Neutron. If no networks are passed,\\n        or 'None' is one of the networks, sync all of the networks.\\n        \"\n    only_nets = set(([] if ((not networks) or (None in networks)) else networks))\n    LOG.info(_LI('Synchronizing state'))\n    pool = eventlet.GreenPool(self.conf.num_sync_threads)\n    known_network_ids = set(self.cache.get_network_ids())\n    try:\n        active_networks = self.plugin_rpc.get_active_networks_info()\n        LOG.info(_LI('All active networks have been fetched through RPC.'))\n        active_network_ids = set((network.id for network in active_networks))\n        for deleted_id in (known_network_ids - active_network_ids):\n            try:\n                self.disable_dhcp_helper(deleted_id)\n            except Exception as e:\n                self.schedule_resync(e, deleted_id)\n                LOG.exception(_LE('Unable to sync network state on deleted network %s'), deleted_id)\n        for network in active_networks:\n            if ((not only_nets) or (network.id not in known_network_ids) or (network.id in only_nets)):\n                pool.spawn(self.safe_configure_dhcp_for_network, network)\n        pool.waitall()\n        LOG.info(_LI('Synchronizing state complete'))\n    except Exception as e:\n        if only_nets:\n            for network_id in only_nets:\n                self.schedule_resync(e, network_id)\n        else:\n            self.schedule_resync(e)\n        LOG.exception(_LE('Unable to sync network state.'))\n", "label": 1}
{"function": "\n\ndef safe_create_instance(username, xml_file, media_files, uuid, request):\n    'Create an instance and catch exceptions.\\n\\n    :returns: A list [error, instance] where error is None if there was no\\n        error.\\n    '\n    error = instance = None\n    try:\n        instance = create_instance(username, xml_file, media_files, uuid=uuid, request=request)\n    except InstanceInvalidUserError:\n        error = OpenRosaResponseBadRequest(_('Username or ID required.'))\n    except InstanceEmptyError:\n        error = OpenRosaResponseBadRequest(_('Received empty submission. No instance was created'))\n    except FormInactiveError:\n        error = OpenRosaResponseNotAllowed(_('Form is not active'))\n    except XForm.DoesNotExist:\n        error = OpenRosaResponseNotFound(_('Form does not exist on this account'))\n    except ExpatError as e:\n        error = OpenRosaResponseBadRequest(_('Improperly formatted XML.'))\n    except DuplicateInstance:\n        response = OpenRosaResponse(_('Duplicate submission'))\n        response.status_code = 202\n        response['Location'] = request.build_absolute_uri(request.path)\n        error = response\n    except PermissionDenied as e:\n        error = OpenRosaResponseForbidden(e)\n    except InstanceMultipleNodeError as e:\n        error = OpenRosaResponseBadRequest(e)\n    except DjangoUnicodeDecodeError:\n        error = OpenRosaResponseBadRequest(_('File likely corrupted during transmission, please try later.'))\n    return [error, instance]\n", "label": 1}
{"function": "\n\ndef _fill_remote(cur, keep_files):\n    'Add references to remote Keep files if present and not local.\\n    '\n    if isinstance(cur, (list, tuple)):\n        return [_fill_remote(x, keep_files) for x in cur]\n    elif isinstance(cur, dict):\n        out = {\n            \n        }\n        for (k, v) in cur.items():\n            out[k] = _fill_remote(v, keep_files)\n        return out\n    elif (isinstance(cur, basestring) and os.path.splitext(cur)[(- 1)] and (not os.path.exists(cur))):\n        for test_keep in keep_files:\n            if test_keep.endswith(cur):\n                return test_keep\n        return cur\n    else:\n        return cur\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kwargs):\n    evaluate = kwargs.get('evaluate', global_evaluate[0])\n    if iterable(args[0]):\n        if (isinstance(args[0], Point) and (not evaluate)):\n            return args[0]\n        args = args[0]\n    coords = Tuple(*args)\n    if any(((a.is_number and im(a)) for a in coords)):\n        raise ValueError('Imaginary coordinates not permitted.')\n    if evaluate:\n        coords = coords.xreplace(dict([(f, simplify(nsimplify(f, rational=True))) for f in coords.atoms(Float)]))\n    if (len(coords) == 2):\n        return Point2D(coords, **kwargs)\n    if (len(coords) == 3):\n        return Point3D(coords, **kwargs)\n    return GeometryEntity.__new__(cls, *coords)\n", "label": 1}
{"function": "\n\ndef __init__(self, path, upload_to=None, preview_w=None, preview_h=None):\n    self.upload_to = upload_to\n    self.preview_width = preview_w\n    self.preview_height = preview_h\n    self.metadata = {\n        \n    }\n    if (not path):\n        self.name = None\n        return\n    if ('%' in path):\n        path = urlunquote_plus(path)\n    if path.startswith(settings.MEDIA_URL):\n        self._path = get_relative_media_url(path, clean_slashes=False)\n    elif re.search('^(?:http(?:s)?:)?//', path):\n        self._path = self.download_image_url(path)\n    else:\n        abs_path = get_media_path(path)\n        if os.path.exists(abs_path):\n            self._path = get_relative_media_url(abs_path)\n    if ((not self._path) or (not os.path.exists(os.path.join(settings.MEDIA_ROOT, self._path)))):\n        self.name = None\n        return\n    super(ImageFile, self).__init__(self._path)\n    if self:\n        self.preview_image = self.get_for_size('preview')\n", "label": 0}
{"function": "\n\ndef _dirichlet_check_input(alpha, x):\n    x = np.asarray(x)\n    if (((x.shape[0] + 1) != alpha.shape[0]) and (x.shape[0] != alpha.shape[0])):\n        raise ValueError((\"Vector 'x' must have either the same number of entries as, or one entry fewer than, parameter vector 'a', but alpha.shape = %s and x.shape = %s.\" % (alpha.shape, x.shape)))\n    if (x.shape[0] != alpha.shape[0]):\n        xk = np.array([(1 - np.sum(x, 0))])\n        if (xk.ndim == 1):\n            x = np.append(x, xk)\n        elif (xk.ndim == 2):\n            x = np.vstack((x, xk))\n        else:\n            raise ValueError('The input must be one dimensional or a two dimensional matrix containing the entries.')\n    if (np.min(x) <= 0):\n        raise ValueError(\"Each entry in 'x' must be greater than zero.\")\n    if (np.max(x) > 1):\n        raise ValueError(\"Each entry in 'x' must be smaller or equal one.\")\n    if (np.abs((np.sum(x, 0) - 1.0)) > 1e-09).any():\n        raise ValueError((\"The input vector 'x' must lie within the normal simplex. but np.sum(x, 0) = %s.\" % np.sum(x, 0)))\n    return x\n", "label": 0}
{"function": "\n\ndef guess_seqtype(s):\n    dna_letters = 'AaTtUuGgCcNn'\n    ndna = 0\n    nU = 0\n    nT = 0\n    for l in s:\n        if (l in dna_letters):\n            ndna += 1\n        if ((l == 'U') or (l == 'u')):\n            nU += 1\n        elif ((l == 'T') or (l == 't')):\n            nT += 1\n    ratio = (ndna / float(len(s)))\n    if (ratio > 0.85):\n        if (nT > nU):\n            return DNA_SEQTYPE\n        else:\n            return RNA_SEQTYPE\n    else:\n        return PROTEIN_SEQTYPE\n", "label": 0}
{"function": "\n\ndef test_docopt_parser_with_opts():\n    help_string = '    Some Tool\\n\\n    Usage: tools [-t] [-i <input>...] [-o <output>] <cmd>\\n\\n    Inputs:\\n        -i, --input <input>    The input\\n\\n    Outputs:\\n        -o, --output <output>  The output\\n\\n    Options:\\n        -t, --test             Some option\\n        -h, --help             Show help\\n\\n    '\n    opts = Options.from_docopt(help_string)\n    assert (len(opts) == 4)\n    assert (opts['input'] is not None)\n    assert (opts['input'].nargs == '*')\n    assert (opts['input'].option_type == TYPE_INPUT)\n    assert (not opts['input'].required)\n    assert (opts['output'] is not None)\n    assert (opts['output'].nargs == 1)\n    assert (opts['output'].option_type == TYPE_OUTPUT)\n    assert (not opts['output'].required)\n    assert (opts['test'] is not None)\n    assert (opts['test'].nargs == 0)\n    assert (not opts['test'].required)\n    assert (opts['test'].option_type == TYPE_OPTION)\n    assert (opts['cmd'] is not None)\n    assert (opts['cmd'].nargs == 1)\n    assert opts['cmd'].required\n    assert (opts['cmd'].option_type == TYPE_OPTION)\n", "label": 1}
{"function": "\n\ndef iter_version_links(html, name):\n    '\\n    Iterate through version links (in order) within HTML.\\n\\n    Filtering out links that don\\'t \"look\" like versions.\\n\\n    Either yields hrefs to be recursively searches or tuples of (name, href)\\n    that match the given name.\\n    '\n    soup = BeautifulSoup(html)\n    for node in soup.findAll('a'):\n        if (node.get('href') is None):\n            continue\n        try:\n            (guessed_name, _) = guess_name_and_version(node.text)\n        except ValueError:\n            href = node['href']\n            for extension in ['.tar.gz', '.zip']:\n                if href.endswith(extension):\n                    (yield (basename(href), href))\n                    break\n            else:\n                if (node.get('rel') == 'download'):\n                    (yield href)\n        else:\n            if (guessed_name.replace('_', '-').lower() != name.replace('_', '-').lower()):\n                continue\n            (yield (node.text, node['href']))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _tensAdd_flatten(args):\n    if (not all((isinstance(x, TensExpr) for x in args))):\n        args1 = []\n        for x in args:\n            if isinstance(x, TensExpr):\n                if isinstance(x, TensAdd):\n                    args1.extend(list(x.args))\n                else:\n                    args1.append(x)\n        args1 = [x for x in args1 if (isinstance(x, TensExpr) and x.coeff)]\n        args2 = [x for x in args if (not isinstance(x, TensExpr))]\n        t1 = TensMul.from_data(Add(*args2), [], [], [])\n        args = ([t1] + args1)\n    a = []\n    for x in args:\n        if isinstance(x, TensAdd):\n            a.extend(list(x.args))\n        else:\n            a.append(x)\n    args = [x for x in a if x.coeff]\n    return args\n", "label": 1}
{"function": "\n\ndef the_local_prediction_is(step, prediction):\n    if (isinstance(world.local_prediction, list) or isinstance(world.local_prediction, tuple)):\n        local_prediction = world.local_prediction[0]\n    elif isinstance(world.local_prediction, dict):\n        local_prediction = world.local_prediction['prediction']\n    else:\n        local_prediction = world.local_prediction\n    try:\n        local_model = world.local_model\n        if (not isinstance(world.local_model, LogisticRegression)):\n            if isinstance(local_model, MultiModel):\n                local_model = local_model.models[0]\n            if local_model.tree.regression:\n                local_prediction = round(float(local_prediction), 4)\n                prediction = round(float(prediction), 4)\n    except AttributeError:\n        local_model = world.local_ensemble.multi_model.models[0]\n        if local_model.tree.regression:\n            local_prediction = round(float(local_prediction), 4)\n            prediction = round(float(prediction), 4)\n    if (local_prediction == prediction):\n        assert True\n    else:\n        assert False, ('found: %s, expected %s' % (local_prediction, prediction))\n", "label": 1}
{"function": "\n\ndef test_updating_state(db, tmpdir):\n    if (not db.startswith('mysql')):\n        db = os.path.join(str(tmpdir), db)\n    jip.db.init(db)\n    j = jip.db.Job()\n    jip.db.save(j)\n    j = jip.db.get(j.id)\n    assert (j is not None)\n    assert (j.create_date is not None)\n    assert (j.start_date is None)\n    assert (j.finish_date is None)\n    assert (j.job_id is None)\n    assert (j.state == jip.db.STATE_HOLD)\n    assert (len(j.pipe_to) == 0)\n    assert (len(j.pipe_from) == 0)\n    date = datetime.datetime.now()\n    j.job_id = 10\n    j.start_date = date\n    j.finish_date = date\n    j.state = jip.db.STATE_DONE\n    jip.db.update_job_states(j)\n    fresh = jip.db.get(j.id)\n    assert (fresh is not None)\n    assert (fresh.job_id == '10')\n    assert (fresh.state == jip.db.STATE_DONE)\n    assert (len(jip.db.get_all()) == 1)\n", "label": 1}
{"function": "\n\ndef test_tweet_ordering():\n    now = datetime.now(timezone.utc)\n    tweet_1 = Tweet('A', now)\n    tweet_2 = Tweet('B', (now + timedelta(hours=1)))\n    tweet_3 = Tweet('C', (now + timedelta(hours=2)))\n    tweet_4 = Tweet('D', (now + timedelta(hours=2)))\n    tweet_5 = Tweet('D', (now + timedelta(hours=2)))\n    source = Source('foo', 'bar')\n    with pytest.raises(TypeError):\n        (tweet_1 < source)\n    with pytest.raises(TypeError):\n        (tweet_1 <= source)\n    with pytest.raises(TypeError):\n        (tweet_1 > source)\n    with pytest.raises(TypeError):\n        (tweet_1 >= source)\n    assert (tweet_1 != source)\n    assert (tweet_1 < tweet_2)\n    assert (tweet_1 <= tweet_2)\n    assert (tweet_2 > tweet_1)\n    assert (tweet_2 >= tweet_1)\n    assert (tweet_3 != tweet_4)\n    assert (tweet_5 == tweet_4)\n    assert (tweet_5 >= tweet_4)\n    assert (tweet_5 <= tweet_4)\n    assert (not (tweet_3 <= tweet_4))\n    assert (not (tweet_3 >= tweet_4))\n", "label": 1}
{"function": "\n\ndef _on_headers(self, data):\n    try:\n        data = native_str(data.decode('latin1'))\n        eol = data.find('\\r\\n')\n        start_line = data[:eol]\n        try:\n            (method, uri, version) = start_line.split(' ')\n        except ValueError:\n            raise _BadRequestException('Malformed HTTP request line')\n        if (not version.startswith('HTTP/')):\n            raise _BadRequestException('Malformed HTTP version in HTTP Request-Line')\n        try:\n            headers = httputil.HTTPHeaders.parse(data[eol:])\n        except ValueError:\n            raise _BadRequestException('Malformed HTTP headers')\n        if (self.address_family in (socket.AF_INET, socket.AF_INET6)):\n            remote_ip = self.address[0]\n        else:\n            remote_ip = '0.0.0.0'\n        self._request = HTTPRequest(connection=self, method=method, uri=uri, version=version, headers=headers, remote_ip=remote_ip, protocol=self.protocol)\n        content_length = headers.get('Content-Length')\n        if content_length:\n            content_length = int(content_length)\n            if (content_length > self.stream.max_buffer_size):\n                raise _BadRequestException('Content-Length too long')\n            if (headers.get('Expect') == '100-continue'):\n                self.stream.write(b'HTTP/1.1 100 (Continue)\\r\\n\\r\\n')\n            self.stream.read_bytes(content_length, self._on_request_body)\n            return\n        self.request_callback(self._request)\n    except _BadRequestException as e:\n        gen_log.info('Malformed HTTP request from %s: %s', self.address[0], e)\n        self.close()\n        return\n", "label": 0}
{"function": "\n\ndef io_connection_pattern(inputs, outputs):\n    '\\n    Returns the connection pattern of a subgraph defined by given\\n    inputs and outputs.\\n\\n    '\n    inner_nodes = io_toposort(inputs, outputs)\n    connect_pattern_by_var = {\n        \n    }\n    nb_inputs = len(inputs)\n    for i in range(nb_inputs):\n        input = inputs[i]\n        inp_connection_pattern = [(i == j) for j in range(nb_inputs)]\n        connect_pattern_by_var[input] = inp_connection_pattern\n    for n in inner_nodes:\n        try:\n            op_connection_pattern = n.op.connection_pattern(n)\n        except AttributeError:\n            op_connection_pattern = ([([True] * len(n.outputs))] * len(n.inputs))\n        for out_idx in range(len(n.outputs)):\n            out = n.outputs[out_idx]\n            out_connection_pattern = ([False] * nb_inputs)\n            for inp_idx in range(len(n.inputs)):\n                inp = n.inputs[inp_idx]\n                if (inp in connect_pattern_by_var):\n                    inp_connection_pattern = connect_pattern_by_var[inp]\n                    if op_connection_pattern[inp_idx][out_idx]:\n                        out_connection_pattern = [(out_connection_pattern[i] or inp_connection_pattern[i]) for i in range(nb_inputs)]\n            connect_pattern_by_var[out] = out_connection_pattern\n    global_connection_pattern = [[] for o in range(len(inputs))]\n    for out in outputs:\n        out_connection_pattern = connect_pattern_by_var[out]\n        for i in range(len(inputs)):\n            global_connection_pattern[i].append(out_connection_pattern[i])\n    return global_connection_pattern\n", "label": 1}
{"function": "\n\n@expose('/delete/', methods=('POST',))\ndef delete(self):\n    '\\n            Delete view method\\n        '\n    form = self.delete_form()\n    path = form.path.data\n    if path:\n        return_url = self._get_dir_url('.index', op.dirname(path))\n    else:\n        return_url = self.get_url('.index')\n    if self.validate_form(form):\n        (base_path, full_path, path) = self._normalize_path(path)\n        if (not self.can_delete):\n            flash(gettext('Deletion is disabled.'), 'error')\n            return redirect(return_url)\n        if (not self.is_accessible_path(path)):\n            flash(gettext('Permission denied.'), 'error')\n            return redirect(self._get_dir_url('.index'))\n        if op.isdir(full_path):\n            if (not self.can_delete_dirs):\n                flash(gettext('Directory deletion is disabled.'), 'error')\n                return redirect(return_url)\n            try:\n                shutil.rmtree(full_path)\n                self.on_directory_delete(full_path, path)\n                flash(gettext('Directory \"%(path)s\" was successfully deleted.', path=path))\n            except Exception as ex:\n                flash(gettext('Failed to delete directory: %(error)s', error=ex), 'error')\n        else:\n            try:\n                os.remove(full_path)\n                self.on_file_delete(full_path, path)\n                flash(gettext('File \"%(name)s\" was successfully deleted.', name=path))\n            except Exception as ex:\n                flash(gettext('Failed to delete file: %(name)s', name=ex), 'error')\n    else:\n        helpers.flash_errors(form, message='Failed to delete file. %(error)s')\n    return redirect(return_url)\n", "label": 0}
{"function": "\n\ndef CallExpression(traverser, node):\n    args = [traverser.traverse_node(a) for a in node['arguments']]\n    member = traverser.traverse_node(node['callee'])\n    if ((node['callee']['type'] == 'MemberExpression') and (node['callee']['property']['type'] == 'Identifier')):\n        identifier_name = node['callee']['property']['name']\n        if (identifier_name in instanceactions.INSTANCE_DEFINITIONS):\n            traverser._debug('Calling instance action...')\n            result = instanceactions.INSTANCE_DEFINITIONS[identifier_name](args, traverser, traverser.traverse_node(node['callee']['object']))\n            if (result is not None):\n                return result\n    if (isinstance(member, JSGlobal) and ('return' in member.global_data)):\n        traverser._debug('EVALUATING RETURN...')\n        output = member.global_data['return'](wrapper=member, arguments=args, traverser=traverser)\n        if (output is not None):\n            return output\n    return JSObject(traverser=traverser)\n", "label": 0}
{"function": "\n\n@skipif(('GPy' not in sys.modules), 'this test requires hyperopt')\ndef test_gp():\n    searchspace = SearchSpace()\n    searchspace.add_float('x', (- 10), 10)\n    searchspace.add_float('y', 1, 10, warp='log')\n    searchspace.add_int('z', (- 10), 10)\n    searchspace.add_enum('w', ['opt1', 'opt2'])\n    history = [(searchspace.rvs(), np.random.random(), 'SUCCEEDED') for _ in range(4)]\n    params = GP().suggest(history, searchspace)\n    for (k, v) in iteritems(params):\n        assert (k in searchspace.variables)\n        if isinstance(searchspace[k], EnumVariable):\n            assert (v in searchspace[k].choices)\n        elif isinstance(searchspace[k], FloatVariable):\n            assert (searchspace[k].min <= v <= searchspace[k].max)\n        elif isinstance(searchspace[k], IntVariable):\n            assert (searchspace[k].min <= v <= searchspace[k].max)\n        else:\n            assert False\n", "label": 1}
{"function": "\n\ndef _prepare_plots(self, mixing=False, unmixing=False):\n    if (self.locations_ is None):\n        raise RuntimeError('Need sensor locations for plotting')\n    if (self.topo_ is None):\n        from scot.eegtopo.topoplot import Topoplot\n        self.topo_ = Topoplot(clipping=self.topo_clipping)\n        self.topo_.set_locations(self.locations_)\n    if (mixing and (not self.mixmaps_)):\n        premix = (self.premixing_ if (self.premixing_ is not None) else np.eye(self.mixing_.shape[1]))\n        self.mixmaps_ = self.plotting.prepare_topoplots(self.topo_, np.dot(self.mixing_, premix))\n    if (unmixing and (not self.unmixmaps_)):\n        preinv = (np.linalg.pinv(self.premixing_) if (self.premixing_ is not None) else np.eye(self.unmixing_.shape[0]))\n        self.unmixmaps_ = self.plotting.prepare_topoplots(self.topo_, np.dot(preinv, self.unmixing_).T)\n", "label": 0}
{"function": "\n\ndef read_fasta_lengths(ifile):\n    'Generate sequence ID,length from stream ifile'\n    id = None\n    seqLength = 0\n    isEmpty = True\n    for line in ifile:\n        if ('>' == line[0]):\n            if ((id is not None) and (seqLength > 0)):\n                (yield (id, seqLength))\n                isEmpty = False\n            id = line[1:].split()[0]\n            seqLength = 0\n        elif (id is not None):\n            for word in line.split():\n                seqLength += len(word)\n    if ((id is not None) and (seqLength > 0)):\n        (yield (id, seqLength))\n    elif isEmpty:\n        raise IOError('no readable sequence in FASTA file!')\n", "label": 1}
{"function": "\n\ndef communicate(self, input=None):\n    '\\n        Interact with process: Enqueue data to be sent to stdin.  Return data\\n        read from stdout and stderr as a tuple (stdoutdata, stderrdata).  Do\\n        NOT wait for process to terminate.\\n        '\n    if (self.use_stdin and input):\n        self.stdin_lock.acquire()\n        self.stdin_queue.append(input)\n        self.stdin_lock.release()\n    stdoutdata = None\n    stderrdata = None\n    if self.use_stdout:\n        data = b''\n        self.stderr_lock.acquire()\n        try:\n            while (len(self.stdout_queue) > 0):\n                data += self.stdout_queue.popleft()\n        except:\n            self.stderr_lock.release()\n            raise\n        self.stderr_lock.release()\n        if data:\n            stdoutdata = data\n    if self.use_stderr:\n        data = b''\n        self.stderr_lock.acquire()\n        try:\n            while (len(self.stderr_queue) > 0):\n                data += self.stderr_queue.popleft()\n        except:\n            self.stderr_lock.release()\n            raise\n        self.stderr_lock.release()\n        if data:\n            stderrdata = data\n    return (stdoutdata, stderrdata)\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    assert name.endswith('PropMap'), 'Please use convention: ___PropMap, e.g. ElectromagneticPropMap'\n    _properties = {\n        \n    }\n    for base in bases:\n        for baseProp in getattr(base, '_properties', {\n            \n        }):\n            _properties[baseProp] = base._properties[baseProp]\n    keys = [key for key in attrs]\n    for attr in keys:\n        if isinstance(attrs[attr], Property):\n            attrs[attr].name = attr\n            attrs[(attr + 'Map')] = attrs[attr]._getMapProperty()\n            attrs[(attr + 'Index')] = attrs[attr]._getIndexProperty()\n            _properties[attr] = attrs[attr]\n            attrs.pop(attr)\n    attrs['_properties'] = _properties\n    defaultInvProps = []\n    for p in _properties:\n        prop = _properties[p]\n        if prop.defaultInvProp:\n            defaultInvProps += [p]\n        if (prop.propertyLink is not None):\n            assert (prop.propertyLink[0] in _properties), (\"You can only link to things that exist: '%s' is trying to link to '%s'\" % (prop.name, prop.propertyLink[0]))\n    if (len(defaultInvProps) > 1):\n        raise Exception(('You have more than one default inversion property: %s' % defaultInvProps))\n    newClass = super(_PropMapMetaClass, cls).__new__(cls, name, bases, attrs)\n    newClass.PropModel = cls.createPropModelClass(newClass, name, _properties)\n    _PROPMAPCLASSREGISTRY[name] = newClass\n    return newClass\n", "label": 1}
{"function": "\n\n@classmethod\ndef delete(cls, repo, path):\n    'Delete the reference at the given path\\n\\n        :param repo:\\n            Repository to delete the reference from\\n\\n        :param path:\\n            Short or full path pointing to the reference, i.e. refs/myreference\\n            or just \"myreference\", hence \\'refs/\\' is implied.\\n            Alternatively the symbolic reference to be deleted'\n    full_ref_path = cls.to_full_path(path)\n    abs_path = join(repo.git_dir, full_ref_path)\n    if exists(abs_path):\n        os.remove(abs_path)\n    else:\n        pack_file_path = cls._get_packed_refs_path(repo)\n        try:\n            reader = open(pack_file_path, 'rb')\n        except (OSError, IOError):\n            pass\n        else:\n            new_lines = list()\n            made_change = False\n            dropped_last_line = False\n            for line in reader:\n                line = line.decode(defenc)\n                if ((line.startswith('#') or (full_ref_path not in line)) and ((not dropped_last_line) or (dropped_last_line and (not line.startswith('^'))))):\n                    new_lines.append(line)\n                    dropped_last_line = False\n                    continue\n                made_change = True\n                dropped_last_line = True\n            reader.close()\n            if made_change:\n                open(pack_file_path, 'wb').writelines((l.encode(defenc) for l in new_lines))\n    reflog_path = RefLog.path(cls(repo, full_ref_path))\n    if os.path.isfile(reflog_path):\n        os.remove(reflog_path)\n", "label": 1}
{"function": "\n\ndef mouseMoveEvent(self, event):\n    self.oldPosition = self.pos()\n    if (self.isSideClicked and self.isCusorRightSide):\n        w = max(self.minimumWidth(), ((self.currentWidth + event.x()) - self.rdragx))\n        h = self.currentHeight\n        self.resize(w, h)\n    elif (self.isSideClicked and self.isCusorDownSide):\n        w = self.currentWidth\n        h = max(self.minimumHeight(), ((self.currentHeight + event.y()) - self.rdragy))\n        self.resize(w, h)\n    elif self.isMaximized():\n        event.ignore()\n    elif (not self.isLocked()):\n        if hasattr(self, 'dragPosition'):\n            if (event.buttons() == Qt.LeftButton):\n                self.move((event.globalPos() - self.dragPosition))\n                event.accept()\n", "label": 0}
{"function": "\n\ndef test_remove_role():\n    db = SQLAlchemy('sqlite:///:memory:')\n    auth = authcode.Auth(SECRET_KEY, db=db, roles=True)\n    User = auth.User\n    Role = auth.Role\n    db.create_all()\n    user = User(login='meh', password='foobar')\n    db.session.add(user)\n    db.session.commit()\n    assert hasattr(auth, 'Role')\n    assert hasattr(User, 'roles')\n    user.add_role('admin')\n    db.session.commit()\n    assert user.has_role('admin')\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('admin')\n    db.session.commit()\n    assert (not user.has_role('admin'))\n    assert (list(user.roles) == [])\n    assert (db.query(Role).count() == 1)\n    user.remove_role('foobar')\n    db.session.commit()\n    assert (db.query(Role).count() == 1)\n", "label": 1}
{"function": "\n\ndef test_virtual_memory(self):\n    mem = psutil.virtual_memory()\n    assert (mem.total > 0), mem\n    assert (mem.available > 0), mem\n    assert (0 <= mem.percent <= 100), mem\n    assert (mem.used > 0), mem\n    assert (mem.free >= 0), mem\n    for name in mem._fields:\n        if (name != 'total'):\n            value = getattr(mem, name)\n            if (not (value >= 0)):\n                self.fail(('%r < 0 (%s)' % (name, value)))\n            if (value > mem.total):\n                self.fail(('%r > total (total=%s, %s=%s)' % (name, mem.total, name, value)))\n", "label": 1}
{"function": "\n\ndef configure_dhcp_for_network(self, network):\n    if (not network.admin_state_up):\n        return\n    enable_metadata = self.dhcp_driver_cls.should_enable_metadata(self.conf, network)\n    dhcp_network_enabled = False\n    for subnet in network.subnets:\n        if subnet.enable_dhcp:\n            if self.call_driver('enable', network):\n                dhcp_network_enabled = True\n                self.cache.put(network)\n            break\n    if (enable_metadata and dhcp_network_enabled):\n        for subnet in network.subnets:\n            if ((subnet.ip_version == 4) and subnet.enable_dhcp):\n                self.enable_isolated_metadata_proxy(network)\n                break\n    elif ((not self.conf.force_metadata) and (not self.conf.enable_isolated_metadata)):\n        self.disable_isolated_metadata_proxy(network)\n", "label": 1}
{"function": "\n\ndef main():\n    (options, args) = parse_args()\n    names = sorted((name for name in resource_listdir('trac.wiki', 'default-pages') if (not name.startswith('.'))))\n    if args:\n        args = sorted((set(names) & set(map(os.path.basename, args))))\n    else:\n        args = names\n    if options.download:\n        download_default_pages(args, options.prefix)\n    env = EnvironmentStub(disable=['trac.mimeview.pygments.*'])\n    load_components(env)\n    with env.db_transaction:\n        for name in names:\n            wiki = WikiPage(env, name)\n            wiki.text = resource_string('trac.wiki', ('default-pages/' + name)).decode('utf-8')\n            if wiki.text:\n                wiki.save('trac', '')\n            else:\n                printout(('%s: Skipped empty page' % name))\n    req = Mock(href=Href('/'), abs_href=Href('http://localhost/'), perm=MockPerm())\n    for name in args:\n        wiki = WikiPage(env, name)\n        if (not wiki.exists):\n            continue\n        context = web_context(req, wiki.resource)\n        out = DummyIO()\n        DefaultWikiChecker(env, context, name).format(wiki.text, out)\n", "label": 0}
{"function": "\n\ndef get_form(self, form_class=None):\n    '\\n        Returns an instance of the form to be used in this view.\\n        '\n    self.form = super(SmartFormMixin, self).get_form(form_class)\n    fields = list(self.derive_fields())\n    exclude = self.derive_exclude()\n    exclude += self.derive_readonly()\n    for field in exclude:\n        if (field in self.form.fields):\n            del self.form.fields[field]\n    if (fields is not None):\n        for (name, field) in self.form.fields.items():\n            if (not (name in fields)):\n                del self.form.fields[name]\n    location = forms.CharField(widget=forms.widgets.HiddenInput(), required=False)\n    if ('HTTP_REFERER' in self.request.META):\n        location.initial = self.request.META['HTTP_REFERER']\n    self.form.fields['loc'] = location\n    if fields:\n        fields.append('loc')\n    for (name, field) in self.form.fields.items():\n        field = self.customize_form_field(name, field)\n        self.form.fields[name] = field\n    return self.form\n", "label": 0}
{"function": "\n\ndef get_ambient_temperature(self, n=5):\n    '\\n        Populates the self.ambient_temp property\\n\\n        Calculation is taken from Rs232_Comms_v100.pdf section \"Converting values\\n        sent by the device to meaningful units\" item 5.\\n        '\n    if self.logger:\n        self.logger.info('Getting ambient temperature')\n    values = []\n    for i in range(0, n):\n        try:\n            value = (float(self.query('!T')[0]) / 100.0)\n        except:\n            pass\n        else:\n            if self.logger:\n                self.logger.debug('  Ambient Temperature Query = {:.1f}'.format(value))\n            values.append(value)\n    if (len(values) >= (n - 1)):\n        self.ambient_temp = (np.median(values) * u.Celsius)\n        if self.logger:\n            self.logger.info('  Ambient Temperature = {:.1f}'.format(self.ambient_temp))\n    else:\n        self.ambient_temp = None\n        if self.logger:\n            self.logger.info('  Failed to Read Ambient Temperature')\n    return self.ambient_temp\n", "label": 0}
{"function": "\n\ndef lex(s, name=None, trim_whitespace=True, line_offset=0, delimeters=None):\n    if (delimeters is None):\n        delimeters = (Template.default_namespace['start_braces'], Template.default_namespace['end_braces'])\n    in_expr = False\n    chunks = []\n    last = 0\n    last_pos = ((line_offset + 1), 1)\n    token_re = re.compile(('%s|%s' % (re.escape(delimeters[0]), re.escape(delimeters[1]))))\n    for match in token_re.finditer(s):\n        expr = match.group(0)\n        pos = find_position(s, match.end(), last, last_pos)\n        if ((expr == delimeters[0]) and in_expr):\n            raise TemplateError(('%s inside expression' % delimeters[0]), position=pos, name=name)\n        elif ((expr == delimeters[1]) and (not in_expr)):\n            raise TemplateError(('%s outside expression' % delimeters[1]), position=pos, name=name)\n        if (expr == delimeters[0]):\n            part = s[last:match.start()]\n            if part:\n                chunks.append(part)\n            in_expr = True\n        else:\n            chunks.append((s[last:match.start()], last_pos))\n            in_expr = False\n        last = match.end()\n        last_pos = pos\n    if in_expr:\n        raise TemplateError(('No %s to finish last expression' % delimeters[1]), name=name, position=last_pos)\n    part = s[last:]\n    if part:\n        chunks.append(part)\n    if trim_whitespace:\n        chunks = trim_lex(chunks)\n    return chunks\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('grid_type', ALL_GRID_TYPES)\ndef test_append_two_quantities_invalid(self, grid_type):\n    g = self.grid[grid_type]\n    g['density'] = []\n    g['energy'] = []\n    g['density'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    g['energy'].append(self.density[grid_type])\n    assert g['density'][0]\n    assert g['density'][(- 1)]\n    with pytest.raises(IndexError) as exc:\n        assert g['density'][1]\n    assert (exc_msg(exc) == 'list index out of range')\n    assert g['energy'][0]\n    assert g['energy'][1]\n    assert g['energy'][(- 1)]\n    assert g['energy'][(- 2)]\n    with pytest.raises(IndexError) as exc:\n        assert g['energy'][2]\n    assert (exc_msg(exc) == 'list index out of range')\n    with pytest.raises(ValueError) as exc:\n        g.n_dust\n    assert (exc_msg(exc) == 'Not all dust lists in the grid have the same size')\n", "label": 1}
{"function": "\n\ndef add_task(self, task, raise_error=False):\n    '\\n        Add task to the task queue.\\n        '\n    if self.parser_mode:\n        self.parser_result_queue.put((task, None))\n        return\n    if (self.task_queue is None):\n        raise SpiderMisuseError('You should configure task queue before adding tasks. Use `setup_queue` method.')\n    if ((task.priority is None) or (not task.priority_is_custom)):\n        task.priority = self.generate_task_priority()\n        task.priority_is_custom = False\n    else:\n        task.priority_is_custom = True\n    try:\n        if (not task.url.startswith(('http://', 'https://', 'ftp://', 'file://', 'feed://'))):\n            if (self.base_url is None):\n                msg = ('Could not resolve relative URL because base_url is not specified. Task: %s, URL: %s' % (task.name, task.url))\n                raise SpiderError(msg)\n            else:\n                warn('Class attribute `Spider::base_url` is deprecated. Use Task objects with absolute URLs')\n                task.url = urljoin(self.base_url, task.url)\n                if task.grab_config:\n                    task.grab_config['url'] = task.url\n    except Exception as ex:\n        self.stat.collect('task-with-invalid-url', task.url)\n        if raise_error:\n            raise\n        else:\n            logger.error('', exc_info=ex)\n            return False\n    self.task_queue.put(task, task.priority, schedule_time=task.schedule_time)\n    return True\n", "label": 1}
{"function": "\n\ndef commit_dirtiness_flags(self):\n    '\\n        Updates any dirtiness flags in the database.\\n        '\n    if self.domain:\n        flags_to_save = self.get_flags_to_save()\n        if should_create_flags_on_submission(self.domain):\n            assert settings.UNIT_TESTING\n            all_touched_ids = (set(flags_to_save.keys()) | self.get_clean_owner_ids())\n            to_update = {f.owner_id: f for f in OwnershipCleanlinessFlag.objects.filter(domain=self.domain, owner_id__in=list(all_touched_ids))}\n            for owner_id in all_touched_ids:\n                if (owner_id not in to_update):\n                    flag = OwnershipCleanlinessFlag(domain=self.domain, owner_id=owner_id, is_clean=True)\n                    if (owner_id in flags_to_save):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                    flag.save()\n                else:\n                    flag = to_update[owner_id]\n                    if ((owner_id in flags_to_save) and (flag.is_clean or (not flag.hint))):\n                        flag.is_clean = False\n                        flag.hint = flags_to_save[owner_id]\n                        flag.save()\n        else:\n            flags_to_update = OwnershipCleanlinessFlag.objects.filter(Q(domain=self.domain), Q(owner_id__in=flags_to_save.keys()), (Q(is_clean=True) | Q(hint__isnull=True)))\n            for flag in flags_to_update:\n                flag.is_clean = False\n                flag.hint = flags_to_save[flag.owner_id]\n                flag.save()\n", "label": 1}
{"function": "\n\ndef _compute_generator_info(self):\n    \"\\n        Compute the generator's state variables as the union of live variables\\n        at all yield points.\\n        \"\n    gi = self.generator_info\n    for yp in gi.get_yield_points():\n        live_vars = set(self.block_entry_vars[yp.block])\n        weak_live_vars = set()\n        stmts = iter(yp.block.body)\n        for stmt in stmts:\n            if isinstance(stmt, ir.Assign):\n                if (stmt.value is yp.inst):\n                    break\n                live_vars.add(stmt.target.name)\n            elif isinstance(stmt, ir.Del):\n                live_vars.remove(stmt.value)\n        else:\n            assert 0, \"couldn't find yield point\"\n        for stmt in stmts:\n            if isinstance(stmt, ir.Del):\n                name = stmt.value\n                if (name in live_vars):\n                    live_vars.remove(name)\n                    weak_live_vars.add(name)\n            else:\n                break\n        yp.live_vars = live_vars\n        yp.weak_live_vars = weak_live_vars\n    st = set()\n    for yp in gi.get_yield_points():\n        st |= yp.live_vars\n        st |= yp.weak_live_vars\n    gi.state_vars = sorted(st)\n", "label": 1}
{"function": "\n\ndef pil_from_ndarray(ndarray):\n    '\\n    Converts an ndarray to a PIL image.\\n\\n    Parameters\\n    ----------\\n    ndarray : ndarray\\n        An ndarray containing an image.\\n\\n    Returns\\n    -------\\n    pil : PIL Image\\n        A PIL Image containing the image.\\n    '\n    try:\n        if ((ndarray.dtype == 'float32') or (ndarray.dtype == 'float64')):\n            assert (ndarray.min() >= 0.0)\n            assert (ndarray.max() <= 1.0)\n            ndarray = np.cast['uint8']((ndarray * 255))\n            if ((len(ndarray.shape) == 3) and (ndarray.shape[2] == 1)):\n                ndarray = ndarray[:, :, 0]\n        ensure_Image()\n        rval = Image.fromarray(ndarray)\n        return rval\n    except Exception as e:\n        logger.exception('original exception: ')\n        logger.exception(e)\n        logger.exception('ndarray.dtype: {0}'.format(ndarray.dtype))\n        logger.exception('ndarray.shape: {0}'.format(ndarray.shape))\n        raise\n    assert False\n", "label": 0}
{"function": "\n\ndef levenshtein_distance(first, second):\n    'Find the Levenshtein distance between two strings.'\n    if (len(first) > len(second)):\n        (first, second) = (second, first)\n    if (len(second) == 0):\n        return len(first)\n    first_length = (len(first) + 1)\n    second_length = (len(second) + 1)\n    distance_matrix = [([0] * second_length) for x in range(first_length)]\n    for i in range(first_length):\n        distance_matrix[i][0] = i\n    for j in range(second_length):\n        distance_matrix[0][j] = j\n    for i in xrange(1, first_length):\n        for j in range(1, second_length):\n            deletion = (distance_matrix[(i - 1)][j] + 1)\n            insertion = (distance_matrix[i][(j - 1)] + 1)\n            substitution = distance_matrix[(i - 1)][(j - 1)]\n            if (first[(i - 1)] != second[(j - 1)]):\n                substitution += 1\n            distance_matrix[i][j] = min(insertion, deletion, substitution)\n    return distance_matrix[(first_length - 1)][(second_length - 1)]\n", "label": 0}
{"function": "\n\ndef test05_ManyActorsUniqueAddress(self):\n    asys = ActorSystem()\n    addresses = [asys.createActor(Juliet) for n in range(100)]\n    uniqueAddresses = set(addresses)\n    if (len(addresses) != len(uniqueAddresses)):\n        duplicates = [A for A in uniqueAddresses if (len([X for X in addresses if (X == A)]) > 1)]\n        print(('Duplicates: %s' % map(str, duplicates)))\n        if duplicates:\n            for each in duplicates:\n                print(('... %s at: %s' % (str(each), str([N for (N, A) in enumerate(addresses) if (A == each)]))))\n        print('Note: if this is a UDPTransport test, be advised that Linux occasionally does seem to assign the same UDP port multiple times.  Linux bug?')\n    self.assertEqual(len(addresses), len(uniqueAddresses))\n", "label": 1}
{"function": "\n\ndef test_fermionoperator():\n    c = FermionOp('c')\n    d = FermionOp('d')\n    assert isinstance(c, FermionOp)\n    assert isinstance(Dagger(c), FermionOp)\n    assert c.is_annihilation\n    assert (not Dagger(c).is_annihilation)\n    assert (FermionOp('c') == FermionOp('c'))\n    assert (FermionOp('c') != FermionOp('d'))\n    assert (FermionOp('c', True) != FermionOp('c', False))\n    assert (AntiCommutator(c, Dagger(c)).doit() == 1)\n    assert (AntiCommutator(c, Dagger(d)).doit() == ((c * Dagger(d)) + (Dagger(d) * c)))\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    r = []\n    try:\n        for el in self:\n            c = el.get('class')\n            c = ((c and ('.' + '.'.join(c.split(' ')))) or '')\n            id = el.get('id')\n            id = ((id and ('#' + id)) or '')\n            r.append(('<%s%s%s>' % (el.tag, id, c)))\n        return (('[' + ', '.join(r)) + ']')\n    except AttributeError:\n        if PY3k:\n            return list.__repr__(self)\n        else:\n            for el in self:\n                if isinstance(el, unicode):\n                    r.append(el.encode('utf-8'))\n                else:\n                    r.append(el)\n            return repr(r)\n", "label": 1}
{"function": "\n\ndef load_middleware(self):\n    '\\n        Populate middleware lists from settings.MIDDLEWARE_CLASSES.\\n\\n        Must be called after the environment is fixed (see __call__ in subclasses).\\n        '\n    self._view_middleware = []\n    self._template_response_middleware = []\n    self._response_middleware = []\n    self._exception_middleware = []\n    request_middleware = []\n    for middleware_path in settings.MIDDLEWARE_CLASSES:\n        mw_class = import_string(middleware_path)\n        try:\n            mw_instance = mw_class()\n        except MiddlewareNotUsed as exc:\n            if settings.DEBUG:\n                if six.text_type(exc):\n                    logger.debug('MiddlewareNotUsed(%r): %s', middleware_path, exc)\n                else:\n                    logger.debug('MiddlewareNotUsed: %r', middleware_path)\n            continue\n        if hasattr(mw_instance, 'process_request'):\n            request_middleware.append(mw_instance.process_request)\n        if hasattr(mw_instance, 'process_view'):\n            self._view_middleware.append(mw_instance.process_view)\n        if hasattr(mw_instance, 'process_template_response'):\n            self._template_response_middleware.insert(0, mw_instance.process_template_response)\n        if hasattr(mw_instance, 'process_response'):\n            self._response_middleware.insert(0, mw_instance.process_response)\n        if hasattr(mw_instance, 'process_exception'):\n            self._exception_middleware.insert(0, mw_instance.process_exception)\n    self._request_middleware = request_middleware\n", "label": 1}
{"function": "\n\ndef add_spatial_info(features, add_x=True, add_y=True, inplace=False, dtype=None):\n    \"\\n    Adds spatial information to image features (which should contain a frames\\n    attribute in the format created by extract_image_features).\\n\\n    Adds a feature for x (if add_x) and y (if add_y), which are relative (x, y)\\n    locations within the image of the feature between 0 and 1 (inclusive).\\n\\n    Returns a new Features object with these additional features, or modifies\\n    features and returns None if inplace is True.\\n\\n    If dtype is not None, the resulting array will have that dtype. Otherwise,\\n    it will maintain features.dtype if it's a float type, or float32 if not.\\n    \"\n    if ((not add_x) and (not add_y)):\n        return (None if inplace else features)\n    indices = []\n    if add_x:\n        indices.append(0)\n    if add_y:\n        indices.append(1)\n    if (dtype is None):\n        dtype = features.dtype\n        if (dtype.kind != 'f'):\n            dtype = np.float32\n    spatial = np.asarray(np.vstack(features.frames)[:, indices], dtype=dtype)\n    spatial /= spatial.max(axis=0)\n    new_feats = np.hstack((features._features, spatial))\n    if inplace:\n        features._features = new_feats\n        features._refresh_features()\n    else:\n        return Features(new_feats, n_pts=features._n_pts, categories=features.categories, names=features.names, **dict(((k, features.data[k]) for k in features._extra_names)))\n", "label": 1}
{"function": "\n\ndef lookup(self, *features):\n    if (len(self.builders) == 0):\n        return None\n    if (len(features) == 0):\n        return self.builders[0]\n    features = list(features)\n    features.reverse()\n    candidates = None\n    candidate_set = None\n    while (len(features) > 0):\n        feature = features.pop()\n        we_have_the_feature = self.builders_for_feature.get(feature, [])\n        if (len(we_have_the_feature) > 0):\n            if (candidates is None):\n                candidates = we_have_the_feature\n                candidate_set = set(candidates)\n            else:\n                candidate_set = candidate_set.intersection(set(we_have_the_feature))\n    if (candidate_set is None):\n        return None\n    for candidate in candidates:\n        if (candidate in candidate_set):\n            return candidate\n    return None\n", "label": 0}
{"function": "\n\ndef salvage_broken_user_settings_document(document):\n    if ((not document['access_token']) or (not document['dropbox_id'])):\n        return False\n    if ((not document['owner']) or (not User.load(document['owner']).is_active)):\n        return False\n    if document['deleted']:\n        return False\n    if ((not document.get('dropbox_info')) or (not document['dropbox_info']['display_name'])):\n        logger.info('Attempting dropbox_info population for document (id:{0})'.format(document['_id']))\n        client = DropboxClient(document['access_token'])\n        document['dropbox_info'] = {\n            \n        }\n        try:\n            database['dropboxusersettings'].find_and_modify({\n                '_id': document['_id'],\n            }, {\n                '$set': {\n                    'dropbox_info': client.account_info(),\n                },\n            })\n        except Exception:\n            return True\n        else:\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef __eq__(self, other):\n    if (self is other):\n        return True\n    return (isinstance(other, self.__class__) and (self._numeric_id == other._numeric_id) and (self._id == other._id) and (self._cells_table == other._cells_table) and (self._contents_table == other._contents_table) and (self._deps_table == other._deps_table) and (self._renames == other._renames) and (self._deltas == other._deltas) and (self._cell_count == other._cell_count) and (self._content_count == other._content_count))\n", "label": 1}
{"function": "\n\ndef get_dot_completions(view, prefix, position, info):\n    if ((not get_setting(view, 'fw1_enabled')) or (len(info['dot_context']) == 0)):\n        return None\n    if extends_fw1(view):\n        if (info['dot_context'][(- 1)].name == 'variables'):\n            key = '.'.join([symbol.name for symbol in reversed(info['dot_context'])])\n            if (key in fw1['settings']):\n                return CompletionList(fw1['settings'][key], 1, False)\n        if (info['dot_context'][(- 1)].name in ['renderdata', 'renderer']):\n            return CompletionList(fw1['methods']['renderdata'], 1, False)\n    if (get_file_type(view) == 'controller'):\n        if ((len(info['dot_context']) > 1) and (info['dot_context'][(- 2)].name in ['renderdata', 'renderer'])):\n            return CompletionList(fw1['methods']['renderdata'], 1, False)\n        if (info['dot_context'][(- 1)].name in ['fw', 'framework']):\n            return CompletionList(fw1['methods']['calls'], 1, False)\n    return None\n", "label": 1}
{"function": "\n\n@login_required\n@transaction.atomic\ndef delete_posts(request, topic_id):\n    topic = Topic.objects.select_related().get(pk=topic_id)\n    if forum_moderated_by(topic, request.user):\n        deleted = False\n        post_list = request.POST.getlist('post')\n        for post_id in post_list:\n            if (not deleted):\n                deleted = True\n            delete_post(request, post_id)\n        if deleted:\n            messages.success(request, _('Post deleted.'))\n            return HttpResponseRedirect(topic.get_absolute_url())\n    last_post = topic.posts.latest()\n    if request.user.is_authenticated():\n        topic.update_read(request.user)\n    posts = topic.posts.all().select_related()\n    initial = {\n        \n    }\n    if request.user.is_authenticated():\n        initial = {\n            'markup': request.user.forum_profile.markup,\n        }\n    form = AddPostForm(topic=topic, initial=initial)\n    moderator = (request.user.is_superuser or (request.user in topic.forum.moderators.all()))\n    if (request.user.is_authenticated() and (request.user in topic.subscribers.all())):\n        subscribed = True\n    else:\n        subscribed = False\n    return render(request, 'djangobb_forum/delete_posts.html', {\n        'topic': topic,\n        'last_post': last_post,\n        'form': form,\n        'moderator': moderator,\n        'subscribed': subscribed,\n        'posts_page': get_page(posts, request, forum_settings.TOPIC_PAGE_SIZE),\n    })\n", "label": 1}
{"function": "\n\ndef test_tb_option(self, testdir, option):\n    testdir.makepyfile('\\n            import pytest\\n            def g():\\n                raise IndexError\\n            def test_func():\\n                print (6*7)\\n                g()  # --calling--\\n        ')\n    for tbopt in ['long', 'short', 'no']:\n        print(('testing --tb=%s...' % tbopt))\n        result = testdir.runpytest(('--tb=%s' % tbopt))\n        s = result.stdout.str()\n        if (tbopt == 'long'):\n            assert ('print (6*7)' in s)\n        else:\n            assert ('print (6*7)' not in s)\n        if (tbopt != 'no'):\n            assert ('--calling--' in s)\n            assert ('IndexError' in s)\n        else:\n            assert ('FAILURES' not in s)\n            assert ('--calling--' not in s)\n            assert ('IndexError' not in s)\n", "label": 1}
{"function": "\n\n@register.simple_tag\ndef aggregate_section_totals(section_name, results_arr, daily):\n    'Hackish function to do a summation of a section in the org_report'\n    startindex = (- 1)\n    endindex = (- 1)\n    for itemarr in results_arr:\n        if (itemarr[1] == section_name):\n            startindex = results_arr.index(itemarr)\n            continue\n        if (startindex >= 0):\n            if (itemarr[1] != None):\n                endindex = results_arr.index(itemarr)\n                break\n    summation = []\n    section_arr = []\n    if (endindex == (- 1)):\n        section_arr = results_arr[startindex:]\n    else:\n        section_arr = results_arr[startindex:(endindex + 1)]\n    for itemarr in section_arr:\n        if (summation == []):\n            summation = (summation + itemarr[(- 1)])\n        else:\n            for i in range(0, len(itemarr[(- 1)])):\n                summation[i] += itemarr[(- 1)][i]\n    ret = ''\n    if daily:\n        for item in summation:\n            ret += ('<td style=\"background:#99FFFF\"><strong>%d</strong></td>' % item)\n    else:\n        sum = 0\n        for item in summation:\n            sum += item\n        ret = ('<td>%d</td>' % sum)\n    return ret\n", "label": 1}
{"function": "\n\n@data.setter\ndef data(self, data):\n    numpy = import_module('numpy')\n    data = _TensorDataLazyEvaluator.parse_data(data)\n    if (data.ndim > 2):\n        raise ValueError('data have to be of rank 1 (diagonal metric) or 2.')\n    if (data.ndim == 1):\n        if (self.dim is not None):\n            nda_dim = data.shape[0]\n            if (nda_dim != self.dim):\n                raise ValueError('Dimension mismatch')\n        dim = data.shape[0]\n        newndarray = numpy.zeros((dim, dim), dtype=object)\n        for (i, val) in enumerate(data):\n            newndarray[(i, i)] = val\n        data = newndarray\n    (dim1, dim2) = data.shape\n    if (dim1 != dim2):\n        raise ValueError('Non-square matrix tensor.')\n    if (self.dim is not None):\n        if (self.dim != dim1):\n            raise ValueError('Dimension mismatch')\n    _tensor_data_substitution_dict[self] = data\n    _tensor_data_substitution_dict.add_metric_data(self.metric, data)\n    delta = self.get_kronecker_delta()\n    i1 = TensorIndex('i1', self)\n    i2 = TensorIndex('i2', self)\n    delta(i1, (- i2)).data = _TensorDataLazyEvaluator.parse_data(eye(dim1))\n", "label": 0}
{"function": "\n\n@staticmethod\ndef budget_monitoring_percentage(row):\n    '\\n            Virtual Field to show the percentage used of the Budget\\n        '\n    if hasattr(row, 'budget_monitoring'):\n        row = row.budget_monitoring\n    if hasattr(row, 'planned'):\n        planned = row.planned\n        if (planned == 0.0):\n            return current.messages['NONE']\n    else:\n        planned = None\n    if hasattr(row, 'value'):\n        actual = row.value\n    else:\n        actual = None\n    if ((planned is not None) and (actual is not None)):\n        percentage = ((actual / planned) * 100)\n        return ('%s %%' % '{0:.2f}'.format(percentage))\n    if hasattr(row, 'id'):\n        table = current.s3db.budget_monitoring\n        r = current.db((table.id == row.id)).select(table.planned, table.value, limitby=(0, 1)).first()\n        if r:\n            planned = r.planned\n            if (planned == 0.0):\n                return current.messages['NONE']\n            percentage = ((r.value / planned) * 100)\n            return ('%s %%' % percentage)\n    return current.messages['NONE']\n", "label": 1}
{"function": "\n\ndef _InvokeGitkitApi(self, method, params=None, need_service_account=True):\n    'Invokes Gitkit API, with optional access token for service account.\\n\\n    Args:\\n      method: string, the api method name.\\n      params: dict of optional parameters for the API.\\n      need_service_account: false if service account is not needed.\\n\\n    Raises:\\n      GitkitClientError: if the request is bad.\\n      GitkitServerError: if Gitkit can not handle the request.\\n\\n    Returns:\\n      API response as dict.\\n    '\n    body = (simplejson.dumps(params) if params else None)\n    req = urllib_request.Request((self.google_api_url + method))\n    req.add_header('Content-type', 'application/json')\n    if need_service_account:\n        if self.credentials:\n            access_token = self.credentials.get_access_token().access_token\n        elif (self.service_account_email and self.service_account_key):\n            access_token = self._GetAccessToken()\n        else:\n            raise errors.GitkitClientError('Missing service account credentials')\n        req.add_header('Authorization', ('Bearer ' + access_token))\n    try:\n        binary_body = (body.encode('utf-8') if body else None)\n        raw_response = urllib_request.urlopen(req, binary_body).read()\n    except urllib_request.HTTPError as err:\n        if (err.code == 400):\n            raw_response = err.read()\n        else:\n            raise\n    return self._CheckGitkitError(raw_response)\n", "label": 0}
{"function": "\n\ndef nome(ctx, m):\n    m = ctx.convert(m)\n    if (not m):\n        return m\n    if (m == ctx.one):\n        return m\n    if ctx.isnan(m):\n        return m\n    if ctx.isinf(m):\n        if (m == ctx.ninf):\n            return type(m)((- 1))\n        else:\n            return ctx.mpc((- 1))\n    a = ctx.ellipk((ctx.one - m))\n    b = ctx.ellipk(m)\n    v = ctx.exp((((- ctx.pi) * a) / b))\n    if ((not ctx._im(m)) and (ctx._re(m) < 1)):\n        if ctx._is_real_type(m):\n            return v.real\n        else:\n            return (v.real + 0j)\n    elif (m == 2):\n        v = ctx.mpc(0, v.imag)\n    return v\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _tensAdd_check_automatrix(args):\n    if (not args):\n        return args\n    auto_left_types = set([])\n    auto_right_types = set([])\n    args_auto_left_types = []\n    args_auto_right_types = []\n    for (i, arg) in enumerate(args):\n        arg_auto_left_types = set([])\n        arg_auto_right_types = set([])\n        for index in get_indices(arg):\n            if (index in (index._tensortype.auto_left, (- index._tensortype.auto_left))):\n                auto_left_types.add(index._tensortype)\n                arg_auto_left_types.add(index._tensortype)\n            if (index in (index._tensortype.auto_right, (- index._tensortype.auto_right))):\n                auto_right_types.add(index._tensortype)\n                arg_auto_right_types.add(index._tensortype)\n        args_auto_left_types.append(arg_auto_left_types)\n        args_auto_right_types.append(arg_auto_right_types)\n    for (arg, aas_left, aas_right) in zip(args, args_auto_left_types, args_auto_right_types):\n        missing_left = (auto_left_types - aas_left)\n        missing_right = (auto_right_types - aas_right)\n        missing_intersection = (missing_left & missing_right)\n        for j in missing_intersection:\n            args[i] *= j.delta(j.auto_left, (- j.auto_right))\n        if (missing_left != missing_right):\n            raise ValueError('cannot determine how to add auto-matrix indices on some args')\n    return args\n", "label": 0}
{"function": "\n\ndef test_log_pdf(self):\n    X_1 = numpy.array([[1], [0]])\n    parameters = dict(mu=0.0, rho=1.0)\n    log_pdf = self.component_class.log_pdf(X_1, parameters)\n    assert (len(log_pdf) == 2)\n    assert (math.fabs(((- 1.4189385332046727) - log_pdf[(0, 0)])) < 1e-08)\n    assert (math.fabs(((- 0.9189385332046727) - log_pdf[(1, 0)])) < 1e-08)\n    parameters = dict(mu=2.2, rho=12.1)\n    log_pdf = self.component_class.log_pdf(X_1, parameters)\n    assert (len(log_pdf) == 2)\n    assert (math.fabs(((- 8.38433580690333) - log_pdf[(0, 0)])) < 1e-08)\n    assert (math.fabs(((- 28.954335806903334) - log_pdf[(1, 0)])) < 1e-08)\n    parameters = dict(mu=0.0, rho=1.0)\n    lspc = numpy.linspace(0, 10, num=20)\n    X_2 = numpy.array([[x] for x in lspc])\n    log_pdf = self.component_class.log_pdf(X_2, parameters)\n    assert (len(log_pdf) == 20)\n    for n in range(1, 20):\n        assert (log_pdf[((n - 1), 0)] > log_pdf[(n, 0)])\n", "label": 1}
{"function": "\n\ndef _construct_keymaps(self, config):\n    '\\n        Construct keymaps for handling input\\n        '\n    keymap = {\n        \n    }\n    move_keymap = {\n        \n    }\n    for key in config.move_left:\n        keymap[key] = self._move\n        move_keymap[key] = 7\n    for key in config.move_up:\n        keymap[key] = self._move\n        move_keymap[key] = 1\n    for key in config.move_right:\n        keymap[key] = self._move\n        move_keymap[key] = 3\n    for key in config.move_down:\n        keymap[key] = self._move\n        move_keymap[key] = 5\n    for key in config.start:\n        keymap[key] = self._menu\n    for key in config.action_a:\n        keymap[key] = self._action_a\n    for key in config.back:\n        keymap[key] = self._back\n    for key in config.left_shoulder:\n        keymap[key] = self._shoulder_left\n    for key in config.right_shoulder:\n        keymap[key] = self._shoulder_right\n    for key in config.mode_1:\n        keymap[key] = self._zoom_out\n    for key in config.mode_2:\n        keymap[key] = self._zoom_in\n    return (keymap, move_keymap)\n", "label": 1}
{"function": "\n\ndef load(self, filename):\n    if (not filename):\n        import traceback\n        traceback.print_stack()\n        return\n    try:\n        im = None\n        if self._inline:\n            im = pygame.image.load(filename, 'x.{}'.format(self._ext))\n        elif isfile(filename):\n            with open(filename, 'rb') as fd:\n                im = pygame.image.load(fd)\n        elif isinstance(filename, bytes):\n            try:\n                fname = filename.decode()\n                if isfile(fname):\n                    with open(fname, 'rb') as fd:\n                        im = pygame.image.load(fd)\n            except UnicodeDecodeError:\n                pass\n        if (im is None):\n            im = pygame.image.load(filename)\n    except:\n        raise\n    fmt = ''\n    if (im.get_bytesize() == 3):\n        fmt = 'rgb'\n    elif (im.get_bytesize() == 4):\n        fmt = 'rgba'\n    if (fmt not in ('rgb', 'rgba')):\n        try:\n            imc = im.convert(32)\n            fmt = 'rgba'\n        except:\n            try:\n                imc = im.convert_alpha()\n                fmt = 'rgba'\n            except:\n                Logger.warning(('Image: Unable to convert image %r to rgba (was %r)' % (filename, im.fmt)))\n                raise\n        im = imc\n    if (not self._inline):\n        self.filename = filename\n    data = pygame.image.tostring(im, fmt.upper())\n    return [ImageData(im.get_width(), im.get_height(), fmt, data, source=filename)]\n", "label": 1}
{"function": "\n\ndef keyPressEvent(self, ev):\n    '\\n        Re-implemented to handle the user input a key at a time.\\n        \\n        @param ev key event (QKeyEvent)\\n        '\n    txt = ev.text()\n    key = ev.key()\n    ctrl = (ev.modifiers() & Qt.ControlModifier)\n    shift = (ev.modifiers() & Qt.ShiftModifier)\n    if (self.keymap.has_key(key) and (not shift) and (not ctrl)):\n        self.keymap[key]()\n    elif (ev == QtGui.QKeySequence.Paste):\n        self.paste()\n    elif (self.__isCursorOnLastLine() and txt.length()):\n        QsciScintilla.keyPressEvent(self, ev)\n        self.incrementalSearchActive = True\n        if (txt == '.'):\n            self.__showDynCompletion()\n    elif (ctrl or shift):\n        QsciScintilla.keyPressEvent(self, ev)\n    else:\n        ev.ignore()\n", "label": 1}
{"function": "\n\ndef test_file_metadata_drive(basepath):\n    item = fixtures.list_file['items'][0]\n    path = basepath.child(item['title'])\n    parsed = GoogleDriveFileMetadata(item, path)\n    assert (parsed.provider == 'googledrive')\n    assert (parsed.id == item['id'])\n    assert (path.name == item['title'])\n    assert (parsed.name == item['title'])\n    assert (parsed.size == item['fileSize'])\n    assert (parsed.modified == item['modifiedDate'])\n    assert (parsed.content_type == item['mimeType'])\n    assert (parsed.extra == {\n        'revisionId': item['version'],\n        'webView': item['alternateLink'],\n    })\n    assert (parsed.path == ('/' + os.path.join(*[x.raw for x in path.parts])))\n    assert (parsed.materialized_path == str(path))\n    assert (parsed.is_google_doc == False)\n    assert (parsed.export_name == item['title'])\n", "label": 1}
{"function": "\n\ndef extend_list(self, data, parsed_args):\n    'Add subnet information to a network list.'\n    neutron_client = self.get_client()\n    search_opts = {\n        'fields': ['id', 'cidr'],\n    }\n    if self.pagination_support:\n        page_size = parsed_args.page_size\n        if page_size:\n            search_opts.update({\n                'limit': page_size,\n            })\n    subnet_ids = []\n    for n in data:\n        if ('subnets' in n):\n            subnet_ids.extend(n['subnets'])\n\n    def _get_subnet_list(sub_ids):\n        search_opts['id'] = sub_ids\n        return neutron_client.list_subnets(**search_opts).get('subnets', [])\n    try:\n        subnets = _get_subnet_list(subnet_ids)\n    except exceptions.RequestURITooLong as uri_len_exc:\n        subnet_count = len(subnet_ids)\n        max_size = ((self.subnet_id_filter_len * subnet_count) - uri_len_exc.excess)\n        chunk_size = (max_size // self.subnet_id_filter_len)\n        subnets = []\n        for i in range(0, subnet_count, chunk_size):\n            subnets.extend(_get_subnet_list(subnet_ids[i:(i + chunk_size)]))\n    subnet_dict = dict([(s['id'], s) for s in subnets])\n    for n in data:\n        if ('subnets' in n):\n            n['subnets'] = [(subnet_dict.get(s) or {\n                'id': s,\n            }) for s in n['subnets']]\n", "label": 1}
{"function": "\n\ndef iterbusinessdays(self, d1, d2):\n    '\\n        Date iterator returning dates in d1 <= x < d2, excluding weekends and holidays\\n        '\n    assert (d2 >= d1)\n    if ((d1.date() == d2.date()) and (d2.time() < self.business_hours[0])):\n        return\n    first = True\n    for dt in self.iterdays(d1, d2):\n        if (first and (d1.time() > self.business_hours[1])):\n            first = False\n            continue\n        first = False\n        if ((not self.isweekend(dt)) and (not self.isholiday(dt))):\n            (yield dt)\n", "label": 0}
{"function": "\n\ndef create_table_constraints(self, table):\n    constraints = []\n    if table.primary_key:\n        constraints.append(table.primary_key)\n    constraints.extend([c for c in table._sorted_constraints if (c is not table.primary_key)])\n    return ', \\n\\t'.join((p for p in (self.process(constraint) for constraint in constraints if (((constraint._create_rule is None) or constraint._create_rule(self)) and ((not self.dialect.supports_alter) or (not getattr(constraint, 'use_alter', False))))) if (p is not None)))\n", "label": 1}
{"function": "\n\ndef install(self, install_options, global_options=(), *args, **kwargs):\n    '\\n        Install everything in this set (after having downloaded and unpacked\\n        the packages)\\n        '\n    to_install = self._to_install()\n    if to_install:\n        logger.info('Installing collected packages: %s', ', '.join([req.name for req in to_install]))\n    with indent_log():\n        for requirement in to_install:\n            if requirement.conflicts_with:\n                logger.info('Found existing installation: %s', requirement.conflicts_with)\n                with indent_log():\n                    requirement.uninstall(auto_confirm=True)\n            try:\n                requirement.install(install_options, global_options, *args, **kwargs)\n            except:\n                if (requirement.conflicts_with and (not requirement.install_succeeded)):\n                    requirement.rollback_uninstall()\n                raise\n            else:\n                if (requirement.conflicts_with and requirement.install_succeeded):\n                    requirement.commit_uninstall()\n            requirement.remove_temporary_source()\n    self.successfully_installed = to_install\n", "label": 1}
{"function": "\n\ndef __dir__(self):\n    'return list of member names'\n    cls_members = []\n    cname = self.__class__.__name__\n    if ((cname != 'SymbolTable') and hasattr(self, '__class__')):\n        cls_members = dir(self.__class__)\n    dict_keys = [key for key in self.__dict__ if (key not in cls_members)]\n    return [key for key in (cls_members + dict_keys) if ((not key.startswith('_SymbolTable_')) and (not key.startswith('_Group_')) and (not key.startswith(('_%s_' % cname))) and (not (key.startswith('__') and key.endswith('__'))) and (key not in self.__private))]\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.LIST):\n                self.expressions = []\n                (_etype17, _size14) = iprot.readListBegin()\n                for _i18 in xrange(_size14):\n                    _elem19 = IndexExpression()\n                    _elem19.read(iprot)\n                    self.expressions.append(_elem19)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.start_key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef from_dict(values):\n    '\\n        Instantiate a BlockadeConfig instance based on\\n        a given dictionary of configuration values\\n        '\n    try:\n        containers = values['containers']\n        parsed_containers = {\n            \n        }\n        for (name, container_dict) in containers.items():\n            try:\n                for cnt in BlockadeContainerConfig.from_dict(name, container_dict):\n                    if cnt.container_name:\n                        cname = cnt.container_name\n                        existing = [c for c in parsed_containers.values() if (c.container_name == cname)]\n                        if existing:\n                            raise BlockadeConfigError((\"Duplicate 'container_name' definition: %s\" % cname))\n                    parsed_containers[cnt.name] = cnt\n            except Exception as err:\n                raise BlockadeConfigError((\"Container '%s' config problem: %s\" % (name, err)))\n        network = values.get('network')\n        if network:\n            defaults = _DEFAULT_NETWORK_CONFIG.copy()\n            defaults.update(network)\n            network = defaults\n        else:\n            network = _DEFAULT_NETWORK_CONFIG.copy()\n        return BlockadeConfig(parsed_containers, network=network)\n    except KeyError as err:\n        raise BlockadeConfigError(('Config missing value: ' + str(err)))\n    except Exception as err:\n        raise BlockadeConfigError(('Failed to load config: ' + str(err)))\n", "label": 1}
{"function": "\n\ndef match(self, other):\n    'Return true if this MediaType satisfies the given MediaType.'\n    for key in self.params.keys():\n        if ((key != 'q') and (other.params.get(key, None) != self.params.get(key, None))):\n            return False\n    if ((self.sub_type != '*') and (other.sub_type != '*') and (other.sub_type != self.sub_type)):\n        return False\n    if ((self.main_type != '*') and (other.main_type != '*') and (other.main_type != self.main_type)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_success(self):\n    count = CompatReport.objects.count()\n    r = self.client.post(self.url, self.json, content_type='application/json')\n    assert (r.status_code == 204)\n    assert (CompatReport.objects.count() == (count + 1))\n    cr = CompatReport.objects.order_by('-id')[0]\n    assert (cr.app_build == incoming_data['appBuild'])\n    assert (cr.app_guid == incoming_data['appGUID'])\n    assert (cr.works_properly == incoming_data['worksProperly'])\n    assert (cr.comments == incoming_data['comments'])\n    assert (cr.client_ip == '127.0.0.1')\n    vals = CompatReport.objects.filter(id=cr.id).values('other_addons')\n    assert (vals[0]['other_addons'] == json.dumps(incoming_data['otherAddons'], separators=(',', ':')))\n", "label": 0}
{"function": "\n\ndef get_prep_lookup(self, lookup_type, value, prepared=False):\n    ' Cleanup value for the jsonb lookup types\\n\\n        contains requires json encoded string\\n        has_any and has_all require array of string_types\\n        has requires string, but we can easily convert int to string\\n\\n        '\n    if (lookup_type in ['jcontains']):\n        if (not isinstance(value, six.string_types)):\n            value = json.dumps(value, cls=get_encoder_class(), **self._options)\n    if (lookup_type in ['jhas_any', 'jhas_all']):\n        if isinstance(value, six.string_types):\n            value = [value]\n        value = [('%s' % v) for v in value]\n    elif ((lookup_type in ['jhas']) and (not isinstance(value, six.string_types))):\n        if isinstance(value, six.integer_types):\n            value = str(value)\n        else:\n            raise TypeError('jhas lookup requires str or int')\n    return value\n", "label": 0}
{"function": "\n\ndef _read_structure(f, array_desc, struct_desc):\n    '\\n    Read a structure, with the array and structure descriptors given as\\n    `array_desc` and `structure_desc` respectively.\\n    '\n    nrows = array_desc['nelements']\n    columns = struct_desc['tagtable']\n    dtype = []\n    for col in columns:\n        if (col['structure'] or col['array']):\n            dtype.append(((col['name'].lower(), col['name']), np.object_))\n        elif (col['typecode'] in DTYPE_DICT):\n            dtype.append(((col['name'].lower(), col['name']), DTYPE_DICT[col['typecode']]))\n        else:\n            raise Exception(('Variable type %i not implemented' % col['typecode']))\n    structure = np.recarray((nrows,), dtype=dtype)\n    for i in range(nrows):\n        for col in columns:\n            dtype = col['typecode']\n            if col['structure']:\n                structure[col['name']][i] = _read_structure(f, struct_desc['arrtable'][col['name']], struct_desc['structtable'][col['name']])\n            elif col['array']:\n                structure[col['name']][i] = _read_array(f, dtype, struct_desc['arrtable'][col['name']])\n            else:\n                structure[col['name']][i] = _read_data(f, dtype)\n    if (array_desc['ndims'] > 1):\n        dims = array_desc['dims'][:int(array_desc['ndims'])]\n        dims.reverse()\n        structure = structure.reshape(dims)\n    return structure\n", "label": 1}
{"function": "\n\ndef run(self, show_trees=False):\n    '\\n        Sentences in the test suite are divided into two classes:\\n         - grammatical (``accept``) and\\n         - ungrammatical (``reject``).\\n        If a sentence should parse accordng to the grammar, the value of\\n        ``trees`` will be a non-empty list. If a sentence should be rejected\\n        according to the grammar, then the value of ``trees`` will be None.\\n        '\n    for test in self.suite:\n        print((test['doc'] + ':'), end=' ')\n        for key in ['accept', 'reject']:\n            for sent in test[key]:\n                tokens = sent.split()\n                trees = list(self.cp.parse(tokens))\n                if (show_trees and trees):\n                    print()\n                    print(sent)\n                    for tree in trees:\n                        print(tree)\n                if (key == 'accept'):\n                    if (trees == []):\n                        raise ValueError((\"Sentence '%s' failed to parse'\" % sent))\n                    else:\n                        accepted = True\n                elif trees:\n                    raise ValueError((\"Sentence '%s' received a parse'\" % sent))\n                else:\n                    rejected = True\n        if (accepted and rejected):\n            print('All tests passed!')\n", "label": 1}
{"function": "\n\ndef test_sqrt_rounding():\n    for i in [2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15]:\n        i = from_int(i)\n        for dps in [7, 15, 83, 106, 2000]:\n            mp.dps = dps\n            a = mpf_pow_int(mpf_sqrt(i, mp.prec, round_down), 2, mp.prec, round_down)\n            b = mpf_pow_int(mpf_sqrt(i, mp.prec, round_up), 2, mp.prec, round_up)\n            assert mpf_lt(a, i)\n            assert mpf_gt(b, i)\n    random.seed(1234)\n    prec = 100\n    for rnd in [round_down, round_nearest, round_ceiling]:\n        for i in range(100):\n            a = mpf_rand(prec)\n            b = mpf_mul(a, a)\n            assert (mpf_sqrt(b, prec, rnd) == a)\n    mp.dps = 100\n    a = (mpf(9) + 1e-90)\n    b = (mpf(9) - 1e-90)\n    mp.dps = 15\n    assert (sqrt(a, rounding='d') == 3)\n    assert (sqrt(a, rounding='n') == 3)\n    assert (sqrt(a, rounding='u') > 3)\n    assert (sqrt(b, rounding='d') < 3)\n    assert (sqrt(b, rounding='n') == 3)\n    assert (sqrt(b, rounding='u') == 3)\n    assert (sqrt(mpf('7.0503726185518891')) == mpf('2.655253776675949'))\n", "label": 1}
{"function": "\n\ndef optwrap(text):\n    'Wrap all paragraphs in the provided text.'\n    if (not BODY_WIDTH):\n        return text\n    assert wrap, 'Requires Python 2.3.'\n    result = ''\n    newlines = 0\n    for para in text.split('\\n'):\n        if (len(para) > 0):\n            if ((para[0] != ' ') and (para[0] != '-') and (para[0] != '*')):\n                for line in wrap(para, BODY_WIDTH):\n                    result += (line + '\\n')\n                result += '\\n'\n                newlines = 2\n            elif (not onlywhite(para)):\n                result += (para + '\\n')\n                newlines = 1\n        elif (newlines < 2):\n            result += '\\n'\n            newlines += 1\n    return result\n", "label": 1}
{"function": "\n\ndef _apply_path_joins(self, query, joins, path, inner_join=True):\n    '\\n            Apply join path to the query.\\n\\n            :param query:\\n                Query to add joins to\\n            :param joins:\\n                List of current joins. Used to avoid joining on same relationship more than once\\n            :param path:\\n                Path to be joined\\n            :param fn:\\n                Join function\\n        '\n    last = None\n    if path:\n        for item in path:\n            key = (inner_join, item)\n            alias = joins.get(key)\n            if (key not in joins):\n                if (not isinstance(item, Table)):\n                    alias = aliased(item.property.mapper.class_)\n                fn = (query.join if inner_join else query.outerjoin)\n                if (last is None):\n                    query = (fn(item) if (alias is None) else fn(alias, item))\n                else:\n                    prop = getattr(last, item.key)\n                    query = (fn(prop) if (alias is None) else fn(alias, prop))\n                joins[key] = alias\n            last = alias\n    return (query, joins, last)\n", "label": 0}
{"function": "\n\ndef maximalSquare(self, matrix):\n    if (not matrix):\n        return 0\n    (H, W) = (0, 1)\n    table = [[[0, 0] for j in xrange(len(matrix[0]))] for i in xrange(len(matrix))]\n    for i in reversed(xrange(len(matrix))):\n        for j in reversed(xrange(len(matrix[i]))):\n            if (matrix[i][j] == '1'):\n                (h, w) = (1, 1)\n                if ((i + 1) < len(matrix)):\n                    h = (table[(i + 1)][j][H] + 1)\n                if ((j + 1) < len(matrix[i])):\n                    w = (table[i][(j + 1)][W] + 1)\n                table[i][j] = [h, w]\n    s = [[0 for j in xrange(len(matrix[0]))] for i in xrange(len(matrix))]\n    max_square_area = 0\n    for i in reversed(xrange(len(matrix))):\n        for j in reversed(xrange(len(matrix[i]))):\n            side = min(table[i][j][H], table[i][j][W])\n            if (matrix[i][j] == '1'):\n                if (((i + 1) < len(matrix)) and ((j + 1) < len(matrix[(i + 1)]))):\n                    side = min((s[(i + 1)][(j + 1)] + 1), side)\n                s[i][j] = side\n                max_square_area = max(max_square_area, (side * side))\n    return max_square_area\n", "label": 1}
{"function": "\n\ndef download(self):\n    subdata = self.http.request('get', self.url, cookies=self.options.cookies)\n    data = None\n    if (self.subtype == 'tt'):\n        data = self.tt(subdata)\n    if (self.subtype == 'json'):\n        data = self.json(subdata)\n    if (self.subtype == 'sami'):\n        data = self.sami(subdata)\n    if (self.subtype == 'smi'):\n        data = self.smi(subdata)\n    if (self.subtype == 'wrst'):\n        data = self.wrst(subdata)\n    if (self.subtype == 'raw'):\n        if is_py2:\n            data = subdata.text.encode('utf-8')\n        else:\n            data = subdata.text\n    if ((platform.system() == 'Windows') and is_py3):\n        file_d = output(self.options, 'srt', mode='wt', encoding='utf-8')\n    else:\n        file_d = output(self.options, 'srt', mode='wt')\n    if (hasattr(file_d, 'read') is False):\n        return\n    file_d.write(data)\n    file_d.close()\n", "label": 1}
{"function": "\n\ndef flatten_data(self, resource_object, parser_context, is_list):\n    '\\n        Flattens data objects, making attributes and relationships fields the same level as id and type.\\n        '\n    relationships = resource_object.get('relationships')\n    is_relationship = parser_context.get('is_relationship')\n    request_method = parser_context['request'].method\n    if (is_relationship and (request_method == 'POST')):\n        if (not relationships):\n            raise JSONAPIException(source={\n                'pointer': '/data/relationships',\n            }, detail=NO_RELATIONSHIPS_ERROR)\n    elif (('attributes' not in resource_object) and (request_method != 'DELETE')):\n        raise JSONAPIException(source={\n            'pointer': '/data/attributes',\n        }, detail=NO_ATTRIBUTES_ERROR)\n    object_id = resource_object.get('id')\n    object_type = resource_object.get('type')\n    if (is_list and (request_method == 'DELETE')):\n        if (object_id is None):\n            raise JSONAPIException(source={\n                'pointer': '/data/id',\n            }, detail=NO_ID_ERROR)\n        if (object_type is None):\n            raise JSONAPIException(source={\n                'pointer': '/data/type',\n            }, detail=NO_TYPE_ERROR)\n    attributes = resource_object.get('attributes')\n    parsed = {\n        'id': object_id,\n        'type': object_type,\n    }\n    if attributes:\n        parsed.update(attributes)\n    if relationships:\n        relationships = self.flatten_relationships(relationships)\n        parsed.update(relationships)\n    return parsed\n", "label": 1}
{"function": "\n\ndef parse_options(self, arg):\n    '\\n        Parse options with the argv\\n\\n        :param arg: one arg from argv\\n        '\n    if (not arg.startswith('-')):\n        return False\n    value = None\n    if ('=' in arg):\n        (arg, value) = arg.split('=')\n    for option in self._option_list:\n        if (arg not in (option.shortname, option.longname)):\n            continue\n        action = option.action\n        if action:\n            action()\n        if (option.key == option.shortname):\n            self._results[option.key] = True\n            return True\n        if (option.boolean and option.default):\n            self._results[option.key] = False\n            return True\n        if option.boolean:\n            self._results[option.key] = True\n            return True\n        if (not value):\n            if self._argv:\n                value = self._argv[0]\n                self._argv = self._argv[1:]\n        if (not value):\n            raise RuntimeError(('Missing value for: %s' % option.name))\n        self._results[option.key] = option.to_python(value)\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef diagonalize(S, n, m):\n    for k in range(min(n, m)):\n        (val, i, j) = max(((abs(S[i][j]), i, j) for i in range(k, m) for j in range(k, n)))\n        if is_zero(val):\n            return k\n        (S[i], S[k]) = (S[k], S[i])\n        for r in range((m + 1)):\n            (S[r][j], S[r][k]) = (S[r][k], S[r][j])\n        pivot = float(S[k][k])\n        for j in range(k, (n + 1)):\n            S[k][j] /= pivot\n        for i in range(m):\n            if (i != k):\n                fact = S[i][k]\n                for j in range(k, (n + 1)):\n                    S[i][j] -= (fact * S[k][j])\n    return min(n, m)\n", "label": 1}
{"function": "\n\ndef rule_params_SA(parser, node, children):\n    params = {\n        \n    }\n    for (name, value) in children[0].items():\n        if (name not in ['skipws', 'ws']):\n            raise TextXSyntaxError('Invalid rule param \"{}\" at {}.'.format(name, parser.pos_to_linecol(node.position)))\n        if ((name == 'ws') and ('\\\\' in value)):\n            new_value = ''\n            if ('\\\\n' in value):\n                new_value += '\\n'\n            if ('\\\\r' in value):\n                new_value += '\\r'\n            if ('\\\\t' in value):\n                new_value += '\\t'\n            if (' ' in value):\n                new_value += ' '\n            value = new_value\n        params[name] = value\n    return params\n", "label": 0}
{"function": "\n\ndef test_define(self):\n    assert (self.l2_net_dev.forward.mode == 'nat')\n    self.l2_net_dev.define()\n    assert isinstance(self.l2_net_dev.uuid, str)\n    assert (len(self.l2_net_dev.uuid) == 36)\n    assert (self.l2_net_dev.network_name() == 'test_env_test_l2_net_dev')\n    assert (self.l2_net_dev.exists() is True)\n    assert (self.l2_net_dev.is_active() == 0)\n    assert (self.l2_net_dev.bridge_name() == 'virbr1')\n    assert (self.l2_net_dev._libvirt_network.autostart() == 1)\n    xml = self.l2_net_dev._libvirt_network.XMLDesc(0)\n    assert (xml == \"<network>\\n  <name>test_env_test_l2_net_dev</name>\\n  <uuid>{0}</uuid>\\n  <forward mode='nat'/>\\n  <bridge name='virbr1' stp='on' delay='0'/>\\n  <ip address='172.0.0.1' prefix='24'>\\n  </ip>\\n</network>\\n\".format(self.l2_net_dev.uuid))\n", "label": 1}
{"function": "\n\ndef sudoku(G):\n    'Solving Sudoku\\n\\n    :param G: integer matrix with 0 at empty cells\\n    :returns bool: True if grid could be solved\\n    :modifies: G will contain the solution\\n    :complexity: huge, but linear for usual published 9x9 grids\\n    '\n    global N, N2, N4\n    if (len(G) == 16):\n        (N, N2, N4) = (4, 16, 256)\n    e = (4 * N4)\n    univers = (e + 1)\n    S = [[rc(a), rv(a), cv(a), bv(a)] for a in range((N4 * N2))]\n    A = [e]\n    for r in range(N2):\n        for c in range(N2):\n            if (G[r][c] != 0):\n                a = assignation(r, c, (G[r][c] - 1))\n                A += S[a]\n    sol = dancing_links(univers, (S + [A]))\n    if sol:\n        for a in sol:\n            if (a < len(S)):\n                G[row(a)][col(a)] = (val(a) + 1)\n        return True\n    else:\n        return False\n", "label": 0}
{"function": "\n\ndef get_placeholders(template_name):\n    'Return a list of PlaceholderNode found in the given template.\\n\\n    :param template_name: the name of the template file\\n    '\n    dummy_context.template = template.Template('')\n    try:\n        temp_wrapper = template.loader.get_template(template_name)\n    except template.TemplateDoesNotExist:\n        return []\n    (plist, blist) = ([], [])\n    temp = temp_wrapper.template\n    _placeholders_recursif(temp.nodelist, plist, blist)\n    previous = {\n        \n    }\n    block_to_remove = []\n    for block in blist:\n        if (block.name in previous):\n            if (not hasattr(block, 'has_super_var')):\n                block_to_remove.append(previous[block.name])\n        previous[block.name] = block\n\n    def keep(p):\n        return (not (p.found_in_block in block_to_remove))\n    placeholders = [p for p in plist if keep(p)]\n    names = []\n    pfiltered = []\n    for p in placeholders:\n        if (p.ctype not in names):\n            pfiltered.append(p)\n            names.append(p.ctype)\n    return pfiltered\n", "label": 0}
{"function": "\n\ndef _is_removing_self_admin_role(self, request, project_id, user_id, available_roles, current_role_ids):\n    is_current_user = (user_id == request.user.id)\n    is_current_project = (project_id == request.user.tenant_id)\n    _admin_roles = self.get_admin_roles()\n    available_admin_role_ids = [role.id for role in available_roles if (role.name.lower() in _admin_roles)]\n    admin_roles = [role for role in current_role_ids if (role in available_admin_role_ids)]\n    if len(admin_roles):\n        removing_admin = any([(role in current_role_ids) for role in admin_roles])\n    else:\n        removing_admin = False\n    if (is_current_user and is_current_project and removing_admin):\n        msg = _('You cannot revoke your administrative privileges from the project you are currently logged into. Please switch to another project with administrative privileges or remove the administrative role manually via the CLI.')\n        messages.warning(request, msg)\n        return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef test_generate_with_params(self):\n    api = APISpecification(version='v1', base_url='http://api.globo.com')\n    api.add_resource(Resource('dogs', paths=[Path('/dogs/{key}', params=[Param('key')], methods=[Method('POST'), Method('GET')])]))\n    result = self.gen(api)\n    doc = ElementTree.fromstring(result)\n    resources = doc.getchildren()[0]\n    resource = resources.getchildren()[0]\n    param = resource.getchildren()[0]\n    assert param.tag.endswith('param')\n    assert (param.get('name') == 'key')\n    assert (param.get('required') == 'true')\n    assert (param.get('type') == 'xsd:string')\n    assert (param.get('style') == 'template')\n    method_1 = resource.getchildren()[1]\n    assert method_1.tag.endswith('method')\n    assert (method_1.get('name') == 'POST')\n    method_2 = resource.getchildren()[2]\n    assert method_2.tag.endswith('method')\n    assert (method_2.get('name') == 'GET')\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I64):\n                self.latency_ns = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I64):\n                self.cpu_time_ns = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.cardinality = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I64):\n                self.memory_used = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _list_xml_members(cls):\n    \"Generator listing all members which are XML elements or attributes.\\n    \\n    The following members would be considered XML members:\\n    foo = 'abc' - indicates an XML attribute with the qname abc\\n    foo = SomeElement - indicates an XML child element\\n    foo = [AnElement] - indicates a repeating XML child element, each instance\\n        will be stored in a list in this member\\n    foo = ('att1', '{http://example.com/namespace}att2') - indicates an XML\\n        attribute which has different parsing rules in different versions of \\n        the protocol. Version 1 of the XML parsing rules will look for an\\n        attribute with the qname 'att1' but verion 2 of the parsing rules will\\n        look for a namespaced attribute with the local name of 'att2' and an\\n        XML namespace of 'http://example.com/namespace'.\\n    \"\n    members = []\n    for pair in inspect.getmembers(cls):\n        if ((not pair[0].startswith('_')) and (pair[0] != 'text')):\n            member_type = pair[1]\n            if (isinstance(member_type, tuple) or isinstance(member_type, list) or isinstance(member_type, (str, unicode)) or (inspect.isclass(member_type) and issubclass(member_type, XmlElement))):\n                members.append(pair)\n    return members\n", "label": 0}
{"function": "\n\ndef resolve(url):\n    try:\n        url = url.split('/preview', 1)[0]\n        url = url.replace('drive.google.com', 'docs.google.com')\n        result = client.request(url)\n        result = re.compile('\"fmt_stream_map\",(\".+?\")').findall(result)[0]\n        u = json.loads(result)\n        u = [i.split('|')[(- 1)] for i in u.split(',')]\n        u = sum([tag(i) for i in u], [])\n        url = []\n        try:\n            url += [[i for i in u if (i['quality'] == '1080p')][0]]\n        except:\n            pass\n        try:\n            url += [[i for i in u if (i['quality'] == 'HD')][0]]\n        except:\n            pass\n        try:\n            url += [[i for i in u if (i['quality'] == 'SD')][0]]\n        except:\n            pass\n        if (url == []):\n            return\n        return url\n    except:\n        return\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    if ('REQUEST_URI' not in environ):\n        environ['REQUEST_URI'] = (quote(environ.get('SCRIPT_NAME', '')) + quote(environ.get('PATH_INFO', '')))\n    if self.include_os_environ:\n        cgi_environ = os.environ.copy()\n    else:\n        cgi_environ = {\n            \n        }\n    for name in environ:\n        if ((name.upper() == name) and isinstance(environ[name], str)):\n            cgi_environ[name] = environ[name]\n    if (self.query_string is not None):\n        old = cgi_environ.get('QUERY_STRING', '')\n        if old:\n            old += '&'\n        cgi_environ['QUERY_STRING'] = (old + self.query_string)\n    cgi_environ['SCRIPT_FILENAME'] = self.script\n    proc = subprocess.Popen([self.script], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=cgi_environ, cwd=os.path.dirname(self.script))\n    writer = CGIWriter(environ, start_response)\n    if (select and (sys.platform != 'win32')):\n        proc_communicate(proc, stdin=StdinReader.from_environ(environ), stdout=writer, stderr=environ['wsgi.errors'])\n    else:\n        (stdout, stderr) = proc.communicate(StdinReader.from_environ(environ).read())\n        if stderr:\n            environ['wsgi.errors'].write(stderr)\n        writer.write(stdout)\n    if (not writer.headers_finished):\n        start_response(writer.status, writer.headers)\n    return []\n", "label": 1}
{"function": "\n\ndef values_from_response(self, response, sreg_names=None, ax_names=None):\n    'Return values from SimpleRegistration response or\\n        AttributeExchange response if present.\\n\\n        @sreg_names and @ax_names must be a list of name and aliases\\n        for such name. The alias will be used as mapping key.\\n        '\n    values = {\n        \n    }\n    if sreg_names:\n        resp = sreg.SRegResponse.fromSuccessResponse(response)\n        if resp:\n            values.update(((alias, (resp.get(name) or '')) for (name, alias) in sreg_names))\n    if ax_names:\n        resp = ax.FetchResponse.fromSuccessResponse(response)\n        if resp:\n            for (src, alias) in ax_names:\n                name = alias.replace('old_', '')\n                values[name] = (resp.getSingle(src, '') or values.get(name))\n    return values\n", "label": 0}
{"function": "\n\ndef collect(self):\n    '\\n        Collect metrics\\n        '\n    collected = {\n        \n    }\n    files = []\n    if isinstance(self.config['dir'], basestring):\n        dirs = [d.strip() for d in self.config['dir'].split(',')]\n    elif isinstance(self.config['dir'], list):\n        dirs = self.config['dir']\n    if isinstance(self.config['files'], basestring):\n        files = [f.strip() for f in self.config['files'].split(',')]\n    elif isinstance(self.config['files'], list):\n        files = self.config['files']\n    for sdir in dirs:\n        for sfile in files:\n            if sfile.endswith('conntrack_count'):\n                metric_name = 'ip_conntrack_count'\n            elif sfile.endswith('conntrack_max'):\n                metric_name = 'ip_conntrack_max'\n            else:\n                self.log.error('Unknown file for collection: %s', sfile)\n                continue\n            fpath = os.path.join(sdir, sfile)\n            if (not os.path.exists(fpath)):\n                continue\n            try:\n                with open(fpath, 'r') as fhandle:\n                    metric = float(fhandle.readline().rstrip('\\n'))\n                    collected[metric_name] = metric\n            except Exception as exception:\n                self.log.error(\"Failed to collect from '%s': %s\", fpath, exception)\n    if (not collected):\n        self.log.error('No metric was collected, looks like nf_conntrack/ip_conntrack kernel module was not loaded')\n    else:\n        for key in collected.keys():\n            self.publish(key, collected[key])\n", "label": 1}
{"function": "\n\ndef set_content(self, *values):\n    ' Sets the content of a view.\\n        '\n    content = []\n    accum = []\n    for value in values:\n        if isinstance(value, ViewSubElement):\n            content.append(value)\n        elif (type(value) in SequenceTypes):\n            content.append(Group(*value))\n        elif (isinstance(value, basestring) and (value[:1] == '<') and (value[(- 1):] == '>')):\n            content.append(Include(value[1:(- 1)].strip()))\n        else:\n            content.append(Item(value))\n    for item in content:\n        if isinstance(item, Item):\n            content = [Group(*content)]\n            break\n    self.content = Group(*content, container=self)\n", "label": 0}
{"function": "\n\ndef arg2nodes(self, args, node_factory=_null, lookup_list=_null, **kw):\n    if (node_factory is _null):\n        node_factory = self.fs.File\n    if (lookup_list is _null):\n        lookup_list = self.lookup_list\n    if (not args):\n        return []\n    args = SCons.Util.flatten(args)\n    nodes = []\n    for v in args:\n        if SCons.Util.is_String(v):\n            n = None\n            for l in lookup_list:\n                n = l(v)\n                if (n is not None):\n                    break\n            if (n is not None):\n                if SCons.Util.is_String(n):\n                    kw['raw'] = 1\n                    n = self.subst(n, **kw)\n                    if node_factory:\n                        n = node_factory(n)\n                if SCons.Util.is_List(n):\n                    nodes.extend(n)\n                else:\n                    nodes.append(n)\n            elif node_factory:\n                kw['raw'] = 1\n                v = node_factory(self.subst(v, **kw))\n                if SCons.Util.is_List(v):\n                    nodes.extend(v)\n                else:\n                    nodes.append(v)\n        else:\n            nodes.append(v)\n    return nodes\n", "label": 1}
{"function": "\n\ndef equals(self, other):\n    other = sympify(other)\n    if (isinstance(other, TensMul) and (other._coeff == 0)):\n        return all(((x._coeff == 0) for x in self.args))\n    if isinstance(other, TensExpr):\n        if (self.rank != other.rank):\n            return False\n    if isinstance(other, TensAdd):\n        if (set(self.args) != set(other.args)):\n            return False\n        else:\n            return True\n    t = (self - other)\n    if (not isinstance(t, TensExpr)):\n        return (t == 0)\n    elif isinstance(t, TensMul):\n        return (t._coeff == 0)\n    else:\n        return all(((x._coeff == 0) for x in t.args))\n", "label": 1}
{"function": "\n\ndef download_default_pages(names, prefix):\n    from httplib import HTTPSConnection\n    host = 'trac.edgewall.org'\n    if (prefix and (not prefix.endswith('/'))):\n        prefix += '/'\n    conn = HTTPSConnection(host)\n    for name in names:\n        if (name in ('WikiStart', 'SandBox')):\n            continue\n        sys.stdout.write(('Downloading %s%s' % (prefix, name)))\n        conn.request('GET', ('/wiki/%s%s?format=txt' % (prefix, name)))\n        response = conn.getresponse()\n        content = response.read()\n        if (prefix and ((response.status != 200) or (not content))):\n            sys.stdout.write((' %s' % name))\n            conn.request('GET', ('/wiki/%s?format=txt' % name))\n            response = conn.getresponse()\n            content = response.read()\n        if ((response.status == 200) and content):\n            with open(('trac/wiki/default-pages/' + name), 'w') as f:\n                lines = content.replace('\\r\\n', '\\n').splitlines(True)\n                f.write(''.join((line for line in lines if (line.strip() != '[[TranslatedPages]]'))))\n            sys.stdout.write('\\tdone.\\n')\n        else:\n            sys.stdout.write('\\tmissing or empty.\\n')\n    conn.close()\n", "label": 1}
{"function": "\n\ndef test_reset_repeating(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, repeat=True, _time_function=time)\n    time.time = 12\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 8)\n    timer.reset()\n    assert (timer.sleep_time() == 10)\n    time.time = 34\n    assert (timer.sleep_time() == 0)\n    timer.reset()\n    assert (timer.sleep_time() == 10)\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 10)\n", "label": 1}
{"function": "\n\ndef import_(module, objects=None, via=None):\n    '\\n    :param module: py3 compatiable module path\\n    :param objects: objects want to imported, it should be a list\\n    :param via: for some py2 module, you should give the import path according the\\n        objects which you want to imported\\n    :return: object or module\\n    '\n    if PY3:\n        mod = __import__(module, fromlist=['*'])\n    else:\n        path = modules_mapping.get(module)\n        if (not path):\n            raise Exception((\"Can't find the module %s in mappings.\" % module))\n        if isinstance(path, list):\n            if (not via):\n                raise Exception('You should give a via parameter to enable import from py2.')\n            path = via\n        mod = __import__(path, fromlist=['*'])\n    if objects:\n        if (not isinstance(objects, (list, tuple))):\n            raise Exception('objects parameter should be a list or tuple.')\n        r = []\n        for x in objects:\n            r.append(getattr(mod, x))\n        if (len(r) > 1):\n            return tuple(r)\n        else:\n            return r[0]\n    else:\n        return mod\n", "label": 0}
{"function": "\n\ndef _process_axis_and_grid(plot, axis_type, axis_location, minor_ticks, axis_label, rng, dim):\n    axiscls = _get_axis_class(axis_type, rng)\n    if axiscls:\n        if (axiscls is LogAxis):\n            if (dim == 0):\n                plot.x_mapper_type = 'log'\n            elif (dim == 1):\n                plot.y_mapper_type = 'log'\n            else:\n                raise ValueError(('received invalid dimension value: %r' % dim))\n        axis = axiscls(plot=(plot if axis_location else None))\n        if isinstance(axis.ticker, ContinuousTicker):\n            axis.ticker.num_minor_ticks = _get_num_minor_ticks(axiscls, minor_ticks)\n        axis_label = axis_label\n        if axis_label:\n            axis.axis_label = axis_label\n        grid = Grid(plot=plot, dimension=dim, ticker=axis.ticker)\n        grid\n        if (axis_location is not None):\n            getattr(plot, axis_location).append(axis)\n", "label": 0}
{"function": "\n\ndef parse(self, path):\n    try:\n        if (path.startswith('http://') or path.startswith('https://')):\n            if ('requests' not in IMPORTS):\n                e = 'HTTP library not found: requests'\n                raise ImportError(e)\n            headers = {\n                'User-Agent': 'Mozilla/5.0 Gecko Firefox',\n            }\n            r = requests.get(path, headers=headers)\n            r.raise_for_status()\n            f = StringIO(r.content)\n            self.parser_func(f, path)\n            return\n        elif os.path.isfile(path):\n            with open(path, 'rb') as f:\n                self.parser_func(f, path)\n            return\n        elif os.path.isdir(path):\n            for (walk_root, walk_dirs, walk_files) in os.walk(path):\n                for walk_file in fnmatch.filter(walk_files, self.ext_filter):\n                    fpath = os.path.join(walk_root, walk_file)\n                    with open(fpath, 'rb') as f:\n                        self.parser_func(f, fpath)\n            return\n        e = ('File path is not a file, directory or URL: %s' % path)\n        raise IOError(e)\n    except (KeyboardInterrupt, SystemExit):\n        raise\n    except Exception as e:\n        self.handler.print_error(path, e)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _item_style_sheet_changed(cls, item):\n    item_styles = cls._item_styles\n    item_sheets = cls._item_style_sheets\n    items = [i for i in item.traverse() if (i in cls._queried_items)]\n    for item in items:\n        sheets = item_sheets.pop(item, None)\n        if (sheets is not None):\n            sheet_items = cls._style_sheet_items\n            for sheet in sheets:\n                if (sheet in sheet_items):\n                    sheet_items[sheet].discard(item)\n        styles = item_styles.pop(item, None)\n        if (styles is not None):\n            style_items = cls._style_items\n            for style in styles:\n                if (style in style_items):\n                    style_items[style].discard(item)\n    cls._request_restyle(items)\n", "label": 1}
{"function": "\n\ndef require_instances(self, instances=(), instance_ids=()):\n    used = {(instance.id, instance.src) for instance in self.instances}\n    for instance in instances:\n        if ((instance.id, instance.src) not in used):\n            self.instances.append(Instance(id=instance.id, src=instance.src))\n            if (len(self.instances) == 1):\n                instance_node = self.node.find('instance')\n                command_node = self.node.find('command')\n                self.node.remove(instance_node)\n                self.node.insert((self.node.index(command_node) + 1), instance_node)\n    covered_ids = {instance_id for (instance_id, _) in used}\n    for instance_id in instance_ids:\n        if (instance_id not in covered_ids):\n            raise UnknownInstanceError('Instance reference not recognized: {} in xpath \"{}\"'.format(instance_id, getattr(instance_id, 'xpath', '(Xpath Unknown)')))\n    sorted_instances = sorted(self.instances, key=(lambda instance: instance.id))\n    if (sorted_instances != self.instances):\n        self.instances = sorted_instances\n", "label": 0}
{"function": "\n\n@classmethod\ndef eval(cls, n):\n    n = sympify(n)\n    if n.is_Number:\n        if (n is S.Zero):\n            return S.One\n        elif (n is S.Infinity):\n            return S.Infinity\n        elif n.is_Integer:\n            if n.is_negative:\n                return S.ComplexInfinity\n            else:\n                n = n.p\n                if (n < 20):\n                    if (not cls._small_factorials):\n                        result = 1\n                        for i in range(1, 20):\n                            result *= i\n                            cls._small_factorials.append(result)\n                    result = cls._small_factorials[(n - 1)]\n                else:\n                    bits = bin(n).count('1')\n                    result = (cls._recursive(n) * (2 ** (n - bits)))\n                return Integer(result)\n", "label": 0}
{"function": "\n\ndef email(request, form_class=AddEmailForm, template_name='account/email.html'):\n    if ((request.method == 'POST') and request.user.is_authenticated()):\n        if (request.POST['action'] == 'add'):\n            add_email_form = form_class(request.user, request.POST)\n            if add_email_form.is_valid():\n                add_email_form.save()\n                add_email_form = form_class()\n        else:\n            add_email_form = form_class()\n            if (request.POST['action'] == 'send'):\n                email = request.POST['email']\n                try:\n                    email_address = EmailAddress.objects.get(user=request.user, email=email)\n                    request.user.message_set.create(message=('Confirmation email sent to %s' % email))\n                    EmailConfirmation.objects.send_confirmation(email_address)\n                except EmailAddress.DoesNotExist:\n                    pass\n            elif (request.POST['action'] == 'remove'):\n                email = request.POST['email']\n                try:\n                    email_address = EmailAddress.objects.get(user=request.user, email=email)\n                    email_address.delete()\n                    request.user.message_set.create(message=('Removed email address %s' % email))\n                except EmailAddress.DoesNotExist:\n                    pass\n            elif (request.POST['action'] == 'primary'):\n                email = request.POST['email']\n                email_address = EmailAddress.objects.get(user=request.user, email=email)\n                email_address.set_as_primary()\n    else:\n        add_email_form = form_class()\n    return render_to_response(template_name, {\n        'add_email_form': add_email_form,\n    }, context_instance=RequestContext(request))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.groupName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_versions(default=DEFAULT, verbose=False):\n    assert (versionfile_source is not None), 'please set versioneer.versionfile_source'\n    assert (tag_prefix is not None), 'please set versioneer.tag_prefix'\n    assert (parentdir_prefix is not None), 'please set versioneer.parentdir_prefix'\n    root = get_root()\n    versionfile_abs = os.path.join(root, versionfile_source)\n    variables = get_expanded_variables(versionfile_abs)\n    if variables:\n        ver = versions_from_expanded_variables(variables, tag_prefix)\n        if ver:\n            if verbose:\n                print(('got version from expanded variable %s' % ver))\n            return ver\n    ver = versions_from_file(versionfile_abs)\n    if ver:\n        if verbose:\n            print(('got version from file %s %s' % (versionfile_abs, ver)))\n        return ver\n    ver = versions_from_vcs(tag_prefix, root, verbose)\n    if ver:\n        if verbose:\n            print(('got version from git %s' % ver))\n        return ver\n    ver = versions_from_parentdir(parentdir_prefix, root, verbose)\n    if ver:\n        if verbose:\n            print(('got version from parentdir %s' % ver))\n        return ver\n    if verbose:\n        print(('got version from default %s' % ver))\n    return default\n", "label": 1}
{"function": "\n\ndef test_assertRaises(self):\n\n    def _raise(e):\n        raise e\n    self.assertRaises(KeyError, _raise, KeyError)\n    self.assertRaises(KeyError, _raise, KeyError('key'))\n    try:\n        self.assertRaises(KeyError, (lambda : None))\n    except self.failureException as e:\n        self.assertIn('KeyError not raised', str(e))\n    else:\n        self.fail(\"assertRaises() didn't fail\")\n    try:\n        self.assertRaises(KeyError, _raise, ValueError)\n    except ValueError:\n        pass\n    else:\n        self.fail(\"assertRaises() didn't let exception pass through\")\n    with self.assertRaises(KeyError) as cm:\n        try:\n            raise KeyError\n        except Exception as e:\n            exc = e\n            raise\n    self.assertIs(cm.exception, exc)\n    with self.assertRaises(KeyError):\n        raise KeyError('key')\n    try:\n        with self.assertRaises(KeyError):\n            pass\n    except self.failureException as e:\n        self.assertIn('KeyError not raised', str(e))\n    else:\n        self.fail(\"assertRaises() didn't fail\")\n    try:\n        with self.assertRaises(KeyError):\n            raise ValueError\n    except ValueError:\n        pass\n    else:\n        self.fail(\"assertRaises() didn't let exception pass through\")\n", "label": 1}
{"function": "\n\ndef match(self, left, collected=None):\n    collected = ([] if (collected is None) else collected)\n    args = [l for l in left if (type(l) is Argument)]\n    if (not len(args)):\n        return (False, left, collected)\n    left.remove(args[0])\n    if (type(self.value) is not list):\n        return (True, left, (collected + [Argument(self.name, args[0].value)]))\n    same_name = [a for a in collected if ((type(a) is Argument) and (a.name == self.name))]\n    if len(same_name):\n        same_name[0].value += [args[0].value]\n        return (True, left, collected)\n    else:\n        return (True, left, (collected + [Argument(self.name, [args[0].value])]))\n", "label": 1}
{"function": "\n\ndef indication(self, pdu):\n    if _debug:\n        MiddleMan._debug('indication %r', pdu)\n    if (not pdu.pduData):\n        stop()\n        return\n    line = pdu.pduData.decode('utf_8')[:(- 1)]\n    if _debug:\n        MiddleMan._debug('    - line: %r', line)\n    line_parts = line.split(' ', 1)\n    if _debug:\n        MiddleMan._debug('    - line_parts: %r', line_parts)\n    if (len(line_parts) != 2):\n        sys.stderr.write(('err: invalid line: %r\\n' % (line,)))\n        return\n    (addr, msg) = line_parts\n    if (addr == '*'):\n        dest = local_broadcast_tuple\n    elif (':' in addr):\n        (addr, port) = addr.split(':')\n        if (addr == '*'):\n            dest = (local_broadcast_tuple[0], int(port))\n        else:\n            dest = (addr, int(port))\n    else:\n        dest = (addr, local_unicast_tuple[1])\n    if _debug:\n        MiddleMan._debug('    - dest: %r', dest)\n    try:\n        self.request(PDU(msg.encode('utf_8'), destination=dest))\n    except Exception as err:\n        sys.stderr.write(('err: %r\\n' % (err,)))\n        return\n", "label": 1}
{"function": "\n\ndef single_step(s, x86_64=False):\n    i = fetch_instruction(s, x86_64)\n    if (i is None):\n        return []\n    if (i.address in _hit_count):\n        hc = _hit_count[i.address] = (_hit_count[i.address] + 1)\n    else:\n        hc = _hit_count[i.address] = 1\n    if (i.address in s.symbols):\n        symbol = s.symbols[i.address]\n        if (s.symbols[i.address] in s.function_hooks):\n            ss = s.function_hooks[symbol](s)\n            for s in ss:\n                if (s.solver.solve_time() > 30):\n                    s.log.warning('concretising (last solve took: {}s)', s.solver.solve_time)\n                    s.solver.concretise()\n                s.clear_il_state()\n            return ss\n        else:\n            s.log.function_call(None, symbol)\n    s.log.native_instruction(hc, i, x86_64)\n    max_il_index = len(i.il_instructions)\n    states = [s]\n    exit_states = []\n    while (len(states) > 0):\n        s = states.pop()\n        if (s.il_index >= max_il_index):\n            if (s.solver.solve_time() > 30):\n                s.log.warning('concretising (last solve took: {}s)'.format(s.solver.solve_time()))\n                s.solver.concretise()\n            s.clear_il_state()\n            exit_states.append(s)\n            continue\n        ri = i.il_instructions[s.il_index]\n        s.il_index += 1\n        s.log.reil_instruction(ri)\n        states += reil_single_step(ri, s)\n    return exit_states\n", "label": 1}
{"function": "\n\ndef test_profile(self):\n\n    def g():\n        pass\n\n    def f():\n        g()\n\n    def main():\n        f()\n        f()\n        f()\n    with tracebin.record() as recorder:\n        main()\n    assert (recorder.calls is None)\n    start = time.time()\n    with tracebin.record(profile=True) as recorder:\n        main()\n    assert (len(recorder.calls) == 3)\n    [call1, call2, call3] = recorder.calls\n    assert (call1.func_name == 'high_res_time')\n    assert (len(call1.subcalls) == 1)\n    assert (call2.func_name == 'main')\n    assert ((call2.start_time - start) < 1)\n    assert ((call2.end_time - call2.start_time) < 1)\n    assert (len(call2.subcalls) == 3)\n    assert ({c.func_name for c in call2.subcalls} == {'f'})\n    assert (call2.subcalls[1].subcalls[0].func_name == 'g')\n", "label": 1}
{"function": "\n\ndef checkNode(graph, superpipeline, source, target, nodeType, expectedDataType, values, isInput):\n    errors = er.consistencyErrors()\n    data = superpipeline.pipelineConfigurationData[superpipeline.pipeline]\n    longFormArgument = graph.CM_getArgumentAttribute(graph.graph, source, target, 'longFormArgument')\n    dataType = graph.CM_getArgumentAttribute(graph.graph, source, target, 'dataType')\n    isLinkOnly = graph.CM_getArgumentAttribute(graph.graph, source, target, 'isLinkOnly')\n    if (not isLinkOnly):\n        if (not expectedDataType):\n            expectedDataType = dataType\n        elif (expectedDataType != dataType):\n            print('dataConsistency.checkNode - 1', dataType, expectedDataType)\n            exit(0)\n        for value in values:\n            if (not isCorrectDataType(value, expectedDataType)):\n                print('dataConsistency.checkNode - 2', longFormArgument, value, dataType, type(value))\n                exit(0)\n            if (nodeType == 'file'):\n                if (not graph.CM_getArgumentAttribute(graph.graph, source, target, 'isStub')):\n                    extensions = graph.CM_getArgumentAttribute(graph.graph, source, target, 'extensions')\n                    if extensions:\n                        task = (target if isInput else source)\n                        fileNodeId = (source if isInput else target)\n                        if (not checkExtensions(value, extensions)):\n                            if (longFormArgument in data.longFormArguments.keys()):\n                                shortFormArgument = data.longFormArguments[longFormArgument].shortFormArgument\n                                errors.invalidExtensionPipeline(longFormArgument, shortFormArgument, value, extensions)\n                            else:\n                                shortFormArgument = graph.CM_getArgumentAttribute(graph.graph, source, target, 'shortFormArgument')\n                                errors.invalidExtension(task, longFormArgument, shortFormArgument, value, extensions)\n        return expectedDataType\n", "label": 1}
{"function": "\n\ndef execute():\n    for table in frappe.db.get_tables():\n        doctype = table[3:]\n        if frappe.db.exists('DocType', doctype):\n            fieldnames = [df['fieldname'] for df in frappe.get_all('DocField', fields=['fieldname'], filters={\n                'parent': doctype,\n            })]\n            custom_fieldnames = [df['fieldname'] for df in frappe.get_all('Custom Field', fields=['fieldname'], filters={\n                'dt': doctype,\n            })]\n        else:\n            fieldnames = custom_fieldnames = []\n        for column in frappe.db.sql('desc `{0}`'.format(table), as_dict=True):\n            if (column['Type'] == 'int(1)'):\n                fieldname = column['Field']\n                if (not ((fieldname in default_fields) or (fieldname in fieldnames) or (fieldname in custom_fieldnames))):\n                    continue\n                frappe.db.sql('update `{table}` set `{column}`=0 where `{column}` is null'.format(table=table, column=fieldname))\n                frappe.db.commit()\n                frappe.db.sql_ddl('alter table `{table}`\\n\\t\\t\\t\\t\\tmodify `{column}` int(1) not null default {default}'.format(table=table, column=fieldname, default=cint(column['Default'])))\n", "label": 1}
{"function": "\n\ndef tag(self, data):\n    now = [((('', 'BOS'), ('', 'BOS')), 0.0, [])]\n    for w in data:\n        stage = {\n            \n        }\n        not_found = True\n        for s in self.status:\n            if (self.uni.freq((w, s)) != 0):\n                not_found = False\n                break\n        if not_found:\n            for s in self.status:\n                for pre in now:\n                    stage[(pre[0][1], (w, s))] = (pre[1], (pre[2] + [s]))\n            now = list(map((lambda x: (x[0], x[1][0], x[1][1])), stage.items()))\n            continue\n        for s in self.status:\n            for pre in now:\n                p = (pre[1] + self.log_prob(pre[0][0], pre[0][1], (w, s)))\n                if ((not ((pre[0][1], (w, s)) in stage)) or (p > stage[(pre[0][1], (w, s))][0])):\n                    stage[(pre[0][1], (w, s))] = (p, (pre[2] + [s]))\n        now = list(map((lambda x: (x[0], x[1][0], x[1][1])), stage.items()))\n    return zip(data, max(now, key=(lambda x: x[1]))[2])\n", "label": 1}
{"function": "\n\ndef _write_viewing_info(self, group):\n    if ((self.peeloff_origin is not None) and (self.inside_observer is not None)):\n        raise Exception('Cannot specify inside observer and peeloff origin at the same time')\n    if (self.inside_observer is not None):\n        group.attrs['inside_observer'] = bool2str(True)\n        self._write_inside_observer(group)\n        if (self.viewing_angles == []):\n            self.set_viewing_angles([90.0], [0.0])\n        if (self.image and (self.xmin < self.xmax)):\n            raise ValueError('longitudes should increase towards the left for inside observers')\n        if (self.d_min < 0.0):\n            if (self.d_min != (- np.inf)):\n                raise ValueError('Lower limit of depth should be positive for inside observer')\n            self.d_min = 0.0\n        if (self.d_max < 0.0):\n            raise ValueError('Upper limit of depth should be positive for inside observer')\n    elif (len(self.viewing_angles) > 0):\n        group.attrs['inside_observer'] = bool2str(False)\n        if (self.peeloff_origin is None):\n            self.set_peeloff_origin((0.0, 0.0, 0.0))\n        self._write_peeloff_origin(group)\n    else:\n        raise Exception('Need to specify either observer position, or viewing angles')\n    self._write_ignore_optical_depth(group)\n    self._write_viewing_angles(group)\n    self._write_depth(group)\n", "label": 1}
{"function": "\n\ndef line_contains_open_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)/\\\\*', line) and (not re.match('^(\\\\+|\\\\-)(.*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(.*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef _get_container_description(self, state, name, network_state=True, ip_partitions=None):\n    state_container = state.containers[name]\n    container_id = state_container['id']\n    try:\n        container = self.docker_client.inspect_container(container_id)\n    except docker.errors.APIError as err:\n        if (err.response.status_code == 404):\n            return Container(name, container_id, ContainerState.MISSING)\n        else:\n            raise\n    state_dict = container.get('State')\n    if (state_dict and state_dict.get('Running')):\n        container_state = ContainerState.UP\n    else:\n        container_state = ContainerState.DOWN\n    extras = {\n        \n    }\n    network = container.get('NetworkSettings')\n    ip = None\n    if network:\n        ip = network.get('IPAddress')\n        if ip:\n            extras['ip_address'] = ip\n    if (network_state and (name in state.containers) and (container_state == ContainerState.UP)):\n        device = state_container['device']\n        extras['device'] = device\n        extras['network_state'] = self.network.network_state(device)\n        if (ip_partitions and ip):\n            extras['partition'] = ip_partitions.get(ip)\n    else:\n        extras['network_state'] = NetworkState.UNKNOWN\n        extras['device'] = None\n    cfg_container = self.config.containers.get(name)\n    extras['neutral'] = (cfg_container.neutral if cfg_container else False)\n    extras['holy'] = (cfg_container.holy if cfg_container else False)\n    return Container(name, container_id, container_state, **extras)\n", "label": 1}
{"function": "\n\ndef _filter_metadata(metadata, **kwargs):\n    if (not isinstance(metadata, dict)):\n        return metadata\n    filtered_metadata = {\n        \n    }\n    for (key, value) in metadata.items():\n        if (key == '_self'):\n            default_value = value.get('default_value', None)\n            if (default_value is None):\n                default_callback_params = value.get('default_callback_params', {\n                    \n                })\n                callback_params = dict(kwargs)\n                if default_callback_params:\n                    callback_params.update(default_callback_params)\n                default_callback = value.get('default_callback', None)\n                if default_callback:\n                    default_value = default_callback(key, **callback_params)\n            options = value.get('options', None)\n            if (options is None):\n                options_callback_params = value.get('options_callback_params', {\n                    \n                })\n                callback_params = dict(kwargs)\n                if options_callback_params:\n                    callback_params.update(options_callback_params)\n                options_callback = value.get('options_callback', None)\n                if options_callback:\n                    options = options_callback(key, **callback_params)\n            filtered_metadata[key] = value\n            if (default_value is not None):\n                filtered_metadata[key]['default_value'] = default_value\n            if (options is not None):\n                filtered_metadata[key]['options'] = options\n        else:\n            filtered_metadata[key] = _filter_metadata(value, **kwargs)\n    return filtered_metadata\n", "label": 1}
{"function": "\n\ndef server_list(request, search_opts=None, all_tenants=False):\n    page_size = utils.get_page_size(request)\n    c = novaclient(request)\n    paginate = False\n    if (search_opts is None):\n        search_opts = {\n            \n        }\n    elif ('paginate' in search_opts):\n        paginate = search_opts.pop('paginate')\n        if paginate:\n            search_opts['limit'] = (page_size + 1)\n    if all_tenants:\n        search_opts['all_tenants'] = True\n    else:\n        search_opts['project_id'] = request.user.tenant_id\n    servers = [Server(s, request) for s in c.servers.list(True, search_opts)]\n    has_more_data = False\n    if (paginate and (len(servers) > page_size)):\n        servers.pop((- 1))\n        has_more_data = True\n    elif (paginate and (len(servers) == getattr(settings, 'API_RESULT_LIMIT', 1000))):\n        has_more_data = True\n    return (servers, has_more_data)\n", "label": 1}
{"function": "\n\ndef query_library(self, query, tie_breaker=no_tiebreak, modifiers=None, auto=False):\n    'Queries the library for songs.\\n        returns a list of matches, or None.\\n        '\n    if (not modifiers):\n        modifiers = []\n    try:\n        if (not auto):\n            return self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, modifiers, auto))\n        else:\n            current_mods = modifiers[:]\n            future_mods = (m for m in self.auto_modifiers if (m not in modifiers))\n            while True:\n                results = self.query_library_rec(query, self.library, self.QueryState(query, tie_breaker, current_mods, auto))\n                if (not results):\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        return results\n                elif (len(results) == 1):\n                    return results\n                else:\n                    try:\n                        current_mods.append(next(future_mods))\n                    except StopIteration:\n                        raise self.TieBroken(tie_breaker(query, results))\n                    next_results = self.query_library(query, tie_breaker, current_mods, auto)\n                    if (not next_results):\n                        raise self.TieBroken(tie_breaker(query, results))\n                    else:\n                        return next_results\n    except self.TieBroken as tie:\n        return tie.results\n", "label": 1}
{"function": "\n\ndef walk_branches(self, sort, *branches):\n    \"\\n        Simple iterator which take a sorting strategy and some branch and\\n        iterates through those branches one commit at a time, yielding a list\\n        of commits\\n\\n        :param sort: a sorting option `GIT_SORT_NONE, GIT_SORT_TOPOLOGICAL,\\n        GIT_SORT_TIME, GIT_SORT_REVERSE`. Default is 'GIT_SORT_TOPOLOGICAL'\\n        :param branches: branch to iterate through\\n        :type branches: list\\n        :returns: yields a list of commits corresponding to given branches\\n        :rtype: list\\n\\n        \"\n    iterators = [self._repo.walk(branch.target, sort) for branch in branches]\n    stop_iteration = [False for branch in branches]\n    commits = []\n    for iterator in iterators:\n        try:\n            commit = next(iterator)\n        except StopIteration:\n            commit = None\n        commits.append(commit)\n    (yield (commit for commit in commits))\n    while (not all(stop_iteration)):\n        for (index, iterator) in enumerate(iterators):\n            try:\n                commit = next(iterator)\n                commits[index] = commit\n            except StopIteration:\n                stop_iteration[index] = True\n        if (not all(stop_iteration)):\n            (yield (commit for commit in commits))\n", "label": 1}
{"function": "\n\ndef find_corpus_fileids(root, regexp):\n    if (not isinstance(root, PathPointer)):\n        raise TypeError('find_corpus_fileids: expected a PathPointer')\n    regexp += '$'\n    if isinstance(root, ZipFilePathPointer):\n        fileids = [name[len(root.entry):] for name in root.zipfile.namelist() if (not name.endswith('/'))]\n        items = [name for name in fileids if re.match(regexp, name)]\n        return sorted(items)\n    elif isinstance(root, FileSystemPathPointer):\n        items = []\n        kwargs = {\n            \n        }\n        if (not py25()):\n            kwargs = {\n                'followlinks': True,\n            }\n        for (dirname, subdirs, fileids) in os.walk(root.path, **kwargs):\n            prefix = ''.join((('%s/' % p) for p in _path_from(root.path, dirname)))\n            items += [(prefix + fileid) for fileid in fileids if re.match(regexp, (prefix + fileid))]\n            if ('.svn' in subdirs):\n                subdirs.remove('.svn')\n        return sorted(items)\n    else:\n        raise AssertionError((\"Don't know how to handle %r\" % root))\n", "label": 1}
{"function": "\n\n@defer.inlineCallbacks\ndef action_for_event_by_user(self, event, handler, current_state):\n    actions_by_user = {\n        \n    }\n    users_dict = (yield self.store.are_guests(self.rules_by_user.keys()))\n    filtered_by_user = (yield handler.filter_events_for_clients(users_dict.items(), [event], {\n        event.event_id: current_state,\n    }))\n    room_members = (yield self.store.get_users_in_room(self.room_id))\n    evaluator = PushRuleEvaluatorForEvent(event, len(room_members))\n    condition_cache = {\n        \n    }\n    display_names = {\n        \n    }\n    for ev in current_state.values():\n        nm = ev.content.get('displayname', None)\n        if (nm and (ev.type == EventTypes.Member)):\n            display_names[ev.state_key] = nm\n    for (uid, rules) in self.rules_by_user.items():\n        display_name = display_names.get(uid, None)\n        filtered = filtered_by_user[uid]\n        if (len(filtered) == 0):\n            continue\n        if (filtered[0].sender == uid):\n            continue\n        for rule in rules:\n            if (('enabled' in rule) and (not rule['enabled'])):\n                continue\n            matches = _condition_checker(evaluator, rule['conditions'], uid, display_name, condition_cache)\n            if matches:\n                actions = [x for x in rule['actions'] if (x != 'dont_notify')]\n                if (actions and ('notify' in actions)):\n                    actions_by_user[uid] = actions\n                break\n    defer.returnValue(actions_by_user)\n", "label": 1}
{"function": "\n\ndef get_distance(self, dist_val, lookup_type):\n    '\\n        Returns a distance number in units of the field.  For example, if\\n        `D(km=1)` was passed in and the units of the field were in meters,\\n        then 1000 would be returned.\\n        '\n    if (len(dist_val) == 1):\n        (dist, option) = (dist_val[0], None)\n    else:\n        (dist, option) = dist_val\n    if isinstance(dist, Distance):\n        if self.geodetic:\n            if (SpatialBackend.postgis and (lookup_type == 'dwithin')):\n                raise TypeError('Only numeric values of degree units are allowed on geographic DWithin queries.')\n            dist_param = dist.m\n        else:\n            dist_param = getattr(dist, Distance.unit_attname(self.units_name))\n    else:\n        dist_param = dist\n    if (SpatialBackend.postgis and self.geodetic and (lookup_type != 'dwithin') and (option == 'spheroid')):\n        return [gqn(self._spheroid), dist_param]\n    else:\n        return [dist_param]\n", "label": 1}
{"function": "\n\ndef create_tree_from_coverage(cov, strip_prefix=None, path_aliases=None, cover=[], exclude=[]):\n    'Create a tree with coverage statistics.\\n\\n    Takes a coverage.coverage() instance.\\n\\n    Returns the root node of the tree.\\n    '\n    root = CoverageNode()\n    if path_aliases:\n        apply_path_aliases(cov, dict([alias.partition('=')[::2] for alias in path_aliases]))\n    for filename in cov.data.measured_files():\n        if (not any(((pattern in filename.replace('/', '.')) for pattern in cover))):\n            continue\n        if any(((pattern in filename.replace('/', '.')) for pattern in exclude)):\n            continue\n        if (strip_prefix and filename.startswith(strip_prefix)):\n            short_name = filename[len(strip_prefix):]\n            short_name = short_name.replace('/', os.path.sep)\n            short_name = short_name.lstrip(os.path.sep)\n        else:\n            short_name = cov.file_locator.relative_filename(filename)\n        tree_index = filename_to_list(short_name.replace(os.path.sep, '.'))\n        if (('tests' in tree_index) or ('ftests' in tree_index)):\n            continue\n        root.set_at(tree_index, CoverageCoverageNode(cov, filename))\n    return root\n", "label": 1}
{"function": "\n\ndef communicate(params, request):\n\n    def authorize_request():\n        request['client_time'] = current_milli_time()\n        request['2fa_token'] = (params.mfa_token,)\n        request['2fa_type'] = params.mfa_type\n        request['session_token'] = params.session_token\n        request['username'] = params.user\n    if (not params.session_token):\n        try:\n            login(params)\n        except:\n            raise\n    authorize_request()\n    if params.debug:\n        print(('payload: ' + str(request)))\n    try:\n        r = requests.post(params.server, json=request)\n    except:\n        raise CommunicationError(sys.exc_info()[0])\n    response_json = r.json()\n    if params.debug:\n        debug_response(params, request, r)\n    if (response_json['result_code'] == 'auth_failed'):\n        if params.debug:\n            print('Re-authorizing.')\n        try:\n            login(params)\n        except:\n            raise\n        authorize_request()\n        try:\n            r = requests.post(params.server, json=request)\n        except:\n            print('Comm error during re-auth')\n            raise CommunicationError(sys.exc_info()[0])\n        response_json = r.json()\n        if params.debug:\n            debug_response(params, request, r)\n    if (response_json['result'] != 'success'):\n        if response_json['result_code']:\n            raise CommunicationError(('Unexpected problem: ' + response_json['result_code']))\n    return response_json\n", "label": 1}
{"function": "\n\ndef set_PWM(self, percent, ntries=15):\n    '\\n        '\n    count = 0\n    success = False\n    if (percent < 0.0):\n        percent = 0.0\n    if (percent > 100.0):\n        percent = 100.0\n    while ((not success) and (count <= ntries)):\n        if self.logger:\n            self.logger.info('Setting PWM value to {:.1f} %'.format(percent))\n        send_digital = int(((1023.0 * float(percent)) / 100.0))\n        send_string = 'P{:04d}!'.format(send_digital)\n        result = self.query(send_string)\n        count += 1\n        if result:\n            self.PWM = ((float(result[0]) * 100.0) / 1023.0)\n            if (abs((self.PWM - percent)) > 5.0):\n                if self.logger:\n                    self.logger.warning('  Failed to set PWM value!')\n                time.sleep(2)\n            else:\n                success = True\n            if self.logger:\n                self.logger.info('  PWM Value = {:.1f}'.format(self.PWM))\n", "label": 1}
{"function": "\n\ndef frictionforce(rbtdef, ifunc=None):\n    'Generate friction forces (Coulomb/viscouse model plus offset).'\n    if (not ifunc):\n        ifunc = identity\n    fric = zeros(rbtdef.dof, 1)\n    if ((rbtdef.frictionmodel is None) or (len(rbtdef.frictionmodel) == 0)):\n        pass\n    else:\n        askedterms = set(rbtdef.frictionmodel)\n        if askedterms.issubset(_frictionterms):\n            if ('viscous' in askedterms):\n                for i in range(rbtdef.dof):\n                    fric[i] += (rbtdef.fv[i] * rbtdef.dq[i])\n            if ('Coulomb' in askedterms):\n                for i in range(rbtdef.dof):\n                    fric[i] += (rbtdef.fc[i] * sign(rbtdef.dq[i]))\n            if ('offset' in askedterms):\n                for i in range(rbtdef.dof):\n                    fric[i] += rbtdef.fo[i]\n            fric[i] = ifunc(fric[i])\n        else:\n            raise Exception((\"Friction model terms '%s' not understanded. Use None or a combination of %s.\" % (str((askedterms - _frictionterms)), _frictionterms)))\n    return fric\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.threadName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.threadStringRepresentation = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.isDaemon = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.LIST):\n                self.stackTrace = []\n                (_etype3, _size0) = iprot.readListBegin()\n                for _i4 in xrange(_size0):\n                    _elem5 = StackTraceElement()\n                    _elem5.read(iprot)\n                    self.stackTrace.append(_elem5)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef parse_headers(message):\n    '\\n    Turn a Message object into a list of WSGI-style headers.\\n    '\n    headers_out = []\n    if six.PY3:\n        for (header, value) in message.items():\n            if (header.lower() not in filtered_headers):\n                headers_out.append((header, value))\n    else:\n        for full_header in message.headers:\n            if (not full_header):\n                continue\n            if full_header[0].isspace():\n                if (not headers_out):\n                    raise ValueError(('First header starts with a space (%r)' % full_header))\n                (last_header, last_value) = headers_out.pop()\n                value = ((last_value + ' ') + full_header.strip())\n                headers_out.append((last_header, value))\n                continue\n            try:\n                (header, value) = full_header.split(':', 1)\n            except:\n                raise ValueError(('Invalid header: %r' % full_header))\n            value = value.strip()\n            if (header.lower() not in filtered_headers):\n                headers_out.append((header, value))\n    return headers_out\n", "label": 1}
{"function": "\n\ndef analyze_manifest(self):\n    self.manifest_files = mf = {\n        \n    }\n    if (not self.distribution.include_package_data):\n        return\n    src_dirs = {\n        \n    }\n    for package in (self.packages or ()):\n        src_dirs[assert_relative(self.get_package_dir(package))] = package\n    self.run_command('egg_info')\n    ei_cmd = self.get_finalized_command('egg_info')\n    for path in ei_cmd.filelist.files:\n        (d, f) = os.path.split(assert_relative(path))\n        prev = None\n        oldf = f\n        while (d and (d != prev) and (d not in src_dirs)):\n            prev = d\n            (d, df) = os.path.split(d)\n            f = os.path.join(df, f)\n        if (d in src_dirs):\n            if (path.endswith('.py') and (f == oldf)):\n                continue\n            mf.setdefault(src_dirs[d], []).append(path)\n", "label": 1}
{"function": "\n\ndef InstallLibrary(name, version, explicit=True):\n    'Install a package.\\n\\n  If the installation is explicit then the user made the installation request,\\n  not a package as a dependency. Explicit installation leads to stricter\\n  version checking.\\n\\n  Args:\\n    name: Name of the requested package (already validated as available).\\n    version: The desired version (already validated as available).\\n    explicit: Explicitly requested by the user or implicitly because of a\\n      dependency.\\n  '\n    (installed_version, explicitly_installed) = installed.get(name, ([None] * 2))\n    if (name in sys.modules):\n        if explicit:\n            CheckInstalledVersion(name, version, explicit=True)\n        return\n    elif installed_version:\n        if (version == installed_version):\n            return\n        if explicit:\n            if explicitly_installed:\n                raise ValueError(('%s %s requested, but %s already in use' % (name, version, installed_version)))\n            RemoveLibrary(name)\n        else:\n            version_ob = distutils.version.LooseVersion(version)\n            installed_ob = distutils.version.LooseVersion(installed_version)\n            if (version_ob <= installed_ob):\n                return\n            else:\n                RemoveLibrary(name)\n    AddLibrary(name, version, explicit)\n    dep_details = PACKAGES[name][1][version]\n    if (not dep_details):\n        return\n    for (dep_name, dep_version) in dep_details:\n        InstallLibrary(dep_name, dep_version, explicit=False)\n", "label": 1}
{"function": "\n\ndef _prettifyETree(self, elem):\n    ' Recursively add linebreaks to ElementTree children. '\n    i = '\\n'\n    if (markdown.isBlockLevel(elem.tag) and (elem.tag not in ['code', 'pre'])):\n        if (((not elem.text) or (not elem.text.strip())) and len(elem) and markdown.isBlockLevel(elem[0].tag)):\n            elem.text = i\n        for e in elem:\n            if markdown.isBlockLevel(e.tag):\n                self._prettifyETree(e)\n        if ((not elem.tail) or (not elem.tail.strip())):\n            elem.tail = i\n    if ((not elem.tail) or (not elem.tail.strip())):\n        elem.tail = i\n", "label": 1}
{"function": "\n\ndef write(self, s):\n    self._checkWritable()\n    if self.closed:\n        raise ValueError('write to closed file')\n    if (not isinstance(s, unicode)):\n        raise TypeError((\"can't write %s to text stream\" % s.__class__.__name__))\n    length = len(s)\n    haslf = ((self._writetranslate or self._line_buffering) and ('\\n' in s))\n    if (haslf and self._writetranslate and (self._writenl != '\\n')):\n        s = s.replace('\\n', self._writenl)\n    encoder = (self._encoder or self._get_encoder())\n    b = encoder.encode(s)\n    self.buffer.write(b)\n    if (self._line_buffering and (haslf or ('\\r' in s))):\n        self.flush()\n    self._snapshot = None\n    if self._decoder:\n        self._decoder.reset()\n    return length\n", "label": 1}
{"function": "\n\ndef norm_cspace_id(cspace):\n    try:\n        cspace = ALIASES[cspace]\n    except (KeyError, TypeError):\n        pass\n    if isinstance(cspace, str):\n        if _CIECAM02_axes.issuperset(cspace):\n            return {\n                'name': 'CIECAM02-subset',\n                'ciecam02_space': CIECAM02Space.sRGB,\n                'axes': cspace,\n            }\n        else:\n            return {\n                'name': cspace,\n            }\n    elif isinstance(cspace, CIECAM02Space):\n        return {\n            'name': 'CIECAM02',\n            'ciecam02_space': cspace,\n        }\n    elif isinstance(cspace, LuoEtAl2006UniformSpace):\n        return {\n            'name': \"J'a'b'\",\n            'ciecam02_space': CIECAM02Space.sRGB,\n            'luoetal2006_space': cspace,\n        }\n    elif isinstance(cspace, dict):\n        if (cspace['name'] in ALIASES):\n            base = ALIASES[cspace['name']]\n            if (isinstance(base, dict) and (base['name'] == cspace['name'])):\n                return cspace\n            else:\n                base = norm_cspace_id(base)\n                cspace = dict(cspace)\n                del cspace['name']\n                base = dict(base)\n                base.update(cspace)\n                return base\n        return cspace\n    else:\n        raise ValueError(('unrecognized color space %r' % (cspace,)))\n", "label": 1}
{"function": "\n\ndef repeatable_expr_SA(parser, node, children):\n    expr = children[0]\n    rule = expr\n    if (len(children) > 1):\n        repeat_op = children[1]\n        if (len(repeat_op) > 1):\n            (repeat_op, modifiers) = repeat_op\n        else:\n            repeat_op = repeat_op[0]\n            modifiers = None\n        if (repeat_op == '?'):\n            rule = Optional(nodes=[expr])\n        elif (repeat_op == '*'):\n            rule = ZeroOrMore(nodes=[expr])\n        else:\n            rule = OneOrMore(nodes=[expr])\n        if modifiers:\n            (modifiers, position) = modifiers\n            if (repeat_op == '?'):\n                (line, col) = parser.pos_to_linecol(position)\n                raise TextXSyntaxError('Modifiers are not allowed for \"?\" operator at {}'.format(text((line, col))), line, col)\n            if ('sep' in modifiers):\n                sep = modifiers['sep']\n                rule = Sequence(nodes=[expr, ZeroOrMore(nodes=[Sequence(nodes=[sep, expr])])])\n                if (repeat_op == '*'):\n                    rule = Optional(nodes=[rule])\n            if ('eolterm' in modifiers):\n                rule.eolterm = True\n    return rule\n", "label": 1}
{"function": "\n\ndef geo_apps(namespace=True, runtests=False):\n    '\\n    Returns a list of GeoDjango test applications that reside in\\n    `django.contrib.gis.tests` that can be used with the current\\n    database and the spatial libraries that are installed.\\n    '\n    from django.db import connection\n    from django.contrib.gis.geos import GEOS_PREPARE\n    from django.contrib.gis.gdal import HAS_GDAL\n    apps = ['geoapp', 'relatedapp']\n    if (not connection.ops.mysql):\n        apps.append('distapp')\n    if (connection.ops.postgis and connection.ops.geography):\n        apps.append('geogapp')\n    if HAS_GDAL:\n        if (connection.ops.postgis and GEOS_PREPARE):\n            apps.append('geo3d')\n        apps.append('layermap')\n    if runtests:\n        return [('django.contrib.gis.tests', app) for app in apps]\n    elif namespace:\n        return [('django.contrib.gis.tests.%s' % app) for app in apps]\n    else:\n        return apps\n", "label": 1}
{"function": "\n\ndef dfs(self, s, cur, left, pi, i, rmcnt, ret):\n    '\\n        Remove parenthesis\\n        backtracking, post-check\\n        :param s: original string\\n        :param cur: current string builder\\n        :param left: number of remaining left parentheses in s[0..i] not consumed by \")\"\\n        :param pi: last removed char\\n        :param i: current index\\n        :param rmcnt: number of remaining removals needed\\n        :param ret: results\\n        '\n    if ((left < 0) or (rmcnt < 0) or (i > len(s))):\n        return\n    if (i == len(s)):\n        if ((rmcnt == 0) and (left == 0)):\n            ret.append(cur)\n        return\n    if (s[i] not in ('(', ')')):\n        self.dfs(s, (cur + s[i]), left, None, (i + 1), rmcnt, ret)\n    elif (pi == s[i]):\n        while ((i < len(s)) and pi and (pi == s[i])):\n            (i, rmcnt) = ((i + 1), (rmcnt - 1))\n        self.dfs(s, cur, left, pi, i, rmcnt, ret)\n    else:\n        self.dfs(s, cur, left, s[i], (i + 1), (rmcnt - 1), ret)\n        L = ((left + 1) if (s[i] == '(') else (left - 1))\n        self.dfs(s, (cur + s[i]), L, None, (i + 1), rmcnt, ret)\n", "label": 1}
{"function": "\n\ndef convert(self, model, field, field_args, multiple=False):\n    kwargs = {\n        'label': unicode((field.verbose_name or field.name or '')),\n        'description': (field.help_text or ''),\n        'validators': [],\n        'filters': [],\n        'default': field.default,\n    }\n    if field_args:\n        kwargs.update(field_args)\n    if field.required:\n        if (isinstance(field, IntField) or isinstance(field, FloatField)):\n            kwargs['validators'].append(validators.InputRequired())\n        else:\n            kwargs['validators'].append(validators.Required())\n    else:\n        kwargs['validators'].append(validators.Optional())\n    if field.choices:\n        kwargs['choices'] = field.choices\n        if isinstance(field, IntField):\n            kwargs['coerce'] = int\n        if (not multiple):\n            return f.SelectField(**kwargs)\n        else:\n            return f.SelectMultipleField(**kwargs)\n    ftype = type(field).__name__\n    if hasattr(field, 'to_form_field'):\n        return field.to_form_field(model, kwargs)\n    if (ftype in self.converters):\n        return self.converters[ftype](model, field, kwargs)\n", "label": 1}
{"function": "\n\n@classmethod\ndef update_next(cls, seconds, now):\n    'Setup theme for next update.'\n    cls.next_change = None\n    cls.day = None\n    closest = None\n    lowest = None\n    for t in cls.themes:\n        if ((seconds < t.time) and ((closest is None) or (t.time < closest.time))):\n            closest = t\n        if ((lowest is None) or (t.time < lowest.time)):\n            lowest = t\n    if (closest is not None):\n        cls.next_change = closest\n    elif (lowest is not None):\n        cls.next_change = lowest\n    if (lowest is not None):\n        cls.lowest = lowest\n    if ((cls.next_change.time == cls.lowest.time) and (seconds < cls.lowest.time)):\n        cls.day = (- 1)\n    else:\n        cls.day = now.day\n    debug_log(('Today = %d' % cls.day))\n    debug_log(('%s - Next Change @ %s' % (time.ctime(), str(cls.next_change))))\n", "label": 1}
{"function": "\n\ndef formfield_for_dbfield(self, db_field, **kwargs):\n    \"\\n        Hook for specifying the form Field instance for a given database Field\\n        instance.\\n\\n        If kwargs are given, they're passed to the form Field's constructor.\\n        \"\n    request = kwargs.pop('request', None)\n    if db_field.choices:\n        return self.formfield_for_choice_field(db_field, request, **kwargs)\n    if isinstance(db_field, (models.ForeignKey, models.ManyToManyField)):\n        if (db_field.__class__ in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[db_field.__class__], **kwargs)\n        if isinstance(db_field, models.ForeignKey):\n            formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)\n        elif isinstance(db_field, models.ManyToManyField):\n            formfield = self.formfield_for_manytomany(db_field, request, **kwargs)\n        if (formfield and (db_field.name not in self.raw_id_fields)):\n            formfield.widget = widgets.RelatedFieldWidgetWrapper(formfield.widget, db_field.rel, self.admin_site)\n        return formfield\n    for klass in db_field.__class__.mro():\n        if (klass in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[klass], **kwargs)\n            return db_field.formfield(**kwargs)\n    return db_field.formfield(**kwargs)\n", "label": 1}
{"function": "\n\ndef test_get_open_files(self):\n    p = psutil.Process(os.getpid())\n    files = p.get_open_files()\n    self.assertFalse((TESTFN in files))\n    f = open(TESTFN, 'w')\n    call_until(p.get_open_files, ('len(ret) != %i' % len(files)))\n    filenames = [x.path for x in p.get_open_files()]\n    self.assertIn(TESTFN, filenames)\n    f.close()\n    for file in filenames:\n        assert os.path.isfile(file), file\n    cmdline = (\"import time; f = open(r'%s', 'r'); time.sleep(100);\" % TESTFN)\n    sproc = get_test_subprocess([PYTHON, '-c', cmdline], wait=True)\n    p = psutil.Process(sproc.pid)\n    for x in range(100):\n        filenames = [x.path for x in p.get_open_files()]\n        if (TESTFN in filenames):\n            break\n        time.sleep(0.01)\n    else:\n        self.assertIn(TESTFN, filenames)\n    for file in filenames:\n        assert os.path.isfile(file), file\n", "label": 1}
{"function": "\n\n@classmethod\ndef read(cls, fd, str_cache=None, object_cache=None, traits_cache=None):\n    type_ = U8.read(fd)\n    if (str_cache is None):\n        str_cache = []\n    if (object_cache is None):\n        object_cache = []\n    if (traits_cache is None):\n        traits_cache = []\n    if ((type_ == AMF3_TYPE_UNDEFINED) or (type_ == AMF3_TYPE_NULL)):\n        return None\n    elif (type_ == AMF3_TYPE_FALSE):\n        return False\n    elif (type_ == AMF3_TYPE_TRUE):\n        return True\n    elif (type_ == AMF3_TYPE_STRING):\n        return AMF3String.read(fd, cache=str_cache)\n    elif (type_ == AMF3_TYPE_ARRAY):\n        return AMF3ArrayPacker.read(fd, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif (type_ == AMF3_TYPE_OBJECT):\n        return AMF3ObjectPacker.read(fd, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif (type_ == AMF3_TYPE_DATE):\n        return AMF3DatePacker.read(fd, cache=object_cache)\n    elif (type_ in cls.Readers):\n        return cls.Readers[type_].read(fd)\n    else:\n        raise IOError('Unhandled AMF3 type: {0}'.format(hex(type_)))\n", "label": 1}
{"function": "\n\ndef __delitem__(self, key):\n    if (isinstance(key, slice) or self._haswildcard(key)):\n        if isinstance(key, slice):\n            indices = range(*key.indices(len(self)))\n            if (key.step and (key.step < 0)):\n                indices = reversed(indices)\n        else:\n            indices = self._wildcardmatch(key)\n        for idx in reversed(indices):\n            del self[idx]\n        return\n    elif isinstance(key, string_types):\n        key = Card.normalize_keyword(key)\n        indices = self._keyword_indices\n        if (key not in self._keyword_indices):\n            indices = self._rvkc_indices\n        if (key not in indices):\n            raise KeyError((\"Keyword '%s' not found.\" % key))\n        for idx in reversed(indices[key]):\n            del self[idx]\n        return\n    idx = self._cardindex(key)\n    card = self._cards[idx]\n    keyword = card.keyword\n    del self._cards[idx]\n    indices = self._keyword_indices[keyword]\n    indices.remove(idx)\n    if (not indices):\n        del self._keyword_indices[keyword]\n    if (card.field_specifier is not None):\n        indices = self._rvkc_indices[card.rawkeyword]\n        indices.remove(idx)\n        if (not indices):\n            del self._rvkc_indices[card.rawkeyword]\n    self._updateindices(idx, increment=False)\n    self._modified = True\n", "label": 1}
{"function": "\n\ndef populate_database(self, model, level=1, parents=(None,)):\n    n_siblings = self.siblings_per_level[(level - 1)]\n    for parent in parents:\n        if (model in (TreePlace, TreebeardALPlace)):\n            bulk = [model(parent=parent) for _ in range(n_siblings)]\n            model.objects.bulk_create(bulk)\n            objects = model.objects.filter(parent=parent)\n        elif (model in (TreebeardMPPlace, TreebeardNSPlace)):\n            if (parent is not None):\n                parent = model.objects.get(pk=parent.pk)\n            objects = [(model.add_root() if (parent is None) else parent.add_child()) for _ in range(n_siblings)]\n        else:\n            objects = [model.objects.create(parent=parent) for _ in range(n_siblings)]\n        (yield model.objects.count())\n        if (level < len(self.siblings_per_level)):\n            for count in self.populate_database(model, (level + 1), objects):\n                (yield count)\n", "label": 1}
{"function": "\n\ndef classify(url):\n    '\\n    Classifies an URL and returns the corresponding WebEntity derivative.\\n    '\n    original = url\n    if isinstance(url, WebEntity):\n        return url\n    if (not isinstance(url, str)):\n        raise TypeError(('%s not valid type for 4chan URL' % type(url)))\n    if url.startswith('/'):\n        url = url.lstrip('/')\n    if url.endswith('/'):\n        url = url.rstrip('/')\n    if (not ('4chan.org' in url)):\n        url = '{}://{}/{}'.format(Links.scheme, Links.netloc, url)\n    if (not (url.startswith('http://') or url.startswith('https://'))):\n        url = '{}://{}'.format(Links.scheme, url)\n    path = urlparse.urlparse(url).path\n    match = Links.thread_pattern.match(path)\n    if match:\n        return Thread(*match.groups())\n    match = Links.page_pattern.match(path)\n    if match:\n        return Page(*match.groups())\n    match = Links.board_pattern.match(path)\n    if match:\n        return Board(*match.groups())\n    raise ValueError(('invalid 4chan URL: %s' % repr(original)))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef assignment_onvalidation(form):\n    '\\n            Validation callback for work assignments:\\n            - a worker can only be assigned once to the same job\\n\\n            @param form: the FORM\\n        '\n    db = current.db\n    s3db = current.s3db\n    table = s3db.work_assignment\n    form_vars = form.vars\n    if ('id' in form_vars):\n        record_id = form_vars.id\n    elif hasattr(form, 'record_id'):\n        record_id = form.record_id\n    else:\n        record_id = None\n    try:\n        job_id = form_vars.job_id\n    except AttributeError:\n        job_id = None\n    try:\n        person_id = form_vars.person_id\n    except AttributeError:\n        person_id = None\n    if ((job_id is None) or (person_id is None)):\n        if record_id:\n            query = ((table.id == record_id) & (table.deleted != True))\n            record = db(query).select(table.job_id, table.person_id, limitby=(0, 1)).first()\n            if record:\n                job_id = record.job_id\n                person_id = record.person_id\n        else:\n            if (job_id is None):\n                job_id = table.job_id.default\n            if (person_id is None):\n                person_id = table.person_id.default\n    if (job_id and person_id):\n        query = (((table.job_id == job_id) & (table.person_id == person_id)) & (table.deleted != True))\n        if record_id:\n            query = ((table.id != record_id) & query)\n        duplicate = db(query).select(table.id, limitby=(0, 1)).first()\n        if duplicate:\n            msg = current.T('This person is already assigned to the job')\n            form.errors['person_id'] = msg\n", "label": 1}
{"function": "\n\ndef _collect(self):\n    LOG.debug(('collecting arguments/commands for %s' % self))\n    arguments = []\n    commands = []\n    arguments = list(self._meta.arguments)\n    for member in dir(self.__class__):\n        if member.startswith('_'):\n            continue\n        try:\n            func = getattr(self.__class__, member).__cement_meta__\n        except AttributeError:\n            continue\n        else:\n            func['controller'] = self\n            commands.append(func)\n    for contr in handler.list('controller'):\n        if (contr == self.__class__):\n            continue\n        contr = contr()\n        contr._setup(self.app)\n        if (contr._meta.stacked_on == self._meta.label):\n            if (contr._meta.stacked_type == 'embedded'):\n                (contr_arguments, contr_commands) = contr._collect()\n                for arg in contr_arguments:\n                    arguments.append(arg)\n                for func in contr_commands:\n                    commands.append(func)\n            elif (contr._meta.stacked_type == 'nested'):\n                metadict = {\n                    \n                }\n                metadict['label'] = re.sub('_', '-', contr._meta.label)\n                metadict['func_name'] = '_dispatch'\n                metadict['exposed'] = True\n                metadict['hide'] = contr._meta.hide\n                metadict['help'] = contr._meta.description\n                metadict['aliases'] = contr._meta.aliases\n                metadict['aliases_only'] = contr._meta.aliases_only\n                metadict['controller'] = contr\n                commands.append(metadict)\n    return (arguments, commands)\n", "label": 1}
{"function": "\n\ndef _get_option_tuples(self, option_string):\n    result = []\n    chars = self.prefix_chars\n    if ((option_string[0] in chars) and (option_string[1] in chars)):\n        if ('=' in option_string):\n            (option_prefix, explicit_arg) = option_string.split('=', 1)\n        else:\n            option_prefix = option_string\n            explicit_arg = None\n        for option_string in self._option_string_actions:\n            if option_string.startswith(option_prefix):\n                action = self._option_string_actions[option_string]\n                tup = (action, option_string, explicit_arg)\n                result.append(tup)\n    elif ((option_string[0] in chars) and (option_string[1] not in chars)):\n        option_prefix = option_string\n        explicit_arg = None\n        short_option_prefix = option_string[:2]\n        short_explicit_arg = option_string[2:]\n        for option_string in self._option_string_actions:\n            if (option_string == short_option_prefix):\n                action = self._option_string_actions[option_string]\n                tup = (action, option_string, short_explicit_arg)\n                result.append(tup)\n            elif option_string.startswith(option_prefix):\n                action = self._option_string_actions[option_string]\n                tup = (action, option_string, explicit_arg)\n                result.append(tup)\n    else:\n        self.error((_('unexpected option string: %s') % option_string))\n    return result\n", "label": 1}
{"function": "\n\ndef file_filter(state, dirname, fnames):\n    if (args.dir_masks and (not any([re.search(x, dirname) for x in args.dir_masks]))):\n        return\n    for f in fnames:\n        p = os.path.abspath(os.path.join(os.path.realpath(dirname), f))\n        if (any([re.search(x, f) for x in args.file_masks]) or any([re.search(x, p) for x in args.path_masks])):\n            if os.path.isfile(p):\n                state['files'].append(p)\n", "label": 1}
{"function": "\n\ndef test_docopt_parser_with_tabs():\n    help_string = 'Some Tool\\n\\nUsage: tools [-t] [-i <input>...] <cmd>\\n\\nInputs:\\n\\t-i, --input <input>...\\tThe input\\n\\nOptions:\\n\\t-t\\tSome boolean\\n\\t<cmd>\\tThe command\\n    '\n    opts = Options.from_docopt(help_string)\n    assert (len(opts) == 3)\n    assert (opts['input'] is not None)\n    assert (opts['input'].nargs == '*')\n    assert (not opts['input'].required)\n    assert (opts['t'] is not None)\n    assert (opts['t'].nargs == 0)\n    assert (not opts['t'].required)\n    assert (opts['cmd'] is not None)\n    assert (opts['cmd'].nargs == 1)\n    assert opts['cmd'].required\n", "label": 1}
{"function": "\n\ndef _raise_ssl_error(self, ssl, result):\n    if (self._context._verify_helper is not None):\n        self._context._verify_helper.raise_if_problem()\n    if (self._context._npn_advertise_helper is not None):\n        self._context._npn_advertise_helper.raise_if_problem()\n    if (self._context._npn_select_helper is not None):\n        self._context._npn_select_helper.raise_if_problem()\n    if (self._context._alpn_select_helper is not None):\n        self._context._alpn_select_helper.raise_if_problem()\n    error = _lib.SSL_get_error(ssl, result)\n    if (error == _lib.SSL_ERROR_WANT_READ):\n        raise WantReadError()\n    elif (error == _lib.SSL_ERROR_WANT_WRITE):\n        raise WantWriteError()\n    elif (error == _lib.SSL_ERROR_ZERO_RETURN):\n        raise ZeroReturnError()\n    elif (error == _lib.SSL_ERROR_WANT_X509_LOOKUP):\n        raise WantX509LookupError()\n    elif (error == _lib.SSL_ERROR_SYSCALL):\n        if (_lib.ERR_peek_error() == 0):\n            if (result < 0):\n                if (platform == 'win32'):\n                    errno = _ffi.getwinerror()[0]\n                else:\n                    errno = _ffi.errno\n                raise SysCallError(errno, errorcode.get(errno))\n            else:\n                raise SysCallError((- 1), 'Unexpected EOF')\n        else:\n            _raise_current_error()\n    elif (error == _lib.SSL_ERROR_NONE):\n        pass\n    else:\n        _raise_current_error()\n", "label": 1}
{"function": "\n\n@wsgi.extends\ndef detail(self, req, resp_obj):\n    context = req.environ['nova.context']\n    authorize_extend = False\n    authorize_host_status = False\n    if authorize(context):\n        authorize_extend = True\n    if (api_version_request.is_supported(req, min_version='2.16') and soft_authorize(context, action='show:host_status')):\n        authorize_host_status = True\n    if (authorize_extend or authorize_host_status):\n        servers = list(resp_obj.obj['servers'])\n        instances = req.get_db_instances()\n        if authorize_host_status:\n            host_statuses = self.compute_api.get_instances_host_statuses(instances.values())\n        for server in servers:\n            if authorize_extend:\n                instance = instances[server['id']]\n                self._extend_server(context, server, instance, req)\n            if authorize_host_status:\n                server['host_status'] = host_statuses[server['id']]\n", "label": 1}
{"function": "\n\ndef CheckRemoteGitState(self):\n    'Checks the state of the remote git repository.\\n\\n    Returns:\\n      A boolean value to indicate if the state is sane.\\n    '\n    if (self._command == 'close'):\n        if (not self._git_helper.SynchronizeWithUpstream()):\n            print('{0:s} aborted - unable to synchronize with upstream/master.'.format(self._command.title()))\n            return False\n    elif (self._command in ('create', 'update')):\n        if (not self._git_helper.CheckSynchronizedWithUpstream()):\n            if (not self._git_helper.SynchronizeWithUpstream()):\n                print('{0:s} aborted - unable to synchronize with upstream/master.'.format(self._command.title()))\n                return False\n            force_push = True\n        else:\n            force_push = False\n        if (not self._git_helper.PushToOrigin(self._active_branch, force=force_push)):\n            print('{0:s} aborted - unable to push updates to origin/{1:s}.'.format(self._command.title(), self._active_branch))\n            return False\n    elif (self._command == 'lint'):\n        self._git_helper.CheckSynchronizedWithUpstream()\n    elif (self._command == 'merge'):\n        if (not self._git_helper.SynchronizeWithOrigin()):\n            print('{0:s} aborted - unable to synchronize with origin/master.'.format(self._command.title()))\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef log(self, level, msg, *args, **kw):\n    if args:\n        if kw:\n            raise TypeError('You may give positional or keyword arguments, not both')\n    args = (args or kw)\n    rendered = None\n    for (consumer_level, consumer) in self.consumers:\n        if self.level_matches(level, consumer_level):\n            if (self.in_progress_hanging and (consumer in (sys.stdout, sys.stderr))):\n                self.in_progress_hanging = False\n                sys.stdout.write('\\n')\n                sys.stdout.flush()\n            if (rendered is None):\n                if args:\n                    rendered = (msg % args)\n                else:\n                    rendered = msg\n                rendered = ((' ' * self.indent) + rendered)\n            if hasattr(consumer, 'write'):\n                consumer.write((rendered + '\\n'))\n            else:\n                consumer(rendered)\n", "label": 1}
{"function": "\n\ndef reverse_apicontroller_action(url, status, response):\n    '\\n    Make an API call look like a direct action call by reversing the\\n    exception -> HTTP response translation that ApiController.action does\\n    '\n    try:\n        parsed = json.loads(response)\n        if parsed.get('success'):\n            return parsed['result']\n        if hasattr(parsed, 'get'):\n            err = parsed.get('error', {\n                \n            })\n        else:\n            err = {\n                \n            }\n    except (AttributeError, ValueError):\n        err = {\n            \n        }\n    if (not isinstance(err, dict)):\n        raise ServerIncompatibleError(repr([url, status, response]))\n    etype = err.get('__type')\n    emessage = err.get('message', '').split(': ', 1)[(- 1)]\n    if (etype == 'Search Query Error'):\n        raise SearchQueryError(emessage)\n    elif (etype == 'Search Error'):\n        raise SearchError(emessage)\n    elif (etype == 'Search Index Error'):\n        raise SearchIndexError(emessage)\n    elif (etype == 'Validation Error'):\n        raise ValidationError(err)\n    elif (etype == 'Not Found Error'):\n        raise NotFound(emessage)\n    elif (etype == 'Authorization Error'):\n        raise NotAuthorized(err)\n    raise CKANAPIError(repr([url, status, response]))\n", "label": 1}
{"function": "\n\ndef line_is_partially_commented(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'c', 'cpp', 'cc', 'scala')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(/\\\\*)(.*)\\\\*/', line) or re.match('^(\\\\+|\\\\-)(.*)//', line)):\n            flag = True\n    elif (ext in ('py', 'rb')):\n        if re.match('^(\\\\+|\\\\-)(.*)\\\\#', line):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line):\n            flag = True\n    elif (ext == 'sql'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(/\\\\*)(.*)(\\\\*/)', line) or re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\-\\\\s)', line)):\n            flag = True\n    elif (ext == 'php'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(/\\\\*)(.*)\\\\*/', line) or re.match('^(\\\\+|\\\\-)(.*)//', line) or re.match('^(\\\\+|\\\\-)(.*)\\\\#', line)):\n            flag = True\n    elif (ext == 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%)', line) or re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line)):\n            flag = True\n    else:\n        self.logger.warning(('GitQuerier: impossible to identify comments for extension: ' + ext))\n    return flag\n", "label": 1}
{"function": "\n\ndef install_pyfile(self, node):\n    path = self.bld.get_install_path(((self.install_path + os.sep) + node.name), self.env)\n    self.bld.install_files(self.install_path, [node], self.env, self.chmod, postpone=False)\n    if (self.bld.is_install < 0):\n        info('* removing byte compiled python files')\n        for x in 'co':\n            try:\n                os.remove((path + x))\n            except OSError:\n                pass\n    if (self.bld.is_install > 0):\n        if (self.env['PYC'] or self.env['PYO']):\n            info(('* byte compiling %r' % path))\n        if self.env['PYC']:\n            program = \"\\nimport sys, py_compile\\nfor pyfile in sys.argv[1:]:\\n\\tpy_compile.compile(pyfile, pyfile + 'c')\\n\"\n            argv = [self.env['PYTHON'], '-c', program, path]\n            ret = Utils.pproc.Popen(argv).wait()\n            if ret:\n                raise Utils.WafError(('bytecode compilation failed %r' % path))\n        if self.env['PYO']:\n            program = \"\\nimport sys, py_compile\\nfor pyfile in sys.argv[1:]:\\n\\tpy_compile.compile(pyfile, pyfile + 'o')\\n\"\n            argv = [self.env['PYTHON'], self.env['PYFLAGS_OPT'], '-c', program, path]\n            ret = Utils.pproc.Popen(argv).wait()\n            if ret:\n                raise Utils.WafError(('bytecode compilation failed %r' % path))\n", "label": 1}
{"function": "\n\ndef wait_for_services(self, instances, callback=None):\n    assert self.connected\n    pending = set(instances)\n    ready = []\n    failed = []\n    while len(pending):\n        toRemove = []\n        for i in pending:\n            i.update()\n            if ('status' not in i.tags):\n                continue\n            if i.tags['status'].endswith('failed'):\n                toRemove.append(i)\n                failed.append(i)\n            elif (i.tags['status'] == 'ready'):\n                toRemove.append(i)\n                ready.append(i)\n        for i in toRemove:\n            pending.remove(i)\n        status = {\n            \n        }\n        for i in itertools.chain(pending, ready, failed):\n            if ('status' not in i.tags):\n                status_name = 'installing dependencies'\n            else:\n                status_name = i.tags['status']\n            if (status_name not in status):\n                status[status_name] = []\n            status[status_name].append(i)\n        if callback:\n            callback(status)\n        if len(pending):\n            time.sleep(1)\n    return (len(failed) == 0)\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw_args):\n    args = [sympify(arg) for arg in args]\n    args = TensMul._flatten(args)\n    is_canon_bp = kw_args.get('is_canon_bp', False)\n    if (not any([isinstance(arg, TensExpr) for arg in args])):\n        tids = TIDS([], [], [])\n    else:\n        tids_list = [arg._tids for arg in args if isinstance(arg, (Tensor, TensMul))]\n        if (len(tids_list) == 1):\n            for arg in args:\n                if (not isinstance(arg, Tensor)):\n                    continue\n                is_canon_bp = kw_args.get('is_canon_bp', arg._is_canon_bp)\n        tids = reduce((lambda a, b: (a * b)), tids_list)\n    if any([isinstance(arg, TensAdd) for arg in args]):\n        add_args = TensAdd._tensAdd_flatten(args)\n        return TensAdd(*add_args)\n    coeff = reduce((lambda a, b: (a * b)), ([S.One] + [arg for arg in args if (not isinstance(arg, TensExpr))]))\n    args = tids.get_tensors()\n    if (coeff != 1):\n        args = ([coeff] + args)\n    if (len(args) == 1):\n        return args[0]\n    obj = Basic.__new__(cls, *args)\n    obj._types = []\n    for t in tids.components:\n        obj._types.extend(t._types)\n    obj._tids = tids\n    obj._ext_rank = (len(obj._tids.free) + (2 * len(obj._tids.dum)))\n    obj._coeff = coeff\n    obj._is_canon_bp = is_canon_bp\n    return obj\n", "label": 1}
{"function": "\n\ndef get_query_string(self, new_params=None, remove=None):\n    if (new_params is None):\n        new_params = {\n            \n        }\n    if (remove is None):\n        remove = []\n    p = copy(self.params)\n    for r in remove:\n        for k in p.keys():\n            if k.startswith(r):\n                del p[k]\n    for (k, v) in new_params.items():\n        if (v is None):\n            if (k in p):\n                del p[k]\n        else:\n            p[k] = v\n    if hasattr(p, 'urlencode'):\n        return ('?%s' % p.urlencode())\n    return ('?%s' % urlencode(p))\n", "label": 1}
{"function": "\n\ndef setup_databases(verbosity, interactive, **kwargs):\n    from django.db import connections, DEFAULT_DB_ALIAS\n    mirrored_aliases = {\n        \n    }\n    test_databases = {\n        \n    }\n    dependencies = {\n        \n    }\n    default_sig = connections[DEFAULT_DB_ALIAS].creation.test_db_signature()\n    for alias in connections:\n        connection = connections[alias]\n        if connection.settings_dict['TEST_MIRROR']:\n            mirrored_aliases[alias] = connection.settings_dict['TEST_MIRROR']\n        else:\n            item = test_databases.setdefault(connection.creation.test_db_signature(), (connection.settings_dict['NAME'], set()))\n            item[1].add(alias)\n            if ('TEST_DEPENDENCIES' in connection.settings_dict):\n                dependencies[alias] = connection.settings_dict['TEST_DEPENDENCIES']\n            elif ((alias != DEFAULT_DB_ALIAS) and (connection.creation.test_db_signature() != default_sig)):\n                dependencies[alias] = connection.settings_dict.get('TEST_DEPENDENCIES', [DEFAULT_DB_ALIAS])\n    old_names = []\n    mirrors = []\n    for (signature, (db_name, aliases)) in dependency_ordered(test_databases.items(), dependencies):\n        test_db_name = None\n        for alias in aliases:\n            connection = connections[alias]\n            if (test_db_name is None):\n                test_db_name = connection.creation.create_test_db(verbosity, autoclobber=(not interactive))\n                destroy = True\n            else:\n                connection.settings_dict['NAME'] = test_db_name\n                destroy = False\n            old_names.append((connection, db_name, destroy))\n    for (alias, mirror_alias) in mirrored_aliases.items():\n        mirrors.append((alias, connections[alias].settings_dict['NAME']))\n        connections[alias].settings_dict['NAME'] = connections[mirror_alias].settings_dict['NAME']\n    return (old_names, mirrors)\n", "label": 1}
{"function": "\n\ndef testInclude(self):\n    (fd, tempPath) = tempfile.mkstemp(suffix='.cfg')\n\n    def unlinkTemp(path):\n        try:\n            os.unlink(path)\n        except:\n            pass\n    atexit.register(unlinkTemp, tempPath)\n    fp = os.fdopen(fd, 'w')\n    ((print >> fp), '[section3]\\nbaz = somevalue\\n')\n    fp.close()\n    s = ('%s\\n\\n%%include \"%s\"\\n' % (CONFIG2, tempPath))\n    os.environ['SOME_ENV_VAR'] = 'test_test_test'\n    config = Configuration()\n    config.readfp(StringIO(s))\n    unlinkTemp(tempPath)\n    assert config.has_section('section1')\n    assert config.has_section('section2')\n    assert config.has_section('section3')\n    assert (not config.has_section('foo'))\n    assert (not config.has_section('bar'))\n    assert (not config.has_section('bar2'))\n    assert config.has_option('section1', 'foo')\n    assert config.has_option('section1', 'bar')\n    assert (not config.has_option('section1', 'bar2'))\n    assert config.has_option('section2', 'foo')\n    assert config.has_option('section2', 'bar')\n    assert config.has_option('section3', 'baz')\n    assert (config.get('section1', 'foo') == 'bar')\n    assert (config.get('section1', 'bar') == 'bar')\n    assert (config.get('section2', 'foo') == 'bar')\n    assert (config.get('section2', 'bar') == os.environ['SOME_ENV_VAR'])\n    assert (config.get('section3', 'baz') == 'somevalue')\n", "label": 1}
{"function": "\n\ndef _read_unlocked(self, n=None):\n    nodata_val = b''\n    empty_values = (b'', None)\n    buf = self._read_buf\n    pos = self._read_pos\n    if ((n is None) or (n == (- 1))):\n        self._reset_read_buf()\n        chunks = [buf[pos:]]\n        current_size = 0\n        while True:\n            chunk = self.raw.read()\n            if (chunk in empty_values):\n                nodata_val = chunk\n                break\n            current_size += len(chunk)\n            chunks.append(chunk)\n        return (b''.join(chunks) or nodata_val)\n    avail = (len(buf) - pos)\n    if (n <= avail):\n        self._read_pos += n\n        return buf[pos:(pos + n)]\n    chunks = [buf[pos:]]\n    wanted = max(self.buffer_size, n)\n    while (avail < n):\n        chunk = self.raw.read(wanted)\n        if (chunk in empty_values):\n            nodata_val = chunk\n            break\n        avail += len(chunk)\n        chunks.append(chunk)\n    n = min(n, avail)\n    out = b''.join(chunks)\n    self._read_buf = out[n:]\n    self._read_pos = 0\n    return (out[:n] if out else nodata_val)\n", "label": 1}
{"function": "\n\ndef processArgs(self, parsedArguments):\n    \"Update the Setup's config to reflect parsedArguments.\\n\\n        parsedArguments - the result of an argparse parse action.\\n        \"\n    parsed = parsedArguments\n    if (('stackLogfile' in parsed) and parsed.stackLogfile):\n        self.config.backgroundStackTraceLoopFilename = parsed.stackLogfile\n    if (('memoryLogfile' in parsed) and parsed.memoryLogfile):\n        self.config.backgroundMemoryUsageLoopFilename = parsed.memoryLogfile\n    if (('logging' in parsed) and parsed.logging):\n        self.config.setLoggingLevel(parsed.logging, parsed.background_logging)\n    if (('target' in parsed) and parsed.target):\n        self.config.target = parsed.target\n    if (('dataroot' in parsed) and parsed.dataroot):\n        path = os.path.expanduser(parsed.dataroot)\n        self.config.setRootDataDir(path)\n    if (('datarootsubdir' in parsed) and parsed.datarootsubdir):\n        path = os.path.join(self.config.rootDataDir, parsed.datarootsubdir)\n        self.config.setRootDataDir(path)\n    if (('baseport' in parsed) and parsed.baseport):\n        self.config.setAllPorts(parsed.baseport)\n    self.parsedArguments = parsedArguments\n", "label": 1}
{"function": "\n\ndef _return_handler(self, ret_value, func, arguments):\n    'Check return values for errors and warnings.\\n        '\n    logger.debug('%s%s -> %r', func.__name__, _args_to_str(arguments), ret_value, extra=self._logging_extra)\n    try:\n        ret_value = constants.StatusCode(ret_value)\n    except ValueError:\n        pass\n    self._last_status = ret_value\n    session = None\n    if (func.__name__ not in ('viFindNext',)):\n        try:\n            session = arguments[0]\n        except KeyError:\n            raise Exception(('Function %r does not seem to be a valid visa function (len args %d)' % (func, len(arguments))))\n        if (func.__name__ in ('viOpenDefaultRM',)):\n            session = session._obj.value\n        if isinstance(session, integer_types):\n            self._last_status_in_session[session] = ret_value\n        elif (func.__name__ not in ('viClose', 'viGetAttribute', 'viSetAttribute', 'viStatusDesc')):\n            raise Exception(('Function %r does not seem to be a valid visa function (type args[0] %r)' % (func, type(session))))\n    if (ret_value < 0):\n        raise errors.VisaIOError(ret_value)\n    if (ret_value in self.issue_warning_on):\n        if (session and (ret_value not in self._ignore_warning_in_session[session])):\n            warnings.warn(errors.VisaIOWarning(ret_value), stacklevel=2)\n    return ret_value\n", "label": 1}
{"function": "\n\ndef parse(self, argv=None):\n    '\\n        Parse argv of terminal\\n\\n        :param argv: default is sys.argv\\n        '\n    if (not argv):\n        argv = sys.argv\n    elif isinstance(argv, str):\n        argv = argv.split()\n    self._argv = argv[1:]\n    if (not self._argv):\n        self.validate_options()\n        if self._command_func:\n            self._command_func(**self._results)\n            return True\n        return False\n    cmd = self._argv[0]\n    if (not cmd.startswith('-')):\n        for command in self._command_list:\n            if (isinstance(command, Command) and (command._name == cmd)):\n                command._parent = self\n                return command.parse(self._argv)\n    _positional_index = 0\n    while self._argv:\n        arg = self._argv[0]\n        self._argv = self._argv[1:]\n        if (not self.parse_options(arg)):\n            self._args_results.append(arg)\n            if (len(self._positional_list) > _positional_index):\n                key = self._positional_list[_positional_index]\n                self._results[key] = arg\n                _positional_index += 1\n    self.validate_options()\n    if (self._parent and isinstance(self._parent, Command)):\n        self._parent._args_results = self._args_results\n    if self._command_func:\n        self._command_func(**self._results)\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef _key_opts(self):\n    '\\n        Return options for the ssh command base for Salt to call\\n        '\n    options = ['KbdInteractiveAuthentication=no']\n    if self.passwd:\n        options.append('PasswordAuthentication=yes')\n    else:\n        options.append('PasswordAuthentication=no')\n    if (self.opts.get('_ssh_version', (0,)) > (4, 9)):\n        options.append('GSSAPIAuthentication=no')\n    options.append('ConnectTimeout={0}'.format(self.timeout))\n    if self.opts.get('ignore_host_keys'):\n        options.append('StrictHostKeyChecking=no')\n    if self.opts.get('no_host_keys'):\n        options.extend(['StrictHostKeyChecking=no', 'UserKnownHostsFile=/dev/null'])\n    known_hosts = self.opts.get('known_hosts_file')\n    if (known_hosts and os.path.isfile(known_hosts)):\n        options.append('UserKnownHostsFile={0}'.format(known_hosts))\n    if self.port:\n        options.append('Port={0}'.format(self.port))\n    if self.priv:\n        options.append('IdentityFile={0}'.format(self.priv))\n    if self.user:\n        options.append('User={0}'.format(self.user))\n    if self.identities_only:\n        options.append('IdentitiesOnly=yes')\n    ret = []\n    for option in options:\n        ret.append('-o {0} '.format(option))\n    return ''.join(ret)\n", "label": 1}
{"function": "\n\ndef _parse_waf(context, repos, record, identifier):\n    recobjs = []\n    content = util.http_request('GET', record)\n    LOGGER.debug(content)\n    try:\n        parser = etree.HTMLParser()\n        tree = etree.fromstring(content, parser)\n    except Exception as err:\n        raise Exception(('Could not parse WAF: %s' % str(err)))\n    up = urlparse(record)\n    links = []\n    LOGGER.debug('collecting links')\n    for link in tree.xpath('//a/@href'):\n        link = link.strip()\n        if (not link):\n            continue\n        if (link.find('?') != (- 1)):\n            continue\n        if (not link.endswith('.xml')):\n            LOGGER.debug('Skipping, not .xml')\n            continue\n        if ('/' in link):\n            if (link[(- 1)] == '/'):\n                continue\n            if (link[0] == '/'):\n                link = ('%s://%s%s' % (up.scheme, up.netloc, link))\n        else:\n            link = ('%s/%s' % (record, link))\n        LOGGER.debug('URL is: %s', link)\n        links.append(link)\n    LOGGER.debug('%d links found', len(links))\n    for link in links:\n        LOGGER.debug('Processing link %s', link)\n        linkcontent = util.http_request('GET', link)\n        recobj = _parse_metadata(context, repos, linkcontent)[0]\n        recobj.source = link\n        recobj.mdsource = link\n        recobjs.append(recobj)\n    return recobjs\n", "label": 1}
{"function": "\n\ndef _get_article_metadata(self, meta):\n    adict = PYConf()\n    adict.title = meta['title'][0]\n    adict.postid = meta['postid'][0]\n    adict.nicename = meta['nicename'][0]\n    adict.slug = meta['slug'][0]\n    adict.date = self.get_datetime(meta['date'][0])\n    adict.author = meta['author'][0]\n    tags = meta.get('tags')\n    if tags:\n        adict.tags = [tag.strip() for tag in tags[0].split(',')]\n    category = meta.get('category')\n    if category:\n        adict.category = [cat.strip() for cat in category[0].split(',')]\n    modified = meta.get('modified')\n    if modified:\n        adict.modified = self.get_datetime(modified[0])\n    posttype = meta.get('posttype')\n    if posttype:\n        adict.posttype = posttype[0]\n    else:\n        adict.posttype = 'post'\n    poststatus = meta.get('poststatus')\n    if poststatus:\n        adict.poststatus = poststatus[0]\n    else:\n        adict.poststatus = 'publish'\n    attachments = meta.get('attachments')\n    if attachments:\n        adict.attachments = [att.strip() for att in attachments[0].split(',')]\n    return adict\n", "label": 1}
{"function": "\n\ndef get_query_string(p, new_params=None, remove=None):\n    '\\n    Add and remove query parameters. From `django.contrib.admin`.\\n    '\n    if (new_params is None):\n        new_params = {\n            \n        }\n    if (remove is None):\n        remove = []\n    for r in remove:\n        for k in list(p.keys()):\n            if (k == r):\n                del p[k]\n    for (k, v) in list(new_params.items()):\n        if ((k in p) and (v is None)):\n            del p[k]\n        elif (v is not None):\n            p[k] = v\n    return ('?' + '&'.join([('%s=%s' % (urlquote(k), urlquote(v))) for (k, v) in p.items()]))\n", "label": 1}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a time. Returns a\\n        Python datetime.time object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value.time()\n    if isinstance(value, datetime.time):\n        return value\n    if isinstance(value, list):\n        if (len(value) != 2):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES)):\n            return None\n        start_value = value[0]\n        end_value = value[1]\n    start_time = None\n    end_time = None\n    for format in (self.input_formats or formats.get_format('TIME_INPUT_FORMATS')):\n        try:\n            start_time = datetime.datetime(*time.strptime(start_value, format)[:6]).time()\n        except ValueError:\n            if start_time:\n                continue\n            else:\n                raise ValidationError(self.error_messages['invalid'])\n    for format in (self.input_formats or formats.get_format('TIME_INPUT_FORMATS')):\n        try:\n            end_time = datetime.datetime(*time.strptime(end_value, format)[:6]).time()\n        except ValueError:\n            if end_time:\n                continue\n            else:\n                raise ValidationError(self.error_messages['invalid'])\n    return (start_time, end_time)\n", "label": 1}
{"function": "\n\ndef format_progress_line(self):\n    show_percent = self.show_percent\n    info_bits = []\n    if self.length_known:\n        bar_length = int((self.pct * self.width))\n        bar = (self.fill_char * bar_length)\n        bar += (self.empty_char * (self.width - bar_length))\n        if (show_percent is None):\n            show_percent = (not self.show_pos)\n    elif self.finished:\n        bar = (self.fill_char * self.width)\n    else:\n        bar = list((self.empty_char * (self.width or 1)))\n        if (self.time_per_iteration != 0):\n            bar[int((((math.cos((self.pos * self.time_per_iteration)) / 2.0) + 0.5) * self.width))] = self.fill_char\n        bar = ''.join(bar)\n    if self.show_pos:\n        info_bits.append(self.format_pos())\n    if show_percent:\n        info_bits.append(self.format_pct())\n    if (self.show_eta and self.eta_known and (not self.finished)):\n        info_bits.append(self.format_eta())\n    if (self.item_show_func is not None):\n        item_info = self.item_show_func(self.current_item)\n        if (item_info is not None):\n            info_bits.append(item_info)\n    return (self.bar_template % {\n        'label': self.label,\n        'bar': bar,\n        'info': self.info_sep.join(info_bits),\n    }).rstrip()\n", "label": 1}
{"function": "\n\ndef __init__(self, kernel, itspace, *args, **kwargs):\n    base.ParLoop.__init__(self, kernel, itspace, *args, **kwargs)\n    self.__unwound_args = []\n    self.__unique_args = []\n    self._arg_dict = {\n        \n    }\n    seen = set()\n    c = 0\n    for arg in self._actual_args:\n        if arg._is_mat:\n            for a in arg:\n                self.__unwound_args.append(a)\n        elif (arg._is_vec_map or arg._uses_itspace):\n            for (d, m) in zip(arg.data, arg.map):\n                for i in range(m.arity):\n                    a = d(arg.access, m[i])\n                    a.position = arg.position\n                    self.__unwound_args.append(a)\n        else:\n            for a in arg:\n                self.__unwound_args.append(a)\n        if arg._is_dat:\n            key = (arg.data, arg.map)\n            if arg._is_indirect:\n                arg._which_indirect = c\n                if (arg._is_vec_map or arg._flatten):\n                    c += arg.map.arity\n                elif arg._uses_itspace:\n                    c += self._it_space.extents[arg.idx.index]\n                else:\n                    c += 1\n            if (key not in seen):\n                self.__unique_args.append(arg)\n                seen.add(key)\n        else:\n            self.__unique_args.append(arg)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef parse_content(node):\n    \" Parse content from input node and returns ContentHandler object\\n        it'll look like:\\n\\n            - template:\\n                - file:\\n                    - temple: path\\n\\n            or something\\n\\n        \"\n    output = ContentHandler()\n    is_template_path = False\n    is_template_content = False\n    is_file = False\n    is_done = False\n    while (node and (not is_done)):\n        if isinstance(node, basestring):\n            output.content = node\n            output.setup(node, is_file=is_file, is_template_path=is_template_path, is_template_content=is_template_content)\n            return output\n        elif ((not isinstance(node, dict)) and (not isinstance(node, list))):\n            raise TypeError('Content must be a string, dictionary, or list of dictionaries')\n        is_done = True\n        flat = lowercase_keys(flatten_dictionaries(node))\n        for (key, value) in flat.items():\n            if (key == 'template'):\n                if isinstance(value, basestring):\n                    if is_file:\n                        value = os.path.abspath(value)\n                    output.content = value\n                    is_template_content = (is_template_content or (not is_file))\n                    output.is_template_content = is_template_content\n                    output.is_template_path = is_file\n                    output.is_file = is_file\n                    return output\n                else:\n                    is_template_content = True\n                    node = value\n                    is_done = False\n                    break\n            elif (key == 'file'):\n                if isinstance(value, basestring):\n                    output.content = os.path.abspath(value)\n                    output.is_file = True\n                    output.is_template_content = is_template_content\n                    return output\n                else:\n                    is_file = True\n                    node = value\n                    is_done = False\n                    break\n    raise Exception('Invalid configuration for content.')\n", "label": 1}
{"function": "\n\ndef _check_1d(self, routine, dtype, shape, axis, overwritable_dtypes):\n    np.random.seed(1234)\n    if np.issubdtype(dtype, np.complexfloating):\n        data = (np.random.randn(*shape) + (1j * np.random.randn(*shape)))\n    else:\n        data = np.random.randn(*shape)\n    data = data.astype(dtype)\n    for type in [1, 2, 3]:\n        for overwrite_x in [True, False]:\n            for norm in [None, 'ortho']:\n                if ((type == 1) and (norm == 'ortho')):\n                    continue\n                should_overwrite = (overwrite_x and (dtype in overwritable_dtypes) and ((len(shape) == 1) or ((axis % len(shape)) == (len(shape) - 1))))\n                self._check(data, routine, type, None, axis, norm, overwrite_x, should_overwrite)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.roleName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.SET):\n                self.groups = set()\n                (_etype10, _size7) = iprot.readSetBegin()\n                for _i11 in xrange(_size7):\n                    _elem12 = iprot.readString()\n                    self.groups.add(_elem12)\n                iprot.readSetEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.device = TrafficControlledDevice()\n                self.device.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.timeout = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef step(self, dependency_states, step_context):\n    input_node = self._input_node()\n    input_state = dependency_states.get(input_node, None)\n    if ((input_state is None) or (type(input_state) == Waiting)):\n        return Waiting([input_node])\n    elif (type(input_state) == Throw):\n        return input_state\n    elif (type(input_state) == Noop):\n        return Noop('Could not compute {} in order to project its fields.'.format(input_node))\n    elif (type(input_state) != Return):\n        State.raise_unrecognized(input_state)\n    input_product = input_state.value\n    values = []\n    for field in self.fields:\n        values.append(getattr(input_product, field))\n    if ((len(values) == 1) and (type(values[0]) is self.projected_subject)):\n        projected_subject = values[0]\n    else:\n        projected_subject = self.projected_subject(*values)\n    output_node = self._output_node(step_context, projected_subject)\n    output_state = dependency_states.get(output_node, None)\n    if ((output_state is None) or (type(output_state) == Waiting)):\n        return Waiting([input_node, output_node])\n    elif (type(output_state) == Noop):\n        return Noop('Successfully projected, but no source of output product for {}.'.format(output_node))\n    elif (type(output_state) in [Throw, Return]):\n        return output_state\n    else:\n        raise State.raise_unrecognized(output_state)\n", "label": 1}
{"function": "\n\ndef extract_conf_from(mod, conf=ModuleConfig(CONF_SPEC), depth=0, max_depth=2):\n    'recursively extract keys from module or object\\n    by passed config scheme\\n    '\n    for (key, default_value) in six.iteritems(conf):\n        conf[key] = _get_key_from_module(mod, key, default_value)\n    try:\n        filtered_apps = [app for app in conf['apps'] if (app not in BLACKLIST)]\n    except TypeError:\n        pass\n    except Exception as e:\n        warnings.warn(('Error %s during loading %s' % (e, conf['apps'])))\n    for app in filtered_apps:\n        try:\n            app_module = import_module(app)\n            if (app_module != mod):\n                app_module = _get_correct_module(app_module)\n                if (depth < max_depth):\n                    mod_conf = extract_conf_from(app_module, depth=(depth + 1))\n                    for (k, v) in six.iteritems(mod_conf):\n                        if (k == 'config'):\n                            continue\n                        if isinstance(v, dict):\n                            conf[k].update(v)\n                        elif isinstance(v, (list, tuple)):\n                            conf[k] = merge(conf[k], v)\n        except Exception as e:\n            pass\n    return conf\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = QueryNotFoundException()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.error2 = BeeswaxException()\n                self.error2.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _build_root_message(self, message_cls=None, **kw):\n    msg = (message_cls or SafeMIMEMultipart)(**kw)\n    if self.policy:\n        msg.policy = self.policy\n    msg.preamble = self.ROOT_PREAMBLE\n    self.set_header(msg, 'Date', self.date, encode=False)\n    self.set_header(msg, 'Message-ID', self.message_id, encode=False)\n    if self._headers:\n        for (name, value) in self._headers.items():\n            self.set_header(msg, name, value)\n    subject = self.subject\n    if (subject is not None):\n        self.set_header(msg, 'Subject', subject)\n    self.set_header(msg, 'From', self.encode_address_header(self._mail_from), encode=False)\n    if self._mail_to:\n        self.set_header(msg, 'To', ', '.join([self.encode_address_header(addr) for addr in self._mail_to]), encode=False)\n    if self._cc:\n        self.set_header(msg, 'Cc', ', '.join([self.encode_address_header(addr) for addr in self._cc]), encode=False)\n    return msg\n", "label": 1}
{"function": "\n\ndef parse(self, lines):\n    'parse the input lines from a robots.txt file.\\n           We allow that a user-agent: line is not preceded by\\n           one or more blank lines.'\n    state = 0\n    linenumber = 0\n    entry = Entry()\n    for line in lines:\n        linenumber += 1\n        if (not line):\n            if (state == 1):\n                entry = Entry()\n                state = 0\n            elif (state == 2):\n                self._add_entry(entry)\n                entry = Entry()\n                state = 0\n        i = line.find('#')\n        if (i >= 0):\n            line = line[:i]\n        line = line.strip()\n        if (not line):\n            continue\n        line = line.split(':', 1)\n        if (len(line) == 2):\n            line[0] = line[0].strip().lower()\n            line[1] = urllib.unquote(line[1].strip())\n            if (line[0] == 'user-agent'):\n                if (state == 2):\n                    self._add_entry(entry)\n                    entry = Entry()\n                entry.useragents.append(line[1])\n                state = 1\n            elif (line[0] == 'disallow'):\n                if (state != 0):\n                    entry.rulelines.append(RuleLine(line[1], False))\n                    state = 2\n            elif (line[0] == 'allow'):\n                if (state != 0):\n                    entry.rulelines.append(RuleLine(line[1], True))\n                    state = 2\n    if (state == 2):\n        self._add_entry(entry)\n", "label": 1}
{"function": "\n\ndef save(self, **kwargs):\n    if (not self.content_type_id):\n        self.content_type = ContentType.objects.get_for_model(self)\n    send_signal = None\n    old_self = None\n    if self.pk:\n        try:\n            old_self = self.__class__.objects.get(pk=self.pk)\n        except Publishable.DoesNotExist:\n            pass\n    if old_self:\n        old_path = old_self.get_absolute_url()\n        new_path = self.get_absolute_url()\n        if ((old_path != new_path) and new_path and (not old_self.static)):\n            redirect = Redirect.objects.get_or_create(old_path=old_path, site=self.category.site)[0]\n            redirect.new_path = new_path\n            redirect.save(force_update=True)\n            Redirect.objects.filter(new_path=old_path).exclude(pk=redirect.pk).update(new_path=new_path)\n        if (old_self.is_published() != self.is_published()):\n            if self.is_published():\n                send_signal = content_published\n                self.announced = True\n            else:\n                send_signal = content_unpublished\n                self.announced = False\n        if ((old_self.published != self.published) and (self.published is False)):\n            send_signal = content_unpublished\n            self.announced = False\n        if ((old_self.last_updated == old_self.publish_from) and (self.last_updated == old_self.last_updated)):\n            self.last_updated = self.publish_from\n    elif self.is_published():\n        send_signal = content_published\n        self.announced = True\n    if (not self.last_updated):\n        self.last_updated = self.publish_from\n    super(Publishable, self).save(**kwargs)\n    if send_signal:\n        send_signal.send(sender=self.__class__, publishable=self)\n", "label": 1}
{"function": "\n\ndef volume_create_attach(name, call=None, **kwargs):\n    '\\n    Create and attach volumes to created node\\n    '\n    if (call == 'function'):\n        raise SaltCloudSystemExit('The create_attach_volumes action must be called with -a or --action.')\n    if (type(kwargs['volumes']) is str):\n        volumes = yaml.safe_load(kwargs['volumes'])\n    else:\n        volumes = kwargs['volumes']\n    ret = []\n    for volume in volumes:\n        created = False\n        volume_dict = {\n            'name': volume['name'],\n        }\n        if ('volume_id' in volume):\n            volume_dict['volume_id'] = volume['volume_id']\n        elif ('snapshot' in volume):\n            volume_dict['snapshot'] = volume['snapshot']\n        else:\n            volume_dict['size'] = volume['size']\n            if ('type' in volume):\n                volume_dict['type'] = volume['type']\n            if ('iops' in volume):\n                volume_dict['iops'] = volume['iops']\n        if ('id' not in volume_dict):\n            created_volume = create_volume(**volume_dict)\n            created = True\n            volume_dict.update(created_volume)\n        attach = attach_volume(name=volume['name'], server_name=name, device=volume.get('device', None), call='action')\n        if attach:\n            msg = '{0} attached to {1} (aka {2})'.format(volume_dict['id'], name, volume_dict['name'])\n            log.info(msg)\n            ret.append(msg)\n    return ret\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    new_class = type.__new__(cls, name, bases, attrs)\n    delegate_class = new_class.__delegate_class__\n    if delegate_class:\n        delegated_attrs = {\n            \n        }\n        for klass in reversed(inspect.getmro(delegate_class)):\n            delegated_attrs.update(klass.__dict__)\n        for (attrname, delegate_attr) in delegated_attrs.items():\n            if (attrname not in attrs):\n                if getattr(delegate_attr, 'is_async_method', False):\n                    sync_method = Sync(attrname, delegate_attr.has_write_concern)\n                    setattr(new_class, attrname, sync_method)\n                elif isinstance(delegate_attr, Unwrap):\n                    sync_method = Sync(attrname, delegate_attr.prop.has_write_concern)\n                    setattr(new_class, attrname, sync_method)\n                elif getattr(delegate_attr, 'is_motorcursor_chaining_method', False):\n                    wrapper = WrapOutgoing()\n                    wrapper.name = attrname\n                    setattr(new_class, attrname, wrapper)\n                elif isinstance(delegate_attr, ReadOnlyPropertyDescriptor):\n                    setattr(new_class, attrname, delegate_attr)\n    for (name, attr) in attrs.items():\n        if isinstance(attr, (MotorAttributeFactory, SynchroProperty, WrapOutgoing)):\n            attr.name = name\n    return new_class\n", "label": 1}
{"function": "\n\ndef _check_true_dir(self, text):\n    is_rtl = False\n    is_ltr = False\n    quoted_text = False\n    last_inline_html_char_pos = text.rfind('>')\n    if (last_inline_html_char_pos > (- 1)):\n        it_here = text[(last_inline_html_char_pos + 1):]\n    else:\n        it_here = text\n    for ch in it_here:\n        res = UD.bidirectional(ch)\n        if (ch == '\"'):\n            quoted_text = (not quoted_text)\n        elif ((not quoted_text) and (res in {'R', 'AL'})):\n            is_rtl = True\n        elif ((not quoted_text) and (res == 'L')):\n            is_ltr = True\n    if is_rtl:\n        return 'rtl'\n    elif is_ltr:\n        return 'ltr'\n    else:\n        return 'auto'\n", "label": 1}
{"function": "\n\ndef _get_pages(self, locations, project_name):\n    '\\n        Yields (page, page_url) from the given locations, skipping\\n        locations that have errors, and adding download/homepage links\\n        '\n    all_locations = list(locations)\n    seen = set()\n    normalized = normalize_name(project_name)\n    while all_locations:\n        location = all_locations.pop(0)\n        if (location in seen):\n            continue\n        seen.add(location)\n        page = self._get_page(location)\n        if (page is None):\n            continue\n        (yield page)\n        for link in page.rel_links():\n            if ((normalized not in self.allow_external) and (not self.allow_all_external)):\n                self.need_warn_external = True\n                logger.debug('Not searching %s for files because external urls are disallowed.', link)\n                continue\n            if ((link.trusted is not None) and (not link.trusted) and (normalized not in self.allow_unverified)):\n                logger.debug('Not searching %s for urls, it is an untrusted link and cannot produce safe or verifiable files.', link)\n                self.need_warn_unverified = True\n                continue\n            all_locations.append(link)\n", "label": 1}
{"function": "\n\ndef test_getitem(session):\n    set_ = session.set(key('test_sortedset_getitem'), S('abc'), SortedSet)\n    assert (set_['a'] == 1)\n    assert (set_['b'] == 1)\n    assert (set_['c'] == 1)\n    with raises(KeyError):\n        set_['d']\n    with raises(TypeError):\n        set_[123]\n    set_.update(a=2.1, c=(- 2))\n    assert (set_['a'] == 3.1)\n    assert (set_['b'] == 1)\n    assert (set_['c'] == (- 1))\n    setx = session.set(key('test_sortedsetx_getitem'), S([1, 2, 3]), IntSet)\n    assert (setx[1] == 1)\n    assert (setx[2] == 1)\n    assert (setx[3] == 1)\n    with raises(KeyError):\n        setx[4]\n    with raises(TypeError):\n        setx['a']\n    setx.update({\n        1: 2.1,\n        3: (- 2),\n    })\n    assert (setx[1] == 3.1)\n    assert (setx[2] == 1)\n    assert (setx[3] == (- 1))\n", "label": 1}
{"function": "\n\ndef __new__(mcs, cls_name, cls_bases, cls_attrs):\n    for (name, attr) in cls_attrs.items():\n        if (getattr(attr, '_unguarded', False) or (name in mcs.ALWAYS_UNGUARDED)):\n            continue\n        if name.startswith('_'):\n            continue\n        if isinstance(attr, type):\n            continue\n        is_property = isinstance(attr, property)\n        if (not (callable(attr) or is_property)):\n            continue\n        if is_property:\n            property_methods = defaultdict(None)\n            for fn_name in ('fdel', 'fset', 'fget'):\n                prop_fn = getattr(cls_attrs[name], fn_name, None)\n                if (prop_fn is not None):\n                    if getattr(prop_fn, '_unguarded', False):\n                        property_methods[fn_name] = prop_fn\n                    else:\n                        property_methods[fn_name] = pre_verify(prop_fn)\n            cls_attrs[name] = property(**property_methods)\n        else:\n            cls_attrs[name] = pre_verify(attr)\n    return super(_PageObjectMetaclass, mcs).__new__(mcs, cls_name, cls_bases, cls_attrs)\n", "label": 1}
{"function": "\n\ndef _get_desktop_streams(self, channel_id):\n    password = self.options.get('password')\n    channel = self._get_module_info('channel', channel_id, password, schema=_channel_schema)\n    if (not isinstance(channel.get('stream'), list)):\n        raise NoStreamsError(self.url)\n    streams = {\n        \n    }\n    for provider in channel['stream']:\n        if (provider['name'] == 'uhs_akamai'):\n            continue\n        provider_url = provider['url']\n        provider_name = provider['name']\n        for (stream_index, stream_info) in enumerate(provider['streams']):\n            stream = None\n            stream_height = int(stream_info.get('height', 0))\n            stream_name = stream_info.get('description')\n            if (not stream_name):\n                if (stream_height > 0):\n                    if (not stream_info.get('isTranscoded')):\n                        stream_name = '{0}p+'.format(stream_height)\n                    else:\n                        stream_name = '{0}p'.format(stream_height)\n                else:\n                    stream_name = 'live'\n            if (stream_name in streams):\n                provider_name_clean = provider_name.replace('uhs_', '')\n                stream_name += '_alt_{0}'.format(provider_name_clean)\n            if provider_name.startswith('uhs_'):\n                stream = UHSStream(self.session, channel_id, self.url, provider_name, stream_index, password)\n            elif provider_url.startswith('rtmp'):\n                playpath = stream_info['streamName']\n                stream = self._create_rtmp_stream(provider_url, playpath)\n            if stream:\n                streams[stream_name] = stream\n    return streams\n", "label": 1}
{"function": "\n\ndef write(self, oprot):\n    if ((oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))\n        return\n    oprot.writeStructBegin('Shaping')\n    if (self.rate is not None):\n        oprot.writeFieldBegin('rate', TType.I32, 1)\n        oprot.writeI32(self.rate)\n        oprot.writeFieldEnd()\n    if (self.delay is not None):\n        oprot.writeFieldBegin('delay', TType.STRUCT, 2)\n        self.delay.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.loss is not None):\n        oprot.writeFieldBegin('loss', TType.STRUCT, 3)\n        self.loss.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.reorder is not None):\n        oprot.writeFieldBegin('reorder', TType.STRUCT, 4)\n        self.reorder.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.corruption is not None):\n        oprot.writeFieldBegin('corruption', TType.STRUCT, 5)\n        self.corruption.write(oprot)\n        oprot.writeFieldEnd()\n    if (self.iptables_options is not None):\n        oprot.writeFieldBegin('iptables_options', TType.LIST, 6)\n        oprot.writeListBegin(TType.STRING, len(self.iptables_options))\n        for iter6 in self.iptables_options:\n            oprot.writeString(iter6)\n        oprot.writeListEnd()\n        oprot.writeFieldEnd()\n    oprot.writeFieldStop()\n    oprot.writeStructEnd()\n", "label": 1}
{"function": "\n\ndef test_operations():\n    per = SeqPer((1, 2), (n, 0, oo))\n    per2 = SeqPer((2, 4), (n, 0, oo))\n    form = SeqFormula((n ** 2))\n    form2 = SeqFormula((n ** 3))\n    assert (((per + form) + form2) == SeqAdd(per, form, form2))\n    assert (((per + form) - form2) == SeqAdd(per, form, (- form2)))\n    assert (((per + form) - S.EmptySequence) == SeqAdd(per, form))\n    assert (((per + per2) + form) == SeqAdd(SeqPer((3, 6), (n, 0, oo)), form))\n    assert ((S.EmptySequence - per) == (- per))\n    assert ((form + form) == SeqFormula((2 * (n ** 2))))\n    assert (((per * form) * form2) == SeqMul(per, form, form2))\n    assert ((form * form) == SeqFormula((n ** 4)))\n    assert ((form * (- form)) == SeqFormula((- (n ** 4))))\n    assert ((form * (per + form2)) == SeqMul(form, SeqAdd(per, form2)))\n    assert ((form * (per + per)) == SeqMul(form, per2))\n    assert (form.coeff_mul(m) == SeqFormula((m * (n ** 2)), (n, 0, oo)))\n    assert (per.coeff_mul(m) == SeqPer((m, (2 * m)), (n, 0, oo)))\n", "label": 1}
{"function": "\n\ndef rAssertAlmostEqual(self, a, b, rel_err=2e-15, abs_err=5e-323, msg=None):\n    'Fail if the two floating-point numbers are not almost equal.\\n\\n        Determine whether floating-point values a and b are equal to within\\n        a (small) rounding error.  The default values for rel_err and\\n        abs_err are chosen to be suitable for platforms where a float is\\n        represented by an IEEE 754 double.  They allow an error of between\\n        9 and 19 ulps.\\n        '\n    if math.isnan(a):\n        if math.isnan(b):\n            return\n        self.fail((msg or '{!r} should be nan'.format(b)))\n    if math.isinf(a):\n        if (a == b):\n            return\n        self.fail((msg or 'finite result where infinity expected: expected {!r}, got {!r}'.format(a, b)))\n    if ((not a) and (not b)):\n        if (math.copysign(1.0, a) != math.copysign(1.0, b)):\n            self.fail((msg or 'zero has wrong sign: expected {!r}, got {!r}'.format(a, b)))\n    try:\n        absolute_error = abs((b - a))\n    except OverflowError:\n        pass\n    else:\n        if (absolute_error <= max(abs_err, (rel_err * abs(a)))):\n            return\n    self.fail((msg or '{!r} and {!r} are not sufficiently close'.format(a, b)))\n", "label": 1}
{"function": "\n\ndef test_fetchable_get_item():\n    call_counter = mock.Mock()\n\n    def fetch():\n        for i in range(3):\n            for i in range(5):\n                (yield i)\n            call_counter()\n    fetcher = mock.Mock(fetch=fetch)\n    fetchable = util.Fetchable(fetcher)\n    assert (fetchable[:2] == [0, 1])\n    assert (fetchable[:3] == [0, 1, 2])\n    assert (fetchable[3] == 3)\n    assert (call_counter.call_count == 0)\n    assert (fetchable[9] == 4)\n    assert (call_counter.call_count == 1)\n    assert (fetchable[2:3] == [2])\n    assert (fetchable[:] == fetchable[:])\n    assert (len(fetchable[:]) == 15)\n    assert (fetchable[:100000] == fetchable[:])\n    assert (fetchable[100000:] == [])\n    assert (fetchable[100000:10000000] == [])\n    with pytest.raises(IndexError):\n        fetchable[12312312]\n", "label": 1}
{"function": "\n\ndef testMemory(self):\n    global logger\n    res = Memory(logger).check({\n        \n    })\n    if Platform.is_linux():\n        MEM_METRICS = ['swapTotal', 'swapFree', 'swapPctFree', 'swapUsed', 'physTotal', 'physFree', 'physUsed', 'physBuffers', 'physCached', 'physUsable', 'physPctUsable', 'physShared']\n        for k in MEM_METRICS:\n            if ((k == 'swapPctFree') and (res['swapTotal'] == 0)):\n                continue\n            assert (k in res), res\n        assert (res['swapTotal'] == (res['swapFree'] + res['swapUsed']))\n        assert (res['physTotal'] == (res['physFree'] + res['physUsed']))\n    elif (sys.platform == 'darwin'):\n        for k in ('swapFree', 'swapUsed', 'physFree', 'physUsed'):\n            assert (k in res), res\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    self.login_default_admin()\n    project = self.create_project()\n    plan = self.create_plan(project, label='Foo')\n    step = self.create_step(plan=plan)\n    self.login_default_admin()\n    path = '/api/0/steps/{0}/'.format(step.id.hex)\n    resp = self.client.post(path, data={\n        'order': 1,\n        'implementation': 'changes.buildsteps.dummy.DummyBuildStep',\n        'data': '{}',\n        'build.timeout': '1',\n    })\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (data['data'] == '{}')\n    assert (data['order'] == 1)\n    assert (data['implementation'] == 'changes.buildsteps.dummy.DummyBuildStep')\n    assert (data['options'] == {\n        'build.timeout': '1',\n    })\n    db.session.expire(step)\n    step = Step.query.get(step.id)\n    assert (step.data == {\n        \n    })\n    assert (step.order == 1)\n    assert (step.implementation == 'changes.buildsteps.dummy.DummyBuildStep')\n    options = list(ItemOption.query.filter((ItemOption.item_id == step.id)))\n    assert (len(options) == 1)\n    assert (options[0].name == 'build.timeout')\n    assert (options[0].value == '1')\n", "label": 1}
{"function": "\n\ndef __init__(self, im):\n    data = None\n    colortable = None\n    if hasattr(im, 'toUtf8'):\n        im = unicode(im.toUtf8(), 'utf-8')\n    if Image.isStringType(im):\n        im = Image.open(im)\n    if (im.mode == '1'):\n        format = QImage.Format_Mono\n    elif (im.mode == 'L'):\n        format = QImage.Format_Indexed8\n        colortable = []\n        for i in range(256):\n            colortable.append(rgb(i, i, i))\n    elif (im.mode == 'P'):\n        format = QImage.Format_Indexed8\n        colortable = []\n        palette = im.getpalette()\n        for i in range(0, len(palette), 3):\n            colortable.append(rgb(*palette[i:(i + 3)]))\n    elif (im.mode == 'RGB'):\n        data = im.tostring('raw', 'BGRX')\n        format = QImage.Format_RGB32\n    elif (im.mode == 'RGBA'):\n        try:\n            data = im.tostring('raw', 'BGRA')\n        except SystemError:\n            (r, g, b, a) = im.split()\n            im = Image.merge('RGBA', (b, g, r, a))\n        format = QImage.Format_ARGB32\n    else:\n        raise ValueError(('unsupported image mode %r' % im.mode))\n    self.__data = (data or im.tostring())\n    QImage.__init__(self, self.__data, im.size[0], im.size[1], format)\n    if colortable:\n        self.setColorTable(colortable)\n", "label": 1}
{"function": "\n\ndef get_insert_pt(self, view):\n    sel = view.sel()\n    pt = sel[0].end()\n    content = view.substr(view.line(pt))\n    wrap_width = get_wrap_width(view)\n    if (len(content) <= wrap_width):\n        return None\n    if view.settings().get('auto_wrap_beyond_only', False):\n        if (view.rowcol(pt)[1] < wrap_width):\n            return None\n    default = ['\\\\[', '\\\\(', '\\\\{', ' ', '\\\\n']\n    if view.score_selector(pt, 'text.tex.latex'):\n        default = (['\\\\\\\\left\\\\\\\\.', '\\\\\\\\left.', '\\\\\\\\\\\\{'] + default)\n    break_chars = '|'.join(view.settings().get('auto_wrap_break_patterns', default))\n    results = re.finditer(break_chars, content)\n    indices = ([m.start(0) for m in results] + [len(content)])\n    index = next((x[0] for x in enumerate(indices) if (x[1] > wrap_width)))\n    if (view.settings().get('auto_wrap_break_long_word', True) and (index > 0)):\n        return (view.line(pt).begin() + indices[(index - 1)])\n    else:\n        if (index == (len(indices) - 1)):\n            return None\n        return (view.line(pt).begin() + indices[index])\n", "label": 1}
{"function": "\n\ndef initialize(self, context):\n    cmd_options = {\n        \n    }\n    if (context.device.get_sdk_version() >= 23):\n        if self.app_names:\n            cmd_options['-a'] = ','.join(self.app_names)\n        if self.buffer_size:\n            cmd_options['-b'] = self.buffer_size\n        if self.use_circular_buffer:\n            cmd_options['-c'] = None\n        if self.kernel_functions:\n            cmd_options['-k'] = ','.join(self.kernel_functions)\n        if self.ignore_signals:\n            cmd_options['-n'] = None\n        opt_string = ''.join(['{} {} '.format(name, (value or '')) for (name, value) in cmd_options.iteritems()])\n        self.start_cmd = 'atrace --async_start {} {}'.format(opt_string, ' '.join(self.categories))\n        self.output_file = os.path.join(self.device.working_directory, 'atrace.txt')\n        self.stop_cmd = 'atrace --async_stop {} > {}'.format(('-z' if self.compress_trace else ''), self.output_file)\n        available_categories = [cat.strip().split(' - ')[0] for cat in context.device.execute('atrace --list_categories').splitlines()]\n        for category in self.categories:\n            if (category not in available_categories):\n                raise ConfigError(\"Unknown category '{}'; Must be one of: {}\".format(category, available_categories))\n    else:\n        raise InstrumentError('Only android devices with an API level >= 23 can use systrace properly')\n", "label": 1}
{"function": "\n\ndef activate(self, test=False):\n    if (self.is_active and (not test)):\n        return True\n    logger.debug('Site activation started')\n    for dashboard in self.dashboards:\n        for report in dashboard.reports:\n            ct = ContentType.objects.get_for_model(report.model)\n            (report.object, created) = Report.objects.get_or_create(key=report.key, contenttype=ct)\n            if created:\n                logger.debug(('Reportobject for report %s created' % report.key))\n    register_settings = list(self.settings.keys())\n    for setting in Configuration.objects.all():\n        key = '.'.join((setting.app_label, setting.field_name))\n        if (key in self.settings):\n            if (not setting.active):\n                setting.active = True\n                setting.save()\n            register_settings.remove(key)\n        elif setting.active:\n            setting.active = False\n            setting.save()\n    if register_settings:\n        logger.debug('Need to register new settings')\n        for setting in register_settings:\n            (app, name) = setting.split('.', 1)\n            Configuration.objects.create(app_label=app, field_name=name)\n            logger.debug(('Registered setting %s' % setting))\n    self.is_active = True\n    logger.debug('Site is now active')\n    return True\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    a = self.msg\n    b = ''\n    if self.http_scheme:\n        b += ('%s://' % self.http_scheme)\n    if self.http_host:\n        b += self.http_host\n    if self.http_port:\n        b += (':%s' % self.http_port)\n    if self.http_path:\n        b += self.http_path\n    if self.http_query:\n        b += ('?%s' % self.http_query)\n    if self.http_status:\n        if b:\n            b = ('%s %s' % (b, self.http_status))\n        else:\n            b = str(self.http_status)\n    if self.http_reason:\n        if b:\n            b = ('%s %s' % (b, self.http_reason))\n        else:\n            b = ('- %s' % self.http_reason)\n    if self.http_device:\n        if b:\n            b = ('%s: device %s' % (b, self.http_device))\n        else:\n            b = ('device %s' % self.http_device)\n    if self.http_response_content:\n        if (len(self.http_response_content) <= 60):\n            b += ('   %s' % self.http_response_content)\n        else:\n            b += ('  [first 60 chars of response] %s' % self.http_response_content[:60])\n    return ((b and ('%s: %s' % (a, b))) or a)\n", "label": 1}
{"function": "\n\ndef execute_sql(self, sql, params=None, require_commit=True, named_cursor=False):\n    logger.debug((sql, params))\n    use_named_cursor = (named_cursor or (self.server_side_cursors and sql.lower().startswith('select')))\n    with self.exception_wrapper():\n        if use_named_cursor:\n            cursor = self.get_cursor(name=str(uuid.uuid1()))\n            require_commit = False\n        else:\n            cursor = self.get_cursor()\n        try:\n            cursor.execute(sql, (params or ()))\n        except Exception as exc:\n            if (self.get_autocommit() and self.autorollback):\n                self.rollback()\n            raise\n        else:\n            if (require_commit and self.get_autocommit()):\n                self.commit()\n    return cursor\n", "label": 1}
{"function": "\n\ndef __init__(self, parent=None, **traits):\n    super(TableEditorToolbar, self).__init__(**traits)\n    editor = self.editor\n    factory = editor.factory\n    actions = []\n    if (factory.sortable and (not factory.sort_model)):\n        actions.append(self.no_sort)\n    if ((not editor.in_column_mode) and factory.reorderable):\n        actions.append(self.move_up)\n        actions.append(self.move_down)\n    if (editor.in_row_mode and (factory.search is not None)):\n        actions.append(self.search)\n    if factory.editable:\n        if ((factory.row_factory is not None) and (not factory.auto_add)):\n            actions.append(self.add)\n        if ((factory.deletable != False) and (not editor.in_column_mode)):\n            actions.append(self.delete)\n    if factory.configurable:\n        actions.append(self.prefs)\n    if (len(actions) > 0):\n        toolbar = ToolBar(*actions, image_size=(16, 16), show_tool_names=False, show_divider=False)\n        self.control = toolbar.create_tool_bar(parent, self)\n        self.control.SetBackgroundColour(parent.GetBackgroundColour())\n        self.control.SetSize(wx.Size((23 * len(actions)), 16))\n", "label": 1}
{"function": "\n\ndef on_changed(self, which):\n    if (not hasattr(self, '_lpl')):\n        self.add_dterm('_lpl', maximum(multiply(a=multiply()), 0.0))\n    if (not hasattr(self, 'ldn')):\n        self.ldn = LightDotNormal((self.v.r.size / 3))\n    if (not hasattr(self, 'vn')):\n        logger.info('LambertianPointLight using auto-normals. This will be slow for derivative-free computations.')\n        self.vn = VertNormals(f=self.f, v=self.v)\n        self.vn.needs_autoupdate = True\n    if (('v' in which) and hasattr(self.vn, 'needs_autoupdate') and self.vn.needs_autoupdate):\n        self.vn.v = self.v\n    ldn_args = {k: getattr(self, k) for k in which if (k in ('light_pos', 'v', 'vn'))}\n    if (len(ldn_args) > 0):\n        self.ldn.set(**ldn_args)\n        self._lpl.a.a.a = self.ldn.reshape(((- 1), 1))\n    if (('num_verts' in which) or ('light_color' in which)):\n        self._lpl.a.a.b = self.light_color.reshape((1, self.num_channels))\n    if ('vc' in which):\n        self._lpl.a.b = self.vc.reshape(((- 1), self.num_channels))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.a = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.I32):\n                self.b = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_application_key_ != x.has_application_key_):\n        return 0\n    if (self.has_application_key_ and (self.application_key_ != x.application_key_)):\n        return 0\n    if (self.has_message_ != x.has_message_):\n        return 0\n    if (self.has_message_ and (self.message_ != x.message_)):\n        return 0\n    if (self.has_tag_ != x.has_tag_):\n        return 0\n    if (self.has_tag_ and (self.tag_ != x.tag_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef strChanger(s, a):\n    sList = list(s)\n    for x in a:\n        for i in range(len(s)):\n            if (((i + 1) < len(s)) and (((i + 1) % x) == 0)):\n                if ((ord(sList[(i + 1)]) < 122) and (ord(sList[(i + 1)]) >= 90)):\n                    sList[(i + 1)] = chr((ord(sList[(i + 1)]) + 1))\n                elif (ord(sList[(i + 1)]) == 122):\n                    sList[(i + 1)] = chr(97)\n                elif ((ord(sList[(i + 1)]) < 90) and (ord(sList[(i + 1)]) >= 65)):\n                    sList[(i + 1)] = chr((ord(sList[(i + 1)]) + 1))\n                elif (ord(sList[(i + 1)]) == 90):\n                    sList[(i + 1)] = chr(65)\n    newS = ''.join(sList)\n    return newS\n", "label": 1}
{"function": "\n\ndef print_help(self):\n    '\\n        Print the help menu.\\n        '\n    print(('\\n  %s %s' % ((self._title or self._name), (self._version or ''))))\n    if self._usage:\n        print(('\\n  %s' % self._usage))\n    else:\n        cmd = self._name\n        if (hasattr(self, '_parent') and isinstance(self._parent, Command)):\n            cmd = ('%s %s' % (self._parent._name, cmd))\n        if self._command_list:\n            usage = ('Usage: %s <command> [option]' % cmd)\n        else:\n            usage = ('Usage: %s [option]' % cmd)\n        pos = ' '.join([('<%s>' % name) for name in self._positional_list])\n        print(('\\n  %s %s' % (usage, pos)))\n    arglen = max((len(o.name) for o in self._option_list))\n    arglen += 2\n    self.print_title('\\n  Options:\\n')\n    for o in self._option_list:\n        print(('    %s %s' % (_pad(o.name, arglen), (o.description or ''))))\n    print('')\n    if self._command_list:\n        self.print_title('  Commands:\\n')\n        for cmd in self._command_list:\n            if isinstance(cmd, Command):\n                name = _pad(cmd._name, arglen)\n                desc = (cmd._description or '')\n                print(('    %s %s' % (_pad(name, arglen), desc)))\n        print('')\n    if self._help_footer:\n        print(self._help_footer)\n        print('')\n    return self\n", "label": 1}
{"function": "\n\ndef __str__(self):\n    'Return the string value of the header.'\n    self._normalize()\n    uchunks = []\n    lastcs = None\n    lastspace = None\n    for (string, charset) in self._chunks:\n        nextcs = charset\n        if (nextcs == _charset.UNKNOWN8BIT):\n            original_bytes = string.encode('ascii', 'surrogateescape')\n            string = original_bytes.decode('ascii', 'replace')\n        if uchunks:\n            hasspace = (string and self._nonctext(string[0]))\n            if (lastcs not in (None, 'us-ascii')):\n                if ((nextcs in (None, 'us-ascii')) and (not hasspace)):\n                    uchunks.append(SPACE)\n                    nextcs = None\n            elif ((nextcs not in (None, 'us-ascii')) and (not lastspace)):\n                uchunks.append(SPACE)\n        lastspace = (string and self._nonctext(string[(- 1)]))\n        lastcs = nextcs\n        uchunks.append(string)\n    return EMPTYSTRING.join(uchunks)\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    fake_author_id = uuid4()\n    project = self.create_project()\n    self.create_build(project)\n    path = '/api/0/authors/{0}/builds/'.format(fake_author_id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 404)\n    data = self.unserialize(resp)\n    assert (len(data) == 0)\n    author = Author(email=self.default_user.email, name='Foo Bar')\n    db.session.add(author)\n    build = self.create_build(project, author=author)\n    path = '/api/0/authors/{0}/builds/'.format(author.id.hex)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 401)\n    self.login(self.default_user)\n    path = '/api/0/authors/me/builds/'\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 1)\n    assert (data[0]['id'] == build.id.hex)\n    (username, domain) = self.default_user.email.split('@', 1)\n    author = self.create_author('{}+foo@{}'.format(username, domain))\n    self.create_build(project, author=author)\n    resp = self.client.get(path)\n    assert (resp.status_code == 200)\n    data = self.unserialize(resp)\n    assert (len(data) == 2)\n", "label": 1}
{"function": "\n\ndef __unicode__(self):\n    qs = ['SELECT']\n    if self.distinct_fields:\n        if self.count:\n            qs += ['DISTINCT COUNT({0})'.format(', '.join(['\"{0}\"'.format(f) for f in self.distinct_fields]))]\n        else:\n            qs += ['DISTINCT {0}'.format(', '.join(['\"{0}\"'.format(f) for f in self.distinct_fields]))]\n    elif self.count:\n        qs += ['COUNT(*)']\n    else:\n        qs += [(', '.join(['\"{0}\"'.format(f) for f in self.fields]) if self.fields else '*')]\n    qs += ['FROM', self.table]\n    if self.where_clauses:\n        qs += [self._where]\n    if (self.order_by and (not self.count)):\n        qs += ['ORDER BY {0}'.format(', '.join((six.text_type(o) for o in self.order_by)))]\n    if self.limit:\n        qs += ['LIMIT {0}'.format(self.limit)]\n    if self.allow_filtering:\n        qs += ['ALLOW FILTERING']\n    return ' '.join(qs)\n", "label": 1}
{"function": "\n\ndef _get_python_variables(python_exe, variables, imports=['import sys']):\n    'Run a python interpreter and print some variables'\n    program = list(imports)\n    program.append('')\n    for v in variables:\n        program.append(('print(repr(%s))' % v))\n    os_env = dict(os.environ)\n    try:\n        del os_env['MACOSX_DEPLOYMENT_TARGET']\n    except KeyError:\n        pass\n    proc = Utils.pproc.Popen([python_exe, '-c', '\\n'.join(program)], stdout=Utils.pproc.PIPE, env=os_env)\n    output = proc.communicate()[0].split('\\n')\n    if proc.returncode:\n        if Options.options.verbose:\n            warn(('Python program to extract python configuration variables failed:\\n%s' % '\\n'.join([('line %03i: %s' % ((lineno + 1), line)) for (lineno, line) in enumerate(program)])))\n        raise RuntimeError\n    return_values = []\n    for s in output:\n        s = s.strip()\n        if (not s):\n            continue\n        if (s == 'None'):\n            return_values.append(None)\n        elif ((s[0] == \"'\") and (s[(- 1)] == \"'\")):\n            return_values.append(s[1:(- 1)])\n        elif s[0].isdigit():\n            return_values.append(int(s))\n        else:\n            break\n    return return_values\n", "label": 1}
{"function": "\n\ndef _run_server_as_subprocess():\n    args = sys.argv[2:]\n    (options, remainder) = getopt.getopt(args, '', ['debug', 'reload', 'host=', 'port=', 'server=', 'baseurl=', 'runtimepath='])\n    host = 'localhost'\n    port = 8080\n    debug = True\n    reloader = False\n    server = 'kokoro'\n    runtime_path = '.runtime/'\n    base_url = '/'\n    for (opt, arg) in options:\n        if (opt[0:2] == '--'):\n            opt = opt[2:]\n        if (opt == 'debug'):\n            debug = True\n        elif (opt == 'reload'):\n            reloader = True\n        elif (opt == 'host'):\n            host = arg\n        elif (opt == 'port'):\n            port = arg\n        elif (opt == 'server'):\n            server = arg\n        elif (opt == 'runtimepath'):\n            runtime_path = arg\n        elif (opt == 'baseurl'):\n            base_url = arg\n    SCRIPT_PATH = os.path.abspath(__file__)\n    RUN_COMMAND = ('%s %s' % (sys.executable, SCRIPT_PATH))\n    ARGUMENTS = ('run_server_once --host=%s --port=%d --server=%s --baseurl=%s --runtimepath=%s' % (host, port, server, base_url, runtime_path))\n    if reloader:\n        ARGUMENTS += ' --reload'\n    if debug:\n        ARGUMENTS += ' --debug'\n    RUN_COMMAND = ((RUN_COMMAND + ' ') + ARGUMENTS)\n    if hasattr(os, 'setsid'):\n        return subprocess.Popen(RUN_COMMAND, shell=True, preexec_fn=os.setsid)\n    else:\n        return subprocess.Popen(RUN_COMMAND, shell=True)\n", "label": 1}
{"function": "\n\ndef _parse_binary(stream, ptr=0):\n    i = ptr\n    deserialized = {\n        \n    }\n    while (i < len(stream)):\n        c = stream[i]\n        if (c == BIN_END):\n            return (deserialized, i)\n        (nodename, i) = _readtonull(stream, (i + 1))\n        if (c == BIN_NONE):\n            (deserialized[nodename], i) = _parse_binary(stream, (i + 1))\n        elif (c == BIN_STRING):\n            (deserialized[nodename], i) = _readtonull(stream, (i + 1))\n        elif (c == BIN_WIDESTRING):\n            raise Exception('NYI')\n        elif ((c == BIN_INT32) or (c == BIN_COLOR) or (c == BIN_POINTER)):\n            if ((len(stream) - i) < 4):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('i', stream, (i + 1))\n            (deserialized[nodename], i) = (value, (i + 4))\n        elif (c == BIN_UINT64):\n            if ((len(stream) - i) < 8):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('q', stream, (i + 1))\n            (deserialized[nodename], i) = (value, (i + 8))\n        elif (c == BIN_FLOAT32):\n            if ((len(stream) - i) < 4):\n                raise Exception('Invalid KV')\n            (value,) = struct.unpack_from('f', stream, (i + 1))\n            (deserialized[nodename], i) = (0, (i + 4))\n        else:\n            raise Exception('Unknown KV type')\n        i += 1\n    return (deserialized, i)\n", "label": 1}
{"function": "\n\n@memoize_generator\ndef process(self, stack, stream):\n    for (token_type, value) in stream:\n        if ((token_type in Name) and (value.upper() == 'INCLUDE')):\n            self.detected = True\n            continue\n        elif self.detected:\n            if (token_type in Whitespace):\n                continue\n            if (token_type in String.Symbol):\n                path = join(self.dirpath, value[1:(- 1)])\n                try:\n                    f = open(path)\n                    raw_sql = f.read()\n                    f.close()\n                except IOError as err:\n                    if self.raiseexceptions:\n                        raise\n                    (yield (Comment, ('-- IOError: %s\\n' % err)))\n                else:\n                    try:\n                        filtr = IncludeStatement(self.dirpath, (self.maxRecursive - 1), self.raiseexceptions)\n                    except ValueError as err:\n                        if self.raiseexceptions:\n                            raise\n                        (yield (Comment, ('-- ValueError: %s\\n' % err)))\n                    stack = FilterStack()\n                    stack.preprocess.append(filtr)\n                    for tv in stack.run(raw_sql):\n                        (yield tv)\n                self.detected = False\n            continue\n        (yield (token_type, value))\n", "label": 1}
{"function": "\n\ndef request(self, **kwargs):\n    '\\n        Perform network request.\\n\\n        You can specify grab settings in ``**kwargs``.\\n        Any keyword argument will be passed to ``self.config``.\\n\\n        Returns: ``Document`` objects.\\n        '\n    self.prepare_request(**kwargs)\n    refresh_count = 0\n    while True:\n        self.log_request()\n        try:\n            self.transport.request()\n        except error.GrabError:\n            self.reset_temporary_options()\n            self.save_failed_dump()\n            raise\n        else:\n            doc = self.process_request_result()\n            if self.config['follow_location']:\n                if (doc.code in (301, 302, 303, 307, 308)):\n                    if doc.headers.get('Location'):\n                        refresh_count += 1\n                        if (refresh_count > self.config['redirect_limit']):\n                            raise error.GrabTooManyRedirectsError()\n                        else:\n                            url = doc.headers.get('Location')\n                            self.prepare_request(url=self.make_url_absolute(url), referer=None)\n                            continue\n            if self.config['follow_refresh']:\n                refresh_url = self.doc.get_meta_refresh_url()\n                if (refresh_url is not None):\n                    refresh_count += 1\n                    if (refresh_count > self.config['redirect_limit']):\n                        raise error.GrabTooManyRedirectsError()\n                    else:\n                        self.prepare_request(url=self.make_url_absolute(refresh_url), referer=None)\n                        continue\n            return doc\n", "label": 1}
{"function": "\n\ndef update(self):\n    'Get the latest state of the sensor.'\n    data = ecobee.NETWORK\n    data.update()\n    for sensor in data.ecobee.get_remote_sensors(self.index):\n        for item in sensor['capability']:\n            if ((item['type'] == self.type) and (self.type == 'temperature') and (self.sensor_name == sensor['name'])):\n                self._state = (float(item['value']) / 10)\n            elif ((item['type'] == self.type) and (self.type == 'humidity') and (self.sensor_name == sensor['name'])):\n                self._state = item['value']\n            elif ((item['type'] == self.type) and (self.type == 'occupancy') and (self.sensor_name == sensor['name'])):\n                self._state = item['value']\n", "label": 1}
{"function": "\n\ndef check(self):\n    script = self.script\n    s = script.engine.current_scene\n    src = s.children[0]\n    g = src.children[0].children[1]\n    assert (g.glyph.glyph_source.glyph_position == 'center')\n    assert (g.glyph.glyph.vector_mode == 'use_normal')\n    assert (g.glyph.glyph.scale_factor == 0.5)\n    assert (g.actor.property.line_width == 1.0)\n    v = src.children[0].children[2]\n    glyph = v.glyph\n    gs = glyph.glyph_source\n    assert (gs.glyph_position == 'tail')\n    assert (gs.glyph_source == gs.glyph_list[1])\n    assert numpy.allclose(v.implicit_plane.normal, (0.0, 1.0, 0.0))\n    v = src.children[0].children[3]\n    glyph = v.glyph\n    gs = glyph.glyph_source\n    assert (gs.glyph_source == gs.glyph_list[2])\n    assert (gs.glyph_position == 'head')\n    assert numpy.allclose(v.implicit_plane.normal, (0.0, 1.0, 0.0))\n", "label": 1}
{"function": "\n\ndef create_slug(self, model_instance, add):\n    if (not isinstance(self._populate_from, (list, tuple))):\n        self._populate_from = (self._populate_from,)\n    slug_field = model_instance._meta.get_field(self.attname)\n    if (add or self.overwrite):\n        slug_for_field = (lambda field: self.slugify_func(getattr(model_instance, field)))\n        slug = self.separator.join(map(slug_for_field, self._populate_from))\n        next = 2\n    else:\n        slug = getattr(model_instance, self.attname)\n        return slug\n    slug_len = slug_field.max_length\n    if slug_len:\n        slug = slug[:slug_len]\n    slug = self._slug_strip(slug)\n    original_slug = slug\n    if self.allow_duplicates:\n        return slug\n    queryset = self.get_queryset(model_instance.__class__, slug_field)\n    if model_instance.pk:\n        queryset = queryset.exclude(pk=model_instance.pk)\n    kwargs = {\n        \n    }\n    for params in model_instance._meta.unique_together:\n        if (self.attname in params):\n            for param in params:\n                kwargs[param] = getattr(model_instance, param, None)\n    kwargs[self.attname] = slug\n    while ((not slug) or queryset.filter(**kwargs)):\n        slug = original_slug\n        end = ('%s%s' % (self.separator, next))\n        end_len = len(end)\n        if (slug_len and ((len(slug) + end_len) > slug_len)):\n            slug = slug[:(slug_len - end_len)]\n            slug = self._slug_strip(slug)\n        slug = ('%s%s' % (slug, end))\n        kwargs[self.attname] = slug\n        next += 1\n    return slug\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\n    'run controller method - old style'\n    if (not args):\n        args = (arg or '')\n    if dt:\n        if (not dn):\n            dn = dt\n        doc = frappe.get_doc(dt, dn)\n    else:\n        doc = frappe.get_doc(json.loads(docs))\n        doc._original_modified = doc.modified\n        doc.check_if_latest()\n    if (not doc.has_permission('read')):\n        frappe.msgprint(_('Not permitted'), raise_exception=True)\n    if doc:\n        try:\n            args = json.loads(args)\n        except ValueError:\n            args = args\n        (fnargs, varargs, varkw, defaults) = inspect.getargspec(getattr(doc, method))\n        if ((not fnargs) or ((len(fnargs) == 1) and (fnargs[0] == 'self'))):\n            r = doc.run_method(method)\n        elif (('args' in fnargs) or (not isinstance(args, dict))):\n            r = doc.run_method(method, args)\n        else:\n            r = doc.run_method(method, **args)\n        if r:\n            if cint(frappe.form_dict.get('as_csv')):\n                make_csv_output(r, doc.doctype)\n            else:\n                frappe.response['message'] = r\n        frappe.response.docs.append(doc)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_class_or_file_name_ != x.has_class_or_file_name_):\n        return 0\n    if (self.has_class_or_file_name_ and (self.class_or_file_name_ != x.class_or_file_name_)):\n        return 0\n    if (self.has_line_number_ != x.has_line_number_):\n        return 0\n    if (self.has_line_number_ and (self.line_number_ != x.line_number_)):\n        return 0\n    if (self.has_function_name_ != x.has_function_name_):\n        return 0\n    if (self.has_function_name_ and (self.function_name_ != x.function_name_)):\n        return 0\n    if (len(self.variables_) != len(x.variables_)):\n        return 0\n    for (e1, e2) in zip(self.variables_, x.variables_):\n        if (e1 != e2):\n            return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef _match_search_query(left, right):\n    left_filter = set([value for (param_name, value) in left if ('filter' in _maybe_decode(param_name))])\n    right_filter = set([value for (param_name, value) in right if ('filter' in _maybe_decode(param_name))])\n    left_rest = set([(param_name, value) for (param_name, value) in left if ('filter' not in _maybe_decode(param_name))])\n    right_rest = set([(param_name, value) for (param_name, value) in right if ('filter' not in _maybe_decode(param_name))])\n    try:\n        log.info(simplejson.dumps({\n            'filter_differences': list(left_filter.symmetric_difference(right_filter)),\n            'rest_differences': list(left_rest.symmetric_difference(right_rest)),\n        }, encoding='utf-8'))\n    except Exception as e:\n        log.warning(e)\n    return ((left_filter == right_filter) and (left_rest == right_rest))\n", "label": 1}
{"function": "\n\ndef user_defined_setup(config, inventory):\n    'Apply user defined entries from config into inventory.\\n\\n    :param config: ``dict``  User defined information\\n    :param inventory: ``dict``  Living dictionary of inventory\\n    '\n    hvs = inventory['_meta']['hostvars']\n    for (key, value) in config.iteritems():\n        if key.endswith('hosts'):\n            if (key not in inventory):\n                inventory[key] = {\n                    'hosts': [],\n                }\n            if (value is None):\n                return\n            for (_key, _value) in value.iteritems():\n                if (_key not in inventory['_meta']['hostvars']):\n                    inventory['_meta']['hostvars'][_key] = {\n                        \n                    }\n                hvs[_key].update({\n                    'ansible_ssh_host': _value['ip'],\n                    'container_address': _value['ip'],\n                    'is_metal': True,\n                    'physical_host_group': key,\n                })\n                properties = hvs[_key].get('properties')\n                if ((not properties) or (not isinstance(properties, dict))):\n                    hvs[_key]['properties'] = dict()\n                hvs[_key]['properties'].update({\n                    'is_metal': True,\n                })\n                if ('host_vars' in _value):\n                    for (_k, _v) in _value['host_vars'].items():\n                        hvs[_key][_k] = _v\n                append_if(array=USED_IPS, item=_value['ip'])\n                append_if(array=inventory[key]['hosts'], item=_key)\n", "label": 1}
{"function": "\n\ndef on_response(self, response):\n    if (not self.first_server_chunk_received):\n        self.first_server_chunk_received = True\n        if ((not self.first_client_chunk_received) and (response.startswith('220 ') or response.startswith('220-'))):\n            self.smtp_detected = True\n    if (not self.smtp_detected):\n        return response\n    if self.ehlo_response_pending:\n        self.ehlo_response_pending = False\n        if (not response.startswith('250-')):\n            return response\n        lines = [l.rstrip() for l in response.splitlines()]\n        starttls_line_index = (- 1)\n        for i in range(len(lines)):\n            line = lines[i]\n            if line[4:].lower().startswith('starttls'):\n                starttls_line_index = i\n                break\n        else:\n            self.smtp_detected = False\n            self.log(logging.DEBUG, 'No STARTTLS in EHLO response')\n            return response\n        if (starttls_line_index == (len(lines) - 1)):\n            lines = lines[:starttls_line_index]\n            lines[(- 1)] = ((lines[(- 1)][0:3] + ' ') + lines[(- 1)][4:])\n        else:\n            lines = (lines[:starttls_line_index] + lines[(starttls_line_index + 1):])\n        response = ('\\r\\n'.join(lines) + '\\r\\n')\n        self.server_starttls_stripped = True\n        self.log(logging.DEBUG, 'Stripped STARTTLS from EHLO response')\n    return response\n", "label": 1}
{"function": "\n\ndef ParseDepends(self, filename, must_exist=None, only_one=0):\n    '\\n        Parse a mkdep-style file for explicit dependencies.  This is\\n        completely abusable, and should be unnecessary in the \"normal\"\\n        case of proper SCons configuration, but it may help make\\n        the transition from a Make hierarchy easier for some people\\n        to swallow.  It can also be genuinely useful when using a tool\\n        that can write a .d file, but for which writing a scanner would\\n        be too complicated.\\n        '\n    filename = self.subst(filename)\n    try:\n        fp = open(filename, 'r')\n    except IOError:\n        if must_exist:\n            raise\n        return\n    lines = SCons.Util.LogicalLines(fp).readlines()\n    lines = [l for l in lines if (l[0] != '#')]\n    tdlist = []\n    for line in lines:\n        try:\n            (target, depends) = line.split(':', 1)\n        except (AttributeError, ValueError):\n            pass\n        else:\n            tdlist.append((target.split(), depends.split()))\n    if only_one:\n        targets = []\n        for td in tdlist:\n            targets.extend(td[0])\n        if (len(targets) > 1):\n            raise SCons.Errors.UserError((\"More than one dependency target found in `%s':  %s\" % (filename, targets)))\n    for (target, depends) in tdlist:\n        self.Depends(target, depends)\n", "label": 1}
{"function": "\n\ndef root(self, request, url):\n    '\\n        DEPRECATED. This function is the old way of handling URL resolution, and\\n        is deprecated in favor of real URL resolution -- see ``get_urls()``.\\n\\n        This function still exists for backwards-compatibility; it will be\\n        removed in Django 1.3.\\n        '\n    import warnings\n    warnings.warn('AdminSite.root() is deprecated; use include(admin.site.urls) instead.', DeprecationWarning)\n    if ((request.method == 'GET') and (not request.path.endswith('/'))):\n        return http.HttpResponseRedirect((request.path + '/'))\n    if settings.DEBUG:\n        self.check_dependencies()\n    self.root_path = re.sub((re.escape(url) + '$'), '', request.path)\n    url = url.rstrip('/')\n    if (url == 'logout'):\n        return self.logout(request)\n    if (not self.has_permission(request)):\n        return self.login(request)\n    if (url == ''):\n        return self.index(request)\n    elif (url == 'password_change'):\n        return self.password_change(request)\n    elif (url == 'password_change/done'):\n        return self.password_change_done(request)\n    elif (url == 'jsi18n'):\n        return self.i18n_javascript(request)\n    elif url.startswith('r/'):\n        from django.contrib.contenttypes.views import shortcut\n        return shortcut(request, *url.split('/')[1:])\n    elif ('/' in url):\n        return self.model_page(request, *url.split('/', 2))\n    else:\n        return self.app_index(request, url)\n    raise http.Http404('The requested admin page does not exist.')\n", "label": 1}
{"function": "\n\ndef _select_range(self, multiselect, keep_anchor, node, idx):\n    'Selects a range between self._anchor and node or idx.\\n        If multiselect is True, it will be added to the selection, otherwise\\n        it will unselect everything before selecting the range. This is only\\n        called if self.multiselect is True.\\n        If keep anchor is False, the anchor is moved to node. This should\\n        always be True for keyboard selection.\\n        '\n    select = self.select_node\n    sister_nodes = self.get_selectable_nodes()\n    end = (len(sister_nodes) - 1)\n    last_node = self._anchor\n    last_idx = self._anchor_idx\n    if (last_node is None):\n        last_idx = end\n        last_node = sister_nodes[end]\n    elif ((last_idx > end) or (sister_nodes[last_idx] != last_node)):\n        try:\n            last_idx = self.get_index_of_node(last_node, sister_nodes)\n        except ValueError:\n            return\n    if ((idx > end) or (sister_nodes[idx] != node)):\n        try:\n            idx = self.get_index_of_node(node, sister_nodes)\n        except ValueError:\n            return\n    if (last_idx > idx):\n        (last_idx, idx) = (idx, last_idx)\n    if (not multiselect):\n        self.clear_selection()\n    for item in sister_nodes[last_idx:(idx + 1)]:\n        select(item)\n    if keep_anchor:\n        self._anchor = last_node\n        self._anchor_idx = last_idx\n    else:\n        self._anchor = node\n        self._anchor_idx = idx\n    self._last_selected_node = node\n    self._last_node_idx = idx\n", "label": 1}
{"function": "\n\ndef _do_setup(self, data):\n    added_magic = []\n    for line in data.split('\\n'):\n        if ((line == 'info/names=') or (line.strip() == '')):\n            continue\n        (name, documentation) = line.split(' ', 1)\n        if (name == 'config/*'):\n            continue\n        if name.endswith('/*'):\n            bits = name[:(- 2)].split('/')\n            takes_arg = True\n        else:\n            bits = name.split('/')\n            takes_arg = False\n        mine = self\n        for bit in bits[:(- 1)]:\n            bit = bit.replace('-', '_')\n            if (bit in mine.attrs):\n                mine = mine.attrs[bit]\n                if (not isinstance(mine, MagicContainer)):\n                    raise RuntimeError(('Already had something: %s for %s' % (bit, name)))\n            else:\n                c = MagicContainer(bit)\n                added_magic.append(c)\n                mine._add_attribute(bit, c)\n                mine = c\n        n = bits[(- 1)].replace('-', '_')\n        if (n in mine.attrs):\n            raise RuntimeError(('Already had something: %s for %s' % (n, name)))\n        mine._add_attribute(n, ConfigMethod('/'.join(bits), self.protocol, takes_arg))\n    for c in added_magic:\n        c._setup_complete()\n    return None\n", "label": 1}
{"function": "\n\ndef build(ctx):\n    if ctx.options.dump_state:\n        ctx.db.dump_database()\n        return 0\n    if ctx.options.delete_function:\n        if (not ctx.db.delete_function(ctx.options.delete_function)):\n            raise fbuild.Error(('function %r not cached' % ctx.options.delete_function))\n        return 0\n    if ctx.options.delete_file:\n        if (not ctx.db.delete_file(ctx.options.delete_file)):\n            raise fbuild.Error(('file %r not cached' % ctx.options.delete_file))\n        return 0\n    targets = (ctx.args or ['build'])\n    if ('install' in targets):\n        if (targets[(- 1)] != 'install'):\n            raise fbuild.Error('install must be last target')\n        if (not (set(targets) - {'configure', 'install'})):\n            targets.insert((targets.index('install') - 1), 'build')\n    for target_name in targets:\n        if (target_name == 'install'):\n            install_files(ctx)\n        else:\n            target = fbuild.target.find(target_name)\n            target.function(ctx)\n    return 0\n", "label": 1}
{"function": "\n\ndef formfield_for_dbfield(self, db_field, request, **kwargs):\n    \"\\n        Hook for specifying the form Field instance for a given database Field\\n        instance.\\n\\n        If kwargs are given, they're passed to the form Field's constructor.\\n        \"\n    if db_field.choices:\n        return self.formfield_for_choice_field(db_field, request, **kwargs)\n    if (db_field.many_to_many or isinstance(db_field, models.ForeignKey)):\n        if (db_field.__class__ in self.formfield_overrides):\n            kwargs = dict(self.formfield_overrides[db_field.__class__], **kwargs)\n        if isinstance(db_field, models.ForeignKey):\n            formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)\n        elif db_field.many_to_many:\n            formfield = self.formfield_for_manytomany(db_field, request, **kwargs)\n        if (formfield and (db_field.name not in self.raw_id_fields)):\n            related_modeladmin = self.admin_site._registry.get(db_field.remote_field.model)\n            wrapper_kwargs = {\n                \n            }\n            if related_modeladmin:\n                wrapper_kwargs.update(can_add_related=related_modeladmin.has_add_permission(request), can_change_related=related_modeladmin.has_change_permission(request), can_delete_related=related_modeladmin.has_delete_permission(request))\n            formfield.widget = widgets.RelatedFieldWidgetWrapper(formfield.widget, db_field.remote_field, self.admin_site, **wrapper_kwargs)\n        return formfield\n    for klass in db_field.__class__.mro():\n        if (klass in self.formfield_overrides):\n            kwargs = dict(copy.deepcopy(self.formfield_overrides[klass]), **kwargs)\n            return db_field.formfield(**kwargs)\n    return db_field.formfield(**kwargs)\n", "label": 1}
{"function": "\n\ndef render_plugin(context, instance, placeholder, template, processors=None, current_app=None):\n    \"\\n    Renders a single plugin and applies the post processors to it's rendered\\n    content.\\n    \"\n    if current_app:\n        context['request'].current_app = current_app\n    if (not processors):\n        processors = []\n    if isinstance(template, six.string_types):\n        content = render_to_string(template, flatten_context(context))\n    elif (isinstance(template, Template) or (hasattr(template, 'template') and hasattr(template, 'render') and isinstance(template.template, Template))):\n        content = template.render(context)\n    else:\n        content = ''\n    for processor in iterload_objects(get_cms_setting('PLUGIN_PROCESSORS')):\n        content = processor(instance, placeholder, content, context)\n    for processor in processors:\n        content = processor(instance, placeholder, content, context)\n    for processor in DEFAULT_PLUGIN_PROCESSORS:\n        content = processor(instance, placeholder, content, context)\n    return content\n", "label": 1}
{"function": "\n\ndef pstream(self, stream, indent=0, multiline=False):\n    ' Pretty-formats the layout item to a stream.\\n        '\n    call = (self.__class__.__name__ + '(')\n    indent += len(call)\n    stream.write(call)\n    args = [(None, arg) for arg in self.pargs()]\n    traits = []\n    for (name, trait) in sorted(self.traits().iteritems()):\n        if ((not trait.pretty_skip) and (not trait.transient)):\n            value = getattr(self, name)\n            if (trait.default != value):\n                traits.append((name, value))\n    traits.sort()\n    args.extend(traits)\n    for (i, (name, value)) in enumerate(args):\n        arg_indent = indent\n        if name:\n            arg_indent += (len(name) + 1)\n            stream.write((name + '='))\n        if isinstance(value, LayoutItem):\n            value.pstream(stream, arg_indent, multiline)\n        else:\n            stream.write(repr(value))\n        if (i < (len(args) - 1)):\n            stream.write(',')\n            if multiline:\n                stream.write(('\\n' + (indent * ' ')))\n            else:\n                stream.write(' ')\n    stream.write(')')\n", "label": 1}
{"function": "\n\ndef process_m2m(self, instance, field):\n    auto_created_through_model = False\n    through = get_remote_field(field).through\n    auto_created_through_model = through._meta.auto_created\n    if auto_created_through_model:\n        return self.process_field(instance, field)\n    kwargs = {\n        \n    }\n    if (field.name in self.generate_m2m):\n        related_fks = [fk for fk in through._meta.fields if (isinstance(fk, related.ForeignKey) and (get_remote_field_to(fk) is get_remote_field_to(field)))]\n        self_fks = [fk for fk in through._meta.fields if (isinstance(fk, related.ForeignKey) and (get_remote_field_to(fk) is self.model))]\n        assert (len(related_fks) == 1)\n        assert (len(self_fks) == 1)\n        related_fk = related_fks[0]\n        self_fk = self_fks[0]\n        (min_count, max_count) = self.generate_m2m[field.name]\n        intermediary_model = generators.MultipleInstanceGenerator(AutoFixture(through, field_values={\n            self_fk.name: instance,\n            related_fk.name: generators.InstanceGenerator(autofixture.get(get_remote_field_to(field))),\n        }), min_count=min_count, max_count=max_count, **kwargs).generate()\n", "label": 1}
{"function": "\n\ndef geo_apps(namespace=True, runtests=False):\n    '\\n    Returns a list of GeoDjango test applications that reside in\\n    `django.contrib.gis.tests` that can be used with the current\\n    database and the spatial libraries that are installed.\\n    '\n    from django.db import connection\n    from django.contrib.gis.geos import GEOS_PREPARE\n    from django.contrib.gis.gdal import HAS_GDAL\n    apps = ['geoapp', 'relatedapp']\n    if (not connection.ops.mysql):\n        apps.append('distapp')\n    if (connection.ops.postgis and connection.ops.geography):\n        apps.append('geogapp')\n    if HAS_GDAL:\n        apps.extend(['geoadmin', 'layermap', 'inspectapp'])\n        if (connection.ops.postgis and GEOS_PREPARE):\n            apps.append('geo3d')\n    if runtests:\n        return [('django.contrib.gis.tests', app) for app in apps]\n    elif namespace:\n        return [('django.contrib.gis.tests.%s' % app) for app in apps]\n    else:\n        return apps\n", "label": 1}
{"function": "\n\ndef _build_http_request(url, method, headers=None, encoding=None, params=empty_params):\n    '\\n    Make an HTTP request and return an HTTP response.\\n    '\n    opts = {\n        'headers': (headers or {\n            \n        }),\n    }\n    if params.query:\n        opts['params'] = params.query\n    if ((params.body is not None) or params.data or params.files):\n        if (encoding == 'application/json'):\n            if (params.body is not None):\n                opts['json'] = params.body\n            else:\n                opts['json'] = params.data\n        elif (encoding == 'multipart/form-data'):\n            opts['data'] = params.data\n            opts['files'] = params.files\n        elif (encoding == 'application/x-www-form-urlencoded'):\n            opts['data'] = params.data\n        elif (encoding == 'application/octet-stream'):\n            opts['data'] = params.body\n            content_type = _get_content_type(params.body)\n            if content_type:\n                opts['headers']['content-type'] = content_type\n    request = requests.Request(method, url, **opts)\n    request = request.prepare()\n    return request\n", "label": 1}
{"function": "\n\n@client.set_process_callback\ndef process(frames):\n    'Main callback.'\n    events = {\n        \n    }\n    buf = memoryview(audioport.get_buffer()).cast('f')\n    for (offset, data) in midiport.incoming_midi_events():\n        if (len(data) == 3):\n            (status, pitch, vel) = bytes(data)\n            status >>= 4\n            if ((status == NOTEON) and (vel > 0)):\n                events.setdefault(offset, []).append((pitch, vel))\n            elif (status in (NOTEON, NOTEOFF)):\n                events.setdefault(offset, []).append((pitch, 0))\n            else:\n                pass\n        else:\n            pass\n    for i in range(len(buf)):\n        buf[i] = 0\n        try:\n            eventlist = events[i]\n        except KeyError:\n            pass\n        else:\n            for (pitch, vel) in eventlist:\n                if (pitch not in voices):\n                    if (not vel):\n                        break\n                    voices[pitch] = Voice(pitch)\n                voices[pitch].trigger(vel)\n        for voice in voices.values():\n            voice.update()\n            if (voice.weight > 0):\n                buf[i] += (voice.weight * math.sin(((2 * math.pi) * voice.time)))\n                voice.time += voice.time_increment\n                if (voice.time >= 1):\n                    voice.time -= 1\n    dead = [k for (k, v) in voices.items() if (v.weight <= 0)]\n    for pitch in dead:\n        del voices[pitch]\n", "label": 1}
{"function": "\n\ndef _process_dns_floatingip_update_precommit(self, context, floatingip_data):\n    plugin = manager.NeutronManager.get_service_plugins().get(service_constants.L3_ROUTER_NAT)\n    if (not utils.is_extension_supported(plugin, dns.Dns.get_alias())):\n        return\n    if (not self.dns_driver):\n        return\n    dns_data_db = context.session.query(FloatingIPDNS).filter_by(floatingip_id=floatingip_data['id']).one_or_none()\n    if (dns_data_db and dns_data_db['dns_name']):\n        return\n    (current_dns_name, current_dns_domain) = self._get_requested_state_for_external_dns_service_update(context, floatingip_data)\n    if dns_data_db:\n        if ((dns_data_db['published_dns_name'] != current_dns_name) or (dns_data_db['published_dns_domain'] != current_dns_domain)):\n            dns_actions_data = DNSActionsData(previous_dns_name=dns_data_db['published_dns_name'], previous_dns_domain=dns_data_db['published_dns_domain'])\n            if (current_dns_name and current_dns_domain):\n                dns_data_db['published_dns_name'] = current_dns_name\n                dns_data_db['published_dns_domain'] = current_dns_domain\n                dns_actions_data.current_dns_name = current_dns_name\n                dns_actions_data.current_dns_domain = current_dns_domain\n            else:\n                context.session.delete(dns_data_db)\n            return dns_actions_data\n        else:\n            return\n    if (current_dns_name and current_dns_domain):\n        context.session.add(FloatingIPDNS(floatingip_id=floatingip_data['id'], dns_name='', dns_domain='', published_dns_name=current_dns_name, published_dns_domain=current_dns_domain))\n        return DNSActionsData(current_dns_name=current_dns_name, current_dns_domain=current_dns_domain)\n", "label": 1}
{"function": "\n\ndef _generic_factor_list(expr, gens, args, method):\n    'Helper function for :func:`sqf_list` and :func:`factor_list`. '\n    options.allowed_flags(args, ['frac', 'polys'])\n    opt = options.build_options(gens, args)\n    expr = sympify(expr)\n    if (isinstance(expr, Expr) and (not expr.is_Relational)):\n        (numer, denom) = together(expr).as_numer_denom()\n        (cp, fp) = _symbolic_factor_list(numer, opt, method)\n        (cq, fq) = _symbolic_factor_list(denom, opt, method)\n        if (fq and (not opt.frac)):\n            raise PolynomialError(('a polynomial expected, got %s' % expr))\n        _opt = opt.clone(dict(expand=True))\n        for factors in (fp, fq):\n            for (i, (f, k)) in enumerate(factors):\n                if (not f.is_Poly):\n                    (f, _) = _poly_from_expr(f, _opt)\n                    factors[i] = (f, k)\n        fp = _sorted_factors(fp, method)\n        fq = _sorted_factors(fq, method)\n        if (not opt.polys):\n            fp = [(f.as_expr(), k) for (f, k) in fp]\n            fq = [(f.as_expr(), k) for (f, k) in fq]\n        coeff = (cp / cq)\n        if (not opt.frac):\n            return (coeff, fp)\n        else:\n            return (coeff, fp, fq)\n    else:\n        raise PolynomialError(('a polynomial expected, got %s' % expr))\n", "label": 1}
{"function": "\n\ndef test_result_generation_when_a_quarantined_test_has_two_cases():\n    jobstep = JobStep(id=uuid.uuid4(), project_id=uuid.uuid4(), job_id=uuid.uuid4())\n    fp = StringIO(SAMPLE_XUNIT_DOUBLE_CASES.replace('<testcase c', '<testcase quarantined=\"1\" c', 1).replace('<testcase c', '<testcase quarantined=\"1\" c', 1))\n    handler = XunitHandler(jobstep)\n    results = handler.get_tests(fp)\n    assert (len(results) == 2)\n    r1 = results[0]\n    assert (type(r1) is TestResult)\n    assert (r1.step == jobstep)\n    assert (r1.package is None)\n    assert (r1.name == 'test_simple.SampleTest.test_falsehood')\n    assert (r1.duration == 750.0)\n    assert (r1.result == Result.quarantined_failed)\n    assert (r1.message == 'test_simple.py:8: in test_falsehood\\n    assert False\\nE   AssertionError: assert False\\n\\ntest_simple.py:4: in tearDown\\n    1/0\\nE   ZeroDivisionError: integer division or modulo by zero')\n    assert (r1.reruns == 3)\n    r2 = results[1]\n    assert (type(r2) is TestResult)\n    assert (r2.step == jobstep)\n    assert (r2.package is None)\n    assert (r2.name == 'test_simple.SampleTest.test_truth')\n    assert (r2.duration == 1250.0)\n    assert (r2.result == Result.failed)\n    assert (r2.message == 'test_simple.py:4: in tearDown\\n    1/0\\nE   ZeroDivisionError: integer division or modulo by zero')\n    assert (r2.reruns == 0)\n", "label": 1}
{"function": "\n\ndef _filtered_candidates(self, plugin, context, r_hd_binding_db):\n    candidates_dict = {c.id: c for c in self.get_candidates(plugin, context, r_hd_binding_db)}\n    if candidates_dict:\n        r_b = r_hd_binding_db.router.redundancy_binding\n        if r_b:\n            if r_b.user_router.hosting_info.hosting_device_id:\n                del candidates_dict[r_b.user_router.hosting_info.hosting_device_id]\n            for rr_b in r_b.user_router.redundancy_bindings:\n                rr = rr_b.redundancy_router\n                if ((rr.id != r_b.redundancy_router_id) and rr.hosting_info.hosting_device_id):\n                    del candidates_dict[rr.hosting_info.hosting_device_id]\n        elif (r_hd_binding_db.role == ROUTER_ROLE_HA_REDUNDANCY):\n            return []\n        for rr_b in r_hd_binding_db.router.redundancy_bindings:\n            rr = rr_b.redundancy_router\n            if rr.hosting_info.hosting_device_id:\n                del candidates_dict[rr.hosting_info.hosting_device_id]\n    return candidates_dict.values()\n", "label": 1}
{"function": "\n\n@classmethod\ndef target_info(cls, obj, ansi=False):\n    if isinstance(obj, type):\n        return ''\n    targets = obj.traverse(cls.get_target)\n    (elements, containers) = zip(*targets)\n    element_set = set((el for el in elements if (el is not None)))\n    container_set = set((c for c in containers if (c is not None)))\n    element_info = None\n    if (len(element_set) == 1):\n        element_info = ('Element: %s' % list(element_set)[0])\n    elif (len(element_set) > 1):\n        element_info = ('Elements:\\n   %s' % '\\n   '.join(sorted(element_set)))\n    container_info = None\n    if (len(container_set) == 1):\n        container_info = ('Container: %s' % list(container_set)[0])\n    elif (len(container_set) > 1):\n        container_info = ('Containers:\\n   %s' % '\\n   '.join(sorted(container_set)))\n    heading = cls.heading('Target Specifications', ansi=ansi, char='-')\n    target_header = '\\nTargets in this object available for customization:\\n'\n    if (element_info and container_info):\n        target_info = ('%s\\n\\n%s' % (element_info, container_info))\n    else:\n        target_info = (element_info if element_info else container_info)\n    target_footer = '\\nTo see the options info for one of these target specifications,\\nwhich are of the form {type}[.{group}[.{label}]], do holoviews.help({type}).'\n    return '\\n'.join([heading, target_header, target_info, target_footer])\n", "label": 1}
{"function": "\n\n@property\ndef type(self):\n    path = self.short_path\n    if any((path.startswith(prefix) for prefix in DOCKER_PREFIXES)):\n        return 'docker'\n    elif path.startswith('/lxc/'):\n        return 'lxc'\n    elif path.startswith('/user.slice/'):\n        (_, parent, name) = path.rsplit('/', 2)\n        if parent.endswith('.scope'):\n            if os.path.isdir(('/home/%s/.local/share/lxc/%s' % (self.owner, name))):\n                return 'lxc-user'\n        return 'systemd'\n    elif ((path == '/user.slice') or (path == '/system.slice') or path.startswith('/system.slice/')):\n        return 'systemd'\n    elif (regexp_ovz_container.match(path) and (path != '/0') and HAS_OPENVZ):\n        return 'openvz'\n    else:\n        return '-'\n", "label": 1}
{"function": "\n\ndef __init__(self, parser, path, source_resource=None):\n    self.parser = parser\n    self.path = path\n    self.source_resource = source_resource\n    self.entities = OrderedDict()\n    self.escape_quotes_on = (('mobile/android/base' in path) and (parser is DTDParser))\n    if source_resource:\n        for (key, entity) in source_resource.entities.items():\n            self.entities[key] = copy_source_entity(entity)\n    try:\n        self.structure = parser.get_structure(read_file(path, uncomment_moz_langpack=((parser is IncParser) and (not source_resource))))\n    except IOError:\n        if source_resource:\n            return\n        else:\n            raise\n    comments = []\n    current_order = 0\n    for obj in self.structure:\n        if isinstance(obj, silme.core.entity.Entity):\n            if self.escape_quotes_on:\n                obj.value = self.unescape_quotes(obj.value)\n            entity = SilmeEntity(obj, comments, current_order)\n            self.entities[entity.key] = entity\n            current_order += 1\n            comments = []\n        elif isinstance(obj, silme.core.structure.Comment):\n            for comment in obj:\n                lines = unicode(comment).strip().split('\\n')\n                comments += [line.strip() for line in lines]\n", "label": 1}
{"function": "\n\ndef determine_result(self, returncode, returnsignal, output, isTimeout):\n    output = '\\n'.join(output)\n    status = result.RESULT_UNKNOWN\n    if self.allInText(['FALSE_DEREF'], output):\n        status = result.RESULT_FALSE_DEREF\n    elif self.allInText(['FALSE_FREE'], output):\n        status = result.RESULT_FALSE_FREE\n    elif self.allInText(['FALSE_MEMTRACK'], output):\n        status = result.RESULT_FALSE_MEMTRACK\n    elif self.allInText(['FALSE_OVERFLOW'], output):\n        status = result.RESULT_FALSE_OVERFLOW\n    elif self.allInText(['FALSE'], output):\n        status = result.RESULT_FALSE_REACH\n    elif ('TRUE' in output):\n        status = result.RESULT_TRUE_PROP\n    if (status == result.RESULT_UNKNOWN):\n        if isTimeout:\n            status = 'TIMEOUT'\n        elif output.endswith(('Z3 Error 9', 'Z3 Error 9\\n')):\n            status = 'ERROR (Z3 Error 9)'\n        elif output.endswith(('error', 'error\\n')):\n            status = 'ERROR'\n        elif ('Encountered Z3 conversion error:' in output):\n            status = 'ERROR (Z3 conversion error)'\n    return status\n", "label": 1}
{"function": "\n\ndef process_handler_result(self, result, task=None):\n    '\\n        Process result received from the task handler.\\n\\n        Result could be:\\n        * None\\n        * Task instance\\n        * Data instance.\\n        '\n    if isinstance(result, Task):\n        self.add_task(result)\n    elif isinstance(result, Data):\n        handler = self.find_data_handler(result)\n        try:\n            data_result = handler(**result.storage)\n            if (data_result is None):\n                pass\n            else:\n                for something in data_result:\n                    self.process_handler_result(something, task)\n        except Exception as ex:\n            self.process_handler_error(('data_%s' % result.handler_key), ex, task)\n    elif (result is None):\n        pass\n    elif isinstance(result, Exception):\n        handler = self.find_task_handler(task)\n        handler_name = getattr(handler, '__name__', 'NONE')\n        self.process_handler_error(handler_name, result, task)\n    elif isinstance(result, dict):\n        if (result.get('type') == 'stat'):\n            for (name, count) in result['counters'].items():\n                self.stat.inc(name, count)\n            for (name, items) in result['collections'].items():\n                for item in items:\n                    self.stat.collect(name, item)\n        else:\n            raise SpiderError(('Unknown result type: %s' % result))\n    else:\n        raise SpiderError(('Unknown result type: %s' % result))\n", "label": 1}
{"function": "\n\ndef _load_00(b, classes):\n    identifier = b[0]\n    if isinstance(identifier, str):\n        identifier = ord(identifier)\n    if (identifier == _SPEC):\n        return _load_spec(b)\n    elif (identifier == _INT_32):\n        return _load_int_32(b)\n    elif ((identifier == _INT) or (identifier == _INT_NEG)):\n        return _load_int(b)\n    elif (identifier == _FLOAT):\n        return _load_float(b)\n    elif (identifier == _COMPLEX):\n        return _load_complex(b)\n    elif (identifier == _STR):\n        return _load_str(b)\n    elif (identifier == _BYTES):\n        return _load_bytes(b)\n    elif (identifier == _TUPLE):\n        return _load_tuple(b, classes)\n    elif (identifier == _NAMEDTUPLE):\n        return _load_namedtuple_00(b, classes)\n    elif (identifier == _LIST):\n        return _load_list(b, classes)\n    elif (identifier == _NPARRAY):\n        return _load_np_array(b)\n    elif (identifier == _DICT):\n        return _load_dict(b, classes)\n    elif (identifier == _GETSTATE):\n        return _load_getstate(b, classes)\n    else:\n        raise BFLoadError(\"unknown identifier '{}'\".format(hex(identifier)))\n", "label": 1}
{"function": "\n\ndef force_text(s, encoding='utf-8', strings_only=False, errors='strict'):\n    \"\\n    Similar to smart_text, except that lazy instances are resolved to\\n    strings, rather than kept as lazy objects.\\n\\n    If strings_only is True, don't convert (some) non-string-like objects.\\n    \"\n    if isinstance(s, text_type):\n        return s\n    if (strings_only and is_protected_type(s)):\n        return s\n    try:\n        if (not isinstance(s, string_types)):\n            if hasattr(s, '__unicode__'):\n                s = s.__unicode__()\n            elif (not PY2):\n                if isinstance(s, bytes):\n                    s = text_type(s, encoding, errors)\n                else:\n                    s = text_type(s)\n            else:\n                s = text_type(bytes(s), encoding, errors)\n        else:\n            s = s.decode(encoding, errors)\n    except UnicodeDecodeError as e:\n        if (not isinstance(s, Exception)):\n            raise UnicodeDecodeError(*e.args)\n        else:\n            s = ' '.join([force_text(arg, encoding, strings_only, errors) for arg in s])\n    return s\n", "label": 1}
{"function": "\n\ndef visit_delete(self, delete_stmt, **kw):\n    self.stack.append({\n        'correlate_froms': set([delete_stmt.table]),\n        'iswrapper': False,\n        'asfrom_froms': set([delete_stmt.table]),\n    })\n    self.isdelete = True\n    text = 'DELETE '\n    if delete_stmt._prefixes:\n        text += self._generate_prefixes(delete_stmt, delete_stmt._prefixes, **kw)\n    text += 'FROM '\n    table_text = delete_stmt.table._compiler_dispatch(self, asfrom=True, iscrud=True)\n    if delete_stmt._hints:\n        dialect_hints = dict([(table, hint_text) for ((table, dialect), hint_text) in delete_stmt._hints.items() if (dialect in ('*', self.dialect.name))])\n        if (delete_stmt.table in dialect_hints):\n            table_text = self.format_from_hint_text(table_text, delete_stmt.table, dialect_hints[delete_stmt.table], True)\n    else:\n        dialect_hints = None\n    text += table_text\n    if delete_stmt._returning:\n        self.returning = delete_stmt._returning\n        if self.returning_precedes_values:\n            text += (' ' + self.returning_clause(delete_stmt, delete_stmt._returning))\n    if (delete_stmt._whereclause is not None):\n        t = delete_stmt._whereclause._compiler_dispatch(self)\n        if t:\n            text += (' WHERE ' + t)\n    if (self.returning and (not self.returning_precedes_values)):\n        text += (' ' + self.returning_clause(delete_stmt, delete_stmt._returning))\n    self.stack.pop((- 1))\n    return text\n", "label": 1}
{"function": "\n\ndef _create_interfaces(self):\n    interfaces = []\n    type_interfaces = None\n    if isinstance(self.type_definition, RelationshipType):\n        if isinstance(self.entity_tpl, dict):\n            if (self.INTERFACES in self.entity_tpl):\n                type_interfaces = self.entity_tpl[self.INTERFACES]\n            else:\n                for (rel_def, value) in self.entity_tpl.items():\n                    if (rel_def != 'type'):\n                        rel_def = self.entity_tpl.get(rel_def)\n                        rel = None\n                        if isinstance(rel_def, dict):\n                            rel = rel_def.get('relationship')\n                        if rel:\n                            if (self.INTERFACES in rel):\n                                type_interfaces = rel[self.INTERFACES]\n                                break\n    else:\n        type_interfaces = self.type_definition.get_value(self.INTERFACES, self.entity_tpl)\n    if type_interfaces:\n        for (interface_type, value) in type_interfaces.items():\n            for (op, op_def) in value.items():\n                iface = InterfacesDef(self.type_definition, interfacetype=interface_type, node_template=self, name=op, value=op_def)\n                interfaces.append(iface)\n    return interfaces\n", "label": 1}
{"function": "\n\ndef dropMimeData(self, mime_data, action, row, column, parent):\n    ' Reimplemented to allow items to be moved.\\n        '\n    if (action == QtCore.Qt.IgnoreAction):\n        return False\n    data = mime_data.data(tabular_mime_type)\n    if ((not data.isNull()) and (action == QtCore.Qt.MoveAction)):\n        id_and_rows = map(int, str(data).split(' '))\n        table_id = id_and_rows[0]\n        if (table_id == id(self)):\n            current_rows = id_and_rows[1:]\n            self.moveRows(current_rows, parent.row())\n            return True\n    data = PyMimeData.coerce(mime_data).instance()\n    if (data is not None):\n        if (not isinstance(data, list)):\n            data = [data]\n        editor = self._editor\n        object = editor.object\n        name = editor.name\n        adapter = editor.adapter\n        if ((row == (- 1)) and parent.isValid()):\n            row = parent.row()\n        if ((row == (- 1)) and (adapter.len(object, name) == 0)):\n            row = 0\n        if all((adapter.get_can_drop(object, name, row, item) for item in data)):\n            for item in reversed(data):\n                self.dropItem(item, row)\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef parse_rule_list_any(self, rules, is_separator, parent, level, begin):\n    if ((self.printer is not None) and (not is_separator)):\n        self.printer(level, (('== Rule list once [' + str(len(rules))) + '] =='))\n    index = 0\n    for rule in rules:\n        index += 1\n        if ((self.printer is not None) and (not is_separator)):\n            self.printer(level, ((((('> Once [' + str(index)) + '/') + str(len(rules))) + '] ') + str(begin)))\n        parse_output = self.parse_rule(rule, is_separator, parent, (level + 1), begin)\n        if parse_output['successive_match']:\n            if ((self.printer is not None) and (not is_separator)):\n                self.printer(level, '> Once Success')\n            return parse_output\n    if ((self.printer is not None) and (not is_separator)):\n        self.printer(level, '> Once Failed')\n    return parse_output\n", "label": 1}
{"function": "\n\ndef show_list(self, connection, app_names=None):\n    '\\n        Shows a list of all migrations on the system, or only those of\\n        some named apps.\\n        '\n    loader = MigrationLoader(connection, ignore_no_migrations=True)\n    graph = loader.graph\n    if app_names:\n        invalid_apps = []\n        for app_name in app_names:\n            if (app_name not in loader.migrated_apps):\n                invalid_apps.append(app_name)\n        if invalid_apps:\n            raise CommandError(('No migrations present for: %s' % ', '.join(invalid_apps)))\n    else:\n        app_names = sorted(loader.migrated_apps)\n    for app_name in app_names:\n        self.stdout.write(app_name, self.style.MIGRATE_LABEL)\n        shown = set()\n        for node in graph.leaf_nodes(app_name):\n            for plan_node in graph.forwards_plan(node):\n                if ((plan_node not in shown) and (plan_node[0] == app_name)):\n                    title = plan_node[1]\n                    if graph.nodes[plan_node].replaces:\n                        title += (' (%s squashed migrations)' % len(graph.nodes[plan_node].replaces))\n                    if (plan_node in loader.applied_migrations):\n                        self.stdout.write((' [X] %s' % title))\n                    else:\n                        self.stdout.write((' [ ] %s' % title))\n                    shown.add(plan_node)\n        if (not shown):\n            self.stdout.write(' (no migrations)', self.style.ERROR)\n", "label": 1}
{"function": "\n\ndef finddirs(pattern, path='.', exclude=None, recursive=True):\n    'Find directories that match *pattern* in *path*'\n    import fnmatch\n    import os\n    if recursive:\n        for (root, dirnames, filenames) in os.walk(path):\n            for pat in _to_list(pattern):\n                for dirname in fnmatch.filter(dirnames, pat):\n                    dirpath = join(abspath(root), dirname)\n                    for excl in _to_list(exclude):\n                        if (excl and fnmatch.fnmatch(dirpath, excl)):\n                            break\n                    else:\n                        (yield dirpath)\n    else:\n        for pat in _to_list(pattern):\n            for dirname in fnmatch.filter(listdirs(path), pat):\n                dirpath = join(abspath(path), dirname)\n                for excl in _to_list(exclude):\n                    if (excl and fnmatch.fnmatch(dirpath, excl)):\n                        break\n                else:\n                    (yield dirpath)\n", "label": 1}
{"function": "\n\ndef compute_scores(self, sequence):\n    num_states = self.get_num_states()\n    length = len(sequence.x)\n    emission_scores = np.zeros([length, num_states])\n    initial_scores = np.zeros(num_states)\n    transition_scores = np.zeros([(length - 1), num_states, num_states])\n    final_scores = np.zeros(num_states)\n    for tag_id in xrange(num_states):\n        initial_features = self.feature_mapper.get_initial_features(sequence, tag_id)\n        score = 0.0\n        for feat_id in initial_features:\n            score += self.parameters[feat_id]\n        initial_scores[tag_id] = score\n    for pos in xrange(length):\n        for tag_id in xrange(num_states):\n            emission_features = self.feature_mapper.get_emission_features(sequence, pos, tag_id)\n            score = 0.0\n            for feat_id in emission_features:\n                score += self.parameters[feat_id]\n            emission_scores[(pos, tag_id)] = score\n        if (pos > 0):\n            for tag_id in xrange(num_states):\n                for prev_tag_id in xrange(num_states):\n                    transition_features = self.feature_mapper.get_transition_features(sequence, pos, tag_id, prev_tag_id)\n                    score = 0.0\n                    for feat_id in transition_features:\n                        score += self.parameters[feat_id]\n                    transition_scores[((pos - 1), tag_id, prev_tag_id)] = score\n    for prev_tag_id in xrange(num_states):\n        final_features = self.feature_mapper.get_final_features(sequence, prev_tag_id)\n        score = 0.0\n        for feat_id in final_features:\n            score += self.parameters[feat_id]\n        final_scores[prev_tag_id] = score\n    return (initial_scores, transition_scores, final_scores, emission_scores)\n", "label": 1}
{"function": "\n\ndef request(self, method, url, data=None, headers=None):\n    if (data and isinstance(data, bytes)):\n        data = data.decode()\n    if (data and (not isinstance(data, basestring))):\n        data = urlencode(data)\n    if (data is not None):\n        data = data.encode('utf8')\n    if (data and (not headers.get('Content-Type', None))):\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    request_headers = self.headers.copy()\n    if headers:\n        for (k, v) in headers.items():\n            if (v is None):\n                del request_headers[k]\n            else:\n                request_headers[k] = v\n    try:\n        func = getattr(requests, method.lower())\n    except AttributeError:\n        raise Exception(\"HTTP method '{}' is not supported\".format(method))\n    response = func(url, data=data, headers=request_headers)\n    if (response.status_code > 399):\n        raise HTTPError(response.status_code, '{}: {}'.format(response.status_code, response.content))\n    return response\n", "label": 1}
{"function": "\n\ndef on_change_input(self, fgraph, node, i, r, new_r, reason):\n    if (new_r not in self.shape_of):\n        self.init_r(new_r)\n    self.update_shape(new_r, r)\n    for (shpnode, idx) in (r.clients + [(node, i)]):\n        if isinstance(getattr(shpnode, 'op', None), Shape_i):\n            idx = shpnode.op.i\n            repl = self.shape_of[new_r][idx]\n            if (repl.owner is shpnode):\n                continue\n            if (repl.owner and (repl.owner.inputs[0] is shpnode.inputs[0]) and isinstance(repl.owner.op, Shape_i) and (repl.owner.op.i == shpnode.op.i)):\n                continue\n            if (shpnode.outputs[0] in theano.gof.graph.ancestors([repl])):\n                raise InconsistencyError(('This substitution would insert a cycle in the graph:node: %s, i: %i, r: %s, new_r: %s' % (node, i, r, new_r)))\n            self.scheduled[shpnode] = new_r\n    unscheduled = [k for (k, v) in self.scheduled.items() if (v == r)]\n    for k in unscheduled:\n        del self.scheduled[k]\n    for v in self.shape_of_reverse_index.get(r, []):\n        for (ii, svi) in enumerate(self.shape_of.get(v, [])):\n            if (svi == r):\n                self.set_shape_i(v, ii, new_r)\n    self.shape_of_reverse_index[r] = set()\n", "label": 1}
{"function": "\n\ndef parse_for(tokens, name, context):\n    (first, pos) = tokens[0]\n    tokens = tokens[1:]\n    context = (('for',) + context)\n    content = []\n    assert first.startswith('for ')\n    if first.endswith(':'):\n        first = first[:(- 1)]\n    first = first[3:].strip()\n    match = in_re.search(first)\n    if (not match):\n        raise TemplateError(('Bad for (no \"in\") in %r' % first), position=pos, name=name)\n    vars = first[:match.start()]\n    if ('(' in vars):\n        raise TemplateError(('You cannot have () in the variable section of a for loop (%r)' % vars), position=pos, name=name)\n    vars = tuple([v.strip() for v in first[:match.start()].split(',') if v.strip()])\n    expr = first[match.end():]\n    while 1:\n        if (not tokens):\n            raise TemplateError('No {{endfor}}', position=pos, name=name)\n        if (isinstance(tokens[0], tuple) and (tokens[0][0] == 'endfor')):\n            return (('for', pos, vars, expr, content), tokens[1:])\n        (next_chunk, tokens) = parse_expr(tokens, name, context)\n        content.append(next_chunk)\n", "label": 1}
{"function": "\n\n@non_atomic_requests\ndef reporter(request):\n    query = request.GET.get('guid')\n    if query:\n        qs = None\n        if query.isdigit():\n            qs = Addon.with_unlisted.filter(id=query)\n        if (not qs):\n            qs = Addon.with_unlisted.filter(slug=query)\n        if (not qs):\n            qs = Addon.with_unlisted.filter(guid=query)\n        if ((not qs) and (len(query) > 4)):\n            qs = CompatReport.objects.filter(guid__startswith=query)\n        if qs:\n            guid = qs[0].guid\n            addon = Addon.with_unlisted.get(guid=guid)\n            if (addon.is_listed or owner_or_unlisted_reviewer(request, addon)):\n                return redirect('compat.reporter_detail', guid)\n    addons = (Addon.with_unlisted.filter(authors=request.user) if request.user.is_authenticated() else [])\n    return render(request, 'compat/reporter.html', dict(query=query, addons=addons))\n", "label": 1}
{"function": "\n\ndef get_index_dtype(arrays=(), maxval=None, check_contents=False):\n    '\\n    Based on input (integer) arrays `a`, determine a suitable index data\\n    type that can hold the data in the arrays.\\n\\n    Parameters\\n    ----------\\n    arrays : tuple of array_like\\n        Input arrays whose types/contents to check\\n    maxval : float, optional\\n        Maximum value needed\\n    check_contents : bool, optional\\n        Whether to check the values in the arrays and not just their types.\\n        Default: False (check only the types)\\n\\n    Returns\\n    -------\\n    dtype : dtype\\n        Suitable index data type (int32 or int64)\\n\\n    '\n    int32max = np.iinfo(np.int32).max\n    dtype = np.intc\n    if (maxval is not None):\n        if (maxval > int32max):\n            dtype = np.int64\n    if isinstance(arrays, np.ndarray):\n        arrays = (arrays,)\n    for arr in arrays:\n        arr = np.asarray(arr)\n        if (arr.dtype > np.int32):\n            if check_contents:\n                if (arr.size == 0):\n                    continue\n                elif np.issubdtype(arr.dtype, np.integer):\n                    maxval = arr.max()\n                    minval = arr.min()\n                    if ((minval >= np.iinfo(np.int32).min) and (maxval <= np.iinfo(np.int32).max)):\n                        continue\n            dtype = np.int64\n            break\n    return dtype\n", "label": 1}
{"function": "\n\ndef whole_test(setup_py, verbose, log):\n    if verbose:\n        show_output = True\n    else:\n        show_output = False\n    if (not test_can_run(setup_py, show_output, log)):\n        pass\n    if verbose:\n        pprint('YELLOW', '----------------- Testing distutils ------------------')\n    use_distutils = test_distutils(setup_py, show_output, log)\n    if verbose:\n        pprint('YELLOW', '----------------- Testing setuptools -----------------')\n    use_setuptools = test_setuptools(setup_py, show_output, log)\n    if verbose:\n        pprint('YELLOW', '------------ Testing numpy.distutils -----------------')\n    use_numpy = test_numpy(setup_py, show_output, log)\n    if verbose:\n        pprint('YELLOW', '--- Testing numpy.distutils patched by setuptools ----')\n    use_setuptools_numpy = test_setuptools_numpy(setup_py, show_output, log)\n    if verbose:\n        print(('Is distutils ? %d' % use_distutils))\n        print(('Is setuptools ? %d' % use_setuptools))\n        print(('Is numpy distutils ? %d' % use_numpy))\n        print(('Is setuptools numpy ? %d' % use_setuptools_numpy))\n    if (use_distutils and (not (use_setuptools or use_numpy or use_setuptools_numpy))):\n        return 'distutils'\n    elif (use_setuptools and (not (use_numpy or use_setuptools_numpy))):\n        return 'setuptools'\n    elif (use_numpy and (not use_setuptools_numpy)):\n        return 'numpy.distutils'\n    elif use_setuptools_numpy:\n        return 'setuptools + numpy.distutils converter'\n    else:\n        return 'Unsupported converter'\n", "label": 1}
{"function": "\n\ndef run(self):\n    'Starts or resumes the generator, running until it reaches a\\n        yield point that is not ready.\\n        '\n    if (self.running or self.finished):\n        return\n    try:\n        self.running = True\n        while True:\n            future = self.future\n            if (not future.done()):\n                return\n            self.future = None\n            try:\n                orig_stack_contexts = stack_context._state.contexts\n                exc_info = None\n                try:\n                    value = future.result()\n                except Exception:\n                    self.had_exception = True\n                    exc_info = sys.exc_info()\n                if (exc_info is not None):\n                    yielded = self.gen.throw(*exc_info)\n                    exc_info = None\n                else:\n                    yielded = self.gen.send(value)\n                if (stack_context._state.contexts is not orig_stack_contexts):\n                    self.gen.throw(stack_context.StackContextInconsistentError('stack_context inconsistency (probably caused by yield within a \"with StackContext\" block)'))\n            except (StopIteration, Return) as e:\n                self.finished = True\n                self.future = _null_future\n                if (self.pending_callbacks and (not self.had_exception)):\n                    raise LeakedCallbackError(('finished without waiting for callbacks %r' % self.pending_callbacks))\n                self.result_future.set_result(_value_from_stopiteration(e))\n                self.result_future = None\n                self._deactivate_stack_context()\n                return\n            except Exception:\n                self.finished = True\n                self.future = _null_future\n                self.result_future.set_exc_info(sys.exc_info())\n                self.result_future = None\n                self._deactivate_stack_context()\n                return\n            if (not self.handle_yield(yielded)):\n                return\n    finally:\n        self.running = False\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    _name_list = [self._mock_new_name]\n    _parent = self._mock_new_parent\n    last = self\n    dot = '.'\n    if (_name_list == ['()']):\n        dot = ''\n    seen = set()\n    while (_parent is not None):\n        last = _parent\n        _name_list.append((_parent._mock_new_name + dot))\n        dot = '.'\n        if (_parent._mock_new_name == '()'):\n            dot = ''\n        _parent = _parent._mock_new_parent\n        if (id(_parent) in seen):\n            break\n        seen.add(id(_parent))\n    _name_list = list(reversed(_name_list))\n    _first = (last._mock_name or 'mock')\n    if (len(_name_list) > 1):\n        if (_name_list[1] not in ('()', '().')):\n            _first += '.'\n    _name_list[0] = _first\n    name = ''.join(_name_list)\n    name_string = ''\n    if (name not in ('mock', 'mock.')):\n        name_string = (' name=%r' % name)\n    spec_string = ''\n    if (self._spec_class is not None):\n        spec_string = ' spec=%r'\n        if self._spec_set:\n            spec_string = ' spec_set=%r'\n        spec_string = (spec_string % self._spec_class.__name__)\n    return (\"<%s%s%s id='%s'>\" % (type(self).__name__, name_string, spec_string, id(self)))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _generate_syntax_file_map():\n    '\\n        Generate a map of all file types to their syntax files.\\n        '\n    syntax_file_map = {\n        \n    }\n    packages_path = sublime.packages_path()\n    packages = [f for f in os.listdir(packages_path) if os.path.isdir(os.path.join(packages_path, f))]\n    for package in packages:\n        package_dir = os.path.join(packages_path, package)\n        syntax_files = [os.path.join(package_dir, f) for f in os.listdir(package_dir) if f.endswith('.tmLanguage')]\n        for syntax_file in syntax_files:\n            try:\n                plist = plistlib.readPlist(syntax_file)\n                if plist:\n                    for file_type in plist['fileTypes']:\n                        syntax_file_map[file_type.lower()] = syntax_file\n            except expat.ExpatError:\n                logger.warn((\"could not parse '%s'\" % syntax_file))\n            except KeyError:\n                pass\n    if hasattr(sublime, 'find_resources'):\n        syntax_files = sublime.find_resources('*.tmLanguage')\n        for syntax_file in syntax_files:\n            try:\n                plist = plistlib.readPlistFromBytes(bytearray(sublime.load_resource(syntax_file), 'utf-8'))\n                if plist:\n                    for file_type in plist['fileTypes']:\n                        syntax_file_map[file_type.lower()] = syntax_file\n            except expat.ExpatError:\n                logger.warn((\"could not parse '%s'\" % syntax_file))\n            except KeyError:\n                pass\n    return syntax_file_map\n", "label": 1}
{"function": "\n\ndef __cmp__(self, other):\n    if (self.__is_evaluated and other.__is_evaluated):\n        return cmp(self.__head, other.__head)\n    elif (len(self.__head) >= len(other.__head)):\n        heads = zip(self.__head[:len(other.__head)], other.__head)\n        heads_comp = (cmp(h1, h2) for (h1, h2) in heads)\n        for comp in heads_comp:\n            if (comp != 0):\n                return comp\n        while (len(self.__head) > len(other.__head)):\n            if other.__is_evaluated:\n                return 1\n            other.__next()\n            comp = cmp(self.__head[(len(other.__head) - 1)], other.__head[(- 1)])\n            if (comp != 0):\n                return comp\n        while ((not self.__is_evaluated) or (not other.__is_evaluated)):\n            if (not self.__is_evaluated):\n                self.__next()\n            if (not other.__is_evaluated):\n                other.__next()\n            len_comp = cmp(len(self.__head), len(other.__head))\n            if (len_comp != 0):\n                return len_comp\n            if (len(self.__head) > 0):\n                value_comp = cmp(self.__head[(- 1)], other.__head[(- 1)])\n                if (value_comp != 0):\n                    return value_comp\n    elif (len(other.__head) > len(self.__head)):\n        return (- other.__cmp__(self))\n    return 0\n", "label": 1}
{"function": "\n\ndef _rpm(b, r, manager, package, version, entry, pathname):\n    '\\n    Resolve dependencies on RPM-based systems.\\n    '\n    p = subprocess.Popen(['rpm', '-qf', pathname], close_fds=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    p.communicate()\n    if (0 == p.returncode):\n        return\n    if pattern_egg.search(entry):\n        p = subprocess.Popen(['rpm', '--qf=%{VERSION}-%{RELEASE}.%{ARCH}', '-q', 'python'], close_fds=True, stdout=subprocess.PIPE)\n        (stdout, stderr) = p.communicate()\n        if (0 != p.returncode):\n            return\n        versions = b.packages['yum']['python']\n        if (stdout not in versions):\n            versions.add(stdout)\n        if (not r.ignore_package('python', package)):\n            b.add_package('python', package, version)\n    elif (pattern_egginfo.search(entry) and os.path.exists(os.path.join(pathname, 'installed-files.txt'))):\n        p = subprocess.Popen(['rpm', '-q', 'python-pip'], close_fds=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        p.communicate()\n        if (0 != p.returncode):\n            if (not r.ignore_package('pip', package)):\n                b.add_package('pip', package, version)\n        elif (not r.ignore_package('python-pip', package)):\n            b.add_package('python-pip', package, version)\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args, **kw_args):\n    args = [sympify(x) for x in args if x]\n    args = TensAdd._tensAdd_flatten(args)\n    if (not args):\n        return S.Zero\n    if ((len(args) == 1) and (not isinstance(args[0], TensExpr))):\n        return args[0]\n    args = TensAdd._tensAdd_check_automatrix(args)\n    TensAdd._tensAdd_check(args)\n    if ((len(args) == 1) and isinstance(args[0], TensMul)):\n        obj = Basic.__new__(cls, *args, **kw_args)\n        return obj\n    args = [canon_bp(x) for x in args if x]\n    args = [x for x in args if x]\n    if (not args):\n        return S.Zero\n    if (len(args) == 1):\n        return args[0]\n\n    def sort_key(t):\n        x = get_tids(t)\n        return (x.components, x.free, x.dum)\n    args.sort(key=sort_key)\n    args = TensAdd._tensAdd_collect_terms(args)\n    if (not args):\n        return S.Zero\n    if (len(args) == 1):\n        return args[0]\n    obj = Basic.__new__(cls, *args, **kw_args)\n    return obj\n", "label": 1}
{"function": "\n\ndef broadcast_like(value, template, fgraph, dtype=None):\n    '\\n    Return a Variable with the same shape and dtype as the template,\\n    filled by broadcasting value through it. `value` will be cast as\\n    necessary.\\n\\n    '\n    value = T.as_tensor_variable(value)\n    if (value.type == template.type):\n        return value\n    if (template not in fgraph.variables):\n        raise NotImplementedError('broadcast_like currently requires the template Variable to be in the fgraph already')\n    if hasattr(fgraph, 'shape_feature'):\n        new_shape = fgraph.shape_feature.shape_of[template]\n    else:\n        new_shape = template.shape\n    if (dtype is None):\n        dtype = template.dtype\n    rval = T.alloc(T.cast(value, dtype), *new_shape)\n    if (rval.broadcastable != template.broadcastable):\n        rval = T.unbroadcast(rval, *[i for i in xrange(rval.ndim) if (rval.broadcastable[i] and (not template.broadcastable[i]))])\n    assert (rval.type.dtype == dtype)\n    if (rval.type.broadcastable != template.broadcastable):\n        raise AssertionError(((('rval.type.broadcastable is ' + str(rval.type.broadcastable)) + ' but template.broadcastable is') + str(template.broadcastable)))\n    return rval\n", "label": 1}
{"function": "\n\ndef _extend_string(self, type_, defaults, spec):\n    'Extend a string-type declaration with standard SQL CHARACTER SET /\\n        COLLATE annotations and MySQL specific extensions.\\n\\n        '\n\n    def attr(name):\n        return getattr(type_, name, defaults.get(name))\n    if attr('charset'):\n        charset = ('CHARACTER SET %s' % attr('charset'))\n    elif attr('ascii'):\n        charset = 'ASCII'\n    elif attr('unicode'):\n        charset = 'UNICODE'\n    else:\n        charset = None\n    if attr('collation'):\n        collation = ('COLLATE %s' % type_.collation)\n    elif attr('binary'):\n        collation = 'BINARY'\n    else:\n        collation = None\n    if attr('national'):\n        return ' '.join([c for c in ('NATIONAL', spec, collation) if (c is not None)])\n    return ' '.join([c for c in (spec, charset, collation) if (c is not None)])\n", "label": 1}
{"function": "\n\ndef commit(self):\n    self.log('in commit')\n    for p in [c for c in self.block_candidates.values() if (c.block.prevhash == self.head.hash)]:\n        assert isinstance(p, BlockProposal)\n        ls = self.heights[p.height].last_quorum_lockset\n        if (ls and (ls.has_quorum == p.blockhash)):\n            self.store_proposal(p)\n            self.store_last_committing_lockset(ls)\n            success = self.chainservice.commit_block(p.block)\n            assert success\n            if success:\n                self.log('commited', p=p, hash=phx(p.blockhash))\n                assert (self.head == p.block)\n                self.commit()\n                return True\n            else:\n                self.log('could not commit', p=p)\n        else:\n            self.log('no quorum for', p=p)\n            if ls:\n                self.log('votes', votes=ls.votes)\n", "label": 1}
{"function": "\n\ndef _process(self):\n    '\\n        Coroutine implementing the key match algorithm. Key strokes are sent\\n        into this generator, and it calls the appropriate handlers.\\n        '\n    buffer = []\n    retry = False\n    while True:\n        if retry:\n            retry = False\n        else:\n            buffer.append((yield))\n        if buffer:\n            is_prefix_of_longer_match = self._is_prefix_of_longer_match(buffer)\n            matches = self._get_matches(buffer)\n            if (matches and matches[(- 1)].eager(self._cli_ref())):\n                is_prefix_of_longer_match = False\n            if ((not is_prefix_of_longer_match) and matches):\n                self._call_handler(matches[(- 1)], key_sequence=buffer)\n                buffer = []\n            elif ((not is_prefix_of_longer_match) and (not matches)):\n                retry = True\n                found = False\n                for i in range(len(buffer), 0, (- 1)):\n                    matches = self._get_matches(buffer[:i])\n                    if matches:\n                        self._call_handler(matches[(- 1)], key_sequence=buffer[:i])\n                        buffer = buffer[i:]\n                        found = True\n                if (not found):\n                    buffer = buffer[1:]\n", "label": 1}
{"function": "\n\ndef init_widget(self):\n    ' Initialize the underlying QWidget object.\\n\\n        '\n    super(QtWidget, self).init_widget()\n    d = self.declaration\n    if d.background:\n        self.set_background(d.background)\n    if d.foreground:\n        self.set_foreground(d.foreground)\n    if d.font:\n        self.set_font(d.font)\n    if (d.show_focus_rect is not None):\n        self.set_show_focus_rect(d.show_focus_rect)\n    if ((- 1) not in d.minimum_size):\n        self.set_minimum_size(d.minimum_size)\n    if ((- 1) not in d.maximum_size):\n        self.set_maximum_size(d.maximum_size)\n    if d.tool_tip:\n        self.set_tool_tip(d.tool_tip)\n    if d.status_tip:\n        self.set_status_tip(d.status_tip)\n    self.set_enabled(d.enabled)\n    if (self.widget.parent() or (not d.visible)):\n        self.set_visible(d.visible)\n", "label": 1}
{"function": "\n\ndef run(self):\n    ' Start polling the socket. '\n    self.heartbeat_controller.reset()\n    self._force_recon = False\n    while (not self.stop.is_set()):\n        try:\n            if (not self._check_connection()):\n                continue\n        except ChromecastConnectionError:\n            break\n        (can_read, _, _) = select.select([self.socket], [], [], POLL_TIME)\n        message = data = None\n        if ((self.socket in can_read) and (not self._force_recon)):\n            try:\n                message = self._read_message()\n            except InterruptLoop as exc:\n                if self.stop.is_set():\n                    self.logger.info('Stopped while reading message, disconnecting.')\n                    break\n                else:\n                    self.logger.exception('Interruption caught without being stopped %s', exc)\n                    break\n            except ssl.SSLError as exc:\n                if (exc.errno == ssl.SSL_ERROR_EOF):\n                    if self.stop.is_set():\n                        break\n                raise\n            except socket.error:\n                self._force_recon = True\n                self.logger.info('Error reading from socket.')\n            else:\n                data = _json_from_message(message)\n        if (not message):\n            continue\n        if self.stop.is_set():\n            break\n        self._route_message(message, data)\n        if (REQUEST_ID in data):\n            callback = self._request_callbacks.pop(data[REQUEST_ID], None)\n            if (callback is not None):\n                event = callback['event']\n                callback['response'] = data\n                event.set()\n    self._cleanup()\n", "label": 1}
{"function": "\n\ndef main(prettypath, verify=False):\n    for pins in range(2, 9):\n        for generator in (top_pth_fp, side_pth_fp, top_smd_fp, side_smd_fp):\n            (name, fp) = generator(pins)\n            path = os.path.join(prettypath, (name + '.kicad_mod'))\n            if verify:\n                print('Verifying', path)\n            if os.path.isfile(path):\n                with open(path) as f:\n                    old = f.read()\n                old = [n for n in sexp_parse(old) if (n[0] != 'tedit')]\n                new = [n for n in sexp_parse(fp) if (n[0] != 'tedit')]\n                if (new == old):\n                    continue\n            if verify:\n                return False\n            else:\n                with open(path, 'w') as f:\n                    f.write(fp)\n    if verify:\n        return True\n", "label": 1}
{"function": "\n\ndef test_read_job3():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'read': ['test.test', 'test1.test'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_read('test') == False)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == False)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == False)\n        assert (key.can_write('test.test') == False)\n        assert (key.can_write('test1') == False)\n        assert (key.can_write('test1.test') == False)\n        assert (key.can_manage('test') == False)\n        assert (key.can_manage('test.test') == False)\n        assert (key.can_manage('test1') == False)\n        assert (key.can_manage('test1.test') == False)\n", "label": 1}
{"function": "\n\ndef subparse(self, end_tokens=None):\n    body = []\n    data_buffer = []\n    add_data = data_buffer.append\n    if (end_tokens is not None):\n        self._end_token_stack.append(end_tokens)\n\n    def flush_data():\n        if data_buffer:\n            lineno = data_buffer[0].lineno\n            body.append(nodes.Output(data_buffer[:], lineno=lineno))\n            del data_buffer[:]\n    try:\n        while self.stream:\n            token = self.stream.current\n            if (token.type == 'data'):\n                if token.value:\n                    add_data(nodes.TemplateData(token.value, lineno=token.lineno))\n                next(self.stream)\n            elif (token.type == 'variable_begin'):\n                next(self.stream)\n                add_data(self.parse_tuple(with_condexpr=True))\n                self.stream.expect('variable_end')\n            elif (token.type == 'block_begin'):\n                flush_data()\n                next(self.stream)\n                if ((end_tokens is not None) and self.stream.current.test_any(*end_tokens)):\n                    return body\n                rv = self.parse_statement()\n                if isinstance(rv, list):\n                    body.extend(rv)\n                else:\n                    body.append(rv)\n                self.stream.expect('block_end')\n            else:\n                raise AssertionError('internal parsing error')\n        flush_data()\n    finally:\n        if (end_tokens is not None):\n            self._end_token_stack.pop()\n    return body\n", "label": 1}
{"function": "\n\ndef handle_noargs(self, **options):\n    if ((not options['watch']) and (not options['initial_scan'])):\n        sys.exit('--no-initial-scan option should be used with --watch.')\n    scanned_dirs = get_scanned_dirs()\n    verbosity = int(options['verbosity'])\n    compilers = utils.get_compilers().values()\n    if ((not options['watch']) or options['initial_scan']):\n        for scanned_dir in scanned_dirs:\n            for (dirname, dirnames, filenames) in os.walk(scanned_dir):\n                for filename in filenames:\n                    path = os.path.join(dirname, filename)[len(scanned_dir):]\n                    if path.startswith('/'):\n                        path = path[1:]\n                    for compiler in compilers:\n                        if compiler.is_supported(path):\n                            try:\n                                compiler.handle_changed_file(path, verbosity=options['verbosity'])\n                            except (exceptions.StaticCompilationError, ValueError) as e:\n                                print(e)\n                            break\n    if options['watch']:\n        from static_precompiler.watch import watch_dirs\n        watch_dirs(scanned_dirs, verbosity)\n", "label": 1}
{"function": "\n\ndef acquire(self, timeout=None):\n    timeout = (((timeout is not None) and timeout) or self.timeout)\n    end_time = time.time()\n    if ((timeout is not None) and (timeout > 0)):\n        end_time += timeout\n    while True:\n        try:\n            os.symlink(self.unique_name, self.lock_file)\n        except OSError:\n            if self.i_am_locking():\n                return\n            else:\n                if ((timeout is not None) and (time.time() > end_time)):\n                    if (timeout > 0):\n                        raise LockTimeout(('Timeout waiting to acquire lock for %s' % self.path))\n                    else:\n                        raise AlreadyLocked(('%s is already locked' % self.path))\n                time.sleep(((timeout / 10) if (timeout is not None) else 0.1))\n        else:\n            return\n", "label": 1}
{"function": "\n\ndef _pick_drop_channels(self, idx):\n    from ..io.base import _BaseRaw\n    from ..epochs import _BaseEpochs\n    from ..evoked import Evoked\n    from ..time_frequency import AverageTFR\n    if isinstance(self, (_BaseRaw, _BaseEpochs)):\n        if (not self.preload):\n            raise RuntimeError('If Raw or Epochs, data must be preloaded to drop or pick channels')\n\n    def inst_has(attr):\n        return (getattr(self, attr, None) is not None)\n    if inst_has('picks'):\n        self.picks = self.picks[idx]\n    if inst_has('_cals'):\n        self._cals = self._cals[idx]\n    pick_info(self.info, idx, copy=False)\n    if inst_has('_projector'):\n        self._projector = self._projector[idx][:, idx]\n    if (isinstance(self, _BaseRaw) and inst_has('_data')):\n        self._data = self._data.take(idx, axis=0)\n    elif (isinstance(self, _BaseEpochs) and inst_has('_data')):\n        self._data = self._data.take(idx, axis=1)\n    elif (isinstance(self, AverageTFR) and inst_has('data')):\n        self.data = self.data.take(idx, axis=0)\n    elif isinstance(self, Evoked):\n        self.data = self.data.take(idx, axis=0)\n", "label": 1}
{"function": "\n\n@property\ndef previous_time(self):\n    now = timezone.now()\n    recurring_end = occurring_end = None\n    try:\n        occurring_rule = self.occurring_rule\n    except OccurringRule.DoesNotExist:\n        pass\n    else:\n        if (occurring_rule and (occurring_rule.dt_end < now)):\n            occurring_end = (occurring_rule.dt_end, occurring_rule)\n    rrules = self.recurring_rules.filter(begin__lt=now)\n    recurring_ends = [(rule.dt_end, rule) for rule in rrules if (rule.dt_end is not None)]\n    recurring_ends.sort(key=itemgetter(0), reverse=True)\n    try:\n        recurring_end = recurring_ends[0]\n    except IndexError:\n        pass\n    ends = [i for i in (recurring_end, occurring_end) if (i is not None)]\n    ends.sort(key=itemgetter(0), reverse=True)\n    try:\n        return ends[0][1]\n    except IndexError:\n        return None\n", "label": 1}
{"function": "\n\ndef get_substream_rstates(self, n_streams, dtype, inc_rstate=True):\n    '\\n        Initialize a matrix in which each row is a MRG stream state,\\n        and they are spaced by 2**72 samples.\\n\\n        '\n    assert isinstance(dtype, str)\n    assert (n_streams < (2 ** 72))\n    assert (n_streams > 0)\n    rval = numpy.zeros((n_streams, 6), dtype='int32')\n    rval[0] = self.rstate\n    if (multMatVect.dot_modulo is None):\n        multMatVect(rval[0], A1p72, M1, A2p72, M2)\n    f = multMatVect.dot_modulo\n    f.input_storage[0].storage[0] = A1p72\n    f.input_storage[2].storage[0] = M1\n    f.input_storage[3].storage[0] = A2p72\n    f.input_storage[5].storage[0] = M2\n    for i in xrange(1, n_streams):\n        v = rval[(i - 1)]\n        f.input_storage[1].storage[0] = v[:3]\n        f.input_storage[4].storage[0] = v[3:]\n        f.fn()\n        rval[i] = f.output_storage[0].storage[0]\n    if inc_rstate:\n        self.inc_rstate()\n    if (self.use_cuda and (dtype == 'float32')):\n        rval = rval.flatten()\n        tmp_float_buf = numpy.frombuffer(rval.data, dtype='float32')\n        assert (tmp_float_buf.shape == rval.shape)\n        assert (tmp_float_buf.view('int32') == rval).all()\n        rval = tmp_float_buf\n    return rval\n", "label": 1}
{"function": "\n\ndef apply(self, tree):\n    '\\n        Applies the configured optimizations to the given node tree. Modifies the tree in-place\\n        to be sure to have a deep copy if you need the original one. It raises an error instance\\n        whenever any optimization could not be applied to the given tree.\\n        '\n    enabled = self.__optimizations\n    if ('wrap' in enabled):\n        try:\n            ClosureWrapper.optimize(tree)\n        except CryptPrivates.Error as err:\n            raise Error(err)\n    if ('declarations' in enabled):\n        try:\n            CombineDeclarations.optimize(tree)\n        except CombineDeclarations.Error as err:\n            raise Error(err)\n    if ('blocks' in enabled):\n        try:\n            BlockReducer.optimize(tree)\n        except BlockReducer.Error as err:\n            raise Error(err)\n    if ('variables' in enabled):\n        try:\n            LocalVariables.optimize(tree)\n        except LocalVariables.Error as err:\n            raise Error(err)\n    if ('privates' in enabled):\n        try:\n            CryptPrivates.optimize(tree, tree.fileId)\n        except CryptPrivates.Error as err:\n            raise Error(err)\n", "label": 1}
{"function": "\n\n@local_optimizer([GpuFromHost, GpuToGpu, host_from_gpu])\ndef local_cut_gpu_transfers(node):\n    if (isinstance(node.op, GpuFromHost) and node.inputs[0].owner and isinstance(node.inputs[0].owner.op, HostFromGpu)):\n        other = node.inputs[0].owner.inputs[0]\n        if (node.op.context_name == other.type.context_name):\n            return [other]\n        else:\n            return [GpuToGpu(node.op.context_name)(other)]\n    elif (isinstance(node.op, HostFromGpu) and node.inputs[0].owner):\n        n2 = node.inputs[0].owner\n        if isinstance(n2.op, GpuFromHost):\n            return [n2.inputs[0]]\n        if isinstance(n2.op, GpuToGpu):\n            return [host_from_gpu(n2.inputs[0])]\n    elif isinstance(node.op, GpuToGpu):\n        if (node.inputs[0].type.context_name == node.op.context_name):\n            return [node.inputs[0]]\n        if node.inputs[0].owner:\n            n2 = node.inputs[0].owner\n            if isinstance(n2.op, GpuFromHost):\n                return [GpuFromHost(node.op.context_name)(n2.inputs[0])]\n            if isinstance(n2.op, GpuToGpu):\n                if (node.op.context_name == n2.inputs[0].type.context_name):\n                    return [n2.inputs[0]]\n                else:\n                    return [node.op(n2.inputs[0])]\n", "label": 1}
{"function": "\n\n@classmethod\ndef _resolve_conflict(cls, existing, proposed):\n    if (existing.rev is None):\n        return proposed\n    if (proposed.rev is None):\n        return existing\n    if (proposed == existing):\n        if proposed.force:\n            return proposed\n        return existing\n    elif (existing.force and proposed.force):\n        raise cls.IvyResolveConflictingDepsError('Cannot force {}#{};{} to both rev {} and {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n    elif existing.force:\n        logger.debug('Ignoring rev {} for {}#{};{} already forced to {}'.format(proposed.rev, proposed.org, proposed.name, (proposed.classifier or ''), existing.rev))\n        return existing\n    elif proposed.force:\n        logger.debug('Forcing {}#{};{} from {} to {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n        return proposed\n    elif (Revision.lenient(proposed.rev) > Revision.lenient(existing.rev)):\n        logger.debug('Upgrading {}#{};{} from rev {}  to {}'.format(proposed.org, proposed.name, (proposed.classifier or ''), existing.rev, proposed.rev))\n        return proposed\n    else:\n        return existing\n", "label": 1}
{"function": "\n\ndef _read_data(f, dtype):\n    'Read a variable with a specified data type'\n    if (dtype == 1):\n        if (_read_int32(f) != 1):\n            raise Exception('Error occurred while reading byte variable')\n        return _read_byte(f)\n    elif (dtype == 2):\n        return _read_int16(f)\n    elif (dtype == 3):\n        return _read_int32(f)\n    elif (dtype == 4):\n        return _read_float32(f)\n    elif (dtype == 5):\n        return _read_float64(f)\n    elif (dtype == 6):\n        real = _read_float32(f)\n        imag = _read_float32(f)\n        return np.complex64((real + (imag * 1j)))\n    elif (dtype == 7):\n        return _read_string_data(f)\n    elif (dtype == 8):\n        raise Exception('Should not be here - please report this')\n    elif (dtype == 9):\n        real = _read_float64(f)\n        imag = _read_float64(f)\n        return np.complex128((real + (imag * 1j)))\n    elif (dtype == 10):\n        return Pointer(_read_int32(f))\n    elif (dtype == 11):\n        return ObjectPointer(_read_int32(f))\n    elif (dtype == 12):\n        return _read_uint16(f)\n    elif (dtype == 13):\n        return _read_uint32(f)\n    elif (dtype == 14):\n        return _read_int64(f)\n    elif (dtype == 15):\n        return _read_uint64(f)\n    else:\n        raise Exception(('Unknown IDL type: %i - please report this' % dtype))\n", "label": 1}
{"function": "\n\ndef getEncodableAttributes(self, obj, codec=None):\n    attrs = pyamf.ClassAlias.getEncodableAttributes(self, obj, codec=codec)\n    gae_objects = (getGAEObjects(codec.context) if codec else None)\n    if (self.reference_properties and gae_objects):\n        for (name, prop) in self.reference_properties.iteritems():\n            klass = prop.reference_class\n            key = prop.get_value_for_datastore(obj)\n            if (not key):\n                continue\n            try:\n                attrs[name] = gae_objects.getClassKey(klass, key)\n            except KeyError:\n                ref_obj = getattr(obj, name)\n                gae_objects.addClassKey(klass, key, ref_obj)\n                attrs[name] = ref_obj\n    for k in attrs.keys()[:]:\n        if k.startswith('_'):\n            del attrs[k]\n    for attr in obj.dynamic_properties():\n        attrs[attr] = getattr(obj, attr)\n    if (not self.no_key_attr):\n        attrs[self.KEY_ATTR] = (str(obj.key()) if obj.is_saved() else None)\n    return attrs\n", "label": 1}
{"function": "\n\ndef tabulate(self, request_stats):\n    'Print review request summary and status in a table.\\n\\n        Args:\\n            request_stats (dict):\\n                A dict that contains statistics about each review request.\\n        '\n    if len(request_stats):\n        has_branches = False\n        has_bookmarks = False\n        table = tt.Texttable(get_terminal_size().columns)\n        header = ['Status', 'Review Request']\n        for request in request_stats:\n            if ('branch' in request):\n                has_branches = True\n            if ('bookmark' in request):\n                has_bookmarks = True\n        if has_branches:\n            header.append('Branch')\n        if has_bookmarks:\n            header.append('Bookmark')\n        table.header(header)\n        for request in request_stats:\n            row = [request['status'], request['summary']]\n            if has_branches:\n                row.append((request.get('branch') or ''))\n            if has_bookmarks:\n                row.append((request.get('bookmark') or ''))\n            table.add_row(row)\n        print(table.draw())\n    else:\n        print('No review requests found.')\n    print()\n", "label": 1}
{"function": "\n\ndef to_field_allowed(self, request, to_field):\n    '\\n        Returns True if the model associated with this admin should be\\n        allowed to be referenced by the specified field.\\n        '\n    opts = self.model._meta\n    try:\n        field = opts.get_field(to_field)\n    except FieldDoesNotExist:\n        return False\n    if field.primary_key:\n        return True\n    for many_to_many in opts.many_to_many:\n        if (many_to_many.m2m_target_field_name() == to_field):\n            return True\n    registered_models = set()\n    for (model, admin) in self.admin_site._registry.items():\n        registered_models.add(model)\n        for inline in admin.inlines:\n            registered_models.add(inline.model)\n    related_objects = (f for f in opts.get_fields(include_hidden=True) if (f.auto_created and (not f.concrete)))\n    for related_object in related_objects:\n        related_model = related_object.related_model\n        remote_field = related_object.field.remote_field\n        if (any((issubclass(model, related_model) for model in registered_models)) and hasattr(remote_field, 'get_related_field') and (remote_field.get_related_field() == field)):\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef __call__(self, environ, start_response):\n    cleaver = Cleaver(environ, self._identity, self._backend, count_humans_only=self.count_humans_only)\n    environ[self.environ_key] = cleaver\n    if self.allow_override:\n        self._handle_variant_overrides(environ)\n    if (self.count_humans_only and (environ.get('REQUEST_METHOD', '') == 'POST') and (self.human_callback_token in environ.get('PATH_INFO', ''))):\n        (fp, length) = SplitMiddleware._copy_body_to_tempfile(environ)\n        environ.setdefault('CONTENT_LENGTH', length)\n        fs = cgi.FieldStorage(fp=fp, environ=environ, keep_blank_values=True)\n        try:\n            try:\n                x = int(fs.getlist('x')[0])\n            except (IndexError, ValueError):\n                x = 0\n            try:\n                y = int(fs.getlist('y')[0])\n            except (IndexError, ValueError):\n                y = 0\n            try:\n                z = int(fs.getlist('z')[0])\n            except (IndexError, ValueError):\n                z = 0\n            if (x and y and z and ((x + y) == z)):\n                self._backend.mark_human(cleaver.identity)\n                for e in self._backend.all_experiments():\n                    variant = self._backend.get_variant(cleaver.identity, e.name)\n                    if variant:\n                        self._backend.mark_participant(e.name, variant)\n                start_response('204 No Content', [('Content-Type', 'text/plain')])\n                return []\n        except (KeyError, ValueError):\n            pass\n        start_response('401 Unauthorized', [('Content-Type', 'text/plain')])\n        return []\n    return self.app(environ, start_response)\n", "label": 1}
{"function": "\n\ndef __init__(self, host='localhost', port=9200, http_auth=None, use_ssl=False, verify_certs=False, ca_certs=None, client_cert=None, client_key=None, **kwargs):\n    if (not REQUESTS_AVAILABLE):\n        raise ImproperlyConfigured('Please install requests to use RequestsHttpConnection.')\n    super(RequestsHttpConnection, self).__init__(host=host, port=port, **kwargs)\n    self.session = requests.session()\n    if (http_auth is not None):\n        if isinstance(http_auth, (tuple, list)):\n            http_auth = tuple(http_auth)\n        elif isinstance(http_auth, string_types):\n            http_auth = tuple(http_auth.split(':', 1))\n        self.session.auth = http_auth\n    self.base_url = ('http%s://%s:%d%s' % (('s' if use_ssl else ''), host, port, self.url_prefix))\n    self.session.verify = verify_certs\n    if (not client_key):\n        self.session.cert = client_cert\n    elif client_cert:\n        self.session.cert = (client_cert, client_key)\n    if ca_certs:\n        if (not verify_certs):\n            raise ImproperlyConfigured('You cannot pass CA certificates when verify SSL is off.')\n        self.session.verify = ca_certs\n    if (use_ssl and (not verify_certs)):\n        warnings.warn(('Connecting to %s using SSL with verify_certs=False is insecure.' % self.base_url))\n", "label": 1}
{"function": "\n\ndef confirmation(self, client, pdu):\n    if _debug:\n        UDPMultiplexer._debug('confirmation %r %r', client, pdu)\n    if (pdu.pduSource == self.addrTuple):\n        if _debug:\n            UDPMultiplexer._debug('    - from us!')\n        return\n    src = Address(pdu.pduSource)\n    if (client is self.direct):\n        dest = self.address\n    elif (client is self.broadcast):\n        dest = LocalBroadcast()\n    else:\n        raise RuntimeError('confirmation mismatch')\n    if (not pdu.pduData):\n        if _debug:\n            UDPMultiplexer._debug('    - no data')\n        return\n    msg_type = struct.unpack('B', pdu.pduData[:1])[0]\n    if _debug:\n        UDPMultiplexer._debug('    - msg_type: %r', msg_type)\n    if (msg_type == 1):\n        if self.annexH.serverPeer:\n            self.annexH.response(PDU(pdu, source=src, destination=dest))\n    elif (msg_type == 129):\n        if self.annexJ.serverPeer:\n            self.annexJ.response(PDU(pdu, source=src, destination=dest))\n    else:\n        UDPMultiplexer._warning('unsupported message')\n", "label": 1}
{"function": "\n\ndef test_m2o_lazy_loader_on_persistent(self):\n    'Compare the behaviors from the lazyloader using\\n        the \"committed\" state in all cases, vs. the lazyloader\\n        using the \"current\" state in all cases except during flush.\\n\\n        '\n    for loadfk in (True, False):\n        for loadrel in (True, False):\n            for autoflush in (True, False):\n                for manualflush in (True, False):\n                    for fake_autoexpire in (True, False):\n                        sess.autoflush = autoflush\n                        if loadfk:\n                            c1.parent_id\n                        if loadrel:\n                            c1.parent\n                        c1.parent_id = p2.id\n                        if manualflush:\n                            sess.flush()\n                        if fake_autoexpire:\n                            sess.expire(c1, ['parent'])\n                        if (loadrel and (not fake_autoexpire)):\n                            assert (c1.parent is p1)\n                        else:\n                            assert (c1.parent is p2)\n                        sess.rollback()\n", "label": 1}
{"function": "\n\ndef _get_svn_url_rev(self, location):\n    from pip.exceptions import InstallationError\n    with open(os.path.join(location, self.dirname, 'entries')) as f:\n        data = f.read()\n    if (data.startswith('8') or data.startswith('9') or data.startswith('10')):\n        data = list(map(str.splitlines, data.split('\\n\\x0c\\n')))\n        del data[0][0]\n        url = data[0][3]\n        revs = ([int(d[9]) for d in data if ((len(d) > 9) and d[9])] + [0])\n    elif data.startswith('<?xml'):\n        match = _svn_xml_url_re.search(data)\n        if (not match):\n            raise ValueError(('Badly formatted data: %r' % data))\n        url = match.group(1)\n        revs = ([int(m.group(1)) for m in _svn_rev_re.finditer(data)] + [0])\n    else:\n        try:\n            xml = self.run_command(['info', '--xml', location], show_stdout=False)\n            url = _svn_info_xml_url_re.search(xml).group(1)\n            revs = [int(m.group(1)) for m in _svn_info_xml_rev_re.finditer(xml)]\n        except InstallationError:\n            (url, revs) = (None, [])\n    if revs:\n        rev = max(revs)\n    else:\n        rev = 0\n    return (url, rev)\n", "label": 1}
{"function": "\n\ndef test_proxy_snake_dict():\n    my_data = {\n        'one': 1,\n        'two': 2,\n        'none': None,\n        'threeOrFor': 3,\n        'inside': {\n            'otherCamelCase': 3,\n        },\n    }\n    p = ProxySnakeDict(my_data)\n    assert ('one' in p)\n    assert ('two' in p)\n    assert ('threeOrFor' in p)\n    assert ('none' in p)\n    assert (len(p) == len(my_data))\n    assert (p['none'] is None)\n    assert (p.get('none') is None)\n    assert (p.get('none_existent') is None)\n    assert ('three_or_for' in p)\n    assert (p.get('three_or_for') == 3)\n    assert ('inside' in p)\n    assert ('other_camel_case' in p['inside'])\n    assert (sorted(p.items()) == sorted(list([('inside', ProxySnakeDict({\n        'other_camel_case': 3,\n    })), ('none', None), ('three_or_for', 3), ('two', 2), ('one', 1)])))\n", "label": 1}
{"function": "\n\n@gof.local_optimizer([T.pow])\ndef local_pow_specialize(node):\n    if (node.op == T.pow):\n        odtype = node.outputs[0].dtype\n        xsym = node.inputs[0]\n        ysym = node.inputs[1]\n        y = local_mul_canonizer.get_constant(ysym)\n        if ((y is not None) and encompasses_broadcastable(xsym.type.broadcastable, ysym.type.broadcastable)):\n            rval = None\n            if N.all((y == 2)):\n                rval = [T.sqr(xsym)]\n            if N.all((y == 1)):\n                rval = [xsym]\n            if N.all((y == 0)):\n                rval = [T.fill(xsym, numpy.asarray(1, dtype=odtype))]\n            if N.all((y == 0.5)):\n                rval = [T.sqrt(xsym)]\n            if N.all((y == (- 0.5))):\n                rval = [T.inv(T.sqrt(xsym))]\n            if N.all((y == (- 1))):\n                rval = [T.inv(xsym)]\n            if N.all((y == (- 2))):\n                rval = [T.inv(T.sqr(xsym))]\n            if rval:\n                rval[0] = T.cast(rval[0], odtype)\n                assert (rval[0].type == node.outputs[0].type), (rval, node.outputs)\n                return rval\n    else:\n        return False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.username = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.Pass = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.Remember = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef mpf_atan2(y, x, prec, rnd=round_fast):\n    (xsign, xman, xexp, xbc) = x\n    (ysign, yman, yexp, ybc) = y\n    if (not yman):\n        if ((y == fzero) and (x != fnan)):\n            if (mpf_sign(x) >= 0):\n                return fzero\n            return mpf_pi(prec, rnd)\n        if (y in (finf, fninf)):\n            if (x in (finf, fninf)):\n                return fnan\n            if (y == finf):\n                return mpf_shift(mpf_pi(prec, rnd), (- 1))\n            return mpf_neg(mpf_shift(mpf_pi(prec, negative_rnd[rnd]), (- 1)))\n        return fnan\n    if ysign:\n        return mpf_neg(mpf_atan2(mpf_neg(y), x, prec, negative_rnd[rnd]))\n    if (not xman):\n        if (x == fnan):\n            return fnan\n        if (x == finf):\n            return fzero\n        if (x == fninf):\n            return mpf_pi(prec, rnd)\n        if (y == fzero):\n            return fzero\n        return mpf_shift(mpf_pi(prec, rnd), (- 1))\n    tquo = mpf_atan(mpf_div(y, x, (prec + 4)), (prec + 4))\n    if xsign:\n        return mpf_add(mpf_pi((prec + 4)), tquo, prec, rnd)\n    else:\n        return mpf_pos(tquo, prec, rnd)\n", "label": 1}
{"function": "\n\n@attr(speed='slow')\ndef test_3d_time():\n    ' reading/writing of 3D RNMRTK time domain file '\n    (dic, data) = ng.rnmrtk.read(os.path.join(DATA_DIR, 'rnmrtk_3d', 'time_3d.sec'))\n    assert (data.shape == (128, 88, 1250))\n    assert (np.abs((data[(0, 1, 2)].real - 7.98)) <= 0.01)\n    assert (np.abs((data[(0, 1, 2)].imag - 33.82)) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].real - (- 9.36))) <= 0.01)\n    assert (np.abs((data[(10, 11, 18)].imag - (- 7.75))) <= 0.01)\n    assert (dic['sw'][2] == 50000.0)\n    assert (dic['sf'][2] == 125.68)\n    assert (dic['ppm'][2] == 56.0)\n    assert (dic['sw'][1] == 2777.778)\n    assert (dic['sf'][1] == 50.65)\n    assert (dic['ppm'][1] == 120.0)\n    assert (dic['sw'][0] == 5555.556)\n    assert (dic['sf'][0] == 125.68)\n    assert (dic['ppm'][0] == 56.0)\n    write_readback(dic, data)\n", "label": 1}
{"function": "\n\ndef process_response(self, request, response):\n    if (not self.is_cms_request(request)):\n        return response\n    from django.utils.cache import add_never_cache_headers\n    if ((hasattr(request, 'toolbar') and request.toolbar.edit_mode) or (not all((ph.cache_placeholder for (ph, __) in getattr(request, 'placeholders', {\n        \n    }).values())))):\n        add_never_cache_headers(response)\n    if (hasattr(request, 'user') and request.user.is_staff and (response.status_code != 500)):\n        try:\n            pk = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE)).only('pk').order_by('-pk')[0].pk\n            if (hasattr(request, 'cms_latest_entry') and (request.cms_latest_entry != pk)):\n                log = LogEntry.objects.filter(user=request.user, action_flag__in=(ADDITION, CHANGE))[0]\n                request.session['cms_log_latest'] = log.pk\n        except IndexError:\n            pass\n    return response\n", "label": 1}
{"function": "\n\ndef _setupPositionProbePattern(self, row, col):\n    for r in range((- 1), 8):\n        for c in range((- 1), 8):\n            if (((row + r) <= (- 1)) or (self.moduleCount <= (row + r)) or ((col + c) <= (- 1)) or (self.moduleCount <= (col + c))):\n                continue\n            self.modules[(row + r)][(col + c)] = (((0 <= r) and (r <= 6) and ((c == 0) or (c == 6))) or ((0 <= c) and (c <= 6) and ((r == 0) or (r == 6))) or ((2 <= r) and (r <= 4) and (2 <= c) and (c <= 4)))\n", "label": 1}
{"function": "\n\ndef __restore__(self):\n    super(Turtle, self).__restore__()\n    try:\n        [panda.setdefault(self.CREATOR, self.creator) for panda in self.pandas]\n        [panda.setdefault(self.NAME, self.name) for panda in self.pandas]\n        self.tigress.setdefault(self.CREATOR, self.creator)\n        self.tigress.setdefault(self.NAME, self.name)\n    except:\n        pass\n    self.pandas = crane.pandaStore.load_or_create_all(self.pandas)\n    self.tigress = crane.tigressStore.load_or_create(self.tigress)\n    self.pandaUids = set((p.uid for p in self.pandas))\n    self.invertedMapping = {tuple(v): k for (k, v) in self.mapping.iteritems()}\n    self.followers = set(self.followers)\n    if (self.FREQUIRES_UIDS in self.requires):\n        uids = self.requires[self.FREQUIRES_UIDS]\n        if isinstance(uids, basestring):\n            uids = eval(uids)\n        [panda.add_features(uids) for panda in self.pandas]\n    elif (self.FREQUIRES_TURTLES in self.requires):\n        turtles = self.store.load_or_create_all([{\n            'name': t,\n            'creator': self.creator,\n        } for t in self.requires[self.FREQUIRES_TURTLES]])\n        [panda.add_features(turtle.get_panda_uids()) for turtle in turtles for panda in self.pandas]\n    elif self.requires:\n        logger.error('dependent features are either in {0} or {1}, but not in {2}'.format(self.FREQUIRES_UIDS, self.FREQUIRES_TURTLES, self.requires))\n", "label": 1}
{"function": "\n\ndef run(self):\n    res = None\n    os.system('clear')\n    while True:\n        try:\n            if (not self.server_version):\n                self.server_version = self.client.perform_request('version')\n                if (self.server_version.capabilities and ('async' in self.server_version.capabilities)):\n                    self.block = False\n                elif self.supports_blocking:\n                    self.block = True\n                else:\n                    raise BlockingNotSupportedError('Debugger requires blocking mode')\n            self.render()\n            if (not self.block):\n                done = False\n                while (not done):\n                    res = self.client.perform_request('version', block=True)\n                    if res.is_success:\n                        done = True\n        except ConnectionError as e:\n            try:\n                msg = e.message.args[1].strerror\n            except:\n                try:\n                    msg = e.message.args[0]\n                except:\n                    msg = str(e)\n            traceback.print_exc()\n            self.do_render(error='Error: {}'.format(msg))\n            self.server_version = None\n            time.sleep(1)\n", "label": 1}
{"function": "\n\ndef get_managed_entity(content, vimtype, moid=None, name=None):\n    if ((not name) and (not moid)):\n        return\n    container = content.viewManager.CreateContainerView(content.rootFolder, [vimtype], True)\n    count = 0\n    for entity in container.view:\n        if (moid and (entity._moId == moid)):\n            results = entity\n            count += 1\n        elif (name and (entity.name == name)):\n            results = entity\n            count += 1\n        if (count >= 2):\n            raise Exception('Multiple Managed Objects found,                            Check Names or IDs provided are unique')\n        elif (count == 1):\n            return results\n    if name:\n        raise Exception(('Inventory Error: Unable to Find Object (%s): %s' % (vimtype, name)))\n    elif moid:\n        raise Exception(('Inventory Error: Unable to Find Object (%s): %s' % (vimtype, moid)))\n    else:\n        raise Exception(('Inventory Error: No Name or moid provided (%s)' % vimtype))\n", "label": 1}
{"function": "\n\ndef create_decl(self, sigtypes, name, width=None, length=None, lineno=0):\n    self.typecheck_decl(sigtypes, length)\n    decls = []\n    signed = False\n    if ('signed' in sigtypes):\n        signed = True\n    if ('input' in sigtypes):\n        decls.append(Input(name=name, width=width, signed=signed, lineno=lineno))\n    if ('output' in sigtypes):\n        decls.append(Output(name=name, width=width, signed=signed, lineno=lineno))\n    if ('inout' in sigtypes):\n        decls.append(Inout(name=name, width=width, signed=signed, lineno=lineno))\n    if ('wire' in sigtypes):\n        if length:\n            decls.append(WireArray(name=name, width=width, signed=signed, length=length, lineno=lineno))\n        else:\n            decls.append(Wire(name=name, width=width, signed=signed, lineno=lineno))\n    if ('reg' in sigtypes):\n        if length:\n            decls.append(RegArray(name=name, width=width, signed=signed, length=length, lineno=lineno))\n        else:\n            decls.append(Reg(name=name, width=width, signed=signed, lineno=lineno))\n    if ('tri' in sigtypes):\n        decls.append(Tri(name=name, width=width, signed=signed, lineno=lineno))\n    if ('supply0' in sigtypes):\n        decls.append(Supply(name=name, value=IntConst('0', lineno=lineno), width=width, signed=signed, lineno=lineno))\n    if ('supply1' in sigtypes):\n        decls.append(Supply(name=name, value=IntConst('1', lineno=lineno), width=width, signed=signed, lineno=lineno))\n    return decls\n", "label": 1}
{"function": "\n\ndef test_has_multiple():\n    f = (((x ** 2) * y) + sin(((2 ** t) + log(z))))\n    assert f.has(x)\n    assert f.has(y)\n    assert f.has(z)\n    assert f.has(t)\n    assert (not f.has(u))\n    assert f.has(x, y, z, t)\n    assert f.has(x, y, z, t, u)\n    i = Integer(4400)\n    assert (not i.has(x))\n    assert (i * (x ** i)).has(x)\n    assert (not (i * (y ** i)).has(x))\n    assert (i * (y ** i)).has(x, y)\n    assert (not (i * (y ** i)).has(x, z))\n", "label": 1}
{"function": "\n\ndef files(self, itemId, sources=None):\n    ret = {\n        'added': [],\n        'removed': [],\n    }\n    files = self.get('item/{}/files'.format(itemId))\n    if (self.module.params['state'] == 'present'):\n        file_dict = {f['name']: f for f in files}\n        source_dict = {os.path.basename(s): {\n            'path': s,\n            'name': os.path.basename(s),\n            'size': os.path.getsize(s),\n        } for s in sources}\n        source_names = set([(s['name'], s['size']) for s in source_dict.values()])\n        file_names = set([(f['name'], f['size']) for f in file_dict.values()])\n        for (n, _) in (file_names - source_names):\n            self.delete('file/{}'.format(file_dict[n]['_id']))\n            ret['removed'].append(file_dict[n])\n        for (n, _) in (source_names - file_names):\n            self.uploadFileToItem(itemId, source_dict[n]['path'])\n            ret['added'].append(source_dict[n])\n    elif (self.module.params['state'] == 'absent'):\n        for f in files:\n            self.delete('file/{}'.format(f['_id']))\n            ret['removed'].append(f)\n    if ((len(ret['added']) != 0) or (len(ret['removed']) != 0)):\n        self.changed = True\n    return ret\n", "label": 1}
{"function": "\n\ndef run(self, cmd, code):\n    'Run the module checker or executable on code and return the output.'\n    if (self.module is not None):\n        use_module = False\n        if (not self.check_version):\n            use_module = True\n        else:\n            settings = self.get_view_settings()\n            version = settings.get('@python')\n            if (version is None):\n                use_module = ((cmd is None) or (cmd[0] == '<builtin>'))\n            else:\n                version = util.find_python(version=version, module=self.module)\n                use_module = (version[0] == '<builtin>')\n        if use_module:\n            if persist.debug_mode():\n                persist.printf('{}: {} <builtin>'.format(self.name, os.path.basename((self.filename or '<unsaved>'))))\n            try:\n                errors = self.check(code, os.path.basename((self.filename or '<unsaved>')))\n            except Exception as err:\n                persist.printf('ERROR: exception in {}.check: {}'.format(self.name, str(err)))\n                errors = ''\n            if isinstance(errors, (tuple, list)):\n                return '\\n'.join([str(e) for e in errors])\n            else:\n                return errors\n        else:\n            cmd = self._cmd\n    else:\n        cmd = (self.cmd or self._cmd)\n    cmd = self.build_cmd(cmd=cmd)\n    if cmd:\n        return super().run(cmd, code)\n    else:\n        return ''\n", "label": 1}
{"function": "\n\ndef test_file():\n    nmeafile = pynmea2.NMEAFile(StringIO(TEST_DATA))\n    nmea_strings = nmeafile.read()\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    del nmeafile\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [_f.readline() for i in range(10)]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [s for s in _f]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n    with pynmea2.NMEAFile(StringIO(TEST_DATA)) as _f:\n        nmea_strings = [_f.next() for i in range(10)]\n    assert (len(nmea_strings) == 10)\n    assert all([isinstance(s, pynmea2.NMEASentence) for s in nmea_strings])\n", "label": 1}
{"function": "\n\ndef has_changes(self):\n    if self.is_dirty:\n        return True\n    for child in self._db_connections:\n        if child.has_changes():\n            return True\n    for child in self._db_annotations:\n        if child.has_changes():\n            return True\n    for child in self._db_abstractions:\n        if child.has_changes():\n            return True\n    for child in self._db_others:\n        if child.has_changes():\n            return True\n    for child in self._db_modules:\n        if child.has_changes():\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef test_unitroots():\n    assert (unitroots(1) == [1])\n    assert (unitroots(2) == [1, (- 1)])\n    (a, b, c) = unitroots(3)\n    assert (a == 1)\n    assert b.ae(((- 0.5) + 0.8660254037844386j))\n    assert c.ae(((- 0.5) - 0.8660254037844386j))\n    assert (unitroots(1, primitive=True) == [1])\n    assert (unitroots(2, primitive=True) == [(- 1)])\n    assert (unitroots(3, primitive=True) == unitroots(3)[1:])\n    assert (unitroots(4, primitive=True) == [j, (- j)])\n    assert (len(unitroots(17, primitive=True)) == 16)\n    assert (len(unitroots(16, primitive=True)) == 8)\n", "label": 1}
{"function": "\n\ndef __new__(cls, clsname, bases, methods):\n    attrs = []\n    nested = []\n    elements = []\n    namespaced = []\n    for (k, v) in methods.items():\n        if isinstance(v, Descriptor):\n            ns = getattr(v, 'namespace', None)\n            if ns:\n                namespaced.append((k, ('{%s}%s' % (ns, k))))\n            if getattr(v, 'nested', False):\n                nested.append(k)\n                elements.append(k)\n            elif isinstance(v, Sequence):\n                elements.append(k)\n            elif isinstance(v, Typed):\n                if hasattr(v.expected_type, 'to_tree'):\n                    elements.append(k)\n                else:\n                    attrs.append(k)\n            elif (not isinstance(v, Alias)):\n                attrs.append(k)\n    if (methods.get('__attrs__') is None):\n        methods['__attrs__'] = tuple(attrs)\n    methods['__namespaced__'] = tuple(namespaced)\n    if (methods.get('__nested__') is None):\n        methods['__nested__'] = tuple(sorted(nested))\n    if (methods.get('__elements__') is None):\n        methods['__elements__'] = tuple(sorted(elements))\n    return MetaStrict.__new__(cls, clsname, bases, methods)\n", "label": 1}
{"function": "\n\ndef visit_bindparam(self, bindparam, within_columns_clause=False, literal_binds=False, skip_bind_expression=False, **kwargs):\n    if ((not skip_bind_expression) and bindparam.type._has_bind_expression):\n        bind_expression = bindparam.type.bind_expression(bindparam)\n        return self.process(bind_expression, skip_bind_expression=True)\n    if (literal_binds or (within_columns_clause and self.ansi_bind_rules)):\n        if ((bindparam.value is None) and (bindparam.callable is None)):\n            raise exc.CompileError((\"Bind parameter '%s' without a renderable value not allowed here.\" % bindparam.key))\n        return self.render_literal_bindparam(bindparam, within_columns_clause=True, **kwargs)\n    name = self._truncate_bindparam(bindparam)\n    if (name in self.binds):\n        existing = self.binds[name]\n        if (existing is not bindparam):\n            if ((existing.unique or bindparam.unique) and (not existing.proxy_set.intersection(bindparam.proxy_set))):\n                raise exc.CompileError((\"Bind parameter '%s' conflicts with unique bind parameter of the same name\" % bindparam.key))\n            elif (existing._is_crud or bindparam._is_crud):\n                raise exc.CompileError((\"bindparam() name '%s' is reserved for automatic usage in the VALUES or SET clause of this insert/update statement.   Please use a name other than column name when using bindparam() with insert() or update() (for example, 'b_%s').\" % (bindparam.key, bindparam.key)))\n    self.binds[bindparam.key] = self.binds[name] = bindparam\n    return self.bindparam_string(name, **kwargs)\n", "label": 1}
{"function": "\n\ndef __parse_comment(self):\n    'Scan through a // or /* comment.'\n    comment_start_pos = self.__scanner.position\n    self.__scanner.read_ubyte()\n    if self.__scanner.at_end:\n        self.__error(\"Unexpected character '/'\")\n    c = self.__scanner.read_ubyte()\n    if (c == '/'):\n        while (not self.__scanner.at_end):\n            c = self.__scanner.read_ubyte()\n            if ((c == '\\n') or (c == '\\r')):\n                break\n        if ((c == '\\r') and (self.__scanner.peek_next_ubyte(none_if_bad_index=True) == '\\n')):\n            self.__scanner.read_ubyte()\n    elif (c == '*'):\n        while (not self.__scanner.at_end):\n            c = self.__scanner.read_ubyte()\n            if ((c == '*') and (self.__scanner.peek_next_ubyte(none_if_bad_index=True) == '/')):\n                self.__scanner.read_ubyte()\n                return\n        self.__error('Unterminated comment', comment_start_pos)\n    else:\n        self.__error((\"Unexpected character '/%s'\" % c))\n", "label": 1}
{"function": "\n\ndef handle_elt(self, elt, context):\n    titles = elt.findall('bibl/title')\n    title = []\n    if titles:\n        title = '\\n'.join((title.text.strip() for title in titles))\n    authors = elt.findall('bibl/author')\n    author = []\n    if authors:\n        author = '\\n'.join((author.text.strip() for author in authors))\n    dates = elt.findall('bibl/date')\n    date = []\n    if dates:\n        date = '\\n'.join((date.text.strip() for date in dates))\n    publishers = elt.findall('bibl/publisher')\n    publisher = []\n    if publishers:\n        publisher = '\\n'.join((publisher.text.strip() for publisher in publishers))\n    idnos = elt.findall('bibl/idno')\n    idno = []\n    if idnos:\n        idno = '\\n'.join((idno.text.strip() for idno in idnos))\n    notes = elt.findall('bibl/note')\n    note = []\n    if notes:\n        note = '\\n'.join((note.text.strip() for note in notes))\n    return {\n        'title': title,\n        'author': author,\n        'date': date,\n        'publisher': publisher,\n        'idno': idno,\n        'note': note,\n    }\n", "label": 1}
{"function": "\n\ndef pytest_cmdline_preparse(config, args):\n    if (('PYTEST_VERBOSE' in os.environ) and ('-v' not in args)):\n        args.insert(0, '-v')\n    if (('PYTEST_EXITFIRST' in os.environ) and ('-x' not in args)):\n        args.insert(0, '-x')\n    if (('PYTEST_NOCAPTURE' in os.environ) and ('-s' not in args)):\n        args.insert(0, '-s')\n    if (('PYTEST_TB' in os.environ) and (not any((('--tb' in a) for a in args)))):\n        args.insert(0, ('--tb=' + os.environ['PYTEST_TB']))\n    else:\n        args.insert(0, '--tb=short')\n    if (('PYTEST_NPROCS' in os.environ) and ('-n' not in args)):\n        args.insert(0, ('-n ' + os.environ['PYTEST_NPROCS']))\n    if (('PYTEST_WATCH' in os.environ) and ('-f' not in args)):\n        args.insert(0, '-f')\n    if ('PYTEST_LAZY' in os.environ):\n        args.insert(0, '--lazy')\n    if ('PYTEST_GREEDY' in os.environ):\n        args.insert(0, '--greedy')\n", "label": 1}
{"function": "\n\ndef fix_repeating_arguments(self):\n    'Fix elements that should accumulate/increment values.'\n    either = [list(c.children) for c in self.either.children]\n    for case in either:\n        for e in [c for c in case if (case.count(c) > 1)]:\n            if ((type(e) is Argument) or ((type(e) is Option) and e.argcount)):\n                if (e.value is None):\n                    e.value = []\n                elif (type(e.value) is not list):\n                    e.value = e.value.split()\n            if ((type(e) is Command) or ((type(e) is Option) and (e.argcount == 0))):\n                e.value = 0\n    return self\n", "label": 1}
{"function": "\n\ndef has_header(self, sample):\n    rdr = reader(StringIO(sample), self.sniff(sample))\n    header = next(rdr)\n    columns = len(header)\n    columnTypes = {\n        \n    }\n    for i in range(columns):\n        columnTypes[i] = None\n    checked = 0\n    for row in rdr:\n        if (checked > 20):\n            break\n        checked += 1\n        if (len(row) != columns):\n            continue\n        for col in list(columnTypes.keys()):\n            for thisType in [int, float, complex]:\n                try:\n                    thisType(row[col])\n                    break\n                except (ValueError, OverflowError):\n                    pass\n            else:\n                thisType = len(row[col])\n            if (thisType != columnTypes[col]):\n                if (columnTypes[col] is None):\n                    columnTypes[col] = thisType\n                else:\n                    del columnTypes[col]\n    hasHeader = 0\n    for (col, colType) in columnTypes.items():\n        if (type(colType) == type(0)):\n            if (len(header[col]) != colType):\n                hasHeader += 1\n            else:\n                hasHeader -= 1\n        else:\n            try:\n                colType(header[col])\n            except (ValueError, TypeError):\n                hasHeader += 1\n            else:\n                hasHeader -= 1\n    return (hasHeader > 0)\n", "label": 1}
{"function": "\n\ndef unpack(ext, source, dest_path):\n    '\\n    Unpack the archive |source| to |dest_path|.\\n    Note: |source| can be a file handle or a path.\\n    |ext| contains the extension of the archive.\\n    '\n    if (ext != '.zip'):\n        close_source = False\n        try:\n            if isinstance(source, basestring):\n                source = open(source, 'rb')\n                close_source = True\n            if ((ext == '.tar.gz') or (ext == '.tgz')):\n                un_tar_directory(source, dest_path, 'gz')\n            elif (ext == '.tar.bz2'):\n                un_tar_directory(source, dest_path, 'bz2')\n            elif (ext == '.gz'):\n                with open(dest_path, 'wb') as f:\n                    shutil.copyfileobj(un_gzip_stream(source), f)\n            else:\n                raise UsageError('Not an archive.')\n        except (tarfile.TarError, IOError):\n            raise UsageError('Invalid archive upload.')\n        finally:\n            if close_source:\n                source.close()\n    else:\n        delete_source = False\n        try:\n            if (not isinstance(source, basestring)):\n                temp_path = (dest_path + '.zip')\n                with open(temp_path, 'wb') as f:\n                    shutil.copyfileobj(source, f)\n                source = temp_path\n                delete_source = True\n            exitcode = subprocess.call(['unzip', '-q', source, '-d', dest_path])\n            if (exitcode != 0):\n                raise UsageError('Invalid archive upload.')\n        finally:\n            if delete_source:\n                path_util.remove(source)\n", "label": 1}
{"function": "\n\ndef _complete(test_finder, thing):\n    if (':' in thing):\n        (module, test_part) = thing.split(':')\n        tests = list(test_finder.get_module_tests(module))\n        if ('.' in test_part):\n            return _get_prefixed(strings=tests, prefix=test_part)\n        funcs = [test for test in tests if (test.count('.') == 0)]\n        classes = [test.split('.')[0] for test in tests if ('.' in test)]\n        if (test_part in classes):\n            return ['.']\n        return _get_prefixed(strings=(funcs + classes), prefix=test_part)\n    if os.path.isdir(thing):\n        if ((thing != '.') and (not thing.endswith('/'))):\n            return ['/']\n        return _get_py_or_dirs(thing, '')\n    if os.path.exists(thing):\n        return [':']\n    (directory, file_part) = os.path.split(thing)\n    return _get_py_or_dirs(directory, file_part)\n", "label": 1}
{"function": "\n\ndef refresh(self):\n    if self.subscribed:\n        self.title = 'Subscribed projects'\n        if self.unreviewed:\n            self.title += ' with unreviewed changes'\n    else:\n        self.title = 'All projects'\n    self.app.status.update(title=self.title)\n    with self.app.db.getSession() as session:\n        i = 0\n        for project in session.getProjects(topicless=True, subscribed=self.subscribed, unreviewed=self.unreviewed):\n            i = self._projectRow(i, project, None)\n        for topic in session.getTopics():\n            i = self._topicRow(i, topic)\n            topic_unreviewed = 0\n            topic_open = 0\n            for project in topic.projects:\n                cache = self.app.project_cache.get(project)\n                topic_unreviewed += cache['unreviewed_changes']\n                topic_open += cache['open_changes']\n                if self.subscribed:\n                    if (not project.subscribed):\n                        continue\n                    if (self.unreviewed and (not cache['unreviewed_changes'])):\n                        continue\n                if (topic.key in self.open_topics):\n                    i = self._projectRow(i, project, topic)\n            topic_row = self.topic_rows.get(topic.key)\n            topic_row.update(topic, topic_unreviewed, topic_open)\n    while (i < len(self.listbox.body)):\n        current_row = self.listbox.body[i]\n        self._deleteRow(current_row)\n", "label": 1}
{"function": "\n\ndef __init__(self, param, cursor, strings_only=False):\n    if (settings.USE_TZ and (isinstance(param, datetime.datetime) and (not isinstance(param, Oracle_datetime)))):\n        if timezone.is_aware(param):\n            warnings.warn(\"The Oracle database adapter received an aware datetime (%s), probably from cursor.execute(). Update your code to pass a naive datetime in the database connection's time zone (UTC by default).\", RemovedInDjango20Warning)\n            param = param.astimezone(timezone.utc).replace(tzinfo=None)\n        param = Oracle_datetime.from_datetime(param)\n    if isinstance(param, datetime.timedelta):\n        param = duration_string(param)\n        if (' ' not in param):\n            param = ('0 ' + param)\n    string_size = 0\n    if (param is True):\n        param = 1\n    elif (param is False):\n        param = 0\n    if hasattr(param, 'bind_parameter'):\n        self.force_bytes = param.bind_parameter(cursor)\n    elif isinstance(param, Database.Binary):\n        self.force_bytes = param\n    else:\n        self.force_bytes = convert_unicode(param, cursor.charset, strings_only)\n        if isinstance(self.force_bytes, six.string_types):\n            string_size = len(force_bytes(param, cursor.charset, strings_only))\n    if hasattr(param, 'input_size'):\n        self.input_size = param.input_size\n    elif (string_size > 4000):\n        self.input_size = Database.CLOB\n    else:\n        self.input_size = None\n", "label": 1}
{"function": "\n\ndef _update_checksum(self, checksum, checksum_keyword='CHECKSUM', datasum_keyword='DATASUM'):\n    \"Update the 'CHECKSUM' and 'DATASUM' keywords in the header (or\\n        keywords with equivalent semantics given by the ``checksum_keyword``\\n        and ``datasum_keyword`` arguments--see for example ``CompImageHDU``\\n        for an example of why this might need to be overridden).\\n        \"\n    modified = (self._header._modified or self._data_loaded)\n    if (checksum == 'remove'):\n        if (checksum_keyword in self._header):\n            del self._header[checksum_keyword]\n        if (datasum_keyword in self._header):\n            del self._header[datasum_keyword]\n    elif (modified or self._new or (checksum and (('CHECKSUM' not in self._header) or ('DATASUM' not in self._header)))):\n        if (checksum == 'datasum'):\n            self.add_datasum(datasum_keyword=datasum_keyword)\n        elif (checksum == 'nonstandard_datasum'):\n            self.add_datasum(blocking='nonstandard', datasum_keyword=datasum_keyword)\n        elif (checksum == 'test'):\n            self.add_datasum(self._datasum_comment, datasum_keyword=datasum_keyword)\n            self.add_checksum(self._checksum_comment, True, checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n        elif (checksum == 'nonstandard'):\n            self.add_checksum(blocking='nonstandard', checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n        elif checksum:\n            self.add_checksum(blocking='standard', checksum_keyword=checksum_keyword, datasum_keyword=datasum_keyword)\n", "label": 1}
{"function": "\n\ndef skip(inbuf, ftype):\n    if ((ftype == TType.BOOL) or (ftype == TType.BYTE)):\n        inbuf.read(1)\n    elif (ftype == TType.I16):\n        inbuf.read(2)\n    elif (ftype == TType.I32):\n        inbuf.read(4)\n    elif (ftype == TType.I64):\n        inbuf.read(8)\n    elif (ftype == TType.DOUBLE):\n        inbuf.read(8)\n    elif (ftype == TType.STRING):\n        inbuf.read(unpack_i32(inbuf.read(4)))\n    elif ((ftype == TType.SET) or (ftype == TType.LIST)):\n        (v_type, sz) = read_list_begin(inbuf)\n        for i in range(sz):\n            skip(inbuf, v_type)\n    elif (ftype == TType.MAP):\n        (k_type, v_type, sz) = read_map_begin(inbuf)\n        for i in range(sz):\n            skip(inbuf, k_type)\n            skip(inbuf, v_type)\n    elif (ftype == TType.STRUCT):\n        while True:\n            (f_type, fid) = read_field_begin(inbuf)\n            if (f_type == TType.STOP):\n                break\n            skip(inbuf, f_type)\n", "label": 1}
{"function": "\n\n@register_opt('fast_compile')\n@local_optimizer([PdbBreakpoint])\ndef local_gpu_pdbbreakpoint_op(node):\n    if isinstance(node.op, PdbBreakpoint):\n        old_inputs = node.inputs\n        old_outputs = node.outputs\n        new_inputs = node.inputs[:1]\n        input_transfered = []\n        nb_monitored_vars = len(node.outputs)\n        for i in range(nb_monitored_vars):\n            inp = old_inputs[(i + 1)]\n            out = old_outputs[i]\n            input_is_from_gpu = (inp.owner and isinstance(inp.owner.op, HostFromGpu))\n            output_goes_to_gpu = False\n            for c in out.clients:\n                if (c == 'output'):\n                    continue\n                if isinstance(c[0].op, GpuFromHost):\n                    output_goes_to_gpu = True\n                    context_name = c[0].op.context_name\n                    break\n            if input_is_from_gpu:\n                new_inputs.append(inp.owner.inputs[0])\n                input_transfered.append(True)\n            elif output_goes_to_gpu:\n                new_inputs.append(GpuFromHost(context_name)(inp))\n                input_transfered.append(True)\n            else:\n                new_inputs.append(inp)\n                input_transfered.append(False)\n        if (not any(input_transfered)):\n            return False\n        new_op_outputs = node.op(*new_inputs, return_list=True)\n        new_outputs = []\n        for i in range(len(new_op_outputs)):\n            if input_transfered[i]:\n                new_outputs.append(host_from_gpu(new_op_outputs[i]))\n            else:\n                new_outputs.append(new_op_outputs[i])\n        return new_outputs\n    return False\n", "label": 1}
{"function": "\n\ndef test_json_conversion():\n    from commonast import Node, Assign, Name, BinOp, Bytes, Num\n    roota = Assign([Name('foo')], BinOp('Add', Name('a'), Num(3)))\n    rootb = Assign([Name('foo')], BinOp('Add', None, Num(3.2)))\n    rootc = Assign([Name('foo')], BinOp('Add', Bytes(b'xx'), Num(4j)))\n    for node1 in (roota, rootb, rootc):\n        js = node1.tojson()\n        node2 = Node.fromjson(js)\n        assert (js.count('BinOp') == 1)\n        assert (js.count('Num') == 1)\n        assert (node2.target_nodes[0].name == node1.target_nodes[0].name)\n        assert (node2.value_node.op == node1.value_node.op)\n        assert (node2.value_node.left_node == node1.value_node.left_node)\n        assert (node2.value_node.right_node.value == node1.value_node.right_node.value)\n        (node1 == node2)\n    assert (roota != rootb)\n    assert (roota != rootc)\n    with raises(ValueError):\n        (roota == 5)\n    assert (str(roota) == roota.tojson())\n    assert (len(repr(roota)) < 80)\n", "label": 1}
{"function": "\n\ndef encode(self, command, source, dest, pretend=False):\n    'Encode `source` to `dest` using command template `command`.\\n\\n        Raises `subprocess.CalledProcessError` if the command exited with a\\n        non-zero status code.\\n        '\n    assert isinstance(command, bytes)\n    assert isinstance(source, bytes)\n    assert isinstance(dest, bytes)\n    quiet = self.config['quiet'].get(bool)\n    if ((not quiet) and (not pretend)):\n        self._log.info('Encoding {0}', util.displayable_path(source))\n    args = shlex.split(command)\n    for (i, arg) in enumerate(args):\n        args[i] = Template(arg).safe_substitute({\n            b'source': source,\n            b'dest': dest,\n        })\n    if pretend:\n        self._log.info(' '.join(ui.decargs(args)))\n        return\n    try:\n        util.command_output(args)\n    except subprocess.CalledProcessError as exc:\n        self._log.info('Encoding {0} failed. Cleaning up...', util.displayable_path(source))\n        self._log.debug('Command {0} exited with status {1}', exc.cmd.decode('utf8', 'ignore'), exc.returncode)\n        util.remove(dest)\n        util.prune_dirs(os.path.dirname(dest))\n        raise\n    except OSError as exc:\n        raise ui.UserError(\"convert: could invoke '{0}': {1}\".format(' '.join(args), exc))\n    if ((not quiet) and (not pretend)):\n        self._log.info('Finished encoding {0}', util.displayable_path(source))\n", "label": 1}
{"function": "\n\ndef _complete_dispatcher(self, dt):\n    'This method is scheduled on all touch up events. It will dispatch\\n        the `on_gesture_complete` event for all completed gestures, and remove\\n        merged gestures from the internal gesture list.'\n    need_cleanup = False\n    gest = self._gestures\n    timeout = self.draw_timeout\n    twin = self.temporal_window\n    get_time = Clock.get_time\n    for (idx, g) in enumerate(gest):\n        if g.was_merged:\n            del gest[idx]\n            continue\n        if ((not g.active) or (g.active_strokes != 0)):\n            continue\n        t1 = (g._update_time + twin)\n        t2 = (get_time() + UNDERSHOOT_MARGIN)\n        if ((not g.accept_stroke()) or (t1 <= t2)):\n            discard = False\n            if ((g.width < 5) and (g.height < 5)):\n                discard = True\n            elif g.single_points_test():\n                discard = True\n            need_cleanup = True\n            g.active = False\n            g._cleanup_time = (get_time() + timeout)\n            if discard:\n                self.dispatch('on_gesture_discard', g)\n            else:\n                self.dispatch('on_gesture_complete', g)\n    if need_cleanup:\n        Clock.schedule_once(self._cleanup, timeout)\n", "label": 1}
{"function": "\n\ndef _repr(self, value, pos):\n    try:\n        if (value is None):\n            return ''\n        if self._unicode:\n            try:\n                value = str(value)\n                if (not is_unicode(value)):\n                    value = value.decode('utf-8')\n            except UnicodeDecodeError:\n                value = bytes(value)\n        else:\n            if (not isinstance(value, basestring_)):\n                value = coerce_text(value)\n            if (is_unicode(value) and self.default_encoding):\n                value = value.encode(self.default_encoding)\n    except:\n        exc_info = sys.exc_info()\n        e = exc_info[1]\n        e.args = (self._add_line_info(e.args[0], pos),)\n        raise e\n    else:\n        if (self._unicode and isinstance(value, bytes)):\n            if (not self.default_encoding):\n                raise UnicodeDecodeError(('Cannot decode bytes value %r into unicode (no default_encoding provided)' % value))\n            try:\n                value = value.decode(self.default_encoding)\n            except UnicodeDecodeError as e:\n                raise UnicodeDecodeError(e.encoding, e.object, e.start, e.end, (e.reason + (' in string %r' % value)))\n        elif ((not self._unicode) and is_unicode(value)):\n            if (not self.default_encoding):\n                raise UnicodeEncodeError(('Cannot encode unicode value %r into bytes (no default_encoding provided)' % value))\n            value = value.encode(self.default_encoding)\n        return value\n", "label": 1}
{"function": "\n\ndef test_source_packages():\n    for ext in ('.tar.gz', '.tar', '.tgz', '.zip', '.tar.bz2'):\n        sl = SourcePackage(('a_p_r-3.1.3' + ext))\n        assert (sl._name == 'a_p_r')\n        assert (sl.name == 'a-p-r')\n        assert (sl.raw_version == '3.1.3')\n        assert (sl.version == parse_version(sl.raw_version))\n        for req in ('a_p_r', 'a_p_r>2', 'a_p_r>3', 'a_p_r>=3.1.3', 'a_p_r==3.1.3', 'a_p_r>3,<3.5'):\n            assert sl.satisfies(req)\n            assert sl.satisfies(Requirement.parse(req))\n        for req in ('foo', 'a_p_r==4.0.0', 'a_p_r>4.0.0', 'a_p_r>3.0.0,<3.0.3', 'a==3.1.3'):\n            assert (not sl.satisfies(req))\n    sl = SourcePackage('python-dateutil-1.5.tar.gz')\n    assert (sl.name == 'python-dateutil')\n    assert (sl.raw_version == '1.5')\n", "label": 1}
{"function": "\n\ndef console_output(self, targets):\n    concrete_targets = set()\n    for target in targets:\n        concrete_target = target.concrete_derived_from\n        concrete_targets.add(concrete_target)\n        if isinstance(concrete_target, ScalaLibrary):\n            concrete_targets.update(concrete_target.java_sources)\n    buildroot = get_buildroot()\n    files = set()\n    output_globs = self.get_options().globs\n    concrete_targets = set([target for target in concrete_targets if (not target.is_synthetic)])\n    for target in concrete_targets:\n        files.add(target.address.build_file.full_path)\n        if (output_globs or target.has_sources()):\n            if output_globs:\n                globs_obj = target.globs_relative_to_buildroot()\n                if globs_obj:\n                    files.update((os.path.join(buildroot, src) for src in globs_obj['globs']))\n            else:\n                files.update((os.path.join(buildroot, src) for src in target.sources_relative_to_buildroot()))\n        if (isinstance(target, JvmApp) and (not output_globs)):\n            files.update(itertools.chain(*[bundle.filemap.keys() for bundle in target.bundles]))\n    return files\n", "label": 1}
{"function": "\n\ndef search_article(keyword, directory, datadir, exclude):\n    '\\n    Search for a keyword in every article within your current directory and\\n    below. Much like recursive grep.\\n    '\n    c = 0\n    r = re.compile(keyword)\n    print('Articles:')\n    for (dirpath, dirs, files) in os.walk(directory):\n        dirs[:] = [d for d in dirs if (d not in exclude)]\n        for fname in files:\n            path = os.path.join(dirpath, fname)\n            if (r.search(path) is not None):\n                print(('* \\x1b[92m%s\\x1b[39m' % os.path.relpath(path, datadir)))\n                c = (c + 1)\n    print('Content:')\n    for (dirpath, dirs, files) in os.walk(directory):\n        dirs[:] = [d for d in dirs if (d not in exclude)]\n        for fname in files:\n            path = os.path.join(dirpath, fname)\n            f = open(path, 'rt')\n            for (i, line) in enumerate(f):\n                if r.search(line):\n                    c = (c + 1)\n                    print(('* \\x1b[92m%s\\x1b[39m: %s' % (os.path.relpath(path, datadir), line.rstrip('\\n'))))\n    return ('Results: %s' % c)\n", "label": 1}
{"function": "\n\ndef test_rolling():\n    time = MockedTime()\n    percentile = RollingPercentile(time, 60000, 12, 1000, True)\n    percentile.add_value(1000)\n    percentile.add_value(1000)\n    percentile.add_value(1000)\n    percentile.add_value(2000)\n    assert (percentile.buckets.size == 1)\n    assert (percentile.percentile(50) == 0)\n    time.increment(6000)\n    assert (percentile.buckets.size == 1)\n    assert (percentile.percentile(50) == 1000)\n    assert (percentile.buckets.size == 2)\n    percentile.add_value(1000)\n    percentile.add_value(500)\n    assert (percentile.buckets.size == 2)\n    percentile.add_value(200)\n    percentile.add_value(200)\n    percentile.add_value(1600)\n    percentile.add_value(200)\n    percentile.add_value(1600)\n    percentile.add_value(1600)\n    assert (percentile.percentile(50) == 1000)\n    time.increment(6000)\n    snapshot = PercentileSnapshot(1000, 1000, 1000, 2000, 1000, 500, 200, 200, 1600, 200, 1600, 1600)\n    assert (snapshot.percentile(0.15) == percentile.percentile(0.15))\n    assert (snapshot.percentile(0.5) == percentile.percentile(0.5))\n    assert (snapshot.percentile(0.9) == percentile.percentile(0.9))\n    assert (snapshot.percentile(0.995) == percentile.percentile(0.995))\n    assert (snapshot.mean() == 991)\n", "label": 1}
{"function": "\n\ndef _convert_names_to_rpm(self, python_names, only_name):\n    if (not python_names):\n        return ({\n            \n        }, {\n            \n        })\n    cmdline = ((self._start_cmdline() + ['--convert']) + python_names)\n    result = collections.defaultdict(set)\n    conflicts = collections.defaultdict(set)\n    current_source = None\n    for line in sh.execute(cmdline)[0].splitlines():\n        if line.startswith('Requires:'):\n            line = line[len('Requires:'):]\n            if only_name:\n                positions = [line.find('>'), line.find('<'), line.find('=')]\n                positions = sorted([p for p in positions if (p != (- 1))])\n                if positions:\n                    line = line[0:positions[0]]\n            result[current_source].add(line.strip())\n        elif line.startswith('Conflicts:'):\n            line = line[len('Conflicts:'):]\n            conflicts[current_source].add(line.strip())\n        elif line.startswith('# Source:'):\n            current_source = line[len('# Source:'):].strip()\n    found_names = set(result.keys())\n    found_names.update(conflicts.keys())\n    (missing_names, extra_names) = _fetch_missing_extra(python_names, found_names)\n    if missing_names:\n        raise AssertionError(('Python names were lost during conversion: %s' % ', '.join(sorted(missing_names))))\n    if extra_names:\n        raise AssertionError(('Extra python names were found during conversion: %s' % ', '.join(sorted(extra_names))))\n    return (result, conflicts)\n", "label": 1}
{"function": "\n\ndef postgresql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None):\n    'PostgreSQL psycopg2 driver  accepts two syntaxes\\n\\n        Plus a string for .pgpass file\\n        '\n    dsn = []\n    if ((dsn_style is None) or (dsn_style == 'all') or (dsn_style == 'keyvalue')):\n        dsnstr = \"host='{0}' dbname='{2}' user='{3}' password='{4}'\"\n        if (dbport is not None):\n            dsnstr += \" port='{1}'\"\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'kwargs')):\n        dsnstr = \"host='{0}', database='{2}', user='{3}', password='{4}'\"\n        if (dbport is not None):\n            dsnstr += \", port='{1}'\"\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'uri')):\n        if (dbport is not None):\n            dsnstr = 'postgresql://{3}:{4}@{0}:{1}/{2}'\n        else:\n            dsnstr = 'postgresql://{3}:{4}@{0}/{2}'\n        dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass))\n    if ((dsn_style == 'all') or (dsn_style == 'pgpass')):\n        if (dbport is not None):\n            dbport = 5432\n        dsn.append('{0}:{1}:{2}:{3}:{4}'.format(dbhost, dbport, dbname, dbuser, dbpass))\n    return dsn\n", "label": 1}
{"function": "\n\ndef add_homology(self, seq, search, id=None, idFormat='%s_%d', autoIncrement=False, maxAnnot=999999, maxLoss=None, sliceInfo=None, **kwargs):\n    'find homology in our seq db and add as annotations'\n    try:\n        if (self.sliceAttrDict['id'] != 0):\n            raise KeyError\n    except KeyError:\n        sliceAttrDict['id'] = 0\n        sliceAttrDict['start'] = 1\n        sliceAttrDict['stop'] = 2\n    if autoIncrement:\n        id = len(self.sliceDB)\n    elif (id is None):\n        id = seq.id\n    if isinstance(search, str):\n        search = getattr(self.seqDB, search)\n    if isinstance(seq, str):\n        seq = Sequence(seq, str(id))\n    al = search(seq, **kwargs)\n    if (maxLoss is not None):\n        kwargs['minAlignSize'] = (len(seq) - maxLoss)\n    hits = al[seq].keys(**kwargs)\n    if (len(hits) > maxAnnot):\n        raise ValueError(('too many hits for %s: %d' % (id, len(hits))))\n    out = []\n    i = 0\n    k = id\n    for ival in hits:\n        if (len(hits) > 1):\n            if autoIncrement:\n                k = len(self.sliceDB)\n            else:\n                k = (idFormat % (id, i))\n            i += 1\n        if (sliceInfo is not None):\n            a = self.new_annotation(k, ((ival.id, ival.start, ival.stop) + sliceInfo))\n        else:\n            a = self.new_annotation(k, (ival.id, ival.start, ival.stop))\n        out.append(a)\n    return out\n", "label": 1}
{"function": "\n\ndef __init__(self, year=None, month=None, day=None, week=None, day_of_week=None, hour=None, minute=None, second=None, start_date=None, end_date=None, timezone=None):\n    if timezone:\n        self.timezone = astimezone(timezone)\n    elif (start_date and start_date.tzinfo):\n        self.timezone = start_date.tzinfo\n    elif (end_date and end_date.tzinfo):\n        self.timezone = end_date.tzinfo\n    else:\n        self.timezone = get_localzone()\n    self.start_date = convert_to_datetime(start_date, self.timezone, 'start_date')\n    self.end_date = convert_to_datetime(end_date, self.timezone, 'end_date')\n    values = dict(((key, value) for (key, value) in six.iteritems(locals()) if ((key in self.FIELD_NAMES) and (value is not None))))\n    self.fields = []\n    assign_defaults = False\n    for field_name in self.FIELD_NAMES:\n        if (field_name in values):\n            exprs = values.pop(field_name)\n            is_default = False\n            assign_defaults = (not values)\n        elif assign_defaults:\n            exprs = DEFAULT_VALUES[field_name]\n            is_default = True\n        else:\n            exprs = '*'\n            is_default = True\n        field_class = self.FIELDS_MAP[field_name]\n        field = field_class(field_name, exprs, is_default)\n        self.fields.append(field)\n", "label": 1}
{"function": "\n\ndef _eval_is_zero(self):\n    if self.function.is_zero:\n        return True\n    got_none = False\n    for l in self.limits:\n        if (len(l) == 3):\n            z = ((l[1] == l[2]) or (l[1] - l[2]).is_zero)\n            if z:\n                return True\n            elif (z is None):\n                got_none = True\n    free = self.function.free_symbols\n    for xab in self.limits:\n        if (len(xab) == 1):\n            free.add(xab[0])\n            continue\n        if ((len(xab) == 2) and (xab[0] not in free)):\n            if xab[1].is_zero:\n                return True\n            elif (xab[1].is_zero is None):\n                got_none = True\n        free.discard(xab[0])\n        for i in xab[1:]:\n            free.update(i.free_symbols)\n    if ((self.function.is_zero is False) and (got_none is False)):\n        return False\n", "label": 1}
{"function": "\n\ndef parse_command_line(args=None):\n    if (args is None):\n        args = sys.argv[1:]\n    usage = __doc__.format(cmd=os.path.basename(sys.argv[0]))\n    options = docopt(usage, args)\n    for opt_name in ('author', 'email', 'description'):\n        options[opt_name] = options.pop(('--' + opt_name))\n    options['command'] = options.pop('<command>')\n    options['project_name'] = options.pop('--project-name')\n    if (not RE_COMMAND_NAME.match(options['command'])):\n        die('command name must match regular expression {!r}', RE_COMMAND_NAME.pattern)\n    for (info, question, default_value) in (('author', 'Your name:  ', None), ('email', 'Your email: ', None), ('description', 'Description: ', None)):\n        if options.get(info, None):\n            continue\n        try:\n            options[info] = ask(question)\n        except EOFError:\n            pass\n        if (not options[info]):\n            if (default_value is None):\n                options[info] = ''\n            else:\n                options[info] = default_value\n    if (options['project_name'] is None):\n        default = 'OpenLMI {command} Script'.format(command=options['command'].capitalize())\n        try:\n            options['project_name'] = ask('Project name [{default}]: '.format(default=default))\n        except EOFError:\n            pass\n        if (not options['project_name']):\n            options['project_name'] = default\n    options = {k: (v.decode('utf-8') if isinstance(v, str) else v) for (k, v) in options.items()}\n    return options\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.settings = TrafficControlSetting()\n                self.settings.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.device = TrafficControlledDevice()\n                self.device.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.timeout = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef _create_joins(self, source_polymorphic=False, source_selectable=None, dest_polymorphic=False, dest_selectable=None, of_type=None):\n    if (source_selectable is None):\n        if (source_polymorphic and self.parent.with_polymorphic):\n            source_selectable = self.parent._with_polymorphic_selectable\n    aliased = False\n    if (dest_selectable is None):\n        if (dest_polymorphic and self.mapper.with_polymorphic):\n            dest_selectable = self.mapper._with_polymorphic_selectable\n            aliased = True\n        else:\n            dest_selectable = self.mapper.mapped_table\n        if (self._is_self_referential and (source_selectable is None)):\n            dest_selectable = dest_selectable.alias()\n            aliased = True\n    else:\n        aliased = True\n    dest_mapper = (of_type or self.mapper)\n    single_crit = dest_mapper._single_table_criterion\n    aliased = (aliased or (source_selectable is not None))\n    (primaryjoin, secondaryjoin, secondary, target_adapter, dest_selectable) = self._join_condition.join_targets(source_selectable, dest_selectable, aliased, single_crit)\n    if (source_selectable is None):\n        source_selectable = self.parent.local_table\n    if (dest_selectable is None):\n        dest_selectable = self.mapper.local_table\n    return (primaryjoin, secondaryjoin, source_selectable, dest_selectable, secondary, target_adapter)\n", "label": 1}
{"function": "\n\ndef dict_to_xml(self, root_elm, data):\n    for key in data.keys():\n        if (key in self.NO_SEND_FIELDS):\n            continue\n        sub_data = data[key]\n        elm = SubElement(root_elm, key)\n        if isinstance(sub_data, dict):\n            self.dict_to_xml(elm, sub_data)\n        elif (isinstance(sub_data, list) or isinstance(sub_data, tuple)):\n            if isplural(key):\n                for d in sub_data:\n                    self.dict_to_xml(SubElement(elm, singular(key)), d)\n            else:\n                for d in sub_data:\n                    self.dict_to_xml(elm, d)\n        else:\n            if (key in self.BOOLEAN_FIELDS):\n                val = ('true' if sub_data else 'false')\n            elif (key in self.DATE_FIELDS):\n                val = sub_data.strftime('%Y-%m-%dT%H:%M:%S')\n            else:\n                val = six.text_type(sub_data)\n            elm.text = val\n    return root_elm\n", "label": 1}
{"function": "\n\ndef has_method(obj, name):\n    if (not hasattr(obj, name)):\n        return False\n    func = getattr(obj, name)\n    if isinstance(func, types.BuiltinMethodType):\n        return True\n    if (not isinstance(func, (types.MethodType, types.FunctionType))):\n        return False\n    base_type = (obj if is_type(obj) else obj.__class__)\n    original = None\n    for subtype in inspect.getmro(base_type):\n        original = vars(subtype).get(name)\n        if (original is not None):\n            break\n    if (original is None):\n        return False\n    if isinstance(original, staticmethod):\n        return True\n    self_attr = ('__self__' if PY3 else 'im_self')\n    if (not hasattr(func, self_attr)):\n        return False\n    bound_to = getattr(func, self_attr)\n    if isinstance(original, classmethod):\n        return issubclass(base_type, bound_to)\n    return isinstance(obj, type(bound_to))\n", "label": 1}
{"function": "\n\n@classmethod\ndef get_data_rows(self, ar):\n    if ar.param_values.show_callables:\n\n        def flt(v):\n            return True\n    else:\n\n        def flt(v):\n            if isinstance(v, (types.FunctionType, types.GeneratorType, types.UnboundMethodType, types.UnboundMethodType, types.BuiltinMethodType, types.BuiltinFunctionType)):\n                return False\n            return True\n    o = self.get_inspected(ar.param_values.inspected)\n    if isinstance(o, (list, tuple)):\n        for (i, v) in enumerate(o):\n            k = (('[' + str(i)) + ']')\n            (yield Inspected(o, '', k, v))\n    elif isinstance(o, AttrDict):\n        for (k, v) in list(o.items()):\n            (yield Inspected(o, '.', k, v))\n    elif isinstance(o, dict):\n        for (k, v) in list(o.items()):\n            k = (('[' + repr(k)) + ']')\n            (yield Inspected(o, '', k, v))\n    else:\n        for k in dir(o):\n            if (not k.startswith('__')):\n                if ((not ar.quick_search) or (ar.quick_search.lower() in k.lower())):\n                    v = getattr(o, k)\n                    if flt(v):\n                        (yield Inspected(o, '.', k, v))\n", "label": 1}
{"function": "\n\ndef __init__(self, method, uri, version='HTTP/1.0', headers=None, body=None, remote_ip=None, protocol=None, host=None, files=None, connection=None):\n    self.method = method\n    self.uri = uri\n    self.version = version\n    self.headers = (headers or httputil.HTTPHeaders())\n    self.body = (body or '')\n    self.remote_ip = remote_ip\n    if protocol:\n        self.protocol = protocol\n    elif (connection and isinstance(connection.stream, iostream.SSLIOStream)):\n        self.protocol = 'https'\n    else:\n        self.protocol = 'http'\n    if (connection and connection.xheaders):\n        ip = self.headers.get('X-Forwarded-For', self.remote_ip)\n        ip = ip.split(',')[(- 1)].strip()\n        ip = self.headers.get('X-Real-Ip', ip)\n        if netutil.is_valid_ip(ip):\n            self.remote_ip = ip\n        proto = self.headers.get('X-Scheme', self.headers.get('X-Forwarded-Proto', self.protocol))\n        if (proto in ('http', 'https')):\n            self.protocol = proto\n    self.host = (host or self.headers.get('Host') or '127.0.0.1')\n    self.files = (files or {\n        \n    })\n    self.connection = connection\n    self._start_time = time.time()\n    self._finish_time = None\n    (self.path, sep, self.query) = uri.partition('?')\n    self.arguments = parse_qs_bytes(self.query, keep_blank_values=True)\n", "label": 1}
{"function": "\n\ndef clean(self):\n    type = self.cleaned_data['recordType']\n    if (type in ('4', 'Z')):\n        for field in ADDRESS_FIELDS:\n            self.cleaned_data[field] = None\n    if (type == 'Z'):\n        self.cleaned_data['zipExtensionLow'] = None\n        self.cleaned_data['zipExtensionHigh'] = None\n        low = self.cleaned_data['zipCodeLow']\n        high = self.cleaned_data['zipCodeHigh']\n        if ((high is None) or (low is None)):\n            raise ValidationError(_('Zip-5 records need a high and a low.'))\n    elif (type == '4'):\n        low = self.cleaned_data['zipCodeLow']\n        high = self.cleaned_data['zipCodeHigh']\n        low_ext = self.cleaned_data['zipExtensionLow']\n        high_ext = self.cleaned_data['zipExtensionHigh']\n        if ((high is None) or (low is None) or (low_ext is None) or (high_ext is None)):\n            raise ValidationError(_('Zip+4 records need a high and a low for both parts.'))\n    else:\n        for field in ZIP_FIELDS:\n            self.cleaned_data[field] = None\n        for field in ('lowAddress', 'highAddress', 'streetName', 'cityName', 'zipCode', 'plus4'):\n            if (not self.cleaned_data[field]):\n                raise ValidationError(_('Address rocord needs: low, high, street, city, zip, zip+4'))\n    return super(TaxBoundryForm, self).clean()\n", "label": 1}
{"function": "\n\n@staticmethod\ndef validate_field(value, field, new_record=True):\n    '\\n        Validates a field value against a field metadata dictionary. Note: this\\n        is not yet intended to be a full validation. There are sure to be\\n        missing validation cases. Returns a list of validation errors.\\n        '\n    errors = []\n    if new_record:\n        if ((not field['createable']) and (value is not None)):\n            errors.append('Cannot create this field')\n    elif ((not field['updateable']) and (value is not None)):\n        errors.append('Cannot update this field')\n    if ((value is not None) and field.get('restrictedPicklist')):\n        values = [i['value'] for i in field['picklistValues'] if i['active']]\n        if (value not in values):\n            errors.append('Bad value for restricted picklist field')\n    if (new_record and (value is None) and (not field['nillable']) and (not field['defaultedOnCreate']) and (field['type'] != 'boolean')):\n        errors.append('This field is required')\n    return errors\n", "label": 1}
{"function": "\n\ndef merge_undo(self, undo_item):\n    ' Merges two undo items if possible.\\n        '\n    if (isinstance(undo_item, self.__class__) and (self.object is undo_item.object) and (self.name == undo_item.name) and (self.index == undo_item.index)):\n        added = undo_item.added\n        removed = undo_item.removed\n        if ((len(self.added) == len(added)) and (len(self.removed) == len(removed))):\n            for (i, item) in enumerate(self.added):\n                if (item is not added[i]):\n                    break\n            else:\n                for (i, item) in enumerate(self.removed):\n                    if (item is not removed[i]):\n                        break\n                else:\n                    return True\n    return False\n", "label": 1}
{"function": "\n\ndef find_selected_files(schema, metadata):\n    targets = []\n    paths = [('', p) for p in schema.schema['pages']]\n    while len(paths):\n        (prefix, path) = paths.pop(0)\n        if path.get('questions'):\n            paths = (paths + [('', q) for q in path['questions']])\n        elif path.get('type'):\n            qid = path.get('qid', path.get('id'))\n            if (path['type'] == 'object'):\n                paths = (paths + [('{}.{}.value'.format(prefix, qid), p) for p in path['properties']])\n            elif (path['type'] == 'osf-upload'):\n                targets.append('{}.{}'.format(prefix, qid).lstrip('.'))\n    selected = {\n        \n    }\n    for t in targets:\n        parts = t.split('.')\n        value = metadata.get(parts.pop(0))\n        while (value and len(parts)):\n            value = value.get(parts.pop(0))\n        if value:\n            selected[t] = value\n    return selected\n", "label": 1}
{"function": "\n\n@defun_wrapped\ndef _ci_generic(ctx, z):\n    if ctx.isinf(z):\n        if (z == ctx.inf):\n            return ctx.zero\n        if (z == ctx.ninf):\n            return (ctx.pi * 1j)\n    jz = ctx.fmul(ctx.j, z, exact=True)\n    njz = ctx.fneg(jz, exact=True)\n    v = (0.5 * (ctx.ei(jz) + ctx.ei(njz)))\n    zreal = ctx._re(z)\n    zimag = ctx._im(z)\n    if (zreal == 0):\n        if (zimag > 0):\n            v += (ctx.pi * 0.5j)\n        if (zimag < 0):\n            v -= (ctx.pi * 0.5j)\n    if (zreal < 0):\n        if (zimag >= 0):\n            v += (ctx.pi * 1j)\n        if (zimag < 0):\n            v -= (ctx.pi * 1j)\n    if (ctx._is_real_type(z) and (zreal > 0)):\n        v = ctx._re(v)\n    return v\n", "label": 1}
{"function": "\n\ndef to_json_message(self):\n    json_message = {\n        'From': self.__sender,\n        'To': self.__to,\n        'Subject': self.__subject,\n    }\n    if self.__reply_to:\n        json_message['ReplyTo'] = self.__reply_to\n    if self.__cc:\n        json_message['Cc'] = self.__cc\n    if self.__bcc:\n        json_message['Bcc'] = self.__bcc\n    if self.__tag:\n        json_message['Tag'] = self.__tag\n    if self.__html_body:\n        json_message['HtmlBody'] = self.__html_body\n    if self.__text_body:\n        json_message['TextBody'] = self.__text_body\n    if self.__track_opens:\n        json_message['TrackOpens'] = True\n    if (len(self.__custom_headers) > 0):\n        cust_headers = []\n        for (key, value) in self.__custom_headers.items():\n            cust_headers.append({\n                'Name': key,\n                'Value': value,\n            })\n        json_message['Headers'] = cust_headers\n    if (len(self.__attachments) > 0):\n        attachments = []\n        for attachment in self.__attachments:\n            if isinstance(attachment, tuple):\n                attachments.append({\n                    'Name': attachment[0],\n                    'Content': attachment[1],\n                    'ContentType': attachment[2],\n                })\n            elif isinstance(attachment, MIMEBase):\n                attachments.append({\n                    'Name': attachment.get_filename(),\n                    'Content': attachment.get_payload(),\n                    'ContentType': attachment.get_content_type(),\n                })\n        json_message['Attachments'] = attachments\n    return json_message\n", "label": 1}
{"function": "\n\ndef _eval_expand_mul(self, **hints):\n    from sympy import fraction\n    expr = self\n    (n, d) = fraction(expr)\n    if d.is_Mul:\n        (n, d) = [(i._eval_expand_mul(**hints) if i.is_Mul else i) for i in (n, d)]\n        expr = (n / d)\n        if (not expr.is_Mul):\n            return expr\n    (plain, sums, rewrite) = ([], [], False)\n    for factor in expr.args:\n        if factor.is_Add:\n            sums.append(factor)\n            rewrite = True\n        elif factor.is_commutative:\n            plain.append(factor)\n        else:\n            sums.append(Basic(factor))\n    if (not rewrite):\n        return expr\n    else:\n        plain = self.func(*plain)\n        if sums:\n            terms = self.func._expandsums(sums)\n            args = []\n            for term in terms:\n                t = self.func(plain, term)\n                if (t.is_Mul and any((a.is_Add for a in t.args))):\n                    t = t._eval_expand_mul()\n                args.append(t)\n            return Add(*args)\n        else:\n            return plain\n", "label": 1}
{"function": "\n\ndef _delete_node(self, nid):\n    ' Deletes a specified tree node and all its children.\\n        '\n    for cnid in self._nodes_for(nid):\n        self._delete_node(cnid)\n    pnid = nid.parent()\n    if ((pnid is not None) and (getattr(pnid, '_dummy', None) is nid)):\n        pnid.removeChild(nid)\n        del pnid._dummy\n        return\n    try:\n        (expanded, node, object) = self._get_node_data(nid)\n    except AttributeError:\n        pass\n    else:\n        id_object = id(object)\n        object_info = self._map[id_object]\n        for (i, info) in enumerate(object_info):\n            if (id(nid) == id(info[1])):\n                del object_info[i]\n                break\n        if (len(object_info) == 0):\n            self._remove_listeners(node, object)\n            del self._map[id_object]\n    if (pnid is None):\n        self._tree.takeTopLevelItem(self._tree.indexOfTopLevelItem(nid))\n    else:\n        pnid.removeChild(nid)\n    if ((self._editor is not None) and (id(nid) == id(self._editor._editor_nid))):\n        self._clear_editor()\n", "label": 1}
{"function": "\n\ndef solve(self, board, i, j):\n    '\\n        dfs\\n        :param board: a 9x9 2D array\\n        :return: Boolean\\n        '\n    if (j >= 9):\n        return self.solve(board, (i + 1), 0)\n    if (i == 9):\n        return True\n    if (board[i][j] == '.'):\n        for num in range(1, 10):\n            num_str = str(num)\n            if (all([(board[i][col] != num_str) for col in xrange(9)]) and all([(board[row][j] != num_str) for row in xrange(9)]) and all([(board[(((i / 3) * 3) + (count / 3))][(((j / 3) * 3) + (count % 3))] != num_str) for count in xrange(9)])):\n                board[i][j] = num_str\n                if (not self.solve(board, i, (j + 1))):\n                    board[i][j] = '.'\n                else:\n                    return True\n    else:\n        return self.solve(board, i, (j + 1))\n    return False\n", "label": 1}
{"function": "\n\ndef read(self, size=1):\n    '        Read size bytes from the serial port. If a timeout is set it may\\n        return less characters as requested. With no timeout it will block\\n        until the requested number of bytes is read.\\n        '\n    if (not self.is_open):\n        raise portNotOpenError\n    if ((self._timeout is not None) and (self._timeout != 0)):\n        timeout = (time.time() + self._timeout)\n    else:\n        timeout = None\n    data = bytearray()\n    while ((size > 0) and self.is_open):\n        try:\n            b = self.queue.get(timeout=self._timeout)\n        except queue.Empty:\n            if (self._timeout == 0):\n                break\n        else:\n            if (data is not None):\n                data += b\n                size -= 1\n            else:\n                break\n        if (timeout and (time.time() > timeout)):\n            if self.logger:\n                self.logger.info('read timeout')\n            break\n    return bytes(data)\n", "label": 1}
{"function": "\n\ndef before_after_sort(items):\n    \" Sort a sequence of items with 'before', 'after', and 'id' attributes.\\n        \\n    The sort is topological. If an item does not specify a 'before' or 'after',\\n    it is placed after the preceding item.\\n\\n    If a cycle is found in the dependencies, a warning is logged and the order\\n    of the items is undefined.\\n    \"\n    if (len(items) < 2):\n        return items\n    item_map = dict(((item.id, item) for item in items if item.id))\n    pairs = []\n    prev_item = None\n    for item in items:\n        new_pairs = []\n        if (hasattr(item, 'before') and item.before):\n            (parent, child) = (item, item_map.get(item.before))\n            if child:\n                new_pairs.append((parent, child))\n        if (hasattr(item, 'after') and item.after):\n            (parent, child) = (item_map.get(item.after), item)\n            if parent:\n                new_pairs.append((parent, child))\n        if new_pairs:\n            pairs.extend(new_pairs)\n        else:\n            if prev_item:\n                pairs.append((prev_item, item))\n            prev_item = item\n    (result, has_cycle) = topological_sort(pairs)\n    if has_cycle:\n        logger.warning('Cycle in before/after sort for items %r', items)\n    return result\n", "label": 1}
{"function": "\n\ndef test_goaa_happy_path(self):\n    (parser, opts, files) = self._run_check(['../gristle_file_converter.py', 'census.csv', '-d', ',', '-D', '|'], 'pass')\n    assert (opts.output is None)\n    assert (opts.recdelimiter is None)\n    assert (opts.delimiter == ',')\n    assert (opts.out_delimiter == '|')\n    assert (opts.recdelimiter is None)\n    assert (opts.out_recdelimiter is None)\n    assert (opts.quoting is False)\n    assert (opts.out_quoting is False)\n    assert (opts.quotechar == '\"')\n    assert (opts.hasheader is False)\n    assert (opts.out_hasheader is False)\n    assert (opts.stripfields is False)\n    assert (opts.verbose is True)\n    self._run_check(['../gristle_file_converter.py', 'census4.csv', '-D', '|'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census5.csv', '-d', ',', '-D', '|'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-d', ',', '-D', '|', '--hasheader', '--outhasheader'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-D', '|', '-r', '-R', '-q', '-Q', '--quotechar', '^'], 'pass')\n    self._run_check(['../gristle_file_converter.py', 'census6.csv', '-D', '|', '--stripfields'], 'pass')\n", "label": 1}
{"function": "\n\ndef linkSamplePredicate(subsample_size, predicate, items1, items2):\n    sample = []\n    predicate_function = predicate.func\n    field = predicate.field\n    red = defaultdict(list)\n    blue = defaultdict(list)\n    for (i, (index, record)) in enumerate(interleave(items1, items2)):\n        if (i == 20000):\n            if ((min(len(red), len(blue)) + len(sample)) < 10):\n                return sample\n        column = record[field]\n        if (not column):\n            (red, blue) = (blue, red)\n            continue\n        block_keys = predicate_function(column)\n        for block_key in block_keys:\n            if blue.get(block_key):\n                pair = sort_pair(blue[block_key].pop(), index)\n                sample.append(pair)\n                subsample_size -= 1\n                if subsample_size:\n                    break\n                else:\n                    return sample\n            else:\n                red[block_key].append(index)\n        (red, blue) = (blue, red)\n    for (index, record) in itertools.islice(items2, len(items1)):\n        column = record[field]\n        if (not column):\n            continue\n        block_keys = predicate_function(column)\n        for block_key in block_keys:\n            if red.get(block_key):\n                pair = sort_pair(red[block_key].pop(), index)\n                sample.append(pair)\n                subsample_size -= 1\n                if subsample_size:\n                    break\n                else:\n                    return sample\n    return sample\n", "label": 1}
{"function": "\n\ndef dataReceived(self, data):\n    chunk = StringIO()\n    for char in data:\n        if self.gotIAC:\n            if self.iacByte:\n                if (self.iacByte == SB):\n                    if (char == SE):\n                        self.iacSBchunk(chunk.getvalue())\n                        chunk = StringIO()\n                        del self.iacByte\n                        del self.gotIAC\n                    else:\n                        chunk.write(char)\n                else:\n                    try:\n                        getattr(self, ('iac_%s' % iacBytes[self.iacByte]))(char)\n                    except KeyError:\n                        pass\n                    del self.iacByte\n                    del self.gotIAC\n            else:\n                self.iacByte = char\n        elif (char == IAC):\n            c = chunk.getvalue()\n            if c:\n                why = self.processChunk(c)\n                if why:\n                    return why\n                chunk = StringIO()\n            self.gotIAC = 1\n        else:\n            chunk.write(char)\n    c = chunk.getvalue()\n    if c:\n        why = self.processChunk(c)\n        if why:\n            return why\n", "label": 1}
{"function": "\n\ndef dispatch(self, pdu):\n    if isinstance(pdu, Symmetry):\n        return\n    if isinstance(pdu, AggregatedFrame):\n        if ((pdu.dsap == 0) and (pdu.ssap == 0)):\n            [log.debug(('     ' + str(p))) for p in pdu]\n            [self.dispatch(p) for p in pdu]\n        return\n    if (isinstance(pdu, Connect) and (pdu.dsap == 1)):\n        addr = self.snl.get(pdu.sn)\n        if ((not addr) or (self.sap[addr] is None)):\n            log.debug(\"no service named '{0}'\".format(pdu.sn))\n            pdu = DisconnectedMode(pdu.ssap, 1, reason=2)\n            self.sap[1].dmpdu.append(pdu)\n            return\n        pdu = Connect(dsap=addr, ssap=pdu.ssap, rw=pdu.rw, miu=pdu.miu)\n    with self.lock:\n        sap = self.sap[pdu.dsap]\n        if sap:\n            sap.enqueue(pdu)\n            return\n    log.debug('discard PDU {0}'.format(str(pdu)))\n    return\n", "label": 1}
{"function": "\n\ndef summary(self):\n    tpg = self.rtsnode\n    status = None\n    msg = []\n    if tpg.has_feature('nexus'):\n        msg.append(str(self.rtsnode.nexus))\n    if (not tpg.enable):\n        return ('disabled', False)\n    if tpg.has_feature('acls'):\n        if (('generate_node_acls' in tpg.list_attributes()) and int(tpg.get_attribute('generate_node_acls'))):\n            msg.append('gen-acls')\n        else:\n            msg.append('no-gen-acls')\n        if tpg.has_feature('auth'):\n            if (not int(tpg.get_attribute('authentication'))):\n                msg.append('no-auth')\n                if int(tpg.get_attribute('generate_node_acls')):\n                    status = True\n            elif (not int(tpg.get_attribute('generate_node_acls'))):\n                msg.append('auth per-acl')\n            else:\n                msg.append('tpg-auth')\n                status = True\n                if (not (tpg.chap_password and tpg.chap_userid)):\n                    status = False\n                if tpg.authenticate_target:\n                    msg.append('mutual auth')\n                else:\n                    msg.append('1-way auth')\n    return (', '.join(msg), status)\n", "label": 1}
{"function": "\n\ndef alignment_is_good(record, a, arange):\n    trimmed_slen = (arange.end - arange.start)\n    if (trimmed_slen < 0):\n        sys.exit('BUG: trimmed_slen bad value\\n')\n    qury_wiggle = (record.qlen * WIGGLE_PCT)\n    if (not (fabs((a.send - a.sstart)) > MIN_ALIGNMENT_SIZE)):\n        return False\n    if ((fabs((a.send - a.sstart)) >= (trimmed_slen * CONTAINED_PCT)) and (record.qlen > record.slen)):\n        return True\n    if ((fabs((a.qend - a.qstart)) > (record.qlen * CONTAINED_PCT)) and (record.slen > record.qlen)):\n        return True\n    if (((a.sstart == arange.start) or (arange.end == a.send)) and ((a.qstart < qury_wiggle) or (a.qend < qury_wiggle) or ((record.qlen - a.qstart) < qury_wiggle) or ((record.qlen - a.qend) < qury_wiggle))):\n        return True\n    return False\n", "label": 1}
{"function": "\n\ndef check_if_doc_is_linked(doc, method='Delete'):\n    '\\n\\t\\tRaises excption if the given doc(dt, dn) is linked in another record.\\n\\t'\n    from frappe.model.rename_doc import get_link_fields\n    link_fields = get_link_fields(doc.doctype)\n    link_fields = [[lf['parent'], lf['fieldname'], lf['issingle']] for lf in link_fields]\n    for (link_dt, link_field, issingle) in link_fields:\n        if (not issingle):\n            item = frappe.db.get_value(link_dt, {\n                link_field: doc.name,\n            }, ['name', 'parent', 'parenttype', 'docstatus'], as_dict=True)\n            if (item and ((item.parent or item.name) != doc.name) and (((method == 'Delete') and (item.docstatus < 2)) or ((method == 'Cancel') and (item.docstatus == 1)))):\n                frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, (item.parenttype if item.parent else link_dt), (item.parent or item.name)), frappe.LinkExistsError)\n", "label": 1}
{"function": "\n\ndef match(self, interp, block, typemap, calltypes):\n    '\\n        Look for potential macros for expand and store their expansions.\\n        '\n    self.block = block\n    self.rewrites = rewrites = {\n        \n    }\n    for inst in block.body:\n        if isinstance(inst, ir.Assign):\n            rhs = inst.value\n            if (isinstance(rhs, ir.Expr) and (rhs.op == 'call') and isinstance(rhs.func, ir.Var)):\n                try:\n                    const = interp.infer_constant(rhs.func)\n                except errors.ConstantInferenceError:\n                    continue\n                if isinstance(const, Macro):\n                    assert const.callable\n                    new_expr = self._expand_callable_macro(interp, rhs, const, rhs.loc)\n                    rewrites[rhs] = new_expr\n            elif (isinstance(rhs, ir.Expr) and (rhs.op == 'getattr')):\n                try:\n                    const = interp.infer_constant(inst.target)\n                except errors.ConstantInferenceError:\n                    continue\n                if (isinstance(const, Macro) and (not const.callable)):\n                    new_expr = self._expand_non_callable_macro(const, rhs.loc)\n                    rewrites[rhs] = new_expr\n    return (len(rewrites) > 0)\n", "label": 1}
{"function": "\n\ndef temp_name(self, expr):\n    c = expr.__class__\n    if (c is PrimCall):\n        return expr.prim.name\n    elif (c is Attribute):\n        if (expr.value.__class__ is Var):\n            return ((names.original(expr.value.name) + '_') + expr.name)\n        else:\n            return expr.name\n    elif (c is Attribute):\n        if (expr.value.__class__ is Var):\n            return ('%s_%s' % (expr.value.name, expr.name))\n        else:\n            return expr.name\n    elif (c is Index):\n        idx_t = expr.index.type\n        if (isinstance(idx_t, SliceT) or (isinstance(idx_t, TupleT) and any((isinstance(elt_t, SliceT) for elt_t in idx_t.elt_types)))):\n            return 'slice'\n        else:\n            return 'elt'\n    elif (c is TupleProj):\n        if (expr.tuple.__class__ is Var):\n            original = names.original(expr.tuple.name)\n            return ('%s%d_elt%d' % (original, names.versions[original], expr.index))\n        else:\n            return ('tuple_elt%d' % expr.index)\n    elif (c is Var):\n        return names.refresh(expr.name)\n    else:\n        return 'temp'\n", "label": 1}
{"function": "\n\ndef on_ssl(self, client_hello):\n    anon_ciphers = [str(c) for c in client_hello.ciphers if ('_anon_' in str(c))]\n    if anon_ciphers:\n        self._handle_bad_ciphers(anon_ciphers, ('Client enabled anonymous TLS/SSL cipher suites %s' % ', '.join(anon_ciphers)))\n    null_ciphers = [str(c) for c in client_hello.ciphers if ('_WITH_NULL_' in str(c))]\n    if null_ciphers:\n        self._handle_bad_ciphers(null_ciphers, ('Client enabled NULL encryption TLS/SSL cipher suites %s' % ', '.join(null_ciphers)))\n    integ_ciphers = [str(c) for c in client_hello.ciphers if str(c).endswith('_NULL')]\n    if integ_ciphers:\n        self._handle_bad_ciphers(integ_ciphers, ('Client enabled NULL integrity TLS/SSL cipher suites %s' % ', '.join(integ_ciphers)))\n    export_ciphers = [str(c) for c in client_hello.ciphers if ('EXPORT' in str(c))]\n    if export_ciphers:\n        self._handle_bad_ciphers(integ_ciphers, ('Client enabled export TLS/SSL cipher suites %s' % ', '.join(export_ciphers)))\n", "label": 1}
{"function": "\n\ndef _parseOptions(options):\n    opts = {\n        \n    }\n    if ('url' not in options):\n        raise Exception('URL needed')\n    else:\n        opts['url'] = options['url']\n    if ('compression' in options):\n        value = options['compression'].lower().strip()\n        if (value == 'true'):\n            opts['enableCompression'] = True\n        elif (value == 'false'):\n            opts['enableCompression'] = False\n        else:\n            raise Exception(\"invalid value '{0}' for compression\".format(value))\n    if ('autofrag' in options):\n        try:\n            value = int(options['autofrag'])\n        except:\n            raise Exception(\"invalid value '{0}' for autofrag\".format(options['autofrag']))\n        if (value < 0):\n            raise Exception(\"negative value '{0}' for autofrag\".format(value))\n        opts['autoFragmentSize'] = value\n    if ('subprotocol' in options):\n        value = options['subprotocol'].lower().strip()\n        opts['subprotocol'] = value\n    if ('debug' in options):\n        value = options['debug'].lower().strip()\n        if (value == 'true'):\n            opts['debug'] = True\n        elif (value == 'false'):\n            opts['debug'] = False\n        else:\n            raise Exception(\"invalid value '{0}' for debug\".format(value))\n    return opts\n", "label": 1}
{"function": "\n\ndef take_action(self, parsed_args):\n    client = self.get_client()\n    extra_values = v2_0.parse_args_to_dict(self.values_specs)\n    if extra_values:\n        raise exceptions.CommandError((_('Invalid argument(s): --%s') % ', --'.join(extra_values)))\n    tenant_id = (parsed_args.tenant_id or parsed_args.pos_tenant_id)\n    if parsed_args.dry_run:\n        data = client.validate_auto_allocated_topology_requirements(tenant_id)\n    else:\n        data = client.get_auto_allocated_topology(tenant_id)\n    if (self.resource in data):\n        for (k, v) in data[self.resource].items():\n            if isinstance(v, list):\n                value = ''\n                for _item in v:\n                    if value:\n                        value += '\\n'\n                    if isinstance(_item, dict):\n                        value += jsonutils.dumps(_item)\n                    else:\n                        value += str(_item)\n                data[self.resource][k] = value\n            elif (v == 'dry-run=pass'):\n                return (('dry-run',), ('pass',))\n            elif (v is None):\n                data[self.resource][k] = ''\n        return zip(*sorted(data[self.resource].items()))\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef tostr(object, encoding=None):\n    ' get a unicode safe string representation of an object '\n    if isinstance(object, basestring):\n        if (encoding is None):\n            return object\n        else:\n            return object.encode(encoding)\n    if isinstance(object, tuple):\n        s = ['(']\n        for item in object:\n            if isinstance(item, basestring):\n                s.append(item)\n            else:\n                s.append(tostr(item))\n            s.append(', ')\n        s.append(')')\n        return ''.join(s)\n    if isinstance(object, list):\n        s = ['[']\n        for item in object:\n            if isinstance(item, basestring):\n                s.append(item)\n            else:\n                s.append(tostr(item))\n            s.append(', ')\n        s.append(']')\n        return ''.join(s)\n    if isinstance(object, dict):\n        s = ['{']\n        for item in object.items():\n            if isinstance(item[0], basestring):\n                s.append(item[0])\n            else:\n                s.append(tostr(item[0]))\n            s.append(' = ')\n            if isinstance(item[1], basestring):\n                s.append(item[1])\n            else:\n                s.append(tostr(item[1]))\n            s.append(', ')\n        s.append('}')\n        return ''.join(s)\n    try:\n        return unicode(object)\n    except:\n        return str(object)\n", "label": 1}
{"function": "\n\ndef test_create_alt(scratch_tree, scratch_pad):\n    sess = scratch_tree.edit('/', alt='de')\n    assert (sess.id == '')\n    assert (sess.path == '/')\n    assert (sess.record is not None)\n    assert (sess['_model'] == 'page')\n    assert (sess['title'] == 'Index')\n    assert (sess['body'] == 'Hello World!')\n    sess['body'] = 'Hallo Welt!'\n    sess.commit()\n    assert sess.closed\n    with open(sess.get_fs_path(alt='de')) as f:\n        assert (f.read().splitlines() == ['body: Hallo Welt!'])\n    scratch_pad.cache.flush()\n    item = scratch_pad.get('/', alt='de')\n    assert (item['_slug'] == '')\n    assert (item['title'] == 'Index')\n    assert (item['body'].source == 'Hallo Welt!')\n    assert (item['_model'] == 'page')\n", "label": 1}
{"function": "\n\ndef parse_docstring(self, s, pnames=None):\n    free_text = []\n    header = []\n    label = None\n    last_argname = None\n    for p in split_docstring(s):\n        argdoc = self.argdoc_re.match(p)\n        if argdoc:\n            (argname, text) = argdoc.groups()\n            if free_text:\n                if free_text[(- 1)].endswith(':'):\n                    label = free_text.pop()\n                if last_argname:\n                    if ((pnames is None) or (last_argname in pnames)):\n                        self.after[last_argname] = free_text\n                else:\n                    header.extend(free_text)\n                free_text = []\n            last_argname = argname\n            try:\n                default_label = self.get_param_type(self.signature.parameters[argname])\n            except KeyError:\n                continue\n            if ((pnames is not None) and (argname not in pnames)):\n                continue\n            if (default_label != LABEL_POS):\n                try:\n                    (param, _) = self.sections[default_label].pop(argname)\n                except KeyError:\n                    continue\n                label_ = (label or default_label)\n                if (label_ not in self.sections):\n                    self.sections[label_] = util.OrderedDict()\n            else:\n                try:\n                    (param, _) = self.sections[default_label][argname]\n                except KeyError:\n                    continue\n                label_ = default_label\n            self.sections[label_][argname] = (param, text)\n        else:\n            free_text.append(p)\n    if (not last_argname):\n        header = free_text\n        footer = []\n    else:\n        footer = free_text\n    return (lines_to_paragraphs(header), lines_to_paragraphs(footer))\n", "label": 1}
{"function": "\n\ndef get_value(self, context, *tag_args, **tag_kwargs):\n    request = self.get_request(context)\n    output = None\n    (slot,) = tag_args\n    template_name = (tag_kwargs.get('template') or None)\n    cachable = is_true(tag_kwargs.get('cachable', (not bool(template_name))))\n    if (template_name and cachable and (not extract_literal(self.kwargs['template']))):\n        raise TemplateSyntaxError(\"{0} tag does not allow 'cachable' for variable template names!\".format(self.tag_name))\n    try_cache = (appsettings.FLUENT_CONTENTS_CACHE_OUTPUT and appsettings.FLUENT_CONTENTS_CACHE_PLACEHOLDER_OUTPUT and cachable)\n    if isinstance(slot, SharedContent):\n        sharedcontent = slot\n        if try_cache:\n            cache_key = get_shared_content_cache_key(sharedcontent)\n            output = cache.get(cache_key)\n    else:\n        site = Site.objects.get_current()\n        if try_cache:\n            cache_key_ptr = get_shared_content_cache_key_ptr(int(site.pk), slot, language_code=get_language())\n            cache_key = cache.get(cache_key_ptr)\n            if (cache_key is not None):\n                output = cache.get(cache_key)\n        if (output is None):\n            try:\n                sharedcontent = SharedContent.objects.parent_site(site).get(slug=slot)\n            except SharedContent.DoesNotExist:\n                return \"<!-- shared content '{0}' does not yet exist -->\".format(slot)\n            if (try_cache and (not cache_key)):\n                cache.set(cache_key_ptr, get_shared_content_cache_key(sharedcontent))\n    if (output is None):\n        output = self.render_shared_content(request, sharedcontent, template_name, cachable=cachable)\n    rendering.register_frontend_media(request, output.media)\n    return output.html\n", "label": 1}
{"function": "\n\ndef __lt__(self, other):\n    if (self.date == 'infinity'):\n        return False\n    if isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return True\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) < other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date < other.date.replace(tzinfo=self.tz))\n        return (self.date < other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            if (other.end.tz and (self.tz is None)):\n                return (self.date.replace(tzinfo=other.end.tz) < other.end.date)\n            elif (self.tz and (other.end.tz is None)):\n                return (self.date < other.end.date.replace(tzinfo=self.tz))\n            return (self.date < other.end.date)\n        else:\n            return self.__lt__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef log(self, sender, message, channel):\n    sender = sender[:10]\n    self.word_table.setdefault(sender, {\n        \n    })\n    if message.startswith('/'):\n        return\n    try:\n        say_something = (self.is_ping(message) or ((sender != self.conn.nick) and (random.random() < self.chattiness)))\n    except AttributeError:\n        say_something = False\n    messages = []\n    seed_key = None\n    if self.is_ping(message):\n        message = self.fix_ping(message)\n    for words in self.split_message(self.sanitize_message(message)):\n        key = tuple(words[:(- 1)])\n        if (key in self.word_table):\n            self.word_table[sender][key].append(words[(- 1)])\n        else:\n            self.word_table[sender][key] = [words[(- 1)]]\n        if ((self.stop_word not in key) and say_something):\n            for person in self.word_table:\n                if (person == sender):\n                    continue\n                if (key in self.word_table[person]):\n                    generated = self.generate_message(person, seed_key=key)\n                    if generated:\n                        messages.append((person, generated))\n    if len(messages):\n        (self.last, message) = random.choice(messages)\n        return message\n", "label": 1}
{"function": "\n\ndef get_line_content(self, patch_content, line_number):\n    content = ''\n    content_list = []\n    lines = patch_content.split('\\n')\n    original_line = 0\n    new_line = 0\n    original_end = 0\n    new_end = 0\n    last_content_added = ''\n    for line in lines:\n        if re.match('^@@(\\\\s|\\\\+|\\\\-|\\\\d|,)+@@', line, re.M):\n            begin = self.get_file_modification_begin(line)\n            original_line = begin[0]\n            new_line = begin[1]\n            end = self.get_file_modification_end(line)\n            deletion_end = end[0]\n            addition_end = end[1]\n        else:\n            if (deletion_end > line_number):\n                if (len(content_list) == 2):\n                    break\n                if ((original_line > line_number) and (new_line > line_number)):\n                    break\n            elif (new_line > line_number):\n                break\n            if re.match('^\\\\+.*', line, re.M):\n                if (new_line == line_number):\n                    content_list.append({\n                        'type': 'addition',\n                        'text': line[1:],\n                    })\n                    last_content_added = 'addition'\n                new_line += 1\n            elif re.match('^\\\\-.*', line, re.M):\n                if (original_line == line_number):\n                    content_list.append({\n                        'type': 'deletion',\n                        'text': line[1:],\n                    })\n                    last_content_added = 'deletion'\n                original_line += 1\n            elif ((line != '\\\\ No newline at end of file') and (line != '')):\n                original_line += 1\n                new_line += 1\n    if content_list:\n        content = content_list[(- 1)]\n    return content\n", "label": 1}
{"function": "\n\ndef execute_plugin_method_series(self, name, args=None, kwargs=None, single_response=False):\n    if (args is None):\n        args = []\n        use_kwargs = True\n    if (kwargs is None):\n        kwargs = {\n            \n        }\n        use_kwargs = False\n    if (use_kwargs and single_response):\n        raise RuntimeError('When executing plugins in series using `single` response mode, you must specify only args.')\n    elif (args and kwargs):\n        raise RuntimeError('Plugins can be ran in series using either args or kwargs, not both.')\n    for plugin in self.plugins:\n        if (not hasattr(plugin, name)):\n            continue\n        method = getattr(plugin, name)\n        plugin_result = method(*args, **kwargs)\n        if (plugin_result is not None):\n            if use_kwargs:\n                kwargs = plugin_result\n            elif single_response:\n                args = (plugin_result,)\n            else:\n                args = plugin_result\n    if use_kwargs:\n        return kwargs\n    elif single_response:\n        return args[0]\n    return args\n", "label": 1}
{"function": "\n\ndef _build_suggestions(request, cat, suggester):\n    results = []\n    q = request.GET.get('q')\n    if (q and (q.isdigit() or (len(q) > 2))):\n        q_ = q.lower()\n        if (cat != 'apps'):\n            for a in amo.APP_USAGE:\n                name_ = unicode(a.pretty).lower()\n                word_matches = [w for w in q_.split() if (name_ in w)]\n                if ((q_ in name_) or word_matches):\n                    results.append({\n                        'id': a.id,\n                        'name': _('{0} Add-ons').format(a.pretty),\n                        'url': locale_url(a.short),\n                        'cls': ('app ' + a.short),\n                    })\n        cats = Category.objects\n        cats = cats.filter((Q(application=request.APP.id) | Q(type=amo.ADDON_SEARCH)))\n        if (cat == 'themes'):\n            cats = cats.filter(type=amo.ADDON_PERSONA)\n        else:\n            cats = cats.exclude(type=amo.ADDON_PERSONA)\n        for c in cats:\n            if (not c.name):\n                continue\n            name_ = unicode(c.name).lower()\n            word_matches = [w for w in q_.split() if (name_ in w)]\n            if ((q_ in name_) or word_matches):\n                results.append({\n                    'id': c.id,\n                    'name': unicode(c.name),\n                    'url': c.get_url_path(),\n                    'cls': 'cat',\n                })\n        results += suggester.items\n    return results\n", "label": 1}
{"function": "\n\ndef visit_Module(self, node):\n    params = node.get_params()\n    for param in params.values():\n        if (param.width is not None):\n            param.width = self.replace_visitor.visit(param.width)\n        param.value = self.replace_visitor.visit(param.value)\n    localparams = node.get_localparams()\n    for localparam in localparams.values():\n        if (localparam.width is not None):\n            localparam.width = self.replace_visitor.visit(localparam.width)\n        localparam.value = self.replace_visitor.visit(localparam.value)\n    ports = node.get_ports()\n    for port in ports.values():\n        if (port.width is not None):\n            port.width = self.replace_visitor.visit(port.width)\n    vars = node.get_vars()\n    for var in vars.values():\n        if (var.width is not None):\n            var.width = self.replace_visitor.visit(var.width)\n    for asg in node.assign:\n        self.visit(asg)\n    for alw in node.always:\n        self.visit(alw)\n    for ini in node.initial:\n        self.visit(ini)\n    for ins in node.instance.values():\n        self.visit(ins)\n    return node\n", "label": 1}
{"function": "\n\ndef test_unix_domain_adapter_monkeypatch():\n    with UnixSocketServerThread() as usock_thread:\n        with requests_unixsocket.monkeypatch('http+unix://'):\n            urlencoded_usock = requests.compat.quote_plus(usock_thread.usock)\n            url = ('http+unix://%s/path/to/page' % urlencoded_usock)\n            for method in ['get', 'post', 'head', 'patch', 'put', 'delete', 'options']:\n                logger.debug('Calling session.%s(%r) ...', method, url)\n                r = getattr(requests, method)(url)\n                logger.debug('Received response: %r with text: %r and headers: %r', r, r.text, r.headers)\n                assert (r.status_code == 200)\n                assert (r.headers['server'] == 'waitress')\n                assert (r.headers['X-Transport'] == 'unix domain socket')\n                assert (r.headers['X-Requested-Path'] == '/path/to/page')\n                assert (r.headers['X-Socket-Path'] == usock_thread.usock)\n                assert isinstance(r.connection, requests_unixsocket.UnixAdapter)\n                assert (r.url == url)\n                if (method == 'head'):\n                    assert (r.text == '')\n                else:\n                    assert (r.text == 'Hello world!')\n    for method in ['get', 'post', 'head', 'patch', 'put', 'delete', 'options']:\n        with pytest.raises(requests.exceptions.InvalidSchema):\n            getattr(requests, method)(url)\n", "label": 1}
{"function": "\n\ndef __getattr__(self, name):\n    if (name in ('_mock_methods', '_mock_unsafe')):\n        raise AttributeError(name)\n    elif (self._mock_methods is not None):\n        if ((name not in self._mock_methods) or (name in _all_magics)):\n            raise AttributeError(('Mock object has no attribute %r' % name))\n    elif _is_magic(name):\n        raise AttributeError(name)\n    if (not self._mock_unsafe):\n        if name.startswith(('assert', 'assret')):\n            raise AttributeError(name)\n    result = self._mock_children.get(name)\n    if (result is _deleted):\n        raise AttributeError(name)\n    elif (result is None):\n        wraps = None\n        if (self._mock_wraps is not None):\n            wraps = getattr(self._mock_wraps, name)\n        result = self._get_child_mock(parent=self, name=name, wraps=wraps, _new_name=name, _new_parent=self)\n        self._mock_children[name] = result\n    elif isinstance(result, _SpecState):\n        result = create_autospec(result.spec, result.spec_set, result.instance, result.parent, result.name)\n        self._mock_children[name] = result\n    return result\n", "label": 1}
{"function": "\n\ndef GetObjectMetadata(self, bucket_name, object_name, generation=None, provider=None, fields=None):\n    'See CloudApi class for function doc strings.'\n    if generation:\n        generation = long(generation)\n    if (bucket_name in self.buckets):\n        bucket = self.buckets[bucket_name]\n        if ((object_name in bucket.objects) and bucket.objects[object_name]):\n            if generation:\n                if ('versioned' in bucket.objects[object_name]):\n                    for obj in bucket.objects[object_name]['versioned']:\n                        if (obj.root_object.generation == generation):\n                            return obj.root_object\n                if ('live' in bucket.objects[object_name]):\n                    if (bucket.objects[object_name]['live'].root_object.generation == generation):\n                        return bucket.objects[object_name]['live'].root_object\n            elif ('live' in bucket.objects[object_name]):\n                return bucket.objects[object_name]['live'].root_object\n        raise CreateObjectNotFoundException(404, self.provider, bucket_name, object_name)\n    raise CreateBucketNotFoundException(404, self.provider, bucket_name)\n", "label": 1}
{"function": "\n\ndef _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):\n    '\\n        Returns context_lines before and after lineno from file.\\n        Returns (pre_context_lineno, pre_context, context_line, post_context).\\n        '\n    source = None\n    if ((loader is not None) and hasattr(loader, 'get_source')):\n        try:\n            source = loader.get_source(module_name)\n        except ImportError:\n            pass\n        if (source is not None):\n            source = source.splitlines()\n    if (source is None):\n        try:\n            with open(filename, 'rb') as fp:\n                source = fp.read().splitlines()\n        except (OSError, IOError):\n            pass\n    if (source is None):\n        return (None, [], None, [])\n    if isinstance(source[0], six.binary_type):\n        encoding = 'ascii'\n        for line in source[:2]:\n            match = re.search(b'coding[:=]\\\\s*([-\\\\w.]+)', line)\n            if match:\n                encoding = match.group(1).decode('ascii')\n                break\n        source = [six.text_type(sline, encoding, 'replace') for sline in source]\n    lower_bound = max(0, (lineno - context_lines))\n    upper_bound = (lineno + context_lines)\n    pre_context = source[lower_bound:lineno]\n    context_line = source[lineno]\n    post_context = source[(lineno + 1):upper_bound]\n    return (lower_bound, pre_context, context_line, post_context)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 0):\n            if (ftype == TType.STRUCT):\n                self.success = Results()\n                self.success.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.error = QueryNotFoundException()\n                self.error.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRUCT):\n                self.error2 = BeeswaxException()\n                self.error2.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef login(self, request):\n    '\\n        Displays the login form for the given HttpRequest.\\n        '\n    from django.contrib.auth.models import User\n    if (not request.POST.has_key(LOGIN_FORM_KEY)):\n        if request.POST:\n            message = _('Please log in again, because your session has expired.')\n        else:\n            message = ''\n        return self.display_login_form(request, message)\n    if (not request.session.test_cookie_worked()):\n        message = _(\"Looks like your browser isn't configured to accept cookies. Please enable cookies, reload this page, and try again.\")\n        return self.display_login_form(request, message)\n    else:\n        request.session.delete_test_cookie()\n    username = request.POST.get('username', None)\n    password = request.POST.get('password', None)\n    user = authenticate(username=username, password=password)\n    if (user is None):\n        message = ERROR_MESSAGE\n        if ((username is not None) and ('@' in username)):\n            try:\n                user = User.objects.get(email=username)\n            except (User.DoesNotExist, User.MultipleObjectsReturned):\n                pass\n            else:\n                if user.check_password(password):\n                    message = (_(\"Your e-mail address is not your username. Try '%s' instead.\") % user.username)\n        return self.display_login_form(request, message)\n    elif (user.is_active and user.is_staff):\n        login(request, user)\n        return http.HttpResponseRedirect(request.get_full_path())\n    else:\n        return self.display_login_form(request, ERROR_MESSAGE)\n", "label": 1}
{"function": "\n\ndef eval_sum(f, limits):\n    from sympy.concrete.delta import deltasummation, _has_simple_delta\n    from sympy.functions import KroneckerDelta\n    (i, a, b) = limits\n    if (f is S.Zero):\n        return S.Zero\n    if (i not in f.free_symbols):\n        return (f * ((b - a) + 1))\n    if (a == b):\n        return f.subs(i, a)\n    if isinstance(f, Piecewise):\n        if (not any(((i in arg.args[1].free_symbols) for arg in f.args))):\n            newargs = []\n            for arg in f.args:\n                newexpr = eval_sum(arg.expr, limits)\n                if (newexpr is None):\n                    return None\n                newargs.append((newexpr, arg.cond))\n            return f.func(*newargs)\n    if (f.has(KroneckerDelta) and _has_simple_delta(f, limits[0])):\n        return deltasummation(f, limits)\n    dif = (b - a)\n    definite = dif.is_Integer\n    if (definite and (dif < 100)):\n        return eval_sum_direct(f, (i, a, b))\n    if isinstance(f, Piecewise):\n        return None\n    value = eval_sum_symbolic(f.expand(), (i, a, b))\n    if (value is not None):\n        return value\n    if definite:\n        return eval_sum_direct(f, (i, a, b))\n", "label": 1}
{"function": "\n\ndef _ContractAperture(self, force=False):\n    \"Attempt to contract the aperture.  By calling this it's assume the aperture\\n    needs to be contracted.\\n\\n    The aperture can be contracted if it's current size is larger than the\\n    min size.\\n    \"\n    if (self._pending_endpoints and (not force)):\n        return\n    num_healthy = len([c for c in self._heap[1:] if c.channel.is_open])\n    if (num_healthy > self._min_size):\n        least_loaded_endpoint = None\n        for n in self._heap[1:]:\n            if (n.channel.is_closed and (n.endpoint not in self._pending_endpoints)):\n                least_loaded_endpoint = n.endpoint\n                break\n        if (not least_loaded_endpoint):\n            for n in self._heap[1:]:\n                if (n.endpoint not in self._pending_endpoints):\n                    least_loaded_endpoint = n.endpoint\n                    break\n        if least_loaded_endpoint:\n            self._idle_endpoints.add(least_loaded_endpoint)\n            super(ApertureBalancerSink, self)._RemoveSink(least_loaded_endpoint)\n            self._log.debug(('Contracting aperture to remove %s' % str(least_loaded_endpoint)))\n            self._UpdateSizeVarz()\n", "label": 1}
{"function": "\n\ndef _parse_pre_yarn_history_log(lines):\n    'Collect useful info from a pre-YARN history file.\\n\\n    See :py:func:`_parse_yarn_history_log` for return format.\\n    '\n    result = {\n        \n    }\n    task_to_counters = {\n        \n    }\n    for record in _parse_pre_yarn_history_records(lines):\n        fields = record['fields']\n        if ((record['type'] == 'Job') and ('COUNTERS' in fields)):\n            result['counters'] = _parse_pre_yarn_counters(fields['COUNTERS'])\n        elif ((record['type'] == 'Task') and ('COUNTERS' in fields) and ('TASKID' in fields)):\n            task_id = fields['TASKID']\n            counters = _parse_pre_yarn_counters(fields['COUNTERS'])\n            task_to_counters[task_id] = counters\n        elif ((record['type'] in ('MapAttempt', 'ReduceAttempt')) and ('TASK_ATTEMPT_ID' in fields) and (fields.get('TASK_STATUS') == 'FAILED') and fields.get('ERROR')):\n            result.setdefault('errors', [])\n            result['errors'].append(dict(hadoop_error=dict(message=fields['ERROR'], start_line=record['start_line'], num_lines=record['num_lines']), attempt_id=fields['TASK_ATTEMPT_ID']))\n    if (('counters' not in result) and task_to_counters):\n        result['counters'] = _sum_counters(*task_to_counters.values())\n    return result\n", "label": 1}
{"function": "\n\ndef _non_dominated_front_old(iterable, key=(lambda x: x), allowequality=True):\n    'Return a subset of items from iterable which are not dominated by any\\n    other item in iterable.'\n    items = list(iterable)\n    keys = dict(((i, key(i)) for i in items))\n    dim = len(list(keys.values())[0])\n    if any(((dim != len(k)) for k in list(keys.values()))):\n        raise ValueError('Wrong tuple size.')\n    dominations = collections.defaultdict((lambda : []))\n    for i in items:\n        for j in items:\n            if allowequality:\n                if all(((keys[i][k] < keys[j][k]) for k in range(dim))):\n                    dominations[i].append(j)\n            elif all(((keys[i][k] <= keys[j][k]) for k in range(dim))):\n                dominations[i].append(j)\n    dominates = (lambda i, j: (j in dominations[i]))\n    res = set()\n    items = set(items)\n    for i in items:\n        res.add(i)\n        for j in list(res):\n            if (i is j):\n                continue\n            if dominates(j, i):\n                res.remove(i)\n                break\n            elif dominates(i, j):\n                res.remove(j)\n    return res\n", "label": 1}
{"function": "\n\n@dispatch((tuple, list, set, frozenset))\ndef discover(seq):\n    if (not seq):\n        return (var * string)\n    unite = do_one([unite_identical, unite_base, unite_merge_dimensions])\n    if (all((isinstance(item, (tuple, list)) for item in seq)) and (len(set(map(len, seq))) == 1)):\n        columns = list(zip(*seq))\n        try:\n            types = [unite([discover(data) for data in column]).subshape[0] for column in columns]\n            unite = do_one([unite_identical, unite_merge_dimensions, Tuple])\n            return (len(seq) * unite(types))\n        except AttributeError:\n            pass\n    if all((isinstance(item, dict) for item in seq)):\n        keys = sorted(set.union(*(set(d) for d in seq)))\n        columns = [[item.get(key) for item in seq] for key in keys]\n        try:\n            types = [unite([discover(data) for data in column]).subshape[0] for column in columns]\n            return (len(seq) * Record(list(zip(keys, types))))\n        except AttributeError:\n            pass\n    types = list(map(discover, seq))\n    return do_one([unite_identical, unite_merge_dimensions, Tuple])(types)\n", "label": 1}
{"function": "\n\ndef get_refs(genome_build, aligner, config):\n    'Retrieve reference genome data from a standard bcbio directory structure.\\n    '\n    ref_collection = tz.get_in(['arvados', 'reference'], config)\n    if (not ref_collection):\n        raise ValueError('Could not find reference collection in bcbio_system YAML for arvados.')\n    cfiles = collection_files(ref_collection, config['arvados'])\n    ref_prefix = None\n    for prefix in ['./%s', './genomes/%s']:\n        cur_prefix = (prefix % genome_build)\n        if any((x.startswith(cur_prefix) for x in cfiles)):\n            ref_prefix = cur_prefix\n            break\n    assert ref_prefix, ('Did not find genome files for %s:\\n%s' % (genome_build, pprint.pformat(cfiles)))\n    out = {\n        \n    }\n    base_targets = (('/%s.fa' % genome_build), '/mainIndex')\n    for dirname in ['seq', 'rtg', aligner]:\n        key = {\n            'seq': 'fasta',\n        }.get(dirname, dirname)\n        cur_files = [x for x in cfiles if x.startswith(('%s/%s/' % (ref_prefix, dirname)))]\n        cur_files = [('keep:%s' % os.path.normpath(os.path.join(ref_collection, x))) for x in cur_files]\n        base_files = [x for x in cur_files if x.endswith(base_targets)]\n        if (len(base_files) > 0):\n            assert (len(base_files) == 1), base_files\n            base_file = base_files[0]\n            del cur_files[cur_files.index(base_file)]\n            out[key] = {\n                'base': base_file,\n                'indexes': cur_files,\n            }\n        else:\n            out[key] = {\n                'indexes': cur_files,\n            }\n    return out\n", "label": 1}
{"function": "\n\ndef make_move(self, coordinates, player):\n    ' Will modify the internal state to represent performing the\\n            specified move for the specified player.\\n        '\n    (x, y) = coordinates\n    moves = [piece.get_position() for piece in self.get_move_pieces(player)]\n    if (coordinates not in moves):\n        raise ValueError\n    if (((x < 0) or (x >= WIDTH)) or ((y < 0) or (y >= HEIGHT))):\n        raise ValueError\n    opponent = get_opponent(player)\n    p = self.pieces[(x + (y * WIDTH))]\n    if (player == WHITE):\n        p.set_white()\n    else:\n        p.set_black()\n    for d in DIRECTIONS:\n        start = ((x + (y * WIDTH)) + d)\n        tile = start\n        to_flip = []\n        if ((tile >= 0) and (tile < (WIDTH * HEIGHT))):\n            while (self.pieces[tile].get_state() != BOARD):\n                to_flip.append(self.pieces[tile])\n                if self.outside_board(tile, d):\n                    break\n                else:\n                    tile += d\n            start_flipping = False\n            for pp in reversed(to_flip):\n                if (not start_flipping):\n                    if (pp.get_state() == opponent):\n                        continue\n                start_flipping = True\n                if (player == WHITE):\n                    pp.set_white()\n                else:\n                    pp.set_black()\n            self.pieces[start].reset_flipped()\n", "label": 1}
{"function": "\n\ndef _hash_internal(method, salt, password):\n    'Internal password hash helper.  Supports plaintext without salt,\\n    unsalted and salted passwords.  In case salted passwords are used\\n    hmac is used.\\n    '\n    if (method == 'plain'):\n        return (password, method)\n    if isinstance(password, text_type):\n        password = password.encode('utf-8')\n    if method.startswith('pbkdf2:'):\n        args = method[7:].split(':')\n        if (len(args) not in (1, 2)):\n            raise ValueError('Invalid number of arguments for PBKDF2')\n        method = args.pop(0)\n        iterations = ((args and int((args[0] or 0))) or DEFAULT_PBKDF2_ITERATIONS)\n        is_pbkdf2 = True\n        actual_method = ('pbkdf2:%s:%d' % (method, iterations))\n    else:\n        is_pbkdf2 = False\n        actual_method = method\n    hash_func = _hash_funcs.get(method)\n    if (hash_func is None):\n        raise TypeError(('invalid method %r' % method))\n    if is_pbkdf2:\n        if (not salt):\n            raise ValueError('Salt is required for PBKDF2')\n        rv = pbkdf2_hex(password, salt, iterations, hashfunc=hash_func)\n    elif salt:\n        if isinstance(salt, text_type):\n            salt = salt.encode('utf-8')\n        rv = hmac.HMAC(salt, password, hash_func).hexdigest()\n    else:\n        h = hash_func()\n        h.update(password)\n        rv = h.hexdigest()\n    return (rv, actual_method)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _get_mixins_(bases):\n    'Returns the type for creating enum members, and the first inherited\\n        enum class.\\n\\n        bases: the tuple of bases that was given to __new__\\n\\n        '\n    if ((not bases) or (Enum is None)):\n        return (object, Enum)\n    member_type = first_enum = None\n    for base in bases:\n        if ((base is not Enum) and issubclass(base, Enum) and base._member_names_):\n            raise TypeError('Cannot extend enumerations')\n    if (not issubclass(base, Enum)):\n        raise TypeError('new enumerations must be created as `ClassName([mixin_type,] enum_type)`')\n    if (not issubclass(bases[0], Enum)):\n        member_type = bases[0]\n        first_enum = bases[(- 1)]\n    else:\n        for base in bases[0].__mro__:\n            if issubclass(base, Enum):\n                if (first_enum is None):\n                    first_enum = base\n            elif (member_type is None):\n                member_type = base\n    return (member_type, first_enum)\n", "label": 1}
{"function": "\n\ndef MoveFiles(rebalance, is_master):\n    'Commit the received files into the database.'\n    loc = data_store.DB.Location()\n    if (not os.path.exists(loc)):\n        return False\n    if (not os.path.isdir(loc)):\n        return False\n    tempdir = _CreateDirectory(loc, rebalance.id)\n    remove_file = _FileWithRemoveList(loc, rebalance)\n    to_remove = []\n    if os.path.exists(remove_file):\n        to_remove = [line.decode('utf8').rstrip('\\n') for line in open(remove_file, 'r')]\n    for fname in to_remove:\n        if (not fname.startswith(loc)):\n            logging.warning('Wrong file to remove: %s', fname)\n            continue\n        if (not os.path.exists(fname)):\n            logging.warning('File does not exist: %s', fname)\n            continue\n        if (not os.path.isfile(fname)):\n            logging.warning('Not a file: %s', fname)\n            continue\n        os.unlink(fname)\n        logging.info('Removing file %s', fname)\n    try:\n        os.unlink(remove_file)\n    except OSError:\n        pass\n    try:\n        _RecMoveFiles(tempdir, loc, '')\n    except OSError:\n        return False\n    if (not is_master):\n        if tempdir.startswith(loc):\n            shutil.rmtree(tempdir)\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, settings_module):\n    for setting in dir(global_settings):\n        if (setting == setting.upper()):\n            setattr(self, setting, getattr(global_settings, setting))\n    self.SETTINGS_MODULE = settings_module\n    try:\n        mod = importlib.import_module(self.SETTINGS_MODULE)\n    except ImportError as e:\n        raise ImportError((\"Could not import settings '%s' (Is it on sys.path?): %s\" % (self.SETTINGS_MODULE, e)))\n    tuple_settings = ('INSTALLED_APPS', 'TEMPLATE_DIRS')\n    for setting in dir(mod):\n        if (setting == setting.upper()):\n            setting_value = getattr(mod, setting)\n            if ((setting in tuple_settings) and isinstance(setting_value, six.string_types)):\n                warnings.warn(('The %s setting must be a tuple. Please fix your settings, as auto-correction is now deprecated.' % setting), PendingDeprecationWarning)\n                setting_value = (setting_value,)\n            setattr(self, setting, setting_value)\n    if (not self.SECRET_KEY):\n        raise ImproperlyConfigured('The SECRET_KEY setting must not be empty.')\n    if (hasattr(time, 'tzset') and self.TIME_ZONE):\n        zoneinfo_root = '/usr/share/zoneinfo'\n        if (os.path.exists(zoneinfo_root) and (not os.path.exists(os.path.join(zoneinfo_root, *self.TIME_ZONE.split('/'))))):\n            raise ValueError(('Incorrect timezone setting: %s' % self.TIME_ZONE))\n        os.environ['TZ'] = self.TIME_ZONE\n        time.tzset()\n", "label": 1}
{"function": "\n\ndef _validate_secure_origin(self, logger, location):\n    parsed = urllib_parse.urlparse(str(location))\n    origin = (parsed.scheme, parsed.hostname, parsed.port)\n    for secure_origin in (SECURE_ORIGINS + self.secure_origins):\n        if ((origin[0] != secure_origin[0]) and (secure_origin[0] != '*')):\n            continue\n        try:\n            addr = ipaddress.ip_address((origin[1] if (isinstance(origin[1], six.text_type) or (origin[1] is None)) else origin[1].decode('utf8')))\n            network = ipaddress.ip_network((secure_origin[1] if isinstance(secure_origin[1], six.text_type) else secure_origin[1].decode('utf8')))\n        except ValueError:\n            if ((origin[1] != secure_origin[1]) and (secure_origin[1] != '*')):\n                continue\n        else:\n            if (addr not in network):\n                continue\n        if ((origin[2] != secure_origin[2]) and (secure_origin[2] != '*') and (secure_origin[2] is not None)):\n            continue\n        return True\n    logger.warning(\"The repository located at %s is not a trusted or secure host and is being ignored. If this repository is available via HTTPS it is recommended to use HTTPS instead, otherwise you may silence this warning and allow it anyways with '--trusted-host %s'.\", parsed.hostname, parsed.hostname)\n    return False\n", "label": 1}
{"function": "\n\n@classmethod\ndef wrap(cls, data):\n    should_save = False\n    if ('original_doc' in data):\n        original_doc = data['original_doc']\n        del data['original_doc']\n        should_save = True\n        if original_doc:\n            original_doc = Domain.get_by_name(original_doc)\n            data['copy_history'] = [original_doc._id]\n    if ('license' in data):\n        if (data.get('license', None) == 'public'):\n            data['license'] = 'cc'\n            should_save = True\n    if (('slug' in data) and data['slug']):\n        data['hr_name'] = data['slug']\n        del data['slug']\n    if (('is_test' in data) and isinstance(data['is_test'], bool)):\n        data['is_test'] = ('true' if data['is_test'] else 'false')\n        should_save = True\n    if ('cloudcare_releases' not in data):\n        data['cloudcare_releases'] = 'nostars'\n    if ('location_types' in data):\n        data['obsolete_location_types'] = data.pop('location_types')\n    self = super(Domain, cls).wrap(data)\n    if (self.deployment is None):\n        self.deployment = Deployment()\n    if should_save:\n        self.save()\n    return self\n", "label": 1}
{"function": "\n\ndef find_description(soup):\n    '\\n    '\n    el = soup.find('h3')\n    if (not el):\n        return\n    while (el.name == 'h3'):\n        next_el = el.findNext('h3')\n        if next_el:\n            el = next_el\n        else:\n            break\n    subtitle = el.findNextSibling('b')\n    if subtitle:\n        el = subtitle\n        (yield el.string)\n        el = el.nextSibling.nextSibling\n    el = el.nextSibling\n    while True:\n        if (el.name is not None):\n            return\n        text = el\n        link = el.findNextSibling('a')\n        if link:\n            text += link.string\n            link = link.nextSibling\n            if (link.name is None):\n                text += link.string\n        if text.strip():\n            (yield unicode(text.strip()))\n        for i in range(2):\n            el = el.nextSibling\n            if (not el):\n                return\n            if (el.name != 'br'):\n                break\n        if (el.name == 'br'):\n            el = el.nextSibling\n        if ('Illustrative Examples:' in el):\n            return\n        if ('Cross-References.' in el):\n            return\n", "label": 1}
{"function": "\n\ndef subscribe(self, subscriber, timeout=None):\n    \"Must be used with 'yield', as, for example,\\n        'yield channel.subscribe(coro)'.\\n\\n        Subscribe to receive messages. Senders don't need to\\n        subscribe. A message sent to this channel is delivered to all\\n        subscribers.\\n\\n        Can also be used on remote channels.\\n        \"\n    if ((not isinstance(subscriber, Coro)) and (not isinstance(subscriber, Channel))):\n        logger.warning('invalid subscriber ignored')\n        raise StopIteration((- 1))\n    if (self._location == Channel._asyncoro._location):\n        if (subscriber._location != self._location):\n            if isinstance(subscriber, Coro):\n                subscriber._id = int(subscriber._id)\n                for s in self._subscribers:\n                    if (isinstance(s, Coro) and (s._id == subscriber._id) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n            elif isinstance(subscriber, Channel):\n                for s in self._subscribers:\n                    if (isinstance(s, Channel) and (s._name == subscriber._name) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n        self._subscribers.add(subscriber)\n        (yield self._subscribe_event.set())\n        reply = 0\n    else:\n        kwargs = {\n            'channel': self._name,\n        }\n        kwargs['subscriber'] = subscriber\n        request = _NetRequest('subscribe', kwargs=kwargs, dst=self._location, timeout=timeout)\n        reply = (yield Channel._asyncoro._sync_reply(request))\n    raise StopIteration(reply)\n", "label": 1}
{"function": "\n\ndef transformations(self, size, target_size):\n    (sw, sh) = size\n    (tw, th) = target_size\n    if ((not self.expand) and (not self.force) and (sw <= tw) and (sh <= th)):\n        return []\n    if (self.force and ((sw <= tw) or (sh <= th))):\n        if ((sw * th) > (sh * tw)):\n            (tw, th) = (((sh * tw) // th), sh)\n        else:\n            (tw, th) = (sw, ((sw * th) // tw))\n    transforms = []\n    if ((sw * th) > (sh * tw)):\n        if ((sh != th) and ((sh > th) or self.expand)):\n            w = ((sw * th) // sh)\n            transforms.append(('resize', (w, th)))\n            (sw, sh) = (w, th)\n        if (sw > tw):\n            wd = ((sw - tw) // 2)\n            transforms.append(('crop', (wd, 0, (tw + wd), sh)))\n    else:\n        if ((sw != tw) and ((sw > tw) or self.expand)):\n            h = ((sh * tw) // sw)\n            transforms.append(('resize', (tw, h)))\n            (sw, sh) = (tw, h)\n        if (sh > th):\n            hd = ((sh - th) // 2)\n            transforms.append(('crop', (0, hd, sw, (th + hd))))\n    return transforms\n", "label": 1}
{"function": "\n\ndef _update_defaults(self, defaults):\n    'Updates the given defaults with values from the config files and\\n        the environ. Does a little special handling for certain types of\\n        options (lists).'\n    config = {\n        \n    }\n    for section in ('global', self.name):\n        config.update(self.normalize_keys(self.get_config_section(section)))\n    if (not self.isolated):\n        config.update(self.normalize_keys(self.get_environ_vars()))\n    self.values = optparse.Values(self.defaults)\n    late_eval = set()\n    for (key, val) in config.items():\n        if (not val):\n            continue\n        option = self.get_option(key)\n        if (option is None):\n            continue\n        if (option.action in ('store_true', 'store_false', 'count')):\n            val = strtobool(val)\n        elif (option.action == 'append'):\n            val = val.split()\n            val = [self.check_default(option, key, v) for v in val]\n        elif (option.action == 'callback'):\n            late_eval.add(option.dest)\n            opt_str = option.get_opt_string()\n            val = option.convert_value(opt_str, val)\n            args = (option.callback_args or ())\n            kwargs = (option.callback_kwargs or {\n                \n            })\n            option.callback(option, opt_str, val, self, *args, **kwargs)\n        else:\n            val = self.check_default(option, key, val)\n        defaults[option.dest] = val\n    for key in late_eval:\n        defaults[key] = getattr(self.values, key)\n    self.values = None\n    return defaults\n", "label": 1}
{"function": "\n\ndef _find_candidate_bbs(self, start_address, end_address, mode=BARF_DISASM_RECURSIVE, symbols=None):\n    if (not symbols):\n        symbols = {\n            \n        }\n    bbs = []\n    addrs_to_process = Queue()\n    addrs_processed = set()\n    addrs_to_process.put(start_address)\n    while (not addrs_to_process.empty()):\n        addr_curr = addrs_to_process.get()\n        if ((addr_curr in addrs_processed) or (not ((addr_curr >= start_address) and (addr_curr <= end_address)))):\n            continue\n        bb = self._disassemble_bb(addr_curr, (end_address + 1), symbols)\n        if bb.empty():\n            continue\n        bbs += [bb]\n        addrs_processed.add(addr_curr)\n        if (mode == BARF_DISASM_LINEAR):\n            next_addr = (bb.address + bb.size)\n            if ((not self._bb_ends_in_direct_jmp(bb)) and (not self._bb_ends_in_return(bb)) and (not (next_addr in addrs_processed))):\n                addrs_to_process.put(next_addr)\n        if (mode == BARF_DISASM_RECURSIVE):\n            for (addr, _) in bb.branches:\n                if (not (addr in addrs_processed)):\n                    addrs_to_process.put(addr)\n    return bbs\n", "label": 1}
{"function": "\n\ndef slowloris(self, host, num, timeout, port=None):\n    port = (port or 80)\n    timeout = int(timeout)\n    conns = [Conn(host, int(port), 5) for i in range(int(num))]\n    failed = 0\n    packets = 0\n    while (not self.stop_flag.is_set()):\n        for conn in conns:\n            if self.stop_flag.is_set():\n                break\n            if (not conn.connected):\n                if conn.connect():\n                    packets += 3\n            if conn.connected:\n                query = ('?%d' % random.randint(1, 9999999999999))\n                payload = (self.primary_payload % (query, conn.host))\n                try:\n                    conn.send(payload)\n                    packets += 1\n                except socket.error:\n                    pass\n            else:\n                pass\n        for conn in conns:\n            if self.stop_flag.is_set():\n                break\n            if conn.connected:\n                try:\n                    conn.send('X-a: b\\r\\n')\n                    packets += 1\n                except socket.error:\n                    pass\n        gevent.sleep(timeout)\n    return ('%s failed, %s packets sent' % (failed, packets))\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (len(self.jid_) != len(x.jid_)):\n        return 0\n    for (e1, e2) in zip(self.jid_, x.jid_):\n        if (e1 != e2):\n            return 0\n    if (self.has_body_ != x.has_body_):\n        return 0\n    if (self.has_body_ and (self.body_ != x.body_)):\n        return 0\n    if (self.has_raw_xml_ != x.has_raw_xml_):\n        return 0\n    if (self.has_raw_xml_ and (self.raw_xml_ != x.raw_xml_)):\n        return 0\n    if (self.has_type_ != x.has_type_):\n        return 0\n    if (self.has_type_ and (self.type_ != x.type_)):\n        return 0\n    if (self.has_from_jid_ != x.has_from_jid_):\n        return 0\n    if (self.has_from_jid_ and (self.from_jid_ != x.from_jid_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef process_packets(self, transaction_id=None, invoked_method=None, timeout=None):\n    start = time()\n    while (self.connected and (transaction_id not in self._invoke_results)):\n        if (timeout and ((time() - start) >= timeout)):\n            raise RTMPTimeoutError('Timeout')\n        packet = self.read_packet()\n        if (packet.type == PACKET_TYPE_INVOKE):\n            try:\n                decoded = decode_amf(packet.body)\n            except IOError:\n                continue\n            try:\n                (method, transaction_id_, obj) = decoded[:3]\n                args = decoded[3:]\n            except ValueError:\n                continue\n            if (method == '_result'):\n                if (len(args) > 0):\n                    result = args[0]\n                else:\n                    result = None\n                self._invoke_results[transaction_id_] = result\n            else:\n                handler = self._invoke_handlers.get(method)\n                if handler:\n                    res = handler(*args)\n                    if (res is not None):\n                        self.call('_result', res, transaction_id=transaction_id_)\n                if (method == invoked_method):\n                    self._invoke_args[invoked_method] = args\n                    break\n            if (transaction_id_ == 1.0):\n                self._connect_result = packet\n            else:\n                self.handle_packet(packet)\n        else:\n            self.handle_packet(packet)\n    if transaction_id:\n        result = self._invoke_results.pop(transaction_id, None)\n        return result\n    if invoked_method:\n        args = self._invoke_args.pop(invoked_method, None)\n        return args\n", "label": 1}
{"function": "\n\ndef CKY(pcfg, norm_words):\n    (x, n) = (([('', '')] + norm_words), len(norm_words))\n    pi = defaultdict(float)\n    bp = defaultdict(tuple)\n    for i in range(1, (n + 1)):\n        for X in pcfg.N:\n            (norm, word) = x[i]\n            if ((X, norm) in pcfg.q1):\n                pi[(i, i, X)] = pcfg.q1[(X, norm)]\n                bp[(i, i, X)] = (X, word, i, i)\n    for l in range(1, n):\n        for i in range(1, ((n - l) + 1)):\n            j = (i + l)\n            for X in pcfg.N:\n                (score, back) = argmax([(((pcfg.q2[(X, Y, Z)] * pi[(i, s, Y)]) * pi[((s + 1), j, Z)]), (X, Y, Z, i, s, j)) for s in range(i, j) for (Y, Z) in pcfg.binary_rules[X] if (pi[(i, s, Y)] > 0.0) if (pi[((s + 1), j, Z)] > 0.0)])\n                if (score > 0.0):\n                    (bp[(i, j, X)], pi[(i, j, X)]) = (back, score)\n    (_, top) = max([(pi[(1, n, X)], bp[(1, n, X)]) for X in pcfg.N])\n    return backtrace(top, bp)\n", "label": 1}
{"function": "\n\n@requires_search\ndef update_user(user, index=None):\n    index = (index or INDEX)\n    if (not user.is_active):\n        try:\n            es.delete(index=index, doc_type='user', id=user._id, refresh=True, ignore=[404])\n        except NotFoundError:\n            pass\n        return\n    names = dict(fullname=user.fullname, given_name=user.given_name, family_name=user.family_name, middle_names=user.middle_names, suffix=user.suffix)\n    normalized_names = {\n        \n    }\n    for (key, val) in names.items():\n        if (val is not None):\n            try:\n                val = six.u(val)\n            except TypeError:\n                pass\n            normalized_names[key] = unicodedata.normalize('NFKD', val).encode('ascii', 'ignore')\n    user_doc = {\n        'id': user._id,\n        'user': user.fullname,\n        'normalized_user': normalized_names['fullname'],\n        'normalized_names': normalized_names,\n        'names': names,\n        'job': (user.jobs[0]['institution'] if user.jobs else ''),\n        'job_title': (user.jobs[0]['title'] if user.jobs else ''),\n        'all_jobs': [job['institution'] for job in user.jobs[1:]],\n        'school': (user.schools[0]['institution'] if user.schools else ''),\n        'all_schools': [school['institution'] for school in user.schools],\n        'category': 'user',\n        'degree': (user.schools[0]['degree'] if user.schools else ''),\n        'social': user.social_links,\n        'boost': 2,\n    }\n    es.index(index=index, doc_type='user', body=user_doc, id=user._id, refresh=True)\n", "label": 1}
{"function": "\n\n@flake8ext\ndef check_builtins_gettext(logical_line, tokens, filename, lines, noqa):\n    \"Check usage of builtins gettext _().\\n\\n    Okay(neutron/foo.py): from neutron._i18n import _\\n_('foo')\\n    N341(neutron/foo.py): _('foo')\\n    Okay(neutron/_i18n.py): _('foo')\\n    Okay(neutron/i18n.py): _('foo')\\n    Okay(neutron/foo.py): _('foo')  # noqa\\n    \"\n    if noqa:\n        return\n    modulename = os.path.normpath(filename).split('/')[0]\n    if (('%s/tests' % modulename) in filename):\n        return\n    if (os.path.basename(filename) in ('i18n.py', '_i18n.py')):\n        return\n    token_values = [t[1] for t in tokens]\n    i18n_wrapper = ('%s._i18n' % modulename)\n    if ('_' in token_values):\n        i18n_import_line_found = False\n        for line in lines:\n            split_line = [elm.rstrip(',') for elm in line.split()]\n            if ((len(split_line) > 1) and (split_line[0] == 'from') and (split_line[1] == i18n_wrapper) and ('_' in split_line)):\n                i18n_import_line_found = True\n                break\n        if (not i18n_import_line_found):\n            msg = ('N341: _ from python builtins module is used. Use _ from %s instead.' % i18n_wrapper)\n            (yield (0, msg))\n", "label": 1}
{"function": "\n\ndef pythonvaluetotime(time_val):\n    'Convert a time or time range from Python datetime to ArcGIS REST server'\n    if (time_val is None):\n        return None\n    elif isinstance(time_val, numeric):\n        return str(long((time_val * 1000.0)))\n    elif isinstance(time_val, date):\n        dtlist = [time_val.year, time_val.month, time_val.day]\n        if isinstance(time_val, datetime.datetime):\n            dtlist += [time_val.hour, time_val.minute, time_val.second]\n        else:\n            dtlist += [0, 0, 0]\n        return long((calendar.timegm(dtlist) * 1000.0))\n    elif (isinstance(time_val, sequence) and (len(time_val) == 2)):\n        if all((isinstance(x, numeric) for x in time_val)):\n            return ','.join((pythonvaluetotime(x) for x in time_val))\n        elif all((isinstance(x, date) for x in time_val)):\n            return ','.join((pythonvaluetotime(x) for x in time_val))\n    raise ValueError(repr(time_val))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.query_id = QueryHandle()\n                self.query_id.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.BOOL):\n                self.start_over = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I32):\n                self.fetch_size = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef versions_from_expanded_variables(variables, tag_prefix, verbose=False):\n    refnames = variables['refnames'].strip()\n    if refnames.startswith('$Format'):\n        if verbose:\n            print('variables are unexpanded, not using')\n        return {\n            \n        }\n    refs = set([r.strip() for r in refnames.strip('()').split(',')])\n    TAG = 'tag: '\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if (not tags):\n        tags = set([r for r in refs if re.search('\\\\d', r)])\n        if verbose:\n            print((\"discarding '%s', no digits\" % ','.join((refs - tags))))\n    if verbose:\n        print(('likely tags: %s' % ','.join(sorted(tags))))\n    for ref in sorted(tags):\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(('picking %s' % r))\n            return {\n                'version': r,\n                'full': variables['full'].strip(),\n            }\n    if verbose:\n        print('no suitable tags, using full revision id')\n    return {\n        'version': variables['full'].strip(),\n        'full': variables['full'].strip(),\n    }\n", "label": 1}
{"function": "\n\ndef step(self, forward, keep):\n    index = self.current_index\n    matches = len(self.regions)\n    if (self.regions and ((index < 0) or ((index == 0) and (not forward)) or ((index == (matches - 1)) and forward))):\n        index = (0 if forward else (matches - 1))\n        if (self.try_wrapped or (not self.regions)):\n            wrapped = True\n            self.try_wrapped = False\n        else:\n            self.try_wrapped = True\n            return None\n    elif ((forward and (index < (matches - 1))) or ((not forward) and (index > 0))):\n        index = ((index + 1) if forward else (index - 1))\n        wrapped = self.wrapped\n    else:\n        return None\n    selected = copy(self.selected)\n    if ((not keep) and (len(selected) > 0)):\n        del selected[(- 1)]\n    return ISearchInfo.StackItem(self.search, self.regions, selected, index, forward, wrapped)\n", "label": 1}
{"function": "\n\ndef convert_to_dict(obj, ident=0, limit_ident=6):\n    ident += 1\n    if (type(obj) in primitive):\n        return obj\n    if (isinstance(obj, inspect.types.InstanceType) or (type(obj) not in (list, tuple, dict))):\n        if (ident <= limit_ident):\n            try:\n                obj = obj.convert_to_dict()\n            except AttributeError:\n                try:\n                    t = obj.__dict__\n                    t['_type_class'] = str(obj.__class__)\n                    obj = t\n                except AttributeError:\n                    return str((obj.__class__ if hasattr(obj, '__class__') else type(obj)))\n        else:\n            return str((obj.__class__ if hasattr(obj, '__class__') else type(obj)))\n    if (type(obj) is dict):\n        res = {\n            \n        }\n        for item in obj:\n            if (ident <= limit_ident):\n                res[item] = convert_to_dict(obj[item], ident)\n            else:\n                res[item] = str(obj[item])\n        return res\n    if (type(obj) in (list, tuple)):\n        res = []\n        for item in obj:\n            if (ident <= limit_ident):\n                res.append(convert_to_dict(item, ident))\n            else:\n                res.append(str(item))\n        return (res if (type(obj) is list) else tuple(res))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.finish = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.BOOL):\n                self.reversed = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef message_attr(self):\n    '\\n        Returns options dict as sent within WAMP messages.\\n        '\n    options = {\n        \n    }\n    if (self.acknowledge is not None):\n        options['acknowledge'] = self.acknowledge\n    if (self.exclude_me is not None):\n        options['exclude_me'] = self.exclude_me\n    if (self.exclude is not None):\n        options['exclude'] = (self.exclude if (type(self.exclude) == list) else [self.exclude])\n    if (self.exclude_authid is not None):\n        options['exclude_authid'] = (self.exclude_authid if (type(self.exclude_authid) == list) else self.exclude_authid)\n    if (self.exclude_authrole is not None):\n        options['exclude_authrole'] = (self.exclude_authrole if (type(self.exclude_authrole) == list) else self.exclude_authrole)\n    if (self.eligible is not None):\n        options['eligible'] = (self.eligible if (type(self.eligible) == list) else self.eligible)\n    if (self.eligible_authid is not None):\n        options['eligible_authid'] = (self.eligible_authid if (type(self.eligible_authid) == list) else self.eligible_authid)\n    if (self.eligible_authrole is not None):\n        options['eligible_authrole'] = (self.eligible_authrole if (type(self.eligible_authrole) == list) else self.eligible_authrole)\n    return options\n", "label": 1}
{"function": "\n\ndef _search_ds(self, method, *args, **kwargs):\n    'Searches the datastore for a file.'\n    ds_path = kwargs.get('datastorePath')\n    matched_files = set()\n    directory = False\n    dname = ('%s/' % ds_path)\n    for file in _db_content.get('files'):\n        if (file == dname):\n            directory = True\n            break\n    if directory:\n        for file in _db_content.get('files'):\n            if (file.find(ds_path) != (- 1)):\n                if (not file.endswith(ds_path)):\n                    path = file.replace(dname, '', 1).split('/')\n                    if path:\n                        matched_files.add(path[0])\n        if (not matched_files):\n            matched_files.add('/')\n    else:\n        for file in _db_content.get('files'):\n            if (file.find(ds_path) != (- 1)):\n                matched_files.add(ds_path)\n    if matched_files:\n        result = DataObject()\n        result.path = ds_path\n        result.file = []\n        for file in matched_files:\n            matched = DataObject()\n            matched.path = file\n            matched.fileSize = 1024\n            result.file.append(matched)\n        task_mdo = create_task(method, 'success', result=result)\n    else:\n        task_mdo = create_task(method, 'error', error_fault=FileNotFound())\n    return task_mdo.obj\n", "label": 1}
{"function": "\n\ndef test_manage_session2():\n    conf = test_config()\n    loop = pyuv.Loop.default_loop()\n    with KeyManager(loop, conf) as h:\n        h.create_key({\n            'manage': ['test', 'test1'],\n        }, key='test1')\n        key = Key.load(h.get_key('test1'))\n        assert (key.can_manage_all() == False)\n        assert (key.can_read_all() == False)\n        assert (key.can_write_all() == False)\n        assert (key.can_read('test') == True)\n        assert (key.can_read('test.test') == True)\n        assert (key.can_read('test1') == True)\n        assert (key.can_read('test1.test') == True)\n        assert (key.can_write('test') == True)\n        assert (key.can_write('test.test') == True)\n        assert (key.can_write('test1') == True)\n        assert (key.can_write('test1.test') == True)\n        assert (key.can_manage('test') == True)\n        assert (key.can_manage('test.test') == True)\n        assert (key.can_manage('test1') == True)\n        assert (key.can_manage('test1.test') == True)\n", "label": 1}
{"function": "\n\ndef django_ordering_comparison(ordering, lhs, rhs):\n    if (not ordering):\n        return (- 1)\n    ASCENDING = 1\n    DESCENDING = 2\n    for (order, direction) in ordering:\n        if (lhs is not None):\n            lhs_value = (lhs.key() if (order == '__key__') else lhs.get(order))\n        else:\n            lhs_value = None\n        if (rhs is not None):\n            rhs_value = (rhs.key() if (order == '__key__') else rhs.get(order))\n        else:\n            rhs_value = None\n        if ((direction == ASCENDING) and (lhs_value != rhs_value)):\n            return ((- 1) if lt(lhs_value, rhs_value) else 1)\n        elif ((direction == DESCENDING) and (lhs_value != rhs_value)):\n            return (1 if lt(lhs_value, rhs_value) else (- 1))\n    return 0\n", "label": 1}
{"function": "\n\ndef _replace_heap(variable, heap):\n    if isinstance(variable, Pointer):\n        while isinstance(variable, Pointer):\n            if (variable.index == 0):\n                variable = None\n            elif (variable.index in heap):\n                variable = heap[variable.index]\n            else:\n                warnings.warn('Variable referenced by pointer not found in heap: variable will be set to None')\n                variable = None\n        (replace, new) = _replace_heap(variable, heap)\n        if replace:\n            variable = new\n        return (True, variable)\n    elif isinstance(variable, np.core.records.recarray):\n        for (ir, record) in enumerate(variable):\n            (replace, new) = _replace_heap(record, heap)\n            if replace:\n                variable[ir] = new\n        return (False, variable)\n    elif isinstance(variable, np.core.records.record):\n        for (iv, value) in enumerate(variable):\n            (replace, new) = _replace_heap(value, heap)\n            if replace:\n                variable[iv] = new\n        return (False, variable)\n    elif isinstance(variable, np.ndarray):\n        if (variable.dtype.type is np.object_):\n            for iv in range(variable.size):\n                (replace, new) = _replace_heap(variable.item(iv), heap)\n                if replace:\n                    variable.itemset(iv, new)\n        return (False, variable)\n    else:\n        return (False, variable)\n", "label": 1}
{"function": "\n\ndef __init__(self, toklist, name=None, asList=True, modal=True, isinstance=isinstance):\n    if self.__doinit:\n        self.__doinit = False\n        self.__name = None\n        self.__parent = None\n        self.__accumNames = {\n            \n        }\n        if isinstance(toklist, list):\n            self.__toklist = toklist[:]\n        elif isinstance(toklist, _generatorType):\n            self.__toklist = list(toklist)\n        else:\n            self.__toklist = [toklist]\n        self.__tokdict = dict()\n    if ((name is not None) and name):\n        if (not modal):\n            self.__accumNames[name] = 0\n        if isinstance(name, int):\n            name = _ustr(name)\n        self.__name = name\n        if (not (isinstance(toklist, (type(None), basestring, list)) and (toklist in (None, '', [])))):\n            if isinstance(toklist, basestring):\n                toklist = [toklist]\n            if asList:\n                if isinstance(toklist, ParseResults):\n                    self[name] = _ParseResultsWithOffset(toklist.copy(), 0)\n                else:\n                    self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]), 0)\n                self[name].__name = name\n            else:\n                try:\n                    self[name] = toklist[0]\n                except (KeyError, TypeError, IndexError):\n                    self[name] = toklist\n", "label": 1}
{"function": "\n\ndef get_value(self, var, cast=None, default=NOTSET, parse_default=False):\n    'Return value for given environment variable.\\n\\n        :param var: Name of variable.\\n        :param cast: Type to cast return value as.\\n        :param default: If var not present in environ, return this instead.\\n        :param parse_default: force to parse default..\\n\\n        :returns: Value from environment or default (if set)\\n        '\n    logger.debug(\"get '{0}' casted as '{1}' with default '{2}'\".format(var, cast, default))\n    if (var in self.scheme):\n        var_info = self.scheme[var]\n        try:\n            has_default = (len(var_info) == 2)\n        except TypeError:\n            has_default = False\n        if has_default:\n            if (not cast):\n                cast = var_info[0]\n            if (default is self.NOTSET):\n                try:\n                    default = var_info[1]\n                except IndexError:\n                    pass\n        elif (not cast):\n            cast = var_info\n    try:\n        value = self.ENVIRON[var]\n    except KeyError:\n        if (default is self.NOTSET):\n            error_msg = 'Set the {0} environment variable'.format(var)\n            raise ImproperlyConfigured(error_msg)\n        value = default\n    if (hasattr(value, 'startswith') and value.startswith('$')):\n        value = value.lstrip('$')\n        value = self.get_value(value, cast=cast, default=default)\n    if ((value != default) or (parse_default and value)):\n        value = self.parse_value(value, cast)\n    return value\n", "label": 1}
{"function": "\n\ndef _HandleImports(self, element, import_manager=None):\n    'Handles imports for the specified element.\\n\\n    Args:\\n      element: (Property|Parameter) The property we want to set the import for.\\n      import_manager: The import manager to import into if not the implied one.\\n\\n    '\n    element = getattr(element, 'referenced_schema', element)\n    if isinstance(element, data_types.ComplexDataType):\n        parent = element\n        data_type = element\n    else:\n        parent = element.schema\n        data_type = element.data_type\n    data_type = getattr(data_type, 'referenced_schema', data_type)\n    if (not import_manager):\n        if self._InnerModelClassesSupported():\n            while parent.parent:\n                parent = parent.parent\n        import_manager = cpp_import_manager.CppImportManager.ForElement(parent)\n    while isinstance(data_type, (data_types.ArrayDataType, data_types.MapDataType)):\n        data_type = data_type._base_type\n        data_type = getattr(data_type, 'referenced_schema', data_type)\n    if (not data_type):\n        return\n    json_type = data_type.json_type\n    json_format = data_type.values.get('format')\n    if (json_type == 'object'):\n        if (not data_type.parent):\n            import_manager.AddDataType(data_type)\n        return\n    datatype_and_imports = self.language_model.type_map.get((json_type, json_format))\n    if datatype_and_imports:\n        import_definition = datatype_and_imports[1]\n        for required_import in import_definition.imports:\n            if required_import:\n                import_manager.AddImport(required_import)\n        for template_value in import_definition.template_values:\n            element.SetTemplateValue(template_value, True)\n    elif data_type:\n        if (not data_type.parent):\n            import_manager.AddDataType(data_type)\n    return\n", "label": 1}
{"function": "\n\ndef __setattr__(self, name, value):\n    if ((name in self.__dict__.keys()) or ('_BaseAWSObject__initialized' not in self.__dict__)):\n        return dict.__setattr__(self, name, value)\n    elif (name in self.attributes):\n        self.resource[name] = value\n        return None\n    elif (name in self.propnames):\n        expected_type = self.props[name][0]\n        if isinstance(value, AWSHelperFn):\n            return self.properties.__setitem__(name, value)\n        elif isinstance(expected_type, types.FunctionType):\n            try:\n                value = expected_type(value)\n            except:\n                sys.stderr.write((\"%s: %s.%s function validator '%s' threw exception:\\n\" % (self.__class__, self.title, name, expected_type.__name__)))\n                raise\n            return self.properties.__setitem__(name, value)\n        elif isinstance(expected_type, list):\n            if (not isinstance(value, list)):\n                self._raise_type(name, value, expected_type)\n            for v in value:\n                if ((not isinstance(v, tuple(expected_type))) and (not isinstance(v, AWSHelperFn))):\n                    self._raise_type(name, v, expected_type)\n            return self.properties.__setitem__(name, value)\n        elif isinstance(value, expected_type):\n            return self.properties.__setitem__(name, value)\n        else:\n            self._raise_type(name, value, expected_type)\n    type_name = getattr(self, 'resource_type', self.__class__.__name__)\n    if ((type_name == 'AWS::CloudFormation::CustomResource') or type_name.startswith('Custom::')):\n        return self.properties.__setitem__(name, value)\n    raise AttributeError(('%s object does not support attribute %s' % (type_name, name)))\n", "label": 1}
{"function": "\n\ndef test_video_fromname_episode(episodes):\n    video = Video.fromname(episodes['bbt_s07e05'].name)\n    assert (type(video) is Episode)\n    assert (video.name == episodes['bbt_s07e05'].name)\n    assert (video.format == episodes['bbt_s07e05'].format)\n    assert (video.release_group == episodes['bbt_s07e05'].release_group)\n    assert (video.resolution == episodes['bbt_s07e05'].resolution)\n    assert (video.video_codec == episodes['bbt_s07e05'].video_codec)\n    assert (video.audio_codec is None)\n    assert (video.imdb_id is None)\n    assert (video.hashes == {\n        \n    })\n    assert (video.size is None)\n    assert (video.subtitle_languages == set())\n    assert (video.series == episodes['bbt_s07e05'].series)\n    assert (video.season == episodes['bbt_s07e05'].season)\n    assert (video.episode == episodes['bbt_s07e05'].episode)\n    assert (video.title is None)\n    assert (video.year is None)\n    assert (video.tvdb_id is None)\n", "label": 1}
{"function": "\n\ndef lookup_allowed(self, lookup, value):\n    model = self.model\n    for l in model._meta.related_fkey_lookups:\n        for (k, v) in widgets.url_params_from_lookup_dict(l).items():\n            if ((k == lookup) and (v == value)):\n                return True\n    parts = lookup.split(LOOKUP_SEP)\n    if ((len(parts) > 1) and (parts[(- 1)] in QUERY_TERMS)):\n        parts.pop()\n    pk_attr_name = None\n    for part in parts[:(- 1)]:\n        (field, _, _, _) = model._meta.get_field_by_name(part)\n        if hasattr(field, 'rel'):\n            model = field.rel.to\n            pk_attr_name = model._meta.pk.name\n        elif isinstance(field, RelatedObject):\n            model = field.model\n            pk_attr_name = model._meta.pk.name\n        else:\n            pk_attr_name = None\n    if (pk_attr_name and (len(parts) > 1) and (parts[(- 1)] == pk_attr_name)):\n        parts.pop()\n    try:\n        self.model._meta.get_field_by_name(parts[0])\n    except FieldDoesNotExist:\n        return True\n    else:\n        if (len(parts) == 1):\n            return True\n        clean_lookup = LOOKUP_SEP.join(parts)\n        return ((clean_lookup in self.list_filter) or (clean_lookup == self.date_hierarchy))\n", "label": 1}
{"function": "\n\n@get('/s/users/{user_id}')\ndef get_user_route(request, user_id):\n    '\\n    Get the user by their ID.\\n    '\n    db_conn = request['db_conn']\n    user = User.get(db_conn, id=user_id)\n    current_user = get_current_user(request)\n    if (not user):\n        return abort(404)\n    data = {\n        \n    }\n    data['user'] = user.deliver(access=('private' if (current_user and (user['id'] == current_user['id'])) else None))\n    if ('posts' in request['params']):\n        data['posts'] = [post.deliver() for post in get_posts_facade(db_conn, user_id=user['id'])]\n    if (('sets' in request['params']) and (user['settings']['view_sets'] == 'public')):\n        u_sets = UserSets.get(db_conn, user_id=user['id'])\n        data['sets'] = [set_.deliver() for set_ in u_sets.list_sets(db_conn)]\n    if (('follows' in request['params']) and (user['settings']['view_follows'] == 'public')):\n        data['follows'] = [follow.deliver() for follow in Follow.list(db_conn, user_id=user['id'])]\n    if ('avatar' in request['params']):\n        size = int(request['params']['avatar'])\n        data['avatar'] = user.get_avatar((size if size else None))\n    return (200, data)\n", "label": 1}
{"function": "\n\ndef uninstall(pkg, package_name, remove_all, app_id, cli, app):\n    'Uninstalls a package.\\n\\n    :param pkg: package manager to uninstall with\\n    :type pkg: PackageManager\\n    :param package_name: The package to uninstall\\n    :type package_name: str\\n    :param remove_all: Whether to remove all instances of the named app\\n    :type remove_all: boolean\\n    :param app_id: App ID of the app instance to uninstall\\n    :type app_id: str\\n    :param init_client: The program to use to run the app\\n    :type init_client: object\\n    :rtype: None\\n    '\n    if ((cli is False) and (app is False)):\n        cli = app = True\n    uninstalled = False\n    installed = installed_packages(pkg, app_id, package_name)\n    installed_cli = next((True for installed_pkg in installed if installed_pkg.get('command')), False)\n    installed_app = next((True for installed_pkg in installed if installed_pkg.get('apps')), False)\n    if (cli and installed_cli):\n        if subcommand.uninstall(package_name):\n            uninstalled = True\n    if (app and installed_app):\n        if pkg.uninstall_app(package_name, remove_all, app_id):\n            uninstalled = True\n    if uninstalled:\n        return None\n    else:\n        msg = 'Package [{}]'.format(package_name)\n        if (app_id is not None):\n            app_id = util.normalize_app_id(app_id)\n            msg += ' with id [{}]'.format(app_id)\n        msg += ' is not installed'\n        raise DCOSException(msg)\n", "label": 1}
{"function": "\n\ndef add_module(self, parent, kwargs=dict()):\n    ' Add the target module to the given object.\\n        '\n    if (parent is not None):\n        module_manager = get_module_manager(parent)\n        if ((module_manager is not None) and (len(module_manager.children) > 0)):\n            scalar_lut = module_manager.scalar_lut_manager\n            vector_lut = module_manager.vector_lut_manager\n            if ('vmin' in kwargs):\n                if ((not scalar_lut.use_default_range) and (kwargs['vmin'] != scalar_lut.data_range[0])):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n                elif ((not scalar_lut.use_default_range) and (kwargs['vmin'] != scalar_lut.data_range[0])):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n            elif ('vmax' in kwargs):\n                if ((not scalar_lut.use_default_range) and (kwargs['vmax'] != scalar_lut.data_range[1])):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n                elif ((not scalar_lut.use_default_range) and (kwargs['vmax'] != scalar_lut.data_range[1])):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n            elif ('colormap' in kwargs):\n                cmap = kwargs['colormap']\n                if ((scalar_lut.lut_mode != cmap) or (vector_lut.lut_mode != cmap)):\n                    parent = self._engine.add_module(ModuleManager(), module_manager.parent)\n    self._engine.add_module(self._target, obj=parent)\n", "label": 1}
{"function": "\n\ndef execute_ls_command(db, objs, tags, options, credentials, unixPaths=None):\n    long_ = (options.long or options.group or options.longest)\n    if (len(tags) == 0):\n        tags = [(('/' if db.unixStyle else '') + db.credentials.username)]\n    for tag in tags:\n        fulltag = db.abs_tag_path(tag, inPref=True)\n        if (options.namespace or options.ns):\n            if db.ns_exists(fulltag):\n                if (long_ or options.longer or options.longest):\n                    nsResult = db.full_perms((fulltag[1:] + '/'), options.longer, options.group, options.longest)\n                else:\n                    nsResult = fulltag\n                db.Print(nsResult)\n            else:\n                nsResult = 'Error status 404'\n        else:\n            nsResult = db.list_sorted_ns(fulltag[1:], long_=long_, recurse=options.recurse, prnt=True, longer=options.longer, longest=options.longest)\n        tagExists = db.tag_exists(fulltag)\n        if (nsResult == 'Error status 404'):\n            if (not tagExists):\n                db.Print(('%s not found' % fulltag))\n        if tagExists:\n            if (long_ or options.longer or options.longest):\n                db.Print(db.full_perms(fulltag[1:], options.longer, options.group, options.longest))\n            else:\n                db.Print(tag)\n", "label": 1}
{"function": "\n\ndef get_filters(self, request):\n    lookup_params = self.get_filters_params()\n    use_distinct = False\n    for (key, value) in lookup_params.items():\n        if (not self.model_admin.lookup_allowed(key, value)):\n            raise DisallowedModelAdminLookup(('Filtering by %s not allowed' % key))\n    filter_specs = []\n    if self.list_filter:\n        for list_filter in self.list_filter:\n            if callable(list_filter):\n                spec = list_filter(request, lookup_params, self.model, self.model_admin)\n            else:\n                field_path = None\n                if isinstance(list_filter, (tuple, list)):\n                    (field, field_list_filter_class) = list_filter\n                else:\n                    (field, field_list_filter_class) = (list_filter, FieldListFilter.create)\n                if (not isinstance(field, models.Field)):\n                    field_path = field\n                    field = get_fields_from_path(self.model, field_path)[(- 1)]\n                spec = field_list_filter_class(field, request, lookup_params, self.model, self.model_admin, field_path=field_path)\n                use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, field_path))\n            if (spec and spec.has_output()):\n                filter_specs.append(spec)\n    try:\n        for (key, value) in lookup_params.items():\n            lookup_params[key] = prepare_lookup_value(key, value)\n            use_distinct = (use_distinct or lookup_needs_distinct(self.lookup_opts, key))\n        return (filter_specs, bool(filter_specs), lookup_params, use_distinct)\n    except FieldDoesNotExist as e:\n        six.reraise(IncorrectLookupParameters, IncorrectLookupParameters(e), sys.exc_info()[2])\n", "label": 1}
{"function": "\n\ndef run(self, group=(- 1), index=(- 1), close_type='single', unsaved_prompt=True, close_unsaved=True):\n    'Close the specified tabs and cleanup sticky states.'\n    TabsExtraListener.extra_command_call = True\n    if ((group >= 0) and (index >= 0)):\n        self.init(close_type, group, index)\n        if (len(self.targets) and (not unsaved_prompt) and (not sublime.ok_cancel_dialog('Are you sure you want to dismiss all targeted unsaved buffers?'))):\n            return\n        for s in self.targets:\n            if SHEET_WORKAROUND:\n                self.window.focus_sheet(s)\n                v = self.window.active_view()\n            else:\n                v = s.view()\n            if (v is not None):\n                if ((not v.settings().get('tabs_extra_sticky', False)) or (close_type == 'single')):\n                    if (not self.persistent):\n                        v.settings().erase('tabs_extra_sticky')\n                    self.window.focus_view(v)\n                    if ((not v.is_dirty()) or close_unsaved):\n                        if (not unsaved_prompt):\n                            v.set_scratch(True)\n                        self.window.run_command('close_file')\n                elif (not self.persistent):\n                    v.settings().erase('tabs_extra_sticky')\n            else:\n                self.window.run_command('close_file')\n        if ((not self.persistent) and self.cleanup):\n            self.window.run_command('tabs_extra_clear_all_sticky', {\n                'group': group,\n            })\n        self.select_view()\n    TabsExtraListener.extra_command_call = False\n", "label": 1}
{"function": "\n\ndef test_update_crossing_duration1_not_duration_not_at_1st_step(self, duration1, duration2):\n    global rec\n    if ((duration1 == 0.0) or (duration2 == 0.0)):\n        return\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    composite = ac.sequence(a1, a2)\n    rec = []\n    node.do(composite)\n    elapsed = 0.0\n    next_elapsed = (duration1 / 2.0)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    elapsed = next_elapsed\n    next_elapsed = ((duration1 + duration2) / 2.0)\n    dt = (next_elapsed - elapsed)\n    rec = []\n    node._step(dt)\n    recx = [e for e in rec if (e[1] != 'step')]\n    rec = [e for e in recx if (e[0] == name1)]\n    print('rec', rec)\n    assert ((rec[0][1] == 'update') and (rec[0][2] == 1.0))\n    assert (rec[1][1] == 'stop')\n    assert (len(rec) == 2)\n    rec = [e for e in recx if (e[0] == name2)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert (rec[1][1] == 'update')\n    assert (abs((rec[1][2] - ((next_elapsed - duration1) / duration2))) < fe)\n    assert (len(rec) == 2)\n", "label": 1}
{"function": "\n\ndef predict(self, data, **kwargs):\n    '\\n        Used in the predict phase, after training.  Override\\n        '\n    voice_scripts = []\n    for i in xrange(0, data.shape[0]):\n        script_lines = data['script'][i].split('\\n')\n        voice_lines = []\n        current_line = ''\n        for (i, line) in enumerate(script_lines):\n            current_line = current_line.strip()\n            line = line.strip()\n            if (line.startswith('[') and line.endswith(']')):\n                continue\n            if line.startswith('-'):\n                continue\n            voice_line = re.search('\\\\w+:', line)\n            if (voice_line is not None):\n                if self.check_for_line_split(current_line):\n                    voice_lines.append(current_line)\n                current_line = line\n            elif (((len(line) == 0) or line.startswith('-')) and (len(current_line) > 0)):\n                if self.check_for_line_split(current_line):\n                    voice_lines.append(current_line)\n                current_line = ''\n                voice_lines.append(' ')\n            elif (len(current_line) > 0):\n                current_line += (' ' + line)\n        script_text = '\\n'.join([l for l in voice_lines if ((len(l) > 0) and ('{' not in l) and ('=' not in l))])\n        script_text = re.sub('\\\\[.+\\\\]', '', script_text)\n        voice_scripts.append(script_text.strip())\n    data['voice_script'] = voice_scripts\n    return data\n", "label": 1}
{"function": "\n\ndef handle(self, *args, **options):\n    tenant_data = {\n        \n    }\n    for field in self.tenant_fields:\n        input_value = options.get(field.name, None)\n        tenant_data[field.name] = input_value\n    domain_data = {\n        \n    }\n    for field in self.domain_fields:\n        input_value = options.get(field.name, None)\n        domain_data[field.name] = input_value\n    clone_schema_from = options.get('clone_from')\n    while ((clone_schema_from == '') or (clone_schema_from is None)):\n        clone_schema_from = input(force_str('Clone schema from: '))\n    tenant = None\n    while True:\n        for field in self.tenant_fields:\n            if (tenant_data.get(field.name, '') == ''):\n                input_msg = field.verbose_name\n                default = field.get_default()\n                if default:\n                    input_msg = (\"%s (leave blank to use '%s')\" % (input_msg, default))\n                input_value = (input(force_str(('%s: ' % input_msg))) or default)\n                tenant_data[field.name] = input_value\n        tenant = self.store_tenant(clone_schema_from, **tenant_data)\n        if (tenant is not None):\n            break\n        tenant_data = {\n            \n        }\n    while True:\n        domain_data['tenant'] = tenant\n        for field in self.domain_fields:\n            if (domain_data.get(field.name, '') == ''):\n                input_msg = field.verbose_name\n                default = field.get_default()\n                if default:\n                    input_msg = (\"%s (leave blank to use '%s')\" % (input_msg, default))\n                input_value = (input(force_str(('%s: ' % input_msg))) or default)\n                domain_data[field.name] = input_value\n        domain = self.store_tenant_domain(**domain_data)\n        if (domain is not None):\n            break\n        domain_data = {\n            \n        }\n", "label": 1}
{"function": "\n\ndef _inverse_binarize_thresholding(y, output_type, classes, threshold):\n    'Inverse label binarization transformation using thresholding.'\n    if ((output_type == 'binary') and (y.ndim == 2) and (y.shape[1] > 2)):\n        raise ValueError(\"output_type='binary', but y.shape = {0}\".format(y.shape))\n    if ((output_type != 'binary') and (y.shape[1] != len(classes))):\n        raise ValueError('The number of class is not equal to the number of dimension of y.')\n    classes = np.asarray(classes)\n    if sp.issparse(y):\n        if (threshold > 0):\n            if (y.format not in ('csr', 'csc')):\n                y = y.tocsr()\n            y.data = np.array((y.data > threshold), dtype=np.int)\n            y.eliminate_zeros()\n        else:\n            y = np.array((y.toarray() > threshold), dtype=np.int)\n    else:\n        y = np.array((y > threshold), dtype=np.int)\n    if (output_type == 'binary'):\n        if sp.issparse(y):\n            y = y.toarray()\n        if ((y.ndim == 2) and (y.shape[1] == 2)):\n            return classes[y[:, 1]]\n        elif (len(classes) == 1):\n            return np.repeat(classes[0], len(y))\n        else:\n            return classes[y.ravel()]\n    elif (output_type == 'multilabel-indicator'):\n        return y\n    else:\n        raise ValueError('{0} format is not supported'.format(output_type))\n", "label": 1}
{"function": "\n\ndef test_episode_fromname(episodes):\n    video = Episode.fromname(episodes['bbt_s07e05'].name)\n    assert (video.name == episodes['bbt_s07e05'].name)\n    assert (video.format == episodes['bbt_s07e05'].format)\n    assert (video.release_group == episodes['bbt_s07e05'].release_group)\n    assert (video.resolution == episodes['bbt_s07e05'].resolution)\n    assert (video.video_codec == episodes['bbt_s07e05'].video_codec)\n    assert (video.audio_codec is None)\n    assert (video.imdb_id is None)\n    assert (video.hashes == {\n        \n    })\n    assert (video.size is None)\n    assert (video.subtitle_languages == set())\n    assert (video.series == episodes['bbt_s07e05'].series)\n    assert (video.season == episodes['bbt_s07e05'].season)\n    assert (video.episode == episodes['bbt_s07e05'].episode)\n    assert (video.title is None)\n    assert (video.year is None)\n    assert (video.tvdb_id is None)\n", "label": 1}
{"function": "\n\ndef _eval_herm_antiherm(self, real):\n    one_nc = zero = one_neither = False\n    for t in self.args:\n        if (not t.is_commutative):\n            if one_nc:\n                return\n            one_nc = True\n        if t.is_antihermitian:\n            real = (not real)\n        elif t.is_hermitian:\n            if (not zero):\n                z = t.is_zero\n                if ((not z) and (zero is False)):\n                    zero = z\n                elif z:\n                    if all((a.is_finite for a in self.args)):\n                        return True\n                    return\n        elif (t.is_hermitian is False):\n            if one_neither:\n                return\n            one_neither = True\n        else:\n            return\n    if one_neither:\n        if real:\n            return zero\n    elif ((zero is False) or real):\n        return real\n", "label": 1}
{"function": "\n\ndef walk(self, N=64, freq_base=A2, freq_max=C8, start=0, end=None, each=True, combine=False, twin=0.02, thop=0.01):\n    ''\n    N = int(N)\n    assert (N > 0)\n    samplerate = self.audio.samplerate\n    assert (1 <= freq_base <= freq_max <= (samplerate / 2.0))\n    step = 1024\n    win = step\n    assert (0 < step <= win)\n    coeffies = self.make_erb_filter_coeffiences(samplerate, N, freq_base, freq_max)\n    zi = None\n    cstep = int(np.ceil((thop * samplerate)))\n    cwin = int(np.ceil((twin * samplerate)))\n    Y = np.zeros((0, N))\n    if combine:\n        assert (0 < thop <= twin)\n        assert (0 < cstep <= cwin)\n    for samples in self.audio.walk(win, step, start, end, join_channels=True):\n        (y, zi) = self.filter(samples, coeffies, zi)\n        if (not combine):\n            if each:\n                for frame in y:\n                    (yield frame)\n            else:\n                (yield y)\n        else:\n            Y = np.append(Y, y, 0)\n            while (Y.shape[0] >= cwin):\n                (wf, Y) = (Y[:cwin], Y[cstep:])\n                (yield np.sqrt(np.mean(np.square(wf), 0)))\n    if (combine and (Y.shape[0] > 0)):\n        (yield np.sqrt(np.mean(np.square(Y), 0)))\n", "label": 1}
{"function": "\n\ndef updateCall(call_node):\n    max_len = 0\n    for argument in call_node:\n        if (argument.type == 'argument_generator_comprehension'):\n            return\n        if (hasattr(argument, 'target') and (argument.target is not None)):\n            key = argument.target.value\n        else:\n            key = None\n        if (key is not None):\n            max_len = max(max_len, len(key))\n    if ('\\n' not in call_node.second_formatting.dumps()):\n        del call_node.second_formatting[:]\n        del call_node.third_formatting[:]\n    for argument in call_node:\n        if (hasattr(argument, 'target') and (argument.target is not None)):\n            key = argument.target.value\n        else:\n            key = None\n        if (key is not None):\n            if (not argument.second_formatting):\n                argument.second_formatting = ' '\n            if ('\\n' in str(call_node.second_formatting)):\n                if (len(argument.first_formatting) > 0):\n                    spacing = argument.first_formatting[0].value\n                else:\n                    spacing = ''\n                if ((len(key) + len(spacing)) != (max_len + 1)):\n                    argument.first_formatting = (' ' * ((max_len - len(key)) + 1))\n            else:\n                argument.first_formatting = ' '\n        elif ('\\n' not in str(call_node.second_formatting)):\n            if (argument.value.type in ('string', 'binary_string', 'raw_string')):\n                argument.value.second_formatting = ''\n", "label": 1}
{"function": "\n\ndef __getitem__(self, lst):\n    if (isinstance(lst, tuple) and (len(lst) < 5) and any(((Ellipsis is x) for x in lst))):\n        if ((len(lst) == 2) and (lst[1] is Ellipsis)):\n            return enumFrom(lst[0])\n        elif ((len(lst) == 3) and (lst[2] is Ellipsis)):\n            return enumFromThen(lst[0], lst[1])\n        elif ((len(lst) == 3) and (lst[1] is Ellipsis)):\n            return enumFromTo(lst[0], lst[2])\n        elif ((len(lst) == 4) and (lst[2] is Ellipsis)):\n            return enumFromThenTo(lst[0], lst[1], lst[3])\n        raise SyntaxError(('Invalid list comprehension: %s' % str(lst)))\n    elif (hasattr(lst, 'next') or hasattr(lst, '__next__')):\n        return List(tail=lst)\n    return List(head=lst)\n", "label": 1}
{"function": "\n\ndef parse_f(self, args):\n    if ((len(self.tex_coords) > 1) and (len(self.normals) == 1)):\n        raise PywavefrontException('Found texture coordinates, but no normals')\n    if (self.mesh is None):\n        self.mesh = mesh.Mesh()\n        self.wavefront.add_mesh(self.mesh)\n    if (self.material is None):\n        self.material = material.Material()\n    self.mesh.add_material(self.material)\n    v1 = None\n    vlast = None\n    points = []\n    for (i, v) in enumerate(args[0:]):\n        if (type(v) is bytes):\n            v = v.decode()\n        (v_index, t_index, n_index) = (list(map(int, [(j or 0) for j in v.split('/')])) + [0, 0])[:3]\n        if (v_index < 0):\n            v_index += (len(self.vertices) - 1)\n        if (t_index < 0):\n            t_index += (len(self.tex_coords) - 1)\n        if (n_index < 0):\n            n_index += (len(self.normals) - 1)\n        vertex = ((list(self.tex_coords[t_index]) + list(self.normals[n_index])) + list(self.vertices[v_index]))\n        if (i >= 3):\n            self.material.vertices += (v1 + vlast)\n        self.material.vertices += vertex\n        if (i == 0):\n            v1 = vertex\n        vlast = vertex\n", "label": 1}
{"function": "\n\ndef model_fields(model, fields=None, readonly_fields=None, exclude=None, field_args=None, converter=None):\n    '\\n    Generate a dictionary of WTForms fields for a given MongoEngine model.\\n\\n    See `model_form` docstring for description of parameters.\\n    '\n    from mongoengine.base import BaseDocument\n    if (BaseDocument not in inspect.getmro(model)):\n        raise TypeError('Model must be a MongoEngine Document schema')\n    readonly_fields = (readonly_fields or [])\n    exclude = (exclude or [])\n    converter = (converter or ModelConverter())\n    field_args = (field_args or {\n        \n    })\n    field_names = (fields if fields else model._fields.keys())\n    field_names = (x for x in field_names if (x not in exclude))\n    field_dict = {\n        \n    }\n    for name in field_names:\n        if ((name not in readonly_fields) and (name not in model._fields)):\n            raise KeyError(('\"%s\" is not read-only and does not appear to be a field on the document.' % name))\n        if ((name in model._fields) and (name not in readonly_fields)):\n            model_field = model._fields[name]\n            field = converter.convert(model, model_field, field_args.get(name))\n            if (field is not None):\n                field_dict[name] = field\n    return field_dict\n", "label": 1}
{"function": "\n\ndef process_urlencoded(entity):\n    'Read application/x-www-form-urlencoded data into entity.params.'\n    qs = entity.fp.read()\n    for charset in entity.attempt_charsets:\n        try:\n            params = {\n                \n            }\n            for aparam in qs.split(ntob('&')):\n                for pair in aparam.split(ntob(';')):\n                    if (not pair):\n                        continue\n                    atoms = pair.split(ntob('='), 1)\n                    if (len(atoms) == 1):\n                        atoms.append(ntob(''))\n                    key = unquote_plus(atoms[0]).decode(charset)\n                    value = unquote_plus(atoms[1]).decode(charset)\n                    if (key in params):\n                        if (not isinstance(params[key], list)):\n                            params[key] = [params[key]]\n                        params[key].append(value)\n                    else:\n                        params[key] = value\n        except UnicodeDecodeError:\n            pass\n        else:\n            entity.charset = charset\n            break\n    else:\n        raise cherrypy.HTTPError(400, ('The request entity could not be decoded. The following charsets were attempted: %s' % repr(entity.attempt_charsets)))\n    for (key, value) in params.items():\n        if (key in entity.params):\n            if (not isinstance(entity.params[key], list)):\n                entity.params[key] = [entity.params[key]]\n            entity.params[key].append(value)\n        else:\n            entity.params[key] = value\n", "label": 1}
{"function": "\n\ndef entity_matches_query(entity, query):\n    '\\n        Return True if the entity would potentially be returned by the datastore\\n        query\\n    '\n    OPERATORS = {\n        '=': (lambda x, y: (x == y)),\n        '<': lt,\n        '>': gt,\n        '<=': lte,\n        '>=': gte,\n    }\n    queries = [query]\n    if isinstance(query, datastore.MultiQuery):\n        raise CouldBeSupportedError(\"We just need to separate the multiquery into 'queries' then everything should work\")\n    for query in queries:\n        comparisons = chain([('_Query__kind', '=', '_Query__kind')], [tuple((x.split(' ') + [x])) for x in query.keys()])\n        for (ent_attr, op, query_attr) in comparisons:\n            if (ent_attr == '__key__'):\n                continue\n            op = OPERATORS[op]\n            if (ent_attr == '_Query__kind'):\n                ent_attr = entity.kind()\n            else:\n                ent_attr = entity.get(ent_attr)\n            if callable(ent_attr):\n                ent_attr = ent_attr()\n            if (not isinstance(query_attr, (list, tuple))):\n                query_attrs = [query_attr]\n            else:\n                query_attrs = query_attr\n            query_attrs = ((getattr(query, x) if (x == '_Query__kind') else query.get(x)) for x in query_attrs)\n            if (not isinstance(ent_attr, (list, tuple))):\n                ent_attr = [ent_attr]\n            matches = False\n            for query_attr in query_attrs:\n                if (not any((op(attr, query_attr) for attr in ent_attr))):\n                    matches = False\n                    break\n            else:\n                matches = True\n            if (not matches):\n                break\n        else:\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef unknown_endtag(self, tag):\n    if (tag.find(':') != (- 1)):\n        (prefix, suffix) = tag.split(':', 1)\n    else:\n        (prefix, suffix) = ('', tag)\n    prefix = self.namespacemap.get(prefix, prefix)\n    if prefix:\n        prefix = (prefix + '_')\n    if ((suffix == 'svg') and self.svgOK):\n        self.svgOK -= 1\n    methodname = (('_end_' + prefix) + suffix)\n    try:\n        if self.svgOK:\n            raise AttributeError()\n        method = getattr(self, methodname)\n        method()\n    except AttributeError:\n        self.pop((prefix + suffix))\n    if (self.incontent and (not self.contentparams.get('type', 'xml').endswith('xml'))):\n        if (tag in ('xhtml:div', 'div')):\n            return\n        self.contentparams['type'] = 'application/xhtml+xml'\n    if (self.incontent and (self.contentparams.get('type') == 'application/xhtml+xml')):\n        tag = tag.split(':')[(- 1)]\n        self.handle_data(('</%s>' % tag), escape=0)\n    if self.basestack:\n        self.basestack.pop()\n        if (self.basestack and self.basestack[(- 1)]):\n            self.baseuri = self.basestack[(- 1)]\n    if self.langstack:\n        self.langstack.pop()\n        if self.langstack:\n            self.lang = self.langstack[(- 1)]\n    self.depth -= 1\n", "label": 1}
{"function": "\n\ndef find(self, path):\n    '\\n        Generate filenames in path that satisfy criteria specified in\\n        the constructor.\\n        This method is a generator and should be repeatedly called\\n        until there are no more results.\\n        '\n    for (dirpath, dirs, files) in os.walk(path):\n        depth = dirpath[(len(path) + len(os.path.sep)):].count(os.path.sep)\n        if (depth >= self.mindepth):\n            for name in (dirs + files):\n                fstat = None\n                matches = True\n                fullpath = None\n                for criterion in self.criteria:\n                    if ((fstat is None) and (criterion.requires() & _REQUIRES_STAT)):\n                        fullpath = os.path.join(dirpath, name)\n                        fstat = os.stat(fullpath)\n                    if (not criterion.match(dirpath, name, fstat)):\n                        matches = False\n                        break\n                if matches:\n                    if (fullpath is None):\n                        fullpath = os.path.join(dirpath, name)\n                    for action in self.actions:\n                        if ((fstat is None) and (action.requires() & _REQUIRES_STAT)):\n                            fstat = os.stat(fullpath)\n                        result = action.execute(fullpath, fstat, test=self.test)\n                        if (result is not None):\n                            (yield result)\n        if (depth == self.maxdepth):\n            dirs[:] = []\n", "label": 1}
{"function": "\n\ndef _qname_matches(tag, namespace, qname):\n    \"Logic determines if a QName matches the desired local tag and namespace.\\n  \\n  This is used in XmlElement.get_elements and XmlElement.get_attributes to\\n  find matches in the element's members (among all expected-and-unexpected\\n  elements-and-attributes).\\n  \\n  Args:\\n    expected_tag: string\\n    expected_namespace: string\\n    qname: string in the form '{xml_namespace}localtag' or 'tag' if there is\\n           no namespace.\\n  \\n  Returns:\\n    boolean True if the member's tag and namespace fit the expected tag and\\n    namespace.\\n  \"\n    if (qname is None):\n        member_tag = None\n        member_namespace = None\n    elif qname.startswith('{'):\n        member_namespace = qname[1:qname.index('}')]\n        member_tag = qname[(qname.index('}') + 1):]\n    else:\n        member_namespace = None\n        member_tag = qname\n    return (((tag is None) and (namespace is None)) or ((namespace is None) and (member_tag == tag)) or ((tag is None) and (member_namespace == namespace)) or ((tag is None) and (namespace == '') and (member_namespace is None)) or ((tag == member_tag) and (namespace == member_namespace)) or ((tag == member_tag) and (namespace == '') and (member_namespace is None)))\n", "label": 1}
{"function": "\n\ndef get_instances(name, lifecycle_state='InService', health_status='Healthy', attribute='private_ip_address', attributes=None, region=None, key=None, keyid=None, profile=None):\n    '\\n    return attribute of all instances in the named autoscale group.\\n\\n    CLI example::\\n\\n        salt-call boto_asg.get_instances my_autoscale_group_name\\n\\n    '\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    ec2_conn = _get_ec2_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        asgs = conn.get_all_groups(names=[name])\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        return False\n    if (len(asgs) != 1):\n        log.debug(\"name '{0}' returns multiple ASGs: {1}\".format(name, [asg.name for asg in asgs]))\n        return False\n    asg = asgs[0]\n    instance_ids = []\n    for i in asg.instances:\n        if ((lifecycle_state is not None) and (i.lifecycle_state != lifecycle_state)):\n            continue\n        if ((health_status is not None) and (i.health_status != health_status)):\n            continue\n        instance_ids.append(i.instance_id)\n    instances = ec2_conn.get_only_instances(instance_ids=instance_ids)\n    if attributes:\n        return [[getattr(instance, attr).encode('ascii') for attr in attributes] for instance in instances]\n    else:\n        return [getattr(instance, attribute).encode('ascii') for instance in instances if getattr(instance, attribute)]\n    return [getattr(instance, attribute).encode('ascii') for instance in instances]\n", "label": 1}
{"function": "\n\ndef RC_calc(ctx, x, y, r, pv=True):\n    if (not (ctx.isnormal(x) and ctx.isnormal(y))):\n        if (ctx.isinf(x) or ctx.isinf(y)):\n            return (1 / (x * y))\n        if (y == 0):\n            return ctx.inf\n        if (x == 0):\n            return ((ctx.pi / ctx.sqrt(y)) / 2)\n        raise ValueError\n    if (pv and (ctx._im(y) == 0) and (ctx._re(y) < 0)):\n        return (ctx.sqrt((x / (x - y))) * RC_calc(ctx, (x - y), (- y), r))\n    if (x == y):\n        return (1 / ctx.sqrt(x))\n    extraprec = (2 * max(0, ((- ctx.mag((x - y))) + ctx.mag(x))))\n    ctx.prec += extraprec\n    if (ctx._is_real_type(x) and ctx._is_real_type(y)):\n        x = ctx._re(x)\n        y = ctx._re(y)\n        a = ctx.sqrt((x / y))\n        if (x < y):\n            b = ctx.sqrt((y - x))\n            v = (ctx.acos(a) / b)\n        else:\n            b = ctx.sqrt((x - y))\n            v = (ctx.acosh(a) / b)\n    else:\n        sx = ctx.sqrt(x)\n        sy = ctx.sqrt(y)\n        v = (ctx.acos((sx / sy)) / (ctx.sqrt((1 - (x / y))) * sy))\n    ctx.prec -= extraprec\n    return v\n", "label": 1}
{"function": "\n\ndef execute(self, fullpath, fstat, test=False):\n    result = []\n    for arg in self.fmt:\n        if (arg == 'path'):\n            result.append(fullpath)\n        elif (arg == 'name'):\n            result.append(os.path.basename(fullpath))\n        elif (arg == 'size'):\n            result.append(fstat[stat.ST_SIZE])\n        elif (arg == 'type'):\n            result.append(_FILE_TYPES.get(stat.S_IFMT(fstat[stat.ST_MODE]), '?'))\n        elif (arg == 'mode'):\n            result.append(int(oct(fstat[stat.ST_MODE])[(- 3):]))\n        elif (arg == 'mtime'):\n            result.append(fstat[stat.ST_MTIME])\n        elif (arg == 'user'):\n            uid = fstat[stat.ST_UID]\n            try:\n                result.append(pwd.getpwuid(uid).pw_name)\n            except KeyError:\n                result.append(uid)\n        elif (arg == 'group'):\n            gid = fstat[stat.ST_GID]\n            try:\n                result.append(grp.getgrgid(gid).gr_name)\n            except KeyError:\n                result.append(gid)\n        elif (arg == 'md5'):\n            if stat.S_ISREG(fstat[stat.ST_MODE]):\n                md5digest = salt.utils.get_hash(fullpath, 'md5')\n                result.append(md5digest)\n            else:\n                result.append('')\n    if (len(result) == 1):\n        return result[0]\n    else:\n        return result\n", "label": 1}
{"function": "\n\ndef _ApplyBarChartStyle(self, chart):\n    'If bar style is specified, fill in the missing data and apply it.'\n    if ((chart.style is None) or (not chart.data)):\n        return {\n            \n        }\n    (bar_thickness, bar_gap, group_gap) = (chart.style.bar_thickness, chart.style.bar_gap, chart.style.group_gap)\n    if ((bar_gap is None) and (group_gap is not None)):\n        bar_gap = max(0, (group_gap / 2))\n        if (not chart.style.use_fractional_gap_spacing):\n            bar_gap = int(bar_gap)\n    if ((group_gap is None) and (bar_gap is not None)):\n        group_gap = max(0, (bar_gap * 2))\n    if (bar_thickness is None):\n        if chart.style.use_fractional_gap_spacing:\n            bar_thickness = 'r'\n        else:\n            bar_thickness = 'a'\n    elif chart.style.use_fractional_gap_spacing:\n        if bar_gap:\n            bar_gap = int((bar_thickness * bar_gap))\n        if group_gap:\n            group_gap = int((bar_thickness * group_gap))\n    spec = [bar_thickness]\n    if (bar_gap is not None):\n        spec.append(bar_gap)\n        if ((group_gap is not None) and (not chart.stacked)):\n            spec.append(group_gap)\n    return util.JoinLists(bar_size=spec)\n", "label": 1}
{"function": "\n\ndef __new__(cls, latitude=None, longitude=None, altitude=None):\n    single_arg = ((longitude is None) and (altitude is None))\n    if (single_arg and (not isinstance(latitude, util.NUMBER_TYPES))):\n        arg = latitude\n        if (arg is None):\n            pass\n        elif isinstance(arg, Point):\n            return cls.from_point(arg)\n        elif isinstance(arg, basestring):\n            return cls.from_string(arg)\n        else:\n            try:\n                seq = iter(arg)\n            except TypeError:\n                raise TypeError(('Failed to create Point instance from %r.' % (arg,)))\n            else:\n                return cls.from_sequence(seq)\n    latitude = float((latitude or 0))\n    if (abs(latitude) > 90):\n        raise ValueError(('Latitude out of range [-90, 90]: %r' % latitude))\n    longitude = float((longitude or 0))\n    if (abs(longitude) > 180):\n        raise ValueError(('Longitude out of range [-180, 180]: %r' % longitude))\n    altitude = float((altitude or 0))\n    self = super(Point, cls).__new__(cls)\n    self.latitude = latitude\n    self.longitude = longitude\n    self.altitude = altitude\n    return self\n", "label": 1}
{"function": "\n\ndef GetArtifacts(self, os_name=None, name_list=None, source_type=None, exclude_dependents=False, provides=None, reload_datastore_artifacts=False):\n    'Retrieve artifact classes with optional filtering.\\n\\n    All filters must match for the artifact to be returned.\\n\\n    Args:\\n      os_name: string to match against supported_os\\n      name_list: list of strings to match against artifact names\\n      source_type: rdf_artifacts.ArtifactSource.SourceType to match against\\n                      source_type\\n      exclude_dependents: if true only artifacts with no dependencies will be\\n                          returned\\n      provides: return the artifacts that provide these dependencies\\n      reload_datastore_artifacts: If true, the data store sources are queried\\n                                  for new artifacts.\\n    Returns:\\n      set of artifacts matching filter criteria\\n    '\n    self._CheckDirty(reload_datastore_artifacts=reload_datastore_artifacts)\n    results = set()\n    for artifact in self._artifacts.itervalues():\n        if (os_name and artifact.supported_os and (os_name not in artifact.supported_os)):\n            continue\n        if (name_list and (artifact.name not in name_list)):\n            continue\n        if source_type:\n            source_types = [c.type for c in artifact.sources]\n            if (source_type not in source_types):\n                continue\n        if (exclude_dependents and artifact.GetArtifactPathDependencies()):\n            continue\n        if provides:\n            for provide_string in artifact.provides:\n                if (provide_string in provides):\n                    results.add(artifact)\n                    continue\n            continue\n        results.add(artifact)\n    return results\n", "label": 1}
{"function": "\n\ndef identifyConceptsUsed(self):\n    self.relationshipSets = [(arcrole, ELR, linkqname, arcqname) for (arcrole, ELR, linkqname, arcqname) in self.modelXbrl.baseSets.keys() if (ELR and (arcrole.startswith('XBRL-') or (linkqname and arcqname)))]\n    conceptsUsed = set((f.qname for f in self.modelXbrl.factsInInstance))\n    for cntx in self.modelXbrl.contexts.values():\n        for dim in cntx.qnameDims.values():\n            conceptsUsed.add(dim.dimensionQname)\n            if dim.isExplicit:\n                conceptsUsed.add(dim.memberQname)\n            else:\n                conceptsUsed.add(dim.typedMember.qname)\n    for (defaultDim, defaultDimMember) in self.modelXbrl.qnameDimensionDefaults.items():\n        conceptsUsed.add(defaultDim)\n        conceptsUsed.add(defaultDimMember)\n    for relationshipSetKey in self.relationshipSets:\n        relationshipSet = self.modelXbrl.relationshipSet(*relationshipSetKey)\n        for rel in relationshipSet.modelRelationships:\n            if isinstance(rel.fromModelObject, ModelConcept):\n                conceptsUsed.add(rel.fromModelObject)\n            if isinstance(rel.toModelObject, ModelConcept):\n                conceptsUsed.add(rel.toModelObject)\n    for qn in (XbrlConst.qnXbrliIdentifier, XbrlConst.qnXbrliPeriod, XbrlConst.qnXbrliUnit):\n        conceptsUsed.add(self.modelXbrl.qnameConcepts[qn])\n    conceptsUsed -= {None}\n    self.conceptsUsed = conceptsUsed\n", "label": 1}
{"function": "\n\ndef _sync_author_detail(self, key='author'):\n    context = self._getContext()\n    detail = context.get(('%ss' % key), [FeedParserDict()])[(- 1)]\n    if detail:\n        name = detail.get('name')\n        email = detail.get('email')\n        if (name and email):\n            context[key] = ('%s (%s)' % (name, email))\n        elif name:\n            context[key] = name\n        elif email:\n            context[key] = email\n    else:\n        (author, email) = (context.get(key), None)\n        if (not author):\n            return\n        emailmatch = re.search('(([a-zA-Z0-9\\\\_\\\\-\\\\.\\\\+]+)@((\\\\[[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.)|(([a-zA-Z0-9\\\\-]+\\\\.)+))([a-zA-Z]{2,4}|[0-9]{1,3})(\\\\]?))(\\\\?subject=\\\\S+)?', author)\n        if emailmatch:\n            email = emailmatch.group(0)\n            author = author.replace(email, '')\n            author = author.replace('()', '')\n            author = author.replace('<>', '')\n            author = author.replace('&lt;&gt;', '')\n            author = author.strip()\n            if (author and (author[0] == '(')):\n                author = author[1:]\n            if (author and (author[(- 1)] == ')')):\n                author = author[:(- 1)]\n            author = author.strip()\n        if (author or email):\n            context.setdefault(('%s_detail' % key), detail)\n        if author:\n            detail['name'] = author\n        if email:\n            detail['email'] = email\n", "label": 1}
{"function": "\n\ndef solve_TLE(self, board):\n    '\\n\\n        :param board: a 9x9 2D array\\n        :return: Boolean\\n        '\n    n = len(board)\n    if all([(board[(i / n)][(i % n)] != '.') for i in xrange((n * n))]):\n        return True\n    for i in xrange(n):\n        for j in xrange(n):\n            if (board[i][j] == '.'):\n                for num in range(1, 10):\n                    num_str = str(num)\n                    condition_row = all([(board[i][col] != num_str) for col in xrange(n)])\n                    condition_col = all([(board[row][j] != num_str) for row in xrange(n)])\n                    condition_square = all([(board[(((i / 3) * 3) + (count / 3))][(((j / 3) * 3) + (count % 3))] != num_str) for count in xrange(n)])\n                    if (condition_col and condition_row and condition_square):\n                        board[i][j] = num_str\n                        if (not self.solve(board)):\n                            board[i][j] = '.'\n                        else:\n                            return True\n    return False\n", "label": 1}
{"function": "\n\ndef _make_cfg_defaults(self, module_name=NotGiven, default_distribution=NotGiven, guess_maintainer=NotGiven):\n    defaults = {\n        \n    }\n    default_re = re.compile('^.* \\\\(Default: (.*)\\\\)$')\n    for (longopt, shortopt, description) in stdeb_cfg_options:\n        assert longopt.endswith('=')\n        assert (longopt.lower() == longopt)\n        key = longopt[:(- 1)]\n        matchobj = default_re.search(description)\n        if (matchobj is not None):\n            groups = matchobj.groups()\n            assert (len(groups) == 1)\n            value = groups[0]\n            if (value == '<source-debianized-setup-name>'):\n                assert (key == 'source')\n                value = source_debianize_name(module_name)\n            elif (value == 'python-<debianized-setup-name>'):\n                assert (key == 'package')\n                value = ('python-' + debianize_name(module_name))\n            elif (value == 'python3-<debianized-setup-name>'):\n                assert (key == 'package3')\n                value = ('python3-' + debianize_name(module_name))\n            elif (value == '<setup-maintainer-or-author>'):\n                assert (key == 'maintainer')\n                value = guess_maintainer\n            if (key == 'suite'):\n                if (default_distribution is not None):\n                    value = default_distribution\n                    log.warn('Deprecation warning: you are using the --default-distribution option. Switch to the --suite option.')\n        else:\n            value = ''\n        defaults[key] = value\n    return defaults\n", "label": 1}
{"function": "\n\ndef check_internet_scheme(self, elb_item):\n    '\\n        alert when an ELB has an \"internet-facing\" scheme.\\n        '\n    scheme = elb_item.config.get('scheme', None)\n    vpc = elb_item.config.get('vpc_id', None)\n    if (scheme and (scheme == 'internet-facing') and (not vpc)):\n        self.add_issue(1, 'ELB is Internet accessible.', elb_item)\n    elif (scheme and (scheme == 'internet-facing') and vpc):\n        security_groups = elb_item.config.get('security_groups', [])\n        for sgid in security_groups:\n            sg = Item.query.filter(Item.name.ilike((('%' + sgid) + '%'))).first()\n            if (not sg):\n                continue\n            sg_cidrs = []\n            config = sg.revisions[0].config\n            for rule in config.get('rules', []):\n                cidr = rule.get('cidr_ip', '')\n                if ((rule.get('rule_type', None) == 'ingress') and cidr):\n                    if ((not _check_rfc_1918(cidr)) and (not self._check_inclusion_in_network_whitelist(cidr))):\n                        sg_cidrs.append(cidr)\n            if sg_cidrs:\n                notes = 'SG [{sgname}] via [{cidr}]'.format(sgname=sg.name, cidr=', '.join(sg_cidrs))\n                self.add_issue(1, 'VPC ELB is Internet accessible.', elb_item, notes=notes)\n", "label": 1}
{"function": "\n\ndef resolve(self, context, ignore_failures=False):\n    if isinstance(self.var, Variable):\n        try:\n            obj = self.var.resolve(context)\n        except VariableDoesNotExist:\n            if ignore_failures:\n                obj = None\n            else:\n                string_if_invalid = context.template.engine.string_if_invalid\n                if string_if_invalid:\n                    if ('%s' in string_if_invalid):\n                        return (string_if_invalid % self.var)\n                    else:\n                        return string_if_invalid\n                else:\n                    obj = string_if_invalid\n    else:\n        obj = self.var\n    for (func, args) in self.filters:\n        arg_vals = []\n        for (lookup, arg) in args:\n            if (not lookup):\n                arg_vals.append(mark_safe(arg))\n            else:\n                arg_vals.append(arg.resolve(context))\n        if getattr(func, 'expects_localtime', False):\n            obj = template_localtime(obj, context.use_tz)\n        if getattr(func, 'needs_autoescape', False):\n            new_obj = func(obj, *arg_vals, autoescape=context.autoescape)\n        else:\n            new_obj = func(obj, *arg_vals)\n        if (getattr(func, 'is_safe', False) and isinstance(obj, SafeData)):\n            obj = mark_safe(new_obj)\n        elif isinstance(obj, EscapeData):\n            obj = mark_for_escaping(new_obj)\n        else:\n            obj = new_obj\n    return obj\n", "label": 1}
{"function": "\n\ndef updateIndels(self, snp, is_negative_strand):\n    contig = snp.chromosome\n    lcontig = self.mFasta.getLength(contig)\n    code = self.mAnnotations.getSequence(contig, '+', snp.pos, (snp.pos + 2))\n    self.mCode = code\n    variants = snp.genotype.split('/')\n    for variant in variants:\n        if (variant[0] == '*'):\n            self.mVariantType.append('W')\n        elif (variant[0] == '+'):\n            toinsert = variant[1:]\n            self.mVariantType.append('I')\n        elif (variant[0] == '-'):\n            todelete = variant[1:]\n            self.mVariantType.append('D')\n        else:\n            raise ValueError((\"unknown variant sign '%s'\" % variant[0]))\n    if (code[0] and (code[1] not in 'abcABC')):\n        return\n    if is_negative_strand:\n        variants = [Genomics.complement(x) for x in variants]\n    for reference_codon in self.mReferenceCodons:\n        variants = snp.genotype.split('/')\n        variants = [x[1:] for x in variants]\n        for variant in variants:\n            if ((len(variant) % 3) != 0):\n                self.mVariantCodons.append('!')\n            else:\n                self.mVariantCodons.append(variant)\n        self.mVariantAAs.extend([Genomics.translate(x) for x in self.mVariantCodons])\n", "label": 1}
{"function": "\n\ndef poll(self, event, timeout):\n    if self.state.SHUTDOWN:\n        raise Error(errno.EBADF)\n    if (not (event in ('recv', 'send', 'acks'))):\n        raise Error(errno.EINVAL)\n    if (event == 'recv'):\n        if (self.state.ESTABLISHED or self.state.CLOSE_WAIT):\n            ptype = super(DataLinkConnection, self).poll(event, timeout)\n            if (self.state.ESTABLISHED or self.state.CLOSE_WAIT):\n                return (ptype == ProtocolDataUnit.Information)\n            else:\n                return False\n    if (event == 'send'):\n        if self.state.ESTABLISHED:\n            if super(DataLinkConnection, self).poll(event, timeout):\n                return self.state.ESTABLISHED\n    if (event == 'acks'):\n        with self.acks_ready:\n            while (not (self.acks_recvd > 0)):\n                self.acks_ready.wait(timeout)\n            if (self.acks_recvd > 0):\n                self.acks_recvd = (self.acks_recvd - 1)\n                return True\n    return False\n", "label": 1}
{"function": "\n\ndef compareAddressEq(self, addr1, addr2):\n    'Checks if two addresses are equal, considering local/useable associations'\n    if (addr1 is None):\n        return (addr2 is None)\n    if (addr2 is None):\n        return False\n    try:\n        if (addr1.addressDetails == addr2.addressDetails):\n            return True\n    except AttributeError:\n        pass\n    if (isinstance(addr1, ActorAddress) and isinstance(addr1.addressDetails, ActorLocalAddress)):\n        if (isinstance(addr2, ActorAddress) and isinstance(addr2.addressDetails, ActorLocalAddress)):\n            return (addr1.addressDetails == addr2.addressDetails)\n        try:\n            return ((addr1.addressDetails.generatingActor == self._thisActorAddr) and self._managed[addr1.addressDetails.addressInstanceNum] and (self._managed[addr1.addressDetails.addressInstanceNum].addressDetails == addr2.addressDetails))\n        except AttributeError:\n            return False\n    if (isinstance(addr2, ActorAddress) and isinstance(addr2.addressDetails, ActorLocalAddress)):\n        try:\n            return ((addr2.addressDetails.generatingActor == self._thisActorAddr) and self._managed[addr2.addressDetails.addressInstanceNum] and (self._managed[addr2.addressDetails.addressInstanceNum].addressDetails == addr1.addressDetails))\n        except AttributeError:\n            return False\n    return False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.end_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.LIST):\n                self.endpoints = []\n                (_etype31, _size28) = iprot.readListBegin()\n                for _i32 in xrange(_size28):\n                    _elem33 = iprot.readString()\n                    self.endpoints.append(_elem33)\n                iprot.readListEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef lock(self):\n    '\\n        Acquire this lock.\\n\\n        @rtype: C{bool}\\n        @return: True if the lock is acquired, false otherwise.\\n\\n        @raise: Any exception os.symlink() may raise, other than\\n        EEXIST.\\n        '\n    clean = True\n    while True:\n        try:\n            symlink(str(os.getpid()), self.name)\n        except OSError as e:\n            if (_windows and (e.errno in (errno.EACCES, errno.EIO))):\n                return False\n            if (e.errno == errno.EEXIST):\n                try:\n                    pid = readlink(self.name)\n                except (IOError, OSError) as e:\n                    if (e.errno == errno.ENOENT):\n                        continue\n                    elif (_windows and (e.errno == errno.EACCES)):\n                        return False\n                    raise\n                try:\n                    if (kill is not None):\n                        kill(int(pid), 0)\n                except OSError as e:\n                    if (e.errno == errno.ESRCH):\n                        try:\n                            rmlink(self.name)\n                        except OSError as e:\n                            if (e.errno == errno.ENOENT):\n                                continue\n                            raise\n                        clean = False\n                        continue\n                    raise\n                return False\n            raise\n        self.locked = True\n        self.clean = clean\n        return True\n", "label": 1}
{"function": "\n\ndef _check_command_response(response, msg=None, allowable_errors=None):\n    'Check the response to a command for errors.\\n    '\n    if ('ok' not in response):\n        raise OperationFailure(response.get('$err'), response.get('code'), response)\n    if response.get('wtimeout', False):\n        raise WTimeoutError(response.get('errmsg', response.get('err')), response.get('code'), response)\n    if (not response['ok']):\n        details = response\n        if ('raw' in response):\n            for shard in itervalues(response['raw']):\n                if (shard.get('errmsg') and (not shard.get('ok'))):\n                    details = shard\n                    break\n        errmsg = details['errmsg']\n        if ((allowable_errors is None) or (errmsg not in allowable_errors)):\n            if (errmsg.startswith('not master') or errmsg.startswith('node is recovering')):\n                raise NotMasterError(errmsg, response)\n            if (errmsg == 'db assertion failure'):\n                errmsg = (\"db assertion failure, assertion: '%s'\" % details.get('assertion', ''))\n                raise OperationFailure(errmsg, details.get('assertionCode'), response)\n            code = details.get('code')\n            if (code in (11000, 11001, 12582)):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif (code == 50):\n                raise ExecutionTimeout(errmsg, code, response)\n            elif (code == 43):\n                raise CursorNotFound(errmsg, code, response)\n            msg = (msg or '%s')\n            raise OperationFailure((msg % errmsg), code, response)\n", "label": 1}
{"function": "\n\n@count_calls\ndef check_referenced_versions(self):\n    'Deeply checks all the references in the scene and returns a\\n        dictionary which uses the ids of the Versions as key and the action as\\n        value.\\n\\n        Uses the top level references to get a Stalker Version instance and\\n        then tracks all the changes from these Version instances.\\n\\n        :return: list\\n        '\n    dfs_version_references = []\n    version = self.get_current_version()\n    resolution_dictionary = empty_reference_resolution(root=self.get_referenced_versions())\n    for v in version.walk_hierarchy():\n        dfs_version_references.append(v)\n    dfs_version_references.pop(0)\n    for v in reversed(dfs_version_references):\n        to_be_updated_list = []\n        for ref_v in v.inputs:\n            if (not ref_v.is_latest_published_version()):\n                to_be_updated_list.append(ref_v)\n        if to_be_updated_list:\n            action = 'create'\n            latest_published_version = v.latest_published_version\n            if (latest_published_version and (not v.is_latest_published_version())):\n                if all([(ref_v.latest_published_version in latest_published_version.inputs) for ref_v in to_be_updated_list]):\n                    action = 'update'\n                else:\n                    action = 'create'\n        else:\n            if v.is_latest_published_version():\n                action = 'leave'\n            else:\n                action = 'update'\n            if any((((rev_v in resolution_dictionary['update']) or (rev_v in resolution_dictionary['create'])) for rev_v in v.inputs)):\n                action = 'create'\n        resolution_dictionary[action].append(v)\n    return resolution_dictionary\n", "label": 1}
{"function": "\n\ndef init_colors(self):\n    'Configure the coloring of the widget'\n    if self.pure:\n        return\n    try:\n        colors = self.config.ZMQInteractiveShell.colors\n    except AttributeError:\n        colors = None\n    try:\n        style = self.config.IPythonWidget.colors\n    except AttributeError:\n        style = None\n    if colors:\n        colors = colors.lower()\n        if (colors in ('lightbg', 'light')):\n            colors = 'lightbg'\n        elif (colors in ('dark', 'linux')):\n            colors = 'linux'\n        else:\n            colors = 'nocolor'\n    elif style:\n        if (style == 'bw'):\n            colors = 'nocolor'\n        elif styles.dark_style(style):\n            colors = 'linux'\n        else:\n            colors = 'lightbg'\n    else:\n        colors = None\n    widget = self.widget\n    if style:\n        widget.style_sheet = styles.sheet_from_template(style, colors)\n        widget.syntax_style = style\n        widget._syntax_style_changed()\n        widget._style_sheet_changed()\n    elif colors:\n        widget.set_default_style(colors=colors)\n    else:\n        widget.set_default_style()\n    if self.stylesheet:\n        if os.path.isfile(self.stylesheet):\n            with open(self.stylesheet) as f:\n                sheet = f.read()\n            widget.style_sheet = sheet\n            widget._style_sheet_changed()\n        else:\n            raise IOError(('Stylesheet %r not found.' % self.stylesheet))\n", "label": 1}
{"function": "\n\ndef List(self, device_path):\n    'Prints a directory listing.\\n\\n  Args:\\n    device_path: Directory to list.\\n  '\n    files = adb_commands.AdbCommands.List(self, device_path)\n    files.sort(key=(lambda x: x.filename))\n    maxname = max((len(f.filename) for f in files))\n    maxsize = max((len(str(f.size)) for f in files))\n    for f in files:\n        mode = (((((((((('d' if stat.S_ISDIR(f.mode) else '-') + ('r' if (f.mode & stat.S_IRUSR) else '-')) + ('w' if (f.mode & stat.S_IWUSR) else '-')) + ('x' if (f.mode & stat.S_IXUSR) else '-')) + ('r' if (f.mode & stat.S_IRGRP) else '-')) + ('w' if (f.mode & stat.S_IWGRP) else '-')) + ('x' if (f.mode & stat.S_IXGRP) else '-')) + ('r' if (f.mode & stat.S_IROTH) else '-')) + ('w' if (f.mode & stat.S_IWOTH) else '-')) + ('x' if (f.mode & stat.S_IXOTH) else '-'))\n        t = time.gmtime(f.mtime)\n        (yield ('%s %*d %04d-%02d-%02d %02d:%02d:%02d %-*s\\n' % (mode, maxsize, f.size, t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec, maxname, f.filename)))\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.name = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.value = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.I64):\n                self.timestamp = iprot.readI64()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.I32):\n                self.ttl = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef fuzzy(event, base=None, date_format=None):\n    if (not base):\n        base = datetime.now()\n    if date_format:\n        event = datetime.strptime(event, date_format)\n    elif (type(event) == str):\n        event = datetime.fromtimestamp(int(event))\n    elif (type(event) == int):\n        event = datetime.fromtimestamp(event)\n    elif (type(event) != datetime):\n        raise Exception('Cannot convert object `{}` to fuzzy date string'.format(event))\n    delta = (base - event)\n    if (delta.days == 0):\n        if (delta.seconds < 60):\n            return '{} seconds ago'.format(delta.seconds)\n        elif (delta.seconds < 120):\n            return '1 min and {} secs ago'.format((delta.seconds - 60))\n        elif (delta.seconds < TEN_MINS):\n            return '{} mins and {} secs ago'.format((delta.seconds // 60), (delta.seconds % 60))\n        elif (delta.seconds < ONE_HOUR):\n            return '{} minutes ago'.format((delta.seconds // 60))\n        elif (delta.seconds < TWO_HOURS):\n            return '1 hour and {} mins ago'.format(((delta.seconds % ONE_HOUR) // 60))\n        return 'over {} hours ago'.format((delta.seconds // ONE_HOUR))\n    elif (delta.days < 2):\n        return 'over a day ago'\n    elif (delta.days < 7):\n        return 'over {} days ago'.format(delta.days)\n    return '{date:%b} {date.day}, {date.year}'.format(date=event)\n", "label": 1}
{"function": "\n\ndef _convert_relation(self, prop, kwargs):\n    form_columns = getattr(self.view, 'form_columns', None)\n    if (form_columns and (prop.key not in form_columns)):\n        return None\n    remote_model = prop.mapper.class_\n    column = prop.local_remote_pairs[0][0]\n    if (not column.foreign_keys):\n        column = prop.local_remote_pairs[0][1]\n    kwargs['label'] = self._get_label(prop.key, kwargs)\n    kwargs['description'] = self._get_description(prop.key, kwargs)\n    requirement_options = (validators.Optional, validators.InputRequired)\n    if (not any((isinstance(v, requirement_options) for v in kwargs['validators']))):\n        if (column.nullable or (prop.direction.name != 'MANYTOONE')):\n            kwargs['validators'].append(validators.Optional())\n        else:\n            kwargs['validators'].append(validators.InputRequired())\n    if ('allow_blank' not in kwargs):\n        kwargs['allow_blank'] = column.nullable\n    override = self._get_field_override(prop.key)\n    if override:\n        return override(**kwargs)\n    if ((prop.direction.name == 'MANYTOONE') or (not prop.uselist)):\n        return self._model_select_field(prop, False, remote_model, **kwargs)\n    elif (prop.direction.name == 'ONETOMANY'):\n        return self._model_select_field(prop, True, remote_model, **kwargs)\n    elif (prop.direction.name == 'MANYTOMANY'):\n        return self._model_select_field(prop, True, remote_model, **kwargs)\n", "label": 1}
{"function": "\n\ndef split_command_line(command_line):\n    \"This splits a command line into a list of arguments. It splits arguments\\n    on spaces, but handles embedded quotes, doublequotes, and escaped\\n    characters. It's impossible to do this with a regular expression, so I\\n    wrote a little state machine to parse the command line. \"\n    arg_list = []\n    arg = ''\n    state_basic = 0\n    state_esc = 1\n    state_singlequote = 2\n    state_doublequote = 3\n    state_whitespace = 4\n    state = state_basic\n    for c in command_line:\n        if ((state == state_basic) or (state == state_whitespace)):\n            if (c == '\\\\'):\n                state = state_esc\n            elif (c == \"'\"):\n                state = state_singlequote\n            elif (c == '\"'):\n                state = state_doublequote\n            elif c.isspace():\n                if (state == state_whitespace):\n                    None\n                else:\n                    arg_list.append(arg)\n                    arg = ''\n                    state = state_whitespace\n            else:\n                arg = (arg + c)\n                state = state_basic\n        elif (state == state_esc):\n            arg = (arg + c)\n            state = state_basic\n        elif (state == state_singlequote):\n            if (c == \"'\"):\n                state = state_basic\n            else:\n                arg = (arg + c)\n        elif (state == state_doublequote):\n            if (c == '\"'):\n                state = state_basic\n            else:\n                arg = (arg + c)\n    if (arg != ''):\n        arg_list.append(arg)\n    return arg_list\n", "label": 1}
{"function": "\n\ndef binomial_pdf(x, a, b):\n    'binomial PDF by H. Gene Shin\\n    \\n    '\n    if (a < 1):\n        return 0\n    elif ((x < 0) or (a < x)):\n        return 0\n    elif (b == 0):\n        if (x == 0):\n            return 1\n        else:\n            return 0\n    elif (b == 1):\n        if (x == a):\n            return 1\n        else:\n            return 0\n    else:\n        if (x > (a - x)):\n            p = (1 - b)\n            mn = (a - x)\n            mx = x\n        else:\n            p = b\n            mn = x\n            mx = (a - x)\n        pdf = 1\n        t = 0\n        for q in xrange(1, (mn + 1)):\n            pdf *= ((((a - q) + 1) * p) / ((mn - q) + 1))\n            if (pdf < 1e-100):\n                while (pdf < 0.001):\n                    pdf /= (1 - p)\n                    t -= 1\n            if (pdf > 1e+100):\n                while ((pdf > 1000.0) and (t < mx)):\n                    pdf *= (1 - p)\n                    t += 1\n        for i in xrange((mx - t)):\n            pdf *= (1 - p)\n        pdf = float(('%.10e' % pdf))\n        return pdf\n", "label": 1}
{"function": "\n\ndef parse_time(val):\n    if (not val):\n        return None\n    hr = mi = 0\n    val = val.lower()\n    amflag = ((- 1) != val.find('a'))\n    pmflag = ((- 1) != val.find('p'))\n    for noise in ':amp.':\n        val = val.replace(noise, ' ')\n    val = val.split()\n    if (len(val) > 1):\n        hr = int(val[0])\n        mi = int(val[1])\n    else:\n        val = val[0]\n        if (len(val) < 1):\n            pass\n        elif ('now' == val):\n            tm = localtime()\n            hr = tm[3]\n            mi = tm[4]\n        elif ('noon' == val):\n            hr = 12\n        elif (len(val) < 3):\n            hr = int(val)\n            if ((not amflag) and (not pmflag) and (hr < 7)):\n                hr += 12\n        elif (len(val) < 5):\n            hr = int(val[:(- 2)])\n            mi = int(val[(- 2):])\n        else:\n            hr = int(val[:1])\n    if (amflag and (hr >= 12)):\n        hr = (hr - 12)\n    if (pmflag and (hr < 12)):\n        hr = (hr + 12)\n    return time(hr, mi)\n", "label": 1}
{"function": "\n\ndef resolve_job_references(io_hash, job_outputs, should_resolve=True):\n    '\\n    :param io_hash: an input or output hash in which to resolve any job-based object references possible\\n    :type io_hash: dict\\n    :param job_outputs: a mapping of finished local jobs to their output hashes\\n    :type job_outputs: dict\\n    :param should_resolve: whether it is an error if a job-based object reference in *io_hash* cannot be resolved yet\\n    :type should_resolve: boolean\\n\\n    Modifies *io_hash* in-place.\\n    '\n    q = []\n    for field in io_hash:\n        if is_job_ref(io_hash[field]):\n            io_hash[field] = resolve_job_ref(io_hash[field], job_outputs, should_resolve)\n        elif (isinstance(io_hash[field], list) or isinstance(io_hash[field], dict)):\n            q.append(io_hash[field])\n    while (len(q) > 0):\n        thing = q.pop()\n        if isinstance(thing, list):\n            for i in range(len(thing)):\n                if is_job_ref(thing[i]):\n                    thing[i] = resolve_job_ref(thing[i], job_outputs, should_resolve)\n                elif (isinstance(thing[i], list) or isinstance(thing[i], dict)):\n                    q.append(thing[i])\n        else:\n            for field in thing:\n                if is_job_ref(thing[field]):\n                    thing[field] = resolve_job_ref(thing[field], job_outputs, should_resolve)\n                elif (isinstance(thing[field], list) or isinstance(thing[field], dict)):\n                    q.append(thing[field])\n", "label": 1}
{"function": "\n\ndef keypress(self, size, key):\n    command = self._command_map[key]\n    if (key == ']'):\n        self.shift_order((+ 1))\n        return True\n    elif (key == '['):\n        self.shift_order((- 1))\n        return True\n    elif (key == '>'):\n        self.focus_hotspot(size)\n        return True\n    elif (key == '\\\\'):\n        layout = {\n            FLAT: NESTED,\n            NESTED: FLAT,\n        }[self.layout]\n        self.set_layout(layout)\n        return True\n    command = self._command_map[key]\n    if (command == 'menu'):\n        self.defocus()\n        return True\n    elif (command == urwid.CURSOR_RIGHT):\n        if (self.layout == FLAT):\n            return True\n        (widget, node) = self.tbody.get_focus()\n        if widget.expanded:\n            heavy_widget = widget.first_child()\n            if (heavy_widget is not None):\n                heavy_node = heavy_widget.get_node()\n                self.tbody.change_focus(size, heavy_node)\n            return True\n    elif (command == urwid.CURSOR_LEFT):\n        if (self.layout == FLAT):\n            return True\n        (widget, node) = self.tbody.get_focus()\n        if (not widget.expanded):\n            parent_node = node.get_parent()\n            if (not parent_node.is_root()):\n                self.tbody.change_focus(size, parent_node)\n            return True\n    elif (command == urwid.ACTIVATE):\n        if self.viewer.paused:\n            self.viewer.resume()\n        else:\n            self.viewer.pause()\n        return True\n    return super(StatisticsTable, self).keypress(size, key)\n", "label": 1}
{"function": "\n\ndef _analyze(self):\n    ' works out the updates to be performed '\n    if ((self.value is None) or (self.value == self.previous)):\n        pass\n    elif (self._operation == 'append'):\n        self._append = self.value\n    elif (self._operation == 'prepend'):\n        self._prepend = self.value\n    elif (self.previous is None):\n        self._assignments = self.value\n    elif (len(self.value) < len(self.previous)):\n        self._assignments = self.value\n    elif (len(self.previous) == 0):\n        self._assignments = self.value\n    else:\n        search_space = (len(self.value) - max(0, (len(self.previous) - 1)))\n        search_size = len(self.previous)\n        for i in range(search_space):\n            j = (i + search_size)\n            sub = self.value[i:j]\n            idx_cmp = (lambda idx: (self.previous[idx] == sub[idx]))\n            if (idx_cmp(0) and idx_cmp((- 1)) and (self.previous == sub)):\n                self._prepend = (self.value[:i] or None)\n                self._append = (self.value[j:] or None)\n                break\n        if (self._prepend is self._append is None):\n            self._assignments = self.value\n    self._analyzed = True\n", "label": 1}
{"function": "\n\n@staticmethod\ndef obtainSystemConstants():\n    lines = filter(None, map(str.strip, subprocess.check_output(['qhost']).split('\\n')))\n    line = lines[0]\n    items = line.strip().split()\n    num_columns = len(items)\n    cpu_index = None\n    mem_index = None\n    for i in range(num_columns):\n        if (items[i] == 'NCPU'):\n            cpu_index = i\n        elif (items[i] == 'MEMTOT'):\n            mem_index = i\n    if ((cpu_index is None) or (mem_index is None)):\n        RuntimeError('qhost command does not return NCPU or MEMTOT columns')\n    maxCPU = 0\n    maxMEM = MemoryString('0')\n    for line in lines[2:]:\n        items = line.strip().split()\n        if (len(items) < num_columns):\n            RuntimeError('qhost output has a varying number of columns')\n        if ((items[cpu_index] != '-') and (items[cpu_index] > maxCPU)):\n            maxCPU = items[cpu_index]\n        if ((items[mem_index] != '-') and (MemoryString(items[mem_index]) > maxMEM)):\n            maxMEM = MemoryString(items[mem_index])\n    if ((maxCPU is 0) or (maxMEM is 0)):\n        RuntimeError('qhost returned null NCPU or MEMTOT info')\n    return (maxCPU, maxMEM)\n", "label": 1}
{"function": "\n\ndef test_reset_late(self):\n    time = TestingTimeFunction()\n    callback = MockCallback()\n    timer = Timer(callback, 10, _time_function=time)\n    time.time = 13\n    assert (timer.sleep_time() == 0)\n    still_alive = timer.run()\n    assert (not still_alive)\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == Decimal('inf'))\n    timer.reset()\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 10)\n    time.time = 21\n    still_alive = timer.run()\n    assert still_alive\n    assert (callback.nb_calls == 1)\n    assert (timer.sleep_time() == 2)\n    time.time = 30\n    assert (timer.sleep_time() == 0)\n    still_alive = timer.run()\n    assert (not still_alive)\n    assert (callback.nb_calls == 2)\n    assert (timer.sleep_time() == Decimal('inf'))\n", "label": 1}
{"function": "\n\ndef _fetch_objects(self, doc_type=None):\n    'Fetch all references and convert to their document objects\\n        '\n    object_map = {\n        \n    }\n    for (collection, dbrefs) in self.reference_map.iteritems():\n        if hasattr(collection, 'objects'):\n            col_name = collection._get_collection_name()\n            refs = [dbref for dbref in dbrefs if ((col_name, dbref) not in object_map)]\n            references = collection.objects.in_bulk(refs)\n            for (key, doc) in references.iteritems():\n                object_map[(col_name, key)] = doc\n        else:\n            if isinstance(doc_type, (ListField, DictField, MapField)):\n                continue\n            refs = [dbref for dbref in dbrefs if ((collection, dbref) not in object_map)]\n            if doc_type:\n                references = doc_type._get_db()[collection].find({\n                    '_id': {\n                        '$in': refs,\n                    },\n                })\n                for ref in references:\n                    doc = doc_type._from_son(ref)\n                    object_map[(collection, doc.id)] = doc\n            else:\n                references = get_db()[collection].find({\n                    '_id': {\n                        '$in': refs,\n                    },\n                })\n                for ref in references:\n                    if ('_cls' in ref):\n                        doc = get_document(ref['_cls'])._from_son(ref)\n                    elif (doc_type is None):\n                        doc = get_document(''.join((x.capitalize() for x in collection.split('_'))))._from_son(ref)\n                    else:\n                        doc = doc_type._from_son(ref)\n                    object_map[(collection, doc.id)] = doc\n    return object_map\n", "label": 1}
{"function": "\n\ndef _cardindex(self, key):\n    'Returns an index into the ._cards list given a valid lookup key.'\n    if isinstance(key, string_types):\n        keyword = key\n        n = 0\n    elif isinstance(key, int):\n        if (key < 0):\n            key += len(self._cards)\n        if ((key < 0) or (key >= len(self._cards))):\n            raise IndexError('Header index out of range.')\n        return key\n    elif isinstance(key, slice):\n        return key\n    elif isinstance(key, tuple):\n        if ((len(key) != 2) or (not isinstance(key[0], string_types)) or (not isinstance(key[1], int))):\n            raise ValueError('Tuple indices must be 2-tuples consisting of a keyword string and an integer index.')\n        (keyword, n) = key\n    else:\n        raise ValueError('Header indices must be either a string, a 2-tuple, or an integer.')\n    keyword = Card.normalize_keyword(keyword)\n    indices = self._keyword_indices.get(keyword, None)\n    if (keyword and (not indices)):\n        if ((len(keyword) > KEYWORD_LENGTH) or ('.' in keyword)):\n            raise KeyError(('Keyword %r not found.' % keyword))\n        else:\n            indices = self._rvkc_indices.get(keyword, None)\n    if (not indices):\n        raise KeyError(('Keyword %r not found.' % keyword))\n    try:\n        return indices[n]\n    except IndexError:\n        raise IndexError(('There are only %d %r cards in the header.' % (len(indices), keyword)))\n", "label": 1}
{"function": "\n\ndef _generate_events(self, event):\n    try:\n        if (not any([self._read, self._write])):\n            return\n        timeout = event.time_left\n        if (timeout < 0):\n            (r, w, _) = select.select(self._read, self._write, [])\n        else:\n            (r, w, _) = select.select(self._read, self._write, [], timeout)\n    except ValueError as e:\n        return self._preenDescriptors()\n    except TypeError as e:\n        return self._preenDescriptors()\n    except (SelectError, SocketError, IOError) as e:\n        if (e.args[0] in (0, 2)):\n            if ((not self._read) and (not self._write)):\n                return\n            else:\n                raise\n        elif (e.args[0] == EINTR):\n            return\n        elif (e.args[0] == EBADF):\n            return self._preenDescriptors()\n        else:\n            raise\n    for sock in w:\n        if self.isWriting(sock):\n            self.fire(_write(sock), self.getTarget(sock))\n    for sock in r:\n        if (sock == self._ctrl_recv):\n            self._read_ctrl()\n            continue\n        if self.isReading(sock):\n            self.fire(_read(sock), self.getTarget(sock))\n", "label": 1}
{"function": "\n\ndef _load_metadata(force_reload=False):\n    'Load metadata information into memory.\\n\\n    If force_reload, the metadata information will be reloaded\\n    even if the metadata is already loaded.\\n    '\n    adapter_api.load_adapters_internal(force_reload=force_reload)\n    global OS_FIELDS\n    if (force_reload or (OS_FIELDS is None)):\n        OS_FIELDS = _get_os_fields_from_configuration()\n    global PACKAGE_FIELDS\n    if (force_reload or (PACKAGE_FIELDS is None)):\n        PACKAGE_FIELDS = _get_package_fields_from_configuration()\n    global FLAVOR_FIELDS\n    if (force_reload or (FLAVOR_FIELDS is None)):\n        FLAVOR_FIELDS = _get_flavor_fields_from_configuration()\n    global OSES_METADATA\n    if (force_reload or (OSES_METADATA is None)):\n        OSES_METADATA = _get_oses_metadata_from_configuration()\n    global PACKAGES_METADATA\n    if (force_reload or (PACKAGES_METADATA is None)):\n        PACKAGES_METADATA = _get_packages_metadata_from_configuration()\n    global FLAVORS_METADATA\n    if (force_reload or (FLAVORS_METADATA is None)):\n        FLAVORS_METADATA = _get_flavors_metadata_from_configuration()\n    global OSES_METADATA_UI_CONVERTERS\n    if (force_reload or (OSES_METADATA_UI_CONVERTERS is None)):\n        OSES_METADATA_UI_CONVERTERS = _get_oses_metadata_ui_converters_from_configuration()\n    global FLAVORS_METADATA_UI_CONVERTERS\n    if (force_reload or (FLAVORS_METADATA_UI_CONVERTERS is None)):\n        FLAVORS_METADATA_UI_CONVERTERS = _get_flavors_metadata_ui_converters_from_configuration()\n", "label": 1}
{"function": "\n\ndef checkpins(contents, designator, errs):\n    pins = re_pins.findall(contents)\n    nums = []\n    for (name, num, x, y, length, numsize, namesize) in pins:\n        if (((int(x) % 100) != 0) or ((int(y) % 100) != 0)):\n            errs.append(\"Pin '{}' not on 100mil grid\".format(name))\n        if ((designator in ('IC', 'U')) and (int(length) not in (100, 150))):\n            errs.append(\"Pin '{}' not 100 or 150mil long, but part is IC or U\".format(name))\n        if ((int(namesize) != 50) or ((int(numsize) != 50) and num.isdigit())):\n            errs.append(\"Pin '{}' font size not 50mil\".format(name))\n        if num.isdigit():\n            nums.append(int(num))\n    if nums:\n        expected = set(range(min(nums), (max(nums) + 1)))\n        if (set(nums) != expected):\n            missing = [str(x) for x in (set(expected) - set(nums))]\n            errs.append('Missing pins {}'.format(', '.join(missing)))\n        duplicates = set([str(x) for x in nums if (nums.count(x) > 1)])\n        if duplicates:\n            errs.append('Duplicated pins {}'.format(', '.join(duplicates)))\n", "label": 1}
{"function": "\n\ndef __init__(self, kernel, it_space, *args, **kwargs):\n    read_args = [a.data for a in args if (a.access in [READ, RW])]\n    written_args = [a.data for a in args if (a.access in [RW, WRITE, MIN, MAX, INC])]\n    inc_args = [a.data for a in args if (a.access in [INC])]\n    LazyComputation.__init__(self, (set(read_args) | Const._defs), set(written_args), set(inc_args))\n    self._kernel = kernel\n    self._actual_args = args\n    self._it_space = it_space\n    for (i, arg) in enumerate(self._actual_args):\n        arg.position = i\n        arg.indirect_position = i\n    for (i, arg1) in enumerate(self._actual_args):\n        if (arg1._is_dat and arg1._is_indirect):\n            for arg2 in self._actual_args[i:]:\n                if ((arg2.data is arg1.data) and (arg2.map is arg1.map)):\n                    arg2.indirect_position = arg1.indirect_position\n    self._all_args = kwargs.get('all_args', [args])\n    self._inspection = kwargs.get('inspection')\n    self._executor = kwargs.get('executor')\n", "label": 1}
{"function": "\n\ndef _click_autocomplete(root, text):\n    'Completer generator for click applications.'\n    try:\n        parts = shlex.split(text)\n    except ValueError:\n        raise StopIteration\n    (location, incomplete) = _click_resolve_command(root, parts)\n    if ((not text.endswith(' ')) and (not incomplete) and text):\n        raise StopIteration\n    if (incomplete and (not incomplete[0:2].isalnum())):\n        for param in location.params:\n            if (not isinstance(param, click.Option)):\n                continue\n            for opt in itertools.chain(param.opts, param.secondary_opts):\n                if opt.startswith(incomplete):\n                    (yield completion.Completion(opt, (- len(incomplete)), display_meta=param.help))\n    elif isinstance(location, (click.MultiCommand, click.core.Group)):\n        ctx = click.Context(location)\n        commands = location.list_commands(ctx)\n        for command in commands:\n            if command.startswith(incomplete):\n                cmd = location.get_command(ctx, command)\n                (yield completion.Completion(command, (- len(incomplete)), display_meta=cmd.short_help))\n", "label": 1}
{"function": "\n\ndef print_tweet(tweet, settings):\n    'Format and print the tweet dict.\\n\\n    Returns:\\n        boolean status of if the tweet was printed\\n    '\n    tweet_text = tweet.get('text')\n    if ((tweet_text is None) or ((not settings['spam']) and AntiSpam.is_spam(tweet_text))):\n        return False\n    if (sys.version_info[0] == 2):\n        for encoding in ['utf-8', 'latin-1']:\n            try:\n                tweet_text.decode(encoding)\n            except UnicodeEncodeError:\n                pass\n            else:\n                break\n        else:\n            return False\n    if settings.get('json'):\n        print(json.dumps(tweet, indent=4, sort_keys=True))\n    else:\n        prepend = []\n        if (settings.get('date') or settings.get('time')):\n            date = parse_date(tweet['created_at'])\n            if settings.get('date'):\n                prepend.append('{0:%b} {1}'.format(date, int(datetime.strftime(date, '%d'))))\n            if settings.get('time'):\n                prepend.append('{0:%H}:{0:%M}:{0:%S}'.format(date))\n        tweet = '{0}{1}@{2}: {3}'.format(' '.join(prepend), (' ' * int((prepend != []))), tweet.get('user', {\n            \n        }).get('screen_name', ''), unescape(tweet_text))\n        print(highlight_tweet(tweet))\n    return True\n", "label": 1}
{"function": "\n\ndef parse_changes_file(file_path, versions=None):\n    '\\n    Parse CHANGES file and return a dictionary with contributors.\\n\\n    Dictionary maps contributor name to the JIRA tickets or Github pull\\n    requests the user has worked on.\\n    '\n    contributors_map = defaultdict(set)\n    in_entry = False\n    active_version = None\n    active_tickets = []\n    with open(file_path, 'r') as fp:\n        for line in fp:\n            line = line.strip()\n            match = re.search('Changes with Apache Libcloud (\\\\d+\\\\.\\\\d+\\\\.\\\\d+(-\\\\w+)?).*?$', line)\n            if match:\n                active_version = match.groups()[0]\n            if (versions and (active_version not in versions)):\n                continue\n            if (line.startswith('-') or line.startswith('*)')):\n                in_entry = True\n                active_tickets = []\n            if (in_entry and (line == '')):\n                in_entry = False\n            if in_entry:\n                match = re.search('\\\\((.+?)\\\\)$', line)\n                if match:\n                    active_tickets = match.groups()[0]\n                    active_tickets = active_tickets.split(', ')\n                    active_tickets = [ticket for ticket in active_tickets if (ticket.startswith('LIBCLOUD-') or ticket.startswith('GITHUB-'))]\n                match = re.search('^\\\\[(.+?)\\\\]$', line)\n                if match:\n                    contributors = match.groups()[0]\n                    contributors = contributors.split(',')\n                    contributors = [name.strip() for name in contributors]\n                    for name in contributors:\n                        name = name.title()\n                        contributors_map[name].update(set(active_tickets))\n    return contributors_map\n", "label": 1}
{"function": "\n\ndef visit_statement(self, tokens):\n    tables = set()\n    if self._tables:\n        table_tokens = self._GetDescendants(tokens, 'table', do_not_cross=('exclude',))\n        table_aliases = self._GetTableAliases(tokens)\n        for table in table_tokens:\n            table_name = str(table[0])\n            if (table_name in table_aliases):\n                table_name = str(table_aliases[table_name]['table'][0])\n            tables.add(table_name)\n        if (not (self._tables & tables)):\n            return\n    columns = set()\n    if self._columns:\n        columns = set((str(x[0]).lower() for x in self._GetDescendants(tokens, 'column', do_not_cross=('exclude',))))\n        if (not (self._columns & columns)):\n            return\n    values = set()\n    if self._values:\n        values = set((str(x[0]) for x in self._GetDescendants(tokens, 'val', do_not_cross=('exclude',))))\n        if (not (self._values & values)):\n            return\n    operations = set()\n    if self._operations:\n        for operation in self._operations:\n            if self._GetDescendants(tokens, operation):\n                operations.add(operation)\n        if (not operations):\n            return\n    msg = (self._msg % {\n        'tables': tables,\n        'columns': columns,\n        'values': values,\n        'operations': operations,\n    })\n    self.AddWarning(tokens, msg)\n", "label": 1}
{"function": "\n\ndef assert_tables_equal(self, table, reflected_table, strict_types=False):\n    assert (len(table.c) == len(reflected_table.c))\n    for (c, reflected_c) in zip(table.c, reflected_table.c):\n        eq_(c.name, reflected_c.name)\n        assert (reflected_c is reflected_table.c[c.name])\n        eq_(c.primary_key, reflected_c.primary_key)\n        eq_(c.nullable, reflected_c.nullable)\n        if strict_types:\n            msg = \"Type '%s' doesn't correspond to type '%s'\"\n            assert isinstance(reflected_c.type, type(c.type)), (msg % (reflected_c.type, c.type))\n        else:\n            self.assert_types_base(reflected_c, c)\n        if isinstance(c.type, sqltypes.String):\n            eq_(c.type.length, reflected_c.type.length)\n        eq_(set([f.column.name for f in c.foreign_keys]), set([f.column.name for f in reflected_c.foreign_keys]))\n        if c.server_default:\n            assert isinstance(reflected_c.server_default, schema.FetchedValue)\n    assert (len(table.primary_key) == len(reflected_table.primary_key))\n    for c in table.primary_key:\n        assert (reflected_table.primary_key.columns[c.name] is not None)\n", "label": 1}
{"function": "\n\ndef main():\n    'Entry point for this script.'\n    (parser, args) = parse_command_line_arguments()\n    logger = initialize_logging(args.debug, args.less_verbose)\n    result = 0\n    if args.download:\n        try:\n            download_listing(args.listing)\n        except DownloadRetfListingFailed as ex:\n            logger.error('Downloading latest RETF listing failed: %s.', ex)\n            result = 1\n    if ((not args.path) and (not args.file) and (not args.download)):\n        parser.print_help()\n        result = 2\n    if ((not result) and (not os.path.isfile(args.listing))):\n        logger.error('RETF listing not found at %s.', args.listing)\n        logger.info('Please download the RETF listing first by using the parameter --download.')\n        result = 1\n    if (not result):\n        files = get_file_listing(args.path, args.file, args.extension)\n        rules = generate_listing(args.listing)\n        disabled = load_disabled_rules(args.disabled)\n        all_findings = 0\n        for check in files:\n            if (not os.path.isfile(check)):\n                continue\n            (findings, content) = check_file(check, rules, disabled)\n            if (findings > 0):\n                all_findings += findings\n                logger.warning('%s finding(s) in file %s.', findings, check)\n            if ((findings > 0) and args.write_changes):\n                write_text_to_file(check, content, args.no_backup, args.in_place)\n        if (all_findings > 0):\n            logger.warning('%s finding(s) in all checked files.', all_findings)\n            result = 1\n    return result\n", "label": 1}
{"function": "\n\ndef get_fields(self):\n    to_update = False\n    doc = inspect.getdoc(self.method)\n    if ((not doc) and issubclass(self.method.im_class.model, models.Model)):\n        fields = None\n        if (self.method.__name__ == 'read'):\n            fields = (self.method.im_class.fields if self.method.im_class.fields else tuple((attr.name for attr in self.method.im_class.model._meta.local_fields)))\n        elif (self.method.__name__ in ('create', 'update')):\n            to_update = True\n            if (hasattr(self.method.im_class, 'form') and hasattr(self.method.im_class.form, '_meta')):\n                fields = self.method.im_class.form._meta.fields\n            else:\n                fields = self.method.im_class.fields\n        if fields:\n            for field in fields:\n                for mfield in self.method.im_class.model._meta.fields:\n                    if (mfield.name == field):\n                        (yield {\n                            'name': mfield.name,\n                            'required': ((not mfield.blank) if to_update else False),\n                            'type': get_field_data_type(mfield),\n                            'verbose': mfield.verbose_name,\n                            'help_text': mfield.help_text,\n                        })\n                        break\n", "label": 1}
{"function": "\n\ndef _parse(stream, ptr=0):\n    i = ptr\n    laststr = None\n    lasttok = None\n    deserialized = {\n        \n    }\n    while (i < len(stream)):\n        c = stream[i]\n        if (c == STRING):\n            (string, i) = _symtostr(stream, i)\n            if (lasttok == STRING):\n                deserialized[laststr] = string\n            laststr = string\n        elif (c == NODE_OPEN):\n            (deserialized[laststr], i) = _parse(stream, (i + 1))\n        elif (c == NODE_CLOSE):\n            return (deserialized, i)\n        elif (c == COMMENT):\n            if (((i + 1) < len(stream)) and (stream[(i + 1)] == '/')):\n                i = stream.find('\\n', i)\n        elif ((c == CR) or (c == LF)):\n            ni = (i + 1)\n            if ((ni < len(stream)) and (stream[ni] == LF)):\n                i = ni\n            if (lasttok != LF):\n                c = LF\n        else:\n            c = lasttok\n        lasttok = c\n        i += 1\n    return (deserialized, i)\n", "label": 1}
{"function": "\n\ndef do_show(self, *args):\n    if 'running-config'.startswith(args[0]):\n        if 'vlan'.startswith(args[1]):\n            self.show_run_vlan()\n        if 'interface'.startswith(args[1]):\n            self.show_run_int(args)\n    elif 'interfaces'.startswith(args[0]):\n        self.show_int(args)\n    elif 'vlan'.startswith(args[0]):\n        if args[1].isdigit():\n            self._show_vlan(int(args[1]))\n        elif 'brief'.startswith(args[1]):\n            self.show_vlan_brief()\n        elif 'ethernet'.startswith(args[1]):\n            self.show_vlan_int(args)\n        else:\n            self.write_line(('Invalid input -> %s' % args[1]))\n            self.write_line('Type ? for a list')\n    elif ('ip'.startswith(args[0]) and 'route'.startswith(args[1]) and 'static'.startswith(args[2])):\n        routes = self.switch_configuration.static_routes\n        if routes:\n            self.write_line('        Destination        Gateway        Port          Cost          Type Uptime src-vrf')\n        for (n, route) in enumerate(routes):\n            self.write_line('{index:<8}{destination:<18} {next_hop:}'.format(index=(n + 1), destination=route.dest, next_hop=route.next_hop))\n        self.write_line('')\n    elif 'version'.startswith(args[0]):\n        self.show_version()\n", "label": 1}
{"function": "\n\ndef check(self, app, sha, config):\n    token = current_app.config['GITHUB_TOKEN']\n    if (not token):\n        raise CheckFailed('GITHUB_TOKEN is not set')\n    api_root = (config.get('api_root') or current_app.config['GITHUB_API_ROOT']).rstrip('/')\n    contexts = set((config.get('contexts') or []))\n    repo = config['repo']\n    url = '{api_root}/repos/{repo}/commits/{ref}/statuses'.format(api_root=api_root, repo=repo, ref=sha)\n    headers = {\n        'Accepts': 'application/json',\n        'Authorization': 'token {}'.format(token),\n    }\n    resp = http.get(url, headers=headers)\n    context_list = resp.json()\n    if (not context_list):\n        raise CheckFailed('No contexts were present in GitHub')\n    valid_contexts = set()\n    for data in context_list:\n        if (data['state'] == 'success'):\n            valid_contexts.add(data['context'])\n            try:\n                contexts.remove(data['context'])\n            except KeyError:\n                pass\n        if (data['context'] in valid_contexts):\n            continue\n        if (contexts and (data['context'] not in contexts)):\n            continue\n        if (data['state'] == 'pending'):\n            raise CheckPending(ERR_CHECK.format(data['context'], data['state']))\n        elif (data['state'] != 'success'):\n            raise CheckFailed(ERR_CHECK.format(data['context'], data['state']))\n        contexts.remove(data['context'])\n    if contexts:\n        raise CheckFailed(ERR_MISSING_CONTEXT.format(iter(contexts).next()))\n", "label": 1}
{"function": "\n\ndef set_rstate(self, seed):\n    if isinstance(seed, int):\n        if (seed == 0):\n            raise ValueError('seed should not be 0', seed)\n        elif (seed >= M2):\n            raise ValueError(('seed should be less than %i' % M2), seed)\n        self.rstate = numpy.asarray(([seed] * 6), dtype='int32')\n    elif (len(seed) == 6):\n        if ((seed[0] == 0) and (seed[1] == 0) and (seed[2] == 0)):\n            raise ValueError('The first 3 values of seed should not be all 0', seed)\n        if ((seed[3] == 0) and (seed[4] == 0) and (seed[5] == 0)):\n            raise ValueError('The last 3 values of seed should not be all 0', seed)\n        if ((seed[0] >= M1) or (seed[1] >= M1) or (seed[2] >= M1)):\n            raise ValueError(('The first 3 values of seed should be less than %i' % M1), seed)\n        if ((seed[3] >= M2) or (seed[4] >= M2) or (seed[5] >= M2)):\n            raise ValueError(('The last 3 values of seed should be less than %i' % M2), seed)\n        self.rstate = numpy.asarray(seed, dtype='int32')\n    else:\n        raise TypeError('seed should be 1 integer or 6 integers')\n", "label": 1}
{"function": "\n\ndef resumeProducing(self):\n    self.paused = False\n    if self._buffer:\n        data = ''.join(self._buffer)\n        bytesSent = self._writeSomeData(data)\n        if (bytesSent < len(data)):\n            unsent = data[bytesSent:]\n            assert (not self.iAmStreaming), 'Streaming producer did not write all its data.'\n            self._buffer[:] = [unsent]\n        else:\n            self._buffer[:] = []\n    else:\n        bytesSent = 0\n    if (self.unregistered and bytesSent and (not self._buffer) and (self.consumer is not None)):\n        self.consumer.unregisterProducer()\n    if (not self.iAmStreaming):\n        self.outstandingPull = (not bytesSent)\n    if (self.producer is not None):\n        bytesBuffered = sum([len(s) for s in self._buffer])\n        if (self.producerPaused and (bytesBuffered < self.bufferSize)):\n            self.producerPaused = False\n            self.producer.resumeProducing()\n        elif self.outstandingPull:\n            self.producer.resumeProducing()\n", "label": 1}
{"function": "\n\ndef clean(self):\n    super(ScrapedObjAttrFormSet, self).clean()\n    cnt_type_b = 0\n    cnt_type_u = 0\n    cnt_type_i = 0\n    cnt_id = 0\n    cnt_wrong_id_type = 0\n    for form in self.forms:\n        if (not hasattr(form, 'cleaned_data')):\n            continue\n        data = form.cleaned_data\n        if (('DELETE' in data) and data['DELETE']):\n            continue\n        if ((not ('attr_type' in data)) or (not ('id_field' in data))):\n            continue\n        at = data['attr_type']\n        if (at == 'B'):\n            cnt_type_b += 1\n        if (at == 'U'):\n            cnt_type_u += 1\n        if (at == 'I'):\n            cnt_type_i += 1\n        id_field = data['id_field']\n        if id_field:\n            cnt_id += 1\n            if ((at != 'S') and (at != 'U')):\n                cnt_wrong_id_type += 1\n    if (cnt_type_b == 0):\n        raise ValidationError('For the scraped object class definition one object attribute of type BASE is required!')\n    if (cnt_type_b > 1):\n        raise ValidationError('Only one object attribute of type BASE allowed!')\n    if (cnt_type_u > 25):\n        raise ValidationError('Maximum number of 25 detail page URLs supported!')\n    if (cnt_type_i > 1):\n        raise ValidationError('Currently only one image per object supported!')\n    if (cnt_wrong_id_type > 0):\n        raise ValidationError('Only STANDARD or DETAIL_PAGE_URL attributes can be defined as ID fields!')\n", "label": 1}
{"function": "\n\ndef post(self, *args, **kwargs):\n    widget = self.object\n    ordering = self.kwargs.get('ordering')\n    if (int(ordering) == 0):\n        widget.ordering = 0\n        widget.save()\n        widgets = getattr(widget.parent.content, widget.region)\n        widgets = [w for w in widgets if (w.id != widget.id)]\n        for (i, _widget) in enumerate(widgets):\n            _widget.ordering = (i + 1)\n            _widget.save()\n    elif (int(ordering) == (- 1)):\n        next_ordering = (widget.ordering - 1)\n        widgets = getattr(widget.parent.content, widget.region)\n        for w in widgets:\n            if (w.ordering == next_ordering):\n                w.ordering = widget.ordering\n                w.save()\n                widget.ordering = next_ordering\n                widget.save()\n    elif (int(ordering) == 1):\n        next_ordering = (widget.ordering + 1)\n        widgets = getattr(widget.parent.content, widget.region)\n        for w in widgets:\n            if (w.ordering == next_ordering):\n                w.ordering = widget.ordering\n                w.save()\n                widget.ordering = next_ordering\n                widget.save()\n    else:\n        widget.ordering = widget.next_ordering\n        widget.save()\n        widgets = getattr(widget.parent.content, widget.region)\n        widgets = [w for w in widgets if (w.id != widget.id)]\n        widgets.sort(key=(lambda w: w.ordering))\n        for (i, _widget) in enumerate(widgets):\n            _widget.ordering = i\n            _widget.save()\n    messages.success(self.request, _('Widget was successfully moved.'))\n    success_url = self.get_success_url()\n    response = HttpResponseRedirect(success_url)\n    response['X-Horizon-Location'] = success_url\n    return response\n", "label": 1}
{"function": "\n\ndef parse(self):\n    '\\n    Parse the vmstat file\\n    :return: status of the metric parse\\n    '\n    file_status = True\n    for input_file in self.infile_list:\n        file_status = (file_status and naarad.utils.is_valid_file(input_file))\n    if (not file_status):\n        return False\n    status = True\n    data = {\n        \n    }\n    for input_file in self.infile_list:\n        logger.info('Processing : %s', input_file)\n        timestamp_format = None\n        with open(input_file) as fh:\n            for line in fh:\n                words = line.split()\n                if (len(words) < 3):\n                    continue\n                ts = ((words[0] + ' ') + words[1])\n                if ((not timestamp_format) or (timestamp_format == 'unknown')):\n                    timestamp_format = naarad.utils.detect_timestamp_format(ts)\n                if (timestamp_format == 'unknown'):\n                    continue\n                ts = naarad.utils.get_standardized_timestamp(ts, timestamp_format)\n                if self.ts_out_of_range(ts):\n                    continue\n                col = words[2]\n                if (self.sub_metrics and (col not in self.sub_metrics)):\n                    continue\n                self.sub_metric_unit[col] = 'pages'\n                if (col in self.column_csv_map):\n                    out_csv = self.column_csv_map[col]\n                else:\n                    out_csv = self.get_csv(col)\n                    data[out_csv] = []\n                data[out_csv].append(((ts + ',') + words[3]))\n    for csv in data.keys():\n        self.csv_files.append(csv)\n        with open(csv, 'w') as fh:\n            fh.write('\\n'.join(data[csv]))\n    return status\n", "label": 1}
{"function": "\n\ndef onColor(self, event=None, item=None):\n    color = hexcolor(event.GetValue())\n    setattr(self.parent.conf, item, color)\n    if (item == 'spectra_color'):\n        self.parent.panel.conf.set_trace_color(color, trace=0)\n    elif (item == 'roi_color'):\n        self.parent.panel.conf.set_trace_color(color, trace=1)\n    elif (item == 'marker_color'):\n        for lmark in self.parent.cursor_markers:\n            if (lmark is not None):\n                lmark.set_color(color)\n    elif ((item == 'roi_fillcolor') and (self.parent.roi_patch is not None)):\n        self.parent.roi_patch.set_color(color)\n    elif (item == 'major_elinecolor'):\n        for l in self.parent.major_markers:\n            l.set_color(color)\n    elif (item == 'minor_elinecolor'):\n        for l in self.parent.minor_markers:\n            l.set_color(color)\n    elif (item == 'hold_elinecolor'):\n        for l in self.parent.hold_markers:\n            l.set_color(color)\n    self.parent.panel.canvas.draw()\n    self.parent.panel.Refresh()\n", "label": 1}
{"function": "\n\ndef get_filters(self):\n    is_public = public_filter()\n    is_active = active_filter()\n    is_datetime = datetime_filter(self.filter_yaml.get_image_date())\n    is_tenant = tenant_filter(self.filter_yaml.get_tenant())\n    images_list = self.filter_yaml.get_image_ids()\n    excluded_images_list = self.filter_yaml.get_excluded_image_ids()\n    if (images_list and excluded_images_list):\n        raise exception.AbortMigrationError(\"In the filter config file specified 'images_list' and 'exclude_images_list'. Must be only one list with images - 'images_list' or 'exclude_images_list'.\")\n    if excluded_images_list:\n        is_image_id = image_id_exclude_filter(excluded_images_list)\n    else:\n        is_image_id = image_id_filter(images_list)\n    is_member = member_filter(self.glance_client, self.filter_yaml.get_tenant())\n    if self.filter_yaml.is_public_and_member_images_filtered():\n        return [(lambda i: (is_active(i) and is_tenant(i) and is_image_id(i) and is_datetime(i)))]\n    else:\n        return [(lambda i: ((is_active(i) and is_public(i)) or (is_active(i) and is_member(i)) or (is_active(i) and is_tenant(i) and is_image_id(i) and is_datetime(i))))]\n", "label": 1}
{"function": "\n\ndef delete_file(self, path, prefixed_path, source_storage, **options):\n    symlink = options['link']\n    if self.storage.exists(prefixed_path):\n        try:\n            target_last_modified = self.storage.modified_time(prefixed_path)\n        except (OSError, NotImplementedError):\n            pass\n        else:\n            try:\n                source_last_modified = source_storage.modified_time(path)\n            except (OSError, NotImplementedError):\n                pass\n            else:\n                if self.local:\n                    full_path = self.storage.path(prefixed_path)\n                else:\n                    full_path = None\n                if (target_last_modified >= source_last_modified):\n                    if (not ((symlink and full_path and (not os.path.islink(full_path))) or ((not symlink) and full_path and os.path.islink(full_path)))):\n                        if (prefixed_path not in self.unmodified_files):\n                            self.unmodified_files.append(prefixed_path)\n                        self.log((\"Skipping '%s' (not modified)\" % path))\n                        return False\n        if options['dry_run']:\n            self.log((\"Pretending to delete '%s'\" % path))\n        else:\n            self.log((\"Deleting '%s'\" % path))\n            self.storage.delete(prefixed_path)\n    return True\n", "label": 1}
{"function": "\n\ndef image_vacuum(name):\n    '\\n    Delete images not in use or installed via image_present\\n    '\n    name = name.lower()\n    ret = {\n        'name': name,\n        'changes': {\n            \n        },\n        'result': None,\n        'comment': '',\n    }\n    images = []\n    for state in __salt__['state.show_lowstate']():\n        if ('state' not in state):\n            continue\n        if (state['state'] != __virtualname__):\n            continue\n        if (state['fun'] not in ['image_present']):\n            continue\n        if ('name' in state):\n            images.append(state['name'])\n    for image_uuid in __salt__['vmadm.list'](order='image_uuid'):\n        if (image_uuid in images):\n            continue\n        images.append(image_uuid)\n    ret['result'] = True\n    for image_uuid in __salt__['imgadm.list']():\n        if (image_uuid in images):\n            continue\n        if (image_uuid in __salt__['imgadm.delete'](image_uuid)):\n            ret['changes'][image_uuid] = None\n        else:\n            ret['result'] = False\n            ret['comment'] = 'failed to delete images'\n    if (ret['result'] and (len(ret['changes']) == 0)):\n        ret['comment'] = 'no images deleted'\n    elif (ret['result'] and (len(ret['changes']) > 0)):\n        ret['comment'] = 'images deleted'\n    return ret\n", "label": 1}
{"function": "\n\n@register_specialize\n@gof.local_optimizer([T.mul, T.true_div])\ndef local_abs_merge(node):\n    \"\\n    Merge abs generated by local_abs_lift when the canonizer don't\\n    need it anymore\\n\\n    \"\n    if ((node.op == T.mul) and (sum([(i.owner.op == T.abs_) for i in node.inputs if i.owner]) > 1)):\n        inputs = []\n        for i in node.inputs:\n            if (i.owner and (i.owner.op == T.abs_)):\n                inputs.append(i.owner.inputs[0])\n            elif isinstance(i, Constant):\n                try:\n                    const = get_scalar_constant_value(i)\n                except NotScalarConstantError:\n                    return False\n                if (not (const >= 0).all()):\n                    return False\n                inputs.append(i)\n            else:\n                return False\n        return [T.abs_(T.mul(*inputs))]\n    if ((node.op == T.true_div) and (sum([(i.owner.op == T.abs_) for i in node.inputs if i.owner]) == 2)):\n        return [T.abs_(T.true_div(node.inputs[0].owner.inputs[0], node.inputs[1].owner.inputs[0]))]\n", "label": 1}
{"function": "\n\ndef _matches(self, expr, repl_dict={\n    \n}):\n    from sympy import Wild\n    sign = 1\n    (a, b) = self.as_two_terms()\n    if (a is S.NegativeOne):\n        if b.is_Mul:\n            sign = (- sign)\n        else:\n            return b.matches((- expr), repl_dict)\n    expr = sympify(expr)\n    if (expr.is_Mul and (expr.args[0] is S.NegativeOne)):\n        expr = (- expr)\n        sign = (- sign)\n    if (not expr.is_Mul):\n        if (len(self.args) == 2):\n            if (b == expr):\n                return a.matches(Rational(sign), repl_dict)\n            dd = b.matches(expr, repl_dict)\n            if (dd is None):\n                return None\n            dd = a.matches(Rational(sign), dd)\n            return dd\n        return None\n    d = repl_dict.copy()\n    pp = list(self.args)\n    ee = list(expr.args)\n    for p in self.args:\n        if (p in expr.args):\n            ee.remove(p)\n            pp.remove(p)\n    if ((len(pp) == 1) and isinstance(pp[0], Wild)):\n        if (len(ee) == 1):\n            d[pp[0]] = (sign * ee[0])\n        else:\n            d[pp[0]] = (sign * expr.func(*ee))\n        return d\n    if (len(ee) != len(pp)):\n        return None\n    for (p, e) in zip(pp, ee):\n        d = p.xreplace(d).matches(e, d)\n        if (d is None):\n            return None\n    return d\n", "label": 1}
{"function": "\n\ndef css(self, *args, **kwargs):\n    'css attributes manipulation\\n        '\n    attr = value = no_default\n    length = len(args)\n    if (length == 1):\n        attr = args[0]\n    elif (length == 2):\n        (attr, value) = args\n    elif kwargs:\n        attr = kwargs\n    else:\n        raise ValueError(('Invalid arguments %s %s' % (args, kwargs)))\n    if isinstance(attr, dict):\n        for tag in self:\n            stripped_keys = [key.strip().replace('_', '-') for key in attr.keys()]\n            current = [el.strip() for el in (tag.get('style') or '').split(';') if (el.strip() and (not (el.split(':')[0].strip() in stripped_keys)))]\n            for (key, value) in attr.items():\n                key = key.replace('_', '-')\n                current.append(('%s: %s' % (key, value)))\n            tag.set('style', '; '.join(current))\n    elif isinstance(value, basestring):\n        attr = attr.replace('_', '-')\n        for tag in self:\n            current = [el.strip() for el in (tag.get('style') or '').split(';') if (el.strip() and (not (el.split(':')[0].strip() == attr.strip())))]\n            current.append(('%s: %s' % (attr, value)))\n            tag.set('style', '; '.join(current))\n    return self\n", "label": 1}
{"function": "\n\ndef visit_Task(self, node):\n    self.push_module(None)\n    name = node.name\n    _task = task.Task(name)\n    statement = [self.visit(s) for s in node.statement]\n    body = []\n    for s in statement:\n        if isinstance(s, (tuple, list)):\n            for d in s:\n                if isinstance(d, vtypes.Input):\n                    t = _task.Input(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Reg):\n                    t = _task.Reg(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Integer):\n                    t = _task.Integer(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                else:\n                    body.append(s)\n        else:\n            body.append(s)\n    _task.Body(*body)\n    self.pop_module()\n    self.add_object(_task)\n    return _task\n", "label": 1}
{"function": "\n\ndef _update(self, frame_no):\n    if self._signal:\n        start_x = int(((self._screen.width - self._signal.max_width) // 2))\n        start_y = int(((self._screen.height - self._signal.max_height) // 2))\n        (text, colours) = self._signal.rendered_text\n    else:\n        start_x = start_y = 0\n        (text, colours) = ('', [])\n    for y in range(self._screen.height):\n        if (self._strength < 1.0):\n            offset = randint(0, int((6 - (6 * self._strength))))\n        else:\n            offset = 0\n        for x in range(self._screen.width):\n            ix = (x - start_x)\n            iy = (y - start_y)\n            if (self._signal and (random() <= self._strength) and (x >= start_x) and (y >= start_y) and (iy < len(text)) and ((ix + offset) < len(text[iy]))):\n                self._screen.paint(text[iy][(ix + offset)], x, y, colour_map=[colours[iy][ix]])\n            elif (random() < 0.2):\n                self._screen.print_at(chr(randint(33, 126)), x, y)\n    self._strength += self._step\n    if ((self._strength >= 1.25) or (self._strength <= (- 0.5))):\n        self._step = (- self._step)\n", "label": 1}
{"function": "\n\ndef Table(*args, **kw):\n    'A schema.Table wrapper/hook for dialect-specific tweaks.'\n    test_opts = dict([(k, kw.pop(k)) for k in list(kw) if k.startswith('test_')])\n    kw.update(table_options)\n    if exclusions.against(config._current, 'mysql'):\n        if (('mysql_engine' not in kw) and ('mysql_type' not in kw)):\n            if (('test_needs_fk' in test_opts) or ('test_needs_acid' in test_opts)):\n                kw['mysql_engine'] = 'InnoDB'\n            else:\n                kw['mysql_engine'] = 'MyISAM'\n    if exclusions.against(config._current, 'firebird'):\n        table_name = args[0]\n        unpack = config.db.dialect.identifier_preparer.unformat_identifiers\n        fks = [fk for col in args if isinstance(col, schema.Column) for fk in col.foreign_keys]\n        for fk in fks:\n            ref = fk._colspec\n            if isinstance(ref, schema.Column):\n                name = ref.table.name\n            else:\n                name = unpack(ref)[0]\n            if (name == table_name):\n                if (fk.ondelete is None):\n                    fk.ondelete = 'CASCADE'\n                if (fk.onupdate is None):\n                    fk.onupdate = 'CASCADE'\n    return schema.Table(*args, **kw)\n", "label": 1}
{"function": "\n\ndef test_complex_inverse_functions():\n    mp.dps = 15\n    iv.dps = 15\n    for (z1, z2) in random_complexes(30):\n        assert sinh(asinh(z1)).ae(z1)\n        assert acosh(z1).ae(cmath.acosh(z1))\n        assert atanh(z1).ae(cmath.atanh(z1))\n        assert atan(z1).ae(cmath.atan(z1))\n        assert asin(z1).ae(cmath.asin(z1), rel_eps=1e-12)\n        assert acos(z1).ae(cmath.acos(z1), rel_eps=1e-12)\n        one = mpf(1)\n    for i in range((- 9), 10, 3):\n        for k in range((- 9), 10, 3):\n            a = (((0.9 * j) * (10 ** k)) + ((0.8 * one) * (10 ** i)))\n            b = cos(acos(a))\n            assert b.ae(a)\n            b = sin(asin(a))\n            assert b.ae(a)\n    one = mpf(1)\n    err = (2 * (10 ** (- 15)))\n    for i in range((- 9), 9, 3):\n        for k in range((- 9), 9, 3):\n            a = (((- 0.9) * (10 ** k)) + (((j * 0.8) * one) * (10 ** i)))\n            b = cosh(acosh(a))\n            assert b.ae(a, err)\n            b = sinh(asinh(a))\n            assert b.ae(a, err)\n", "label": 1}
{"function": "\n\ndef test_get_memory_maps(self):\n    p = psutil.Process(os.getpid())\n    maps = p.get_memory_maps()\n    paths = [x for x in maps]\n    self.assertEqual(len(paths), len(set(paths)))\n    ext_maps = p.get_memory_maps(grouped=False)\n    for nt in maps:\n        if (not nt.path.startswith('[')):\n            assert os.path.isabs(nt.path), nt.path\n            if POSIX:\n                assert os.path.exists(nt.path), nt.path\n            elif ('64' not in os.path.basename(nt.path)):\n                assert os.path.exists(nt.path), nt.path\n    for nt in ext_maps:\n        for fname in nt._fields:\n            value = getattr(nt, fname)\n            if (fname == 'path'):\n                continue\n            elif (fname in ('addr', 'perms')):\n                assert value, value\n            else:\n                assert isinstance(value, (int, long))\n                assert (value >= 0), value\n", "label": 1}
{"function": "\n\ndef update(self):\n    '\\n        The function to draw a new frame for the particle system.\\n        '\n    if (self.time_left > 0):\n        self.time_left -= 1\n        for _ in range(self._count):\n            new_particle = self._new_particle()\n            if (new_particle is not None):\n                self.particles.append(new_particle)\n    for particle in self.particles:\n        last = particle.last()\n        if (last is not None):\n            (char, x, y, fg, attr, bg) = last\n            screen_data = self._screen.get_from(x, y)\n            if (self._blend and screen_data):\n                (char2, fg2, attr2, bg2) = screen_data\n                index = 0\n                for (i, colours) in enumerate(particle.colours):\n                    if ((fg2, attr2, bg2) == colours):\n                        index = i\n                        break\n                index -= 1\n                (fg, attr, bg) = particle.colours[max(index, 0)]\n            self._screen.print_at(' ', x, y, fg, attr, bg)\n        if (particle.time < particle.life_time):\n            (char, x, y, fg, attr, bg) = particle.next()\n            screen_data = self._screen.get_from(x, y)\n            if (self._blend and screen_data):\n                (char2, fg2, attr2, bg2) = screen_data\n                index = (- 1)\n                for (i, colours) in enumerate(particle.colours):\n                    if ((fg2, attr2, bg2) == colours):\n                        index = i\n                        break\n                index += 1\n                (fg, attr, bg) = particle.colours[min(index, (len(particle.colours) - 1))]\n            self._screen.print_at(char, x, y, fg, attr, bg)\n        else:\n            self.particles.remove(particle)\n", "label": 1}
{"function": "\n\ndef _skip_instance(self):\n    skip = self._buffer.read_bits(8)\n    if (skip == 0):\n        length = self._vint()\n        for i in xrange(length):\n            self._skip_instance()\n    elif (skip == 1):\n        length = self._vint()\n        self._buffer.read_aligned_bytes(((length + 7) / 8))\n    elif (skip == 2):\n        length = self._vint()\n        self._buffer.read_aligned_bytes(length)\n    elif (skip == 3):\n        tag = self._vint()\n        self._skip_instance()\n    elif (skip == 4):\n        exists = (self._buffer.read_bits(8) != 0)\n        if exists:\n            self._skip_instance()\n    elif (skip == 5):\n        length = self._vint()\n        for i in xrange(length):\n            tag = self._vint()\n            self._skip_instance()\n    elif (skip == 6):\n        self._buffer.read_aligned_bytes(1)\n    elif (skip == 7):\n        self._buffer.read_aligned_bytes(4)\n    elif (skip == 8):\n        self._buffer.read_aligned_bytes(8)\n    elif (skip == 9):\n        self._vint()\n", "label": 1}
{"function": "\n\ndef _non_dominated_front_fast(iterable, key=(lambda x: x), allowequality=True):\n    'Return a subset of items from iterable which are not dominated by any\\n    other item in iterable.\\n\\n    Faster version.\\n    '\n    items = list(iterable)\n    keys = dict(((i, key(i)) for i in items))\n    dim = len(list(keys.values())[0])\n    dominations = {\n        \n    }\n    for i in items:\n        for j in items:\n            good = True\n            if allowequality:\n                for k in range(dim):\n                    if (keys[i][k] >= keys[j][k]):\n                        good = False\n                        break\n            else:\n                for k in range(dim):\n                    if (keys[i][k] > keys[j][k]):\n                        good = False\n                        break\n            if good:\n                dominations[(i, j)] = None\n    res = set()\n    items = set(items)\n    for i in items:\n        res.add(i)\n        for j in list(res):\n            if (i is j):\n                continue\n            if ((j, i) in dominations):\n                res.remove(i)\n                break\n            elif ((i, j) in dominations):\n                res.remove(j)\n    return res\n", "label": 1}
{"function": "\n\ndef _validate_python(self, value, state):\n    if (not value):\n        raise Invalid(self.message('empty', state), value, state)\n    value = value.strip()\n    splitted = value.split('@', 1)\n    try:\n        (username, domain) = splitted\n    except ValueError:\n        raise Invalid(self.message('noAt', state), value, state)\n    if (not self.usernameRE.search(username)):\n        raise Invalid(self.message('badUsername', state, username=username), value, state)\n    try:\n        idna_domain = [idna.ToASCII(p) for p in domain.split('.')]\n        if (six.text_type is str):\n            idna_domain = [p.decode('ascii') for p in idna_domain]\n        idna_domain = '.'.join(idna_domain)\n    except UnicodeError:\n        raise Invalid(self.message('badDomain', state, domain=domain), value, state)\n    if (not self.domainRE.search(idna_domain)):\n        raise Invalid(self.message('badDomain', state, domain=domain), value, state)\n    if self.resolve_domain:\n        assert have_dns, 'dnspython should be available'\n        global socket\n        if (socket is None):\n            import socket\n        try:\n            try:\n                dns.resolver.query(domain, 'MX')\n            except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                try:\n                    dns.resolver.query(domain, 'A')\n                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                    raise Invalid(self.message('domainDoesNotExist', state, domain=domain), value, state)\n        except (socket.error, dns.exception.DNSException) as e:\n            raise Invalid(self.message('socketError', state, error=e), value, state)\n", "label": 1}
{"function": "\n\ndef transpile_md_to_python(markdown):\n    'A very naive markdown to python converter.'\n    for line in markdown.split('\\n'):\n        line = line.lstrip()\n        if line.startswith('# '):\n            (yield Heading(1, line, 'h1'))\n        elif line.startswith('## '):\n            (yield Heading(2, line, 'h2'))\n        elif line.startswith('### '):\n            (yield Heading(3, line, 'h3'))\n        elif line.startswith('#### '):\n            (yield Heading(4, line, 'h4'))\n        elif line.startswith('##### '):\n            (yield Heading(5, line, 'h5'))\n        elif line.startswith('###### '):\n            (yield Heading(6, line, 'h6'))\n        elif (line.startswith('*') and (not line.endswith('**'))):\n            (yield Tag('emphasis', line, 'em'))\n        elif (line.startswith('**') and line.endswith('**')):\n            (yield Tag('strong', line, 'strong'))\n        elif (line.startswith('[') and line.endswith(')')):\n            (yield Tag('href', line, 'a'))\n", "label": 1}
{"function": "\n\ndef to_python(self, value):\n    '\\n        Validates that the input can be converted to a datetime. Returns a\\n        Python datetime.datetime object.\\n        '\n    if (value in validators.EMPTY_VALUES):\n        return None\n    if isinstance(value, datetime.datetime):\n        return value\n    if isinstance(value, datetime.date):\n        return datetime.datetime(value.year, value.month, value.day)\n    if isinstance(value, list):\n        if (len(value) != 4):\n            raise ValidationError(self.error_messages['invalid'])\n        if ((value[0] in validators.EMPTY_VALUES) and (value[1] in validators.EMPTY_VALUES) and (value[2] in validators.EMPTY_VALUES) and (value[3] in validators.EMPTY_VALUES)):\n            return None\n        start_value = ('%s %s' % tuple(value[:2]))\n        end_value = ('%s %s' % tuple(value[2:]))\n    start_datetime = None\n    end_datetime = None\n    for format in (self.input_formats or formats.get_format('DATETIME_INPUT_FORMATS')):\n        try:\n            start_datetime = datetime.datetime(*time.strptime(start_value, format)[:6])\n        except ValueError:\n            continue\n    for format in (self.input_formats or formats.get_format('DATETIME_INPUT_FORMATS')):\n        try:\n            end_datetime = datetime.datetime(*time.strptime(end_value, format)[:6])\n        except ValueError:\n            continue\n    return (start_datetime, end_datetime)\n", "label": 1}
{"function": "\n\ndef test_lrucache(self):\n    c = LRUCache(2, dispose=(lambda _: None))\n    assert (len(c) == 0)\n    assert (c.items() == set())\n    for (i, x) in enumerate('abc'):\n        c[x] = i\n    assert (len(c) == 2)\n    assert (c.items() == set([('b', 1), ('c', 2)]))\n    assert ('a' not in c)\n    assert ('b' in c)\n    with pytest.raises(KeyError):\n        c['a']\n    assert (c['b'] == 1)\n    assert (c['c'] == 2)\n    c['d'] = 3\n    assert (len(c) == 2)\n    assert (c['c'] == 2)\n    assert (c['d'] == 3)\n    del c['c']\n    assert (len(c) == 1)\n    with pytest.raises(KeyError):\n        c['c']\n    assert (c['d'] == 3)\n    c.clear()\n    assert (c.items() == set())\n", "label": 1}
{"function": "\n\ndef test_disk_partitions(self):\n    for disk in psutil.disk_partitions(all=False):\n        assert os.path.exists(disk.device), disk\n        assert os.path.isdir(disk.mountpoint), disk\n        assert disk.fstype, disk\n        assert isinstance(disk.opts, str)\n    for disk in psutil.disk_partitions(all=True):\n        if (not WINDOWS):\n            try:\n                os.stat(disk.mountpoint)\n            except OSError:\n                err = sys.exc_info()[1]\n                if (err.errno not in (errno.EPERM, errno.EACCES)):\n                    raise\n            else:\n                assert os.path.isdir(disk.mountpoint), disk.mountpoint\n        assert isinstance(disk.fstype, str)\n        assert isinstance(disk.opts, str)\n\n    def find_mount_point(path):\n        path = os.path.abspath(path)\n        while (not os.path.ismount(path)):\n            path = os.path.dirname(path)\n        return path\n    mount = find_mount_point(__file__)\n    mounts = [x.mountpoint for x in psutil.disk_partitions(all=True)]\n    self.assertIn(mount, mounts)\n    psutil.disk_usage(mount)\n", "label": 1}
{"function": "\n\ndef visit_Function(self, node):\n    self.push_module(None)\n    name = node.name\n    width = (self.visit(node.retwidth) if (node.retwidth is not None) else None)\n    func = function.Function(name, width)\n    if (node.retwidth is not None):\n        func._set_raw_width(self.visit(node.retwidth.msb), self.visit(node.retwidth.lsb))\n    statement = [self.visit(s) for s in node.statement]\n    body = []\n    for s in statement:\n        if isinstance(s, (tuple, list)):\n            for d in s:\n                if isinstance(d, vtypes.Input):\n                    t = func.Input(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Reg):\n                    t = func.Reg(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                elif isinstance(d, vtypes.Integer):\n                    t = func.Integer(d.name, d.width, d.length, d.signed, d.value)\n                    if ((d.width_msb is not None) and (d.width_lsb is not None)):\n                        t._set_raw_width(d.width_msb, d.width_lsb)\n                else:\n                    body.append(s)\n        else:\n            body.append(s)\n    func.Body(*body)\n    self.pop_module()\n    self.add_object(func)\n    return func\n", "label": 1}
{"function": "\n\n@coroutine\ndef _wrap_awaitable(x):\n    if hasattr(x, '__await__'):\n        _i = x.__await__()\n    else:\n        _i = iter(x)\n    try:\n        _y = next(_i)\n    except StopIteration as _e:\n        _r = _value_from_stopiteration(_e)\n    else:\n        while 1:\n            try:\n                _s = (yield _y)\n            except GeneratorExit as _e:\n                try:\n                    _m = _i.close\n                except AttributeError:\n                    pass\n                else:\n                    _m()\n                raise _e\n            except BaseException as _e:\n                _x = sys.exc_info()\n                try:\n                    _m = _i.throw\n                except AttributeError:\n                    raise _e\n                else:\n                    try:\n                        _y = _m(*_x)\n                    except StopIteration as _e:\n                        _r = _value_from_stopiteration(_e)\n                        break\n            else:\n                try:\n                    if (_s is None):\n                        _y = next(_i)\n                    else:\n                        _y = _i.send(_s)\n                except StopIteration as _e:\n                    _r = _value_from_stopiteration(_e)\n                    break\n    raise Return(_r)\n", "label": 1}
{"function": "\n\ndef _caas_2_2_8a8f6abc_2745_4d8a_9cbc_8dabe5a7d0e4_network_vlan_ALLFILTERS(self, method, url, body, headers):\n    (_, params) = url.split('?')\n    parameters = params.split('&')\n    for parameter in parameters:\n        (key, value) = parameter.split('=')\n        if (key == 'datacenterId'):\n            assert (value == 'fake_location')\n        elif (key == 'networkDomainId'):\n            assert (value == 'fake_network_domain')\n        elif (key == 'ipv6Address'):\n            assert (value == 'fake_ipv6')\n        elif (key == 'privateIpv4Address'):\n            assert (value == 'fake_ipv4')\n        elif (key == 'name'):\n            assert (value == 'fake_name')\n        elif (key == 'state'):\n            assert (value == 'fake_state')\n        else:\n            raise ValueError('Could not find in url parameters {0}:{1}'.format(key, value))\n    body = self.fixtures.load('network_vlan.xml')\n    return (httplib.OK, body, {\n        \n    }, httplib.responses[httplib.OK])\n", "label": 1}
{"function": "\n\ndef _complete_authz(self, user, areq, sid, **kwargs):\n    _log_debug = logger.debug\n    _log_debug('- in authenticated() -')\n    try:\n        permission = self.authz(user, client_id=areq['client_id'])\n        self.sdb.update(sid, 'permission', permission)\n    except Exception:\n        raise\n    _log_debug(('response type: %s' % areq['response_type']))\n    if self.sdb.is_revoked(sid):\n        return self._error(error='access_denied', descr='Token is revoked')\n    info = self.create_authn_response(areq, sid)\n    if isinstance(info, Response):\n        return info\n    else:\n        (aresp, fragment_enc) = info\n    try:\n        redirect_uri = self.get_redirect_uri(areq)\n    except (RedirectURIError, ParameterError) as err:\n        return BadRequest(('%s' % err))\n    info = self.aresp_check(aresp, areq)\n    if isinstance(info, Response):\n        return info\n    headers = []\n    try:\n        _kaka = kwargs['cookie']\n    except KeyError:\n        pass\n    else:\n        if (_kaka and (self.cookie_name not in _kaka)):\n            headers.append(self.cookie_func(user, typ='sso', ttl=self.sso_ttl))\n    if ('response_mode' in areq):\n        try:\n            resp = self.response_mode(areq, fragment_enc, aresp=aresp, redirect_uri=redirect_uri, headers=headers)\n        except InvalidRequest as err:\n            return self._error('invalid_request', err)\n        else:\n            if (resp is not None):\n                return resp\n    return (aresp, headers, redirect_uri, fragment_enc)\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _unbroadcast(grad_op_tree, x, be):\n    '\\n        Reverse broadcast from shape(grad_op_tree) to shape(x)\\n\\n        Arguments:\\n            grad_op_tree (OpTreeNode or Tensor): The OpTreeNode to broadcast.\\n            x (OpTreeNode or Tensor): Provides the dimension to be broadcasted to.\\n            be: (Backend): The backend to be used.\\n\\n        Returns:\\n            OpTreeNode or Tensor: The broadcasted result.\\n        '\n    if ((not grad_op_tree) or (not x)):\n        return grad_op_tree\n    if (type(x) in _scalar_types):\n        return 0.0\n    in_shape = x.shape\n    out_shape = grad_op_tree.shape\n    if (in_shape == out_shape):\n        return grad_op_tree\n    elif ((len(in_shape) == 2) and (len(out_shape) == 2)):\n        if (in_shape == (1, 1)):\n            return be.sum(grad_op_tree)\n        elif ((in_shape[0] == out_shape[0]) and (in_shape[1] == 1)):\n            return be.sum(grad_op_tree, axis=1)\n        elif ((in_shape[0] == 1) and (in_shape[1] == out_shape[1])):\n            return be.sum(grad_op_tree, axis=0)\n        elif (((out_shape[0] == in_shape[0]) and (out_shape[1] == 1)) or ((out_shape[0] == 1) and (out_shape[1] == in_shape[1]))):\n            return ((0 * x) + grad_op_tree)\n        else:\n            return NotImplemented\n    else:\n        return NotImplemented\n", "label": 1}
{"function": "\n\ndef _match_url(self, request):\n    if (self._url is ANY):\n        return True\n    if hasattr(self._url, 'search'):\n        return (self._url.search(request.url) is not None)\n    if (self._url_parts.scheme and (request.scheme != self._url_parts.scheme)):\n        return False\n    if (self._url_parts.netloc and (request.netloc != self._url_parts.netloc)):\n        return False\n    if ((request.path or '/') != (self._url_parts.path or '/')):\n        return False\n    request_qs = urlparse.parse_qs(request.query)\n    matcher_qs = urlparse.parse_qs(self._url_parts.query)\n    for (k, vals) in six.iteritems(matcher_qs):\n        for v in vals:\n            try:\n                request_qs.get(k, []).remove(v)\n            except ValueError:\n                return False\n    if self._complete_qs:\n        for v in six.itervalues(request_qs):\n            if v:\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef __init__(self, fp, headers, params=None, parts=None):\n    self.processors = self.processors.copy()\n    self.fp = fp\n    self.headers = headers\n    if (params is None):\n        params = {\n            \n        }\n    self.params = params\n    if (parts is None):\n        parts = []\n    self.parts = parts\n    self.content_type = headers.elements('Content-Type')\n    if self.content_type:\n        self.content_type = self.content_type[0]\n    else:\n        self.content_type = httputil.HeaderElement.from_str(self.default_content_type)\n    dec = self.content_type.params.get('charset', None)\n    if dec:\n        self.attempt_charsets = ([dec] + [c for c in self.attempt_charsets if (c != dec)])\n    else:\n        self.attempt_charsets = self.attempt_charsets[:]\n    self.length = None\n    clen = headers.get('Content-Length', None)\n    if ((clen is not None) and ('chunked' not in headers.get('Transfer-Encoding', ''))):\n        try:\n            self.length = int(clen)\n        except ValueError:\n            pass\n    self.name = None\n    self.filename = None\n    disp = headers.elements('Content-Disposition')\n    if disp:\n        disp = disp[0]\n        if ('name' in disp.params):\n            self.name = disp.params['name']\n            if (self.name.startswith('\"') and self.name.endswith('\"')):\n                self.name = self.name[1:(- 1)]\n        if ('filename' in disp.params):\n            self.filename = disp.params['filename']\n            if (self.filename.startswith('\"') and self.filename.endswith('\"')):\n                self.filename = self.filename[1:(- 1)]\n", "label": 1}
{"function": "\n\ndef get(self, getme=None, fromEnd=False):\n    if (not getme):\n        return self\n    try:\n        getme = int(getme)\n        if (getme < 0):\n            return self[:((- 1) * getme)]\n        else:\n            return [self[(getme - 1)]]\n    except IndexError:\n        return []\n    except ValueError:\n        rangeResult = self.rangePattern.search(getme)\n        if rangeResult:\n            start = (rangeResult.group('start') or None)\n            end = (rangeResult.group('start') or None)\n            if start:\n                start = (int(start) - 1)\n            if end:\n                end = int(end)\n            return self[start:end]\n        getme = getme.strip()\n        if (getme.startswith('/') and getme.endswith('/')):\n            finder = re.compile(getme[1:(- 1)], ((re.DOTALL | re.MULTILINE) | re.IGNORECASE))\n\n            def isin(hi):\n                return finder.search(hi)\n        else:\n\n            def isin(hi):\n                return (getme.lower() in hi.lowercase)\n        return [itm for itm in self if isin(itm)]\n", "label": 1}
{"function": "\n\ndef __call__(self, *args, **kwargs):\n    if (len(args) > 1):\n        raise ArgumentError(self.tag)\n    if ((self.parent is not None) and (self.parent.class_ is not None)):\n        if ('class_' not in kwargs):\n            kwargs['class_'] = self.parent.class_\n    if ((self.parent is None) and (len(args) == 1)):\n        x = [self.render(self.tag, False, myarg, mydict) for (myarg, mydict) in _argsdicts(args, kwargs)]\n        return '\\n'.join(x)\n    elif ((self.parent is None) and (len(args) == 0)):\n        x = [self.render(self.tag, True, myarg, mydict) for (myarg, mydict) in _argsdicts(args, kwargs)]\n        return '\\n'.join(x)\n    if (self.tag in self.parent.twotags):\n        for (myarg, mydict) in _argsdicts(args, kwargs):\n            self.render(self.tag, False, myarg, mydict)\n    elif (self.tag in self.parent.onetags):\n        if (len(args) == 0):\n            for (myarg, mydict) in _argsdicts(args, kwargs):\n                self.render(self.tag, True, myarg, mydict)\n        else:\n            raise ClosingError(self.tag)\n    elif ((self.parent.mode == 'strict_html') and (self.tag in self.parent.deptags)):\n        raise DeprecationError(self.tag)\n    else:\n        raise InvalidElementError(self.tag, self.parent.mode)\n", "label": 1}
{"function": "\n\ndef _set_field_names(self, val):\n    val = [self._unicode(x) for x in val]\n    self._validate_option('field_names', val)\n    if self._field_names:\n        old_names = self._field_names[:]\n    self._field_names = val\n    if (self._align and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._align[new_name] = self._align[old_name]\n        for old_name in old_names:\n            if (old_name not in self._align):\n                self._align.pop(old_name)\n    else:\n        for field in self._field_names:\n            self._align[field] = 'c'\n    if (self._valign and old_names):\n        for (old_name, new_name) in zip(old_names, val):\n            self._valign[new_name] = self._valign[old_name]\n        for old_name in old_names:\n            if (old_name not in self._valign):\n                self._valign.pop(old_name)\n    else:\n        for field in self._field_names:\n            self._valign[field] = 't'\n", "label": 1}
{"function": "\n\ndef picture(self, image, duration=1.0, block=True, pos=None, hpr=None, scale=None, color=None, parent=None):\n    'Display a picture on the screen and keep it there for a particular duration.'\n    if ((pos is not None) and (type(pos) not in (int, float)) and (len(pos) == 2)):\n        pos = (pos[0], 0, pos[1])\n    if ((scale is not None) and (type(scale) not in (int, float)) and (len(scale) == 2)):\n        scale = (scale[0], 1, scale[1])\n    if ((hpr is not None) and (type(scale) not in (int, float)) and (len(hpr) == 1)):\n        hpr = (0, 0, hpr)\n    if (duration == 0):\n        block = False\n    obj = self._engine.direct.gui.OnscreenImage.OnscreenImage(image=image, pos=pos, hpr=hpr, scale=scale, color=color, parent=parent)\n    self._to_destroy.append(obj)\n    obj.setTransparency(self._engine.pandac.TransparencyAttrib.MAlpha)\n    if self.implicit_markers:\n        self.marker(248)\n    if block:\n        if ((type(duration) == list) or (type(duration) == tuple)):\n            self.sleep(duration[0])\n            self.waitfor(duration[1])\n        elif (type(duration) == str):\n            self.waitfor(duration)\n        else:\n            self.sleep(duration)\n        self._destroy_object(obj, 249)\n    else:\n        if (duration > 0):\n            self._engine.base.taskMgr.doMethodLater(duration, self._destroy_object, 'ConvenienceFunctions, remove_picture', extraArgs=[obj, 249])\n        return obj\n", "label": 1}
{"function": "\n\n@plumbing.route('/repos/<repo_key>/git/commits/')\n@corsify\n@jsonify\ndef get_commit_list(repo_key):\n    ref_name = (request.args.get('ref_name') or None)\n    start_sha = (request.args.get('start_sha') or None)\n    limit = (request.args.get('limit') or current_app.config['RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT'])\n    try:\n        limit = int(limit)\n    except ValueError:\n        raise BadRequest('invalid limit')\n    if (limit < 0):\n        raise BadRequest('invalid limit')\n    repo = get_repo(repo_key)\n    start_commit_id = None\n    if (start_sha is not None):\n        start_commit_id = start_sha\n    else:\n        if (ref_name is None):\n            ref_name = 'HEAD'\n        ref = lookup_ref(repo, ref_name)\n        if (ref is None):\n            raise NotFound('reference not found')\n        start_ref = lookup_ref(repo, ref_name)\n        try:\n            start_commit_id = start_ref.resolve().target\n        except KeyError:\n            if (ref_name == 'HEAD'):\n                return []\n            else:\n                raise NotFound('reference not found')\n    try:\n        walker = repo.walk(start_commit_id, GIT_SORT_TIME)\n    except ValueError:\n        raise BadRequest('invalid start_sha')\n    except KeyError:\n        raise NotFound('commit not found')\n    commits = [convert_commit(repo_key, commit) for commit in islice(walker, limit)]\n    return commits\n", "label": 1}
{"function": "\n\ndef _download_url(resp, link, temp_location):\n    fp = open(temp_location, 'wb')\n    download_hash = None\n    if (link.hash and link.hash_name):\n        try:\n            download_hash = hashlib.new(link.hash_name)\n        except ValueError:\n            logger.warn(('Unsupported hash name %s for package %s' % (link.hash_name, link)))\n    try:\n        total_length = int(resp.info()['content-length'])\n    except (ValueError, KeyError, TypeError):\n        total_length = 0\n    downloaded = 0\n    show_progress = ((total_length > (40 * 1000)) or (not total_length))\n    show_url = link.show_url\n    try:\n        if show_progress:\n            if total_length:\n                logger.start_progress(('Downloading %s (%s): ' % (show_url, format_size(total_length))))\n            else:\n                logger.start_progress(('Downloading %s (unknown size): ' % show_url))\n        else:\n            logger.notify(('Downloading %s' % show_url))\n        logger.info(('Downloading from URL %s' % link))\n        while True:\n            chunk = resp.read(4096)\n            if (not chunk):\n                break\n            downloaded += len(chunk)\n            if show_progress:\n                if (not total_length):\n                    logger.show_progress(('%s' % format_size(downloaded)))\n                else:\n                    logger.show_progress(('%3i%%  %s' % (((100 * downloaded) / total_length), format_size(downloaded))))\n            if (download_hash is not None):\n                download_hash.update(chunk)\n            fp.write(chunk)\n        fp.close()\n    finally:\n        if show_progress:\n            logger.end_progress(('%s downloaded' % format_size(downloaded)))\n    return download_hash\n", "label": 1}
{"function": "\n\ndef __init__(self, obj=None, default=None, white_list=None, white_pattern=None, black_list=None, ignore_help=False, ignore_return=False, name=None, doc=None, debug=False):\n    obj = (obj or sys.modules['__main__'])\n    self.obj = obj\n    if hasattr(obj, 'items'):\n        obj_items = obj.items()\n    else:\n        obj_items = inspect.getmembers(obj)\n    if ((not white_list) and hasattr(obj, '__all__')):\n        white_list = obj.__all__\n    tests = (inspect.isbuiltin, inspect.isfunction, inspect.ismethod)\n    self.command_funcs = {\n        \n    }\n    for (obj_name, obj) in obj_items:\n        if obj_name.startswith('_'):\n            continue\n        if (not any((test(obj) for test in tests))):\n            continue\n        if ((white_list is not None) and (obj_name not in white_list)):\n            continue\n        if ((black_list is not None) and (obj_name in black_list)):\n            continue\n        if white_pattern:\n            match = white_pattern.match(obj_name)\n            if (not match):\n                continue\n            obj_name = match.group('name')\n        self.command_funcs[obj_name] = obj\n    self.default = default\n    if (len(self.command_funcs) == 1):\n        self.default = list(self.command_funcs.keys())[0]\n    self.ignore_help = ignore_help\n    self.ignore_return = ignore_return\n    self.name = (name or basename(sys.argv[0]))\n    self.doc = doc\n    self.debug = debug\n", "label": 1}
{"function": "\n\ndef __setitem__(self, key, value):\n    if self._set_slice(key, value, self):\n        return\n    if isinstance(value, tuple):\n        if (not (0 < len(value) <= 2)):\n            raise ValueError('A Header item may be set with either a scalar value, a 1-tuple containing a scalar value, or a 2-tuple containing a scalar value and comment string.')\n        if (len(value) == 1):\n            (value, comment) = (value[0], None)\n            if (value is None):\n                value = ''\n        elif (len(value) == 2):\n            (value, comment) = value\n            if (value is None):\n                value = ''\n            if (comment is None):\n                comment = ''\n    else:\n        comment = None\n    card = None\n    if isinstance(key, int):\n        card = self._cards[key]\n    elif isinstance(key, tuple):\n        card = self._cards[self._cardindex(key)]\n    if card:\n        card.value = value\n        if (comment is not None):\n            card.comment = comment\n        if card._modified:\n            self._modified = True\n    else:\n        self._update((key, value, comment))\n", "label": 1}
{"function": "\n\ndef writeBuffer(buff, col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall):\n    for x in buff:\n        printPreceedingCharacter = False\n        altOptions = x[col['alt']].split(',')\n        for y in altOptions:\n            if (((len(x[col['ref']]) != len(y)) or (len(x[col['ref']]) == 0) or (len(y) == 0)) and (not re.search('[^ATGCNatgcn\\\\.-]', y))):\n                printPreceedingCharacter = True\n                break\n        if printPreceedingCharacter:\n            writeRowCheck(x[:], col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall)\n    for x in buff:\n        printPreceedingCharacter = False\n        altOptions = x[col['alt']].split(',')\n        for y in altOptions:\n            if (((len(x[col['ref']]) != len(y)) or (len(x[col['ref']]) == 0) or (len(y) == 0)) and (not re.search('[^ATGCNatgcn\\\\.-]', y))):\n                printPreceedingCharacter = True\n                break\n        if (printPreceedingCharacter == False):\n            writeRowCheck(x[:], col, outputFile, contigSequence, chromosomeOffsets, exportRef, exportNoCall)\n", "label": 1}
{"function": "\n\ndef reset_settings(self):\n    for sgroup in self.settings['setting_groups']:\n        for setting in sgroup.values():\n            widget = self.find_child_by_name(setting.name)\n            if (widget is None):\n                continue\n            if ((setting.type == 'string') or (setting.type == 'file') or (setting.type == 'folder')):\n                old_val = ''\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val.replace('\\\\', '\\\\\\\\')\n                widget.setText(old_val)\n            elif (setting.type == 'strings'):\n                old_val = []\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = [v.replace('\\\\', '\\\\\\\\') for v in old_val]\n                widget.setText(','.join(setting.value))\n            elif (setting.type == 'check'):\n                old_val = False\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val\n                widget.setChecked(old_val)\n            elif (setting.type == 'range'):\n                old_val = 0\n                if (setting.default_value is not None):\n                    old_val = setting.default_value\n                setting.value = old_val\n                widget.setValue(old_val)\n", "label": 1}
{"function": "\n\ndef test_islice():\n    sl = SortedList(load=7)\n    assert ([] == list(sl.islice()))\n    values = list(range(53))\n    sl.update(values)\n    for start in range(53):\n        for stop in range(53):\n            assert (list(sl.islice(start, stop)) == values[start:stop])\n    for start in range(53):\n        for stop in range(53):\n            assert (list(sl.islice(start, stop, reverse=True)) == values[start:stop][::(- 1)])\n    for start in range(53):\n        assert (list(sl.islice(start=start)) == values[start:])\n        assert (list(sl.islice(start=start, reverse=True)) == values[start:][::(- 1)])\n    for stop in range(53):\n        assert (list(sl.islice(stop=stop)) == values[:stop])\n        assert (list(sl.islice(stop=stop, reverse=True)) == values[:stop][::(- 1)])\n", "label": 1}
{"function": "\n\ndef __doStemming(self, word, intact_word):\n    'Perform the actual word stemming\\n        '\n    valid_rule = re.compile('^([a-z]+)(\\\\*?)(\\\\d)([a-z]*)([>\\\\.]?)$')\n    proceed = True\n    while proceed:\n        last_letter_position = self.__getLastLetter(word)\n        if ((last_letter_position < 0) or (word[last_letter_position] not in self.rule_dictionary)):\n            proceed = False\n        else:\n            rule_was_applied = False\n            for rule in self.rule_dictionary[word[last_letter_position]]:\n                rule_match = valid_rule.match(rule)\n                if rule_match:\n                    (ending_string, intact_flag, remove_total, append_string, cont_flag) = rule_match.groups()\n                    remove_total = int(remove_total)\n                    if word.endswith(ending_string[::(- 1)]):\n                        if intact_flag:\n                            if ((word == intact_word) and self.__isAcceptable(word, remove_total)):\n                                word = self.__applyRule(word, remove_total, append_string)\n                                rule_was_applied = True\n                                if (cont_flag == '.'):\n                                    proceed = False\n                                break\n                        elif self.__isAcceptable(word, remove_total):\n                            word = self.__applyRule(word, remove_total, append_string)\n                            rule_was_applied = True\n                            if (cont_flag == '.'):\n                                proceed = False\n                            break\n            if (rule_was_applied == False):\n                proceed = False\n    return word\n", "label": 1}
{"function": "\n\ndef _result__repr__(self):\n    '\\n    This is used as the `__repr__` function for the :class:`Result`\\n    '\n    details = []\n    flags = self.__class__._fldprops\n    rcstr = 'rc=0x{0:X}'.format(self.rc)\n    if (self.rc != 0):\n        rcstr += '[{0}]'.format(self.errstr)\n    details.append(rcstr)\n    if ((flags & C.PYCBC_RESFLD_KEY) and hasattr(self, 'key')):\n        details.append('key={0}'.format(repr(self.key)))\n    if ((flags & C.PYCBC_RESFLD_VALUE) and hasattr(self, 'value')):\n        details.append('value={0}'.format(repr(self.value)))\n    if ((flags & C.PYCBC_RESFLD_CAS) and hasattr(self, 'cas')):\n        details.append('cas=0x{cas:x}'.format(cas=self.cas))\n    if ((flags & C.PYCBC_RESFLD_CAS) and hasattr(self, 'flags')):\n        details.append('flags=0x{flags:x}'.format(flags=self.flags))\n    if ((flags & C.PYCBC_RESFLD_HTCODE) and hasattr(self, 'http_status')):\n        details.append('http_status={0}'.format(self.http_status))\n    if ((flags & C.PYCBC_RESFLD_URL) and hasattr(self, 'url')):\n        details.append('url={0}'.format(self.url))\n    if hasattr(self, '_pycbc_repr_extra'):\n        details += self._pycbc_repr_extra()\n    ret = '{0}<{1}>'.format(self.__class__.__name__, ', '.join(details))\n    return ret\n", "label": 1}
{"function": "\n\ndef setup_platform(hass, config, add_devices, discovery_info=None):\n    'Setup the ISY994 platform.'\n    logger = logging.getLogger(__name__)\n    devs = []\n    if ((ISY is None) or (not ISY.connected)):\n        logger.error('A connection has not been made to the ISY controller.')\n        return False\n    for (path, node) in ISY.nodes:\n        if ((not node.dimmable) and (SENSOR_STRING not in node.name)):\n            if (HIDDEN_STRING in path):\n                node.name += HIDDEN_STRING\n            devs.append(ISYSwitchDevice(node))\n    for (folder_name, states) in (('HA.doors', [STATE_ON, STATE_OFF]), ('HA.switches', [STATE_ON, STATE_OFF])):\n        try:\n            folder = ISY.programs['My Programs'][folder_name]\n        except KeyError:\n            pass\n        else:\n            for (dtype, name, node_id) in folder.children:\n                if (dtype is 'folder'):\n                    custom_switch = folder[node_id]\n                    try:\n                        actions = custom_switch['actions'].leaf\n                        assert (actions.dtype == 'program'), 'Not a program'\n                        node = custom_switch['status'].leaf\n                    except (KeyError, AssertionError):\n                        pass\n                    else:\n                        devs.append(ISYProgramDevice(name, node, actions, states))\n    add_devices(devs)\n", "label": 1}
{"function": "\n\ndef _get_attrs(client_manager, parsed_args):\n    attrs = {\n        \n    }\n    if (parsed_args.name is not None):\n        attrs['name'] = str(parsed_args.name)\n    if parsed_args.enable:\n        attrs['admin_state_up'] = True\n    if parsed_args.disable:\n        attrs['admin_state_up'] = False\n    if parsed_args.share:\n        attrs['shared'] = True\n    if parsed_args.no_share:\n        attrs['shared'] = False\n    if (('project' in parsed_args) and (parsed_args.project is not None)):\n        identity_client = client_manager.identity\n        project_id = identity_common.find_project(identity_client, parsed_args.project, parsed_args.project_domain).id\n        attrs['tenant_id'] = project_id\n    if (('availability_zone_hints' in parsed_args) and (parsed_args.availability_zone_hints is not None)):\n        attrs['availability_zone_hints'] = parsed_args.availability_zone_hints\n    if parsed_args.internal:\n        attrs['router:external'] = False\n    if parsed_args.external:\n        attrs['router:external'] = True\n        if parsed_args.no_default:\n            attrs['is_default'] = False\n        if parsed_args.default:\n            attrs['is_default'] = True\n    if parsed_args.provider_network_type:\n        attrs['provider:network_type'] = parsed_args.provider_network_type\n    if parsed_args.physical_network:\n        attrs['provider:physical_network'] = parsed_args.physical_network\n    if parsed_args.segmentation_id:\n        attrs['provider:segmentation_id'] = parsed_args.segmentation_id\n    return attrs\n", "label": 1}
{"function": "\n\ndef validate_kwargs(func, kwargs):\n    'Validate arguments to be supplied to func.'\n    func_name = func.__name__\n    argspec = inspect.getargspec(func)\n    all_args = argspec.args[:]\n    defaults = list((argspec.defaults or []))\n    if (inspect.ismethod(func) and (all_args[:1] == ['self'])):\n        all_args[:1] = []\n    if defaults:\n        required = all_args[:(- len(defaults))]\n    else:\n        required = all_args[:]\n    trans = {arg: ((arg.endswith('_') and arg[:(- 1)]) or arg) for arg in all_args}\n    for key in list(kwargs):\n        key_adj = ('%s_' % key)\n        if (key_adj in all_args):\n            kwargs[key_adj] = kwargs.pop(key)\n    supplied = sorted(kwargs)\n    missing = [trans.get(arg, arg) for arg in required if (arg not in supplied)]\n    if missing:\n        raise MeteorError(400, func.err, ('Missing required arguments to %s: %s' % (func_name, ' '.join(missing))))\n    extra = [arg for arg in supplied if (arg not in all_args)]\n    if extra:\n        raise MeteorError(400, func.err, ('Unknown arguments to %s: %s' % (func_name, ' '.join(extra))))\n", "label": 1}
{"function": "\n\ndef testSubstitute1(self):\n    config = Configuration()\n    config.readfp(StringIO(CONFIG1))\n    assert config.has_section('section1')\n    assert (not config.has_section('section2'))\n    assert (not config.has_section('foo'))\n    assert (not config.has_section('bar'))\n    assert (not config.has_section('bar2'))\n    assert config.has_option('section1', 'foo')\n    assert config.has_option('section1', 'name')\n    assert (config.get('section1', 'name') == os.path.basename(sys.argv[0]))\n    assert (config.get('section1', 'cwd') == os.getcwd())\n    assert config.has_option('section1', 'bar')\n    assert config.has_option('section1', 'bar2')\n    assert (config.get('section1', 'foo') == 'bar')\n    assert (config.get('section1', 'bar') == 'bar')\n    assert (config.get('section1', 'bar2') == 'bar')\n", "label": 1}
{"function": "\n\ndef timeago(time=False):\n    \"\\n    Get a datetime object or a int() Epoch timestamp and return a\\n    pretty string like 'an hour ago', 'Yesterday', '3 months ago',\\n    'just now', etc\\n    \"\n    from datetime import datetime\n    now = datetime.now()\n    if (type(time) is int):\n        diff = (now - datetime.fromtimestamp(time))\n    elif isinstance(time, datetime):\n        diff = (now - time)\n    elif (not time):\n        diff = (now - now)\n    second_diff = diff.seconds\n    day_diff = diff.days\n    if (day_diff < 0):\n        return ''\n    if (day_diff == 0):\n        if (second_diff < 10):\n            return 'just now'\n        if (second_diff < 60):\n            return (str(second_diff) + ' seconds ago')\n        if (second_diff < 120):\n            return 'a minute ago'\n        if (second_diff < 3600):\n            return (str((second_diff / 60)) + ' minutes ago')\n        if (second_diff < 7200):\n            return 'an hour ago'\n        if (second_diff < 86400):\n            return (str((second_diff / 3600)) + ' hours ago')\n    if (day_diff == 1):\n        return 'Yesterday'\n    if (day_diff < 7):\n        return (str(day_diff) + ' days ago')\n    if (day_diff < 31):\n        return (str((day_diff / 7)) + ' weeks ago')\n    if (day_diff < 365):\n        return (str((day_diff / 30)) + ' months ago')\n    return (str((day_diff / 365)) + ' years ago')\n", "label": 1}
{"function": "\n\ndef test_insertComps_K1_D3(self, K=1, D=3):\n    A = ParamBag(K=K, D=D)\n    s = 123.456\n    A.setField('scalar', s, dims=None)\n    A.setField('N', [1.0], dims='K')\n    A.setField('x', np.random.rand(K, D), dims=('K', 'D'))\n    A.setField('xxT', np.random.rand(K, D, D), dims=('K', 'D', 'D'))\n    Abig = A.copy()\n    Abig.insertComps(A)\n    assert (Abig.K == 2)\n    assert np.allclose(Abig.N, np.hstack([A.N, A.N]))\n    assert (Abig.scalar == (2 * s))\n    assert (Abig.xxT.shape == (2, 3, 3))\n    assert np.allclose(Abig.xxT[0], A.xxT)\n    assert np.allclose(Abig.xxT[1], A.xxT)\n    Abig.insertComps(A)\n    assert (Abig.K == 3)\n    assert np.allclose(Abig.N, np.hstack([A.N, A.N, A.N]))\n    assert (Abig.scalar == (3 * s))\n    assert (Abig.xxT.shape == (3, 3, 3))\n    assert np.allclose(Abig.xxT[0], A.xxT)\n    assert np.allclose(Abig.xxT[1], A.xxT)\n    A.insertComps(Abig)\n    assert (A.K == 4)\n    assert (A.scalar == (4 * s))\n    assert np.allclose(A.N, np.hstack([1, 1, 1, 1]))\n", "label": 1}
{"function": "\n\ndef isSqlConnection(host, port, timeout=10, product=None):\n    t = 2\n    while (t < timeout):\n        try:\n            if ((product == 'postgres') and hasPostgres):\n                pgConnect(user='', host=host, port=int((port or 5432)), socket_timeout=t)\n            elif ((product == 'mysql') and hasMySql):\n                mysqlConnect(user='', host=host, port=int((port or 5432)), socket_timeout=t)\n            elif ((product == 'orcl') and hasOracle):\n                orclConnect = oracleConnect('{}/{}@{}:{}'.format('', '', host, (':{}'.format(port) if port else '')))\n            elif ((product == 'mssql') and hasMSSql):\n                mssqlConnect(user='', host=host, socket_timeout=t)\n            elif ((product == 'sqlite') and hasSQLite):\n                sqliteConnect('', t)\n        except (pgProgrammingError, mysqlProgrammingError, oracleDatabaseError, sqliteProgrammingError):\n            return True\n        except (pgInterfaceError, mysqlInterfaceError, oracleInterfaceError, mssqlOperationalError, mssqlInterfaceError, sqliteOperationalError, sqliteInterfaceError):\n            return False\n        except socket.timeout:\n            t = (t + 2)\n    return False\n", "label": 1}
{"function": "\n\ndef __call__(self, tag=None, ns=None, children=False, root=False, error=True):\n    'Search (even in child nodes) and return a child tag by name'\n    try:\n        if root:\n            return SimpleXMLElement(elements=[self.__document.documentElement], document=self.__document, namespace=self.__ns, prefix=self.__prefix, jetty=self.__jetty, namespaces_map=self.__namespaces_map)\n        if (tag is None):\n            return self.__iter__()\n        if children:\n            return self.children()\n        elements = None\n        if isinstance(tag, int):\n            elements = [self.__elements[tag]]\n        if (ns and (not elements)):\n            for ns_uri in ((isinstance(ns, (tuple, list)) and ns) or (ns,)):\n                elements = self._element.getElementsByTagNameNS(ns_uri, tag)\n                if elements:\n                    break\n        if (self.__ns and (not elements)):\n            elements = self._element.getElementsByTagNameNS(self.__ns, tag)\n        if (not elements):\n            elements = self._element.getElementsByTagName(tag)\n        if (not elements):\n            if error:\n                raise AttributeError('No elements found')\n            else:\n                return\n        return SimpleXMLElement(elements=elements, document=self.__document, namespace=self.__ns, prefix=self.__prefix, jetty=self.__jetty, namespaces_map=self.__namespaces_map)\n    except AttributeError as e:\n        raise AttributeError(('Tag not found: %s (%s)' % (tag, e)))\n", "label": 1}
{"function": "\n\ndef test():\n    pid = get_player('Tim', 'Duncan')\n    vs_pid = get_player('Stephen', 'Curry')\n    assert player.PlayerList()\n    assert player.PlayerSummary(pid)\n    assert player.PlayerLastNGamesSplits(pid)\n    assert player.PlayerInGameSplits(pid)\n    assert player.PlayerClutchSplits(pid)\n    assert player.PlayerPerformanceSplits(pid)\n    assert player.PlayerYearOverYearSplits(pid)\n    assert player.PlayerCareer(pid)\n    assert player.PlayerProfile(pid)\n    assert player.PlayerGameLogs(pid)\n    assert player.PlayerShotTracking(pid)\n    assert player.PlayerReboundTracking(pid)\n    assert player.PlayerPassTracking(pid)\n    assert player.PlayerDefenseTracking(pid)\n    assert player.PlayerVsPlayer(pid, vs_pid)\n", "label": 1}
{"function": "\n\ndef add_total_row(result, columns):\n    total_row = ([''] * len(columns))\n    has_percent = []\n    for row in result:\n        for (i, col) in enumerate(columns):\n            fieldtype = None\n            if isinstance(col, basestring):\n                col = col.split(':')\n                if (len(col) > 1):\n                    fieldtype = col[1]\n                    if ('/' in fieldtype):\n                        fieldtype = fieldtype.split('/')[0]\n            else:\n                fieldtype = col.get('fieldtype')\n            if ((fieldtype in ['Currency', 'Int', 'Float', 'Percent']) and flt(row[i])):\n                total_row[i] = (flt(total_row[i]) + flt(row[i]))\n            if ((fieldtype == 'Percent') and (i not in has_percent)):\n                has_percent.append(i)\n    for i in has_percent:\n        total_row[i] = (total_row[i] / len(result))\n    first_col_fieldtype = None\n    if isinstance(columns[0], basestring):\n        first_col = columns[0].split(':')\n        if (len(first_col) > 1):\n            first_col_fieldtype = first_col[1].split('/')[0]\n    else:\n        first_col_fieldtype = columns[0].get('fieldtype')\n    if (first_col_fieldtype not in ['Currency', 'Int', 'Float', 'Percent']):\n        if (first_col_fieldtype == 'Link'):\n            total_row[0] = ((\"'\" + _('Total')) + \"'\")\n        else:\n            total_row[0] = _('Total')\n    result.append(total_row)\n    return result\n", "label": 1}
{"function": "\n\ndef _check_fusion(self, root):\n    roots = root.table._root_tables()\n    validator = ExprValidator([root.table])\n    fused_exprs = []\n    can_fuse = False\n    resolved = _maybe_resolve_exprs(root.table, self.input_exprs)\n    if (not resolved):\n        return None\n    for val in resolved:\n        lifted_val = substitute_parents(val)\n        if (isinstance(val, ir.TableExpr) and (self.parent.op().is_ancestor(val) or ((len(roots) == 1) and (val._root_tables()[0] is roots[0])))):\n            can_fuse = True\n            have_root = False\n            for y in root.selections:\n                if y.equals(root.table):\n                    fused_exprs.append(root.table)\n                    have_root = True\n                    continue\n                fused_exprs.append(y)\n            if ((not have_root) and (len(root.selections) == 0)):\n                fused_exprs = ([root.table] + fused_exprs)\n        elif validator.validate(lifted_val):\n            can_fuse = True\n            fused_exprs.append(lifted_val)\n        elif (not validator.validate(val)):\n            can_fuse = False\n            break\n        else:\n            fused_exprs.append(val)\n    if can_fuse:\n        return ops.Selection(root.table, fused_exprs, predicates=root.predicates, sort_keys=root.sort_keys)\n    else:\n        return None\n", "label": 1}
{"function": "\n\ndef main(script):\n    'Tests the functions in this module.\\n\\n    script: string script name\\n    '\n    preg = ReadFemPreg()\n    print(preg.shape)\n    assert (len(preg) == 13593)\n    assert (preg.caseid[13592] == 12571)\n    assert (preg.pregordr.value_counts()[1] == 5033)\n    assert (preg.nbrnaliv.value_counts()[1] == 8981)\n    assert (preg.babysex.value_counts()[1] == 4641)\n    assert (preg.birthwgt_lb.value_counts()[7] == 3049)\n    assert (preg.birthwgt_oz.value_counts()[0] == 1037)\n    assert (preg.prglngth.value_counts()[39] == 4744)\n    assert (preg.outcome.value_counts()[1] == 9148)\n    assert (preg.birthord.value_counts()[1] == 4413)\n    assert (preg.agepreg.value_counts()[22.75] == 100)\n    assert (preg.totalwgt_lb.value_counts()[7.5] == 302)\n    weights = preg.finalwgt.value_counts()\n    key = max(weights.keys())\n    assert (preg.finalwgt.value_counts()[key] == 6)\n    print(('%s: All tests passed.' % script))\n", "label": 1}
{"function": "\n\ndef has_local_job_refs(io_hash):\n    '\\n    :param io_hash: input/output hash\\n    :type io_hash: dict\\n    :returns: boolean indicating whether any job-based object references are found in *io_hash*\\n    '\n    q = []\n    for field in io_hash:\n        if is_job_ref(io_hash[field]):\n            if get_job_from_jbor(io_hash[field]).startswith('localjob'):\n                return True\n        elif (isinstance(io_hash[field], list) or isinstance(io_hash[field], dict)):\n            q.append(io_hash[field])\n    while (len(q) > 0):\n        thing = q.pop()\n        if isinstance(thing, list):\n            for i in range(len(thing)):\n                if is_job_ref(thing[i]):\n                    if get_job_from_jbor(thing[i]).startswith('localjob'):\n                        return True\n                elif (isinstance(thing[i], list) or isinstance(thing[i], dict)):\n                    q.append(thing[i])\n        else:\n            for field in thing:\n                if is_job_ref(thing[field]):\n                    if get_job_from_jbor(thing[field]).startswith('localjob'):\n                        return True\n                elif (isinstance(thing[field], list) or isinstance(thing[field], dict)):\n                    q.append(thing[field])\n    return False\n", "label": 1}
{"function": "\n\ndef marshall(self, name, value, add_child=True, add_comments=False, ns=False, add_children_ns=True):\n    'Analyze python value and add the serialized XML element using tag name'\n    name = self._update_ns(name)\n    if isinstance(value, dict):\n        child = ((add_child and self.add_child(name, ns=ns)) or self)\n        for (k, v) in value.items():\n            if (not add_children_ns):\n                ns = False\n            else:\n                ns = getattr(value, 'namespace', None)\n            child.marshall(k, v, add_comments=add_comments, ns=ns)\n    elif isinstance(value, tuple):\n        child = ((add_child and self.add_child(name, ns=ns)) or self)\n        if (not add_children_ns):\n            ns = False\n        for (k, v) in value:\n            getattr(self, name).marshall(k, v, add_comments=add_comments, ns=ns)\n    elif isinstance(value, list):\n        child = self.add_child(name, ns=ns)\n        if (not add_children_ns):\n            ns = False\n        if add_comments:\n            child.add_comment('Repetitive array of:')\n        for t in value:\n            child.marshall(name, t, False, add_comments=add_comments, ns=ns)\n    elif isinstance(value, basestring):\n        self.add_child(name, value, ns=ns)\n    elif (value is None):\n        self.add_child(name, ns=ns)\n    elif (value in TYPE_MAP.keys()):\n        child = self.add_child(name, ns=ns)\n        child.add_comment(TYPE_MAP[value])\n    else:\n        fn = TYPE_MARSHAL_FN.get(type(value), str)\n        self.add_child(name, fn(value), ns=ns)\n", "label": 1}
{"function": "\n\ndef __repr__(self):\n    outcols = []\n    if self.rgb:\n        outcols.append(self.rgb)\n    if (self.thickEnd or outcols):\n        outcols.append((self.thickEnd if self.thickEnd else self.end))\n    if (self.thickStart or outcols):\n        outcols.append((self.thickStart if self.thickStart else self.start))\n    if (self.strand or outcols):\n        outcols.append(self.strand)\n    if ((self.score_int != '') or outcols):\n        outcols.append(self.score_int)\n    if (self.name or outcols):\n        outcols.append(self.name)\n    outcols.append(self.end)\n    outcols.append(self.start)\n    outcols.append(self.chrom)\n    return '\\t'.join([str(x) for x in outcols[::(- 1)]])\n", "label": 1}
{"function": "\n\ndef assertMessage(self, response, content, level=None, tags=None, limit=None):\n    '\\n        Asserts that the response has a particular message in its context.\\n        \\n        If limit is provided, checks that there are at most the specified\\n        number of messages.\\n        \\n        If level or tags are specified, checks for existence of message having\\n        matching content, level and/or tags. Otherwise, it simply checks for a\\n        message with the content.\\n        \\n        '\n    self.assertTrue((hasattr(response, 'context') and response.context), 'The response must have a non-empty context attribute.')\n    messages = list((response.context['messages'] if ('messages' in response.context) else []))\n    self.assertTrue(bool(messages), \"The response's context must contain at least one message.\")\n    if limit:\n        self.assertGreaterEqual(limit, len(messages), \"The response's context must have at most {limit:d} messages, but it has {actual:d} messages.\".format(limit=limit, actual=len(messages)))\n    self.assertTrue(any((((message.message == content) and ((not level) or (message.level == level)) and ((not tags) or (set((tag.strip() for tag in (message.tags or '').split(' ') if tag)) == set((tag.strip() for tag in (tags or '').split(' ') if tag))))) for message in messages)), \"A message matching the content, level and tags was not found in the response's context.\")\n", "label": 1}
{"function": "\n\ndef iterate_over_form(job, form, function, prefix=['form'], indent=''):\n    warnings = False\n    if (not hasattr(form, '__dict__')):\n        return False\n    whitelist_fields = ['BooleanField', 'FloatField', 'HiddenField', 'IntegerField', 'RadioField', 'SelectField', 'SelectMultipleField', 'StringField', 'TextAreaField', 'TextField', 'MultiIntegerField', 'MultiFloatField']\n    blacklist_fields = ['FileField', 'SubmitField']\n    for attr_name in vars(form):\n        if ((attr_name == 'csrf_token') or (attr_name == 'flags')):\n            continue\n        attr = getattr(form, attr_name)\n        if isinstance(attr, object):\n            if isinstance(attr, SubmitField):\n                continue\n            warnings |= iterate_over_form(job, attr, function, (prefix + [attr_name]), (indent + '    '))\n        if (hasattr(attr, 'data') and hasattr(attr, 'type')):\n            if (isinstance(attr.data, int) or isinstance(attr.data, float) or isinstance(attr.data, basestring) or (attr.type in whitelist_fields)):\n                key = ('%s.%s.data' % ('.'.join(prefix), attr_name))\n                warnings |= function(job, attr, key, attr.data)\n            if ((len(attr.type) > 5) and (attr.type[(- 5):] == 'Field') and (attr.type not in whitelist_fields) and (attr.type not in blacklist_fields)):\n                warnings |= add_warning(attr, ('Field type, %s, not cloned' % attr.type))\n    return warnings\n", "label": 1}
{"function": "\n\ndef _compose(self, public=None, private=None, no_cache=None, no_store=False, max_age=None, s_maxage=None, no_transform=False, **extensions):\n    assert isinstance(max_age, (type(None), int))\n    assert isinstance(s_maxage, (type(None), int))\n    expires = 0\n    result = []\n    if (private is True):\n        assert ((not public) and (not no_cache) and (not s_maxage))\n        result.append('private')\n    elif (no_cache is True):\n        assert ((not public) and (not private) and (not max_age))\n        result.append('no-cache')\n    else:\n        assert ((public is None) or (public is True))\n        assert ((not private) and (not no_cache))\n        expires = max_age\n        result.append('public')\n    if no_store:\n        result.append('no-store')\n    if no_transform:\n        result.append('no-transform')\n    if (max_age is not None):\n        result.append(('max-age=%d' % max_age))\n    if (s_maxage is not None):\n        result.append(('s-maxage=%d' % s_maxage))\n    for (k, v) in six.iteritems(extensions):\n        if (k not in self.extensions):\n            raise AssertionError((\"unexpected extension used: '%s'\" % k))\n        result.append(('%s=\"%s\"' % (k.replace('_', '-'), v)))\n    return (result, expires)\n", "label": 1}
{"function": "\n\ndef format(self, value):\n    '\\n        Formats a value.\\n\\n        @type value\\n          `float`\\n        '\n    sign = ('' if (self.__sign is None) else ('-' if (value < 0) else ('+' if (self.__sign == '+') else ' ')))\n    if math.isnan(value):\n        result = text.pad(self.__nan_str, self.__width, pad=' ', left=True)\n    elif ((value < 0) and (self.__sign is None)):\n        result = ('#' * self.__width)\n    elif math.isinf(value):\n        result = text.pad((sign + self.__inf_str), self.__width, pad=' ', left=True)\n    else:\n        precision = (0 if (self.__precision is None) else self.__precision)\n        rnd_value = round(value, precision)\n        abs_value = abs(rnd_value)\n        int_value = int(abs_value)\n        result = str(int_value)\n        if (len(result) > self.__size):\n            return ('#' * self.__width)\n        if (self.__pad == ' '):\n            result = text.pad((sign + result), (self.__size + len(sign)), pad=self.__pad, left=True)\n        else:\n            result = (sign + text.pad(result, self.__size, pad=self.__pad, left=True))\n        if (self.__precision is None):\n            pass\n        elif (self.__precision == 0):\n            result += self.__point\n        else:\n            frac = int(round(((abs_value - int_value) * self.__multiplier)))\n            frac = str(frac)\n            assert (len(frac) <= precision)\n            frac = text.pad(frac, self.__precision, pad='0', left=True)\n            result += (self.__point + frac)\n    return result\n", "label": 1}
{"function": "\n\ndef find(pattern, path='.', exclude=None, recursive=True):\n    'Find files that match *pattern* in *path*'\n    import fnmatch\n    import os\n    if recursive:\n        for (root, dirnames, filenames) in os.walk(path):\n            for pat in _to_list(pattern):\n                for filename in fnmatch.filter(filenames, pat):\n                    filepath = join(abspath(root), filename)\n                    for excl in _to_list(exclude):\n                        if (excl and fnmatch.fnmatch(filepath, excl)):\n                            break\n                    else:\n                        (yield filepath)\n    else:\n        for pat in _to_list(pattern):\n            for filename in fnmatch.filter(list(path), pat):\n                filepath = join(abspath(path), filename)\n                for excl in _to_list(exclude):\n                    if (excl and fnmatch.fnmatch(filepath, excl)):\n                        break\n                    else:\n                        (yield filepath)\n", "label": 1}
{"function": "\n\n@classmethod\ndef deserialize(cls, buf):\n    'Returns a Match object deserialized from a sequence of bytes.\\n\\n        Args:\\n            buf: A ReceiveBuffer object that contains the bytes that\\n                are the serialized form of the Match object.\\n\\n        Returns:\\n            A new Match object deserialized from the buffer.\\n\\n        Raises:\\n            ValueError: The buffer has an invalid number of available\\n                bytes, or some elements cannot be deserialized.\\n        '\n    (wildcards_ser, in_port, dl_src, dl_dst, dl_vlan, dl_vlan_pcp, dl_type, nw_tos, nw_proto, nw_src, nw_dst, tp_src, tp_dst) = buf.unpack(cls.FORMAT)\n    wildcards = Wildcards.deserialize(wildcards_ser)\n    if (nw_tos & 3):\n        nw_tos &= 252\n    nw_src_prefix_length = (32 - wildcards.nw_src)\n    nw_dst_prefix_length = (32 - wildcards.nw_dst)\n    return Match((None if wildcards.in_port else in_port), (None if wildcards.dl_src else dl_src), (None if wildcards.dl_dst else dl_dst), (None if wildcards.dl_vlan else dl_vlan), (None if wildcards.dl_vlan_pcp else dl_vlan_pcp), (None if wildcards.dl_type else dl_type), (None if wildcards.nw_tos else nw_tos), (None if wildcards.nw_proto else nw_proto), ((nw_src, nw_src_prefix_length) if (nw_src_prefix_length > 0) else None), ((nw_dst, nw_dst_prefix_length) if (nw_dst_prefix_length > 0) else None), (None if wildcards.tp_src else tp_src), (None if wildcards.tp_dst else tp_dst))\n", "label": 1}
{"function": "\n\ndef get_substrings(self, min_freq=2, check_positive=True, sort_by_length=False):\n    movetos = set()\n    for (idx, tok) in enumerate(self.rev_keymap):\n        if (isinstance(tok, basestring) and (tok[(- 6):] == 'moveto')):\n            movetos.add(idx)\n    try:\n        hmask = self.rev_keymap.index('hintmask')\n    except ValueError:\n        hmask = None\n    matches = {\n        \n    }\n    for (glyph_idx, program) in enumerate(self.data):\n        cur_start = 0\n        last_op = (- 1)\n        for (pos, tok) in enumerate(program):\n            if (tok in movetos):\n                stop = (last_op + 1)\n                if ((stop - cur_start) > 0):\n                    if (program[cur_start:stop] in matches):\n                        matches[program[cur_start:stop]].freq += 1\n                    else:\n                        span = pyCompressor.CandidateSubr((stop - cur_start), (glyph_idx, cur_start), 1, self.data, self.cost_map)\n                        matches[program[cur_start:stop]] = span\n                cur_start = (pos + 1)\n            elif (tok == hmask):\n                last_op = (pos + 1)\n            elif (type(self.rev_keymap[tok]) == str):\n                last_op = pos\n    constraints = (lambda s: ((s.freq >= min_freq) and ((s.subr_saving() > 0) or (not check_positive))))\n    self.substrings = filter(constraints, matches.values())\n    if sort_by_length:\n        self.substrings.sort(key=(lambda s: len(s)))\n    else:\n        self.substrings.sort(key=(lambda s: s.subr_saving()), reverse=True)\n    return self.substrings\n", "label": 1}
{"function": "\n\ndef _print_Mul(self, expr):\n    prec = precedence(expr)\n    (c, e) = expr.as_coeff_Mul()\n    if (c < 0):\n        expr = _keep_coeff((- c), e)\n        sign = '-'\n    else:\n        sign = ''\n    a = []\n    b = []\n    if (self.order not in ('old', 'none')):\n        args = expr.as_ordered_factors()\n    else:\n        args = Mul.make_args(expr)\n    for item in args:\n        if (item.is_commutative and item.is_Pow and item.exp.is_Rational and item.exp.is_negative):\n            if (item.exp != (- 1)):\n                b.append(Pow(item.base, (- item.exp), evaluate=False))\n            else:\n                b.append(Pow(item.base, (- item.exp)))\n        else:\n            a.append(item)\n    a = (a or [S.One])\n    a_str = [self.parenthesize(x, prec) for x in a]\n    b_str = [self.parenthesize(x, prec) for x in b]\n    if (len(b) == 0):\n        return (sign + '*'.join(a_str))\n    elif (len(b) == 1):\n        return (((sign + '*'.join(a_str)) + '/') + b_str[0])\n    else:\n        return ((sign + '*'.join(a_str)) + ('/(%s)' % '*'.join(b_str)))\n", "label": 1}
{"function": "\n\n@staticmethod\ndef merge(left, right, func):\n    if (left is right):\n        return left\n    if (left is None):\n        (left, right) = (right, left)\n    default = left.default\n    merge = _TreeListSub.merge\n    if (right is None):\n        direct = [func(x, default) for x in left.direct]\n        children = [merge(child, None, func) for child in left.children]\n        if ((direct == left.direct) and (children == left.children)):\n            return left\n        return _TreeListSub(default, direct, children)\n    direct = [func(x, y) for (x, y) in zip(left.direct, right.direct)]\n    children = [merge(c1, c2, func) for (c1, c2) in zip(left.children, right.children)]\n    if ((direct == left.direct) and (children == left.children)):\n        return left\n    if ((direct == right.direct) and (children == right.children)):\n        return right\n    return _TreeListSub(default, direct, children)\n", "label": 1}
{"function": "\n\ndef ui_complete_delete(self, parameters, text, current_param):\n    '\\n        Parameter auto-completion method for user command delete.\\n        @param parameters: Parameters on the command line.\\n        @type parameters: dict\\n        @param text: Current text of parameter being typed by the user.\\n        @type text: str\\n        @param current_param: Name of parameter to complete.\\n        @type current_param: str\\n        @return: Possible completions\\n        @rtype: list of str\\n        '\n    completions = []\n    portals = {\n        \n    }\n    all_ports = set([])\n    for portal in self.tpg.network_portals:\n        all_ports.add(str(portal.port))\n        portal_ip = portal.ip_address.strip('[]')\n        if (not (portal_ip in portals)):\n            portals[portal_ip] = []\n        portals[portal_ip].append(str(portal.port))\n    if (current_param == 'ip_address'):\n        completions = [addr for addr in portals if addr.startswith(text)]\n        if ('ip_port' in parameters):\n            port = parameters['ip_port']\n            completions = [addr for addr in completions if (port in portals[addr])]\n    elif (current_param == 'ip_port'):\n        if ('ip_address' in parameters):\n            addr = parameters['ip_address']\n            if (addr in portals):\n                completions = [port for port in portals[addr] if port.startswith(text)]\n        else:\n            completions = [port for port in all_ports if port.startswith(text)]\n    if (len(completions) == 1):\n        return [(completions[0] + ' ')]\n    else:\n        return completions\n", "label": 1}
{"function": "\n\ndef same_shape(self, x, y, dim_x=None, dim_y=None):\n    'Return True if we are able to assert that x and y have the\\n        same shape.\\n\\n        dim_x and dim_y are optional. If used, they should be an index\\n        to compare only 1 dimension of x and y.\\n\\n        '\n    sx = self.shape_of[x]\n    sy = self.shape_of[y]\n    if ((sx is None) or (sy is None)):\n        return False\n    if (dim_x is not None):\n        sx = [sx[dim_x]]\n    if (dim_y is not None):\n        sy = [sy[dim_y]]\n    assert (len(sx) == len(sy))\n    for (dx, dy) in zip(sx, sy):\n        if (dx is dy):\n            continue\n        if ((not dx.owner) or (not dy.owner)):\n            return False\n        if ((not isinstance(dx.owner.op, Shape_i)) or (not isinstance(dy.owner.op, Shape_i))):\n            return False\n        opx = dx.owner.op\n        opy = dy.owner.op\n        if (not (opx.i == opy.i)):\n            return False\n        if (dx.owner.inputs[0] == dy.owner.inputs[0]):\n            continue\n        from theano.scan_module.scan_utils import equal_computations\n        if (not equal_computations([dx], [dy])):\n            return False\n    return True\n", "label": 1}
{"function": "\n\ndef test_02_atom_init_with_statement(self):\n    s = Atom_Sword_Statement(ATOM_TEST_STATEMENT)\n    assert (len(s.states) == 1)\n    assert (len(s.original_deposits) == 1)\n    assert (len(s.resources) == 1)\n    assert (s.xml_document != None)\n    assert (s.dom != None)\n    assert s.parsed\n    assert s.valid\n    (uri, description) = s.states[0]\n    assert (uri == 'http://purl.org/net/sword/terms/state/Testing')\n    assert (description == 'The work has passed through review and is now in the archive')\n    t = datetime.strptime('2011-03-02T20:50:06Z', '%Y-%m-%dT%H:%M:%SZ')\n    entry = s.resources[0]\n    assert (len(entry.packaging) == 1)\n    assert (entry.deposited_by == 'sword')\n    assert (entry.deposited_on_behalf_of == 'jbloggs')\n    assert (entry.deposited_on == t)\n    assert (entry.uri == 'http://localhost:8080/part-IRI/43/my_deposit/example.zip')\n    assert (entry.packaging[0] == 'http://purl.org/net/sword/package/SimpleZip')\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    if (not [b for b in bases if isinstance(b, HideMetaOpts)]):\n        return super(HideMetaOpts, cls).__new__(cls, name, bases, attrs)\n    else:\n        meta_opts = deepcopy(cls.default_meta_opts)\n        if (('Meta' in attrs) and (attrs['Meta'].__module__ != 'django.db.models.query_utils')):\n            meta = attrs.get('Meta')\n        else:\n            for base in bases:\n                meta = getattr(base, '_meta', None)\n                if meta:\n                    break\n        if meta:\n            for (opt, value) in vars(meta).items():\n                if ((opt not in models.options.DEFAULT_NAMES) and (cls.hide_unknown_opts or (opt in meta_opts))):\n                    meta_opts[opt] = value\n                    delattr(meta, opt)\n        new_class = super(HideMetaOpts, cls).__new__(cls, name, bases, attrs)\n        if meta:\n            for opt in meta_opts:\n                setattr(meta, opt, meta_opts[opt])\n        for opt in meta_opts:\n            setattr(new_class._meta, opt, meta_opts[opt])\n        return new_class\n", "label": 1}
{"function": "\n\ndef cache_response(self, request, response, body=None):\n    '\\n        Algorithm for caching requests.\\n\\n        This assumes a requests Response object.\\n        '\n    if (response.status not in [200, 203, 300, 301]):\n        return\n    response_headers = CaseInsensitiveDict(response.headers)\n    cc_req = self.parse_cache_control(request.headers)\n    cc = self.parse_cache_control(response_headers)\n    cache_url = self.cache_url(request.url)\n    no_store = (cc.get('no-store') or cc_req.get('no-store'))\n    if (no_store and self.cache.get(cache_url)):\n        self.cache.delete(cache_url)\n    if (self.cache_etags and ('etag' in response_headers)):\n        self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n    elif (response.status == 301):\n        self.cache.set(cache_url, self.serializer.dumps(request, response))\n    elif ('date' in response_headers):\n        if (cc and cc.get('max-age')):\n            if (int(cc['max-age']) > 0):\n                self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n        elif ('expires' in response_headers):\n            if response_headers['expires']:\n                self.cache.set(cache_url, self.serializer.dumps(request, response, body=body))\n", "label": 1}
{"function": "\n\ndef replicate_attributes(source, target, cache=None):\n    'Replicates common SQLAlchemy attributes from the `source` object to the\\n    `target` object.'\n    target_manager = manager_of_class(type(target))\n    column_attrs = set()\n    relationship_attrs = set()\n    relationship_columns = set()\n    for attr in manager_of_class(type(source)).attributes:\n        if (attr.key not in target_manager):\n            continue\n        target_attr = target_manager[attr.key]\n        if isinstance(attr.property, ColumnProperty):\n            assert isinstance(target_attr.property, ColumnProperty)\n            column_attrs.add(attr)\n        elif isinstance(attr.property, RelationshipProperty):\n            assert isinstance(target_attr.property, RelationshipProperty)\n            relationship_attrs.add(attr)\n            if (attr.property.direction is MANYTOONE):\n                relationship_columns.update(attr.property.local_columns)\n    for attr in column_attrs:\n        if _column_property_in_registry(attr.property, _excluded):\n            continue\n        elif ((not _column_property_in_registry(attr.property, _included)) and all(((column in relationship_columns) for column in attr.property.columns))):\n            continue\n        setattr(target, attr.key, getattr(source, attr.key))\n    for attr in relationship_attrs:\n        target_attr_model = target_manager[attr.key].property.argument\n        if (not is_relation_replicatable(attr)):\n            continue\n        replicate_relation(source, target, attr, target_manager[attr.key], cache=cache)\n", "label": 1}
{"function": "\n\ndef __getattribute__(self, name):\n    try:\n        result = super(GsxElement, self).__getattribute__(name)\n    except AttributeError:\n        \"\\n            The XML returned by GSX can be pretty inconsistent, especially\\n            between the different environments. It's therefore more\\n            practical to return None than to expect AttributeErrors all\\n            over your application.\\n            \"\n        return\n    if (name in STRING_TYPES):\n        return unicode((result.text or ''))\n    if isinstance(result, objectify.NumberElement):\n        return result.pyval\n    if isinstance(result, objectify.StringElement):\n        name = result.tag\n        result = (result.text or '')\n        result = unicode(result)\n        if (not result):\n            return\n        if (name in DATETIME_TYPES):\n            return gsx_datetime(result)\n        if (name in DIAGS_TIMESTAMP_TYPES):\n            return gsx_diags_timestamp(result)\n        if (name in BASE64_TYPES):\n            return gsx_attachment(result)\n        if (name in FLOAT_TYPES):\n            return gsx_price(result)\n        if name.endswith('Date'):\n            return gsx_date(result)\n        if name.endswith('Timestamp'):\n            return gsx_timestamp(result)\n        if re.search('^[YN]$', result):\n            return gsx_boolean(result)\n    return result\n", "label": 1}
{"function": "\n\ndef reverse(viewname, urlconf=None, args=None, kwargs=None, prefix=None, current_app=None):\n    if (urlconf is None):\n        urlconf = get_urlconf()\n    resolver = get_resolver(urlconf)\n    args = (args or [])\n    kwargs = (kwargs or {\n        \n    })\n    if (prefix is None):\n        prefix = get_script_prefix()\n    if (not isinstance(viewname, six.string_types)):\n        view = viewname\n    else:\n        parts = viewname.split(':')\n        parts.reverse()\n        view = parts[0]\n        path = parts[1:]\n        resolved_path = []\n        ns_pattern = ''\n        while path:\n            ns = path.pop()\n            try:\n                app_list = resolver.app_dict[ns]\n                if (current_app and (current_app in app_list)):\n                    ns = current_app\n                elif (ns not in app_list):\n                    ns = app_list[0]\n            except KeyError:\n                pass\n            try:\n                (extra, resolver) = resolver.namespace_dict[ns]\n                resolved_path.append(ns)\n                ns_pattern = (ns_pattern + extra)\n            except KeyError as key:\n                if resolved_path:\n                    raise NoReverseMatch((\"%s is not a registered namespace inside '%s'\" % (key, ':'.join(resolved_path))))\n                else:\n                    raise NoReverseMatch(('%s is not a registered namespace' % key))\n        if ns_pattern:\n            resolver = get_ns_resolver(ns_pattern, resolver)\n    return iri_to_uri(resolver._reverse_with_prefix(view, prefix, *args, **kwargs))\n", "label": 1}
{"function": "\n\ndef match(self, value=None, name=None):\n    nv = vv = False\n    if value:\n        if re.search(value, self.value):\n            vv = True\n        else:\n            vv = False\n    if name:\n        if re.search(name, self.name):\n            nv = True\n        else:\n            nv = False\n    if (name and value):\n        return (nv and vv)\n    if (name and (not value)):\n        return nv\n    if ((not name) and value):\n        return vv\n    if ((not name) and (not value)):\n        return False\n", "label": 1}
{"function": "\n\n@classmethod\ndef size(cls, val, str_cache=None, object_cache=None, traits_cache=None):\n    if (str_cache is None):\n        str_cache = []\n    if (object_cache is None):\n        object_cache = []\n    if (traits_cache is None):\n        traits_cache = []\n    size = U8.size\n    if (isinstance(val, bool) and (val in (False, True))):\n        pass\n    elif (val is None):\n        pass\n    elif isinstance(val, integer_types):\n        if ((val < AMF3_MIN_INTEGER) or (val > AMF3_MAX_INTEGER)):\n            size += AMF3Double.size\n        else:\n            size += AMF3Integer.size(val)\n    elif isinstance(val, float):\n        size += AMF3Double.size\n    elif isinstance(val, (AMF3Array, list)):\n        size += AMF3ArrayPacker.size(val, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif isinstance(val, string_types):\n        size += AMF3String.size(val, cache=str_cache)\n    elif isinstance(val, AMF3ObjectBase):\n        size += AMF3ObjectPacker.size(val, str_cache=str_cache, object_cache=object_cache, traits_cache=traits_cache)\n    elif isinstance(val, AMF3Date):\n        size += AMF3DatePacker.size(val, cache=object_cache)\n    else:\n        raise ValueError('Unable to pack value of type {0}'.format(type(val)))\n    return size\n", "label": 1}
{"function": "\n\ndef check_if_doc_is_dynamically_linked(doc, method='Delete'):\n    'Raise `frappe.LinkExistsError` if the document is dynamically linked'\n    for df in get_dynamic_link_map().get(doc.doctype, []):\n        if (df.parent in ('Communication', 'ToDo', 'DocShare', 'Email Unsubscribe')):\n            continue\n        meta = frappe.get_meta(df.parent)\n        if meta.issingle:\n            refdoc = frappe.db.get_singles_dict(df.parent)\n            if ((refdoc.get(df.options) == doc.doctype) and (refdoc.get(df.fieldname) == doc.name) and (((method == 'Delete') and (refdoc.docstatus < 2)) or ((method == 'Cancel') and (refdoc.docstatus == 1)))):\n                frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, df.parent, ''), frappe.LinkExistsError)\n        else:\n            for refdoc in frappe.db.sql('select name, docstatus from `tab{parent}` where\\n\\t\\t\\t\\t{options}=%s and {fieldname}=%s'.format(**df), (doc.doctype, doc.name), as_dict=True):\n                if (((method == 'Delete') and (refdoc.docstatus < 2)) or ((method == 'Cancel') and (refdoc.docstatus == 1))):\n                    frappe.throw(_('Cannot delete or cancel because {0} {1} is linked with {2} {3}').format(doc.doctype, doc.name, df.parent, refdoc.name), frappe.LinkExistsError)\n", "label": 1}
{"function": "\n\ndef MergeFrom(self, x):\n    assert (x is not self)\n    if x.has_start_timestamp_milliseconds():\n        self.set_start_timestamp_milliseconds(x.start_timestamp_milliseconds())\n    if x.has_http_method():\n        self.set_http_method(x.http_method())\n    if x.has_http_path():\n        self.set_http_path(x.http_path())\n    if x.has_http_query():\n        self.set_http_query(x.http_query())\n    if x.has_http_status():\n        self.set_http_status(x.http_status())\n    if x.has_duration_milliseconds():\n        self.set_duration_milliseconds(x.duration_milliseconds())\n    if x.has_api_mcycles():\n        self.set_api_mcycles(x.api_mcycles())\n    if x.has_processor_mcycles():\n        self.set_processor_mcycles(x.processor_mcycles())\n    for i in xrange(x.rpc_stats_size()):\n        self.add_rpc_stats().CopyFrom(x.rpc_stats(i))\n    for i in xrange(x.cgi_env_size()):\n        self.add_cgi_env().CopyFrom(x.cgi_env(i))\n    if x.has_overhead_walltime_milliseconds():\n        self.set_overhead_walltime_milliseconds(x.overhead_walltime_milliseconds())\n    if x.has_user_email():\n        self.set_user_email(x.user_email())\n    if x.has_is_admin():\n        self.set_is_admin(x.is_admin())\n    for i in xrange(x.individual_stats_size()):\n        self.add_individual_stats().CopyFrom(x.individual_stats(i))\n", "label": 1}
{"function": "\n\ndef _rule_to_post_data(rule):\n    post_data = {\n        \n    }\n    post_data['ruleType'] = rule['ruleType']\n    if ('filter' in rule):\n        filter = rule['filter']\n        if ('httpProtocol' in filter):\n            post_data['filterProtocol'] = filter['httpProtocol']\n        if ('method' in filter):\n            post_data['filterMethod'] = filter['method']\n        if ('url' in filter):\n            post_data['filterUrl'] = filter['url']\n        if ('statusCode' in filter):\n            post_data['filterstatusCode'] = filter['statusCode']\n    if ('action' in rule):\n        action = rule['action']\n        if ('type' in action):\n            post_data['actionType'] = action['type']\n        if ('httpProtocol' in action):\n            post_data['actionProtocol'] = action['httpProtocol']\n        if ('method' in action):\n            post_data['actionMethod'] = action['method']\n        if ('url' in action):\n            post_data['actionUrl'] = action['url']\n        if ('statusCode' in action):\n            post_data['actionStatusCode'] = action['statusCode']\n        if ('statusDescription' in action):\n            post_data['actionStatusDescription'] = action['statusDescription']\n        if ('payload' in action):\n            post_data['actionPayload'] = action['payload']\n        if ('setHeaders' in action):\n            action['headers'] = action.pop('setHeaders')\n    return post_data\n", "label": 1}
{"function": "\n\ndef publish(self):\n    super(PyFS, self).publish()\n    deploy_fs = OSFS(self.site.config.deploy_root_path.path)\n    for (dirnm, local_filenms) in deploy_fs.walk():\n        logger.info('Making directory: %s', dirnm)\n        self.fs.makedir(dirnm, allow_recreate=True)\n        remote_fileinfos = self.fs.listdirinfo(dirnm, files_only=True)\n        for filenm in local_filenms:\n            filepath = pathjoin(dirnm, filenm)\n            for (nm, info) in remote_fileinfos:\n                if (nm == filenm):\n                    break\n            else:\n                info = {\n                    \n                }\n            if (self.check_etag and ('etag' in info)):\n                with deploy_fs.open(filepath, 'rb') as f:\n                    local_etag = self._calculate_etag(f)\n                if (info['etag'] == local_etag):\n                    logger.info('Skipping file [etag]: %s', filepath)\n                    continue\n            if (self.check_mtime and ('modified_time' in info)):\n                local_mtime = deploy_fs.getinfo(filepath)['modified_time']\n                if (info['modified_time'] > local_mtime):\n                    logger.info('Skipping file [mtime]: %s', filepath)\n                    continue\n            logger.info('Uploading file: %s', filepath)\n            with deploy_fs.open(filepath, 'rb') as f:\n                self.fs.setcontents(filepath, f)\n        for (filenm, info) in remote_fileinfos:\n            filepath = pathjoin(dirnm, filenm)\n            if (filenm not in local_filenms):\n                logger.info('Removing file: %s', filepath)\n                self.fs.remove(filepath)\n", "label": 1}
{"function": "\n\ndef _ask(self, stage, args, tag):\n    self._report_driver.report_sync(tag, report_type=stage)\n    while True:\n        answer = raw_input('Continue? ([d]etailed/[C]oncise report,[y]es,[n]o,[r]etry): ')\n        if ((not answer) or (answer == 'c') or (answer == 'C')):\n            self._report_driver.report_sync(tag, report_type=stage)\n        elif ((answer == 'd') or (answer == 'D')):\n            self._report_driver.report_sync(tag, report_type=stage, detailed=True)\n        elif ((answer == 'Y') or (answer == 'y')):\n            return True\n        elif ((answer == 'N') or (answer == 'n')):\n            return False\n        elif ((answer == 'R') or (answer == 'r')):\n            if (stage == 'fetch'):\n                self._fetch(args)\n            if (stage == 'checkout'):\n                self._checkout(args)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRUCT):\n                self.privilege = TSentryPrivilege()\n                self.privilege.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef group_members(self, r, writes, owner):\n    if (r.isOpen('r') == 'r'):\n        gr = ''.join((('-%s' % e) for e in r.exceptions if (not (e == owner))))\n    else:\n        gr = '+'.join((e for e in r.exceptions if (not (e == owner))))\n    gw = []\n    for w in writes:\n        if (w.isOpen('w') == 'w'):\n            g = ''.join((('-%s' % e) for e in w.exceptions if (not (e == owner))))\n        else:\n            g = '+'.join((e for e in w.exceptions if (not (e == owner))))\n        gw.append(g)\n    gw = (gw[0] if all(((w == gw[0]) for w in gw)) else '<variable>')\n    if (gw == ''):\n        gw = '(world)'\n    if (gr == ''):\n        gr = '(world)'\n    if (gw == gr):\n        gs = gr\n    else:\n        gs = ('r:%s  w:%s' % (gr, gw))\n    return gs\n", "label": 1}
{"function": "\n\ndef unsubscribe(self, subscriber, timeout=None):\n    \"Must be called with 'yield' as, for example,\\n        'yield channel.unsubscribe(coro)'.\\n\\n        Future messages will not be delivered after unsubscribing.\\n\\n        Can also be used on remote channels.\\n        \"\n    if ((not isinstance(subscriber, Coro)) and (not isinstance(subscriber, Channel))):\n        logger.warning('invalid subscriber ignored')\n        raise StopIteration((- 1))\n    if (self._location == Channel._asyncoro._location):\n        if (subscriber._location != self._location):\n            if isinstance(subscriber, Coro):\n                subscriber._id = int(subscriber._id)\n                for s in self._subscribers:\n                    if (isinstance(s, Coro) and (s._id == subscriber._id) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n            elif isinstance(subscriber, Channel):\n                for s in self._subscribers:\n                    if (isinstance(s, Channel) and (s._name == subscriber._name) and (s._location == subscriber._location)):\n                        subscriber = s\n                        break\n        try:\n            self._subscribers.remove(subscriber)\n        except KeyError:\n            reply = (- 1)\n        else:\n            reply = 0\n    else:\n        kwargs = {\n            'channel': self._name,\n        }\n        kwargs['subscriber'] = subscriber\n        request = _NetRequest('unsubscribe', kwargs=kwargs, dst=self._location, timeout=timeout)\n        reply = (yield Channel._asyncoro._sync_reply(request))\n    raise StopIteration(reply)\n", "label": 1}
{"function": "\n\ndef data(self, index, role=Qt.DisplayRole):\n    if ((not index.isValid()) or (not (0 <= index.row() < len(self.packages)))):\n        return to_qvariant()\n    package = self.packages[index.row()]\n    column = index.column()\n    if ((role == Qt.CheckStateRole) and (column == CHECK)):\n        return to_qvariant((package in self.checked))\n    elif (role == Qt.DisplayRole):\n        if (column == NAME):\n            return to_qvariant(package.name)\n        elif (column == VERSION):\n            return to_qvariant(package.version)\n        elif (column == ACTION):\n            action = self.actions.get(package)\n            if (action is not None):\n                return to_qvariant(action)\n        elif (column == DESCRIPTION):\n            return to_qvariant(package.description)\n    elif (role == Qt.TextAlignmentRole):\n        if (column == ACTION):\n            return to_qvariant(int((Qt.AlignRight | Qt.AlignVCenter)))\n        else:\n            return to_qvariant(int((Qt.AlignLeft | Qt.AlignVCenter)))\n    elif (role == Qt.BackgroundColorRole):\n        if (package in self.checked):\n            color = QColor(Qt.darkGreen)\n            color.setAlphaF(0.1)\n            return to_qvariant(color)\n        else:\n            color = QColor(Qt.lightGray)\n            color.setAlphaF(0.3)\n            return to_qvariant(color)\n    return to_qvariant()\n", "label": 1}
{"function": "\n\ndef _get_focus_next(self, focus_dir):\n    current = self\n    walk_tree = ('walk' if (focus_dir is 'focus_next') else 'walk_reverse')\n    while 1:\n        while (getattr(current, focus_dir) is not None):\n            current = getattr(current, focus_dir)\n            if ((current is self) or (current is StopIteration)):\n                return None\n            if (current.is_focusable and (not current.disabled)):\n                return current\n        itr = getattr(current, walk_tree)(loopback=True)\n        if (focus_dir is 'focus_next'):\n            next(itr)\n        for current in itr:\n            if isinstance(current, FocusBehavior):\n                break\n        if isinstance(current, FocusBehavior):\n            if (current is self):\n                return None\n            if (current.is_focusable and (not current.disabled)):\n                return current\n        else:\n            return None\n", "label": 1}
{"function": "\n\ndef _create_result(cmd, params):\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ)\n    result = Result()\n    for line in p.stdout.readlines():\n        if (sys.version_info[0] == 3):\n            line = line.decode(sys.stdout.encoding)\n        result._add_stdout_line(line)\n    for line in p.stderr.readlines():\n        if (sys.version_info[0] == 3):\n            line = line.decode(sys.stderr.encoding)\n        result._add_stderr_line(line)\n    p.wait()\n    if ((_is_param_set(params, _PARAM_PRINT_STDOUT) or config.PRINT_STDOUT_ALWAYS) and (len(result.stdout) > 0)):\n        _print_stdout(result.stdout)\n    if ((_is_param_set(params, _PARAM_PRINT_STDERR) or config.PRINT_STDERR_ALWAYS) and (len(result.stderr) > 0)):\n        if _is_colorama_enabled():\n            _print_stderr(((Fore.RED + result.stderr) + Style.RESET_ALL))\n        else:\n            _print_stderr(result.stderr)\n    result.returncode = p.returncode\n    if ((p.returncode != 0) and (not _is_param_set(params, _PARAM_NO_THROW))):\n        raise NonZeroReturnCodeError(cmd, result)\n    return result\n", "label": 1}
{"function": "\n\ndef test_simple(self):\n    foo = 'aaa\\naaa\\naaa\\n'\n    result = list(chunked(foo, 5))\n    assert (len(result) == 3)\n    assert (result[0] == 'aaa\\n')\n    assert (result[1] == 'aaa\\n')\n    assert (result[2] == 'aaa\\n')\n    result = list(chunked(foo, 8))\n    assert (len(result) == 2)\n    assert (result[0] == 'aaa\\naaa\\n')\n    assert (result[1] == 'aaa\\n')\n    result = list(chunked(foo, 4))\n    assert (len(result) == 3)\n    assert (result[0] == 'aaa\\n')\n    assert (result[1] == 'aaa\\n')\n    assert (result[2] == 'aaa\\n')\n    foo = ('a' * 10)\n    result = list(chunked(foo, 2))\n    assert (len(result) == 5)\n    assert all(((r == 'aa') for r in result))\n    foo = 'aaaa\\naaaa'\n    result = list(chunked(foo, 3))\n    assert (len(result) == 4)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.roleName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.SET):\n                self.groups = set()\n                (_etype17, _size14) = iprot.readSetBegin()\n                for _i18 in xrange(_size14):\n                    _elem19 = iprot.readString()\n                    self.groups.add(_elem19)\n                iprot.readSetEnd()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kw):\n    super(DateConverter, self).__init__(*args, **kw)\n    month_style = (self.month_style or DateConverter.month_style).lower()\n    accept_day = bool(self.accept_day)\n    self.accept_day = self.accept_day\n    if (month_style in ('mdy', 'md', 'mm/dd/yyyy', 'mm/dd', 'us', 'american')):\n        month_style = 'mdy'\n    elif (month_style in ('dmy', 'dm', 'dd/mm/yyyy', 'dd/mm', 'euro', 'european')):\n        month_style = 'dmy'\n    elif (month_style in ('ymd', 'ym', 'yyyy/mm/dd', 'yyyy/mm', 'iso', 'china', 'chinese')):\n        month_style = 'ymd'\n    else:\n        raise TypeError(('Bad month_style: %r' % month_style))\n    self.month_style = month_style\n    separator = self.separator\n    if ((not separator) or (separator == 'auto')):\n        separator = dict(mdy='/', dmy='.', ymd='-')[month_style]\n    elif (separator not in ('-', '.', '/', '\\\\')):\n        raise TypeError(('Bad separator: %r' % separator))\n    self.separator = separator\n    self.format = separator.join((self._formats[part] for part in month_style if ((part != 'd') or accept_day)))\n    self.human_format = separator.join((self._human_formats[part] for part in month_style if ((part != 'd') or accept_day)))\n", "label": 1}
{"function": "\n\ndef _af_pow(a, n):\n    '\\n    Routine for finding powers of a permutation.\\n\\n    Examples\\n    ========\\n\\n    >>> from sympy.combinatorics.permutations import Permutation, _af_pow\\n    >>> Permutation.print_cyclic = False\\n    >>> p = Permutation([2, 0, 3, 1])\\n    >>> p.order()\\n    4\\n    >>> _af_pow(p._array_form, 4)\\n    [0, 1, 2, 3]\\n    '\n    if (n == 0):\n        return list(range(len(a)))\n    if (n < 0):\n        return _af_pow(_af_invert(a), (- n))\n    if (n == 1):\n        return a[:]\n    elif (n == 2):\n        b = [a[i] for i in a]\n    elif (n == 3):\n        b = [a[a[i]] for i in a]\n    elif (n == 4):\n        b = [a[a[a[i]]] for i in a]\n    else:\n        b = list(range(len(a)))\n        while 1:\n            if (n & 1):\n                b = [b[i] for i in a]\n                n -= 1\n                if (not n):\n                    break\n            if ((n % 4) == 0):\n                a = [a[a[a[i]]] for i in a]\n                n = (n // 4)\n            elif ((n % 2) == 0):\n                a = [a[i] for i in a]\n                n = (n // 2)\n    return b\n", "label": 1}
{"function": "\n\ndef _parse_see_also(self, content):\n    '\\n        func_name : Descriptive text\\n            continued text\\n        another_func_name : Descriptive text\\n        func_name1, func_name2, func_name3\\n\\n        '\n    functions = []\n    current_func = None\n    rest = []\n    for line in content:\n        if (not line.strip()):\n            continue\n        if (':' in line):\n            if current_func:\n                functions.append((current_func, rest))\n            r = line.split(':', 1)\n            current_func = r[0].strip()\n            r[1] = r[1].strip()\n            if r[1]:\n                rest = [r[1]]\n            else:\n                rest = []\n        elif (not line.startswith(' ')):\n            if current_func:\n                functions.append((current_func, rest))\n                current_func = None\n                rest = []\n            if (',' in line):\n                for func in line.split(','):\n                    func = func.strip()\n                    if func:\n                        functions.append((func, []))\n            elif line.strip():\n                current_func = line.strip()\n        elif (current_func is not None):\n            rest.append(line.strip())\n    if current_func:\n        functions.append((current_func, rest))\n    return functions\n", "label": 1}
{"function": "\n\ndef _subx(self, template, string, count=0, subn=False):\n    filter = template\n    if ((not callable(template)) and ('\\\\' in template)):\n        import re as sre\n        filter = sre._subx(self, template)\n    state = _State(string, 0, sys.maxsize, self.flags)\n    sublist = []\n    n = last_pos = 0\n    while ((not count) or (n < count)):\n        state.reset()\n        state.string_position = state.start\n        if (not state.search(self._code)):\n            break\n        if (last_pos < state.start):\n            sublist.append(string[last_pos:state.start])\n        if (not ((last_pos == state.start) and (last_pos == state.string_position) and (n > 0))):\n            if callable(filter):\n                sublist.append(filter(SRE_Match(self, state)))\n            else:\n                sublist.append(filter)\n            last_pos = state.string_position\n            n += 1\n        if (state.string_position == state.start):\n            state.start += 1\n        else:\n            state.start = state.string_position\n    if (last_pos < state.end):\n        sublist.append(string[last_pos:state.end])\n    item = ''.join(sublist)\n    if subn:\n        return (item, n)\n    else:\n        return item\n", "label": 1}
{"function": "\n\ndef normpath(path):\n    'Normalize path, eliminating double slashes, etc.'\n    (slash, dot) = (('/', '.') if isinstance(path, unicode) else ('/', '.'))\n    if (path == ''):\n        return dot\n    initial_slashes = path.startswith('/')\n    if (initial_slashes and path.startswith('//') and (not path.startswith('///'))):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if (comp in ('', '.')):\n            continue\n        if ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(- 1)] == '..'))):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = ((slash * initial_slashes) + path)\n    return (path or dot)\n", "label": 1}
{"function": "\n\ndef __parse_number(self):\n    'Parse a numeric literal.'\n    literal_buffer = []\n    all_digits = True\n    while (not self.__scanner.at_end):\n        peek = self.__scanner.peek_next_ubyte()\n        if ((peek != '+') and (peek != '-') and (peek != 'e') and (peek != 'E') and (peek != '.') and (not ('0' <= peek <= '9'))):\n            break\n        if (len(literal_buffer) >= 100):\n            self.__error('numeric literal too long (limit 100 characters)')\n        next_char = self.__scanner.read_ubyte()\n        all_digits = (all_digits and ('0' <= next_char <= '9'))\n        literal_buffer.append(next_char)\n    if (all_digits and (len(literal_buffer) <= 18)):\n        value = 0\n        for digit in literal_buffer:\n            value = (((value * 10) + ord(digit)) - ord('0'))\n        return value\n    number_string = ''.join(literal_buffer)\n    if ((number_string.find('.') < 0) and (number_string.find('e') < 0) and (number_string.find('E') < 0)):\n        try:\n            return long(number_string)\n        except ValueError:\n            self.__error((\"Could not parse number as long '%s'\" % number_string))\n    else:\n        try:\n            return float(number_string)\n        except ValueError:\n            self.__error((\"Could not parse number as float '%s'\" % number_string))\n", "label": 1}
{"function": "\n\ndef _prepare_imports(self, dicts):\n    ' an override for prepare imports that sorts the imports by parent_id dependencies '\n    pseudo_ids = set()\n    pseudo_matches = {\n        \n    }\n    prepared = dict(super(OrganizationImporter, self)._prepare_imports(dicts))\n    for (_, data) in prepared.items():\n        parent_id = (data.get('parent_id', None) or '')\n        if parent_id.startswith('~'):\n            pseudo_ids.add(parent_id)\n    pseudo_ids = [(ppid, get_pseudo_id(ppid)) for ppid in pseudo_ids]\n    for (json_id, data) in prepared.items():\n        for (ppid, spec) in pseudo_ids:\n            match = True\n            for (k, v) in spec.items():\n                if (data[k] != v):\n                    match = False\n                    break\n            if match:\n                if (ppid in pseudo_matches):\n                    raise UnresolvedIdError(('multiple matches for pseudo id: ' + ppid))\n                pseudo_matches[ppid] = json_id\n    network = Network()\n    in_network = set()\n    import_order = []\n    for (json_id, data) in prepared.items():\n        parent_id = data.get('parent_id', None)\n        if (parent_id in pseudo_matches):\n            parent_id = pseudo_matches[parent_id]\n        network.add_node(json_id)\n        if parent_id:\n            network.add_edge(parent_id, json_id)\n    for jid in network.sort():\n        import_order.append((jid, prepared[jid]))\n        in_network.add(jid)\n    if (in_network != set(prepared.keys())):\n        raise PupaInternalError('import is missing nodes in network set')\n    return import_order\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    if (key is ()):\n        return self\n    data_slice = None\n    if isinstance(key, tuple):\n        data_slice = key[1:]\n        key = key[0]\n    if (isinstance(key, int) and (key <= len(self))):\n        if (key == 0):\n            data = self.main\n        if (key == 1):\n            data = self.right\n        if (key == 2):\n            data = self.top\n        if data_slice:\n            data = data[data_slice]\n        return data\n    elif (isinstance(key, str) and (key in self.data)):\n        if (data_slice is None):\n            return self.data[key]\n        else:\n            self.data[key][data_slice]\n    elif (isinstance(key, slice) and (key.start is None) and (key.stop is None)):\n        return (self if (data_slice is None) else self.clone([el[data_slice] for el in self]))\n    else:\n        raise KeyError('Key {0} not found in AdjointLayout.'.format(key))\n", "label": 1}
{"function": "\n\ndef job_logs(self, job, delay=5):\n    'Job log generator.\\n\\n    :param job: job name\\n    :param delay: time in seconds between each server poll\\n\\n    Yields line by line.\\n\\n    '\n    finishing = False\n    offset = 0\n    while True:\n        try:\n            logs = self._session.get_job_logs(exec_id=self.exec_id, job=job, offset=offset)\n        except HTTPError as err:\n            preparing = False\n            while True:\n                sleep(delay)\n                preparing_jobs = set((e['id'] for e in self.status['nodes'] if (e['status'] == 'PREPARING')))\n                if (job in preparing_jobs):\n                    if (not preparing):\n                        preparing = True\n                        _logger.debug('Job %s in execution %s is still preparing.', job, self.exec_id)\n                else:\n                    break\n            if (not preparing):\n                raise err\n        else:\n            if logs['length']:\n                offset += logs['length']\n                lines = (e for e in logs['data'].split('\\n') if e)\n                for line in lines:\n                    (yield line)\n            elif finishing:\n                break\n            else:\n                running_jobs = set((e['id'] for e in self.status['nodes'] if (e['status'] == 'RUNNING')))\n                if (job not in running_jobs):\n                    finishing = True\n        sleep(delay)\n", "label": 1}
{"function": "\n\ndef _load(b, classes):\n    identifier = b[0]\n    if isinstance(identifier, str):\n        identifier = ord(identifier)\n    if (identifier == _SPEC):\n        return _load_spec(b)\n    elif (identifier == _INT_32):\n        return _load_int_32(b)\n    elif ((identifier == _INT) or (identifier == _INT_NEG)):\n        return _load_int(b)\n    elif (identifier == _FLOAT):\n        return _load_float(b)\n    elif (identifier == _COMPLEX):\n        return _load_complex(b)\n    elif (identifier == _STR):\n        return _load_str(b)\n    elif (identifier == _BYTES):\n        return _load_bytes(b)\n    elif (identifier == _TUPLE):\n        return _load_tuple(b, classes)\n    elif (identifier == _NAMEDTUPLE):\n        return _load_namedtuple(b, classes)\n    elif (identifier == _LIST):\n        return _load_list(b, classes)\n    elif (identifier == _NPARRAY):\n        return _load_np_array(b)\n    elif (identifier == _DICT):\n        return _load_dict(b, classes)\n    elif (identifier == _GETSTATE):\n        return _load_getstate(b, classes)\n    elif (identifier == _BFSTATE):\n        raise BFLoadError('BFSTATE objects can not be loaded')\n    else:\n        raise BFLoadError(\"unknown identifier '{}'\".format(hex(identifier)))\n", "label": 1}
{"function": "\n\ndef traverse(obj, *path, **kwargs):\n    '\\n    Traverse the object we receive with the given path. Path\\n    items can be either strings or lists of strings (or any\\n    nested combination thereof). Behavior in given cases is\\n    laid out line by line below.\\n    '\n    if path:\n        if (isinstance(obj, list) or isinstance(obj, tuple)):\n            return [traverse(x, *path) for x in obj]\n        elif isinstance(obj, dict):\n            if (isinstance(path[0], list) or isinstance(path[0], tuple)):\n                for branch in path[0]:\n                    if (not isinstance(branch, basestring)):\n                        raise TraversalError(obj, path[0])\n                return {name: traverse(obj[name], *path[1:], split=True) for name in path[0]}\n            elif (not isinstance(path[0], basestring)):\n                raise TraversalError(obj, path[0])\n            elif (path[0] == '\\\\*'):\n                return {name: traverse(item, *path[1:], split=True) for (name, item) in obj.items()}\n            elif (path[0] in obj):\n                return traverse(obj[path[0]], *path[1:])\n            else:\n                raise TraversalError(obj, path[0])\n        elif kwargs.get('split', False):\n            return obj\n        else:\n            raise TraversalError(obj, path[0])\n    else:\n        return obj\n", "label": 1}
{"function": "\n\ndef _check_command_response(response, reset, msg=None, allowable_errors=None):\n    'Check the response to a command for errors.\\n    '\n    if ('ok' not in response):\n        raise OperationFailure(response.get('$err'), response.get('code'), response)\n    if response.get('wtimeout', False):\n        raise WTimeoutError(response.get('errmsg', response.get('err')), response.get('code'), response)\n    if (not response['ok']):\n        details = response\n        if ('raw' in response):\n            for shard in response['raw'].itervalues():\n                if (not shard.get('ok')):\n                    details = shard\n                    break\n        errmsg = details['errmsg']\n        if ((allowable_errors is None) or (errmsg not in allowable_errors)):\n            if (errmsg.startswith('not master') or errmsg.startswith('node is recovering')):\n                if (reset is not None):\n                    reset()\n                raise AutoReconnect(errmsg)\n            if (errmsg == 'db assertion failure'):\n                errmsg = (\"db assertion failure, assertion: '%s'\" % details.get('assertion', ''))\n                raise OperationFailure(errmsg, details.get('assertionCode'), response)\n            code = details.get('code')\n            if (code in (11000, 11001, 12582)):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif (code == 50):\n                raise ExecutionTimeout(errmsg, code, response)\n            msg = (msg or '%s')\n            raise OperationFailure((msg % errmsg), code, response)\n", "label": 1}
{"function": "\n\ndef cancelUpload(self, upload):\n    '\\n        Delete the temporary files associated with a given upload.\\n        '\n    if ('s3' not in upload):\n        return\n    if ('key' not in upload['s3']):\n        return\n    bucket = self._getBucket()\n    if bucket:\n        key = bucket.get_key(upload['s3']['key'], validate=True)\n        if key:\n            bucket.delete_key(key)\n        if (('s3' in upload) and ('uploadId' in upload['s3']) and ('key' in upload['s3'])):\n            getParams = {\n                \n            }\n            while True:\n                try:\n                    multipartUploads = bucket.get_all_multipart_uploads(**getParams)\n                except boto.exception.S3ResponseError:\n                    break\n                if (not len(multipartUploads)):\n                    break\n                for multipartUpload in multipartUploads:\n                    if ((multipartUpload.id == upload['s3']['uploadId']) and (multipartUpload.key_name == upload['s3']['key'])):\n                        multipartUpload.cancel_upload()\n                if (not multipartUploads.is_truncated):\n                    break\n                getParams['key_marker'] = multipartUploads.next_key_marker\n                getParams['upload_id_marker'] = multipartUploads.next_upload_id_marker\n", "label": 1}
{"function": "\n\n@property\ndef either(self):\n    'Transform pattern into an equivalent, with only top-level Either.'\n    if (not hasattr(self, 'children')):\n        return Either(Required(self))\n    else:\n        ret = []\n        groups = [[self]]\n        while groups:\n            children = groups.pop(0)\n            types = [type(c) for c in children]\n            if (Either in types):\n                either = [c for c in children if (type(c) is Either)][0]\n                children.pop(children.index(either))\n                for c in either.children:\n                    groups.append(([c] + children))\n            elif (Required in types):\n                required = [c for c in children if (type(c) is Required)][0]\n                children.pop(children.index(required))\n                groups.append((list(required.children) + children))\n            elif (Optional in types):\n                optional = [c for c in children if (type(c) is Optional)][0]\n                children.pop(children.index(optional))\n                groups.append((list(optional.children) + children))\n            elif (OneOrMore in types):\n                oneormore = [c for c in children if (type(c) is OneOrMore)][0]\n                children.pop(children.index(oneormore))\n                groups.append(((list(oneormore.children) * 2) + children))\n            else:\n                ret.append(children)\n        return Either(*[Required(*e) for e in ret])\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRUCT):\n                self.schema = hive_metastore.ttypes.Schema()\n                self.schema.read(iprot)\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.table_dir = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.in_tablename = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.delim = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef __init__(self, *args, **kwargs):\n    self.use_greenlets = False\n    self.auto_start_request = False\n    kwargs.pop('auto_start_request', None)\n    if (('read_preference' not in kwargs) and (kwargs.get('tag_sets') not in (None, [], [{\n        \n    }]))):\n        raise ConfigurationError()\n    if (('read_preference' not in kwargs) and (('slaveok' in kwargs) or ('slave_okay' in kwargs))):\n        secondary = kwargs.pop('slave_okay', kwargs.pop('slaveok', False))\n        kwargs['read_preference'] = (ReadPreference.SECONDARY_PREFERRED if secondary else ReadPreference.PRIMARY)\n    gle_opts = dict([(k, v) for (k, v) in kwargs.items() if (k in SAFE_OPTIONS)])\n    if (gle_opts and ('w' not in gle_opts)):\n        kwargs['w'] = 1\n    if ('safe' in kwargs):\n        safe = kwargs.pop('safe')\n        if (not safe):\n            kwargs.setdefault('w', 0)\n    self.delegate = kwargs.pop('delegate', None)\n    if (not self.delegate):\n        self.delegate = self.__delegate_class__(*args, **kwargs)\n        if kwargs.get('_connect', True):\n            self.synchro_connect()\n", "label": 1}
{"function": "\n\ndef poll(self, timeout):\n    self._lock.acquire()\n    if (timeout == 0):\n        self.poll_timeout = 0\n    elif self._timeouts:\n        self.poll_timeout = (self._timeouts[0][0] - _time())\n        if (self.poll_timeout < 0.0001):\n            self.poll_timeout = 0\n        elif (timeout is not None):\n            self.poll_timeout = min(timeout, self.poll_timeout)\n    elif (timeout is None):\n        self.poll_timeout = _AsyncNotifier._Block\n    else:\n        self.poll_timeout = timeout\n    timeout = self.poll_timeout\n    self._lock.release()\n    if (timeout and (timeout != _AsyncNotifier._Block)):\n        timeout = int((timeout * 1000))\n    (err, n, key, overlap) = win32file.GetQueuedCompletionStatus(self.iocp, timeout)\n    while (err != winerror.WAIT_TIMEOUT):\n        if (overlap and overlap.object):\n            overlap.object(err, n)\n        else:\n            logger.warning('invalid overlap!')\n        (err, n, key, overlap) = win32file.GetQueuedCompletionStatus(self.iocp, 0)\n    self.poll_timeout = 0\n    if (timeout == 0):\n        now = _time()\n        self._lock.acquire()\n        while (self._timeouts and (self._timeouts[0][0] <= now)):\n            (fd_timeout, fd) = self._timeouts.pop(0)\n            if (fd._timeout_id == fd_timeout):\n                fd._timeout_id = None\n                fd._timed_out()\n        self._lock.release()\n", "label": 1}
{"function": "\n\n@classmethod\ndef put(cls, kvs, entity=None):\n    for k in kvs.keys():\n        if (not (k in cls.properties())):\n            del kvs[k]\n            continue\n        v = cls.__dict__[k]\n        if isinstance(v, db.IntegerProperty):\n            kvs[k] = int(kvs[k])\n        elif isinstance(v, db.FloatProperty):\n            kvs[k] = float(kvs[k])\n        elif isinstance(v, db.BooleanProperty):\n            kvs[k] = (True if (kvs[k] == 'True') else False)\n        elif (isinstance(v, db.StringProperty) or isinstance(v, db.TextProperty)):\n            kvs[k] = kvs[k]\n        elif isinstance(v, db.DateProperty):\n            kvs[k] = datetime.strptime(kvs[k], settings.DATE_FORMAT).date()\n        elif isinstance(v, db.LinkProperty):\n            kvs[k] = kvs[k]\n        elif isinstance(v, db.EmailProperty):\n            kvs[k] = kvs[k]\n        else:\n            raise UnsupportedFieldTypeError(v)\n    cls.validate(kvs)\n    try:\n        if (not entity):\n            entity = cls(**kvs)\n        else:\n            for k in kvs.keys():\n                setattr(entity, k, kvs[k])\n        db.put(entity)\n    except Exception as e:\n        logging.error((\"Couldn't put entity: %s\" % kvs))\n        logging.error(e)\n        return None\n    return entity\n", "label": 1}
{"function": "\n\ndef __gt__(self, other):\n    if (self.date == 'infinity'):\n        if isinstance(other, Date):\n            return (other.date != 'infinity')\n        else:\n            from .Range import Range\n            if isinstance(other, Range):\n                return (other.end != 'infinity')\n            return (other != 'infinity')\n    elif isinstance(other, Date):\n        if (other.date == 'infinity'):\n            return False\n        elif (other.tz and (self.tz is None)):\n            return (self.date.replace(tzinfo=other.tz) > other.date)\n        elif (self.tz and (other.tz is None)):\n            return (self.date > other.date.replace(tzinfo=self.tz))\n        return (self.date > other.date)\n    else:\n        from .Range import Range\n        if isinstance(other, Range):\n            if (other.end.date == 'infinity'):\n                return False\n            if (other.end.tz and (self.tz is None)):\n                return (self.date.replace(tzinfo=other.end.tz) > other.end.date)\n            elif (self.tz and (other.end.tz is None)):\n                return (self.date > other.end.date.replace(tzinfo=self.tz))\n            return (self.date > other.end.date)\n        else:\n            return self.__gt__(Date(other, tz=self.tz))\n", "label": 1}
{"function": "\n\ndef __new__(cls, *args):\n    ' Construct a Trace object.\\n\\n        Parameters\\n        ==========\\n        args = sympy expression\\n        indices = tuple/list if indices, optional\\n\\n        '\n    if (len(args) == 2):\n        if (not isinstance(args[1], (list, Tuple, tuple))):\n            indices = Tuple(args[1])\n        else:\n            indices = Tuple(*args[1])\n        expr = args[0]\n    elif (len(args) == 1):\n        indices = Tuple()\n        expr = args[0]\n    else:\n        raise ValueError('Arguments to Tr should be of form (expr[, [indices]])')\n    if isinstance(expr, Matrix):\n        return expr.trace()\n    elif (hasattr(expr, 'trace') and callable(expr.trace)):\n        return expr.trace()\n    elif isinstance(expr, Add):\n        return Add(*[Tr(arg, indices) for arg in expr.args])\n    elif isinstance(expr, Mul):\n        (c_part, nc_part) = expr.args_cnc()\n        if (len(nc_part) == 0):\n            return Mul(*c_part)\n        else:\n            obj = Expr.__new__(cls, Mul(*nc_part), indices)\n            return ((Mul(*c_part) * obj) if (len(c_part) > 0) else obj)\n    elif isinstance(expr, Pow):\n        if (_is_scalar(expr.args[0]) and _is_scalar(expr.args[1])):\n            return expr\n        else:\n            return Expr.__new__(cls, expr, indices)\n    else:\n        if _is_scalar(expr):\n            return expr\n        return Expr.__new__(cls, expr, indices)\n", "label": 1}
{"function": "\n\ndef telescopic(L, R, limits):\n    'Tries to perform the summation using the telescopic property\\n\\n    return None if not possible\\n    '\n    (i, a, b) = limits\n    if (L.is_Add or R.is_Add):\n        return None\n    k = Wild('k')\n    sol = (- R).match(L.subs(i, (i + k)))\n    s = None\n    if (sol and (k in sol)):\n        s = sol[k]\n        if (not (s.is_Integer and (L.subs(i, (i + s)) == (- R)))):\n            s = None\n    if (s is None):\n        m = Dummy('m')\n        try:\n            sol = (solve((L.subs(i, (i + m)) + R), m) or [])\n        except NotImplementedError:\n            return None\n        sol = [si for si in sol if (si.is_Integer and (L.subs(i, (i + si)) + R).expand().is_zero)]\n        if (len(sol) != 1):\n            return None\n        s = sol[0]\n    if (s < 0):\n        return telescopic_direct(R, L, abs(s), (i, a, b))\n    elif (s > 0):\n        return telescopic_direct(L, R, s, (i, a, b))\n", "label": 1}
{"function": "\n\ndef get_OPinv_matvec(A, M, sigma, symmetric=False, tol=0):\n    if (sigma == 0):\n        return get_inv_matvec(A, symmetric=symmetric, tol=tol)\n    if (M is None):\n        if isdense(A):\n            if (np.issubdtype(A.dtype, np.complexfloating) or (np.imag(sigma) == 0)):\n                A = np.copy(A)\n            else:\n                A = (A + 0j)\n            A.flat[::(A.shape[1] + 1)] -= sigma\n            return LuInv(A).matvec\n        elif isspmatrix(A):\n            A = (A - (sigma * identity(A.shape[0])))\n            if (symmetric and isspmatrix_csr(A)):\n                A = A.T\n            return SpLuInv(A.tocsc()).matvec\n        else:\n            return IterOpInv(_aslinearoperator_with_dtype(A), M, sigma, tol=tol).matvec\n    elif (((not isdense(A)) and (not isspmatrix(A))) or ((not isdense(M)) and (not isspmatrix(M)))):\n        return IterOpInv(_aslinearoperator_with_dtype(A), _aslinearoperator_with_dtype(M), sigma, tol=tol).matvec\n    elif (isdense(A) or isdense(M)):\n        return LuInv((A - (sigma * M))).matvec\n    else:\n        OP = (A - (sigma * M))\n        if (symmetric and isspmatrix_csr(OP)):\n            OP = OP.T\n        return SpLuInv(OP.tocsc()).matvec\n", "label": 1}
{"function": "\n\ndef doWaitForMultipleEvents(self, timeout):\n    log.msg(channel='system', event='iteration', reactor=self)\n    if (timeout is None):\n        timeout = 100\n    ranUserCode = False\n    for reader in self._closedAndReading.keys():\n        ranUserCode = True\n        self._runAction('doRead', reader)\n    for fd in self._writes.keys():\n        ranUserCode = True\n        log.callWithLogger(fd, self._runWrite, fd)\n    if ranUserCode:\n        timeout = 0\n    if (not (self._events or self._writes)):\n        time.sleep(timeout)\n        return\n    handles = (self._events.keys() or [self.dummyEvent])\n    timeout = int((timeout * 1000))\n    val = MsgWaitForMultipleObjects(handles, 0, timeout, QS_ALLINPUT)\n    if (val == WAIT_TIMEOUT):\n        return\n    elif (val == (WAIT_OBJECT_0 + len(handles))):\n        exit = win32gui.PumpWaitingMessages()\n        if exit:\n            self.callLater(0, self.stop)\n            return\n    elif ((val >= WAIT_OBJECT_0) and (val < (WAIT_OBJECT_0 + len(handles)))):\n        event = handles[(val - WAIT_OBJECT_0)]\n        (fd, action) = self._events[event]\n        if (fd in self._reads):\n            fileno = fd.fileno()\n            if (fileno == (- 1)):\n                self._disconnectSelectable(fd, posixbase._NO_FILEDESC, False)\n                return\n            events = WSAEnumNetworkEvents(fileno, event)\n            if (FD_CLOSE in events):\n                self._closedAndReading[fd] = True\n        log.callWithLogger(fd, self._runAction, action, fd)\n", "label": 1}
{"function": "\n\ndef _read_object(self, relpath, max_symlinks):\n    path_so_far = ''\n    components = list(relpath.split(os.path.sep))\n    symlinks = 0\n    while components:\n        component = components.pop(0)\n        if ((component == '') or (component == '.')):\n            continue\n        parent_tree = self._read_tree(path_so_far)\n        parent_path = path_so_far\n        if (path_so_far != ''):\n            path_so_far += '/'\n        path_so_far += component\n        try:\n            obj = parent_tree[component]\n        except KeyError:\n            raise self.MissingFileException(self.rev, relpath)\n        if isinstance(obj, self.File):\n            if components:\n                raise self.NotADirException(self.rev, relpath)\n            else:\n                return (obj, path_so_far)\n        elif isinstance(obj, self.Dir):\n            if (not components):\n                return (obj, (path_so_far + '/'))\n        elif isinstance(obj, self.Symlink):\n            symlinks += 1\n            if (symlinks > max_symlinks):\n                return (obj, path_so_far)\n            (object_type, path_data) = self._read_object_from_repo(sha=obj.sha)\n            assert (object_type == 'blob')\n            if (path_data[0] == '/'):\n                raise self.ExternalSymlinkException(self.rev, relpath)\n            link_to = os.path.normpath(os.path.join(parent_path, path_data))\n            if (link_to.startswith('../') or (link_to[0] == '/')):\n                raise self.ExternalSymlinkException(self.rev, relpath)\n            components = (link_to.split(SLASH) + components)\n            path_so_far = ''\n        else:\n            raise self.UnexpectedGitObjectTypeException()\n    return (self.Dir('./', None), './')\n", "label": 1}
{"function": "\n\ndef format(self, record):\n    if (not hasattr(record, 'server')):\n        record.server = record.name\n    record.message = record.getMessage()\n    if (self._fmt.find('%(asctime)') >= 0):\n        record.asctime = self.formatTime(record, self.datefmt)\n    msg = (self._fmt % record.__dict__).replace('\\n', '#012')\n    if record.exc_info:\n        if (not record.exc_text):\n            record.exc_text = self.formatException(record.exc_info).replace('\\n', '#012')\n    if record.exc_text:\n        if (not msg.endswith('#012')):\n            msg = (msg + '#012')\n        msg = (msg + record.exc_text)\n    if (hasattr(record, 'txn_id') and record.txn_id and (record.levelno != logging.INFO) and (record.txn_id not in msg)):\n        msg = ('%s (txn: %s)' % (msg, record.txn_id))\n    if (hasattr(record, 'client_ip') and record.client_ip and (record.levelno != logging.INFO) and (record.client_ip not in msg)):\n        msg = ('%s (client_ip: %s)' % (msg, record.client_ip))\n    if ((self.max_line_length > 0) and (len(msg) > self.max_line_length)):\n        if (self.max_line_length < 7):\n            msg = msg[:self.max_line_length]\n        else:\n            approxhalf = ((self.max_line_length - 5) // 2)\n            msg = ((msg[:approxhalf] + ' ... ') + msg[(- approxhalf):])\n    return msg\n", "label": 1}
{"function": "\n\ndef main(argv):\n    api_files = rst_files = [rst for rst in os.listdir('doc/api') if (fnmatch.fnmatch(rst, '*.rst') and (rst not in excluded_docs))]\n    cmd = argv.pop(0)\n\n    def has(*options):\n        for opt in options:\n            if (opt in argv):\n                return argv.pop(argv.index(opt))\n    if has('-h', '--help'):\n        usage(cmd)\n    verbose = has('-v', '--verbose')\n    only_documented = (not has('-a', '--all'))\n    if argv:\n        given_files = []\n        for arg in argv:\n            arg = arg.replace('\\\\', '/').replace((api_doc + '/'), '')\n            arg = (arg.replace('.rst', '') + '.rst')\n            if ('*' in arg):\n                given_files += [rst for rst in api_files if fnmatch.fnmatch(rst, arg)]\n            elif (arg in api_files):\n                given_files.append(arg)\n        api_files = given_files\n    rst_basenames = sorted((f[:(- 4)] for f in rst_files))\n    for rst in api_files:\n        basename = rst.replace('.rst', '')\n        if (verbose or (len(api_files) > 1)):\n            print(('== Checking %s ... ' % rst))\n        check_api_doc(basename, verbose, only_documented, any(((f.startswith(basename) and (f != basename)) for f in rst_basenames)))\n", "label": 1}
{"function": "\n\ndef test_field_assumptions():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.real_elements(X), Q.real_elements(X))\n    assert (not ask(Q.integer_elements(X), Q.real_elements(X)))\n    assert ask(Q.complex_elements(X), Q.real_elements(X))\n    assert (ask(Q.real_elements((X + Y)), Q.real_elements(X)) is None)\n    assert ask(Q.real_elements((X + Y)), (Q.real_elements(X) & Q.real_elements(Y)))\n    from sympy.matrices.expressions.hadamard import HadamardProduct\n    assert ask(Q.real_elements(HadamardProduct(X, Y)), (Q.real_elements(X) & Q.real_elements(Y)))\n    assert ask(Q.complex_elements((X + Y)), (Q.real_elements(X) & Q.complex_elements(Y)))\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X.I), (Q.real_elements(X) & Q.invertible(X)))\n    assert ask(Q.real_elements(Trace(X)), Q.real_elements(X))\n    assert ask(Q.integer_elements(Determinant(X)), Q.integer_elements(X))\n    assert (not ask(Q.integer_elements(X.I), Q.integer_elements(X)))\n    alpha = Symbol('alpha')\n    assert ask(Q.real_elements((alpha * X)), (Q.real_elements(X) & Q.real(alpha)))\n    assert ask(Q.real_elements(LofLU(X)), Q.real_elements(X))\n", "label": 1}
{"function": "\n\ndef process_field(self, key, project_id, result, value):\n    if (value is None):\n        return\n    if (isinstance(value, list) and (not len(value))):\n        return\n    if ((isinstance(value, str) or isinstance(value, unicode)) and (not len(value))):\n        return\n    field_name = self._get_field_name(key, project_id)\n    if ((field_name is None) or (field_name == NUMBER_IN_PROJECT)):\n        return\n    field_type = self._get_field_type(field_name)\n    if ((field_type is None) and (field_name not in youtrack.EXISTING_FIELDS)):\n        return\n    value = self.get_field_value(field_name, field_type, value)\n    if isinstance(value, list):\n        for v in value:\n            self._add_value_to_field(project_id, field_name, field_type, v)\n    else:\n        self._add_value_to_field(project_id, field_name, field_type, value)\n    if ((field_type is not None) and field_type.startswith('user')):\n        if isinstance(value, list):\n            value = [v.login for v in value]\n        else:\n            value = value.login\n    if (not isinstance(value, list)):\n        value = str(value)\n    result[field_name] = value\n", "label": 1}
{"function": "\n\ndef collect(self, delay=None):\n    if delay:\n        time.sleep(delay)\n    pdu_list = list()\n    max_data = None\n    with self.lock:\n        active_sap_list = [sap for sap in self.sap if (sap is not None)]\n        for sap in active_sap_list:\n            pdu = sap.dequeue((max_data if max_data else 2179))\n            if (pdu is not None):\n                if (self.cfg['send-agf'] == False):\n                    return pdu\n                pdu_list.append(pdu)\n                if (max_data is None):\n                    max_data = (self.cfg['send-miu'] + 2)\n                max_data -= len(pdu)\n                if (max_data < (((bool((len(pdu_list) == 1)) * 2) + 2) + 2)):\n                    break\n        else:\n            max_data = (self.cfg['send-miu'] + 2)\n        for sap in active_sap_list:\n            if (sap.mode == DATA_LINK_CONNECTION):\n                pdu = sap.sendack(max_data)\n                if (not (pdu is None)):\n                    if (self.cfg['send-agf'] == False):\n                        return pdu\n                    pdu_list.append(pdu)\n                    max_data -= len(pdu)\n                    if (max_data < (((bool((len(pdu_list) == 1)) * 2) + 2) + 3)):\n                        break\n    if (len(pdu_list) > 1):\n        return AggregatedFrame(aggregate=pdu_list)\n    if (len(pdu_list) == 1):\n        return pdu_list[0]\n    return None\n", "label": 1}
{"function": "\n\ndef filter_subnets(self):\n    subnets = []\n    admin_tenant_id = self.src_cloud.get_tenant_id(self.src_cloud.tenant)\n    for net in config.networks:\n        if (not net.get('subnets')):\n            continue\n        for subnet in net['subnets']:\n            subnet['tenant_id'] = admin_tenant_id\n            subnets.append(subnet)\n    subnets = [i for net in config.networks if net.get('subnets') for i in net['subnets']]\n    for tenant in config.tenants:\n        if (('networks' not in tenant) or tenant.get('deleted')):\n            continue\n        for network in tenant['networks']:\n            if ('subnets' not in network):\n                continue\n            for subnet in network['subnets']:\n                subnet['tenant_id'] = self.src_cloud.get_tenant_id(tenant['name'])\n                subnets.append(subnet)\n    env_subnets = self.src_cloud.neutronclient.list_subnets()['subnets']\n    filtered_subnets = {\n        'subnets': [],\n    }\n    for env_subnet in env_subnets:\n        for subnet in subnets:\n            same_cidr = (env_subnet['cidr'] == subnet['cidr'])\n            same_tenant = (env_subnet['tenant_id'] == subnet['tenant_id'])\n            if (same_cidr and same_tenant):\n                filtered_subnets['subnets'].append(env_subnet)\n    return filtered_subnets\n", "label": 1}
{"function": "\n\ndef check(self):\n    'Do the actual testing.'\n    scene = self.scene\n    src = scene.children[0]\n    ud = src.children[0]\n    o = ud.children[0].children[0].children[0]\n    mm = o.children[0]\n    assert (src.get_output_dataset().point_data.scalars.name == 'temperature')\n    assert (src.get_output_dataset().point_data.vectors.name == 'velocity')\n    expect = ['ScalarGradient', 'Vorticity']\n    expect1 = [(x + '-y') for x in expect]\n    expect2 = [(x + ' magnitude') for x in expect]\n    o.enabled = True\n    assert (o.get_output_dataset().point_data.scalars.name in expect1)\n    assert (o.get_output_dataset().point_data.vectors.name in expect)\n    assert (mm.scalar_lut_manager.data_name in expect1)\n    o.enabled = False\n    assert (o.get_output_dataset().point_data.scalars.name in expect2)\n    assert (o.get_output_dataset().point_data.vectors.name in expect)\n    assert (mm.scalar_lut_manager.data_name in expect2)\n    ud.filter.vector_mode = 'compute_vorticity'\n    assert (o.get_output_dataset().point_data.scalars.name == 'Vorticity magnitude')\n    assert (o.get_output_dataset().point_data.vectors.name == 'Vorticity')\n    assert (mm.scalar_lut_manager.data_name == 'Vorticity magnitude')\n    o.enabled = True\n    assert (o.get_output_dataset().point_data.scalars.name == 'Vorticity-y')\n    assert (o.get_output_dataset().point_data.vectors.name == 'Vorticity')\n    assert (mm.scalar_lut_manager.data_name == 'Vorticity-y')\n    o.enabled = False\n", "label": 1}
{"function": "\n\ndef uniquePathsWithObstacles(self, obstacleGrid):\n    '\\n        dp\\n        :param obstacleGrid:  a list of lists of integers\\n        :return: integer\\n        '\n    m = len(obstacleGrid)\n    n = len(obstacleGrid[0])\n    if ((obstacleGrid[0][0] == 1) or (obstacleGrid[(m - 1)][(n - 1)] == 1)):\n        return 0\n    path = [[0 for _ in range(n)] for _ in range(m)]\n    path[0][0] = 1\n    for i in range(m):\n        for j in range(n):\n            if ((i == 0) and (j == 0)):\n                continue\n            if (i == 0):\n                path[i][j] = (path[i][(j - 1)] if (obstacleGrid[i][(j - 1)] == 0) else 0)\n            elif (j == 0):\n                path[i][j] = (path[(i - 1)][j] if (obstacleGrid[(i - 1)][j] == 0) else 0)\n            elif ((obstacleGrid[i][(j - 1)] == 0) and (obstacleGrid[(i - 1)][j] == 0)):\n                path[i][j] = (path[(i - 1)][j] + path[i][(j - 1)])\n            elif (obstacleGrid[i][(j - 1)] == 0):\n                path[i][j] = path[i][(j - 1)]\n            elif (obstacleGrid[(i - 1)][j] == 0):\n                path[i][j] = path[(i - 1)][j]\n            else:\n                path[i][j] = 0\n    return path[(m - 1)][(n - 1)]\n", "label": 1}
{"function": "\n\ndef restore_prefs(self, prefs):\n    ' Restores any saved user preference information associated with the\\n            editor.\\n        '\n    factory = self.factory\n    try:\n        filters = prefs.get('filters', None)\n        if (filters is not None):\n            factory.filters = ([f for f in factory.filters if f.template] + [f for f in filters if (not f.template)])\n        columns = prefs.get('columns')\n        if (columns is not None):\n            new_columns = []\n            all_columns = (self.columns + factory.other_columns)\n            for column in columns:\n                for column2 in all_columns:\n                    if (column == column2.get_label()):\n                        new_columns.append(column2)\n                        break\n            self.columns = new_columns\n            if (not factory.auto_size):\n                widths = prefs.get('widths')\n                if (widths is not None):\n                    self.grid._user_col_size = True\n                    set_col_size = self.grid._grid.SetColSize\n                    for (i, width) in enumerate(widths):\n                        if (width >= 0):\n                            set_col_size(i, width)\n        structure = prefs.get('structure')\n        if ((structure is not None) and (factory.edit_view != ' ')):\n            self.control.GetSizer().SetStructure(self.control, structure)\n    except:\n        pass\n", "label": 1}
{"function": "\n\ndef assert_both_values(actual, expected_plain, expected_color, kind=None):\n    'Handle asserts for color and non-color strings in color and non-color tests.\\n\\n    :param ColorStr actual: Return value of ColorStr class method.\\n    :param expected_plain: Expected non-color value.\\n    :param expected_color: Expected color value.\\n    :param str kind: Type of string to test.\\n    '\n    if kind.endswith('plain'):\n        assert (actual.value_colors == expected_plain)\n        assert (actual.value_no_colors == expected_plain)\n        assert (actual.has_colors is False)\n    elif kind.endswith('color'):\n        assert (actual.value_colors == expected_color)\n        assert (actual.value_no_colors == expected_plain)\n        if ('\\x1b' in actual.value_colors):\n            assert (actual.has_colors is True)\n        else:\n            assert (actual.has_colors is False)\n    else:\n        assert (actual == expected_plain)\n    if kind.startswith('ColorStr'):\n        assert (actual.__class__ == ColorStr)\n    elif kind.startswith('Color'):\n        assert (actual.__class__ == Color)\n", "label": 1}
{"function": "\n\ndef _possible_configs_for_cls(cls, reasons=None):\n    all_configs = set(config.Config.all_configs())\n    if cls.__unsupported_on__:\n        spec = exclusions.db_spec(*cls.__unsupported_on__)\n        for config_obj in list(all_configs):\n            if spec(config_obj):\n                all_configs.remove(config_obj)\n    if getattr(cls, '__only_on__', None):\n        spec = exclusions.db_spec(*util.to_list(cls.__only_on__))\n        for config_obj in list(all_configs):\n            if (not spec(config_obj)):\n                all_configs.remove(config_obj)\n    if hasattr(cls, '__requires__'):\n        requirements = config.requirements\n        for config_obj in list(all_configs):\n            for requirement in cls.__requires__:\n                check = getattr(requirements, requirement)\n                skip_reasons = check.matching_config_reasons(config_obj)\n                if skip_reasons:\n                    all_configs.remove(config_obj)\n                    if (reasons is not None):\n                        reasons.extend(skip_reasons)\n                    break\n    if hasattr(cls, '__prefer_requires__'):\n        non_preferred = set()\n        requirements = config.requirements\n        for config_obj in list(all_configs):\n            for requirement in cls.__prefer_requires__:\n                check = getattr(requirements, requirement)\n                if (not check.enabled_for_config(config_obj)):\n                    non_preferred.add(config_obj)\n        if all_configs.difference(non_preferred):\n            all_configs.difference_update(non_preferred)\n    return all_configs\n", "label": 1}
{"function": "\n\ndef is_acronym(token, exclude=None):\n    '\\n    Pass single token as a string, return True/False if is/is not valid acronym.\\n\\n    Args:\\n        token (str): single word to check for acronym-ness\\n        exclude (set[str]): if technically valid but not actually good acronyms\\n        are known in advance, pass them in as a set of strings; matching tokens\\n        will return False\\n\\n    Returns:\\n        bool\\n    '\n    if (exclude and (token in exclude)):\n        return False\n    if (not token):\n        return False\n    if (' ' in token):\n        return False\n    if ((len(token) == 2) and (not token.isupper())):\n        return False\n    if token.isdigit():\n        return False\n    if ((not any((char.isupper() for char in token))) and (not (token[0].isdigit() or token[(- 1)].isdigit()))):\n        return False\n    if (not (2 <= sum((1 for char in token if char.isalnum())) <= 10)):\n        return False\n    if (not ACRONYM_REGEX.match(token)):\n        return False\n    return True\n", "label": 1}
{"function": "\n\ndef search(self, query):\n    response = self.command('search -qbd', query)[0].decode(*enc)\n    from re import match\n    lst = {\n        \n    }\n    desc = False\n    blank = False\n    for line in [l for l in response.splitlines() if (not match('\\\\*\\\\*\\\\*', l))]:\n        if ((not desc) and match('(.*)\\\\((.*)\\\\)', line)):\n            (key, val) = match('(.*)\\\\((.*)\\\\)', line).groups()\n            lst[self.package_name(key)] = (('Version: ' + val) + '\\n')\n            desc = True\n        elif (desc and (not blank) and (not line)):\n            blank = True\n        elif (desc and ('Homepage:' in line)):\n            lst[self.package_name(key)] += (line.replace('Homepage: ', '').strip() + '\\n')\n        elif (desc and blank and line):\n            lst[self.package_name(key)] += (line.strip() + ' ')\n        elif (desc and blank and (not line)):\n            desc = False\n            blank = False\n    return lst\n", "label": 1}
{"function": "\n\ndef input(self, prompt, hint=None, validators=None, repeat_until_valid=False, task=None, leave_when_cancel=None, **kwargs):\n    has_default = ('default' in kwargs)\n    default = kwargs.get('default', None)\n    leave_when_cancel = (leave_when_cancel or bool(task))\n    message_components = [prompt, ' ']\n    if (hint is not None):\n        message_components.append('({0})'.format(hint))\n    if has_default:\n        message_components.append('[{0}]'.format(default))\n    message = (''.join(message_components).strip() + ': ')\n    while True:\n        has_result = True\n        try:\n            result = six.moves.input(message).strip()\n        except KeyboardInterrupt:\n            self.show('')\n            error_msg = 'User cancelled input.'\n            self.error(error_msg)\n            if (task and leave_when_cancel):\n                task.exit(1)\n        else:\n            if (((result is None) or (len(result) == 0)) and has_default):\n                result = default\n            if validators:\n                for validator in validators:\n                    try:\n                        result = validator(result)\n                    except Exception as e:\n                        has_result = False\n                        if has_default:\n                            result = validator(default)\n                        else:\n                            self.error(str(e))\n                            break\n            if ((not repeat_until_valid) or has_result):\n                return result\n", "label": 1}
{"function": "\n\ndef parse_diff(self):\n    sections = []\n    state = None\n    prev_file = None\n    current_file = {\n        \n    }\n    current_hunks = []\n    prev_hunk = None\n    current_hunk = None\n    for line in self.view.lines(sublime.Region(0, self.view.size())):\n        linetext = self.view.substr(line)\n        if linetext.startswith('diff --git'):\n            state = 'header'\n            if (prev_file != line):\n                if (prev_file is not None):\n                    if current_hunk:\n                        current_hunks.append(current_hunk)\n                    sections.append((current_file, current_hunks))\n                prev_file = line\n                prev_hunk = None\n            current_file = line\n            current_hunks = []\n        elif ((state == 'header') and RE_DIFF_HEAD.match(linetext)):\n            current_file = current_file.cover(line)\n        elif linetext.startswith('@@'):\n            state = 'hunk'\n            if (prev_hunk != line):\n                if (prev_hunk is not None):\n                    current_hunks.append(current_hunk)\n                prev_hunk = line\n            current_hunk = line\n        elif ((state == 'hunk') and (linetext[0] in (' ', '-', '+'))):\n            current_hunk = current_hunk.cover(line)\n        elif (state == 'header'):\n            current_file = current_file.cover(line)\n    if (current_file and current_hunk):\n        current_hunks.append(current_hunk)\n        sections.append((current_file, current_hunks))\n    return sections\n", "label": 1}
{"function": "\n\ndef _eval_pos_neg(self, sign):\n    saw_NON = saw_NOT = False\n    for t in self.args:\n        if t.is_positive:\n            continue\n        elif t.is_negative:\n            sign = (- sign)\n        elif t.is_zero:\n            if all((a.is_finite for a in self.args)):\n                return False\n            return\n        elif t.is_nonpositive:\n            sign = (- sign)\n            saw_NON = True\n        elif t.is_nonnegative:\n            saw_NON = True\n        elif (t.is_positive is False):\n            sign = (- sign)\n            if saw_NOT:\n                return\n            saw_NOT = True\n        elif (t.is_negative is False):\n            if saw_NOT:\n                return\n            saw_NOT = True\n        else:\n            return\n    if ((sign == 1) and (saw_NON is False) and (saw_NOT is False)):\n        return True\n    if (sign < 0):\n        return False\n", "label": 1}
{"function": "\n\ndef selector(self, output):\n    if isinstance(output, compute_base.Node):\n        return self.parse(output, FieldLists.NODE)\n    elif isinstance(output, compute_base.NodeSize):\n        return self.parse(output, FieldLists.NODE_SIZE)\n    elif isinstance(output, compute_base.NodeImage):\n        return self.parse(output, FieldLists.NODE_IMAGE)\n    elif isinstance(output, compute_base.NodeLocation):\n        return self.parse(output, FieldLists.LOCATION)\n    elif isinstance(output, compute_base.NodeAuthSSHKey):\n        return self.parse(output, FieldLists.NODE_KEY)\n    elif isinstance(output, compute_base.NodeAuthPassword):\n        return self.parse(output, FieldLists.NODE_PASSWORD)\n    elif isinstance(output, compute_base.StorageVolume):\n        return self.parse(output, FieldLists.STORAGE_VOLUME)\n    elif isinstance(output, compute_base.VolumeSnapshot):\n        return self.parse(output, FieldLists.VOLUME_SNAPSHOT)\n    elif isinstance(output, dns_base.Zone):\n        return self.parse(output, FieldLists.ZONE)\n    elif isinstance(output, dns_base.Record):\n        return self.parse(output, FieldLists.RECORD)\n    elif isinstance(output, lb_base.Member):\n        return self.parse(output, FieldLists.MEMBER)\n    elif isinstance(output, lb_base.LoadBalancer):\n        return self.parse(output, FieldLists.BALANCER)\n    elif isinstance(output, container_base.Container):\n        return self.parse(output, FieldLists.CONTAINER)\n    elif isinstance(output, container_base.ContainerImage):\n        return self.parse(output, FieldLists.CONTAINER_IMAGE)\n    elif isinstance(output, container_base.ContainerCluster):\n        return self.parse(output, FieldLists.CONTAINER_CLUSTER)\n    else:\n        return output\n", "label": 1}
{"function": "\n\ndef line_ends_with_close_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)\\\\*/(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(/\\\\*)(.*)\\\\*/(\\\\s*)$', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\"\"\")(.*)\"\"\"(\\\\s*)$', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\->(\\\\s*)$)', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)(\\\\s*)$', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=end)(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\=begin)(.*)(\\\\=end)(\\\\s*)$', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\})(\\\\s*)$', line) and (not re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)(.*)(%\\\\})(\\\\s*)$', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef localStr(self, local_addr=None, local_port=None):\n    l = []\n    w = l.append\n    w('sip:')\n    if (self.username != None):\n        w(self.username)\n        for v in self.userparams:\n            w((';%s' % v))\n        if (self.password != None):\n            w((':%s' % self.password))\n        w('@')\n    if ((local_addr != None) and ('my' in dir(self.host))):\n        w(local_addr)\n    else:\n        w(str(self.host))\n    if (self.port != None):\n        if ((local_port != None) and ('my' in dir(self.port))):\n            w((':%d' % local_port))\n        else:\n            w((':%d' % self.port))\n    if (self.usertype != None):\n        w((';user=%s' % self.usertype))\n    for n in ('transport', 'ttl', 'maddr', 'method', 'tag'):\n        v = getattr(self, n)\n        if (v != None):\n            w((';%s=%s' % (n, v)))\n    if self.lr:\n        w(';lr')\n    for v in self.other:\n        w((';%s' % v))\n    if self.headers:\n        w('?')\n        w('&'.join([('%s=%s' % (h.capitalize(), quote(v))) for (h, v) in self.headers.items()]))\n    return ''.join(l)\n", "label": 1}
{"function": "\n\ndef _handle_actions(self, state, current_run, func, sp_addr, accessed_registers):\n    se = state.se\n    if ((func is not None) and (sp_addr is not None)):\n        new_sp_addr = (sp_addr + self.project.arch.call_sp_fix)\n        actions = [a for a in state.log.actions if (a.bbl_addr == current_run.addr)]\n        for a in actions:\n            if ((a.type == 'mem') and (a.action == 'read')):\n                try:\n                    addr = se.exactly_int(a.addr.ast, default=0)\n                except claripy.ClaripyError:\n                    continue\n                if ((self.project.arch.call_pushes_ret and (addr >= new_sp_addr)) or ((not self.project.arch.call_pushes_ret) and (addr >= new_sp_addr))):\n                    offset = (addr - new_sp_addr)\n                    func._add_argument_stack_variable(offset)\n            elif (a.type == 'reg'):\n                offset = a.offset\n                if ((a.action == 'read') and (offset not in accessed_registers)):\n                    func._add_argument_register(offset)\n                elif (a.action == 'write'):\n                    accessed_registers.add(offset)\n    else:\n        l.error('handle_actions: Function not found, or stack pointer is None. It might indicates unbalanced stack.')\n", "label": 1}
{"function": "\n\ndef get_features2(self):\n    '\\n        Return all features with its names.\\n\\n        Returns\\n        -------\\n        names : list\\n            Feature names.\\n        values : list\\n            Feature values\\n        '\n    feature_names = []\n    feature_values = []\n    all_vars = vars(self)\n    for name in all_vars.keys():\n        if (not ((name == 'date') or (name == 'mag') or (name == 'err') or (name == 'n_threads') or (name == 'min_period'))):\n            if (not ((name == 'f') or (name == 'f_phase') or (name == 'period_log10FAP') or (name == 'weight') or (name == 'weighted_sum') or (name == 'median') or (name == 'mean') or (name == 'std'))):\n                feature_names.append(name)\n    feature_names.sort()\n    for name in feature_names:\n        feature_values.append(all_vars[name])\n    return (feature_names, feature_values)\n", "label": 1}
{"function": "\n\ndef test_scan_video_episode(episodes, tmpdir, monkeypatch):\n    video = episodes['bbt_s07e05']\n    monkeypatch.chdir(str(tmpdir))\n    tmpdir.ensure(video.name)\n    scanned_video = scan_video(video.name)\n    assert scanned_video.name, video.name\n    assert (scanned_video.format == video.format)\n    assert (scanned_video.release_group == video.release_group)\n    assert (scanned_video.resolution == video.resolution)\n    assert (scanned_video.video_codec == video.video_codec)\n    assert (scanned_video.audio_codec is None)\n    assert (scanned_video.imdb_id is None)\n    assert (scanned_video.hashes == {\n        \n    })\n    assert (scanned_video.size == 0)\n    assert (scanned_video.subtitle_languages == set())\n    assert (scanned_video.series == video.series)\n    assert (scanned_video.season == video.season)\n    assert (scanned_video.episode == video.episode)\n    assert (scanned_video.title is None)\n    assert (scanned_video.year is None)\n    assert (scanned_video.tvdb_id is None)\n", "label": 1}
{"function": "\n\ndef restart(args):\n    if (not args.skip_confirm):\n        deploy_utils.confirm_restart(args)\n    _get_chronos_service_config(args)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.chronos_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                stop_job(args, hosts[host_id].ip, job_name, instance_id)\n    for job_name in (args.job or ALL_JOBS):\n        hosts = args.chronos_config.jobs[job_name].hosts\n        args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)\n        for host_id in (args.task_map.keys() or hosts.keys()):\n            for instance_id in (args.task_map.get(host_id) or range(hosts[host_id].instance_num)):\n                instance_id = ((- 1) if (not deploy_utils.is_multiple_instances(host_id, hosts)) else instance_id)\n                deploy_utils.wait_for_job_stopping('chronos', args.chronos_config.cluster.name, job_name, hosts[host_id].ip, instance_id)\n                start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)\n", "label": 1}
{"function": "\n\ndef _setup_document_fields(self):\n    for f in self.document._fields.values():\n        if (not hasattr(f, 'rel')):\n            if isinstance(f, ReferenceField):\n                f.rel = Relation(f.document_type)\n                f.is_relation = True\n            elif (isinstance(f, ListField) and isinstance(f.field, ReferenceField)):\n                f.field.rel = Relation(f.field.document_type)\n                f.field.is_relation = True\n            else:\n                f.many_to_many = None\n                f.many_to_one = None\n                f.one_to_many = None\n                f.one_to_one = None\n                f.related_model = None\n                f.rel = None\n                f.is_relation = False\n        if ((not hasattr(f, 'verbose_name')) or (f.verbose_name is None)):\n            f.verbose_name = capfirst(create_verbose_name(f.name))\n        if (not hasattr(f, 'flatchoices')):\n            flat = []\n            if (f.choices is not None):\n                for (choice, value) in f.choices:\n                    if isinstance(value, (list, tuple)):\n                        flat.extend(value)\n                    else:\n                        flat.append((choice, value))\n            f.flatchoices = flat\n        if (isinstance(f, ReferenceField) and (not isinstance(f.document_type._meta, (DocumentMetaWrapper, LazyDocumentMetaWrapper))) and (self.document != f.document_type)):\n            f.document_type._meta = LazyDocumentMetaWrapper(f.document_type)\n        if (not hasattr(f, 'auto_created')):\n            f.auto_created = False\n", "label": 1}
{"function": "\n\ndef _read_one_coil_point(fid):\n    'Read coil coordinate information from the hc file'\n    one = '#'\n    while ((len(one) > 0) and (one[0] == '#')):\n        one = fid.readline()\n    if (len(one) == 0):\n        return None\n    one = one.strip().decode('utf-8')\n    if ('Unable' in one):\n        raise RuntimeError('HPI information not available')\n    p = dict()\n    p['valid'] = ('measured' in one)\n    for (key, val) in _coord_dict.items():\n        if (key in one):\n            p['coord_frame'] = val\n            break\n    else:\n        p['coord_frame'] = (- 1)\n    for (key, val) in _kind_dict.items():\n        if (key in one):\n            p['kind'] = val\n            break\n    else:\n        p['kind'] = (- 1)\n    p['r'] = np.empty(3)\n    for (ii, coord) in enumerate('xyz'):\n        sp = fid.readline().decode('utf-8').strip()\n        if (len(sp) == 0):\n            continue\n        sp = sp.split(' ')\n        if ((len(sp) != 3) or (sp[0] != coord) or (sp[1] != '=')):\n            raise RuntimeError(('Bad line: %s' % one))\n        p['r'][ii] = (float(sp[2]) / 100.0)\n    return p\n", "label": 1}
{"function": "\n\ndef asXML(self, doctag=None, namedItemsOnly=False, indent='', formatted=True):\n    'Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.'\n    nl = '\\n'\n    out = []\n    namedItems = dict(((v[1], k) for (k, vlist) in self.__tokdict.items() for v in vlist))\n    nextLevelIndent = (indent + '  ')\n    if (not formatted):\n        indent = ''\n        nextLevelIndent = ''\n        nl = ''\n    selfTag = None\n    if (doctag is not None):\n        selfTag = doctag\n    elif self.__name:\n        selfTag = self.__name\n    if (not selfTag):\n        if namedItemsOnly:\n            return ''\n        else:\n            selfTag = 'ITEM'\n    out += [nl, indent, '<', selfTag, '>']\n    worklist = self.__toklist\n    for (i, res) in enumerate(worklist):\n        if isinstance(res, ParseResults):\n            if (i in namedItems):\n                out += [res.asXML(namedItems[i], (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n            else:\n                out += [res.asXML(None, (namedItemsOnly and (doctag is None)), nextLevelIndent, formatted)]\n        else:\n            resTag = None\n            if (i in namedItems):\n                resTag = namedItems[i]\n            if (not resTag):\n                if namedItemsOnly:\n                    continue\n                else:\n                    resTag = 'ITEM'\n            xmlBodyText = _xml_escape(_ustr(res))\n            out += [nl, nextLevelIndent, '<', resTag, '>', xmlBodyText, '</', resTag, '>']\n    out += [nl, indent, '</', selfTag, '>']\n    return ''.join(out)\n", "label": 1}
{"function": "\n\n@classmethod\ndef _select(cls):\n    if cls._selecting[0]:\n        return\n    try:\n        cls._selecting[0] = True\n        for inst in cls._instances:\n            if ((not inst.connected) or (minisix.PY3 and inst.conn._closed) or (minisix.PY2 and (inst.conn._sock.__class__ is socket._closedsocket))):\n                cls._instances.remove(inst)\n            elif (inst.conn.fileno() == (- 1)):\n                inst.reconnect()\n        if (not cls._instances):\n            return\n        (rlist, wlist, xlist) = select.select([x.conn for x in cls._instances], [], [], conf.supybot.drivers.poll())\n        for instance in cls._instances:\n            if (instance.conn in rlist):\n                instance._read()\n    except select.error as e:\n        if (e.args[0] != errno.EINTR):\n            raise\n    finally:\n        cls._selecting[0] = False\n    for instance in cls._instances:\n        if (instance.irc and (not instance.irc.zombie)):\n            instance._sendIfMsgs()\n", "label": 1}
{"function": "\n\ndef on_text_command(self, view, cmd, args):\n    if (isearch_info_for(view) is not None):\n        if (cmd not in ('sbp_inc_search', 'sbp_inc_search_escape')):\n            return ('sbp_inc_search_escape', {\n                'next_cmd': cmd,\n                'next_args': args,\n            })\n        return\n    vs = ViewState.get(view)\n    self.on_anything(view)\n    if (args is None):\n        args = {\n            \n        }\n    if (not cmd.startswith('sbp_')):\n        vs.this_cmd = cmd\n    if (cmd == 'drag_select'):\n        info = isearch_info_for(view)\n        if info:\n            info.done()\n        vs.drag_count = (2 if ('by' in args) else 0)\n    if ((cmd in ('move', 'move_to')) and vs.active_mark and (not args.get('extend', False))):\n        args['extend'] = True\n        return (cmd, args)\n    if (not vs.argument_supplied):\n        return None\n    if (cmd in repeatable_cmds):\n        count = vs.get_count()\n        args.update({\n            'cmd': cmd,\n            '_times': abs(count),\n        })\n        if ((count < 0) and ('forward' in args)):\n            args['forward'] = (not args['forward'])\n        return ('sbp_do_times', args)\n    elif (cmd == 'scroll_lines'):\n        args['amount'] *= vs.get_count()\n        return (cmd, args)\n", "label": 1}
{"function": "\n\ndef _dynamic_init(self, only_fields, include_fields, exclude_fields):\n    \"\\n        Modifies `request_fields` via higher-level dynamic field interfaces.\\n\\n        Arguments:\\n            only_fields: List of field names to render.\\n                All other fields will be deferred (respects sideloads).\\n            include_fields: List of field names to include.\\n                Adds to default field set, (respects sideloads).\\n                `*` means include all fields.\\n            exclude_fields: List of field names to exclude.\\n                Removes from default field set. If set to '*', all fields are\\n                removed, except for ones that are explicitly included.\\n        \"\n    if (not self.dynamic):\n        return\n    if (isinstance(self.request_fields, dict) and (self.request_fields.pop('*', None) is False)):\n        exclude_fields = '*'\n    only_fields = set((only_fields or []))\n    include_fields = (include_fields or [])\n    exclude_fields = (exclude_fields or [])\n    all_fields = set(self.get_all_fields().keys())\n    if only_fields:\n        exclude_fields = '*'\n        include_fields = only_fields\n    if (exclude_fields == '*'):\n        include_fields = set((list(include_fields) + [field for (field, val) in six.iteritems(self.request_fields) if (val or (val == {\n            \n        }))]))\n        exclude_fields = (all_fields - include_fields)\n    elif (include_fields == '*'):\n        include_fields = all_fields\n    for name in exclude_fields:\n        self.request_fields[name] = False\n    for name in include_fields:\n        if (not isinstance(self.request_fields.get(name), dict)):\n            self.request_fields[name] = True\n", "label": 1}
{"function": "\n\ndef connect_to_chunks(connect_to, existing_connections, steps, building_chunks):\n    _connect_layers = {\n        \n    }\n    _all_chunk_keys = []\n    for room_name in existing_connections:\n        for chunk_key in existing_connections[room_name]['chunk_keys']:\n            _all_chunk_keys.append(chunk_key)\n    _allowed = False\n    for room_name in connect_to:\n        if (not (room_name in existing_connections)):\n            continue\n        _neighbors = []\n        for chunk_key in existing_connections[room_name]['chunk_keys']:\n            _temp_neighbors = get_neighbors(chunk_key, only_chunk_keys=building_chunks, avoid_chunk_keys=_all_chunk_keys)\n            if (not _temp_neighbors):\n                return (- 1)\n            _neighbors.extend(_temp_neighbors)\n        _connect_layers[room_name] = {\n            'chunk_keys': existing_connections[room_name]['chunk_keys'],\n            'neighbors': _neighbors,\n        }\n        if _neighbors:\n            _allowed = True\n    if (not _allowed):\n        return (- 1)\n    _common_neighbors = {\n        \n    }\n    _highest = 0\n    for layer in _connect_layers.values():\n        for chunk_key in layer['neighbors']:\n            if (chunk_key in _common_neighbors):\n                _common_neighbors[chunk_key] += 1\n            else:\n                _common_neighbors[chunk_key] = 1\n            if (_common_neighbors[chunk_key] > _highest):\n                _highest = _common_neighbors[chunk_key]\n    for chunk_key in _common_neighbors.keys():\n        if (_common_neighbors[chunk_key] < _highest):\n            del _common_neighbors[chunk_key]\n    if (not _common_neighbors):\n        return False\n    if (_highest < len(_connect_layers)):\n        return (- 1)\n    return random.choice(_common_neighbors.keys())\n", "label": 1}
{"function": "\n\ndef line_contains_close_block_comment(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'sql', 'c', 'cpp', 'cc', 'scala', 'php')):\n        if (re.match('^(\\\\+|\\\\-)(.*)\\\\*/', line) and (not re.match('^(\\\\+|\\\\-)(.*)/\\\\*(.*)(\\\\*/)', line))):\n            flag = True\n    elif (ext == 'py'):\n        if (re.match('^(\\\\+|\\\\-)(.*)\"\"\"', line) and (not re.match('^(\\\\+|\\\\-)(.*)\"\"\"(.*)(\"\"\")', line))):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\-\\\\->)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)', line))):\n            flag = True\n    elif (ext in 'rb'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(\\\\=end)', line) and (not re.match('^(\\\\+|\\\\-)(.*)(\\\\=begin)(.*)(\\\\=end)', line))):\n            flag = True\n    elif (ext in 'm'):\n        if (re.match('^(\\\\+|\\\\-)(.*)(%\\\\})', line) and (not re.match('^(\\\\+|\\\\-)(.*)(%\\\\{)(.*)(%\\\\})', line))):\n            flag = True\n    return flag\n", "label": 1}
{"function": "\n\ndef main(argv):\n    if (len(argv) != 2):\n        print('Usage: ./countLoc.py <file_to_count.[py | c]')\n        return\n    file = argv[1]\n    mode = file.split('.')[1]\n    if (mode == 'py'):\n        cKey = '#'\n    elif (mode == 'c'):\n        cKey = '//'\n        sKey = '/*'\n        eKey = '*/'\n    else:\n        print('Invalid extension, please input a Python or C code file')\n        return\n    comment = loc = blank = 0\n    with open(file, 'r') as f:\n        for line in f.readlines():\n            line = line.strip()\n            if (not line):\n                blank += 1\n            elif line.startswith(cKey):\n                comment += 1\n            elif ((mode == 'c') and line.startswith(sKey) and line.endswith(eKey)):\n                comment += 1\n            elif ((mode == 'c') and line.startswith(sKey)):\n                cFlag = True\n                comment += 1\n            elif ((mode == 'c') and line.endswith(eKey)):\n                cFlag = False\n                comment += 1\n            elif ((mode == 'c') and cFlag):\n                comment += 1\n            else:\n                loc += 1\n    print()\n    print('File: ', file)\n    print('LoC: ', loc)\n    print('Comments: ', comment)\n    print('Blank: ', blank)\n    print()\n", "label": 1}
{"function": "\n\ndef keyPressEvent(self, event):\n    ctrl = ((event.modifiers() & Qt.ControlModifier) != 0)\n    if ((not self.is_running) or self.textCursor().hasSelection()):\n        if ((event.key() == Qt.Key_C) and ctrl):\n            self.copy()\n        return\n    propagate_to_parent = True\n    delete = (event.key() in [Qt.Key_Backspace, Qt.Key_Delete])\n    if (delete and (not self._usr_buffer)):\n        return\n    if ((event.key() == Qt.Key_V) and ctrl):\n        text = QApplication.clipboard().text()\n        self._usr_buffer += text\n        self.setTextColor(self._stdin_col)\n        if self._mask_user_input:\n            text = (len(text) * '*')\n        self.insertPlainText(text)\n        return\n    if (event.key() in [Qt.Key_Return, Qt.Key_Enter]):\n        if (sys.platform == 'win32'):\n            self._usr_buffer += '\\r'\n        self._usr_buffer += '\\n'\n        self.process.write(self.get_user_buffer_as_bytes())\n        self._usr_buffer = ''\n    elif ((not delete) and len(event.text())):\n        txt = event.text()\n        self._usr_buffer += txt\n        if self._mask_user_input:\n            txt = '*'\n        self.setTextColor(self._stdin_col)\n        self.insertPlainText(txt)\n        propagate_to_parent = False\n    elif delete:\n        self._usr_buffer = self._usr_buffer[:(len(self._usr_buffer) - 1)]\n    if propagate_to_parent:\n        super(InteractiveConsole, self).keyPressEvent(event)\n    self.setTextColor(self._stdout_col)\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.I32):\n                self.protocol_version = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.requestorUserName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.roleName = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.component = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.STRUCT):\n                self.privilege = TSentryPrivilege()\n                self.privilege.read(iprot)\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef test_default_updates_chained(self):\n    x = shared(2)\n    y = shared(1)\n    z = shared((- 1))\n    x.default_update = (x - y)\n    y.default_update = z\n    z.default_update = (z - 1)\n    f1 = pfunc([], [x])\n    f1()\n    assert (x.get_value() == 1)\n    assert (y.get_value() == (- 1))\n    assert (z.get_value() == (- 2))\n    f2 = pfunc([], [x, y])\n    f2()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 2))\n    assert (z.get_value() == (- 3))\n    f3 = pfunc([], [y])\n    f3()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 3))\n    assert (z.get_value() == (- 4))\n    f4 = pfunc([], [x, y], no_default_updates=[x])\n    f4()\n    assert (x.get_value() == 2)\n    assert (y.get_value() == (- 4))\n    assert (z.get_value() == (- 5))\n    f5 = pfunc([], [x, y, z], no_default_updates=[z])\n    f5()\n    assert (x.get_value() == 6)\n    assert (y.get_value() == (- 5))\n    assert (z.get_value() == (- 5))\n", "label": 1}
{"function": "\n\ndef is_active(self, key, *instances, **kwargs):\n    \"\\n        Returns ``True`` if any of ``instances`` match an active switch. Otherwise\\n        returns ``False``.\\n\\n        >>> gargoyle.is_active('my_feature', request) #doctest: +SKIP\\n        \"\n    default = kwargs.pop('default', False)\n    parts = key.split(':')\n    if (len(parts) > 1):\n        child_kwargs = kwargs.copy()\n        child_kwargs['default'] = None\n        result = self.is_active(':'.join(parts[:(- 1)]), *instances, **child_kwargs)\n        if (result is False):\n            return result\n        elif (result is True):\n            default = result\n    try:\n        switch = self[key]\n    except KeyError:\n        return default\n    if (switch.status == GLOBAL):\n        return True\n    elif (switch.status == DISABLED):\n        return False\n    elif (switch.status == INHERIT):\n        return default\n    conditions = switch.value\n    if (not conditions):\n        return default\n    if instances:\n        instances = list(instances)\n        for v in instances:\n            if (isinstance(v, HttpRequest) and hasattr(v, 'user')):\n                instances.append(v.user)\n    return_value = False\n    for switch in self._registry.itervalues():\n        result = switch.has_active_condition(conditions, instances)\n        if (result is False):\n            return False\n        elif (result is True):\n            return_value = True\n    return return_value\n", "label": 1}
{"function": "\n\ndef concat(docs):\n    '\\n    Concatenate together the contents of multiple documents from a\\n    single corpus, using an appropriate concatenation function.  This\\n    utility function is used by corpus readers when the user requests\\n    more than one document at a time.\\n    '\n    if (len(docs) == 1):\n        return docs[0]\n    if (len(docs) == 0):\n        raise ValueError('concat() expects at least one object!')\n    types = set((d.__class__ for d in docs))\n    if all((isinstance(doc, string_types) for doc in docs)):\n        return ''.join(docs)\n    for typ in types:\n        if (not issubclass(typ, (StreamBackedCorpusView, ConcatenatedCorpusView))):\n            break\n    else:\n        return ConcatenatedCorpusView(docs)\n    for typ in types:\n        if (not issubclass(typ, AbstractLazySequence)):\n            break\n    else:\n        return LazyConcatenation(docs)\n    if (len(types) == 1):\n        typ = list(types)[0]\n        if issubclass(typ, list):\n            return reduce((lambda a, b: (a + b)), docs, [])\n        if issubclass(typ, tuple):\n            return reduce((lambda a, b: (a + b)), docs, ())\n        if ElementTree.iselement(typ):\n            xmltree = ElementTree.Element('documents')\n            for doc in docs:\n                xmltree.append(doc)\n            return xmltree\n    raise ValueError((\"Don't know how to concatenate types: %r\" % types))\n", "label": 1}
{"function": "\n\ndef _quick_drag_menu(self, object):\n    ' Displays the quick drag menu for a specified drag object.\\n        '\n    feature_lists = []\n    if isinstance(object, IFeatureTool):\n        msg = 'Apply to'\n        for dc in self.dock_control.dock_controls:\n            if (dc.visible and (object.feature_can_drop_on(dc.object) or object.feature_can_drop_on_dock_control(dc))):\n                from feature_tool import FeatureTool\n                feature_lists.append([FeatureTool(dock_control=dc)])\n    else:\n        msg = 'Send to'\n        for dc in self.dock_control.dock_controls:\n            if dc.visible:\n                allowed = [f for f in dc.features if ((f.feature_name != '') and f.can_drop(object))]\n                if (len(allowed) > 0):\n                    feature_lists.append(allowed)\n    if (len(feature_lists) > 0):\n        features = []\n        actions = []\n        for list in feature_lists:\n            if (len(list) > 1):\n                sub_actions = []\n                for feature in list:\n                    sub_actions.append(Action(name=('%s Feature' % feature.feature_name), action=('self._drop_on(%d)' % len(features))))\n                    features.append(feature)\n                actions.append(Menu(*sub_actions, name=('%s the %s' % (msg, feature.dock_control.name))))\n            else:\n                actions.append(Action(name=('%s %s' % (msg, list[0].dock_control.name)), action=('self._drop_on(%d)' % len(features))))\n                features.append(list[0])\n        self._object = object\n        self._features = features\n        self.popup_menu(Menu(*actions, name='popup'))\n        self._object = self._features = None\n", "label": 1}
{"function": "\n\ndef extend_schema(schema, doc):\n    schema_kind = get_kind(schema)\n    doc_kind = get_kind(doc)\n    if (doc_kind == 'null'):\n        return schema\n    if (schema_kind == 'null'):\n        return make_schema(doc)\n    if ((schema_kind != 'list') and (doc_kind == 'list')):\n        schema_kind = 'list'\n        schema = [schema]\n    if ((doc_kind != 'list') and (schema_kind == 'list')):\n        doc_kind = 'list'\n        doc = [doc]\n    if ((schema_kind != 'dict') and (doc_kind == 'dict')):\n        if (not (schema_kind == 'string')):\n            raise SchemaInferenceError((\"%r is type %r but should be type 'string'!!\" % (schema, schema_kind)))\n        schema_kind = 'dict'\n        schema = {\n            '': schema_kind,\n        }\n    if ((doc_kind != 'dict') and (schema_kind == 'dict')):\n        if (not (doc_kind == 'string')):\n            raise SchemaInferenceError((\"%r is type %r but should be type 'string'!!\" % (doc, doc_kind)))\n        doc_kind = 'dict'\n        doc = {\n            '': doc_kind,\n        }\n    if (schema_kind == doc_kind == 'dict'):\n        for key in doc:\n            schema[key] = extend_schema(schema.get(key, None), doc[key])\n        return schema\n    if (schema_kind == doc_kind == 'list'):\n        for doc_ in doc:\n            schema[0] = extend_schema(schema[0], doc_)\n        return schema\n    if (schema_kind == doc_kind == 'string'):\n        return 'string'\n    raise SchemaInferenceError(('Mismatched schema (%r) and doc (%r)' % (schema, doc)))\n", "label": 1}
{"function": "\n\ndef is_open(location, now=None):\n    '\\n    Is the company currently open? Pass \"now\" to test with a specific\\n    timestamp. Can be used stand-alone or as a helper.\\n    '\n    if (now is None):\n        now = get_now()\n    if has_closing_rule_for_now(location):\n        return False\n    now_time = datetime.time(now.hour, now.minute, now.second)\n    if location:\n        ohs = OpeningHours.objects.filter(company=location)\n    else:\n        ohs = Company.objects.first().openinghours_set.all()\n    for oh in ohs:\n        is_open = False\n        if ((oh.weekday == now.isoweekday()) and (oh.from_hour <= now_time) and (now_time <= oh.to_hour)):\n            is_open = oh\n        if ((oh.weekday == now.isoweekday()) and (oh.from_hour <= now_time) and ((oh.to_hour < oh.from_hour) and (now_time < datetime.time(23, 59, 59)))):\n            is_open = oh\n        if ((oh.weekday == ((now.isoweekday() - 1) % 7)) and (oh.from_hour >= now_time) and (oh.to_hour >= now_time) and (oh.to_hour < oh.from_hour)):\n            is_open = oh\n        if (is_open is not False):\n            return oh\n    return False\n", "label": 1}
{"function": "\n\ndef tag_sent(chunks, args):\n    tagged_sent = []\n    global_index = 0\n    for chunk in chunks:\n        if (type(chunk) is Tree):\n            tokens = chunk.leaves()\n        else:\n            tokens = [chunk]\n            chunk_label = 'O'\n        for c in range(global_index, (global_index + len(tokens))):\n            info_label = 'O'\n            local_index = (c - global_index)\n            word = tokens[local_index][0]\n            tag = tokens[local_index][1]\n            if ((type(chunk) is Tree) and (local_index == 0)):\n                chunk_label = ('B-' + chunk.label())\n            elif (type(chunk) is Tree):\n                chunk_label = ('I-' + chunk.label())\n            for arg1 in args[0]:\n                if (c in arg1):\n                    if (c == arg1[0]):\n                        info_label = 'B-Arg1'\n                    else:\n                        info_label = 'I-Arg1'\n            for arg2 in args[1]:\n                if ((info_label == 'O') and (c in arg2)):\n                    if (c == arg2[0]):\n                        info_label = 'B-Arg2'\n                    else:\n                        info_label = 'I-Arg2'\n            for rel in args[2]:\n                if ((info_label == 'O') and (c in rel)):\n                    if (c == rel[0]):\n                        info_label = 'B-Rel'\n                    else:\n                        info_label = 'I-Rel'\n            tagged_sent.append((word, tag, chunk_label, info_label))\n        global_index += len(tokens)\n    return tagged_sent\n", "label": 1}
{"function": "\n\ndef handle_starttag(self, tag, attrs):\n    if (tag == 'a'):\n        for (name, value) in attrs:\n            if ((name == 'class') and (value == 'twelve-days-claim')):\n                self.claimAttrs = attrs\n    elif (tag == 'div'):\n        for (name, value) in attrs:\n            if ((name == 'class') and (value == 'dotd-title')):\n                self.bookTitleTagFound = True\n    elif (tag == 'form'):\n        for (name, value) in attrs:\n            if ((name == 'id') and (value == 'packt-user-login-form')):\n                self.loginFormTagFound = True\n    elif (self.loginFormTagFound and (tag == 'input')):\n        for (name, value) in attrs:\n            if ((name == 'name') and (value == 'form_build_id')):\n                self.formBuildIdAttrs = attrs\n", "label": 1}
{"function": "\n\n@frappe.whitelist()\ndef run(report_name, filters=()):\n    report = get_report_doc(report_name)\n    if (filters and isinstance(filters, basestring)):\n        filters = json.loads(filters)\n    if (not frappe.has_permission(report.ref_doctype, 'report')):\n        frappe.msgprint(_('Must have report permission to access this report.'), raise_exception=True)\n    (columns, result, message) = ([], [], None)\n    if (report.report_type == 'Query Report'):\n        if (not report.query):\n            frappe.msgprint(_('Must specify a Query to run'), raise_exception=True)\n        if (not report.query.lower().startswith('select')):\n            frappe.msgprint(_('Query must be a SELECT'), raise_exception=True)\n        result = [list(t) for t in frappe.db.sql(report.query, filters)]\n        columns = [cstr(c[0]) for c in frappe.db.get_description()]\n    else:\n        module = (report.module or frappe.db.get_value('DocType', report.ref_doctype, 'module'))\n        if (report.is_standard == 'Yes'):\n            method_name = (get_report_module_dotted_path(module, report.name) + '.execute')\n            res = frappe.get_attr(method_name)(frappe._dict(filters))\n            (columns, result) = (res[0], res[1])\n            if (len(res) > 2):\n                message = res[2]\n    if (report.apply_user_permissions and result):\n        result = get_filtered_data(report.ref_doctype, columns, result)\n    if (cint(report.add_total_row) and result):\n        result = add_total_row(result, columns)\n    return {\n        'result': result,\n        'columns': columns,\n        'message': message,\n    }\n", "label": 1}
{"function": "\n\ndef Prepend(self, **kw):\n    'Prepend values to existing construction variables\\n        in an Environment.\\n        '\n    kw = copy_non_reserved_keywords(kw)\n    for (key, val) in kw.items():\n        try:\n            orig = self._dict[key]\n        except KeyError:\n            self._dict[key] = val\n        else:\n            try:\n                update_dict = orig.update\n            except AttributeError:\n                try:\n                    self._dict[key] = (val + orig)\n                except (KeyError, TypeError):\n                    try:\n                        add_to_val = val.append\n                    except AttributeError:\n                        if val:\n                            orig.insert(0, val)\n                    else:\n                        if orig:\n                            add_to_val(orig)\n                        self._dict[key] = val\n            else:\n                if SCons.Util.is_List(val):\n                    for v in val:\n                        orig[v] = None\n                else:\n                    try:\n                        update_dict(val)\n                    except (AttributeError, TypeError, ValueError):\n                        if SCons.Util.is_Dict(val):\n                            for (k, v) in val.items():\n                                orig[k] = v\n                        else:\n                            orig[val] = None\n    self.scanner_map_delete(kw)\n", "label": 1}
{"function": "\n\ndef _check_auto_matrix_indices_in_call(self, *indices):\n    matrix_behavior_kinds = dict()\n    if (len(indices) != len(self.index_types)):\n        if (not self._matrix_behavior):\n            raise ValueError('wrong number of indices')\n        ldiff = (len(self.index_types) - len(indices))\n        if (ldiff > 2):\n            raise ValueError('wrong number of indices')\n        if (ldiff == 2):\n            mat_ind = [len(indices), (len(indices) + 1)]\n        elif (ldiff == 1):\n            mat_ind = [len(indices)]\n        not_equal = True\n    else:\n        not_equal = False\n        mat_ind = [i for (i, e) in enumerate(indices) if (e is True)]\n        if mat_ind:\n            not_equal = True\n        indices = tuple([_ for _ in indices if (_ is not True)])\n        for (i, el) in enumerate(indices):\n            if (not isinstance(el, TensorIndex)):\n                not_equal = True\n                break\n            if (el._tensortype != self.index_types[i]):\n                not_equal = True\n                break\n    if not_equal:\n        for el in mat_ind:\n            eltyp = self.index_types[el]\n            if (eltyp in matrix_behavior_kinds):\n                elind = (- self.index_types[el].auto_right)\n                matrix_behavior_kinds[eltyp].append(elind)\n            else:\n                elind = self.index_types[el].auto_left\n                matrix_behavior_kinds[eltyp] = [elind]\n            indices = ((indices[:el] + (elind,)) + indices[el:])\n    return (indices, matrix_behavior_kinds)\n", "label": 1}
{"function": "\n\ndef table2story(self, ar, column_names=None, header_level=None, nosummary=False, stripped=True, **kwargs):\n    'Render the given table request as reStructuredText to stdout.\\n        See :meth:`ar.show <lino.core.request.BaseRequest.show>`.\\n        '\n    if ((ar.actor.master is not None) and (not nosummary)):\n        if (ar.actor.slave_grid_format == 'summary'):\n            s = E.to_rst(ar.actor.get_slave_summary(ar.master_instance, ar), stripped=stripped)\n            if stripped:\n                s = s.strip()\n            return s\n    (fields, headers, widths) = ar.get_field_info(column_names)\n    sums = [fld.zero for fld in fields]\n    rows = []\n    recno = 0\n    for row in ar.sliced_data_iterator:\n        recno += 1\n        rows.append([x for x in ar.row2text(fields, row, sums)])\n    if (len(rows) == 0):\n        s = str(ar.no_data_text)\n        if (not stripped):\n            s = (('\\n' + s) + '\\n')\n        return s\n    if (not ar.actor.hide_sums):\n        has_sum = False\n        for i in sums:\n            if i:\n                has_sum = True\n                break\n        if has_sum:\n            rows.append([x for x in ar.sums2html(fields, sums)])\n    t = RstTable(headers, **kwargs)\n    s = t.to_rst(rows)\n    if (header_level is not None):\n        h = rstgen.header(header_level, ar.get_title())\n        if stripped:\n            h = h.strip()\n        s = ((h + '\\n') + s)\n    return s\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    for base in bases:\n        for base_name in dir(base):\n            base_value = getattr(base, base_name)\n            if ((not callable(base_value)) or (not base_name.startswith('assert'))):\n                continue\n            if (not (base_name in attrs)):\n                attrs.update({\n                    base_name: base_value,\n                })\n    for (attr_name, attr_value) in attrs.items():\n        if ((not attr_name.startswith('assert')) or (attr_name == 'assert_')):\n            continue\n        if (attr_name[6] == '_'):\n            new_name = underscore_to_camelcase(attr_name)\n        else:\n            new_name = camelcase_to_underscore(attr_name)\n        if (not (new_name in attrs)):\n            attrs[new_name] = attr_value\n    for attr in tools.__all__:\n        attrs.update({\n            attr: getattr(tools, attr),\n        })\n        attrs[attr] = staticmethod(attrs[attr])\n    if datadiff_assert_equal:\n        key = ((attrs.get('use_datadiff', False) and 'assert_equal') or 'datadiff_assert_equal')\n        attrs.update({\n            key: datadiff_assert_equal,\n        })\n        attrs[key] = staticmethod(attrs[key])\n    elif attrs.get('use_datadiff', False):\n        warnings.warn('You enabled ``datadiff.tools.assert_equal``, but looks like you have not ``datadiff`` library installed in your system.')\n    return type.__new__(cls, name, bases, attrs)\n", "label": 1}
{"function": "\n\ndef show_run_vlan(self):\n    self.write_line('spanning-tree')\n    self.write_line('!')\n    self.write_line('!')\n    for vlan in sorted(self.switch_configuration.vlans, key=(lambda v: v.number)):\n        if vlan_name(vlan):\n            self.write_line(('vlan %d name %s' % (vlan.number, vlan_name(vlan))))\n        else:\n            self.write_line(('vlan %d' % vlan.number))\n        untagged_ports = []\n        for port in self.switch_configuration.ports:\n            if (not isinstance(port, VlanPort)):\n                if ((vlan.number == 1) and (port.access_vlan is None) and (port.trunk_native_vlan is None)):\n                    untagged_ports.append(port)\n                elif ((port.access_vlan == vlan.number) or (port.trunk_native_vlan == vlan.number)):\n                    untagged_ports.append(port)\n        if (len(untagged_ports) > 0):\n            if (vlan.number == 1):\n                self.write_line((' no untagged %s' % to_port_ranges(untagged_ports)))\n            else:\n                self.write_line((' untagged %s' % to_port_ranges(untagged_ports)))\n        tagged_ports = [p for p in self.switch_configuration.ports if (p.trunk_vlans and (vlan.number in p.trunk_vlans))]\n        if tagged_ports:\n            self.write_line((' tagged %s' % to_port_ranges(tagged_ports)))\n        vif = self.get_interface_vlan_for(vlan)\n        if (vif is not None):\n            self.write_line((' router-interface %s' % vif.name))\n        self.write_line('!')\n    self.write_line('!')\n    self.write_line('')\n", "label": 1}
{"function": "\n\ndef _parse_signature(self, args, kw):\n    values = {\n        \n    }\n    (sig_args, var_args, var_kw, defaults) = self._func_signature\n    extra_kw = {\n        \n    }\n    for (name, value) in kw.iteritems():\n        if ((not var_kw) and (name not in sig_args)):\n            raise TypeError(('Unexpected argument %s' % name))\n        if (name in sig_args):\n            values[sig_args] = value\n        else:\n            extra_kw[name] = value\n    args = list(args)\n    sig_args = list(sig_args)\n    while args:\n        while (sig_args and (sig_args[0] in values)):\n            sig_args.pop(0)\n        if sig_args:\n            name = sig_args.pop(0)\n            values[name] = args.pop(0)\n        elif var_args:\n            values[var_args] = tuple(args)\n            break\n        else:\n            raise TypeError(('Extra position arguments: %s' % ', '.join((repr(v) for v in args))))\n    for (name, value_expr) in defaults.iteritems():\n        if (name not in values):\n            values[name] = self._template._eval(value_expr, self._ns, self._pos)\n    for name in sig_args:\n        if (name not in values):\n            raise TypeError(('Missing argument: %s' % name))\n    if var_kw:\n        values[var_kw] = extra_kw\n    return values\n", "label": 1}
{"function": "\n\ndef convert_func(self, lib, opts, args):\n    if (not opts.dest):\n        opts.dest = self.config['dest'].get()\n    if (not opts.dest):\n        raise ui.UserError('no convert destination set')\n    opts.dest = util.bytestring_path(opts.dest)\n    if (not opts.threads):\n        opts.threads = self.config['threads'].get(int)\n    if self.config['paths']:\n        path_formats = ui.get_path_formats(self.config['paths'])\n    else:\n        path_formats = ui.get_path_formats()\n    if (not opts.format):\n        opts.format = self.config['format'].get(unicode).lower()\n    pretend = (opts.pretend if (opts.pretend is not None) else self.config['pretend'].get(bool))\n    if (not pretend):\n        ui.commands.list_items(lib, ui.decargs(args), opts.album)\n        if (not (opts.yes or ui.input_yn('Convert? (Y/n)'))):\n            return\n    if opts.album:\n        albums = lib.albums(ui.decargs(args))\n        items = (i for a in albums for i in a.items())\n        if self.config['copy_album_art']:\n            for album in albums:\n                self.copy_album_art(album, opts.dest, path_formats, pretend)\n    else:\n        items = iter(lib.items(ui.decargs(args)))\n    convert = [self.convert_item(opts.dest, opts.keep_new, path_formats, opts.format, pretend) for _ in range(opts.threads)]\n    pipe = util.pipeline.Pipeline([items, convert])\n    pipe.run_parallel()\n", "label": 1}
{"function": "\n\ndef on_spawn(self, view):\n    \"When a new view is spawned, postion the view per user's preference.\"\n    window = view.window()\n    if (window and (window.get_view_index(view)[1] != (- 1))):\n        loaded = view.settings().get('tabs_extra_spawned', False)\n        if (not loaded):\n            sheet = window.active_sheet()\n            spawn = view_spawn_pos()\n            if (spawn != 'none'):\n                sheets = window.sheets()\n                (group, index) = window.get_sheet_index(sheet)\n                last_group = None\n                last_index = None\n                if (LAST_ACTIVE is not None):\n                    for s in sheets:\n                        v = s.view()\n                        if ((v is not None) and (LAST_ACTIVE.id() == v.id())):\n                            (last_group, last_index) = window.get_sheet_index(s)\n                            break\n                active_in_range = ((last_group is not None) and (last_index is not None) and (last_group == group))\n                if (spawn == 'right'):\n                    (group, index) = window.get_sheet_index(sheets[(- 1)])\n                    window.set_sheet_index(sheet, group, index)\n                elif (spawn == 'left'):\n                    (group, index) = window.get_sheet_index(sheets[0])\n                    window.set_sheet_index(sheet, group, index)\n                elif ((spawn == 'active_right') and active_in_range):\n                    window.set_sheet_index(sheet, group, (last_index + 1))\n                elif ((spawn == 'active_left') and active_in_range):\n                    window.set_sheet_index(sheet, group, last_index)\n            view.settings().set('tabs_extra_spawned', True)\n", "label": 1}
{"function": "\n\ndef _load(self, event):\n    if (event.type == START_OBJECT):\n        value = start = '{'\n        end = '}'\n    elif (event.type == START_ARRAY):\n        value = start = '['\n        end = ']'\n    else:\n        raise JSONParseError(JSON_UNEXPECTED_ELEMENT_ERROR, ('Unexpected event: ' + event.type))\n    count = 1\n    tokens = self.tokens\n    tokenIndex = self.tokenIndex\n    inString = False\n    inEscape = False\n    try:\n        while True:\n            startIndex = tokenIndex\n            for token in tokens[startIndex:]:\n                tokenIndex += 1\n                if (token == ''):\n                    pass\n                elif inString:\n                    if inEscape:\n                        inEscape = False\n                    elif (token == '\"'):\n                        inString = False\n                    elif (token == '\\\\'):\n                        inEscape = True\n                elif (token == '\"'):\n                    inString = True\n                elif (token == start):\n                    count += 1\n                elif (token == end):\n                    count -= 1\n                    if (count == 0):\n                        value += ''.join(tokens[startIndex:tokenIndex])\n                        raise StopIteration()\n            value += ''.join(tokens[startIndex:])\n            data = self.stream.read(self.size)\n            if (data == ''):\n                raise JSONParseError(JSON_INCOMPLETE_ERROR, 'Reached end of input before reaching end of JSON structures.')\n            tokens = self.pattern.split(data)\n            tokenIndex = 0\n    except StopIteration:\n        pass\n    self.tokens = tokens\n    self.tokenIndex = tokenIndex\n    try:\n        return json.loads(value, parse_float=decimal.Decimal, parse_int=decimal.Decimal)\n    except ValueError as e:\n        raise JSONParseError(JSON_SYNTAX_ERROR, ''.join(e.args))\n", "label": 1}
{"function": "\n\ndef _scheduleTransmitActual(self, intent):\n    if (intent.targetAddr == self.myAddress):\n        self._processReceivedEnvelope(ReceiveEnvelope(intent.targetAddr, intent.message))\n        return self._finishIntent(intent)\n    if isinstance(intent.targetAddr.addressDetails, RoutedTCPv4ActorAddress):\n        if (not isinstance(intent.message, ForwardMessage)):\n            routing = [(A or self._adminAddr) for A in intent.targetAddr.addressDetails.routing]\n            while (routing and (routing[0] == self.myAddress)):\n                routing = routing[1:]\n            if (self.txOnly and routing and (routing[0] != self._adminAddr)):\n                routing.insert(0, self._adminAddr)\n            if routing:\n                if ((len(routing) != 1) or (routing[0] != intent.targetAddr)):\n                    intent.changeMessage(ForwardMessage(intent.message, intent.targetAddr, self.myAddress, routing))\n                    intent.addCallback((lambda r, i, ta=intent.targetAddr: i.changeTargetAddr(ta)))\n                    intent.changeTargetAddr(intent.message.fwdTargets[0])\n                    self.scheduleTransmit(getattr(self, '_addressMgr', None), intent)\n                    return\n    intent.stage = self._XMITStepSendConnect\n    if self._nextTransmitStep(intent):\n        if hasattr(intent, 'socket'):\n            self._transmitIntents[intent.socket.fileno()] = intent\n        else:\n            self._waitingTransmits.append(intent)\n", "label": 1}
{"function": "\n\ndef test_update_crossing_duration1_not_duration_at_1st_step(self, duration1, duration2):\n    global rec\n    if (duration2 == 0.0):\n        return\n    node = CocosNode()\n    name1 = '1'\n    name2 = '2'\n    a1 = UIntervalAction(name1, duration1)\n    a2 = UIntervalAction(name2, duration2)\n    composite = ac.sequence(a1, a2)\n    rec = []\n    node.do(composite)\n    elapsed = 0.0\n    next_elapsed = ((duration1 + duration2) / 2.0)\n    dt = (next_elapsed - elapsed)\n    node._step(dt)\n    recx = [e for e in rec if (e[1] != 'step')]\n    rec = [e for e in recx if (e[0] == name1)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert ((rec[1][1] == 'update') and (rec[1][2] == 1.0))\n    assert (rec[2][1] == 'stop')\n    assert (len(rec) == 3)\n    rec = [e for e in recx if (e[0] == name2)]\n    print('rec', rec)\n    assert (rec[0][1] == 'start')\n    assert (rec[1][1] == 'update')\n    assert (abs((rec[1][2] - ((next_elapsed - duration1) / duration2))) < fe)\n    assert (len(rec) == 2)\n", "label": 1}
{"function": "\n\ndef norm(x, ord):\n    x = as_tensor_variable(x)\n    ndim = x.ndim\n    if (ndim == 0):\n        raise ValueError(\"'axis' entry is out of bounds.\")\n    elif (ndim == 1):\n        if (ord is None):\n            return (tensor.sum((x ** 2)) ** 0.5)\n        elif (ord == 'inf'):\n            return tensor.max(abs(x))\n        elif (ord == '-inf'):\n            return tensor.min(abs(x))\n        elif (ord == 0):\n            return x[x.nonzero()].shape[0]\n        else:\n            try:\n                z = (tensor.sum(abs((x ** ord))) ** (1.0 / ord))\n            except TypeError:\n                raise ValueError('Invalid norm order for vectors.')\n            return z\n    elif (ndim == 2):\n        if ((ord is None) or (ord == 'fro')):\n            return (tensor.sum(abs((x ** 2))) ** 0.5)\n        elif (ord == 'inf'):\n            return tensor.max(tensor.sum(abs(x), 1))\n        elif (ord == '-inf'):\n            return tensor.min(tensor.sum(abs(x), 1))\n        elif (ord == 1):\n            return tensor.max(tensor.sum(abs(x), 0))\n        elif (ord == (- 1)):\n            return tensor.min(tensor.sum(abs(x), 0))\n        else:\n            raise ValueError(0)\n    elif (ndim > 2):\n        raise NotImplementedError(\"We don't support norm witn ndim > 2\")\n", "label": 1}
{"function": "\n\ndef pytest_generate_tests(metafunc):\n    'Parametrize tests to run on all backends.'\n    if ('backend' in metafunc.fixturenames):\n        skip_backends = set()\n        if hasattr(metafunc.module, 'skip_backends'):\n            skip_backends = skip_backends.union(set(metafunc.module.skip_backends))\n        if hasattr(metafunc.cls, 'skip_backends'):\n            skip_backends = skip_backends.union(set(metafunc.cls.skip_backends))\n        if metafunc.config.option.backend:\n            backend = set([x.lower() for x in metafunc.config.option.backend])\n        else:\n            backend = set(backends.keys())\n        if hasattr(metafunc.module, 'backends'):\n            backend = backend.intersection(set(metafunc.module.backends))\n        if hasattr(metafunc.cls, 'backends'):\n            backend = backend.intersection(set(metafunc.cls.backends))\n        lazy = []\n        if (not (('skip_greedy' in metafunc.fixturenames) or metafunc.config.option.lazy)):\n            lazy.append('greedy')\n        if (not (('skip_lazy' in metafunc.fixturenames) or metafunc.config.option.greedy)):\n            lazy.append('lazy')\n        backend = [b for b in backend.difference(skip_backends) if (not (('skip_' + b) in metafunc.fixturenames))]\n        params = list(product(backend, lazy))\n        metafunc.parametrize('backend', (params or [(None, None)]), indirect=True, ids=['-'.join(p) for p in params])\n", "label": 1}
{"function": "\n\n@specialize.ll()\ndef wrap(*_pyval):\n    if (len(_pyval) == 1):\n        pyval = _pyval[0]\n        if isinstance(pyval, bool):\n            return (w_true if pyval else w_false)\n        if isinstance(pyval, int):\n            return W_Fixnum(pyval)\n        if isinstance(pyval, float):\n            return W_Flonum(pyval)\n        if isinstance(pyval, W_Object):\n            return pyval\n    elif (len(_pyval) == 2):\n        car = _pyval[0]\n        cdr = wrap(_pyval[1])\n        if isinstance(car, bool):\n            if cdr.is_proper_list():\n                return W_WrappedConsProper(wrap(car), cdr)\n            return W_WrappedCons(wrap(car), cdr)\n        if isinstance(car, int):\n            if cdr.is_proper_list():\n                return W_UnwrappedFixnumConsProper(car, cdr)\n            return W_UnwrappedFixnumCons(car, cdr)\n        if isinstance(car, float):\n            if cdr.is_proper_list():\n                return W_UnwrappedFlonumConsProper(car, cdr)\n            return W_UnwrappedFlonumCons(car, cdr)\n        if isinstance(car, W_Object):\n            return W_Cons.make(car, cdr)\n    assert False\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.ArchiveLocalID = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.CapsuleID = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.title = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.description = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.BOOL):\n                self.sandbox = iprot.readBool()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef line_is_fully_commented(self, line, ext):\n    flag = False\n    if (ext in ('java', 'js', 'c', 'cpp', 'cc', 'scala')):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(/\\\\*)(.*)\\\\*/(\\\\s*)$', line) or re.match('^(\\\\+|\\\\-)(\\\\s*)//', line)):\n            flag = True\n    elif (ext in ('py', 'rb')):\n        if re.match('^(\\\\+|\\\\-)(\\\\s*)\\\\#', line):\n            flag = True\n    elif (ext in ('xml', 'html')):\n        if re.match('^(\\\\+|\\\\-)(\\\\s*)(<\\\\!\\\\-\\\\-)(.*)(\\\\-\\\\->)(\\\\s*)$', line):\n            flag = True\n    elif (ext == 'sql'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(/\\\\*)(.*)(\\\\*/)(\\\\s*)$', line) or re.match('^(\\\\+|\\\\-)(\\\\s*)(\\\\-\\\\-\\\\s)', line)):\n            flag = True\n    elif (ext == 'php'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(/\\\\*)(.*)\\\\*/(\\\\s*)$', line) or re.match('^(\\\\+|\\\\-)(\\\\s*)//', line) or re.match('^(\\\\+|\\\\-)(\\\\s*)\\\\#', line)):\n            flag = True\n    elif (ext == 'm'):\n        if (re.match('^(\\\\+|\\\\-)(\\\\s*)(%)', line) or re.match('^(\\\\+|\\\\-)(\\\\s*)(%\\\\{)(.*)(%\\\\})(\\\\s*)$', line)):\n            flag = True\n    else:\n        self.logger.warning(('GitQuerier: impossible to identify comments for extension: ' + ext))\n    return flag\n", "label": 1}
{"function": "\n\ndef body(self, dirname=None, part=None):\n    '\\n        Recursively traverses a directory and generates the multipart encoded\\n        body.\\n        '\n    if (part is None):\n        outer = True\n        part = self.envelope\n        dirname = self.directory\n    else:\n        outer = False\n    if (dirname is None):\n        dirname = part.name\n    for chunk in self.gen_chunks(part.open()):\n        (yield chunk)\n    subpart = BodyGenerator(dirname)\n    for chunk in self.gen_chunks(subpart.write_headers()):\n        (yield chunk)\n    (files, subdirs) = utils.ls_dir(dirname)\n    for fn in files:\n        if (not fnmatch.fnmatch(fn, self.fnpattern)):\n            continue\n        fullpath = os.path.join(dirname, fn)\n        for chunk in self.gen_chunks(subpart.file_open(fullpath)):\n            (yield chunk)\n        with open(fullpath, 'rb') as fp:\n            for chunk in self.file_chunks(fp):\n                (yield chunk)\n        for chunk in self.gen_chunks(subpart.file_close()):\n            (yield chunk)\n    if self.recursive:\n        for subdir in subdirs:\n            fullpath = os.path.join(dirname, subdir)\n            for chunk in self.body(fullpath, subpart):\n                (yield chunk)\n    for chunk in self.gen_chunks(subpart.close()):\n        (yield chunk)\n    if outer:\n        for chunk in self.close():\n            (yield chunk)\n", "label": 1}
{"function": "\n\ndef merge_tops(self, tops):\n    '\\n        Cleanly merge the top files\\n        '\n    top = collections.defaultdict(OrderedDict)\n    orders = collections.defaultdict(OrderedDict)\n    for ctops in six.itervalues(tops):\n        for ctop in ctops:\n            for (saltenv, targets) in six.iteritems(ctop):\n                if (saltenv == 'include'):\n                    continue\n                for tgt in targets:\n                    matches = []\n                    states = OrderedDict()\n                    orders[saltenv][tgt] = 0\n                    ignore_missing = False\n                    for comp in ctop[saltenv][tgt]:\n                        if isinstance(comp, dict):\n                            if ('match' in comp):\n                                matches.append(comp)\n                            if ('order' in comp):\n                                order = comp['order']\n                                if (not isinstance(order, int)):\n                                    try:\n                                        order = int(order)\n                                    except ValueError:\n                                        order = 0\n                                orders[saltenv][tgt] = order\n                            if comp.get('ignore_missing', False):\n                                ignore_missing = True\n                        if isinstance(comp, six.string_types):\n                            states[comp] = True\n                    if ignore_missing:\n                        if (saltenv not in self.ignored_pillars):\n                            self.ignored_pillars[saltenv] = []\n                        self.ignored_pillars[saltenv].extend(states.keys())\n                    top[saltenv][tgt] = matches\n                    top[saltenv][tgt].extend(states)\n    return self.sort_top_targets(top, orders)\n", "label": 1}
{"function": "\n\ndef checkReplaceSubstring(self, arguments):\n    allowedFields = {\n        \n    }\n    allowedFields['replace'] = (str, True)\n    allowedFields['with'] = (str, True)\n    for category in arguments.keys():\n        for argumentAttributes in arguments[category]:\n            if ('replace substring' in argumentAttributes):\n                argument = argumentAttributes['long form argument']\n                replace = argumentAttributes['replace substring']\n                for data in replace:\n                    if (not methods.checkIsDictionary(data, self.allowTermination)):\n                        if self.allowTermination:\n                            self.errors.invalidReplaceSubstring(self.name, category, argument)\n                        else:\n                            return False\n                    observedFields = []\n                    toReplace = None\n                    replaceWith = None\n                    for key in data:\n                        value = data[key]\n                        if (key not in allowedFields):\n                            if self.allowTermination:\n                                self.errors.invalidReplaceSubstring(self.name, category, argument)\n                            else:\n                                return False\n                        if (key == 'replace'):\n                            toReplace = value\n                        elif (key == 'with'):\n                            replaceWith = value\n                        observedFields.append(key)\n                    self.arguments[argument].isReplaceSubstring = True\n                    self.arguments[argument].replaceSubstring.append((str(toReplace), str(replaceWith)))\n                    for field in allowedFields.keys():\n                        if (allowedFields[field][1] and (field not in observedFields)):\n                            if self.allowTermination:\n                                self.errors.invalidReplaceSubstring(self.name, category, argument)\n                            else:\n                                return False\n    return True\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_jid_ != x.has_jid_):\n        return 0\n    if (self.has_jid_ and (self.jid_ != x.jid_)):\n        return 0\n    if (self.has_type_ != x.has_type_):\n        return 0\n    if (self.has_type_ and (self.type_ != x.type_)):\n        return 0\n    if (self.has_show_ != x.has_show_):\n        return 0\n    if (self.has_show_ and (self.show_ != x.show_)):\n        return 0\n    if (self.has_status_ != x.has_status_):\n        return 0\n    if (self.has_status_ and (self.status_ != x.status_)):\n        return 0\n    if (self.has_from_jid_ != x.has_from_jid_):\n        return 0\n    if (self.has_from_jid_ and (self.from_jid_ != x.from_jid_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef __init__(self, bits, length=None):\n    if isinstance(bits, int):\n        if (length is None):\n            length = bits.bit_length()\n        else:\n            assert (length >= bits.bit_length())\n        if (bits < 0):\n            bits &= ((1 << length) - 1)\n        hash_value = None\n    elif isinstance(bits, BitString):\n        (bits, length, hash_value) = (bits._bits, bits._length, bits._hash)\n    elif isinstance(bits, str):\n        bit_str = bits\n        bits = 0\n        for char in bit_str:\n            bits <<= 1\n            if (char == '1'):\n                bits += 1\n            else:\n                assert (char == '0')\n        if (length is None):\n            length = len(bit_str)\n        else:\n            assert (length >= len(bit_str))\n        hash_value = None\n    else:\n        bit_sequence = bits\n        bits = 0\n        count = 0\n        for bit in bit_sequence:\n            count += 1\n            bits <<= 1\n            if bit:\n                bits += 1\n        if (length is None):\n            length = count\n        else:\n            assert (length >= count)\n        hash_value = None\n    super().__init__(bits, hash_value)\n    self._length = length\n", "label": 1}
{"function": "\n\ndef fetch(self, chrom, start, end, strand=None):\n    '\\n        For TABIX indexed BED files, find all regions w/in a range\\n\\n        For non-TABIX index BED files, use the calculated bins, and\\n        output matching regions\\n        '\n    if self.__tabix:\n        for match in self.__tabix.fetch(chrom, start, end):\n            region = BedRegion(*match.split('\\t'))\n            if ((not strand) or (strand and (region.strand == strand))):\n                (yield region)\n    else:\n        startbin = (start / BedFile._bin_const)\n        endbin = (end / BedFile._bin_const)\n        buf = set()\n        for bin in xrange(startbin, (endbin + 1)):\n            if ((chrom, bin) in self._bins):\n                for region in self._bins[(chrom, bin)]:\n                    if (strand and (strand != region.strand)):\n                        continue\n                    if ((start <= region.start <= end) or (start <= region.end <= end)):\n                        if (not (region in buf)):\n                            (yield region)\n                            buf.add(region)\n                    elif ((region.start < start) and (region.end > end)):\n                        if (not (region in buf)):\n                            (yield region)\n                            buf.add(region)\n", "label": 1}
{"function": "\n\ndef parseImpl(self, instring, loc, doActions=True):\n    if self.re:\n        result = self.re.match(instring, loc)\n        if (not result):\n            raise ParseException(instring, loc, self.errmsg, self)\n        loc = result.end()\n        return (loc, result.group())\n    if (not (instring[loc] in self.initChars)):\n        raise ParseException(instring, loc, self.errmsg, self)\n    start = loc\n    loc += 1\n    instrlen = len(instring)\n    bodychars = self.bodyChars\n    maxloc = (start + self.maxLen)\n    maxloc = min(maxloc, instrlen)\n    while ((loc < maxloc) and (instring[loc] in bodychars)):\n        loc += 1\n    throwException = False\n    if ((loc - start) < self.minLen):\n        throwException = True\n    if (self.maxSpecified and (loc < instrlen) and (instring[loc] in bodychars)):\n        throwException = True\n    if self.asKeyword:\n        if (((start > 0) and (instring[(start - 1)] in bodychars)) or ((loc < instrlen) and (instring[loc] in bodychars))):\n            throwException = True\n    if throwException:\n        raise ParseException(instring, loc, self.errmsg, self)\n    return (loc, instring[start:loc])\n", "label": 1}
{"function": "\n\ndef read(self, iprot):\n    if ((iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated) and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None) and (fastbinary is not None)):\n        fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))\n        return\n    iprot.readStructBegin()\n    while True:\n        (fname, ftype, fid) = iprot.readFieldBegin()\n        if (ftype == TType.STOP):\n            break\n        if (fid == 1):\n            if (ftype == TType.STRING):\n                self.start_key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 2):\n            if (ftype == TType.STRING):\n                self.end_key = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 3):\n            if (ftype == TType.STRING):\n                self.start_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 4):\n            if (ftype == TType.STRING):\n                self.end_token = iprot.readString()\n            else:\n                iprot.skip(ftype)\n        elif (fid == 5):\n            if (ftype == TType.I32):\n                self.count = iprot.readI32()\n            else:\n                iprot.skip(ftype)\n        else:\n            iprot.skip(ftype)\n        iprot.readFieldEnd()\n    iprot.readStructEnd()\n", "label": 1}
{"function": "\n\ndef get_params(self, event_collection=None, timeframe=None, timezone=None, interval=None, filters=None, group_by=None, target_property=None, latest=None, email=None, analyses=None, steps=None, property_names=None, percentile=None, max_age=None):\n    params = {\n        \n    }\n    if event_collection:\n        params['event_collection'] = event_collection\n    if timeframe:\n        if (type(timeframe) is dict):\n            params['timeframe'] = json.dumps(timeframe)\n        else:\n            params['timeframe'] = timeframe\n    if timezone:\n        params['timezone'] = timezone\n    if interval:\n        params['interval'] = interval\n    if filters:\n        params['filters'] = json.dumps(filters)\n    if group_by:\n        if (type(group_by) is list):\n            params['group_by'] = json.dumps(group_by)\n        else:\n            params['group_by'] = group_by\n    if target_property:\n        params['target_property'] = target_property\n    if latest:\n        params['latest'] = latest\n    if email:\n        params['email'] = email\n    if analyses:\n        params['analyses'] = json.dumps(analyses)\n    if steps:\n        params['steps'] = json.dumps(steps)\n    if property_names:\n        params['property_names'] = json.dumps(property_names)\n    if percentile:\n        params['percentile'] = percentile\n    if max_age:\n        params['max_age'] = max_age\n    return params\n", "label": 1}
{"function": "\n\ndef _eval_real_imag(self, real):\n    zero = one_neither = False\n    for t in self.args:\n        if (not t.is_complex):\n            return t.is_complex\n        elif t.is_imaginary:\n            real = (not real)\n        elif t.is_real:\n            if (not zero):\n                z = t.is_zero\n                if ((not z) and (zero is False)):\n                    zero = z\n                elif z:\n                    if all((a.is_finite for a in self.args)):\n                        return True\n                    return\n        elif (t.is_real is False):\n            if one_neither:\n                return\n            one_neither = True\n        else:\n            return\n    if one_neither:\n        if real:\n            return zero\n    elif (zero is False):\n        return real\n    elif real:\n        return real\n", "label": 1}
{"function": "\n\ndef _read_response(self, header, buffer, offset):\n    client = self.client\n    (request, async_object, xid) = client._pending.popleft()\n    if (header.zxid and (header.zxid > 0)):\n        client.last_zxid = header.zxid\n    if (header.xid != xid):\n        raise RuntimeError('xids do not match, expected %r received %r', xid, header.xid)\n    exists_error = ((header.err == NoNodeError.code) and (request.type == Exists.type))\n    if (header.err and (not exists_error)):\n        callback_exception = EXCEPTIONS[header.err]()\n        self.logger.debug('Received error(xid=%s) %r', xid, callback_exception)\n        if async_object:\n            async_object.set_exception(callback_exception)\n    elif (request and async_object):\n        if exists_error:\n            async_object.set(None)\n        else:\n            try:\n                response = request.deserialize(buffer, offset)\n            except Exception as exc:\n                self.logger.exception('Exception raised during deserialization of request: %s', request)\n                async_object.set_exception(exc)\n                return\n            self.logger.debug('Received response(xid=%s): %r', xid, response)\n            if (request.type == Transaction.type):\n                response = Transaction.unchroot(client, response)\n            async_object.set(response)\n        watcher = getattr(request, 'watcher', None)\n        if ((not client._stopped.is_set()) and watcher):\n            if isinstance(request, GetChildren):\n                client._child_watchers[request.path].add(watcher)\n            else:\n                client._data_watchers[request.path].add(watcher)\n    if isinstance(request, Close):\n        self.logger.log(BLATHER, 'Read close response')\n        return CLOSE_RESPONSE\n", "label": 1}
{"function": "\n\ndef _event_text_symbol(self, ev):\n    text = None\n    symbol = xlib.KeySym()\n    buffer = create_string_buffer(128)\n    count = xlib.XLookupString(ev.xkey, buffer, (len(buffer) - 1), byref(symbol), None)\n    filtered = xlib.XFilterEvent(ev, ev.xany.window)\n    if ((ev.type == xlib.KeyPress) and (not filtered)):\n        status = c_int()\n        if _have_utf8:\n            encoding = 'utf8'\n            count = xlib.Xutf8LookupString(self._x_ic, ev.xkey, buffer, (len(buffer) - 1), byref(symbol), byref(status))\n            if (status.value == xlib.XBufferOverflow):\n                raise NotImplementedError('TODO: XIM buffer resize')\n        else:\n            encoding = 'ascii'\n            count = xlib.XLookupString(ev.xkey, buffer, (len(buffer) - 1), byref(symbol), None)\n            if count:\n                status.value = xlib.XLookupBoth\n        if (status.value & (xlib.XLookupChars | xlib.XLookupBoth)):\n            text = buffer.value[:count].decode(encoding)\n        if (text and (unicodedata.category(text) == 'Cc') and (text != '\\r')):\n            text = None\n    symbol = symbol.value\n    if ((ev.xkey.keycode == 0) and (not filtered)):\n        symbol = None\n    if (symbol and (symbol not in key._key_names) and ev.xkey.keycode):\n        symbol = ord(unichr(symbol).lower())\n        if (symbol not in key._key_names):\n            symbol = key.user_key(ev.xkey.keycode)\n    if filtered:\n        return (None, symbol)\n    return (text, symbol)\n", "label": 1}
{"function": "\n\ndef lookup_allowed(self, lookup, value):\n    from django.contrib.admin.filters import SimpleListFilter\n    model = self.model\n    for l in model._meta.related_fkey_lookups:\n        if callable(l):\n            l = l()\n        for (k, v) in widgets.url_params_from_lookup_dict(l).items():\n            if ((k == lookup) and (v == value)):\n                return True\n    relation_parts = []\n    prev_field = None\n    for part in lookup.split(LOOKUP_SEP):\n        try:\n            field = model._meta.get_field(part)\n        except FieldDoesNotExist:\n            break\n        if ((not prev_field) or (prev_field.concrete and (field not in prev_field.get_path_info()[(- 1)].target_fields))):\n            relation_parts.append(part)\n        if (not getattr(field, 'get_path_info', None)):\n            break\n        prev_field = field\n        model = field.get_path_info()[(- 1)].to_opts.model\n    if (len(relation_parts) <= 1):\n        return True\n    clean_lookup = LOOKUP_SEP.join(relation_parts)\n    valid_lookups = [self.date_hierarchy]\n    for filter_item in self.list_filter:\n        if (isinstance(filter_item, type) and issubclass(filter_item, SimpleListFilter)):\n            valid_lookups.append(filter_item.parameter_name)\n        elif isinstance(filter_item, (list, tuple)):\n            valid_lookups.append(filter_item[0])\n        else:\n            valid_lookups.append(filter_item)\n    return (clean_lookup in valid_lookups)\n", "label": 1}
{"function": "\n\n@register_canonicalize\n@register_specialize\n@gof.local_optimizer([IncSubtensor])\ndef local_useless_inc_subtensor(node):\n    '\\n    Remove IncSubtensor, when we overwrite the full inputs with the\\n    new value.\\n\\n    '\n    if (not isinstance(node.op, IncSubtensor)):\n        return\n    if (node.op.set_instead_of_inc is False):\n        try:\n            c = get_scalar_constant_value(node.inputs[0])\n            if (c != 0):\n                return\n        except NotScalarConstantError:\n            return\n    if ((node.inputs[0].ndim != node.inputs[1].ndim) or (node.inputs[0].broadcastable != node.inputs[1].broadcastable)):\n        return\n    idx_cst = get_idx_list(node.inputs[1:], node.op.idx_list)\n    if all(((isinstance(e, slice) and (e.start is None) and (e.stop is None) and ((e.step is None) or (T.extract_constant(e.step) == (- 1)))) for e in idx_cst)):\n        if (not hasattr(node.fgraph, 'shape_feature')):\n            return\n        if (not node.fgraph.shape_feature.same_shape(node.inputs[0], node.inputs[1])):\n            return\n        if all(((e.step is None) for e in node.op.idx_list)):\n            return [node.inputs[1]]\n        ret = Subtensor(node.op.idx_list)(*node.inputs[1:])\n        copy_stack_trace(node.outputs, ret)\n        return [ret]\n", "label": 1}
{"function": "\n\ndef typecheck_ioport(self, sigtypes):\n    if (('input' not in sigtypes) and ('output' not in sigtypes) and ('inout' not in sigtypes)):\n        raise ParseError('Syntax Error')\n    if (('input' in sigtypes) and ('output' in sigtypes)):\n        raise ParseError('Syntax Error')\n    if (('inout' in sigtypes) and ('output' in sigtypes)):\n        raise ParseError('Syntax Error')\n    if (('inout' in sigtypes) and ('input' in sigtypes)):\n        raise ParseError('Syntax Error')\n    if (('input' in sigtypes) and ('reg' in sigtypes)):\n        raise ParseError('Syntax Error')\n    if (('inout' in sigtypes) and ('reg' in sigtypes)):\n        raise ParseError('Syntax Error')\n    if (('input' in sigtypes) and ('tri' in sigtypes)):\n        raise ParseError('Syntax Error')\n    if (('output' in sigtypes) and ('tri' in sigtypes)):\n        raise ParseError('Syntax Error')\n", "label": 1}
{"function": "\n\ndef test_equal():\n    b = Symbol('b')\n    a = Symbol('a')\n    e1 = (a + b)\n    e2 = ((2 * a) * b)\n    e3 = ((a ** 3) * (b ** 2))\n    e4 = ((a * b) + (b * a))\n    assert (not (e1 == e2))\n    assert (not (e1 == e2))\n    assert (e1 != e2)\n    assert (e2 == e4)\n    assert (e2 != e3)\n    assert (not (e2 == e3))\n    x = Symbol('x')\n    e1 = exp((x + (1 / x)))\n    y = Symbol('x')\n    e2 = exp((y + (1 / y)))\n    assert (e1 == e2)\n    assert (not (e1 != e2))\n    y = Symbol('y')\n    e2 = exp((y + (1 / y)))\n    assert (not (e1 == e2))\n    assert (e1 != e2)\n    e5 = (((Rational(3) + (2 * x)) - x) - x)\n    assert (e5 == 3)\n    assert (3 == e5)\n    assert (e5 != 4)\n    assert (4 != e5)\n    assert (e5 != (3 + x))\n    assert ((3 + x) != e5)\n", "label": 1}
{"function": "\n\ndef test_has_iterative():\n    (A, B, C) = symbols('A,B,C', commutative=False)\n    f = (((((((x * gamma(x)) * sin(x)) * exp((x * y))) * A) * B) * C) * cos(((x * A) * B)))\n    assert f.has(x)\n    assert f.has((x * y))\n    assert f.has((x * sin(x)))\n    assert (not f.has((x * sin(y))))\n    assert f.has((x * A))\n    assert f.has(((x * A) * B))\n    assert (not f.has(((x * A) * C)))\n    assert f.has((((x * A) * B) * C))\n    assert (not f.has((((x * A) * C) * B)))\n    assert f.has(((((x * sin(x)) * A) * B) * C))\n    assert (not f.has(((((x * sin(x)) * A) * C) * B)))\n    assert (not f.has(((((x * sin(y)) * A) * B) * C)))\n    assert f.has((x * gamma(x)))\n    assert (not f.has((x + sin(x))))\n    assert ((x & y) & z).has((x & z))\n", "label": 1}
{"function": "\n\ndef list_sorted_nshash(self, h, ns, long_=False, columns=True, recurse=False, prnt=False, longer=False, longest=False):\n    if (type(h) in (int, long)):\n        if (h == fishlib.STATUS.UNAUTHORIZED):\n            return 'Permission denied.'\n        else:\n            return ('Error status %s' % h)\n    tagNames = h['tagNames']\n    tagNames.sort()\n    spaces = h['namespaceNames']\n    spaces.sort()\n    items = (tagNames[:] + [('%s/' % space) for space in spaces])\n    items.sort()\n    if items:\n        fmt = string_format(max([len(item) for item in items]))\n    if recurse:\n        self.Print(('\\n%s:' % ns))\n    if (long_ or longer or longest):\n        res = []\n        for item in items:\n            r = self.full_perms(((ns + '/') + (fmt % item)), longer, longest=longest)\n            res.append(r)\n            if prnt:\n                self.Print(r)\n        result = '\\n'.join(res)\n    elif (columns == False):\n        result = '\\n'.join(items)\n        if prnt:\n            self.Print(result)\n    else:\n        result = to_string_grid(items)\n        if prnt:\n            self.Print(result)\n    if recurse:\n        others = '\\n'.join([self.list_sorted_ns(('%s/%s' % (ns, space)), long_, columns, recurse, prnt=prnt, longer=longer, longest=longest) for space in spaces])\n        return ('%s:\\n%s\\n\\n%s' % (ns, result, others))\n    else:\n        return result\n", "label": 1}
{"function": "\n\n@qc(1)\ndef init_db():\n    db = db_utils.init_db('sqlite:///:memory:')\n    assert (type(db) is neat.db.Database)\n    assert isinstance(db.hosts, Table)\n    assert isinstance(db.vms, Table)\n    assert isinstance(db.vm_resource_usage, Table)\n    assert isinstance(db.host_states, Table)\n    assert (db.hosts.c.keys() == ['id', 'hostname', 'cpu_mhz', 'cpu_cores', 'ram'])\n    assert (db.host_resource_usage.c.keys() == ['id', 'host_id', 'timestamp', 'cpu_mhz'])\n    assert (list(db.host_resource_usage.foreign_keys)[0].target_fullname == 'hosts.id')\n    assert (db.vms.c.keys() == ['id', 'uuid'])\n    assert (db.vm_resource_usage.c.keys() == ['id', 'vm_id', 'timestamp', 'cpu_mhz'])\n    assert (list(db.vm_resource_usage.foreign_keys)[0].target_fullname == 'vms.id')\n    assert (db.vm_migrations.c.keys() == ['id', 'vm_id', 'host_id', 'timestamp'])\n    keys = set([list(db.vm_migrations.foreign_keys)[0].target_fullname, list(db.vm_migrations.foreign_keys)[1].target_fullname])\n    assert (keys == set(['vms.id', 'hosts.id']))\n    assert (db.host_states.c.keys() == ['id', 'host_id', 'timestamp', 'state'])\n    assert (list(db.host_states.foreign_keys)[0].target_fullname == 'hosts.id')\n    assert (db.host_overload.c.keys() == ['id', 'host_id', 'timestamp', 'overload'])\n    assert (list(db.host_overload.foreign_keys)[0].target_fullname == 'hosts.id')\n", "label": 1}
{"function": "\n\n@classmethod\ndef set_field_value(cls, obj, field, value):\n    if isinstance(value, basestring):\n        value = value.strip()\n    try:\n        (field, subfield) = field.split(':')\n    except Exception:\n        pass\n    else:\n        field = field.strip()\n        if (field not in obj):\n            obj[field] = {\n                \n            }\n        cls.set_field_value(obj[field], subfield, value)\n        return\n    try:\n        (field, _) = field.split()\n    except Exception:\n        pass\n    else:\n        dud = {\n            \n        }\n        cls.set_field_value(dud, field, value)\n        ((field, value),) = dud.items()\n        if (field not in obj):\n            obj[field] = []\n        elif (not isinstance(obj[field], list)):\n            obj[field] = [obj[field]]\n        if (value not in (None, '')):\n            obj[field].append(value)\n        return\n    try:\n        (field, nothing) = field.split('?')\n        assert (nothing.strip() == '')\n    except Exception:\n        pass\n    else:\n        try:\n            value = {\n                'yes': True,\n                'true': True,\n                'no': False,\n                'false': False,\n                '': False,\n                None: False,\n            }[(value.lower() if hasattr(value, 'lower') else value)]\n        except KeyError:\n            raise JSONReaderError(('Values for field %s must be \"yes\" or \"no\", not \"%s\"' % (field, value)))\n    field = field.strip()\n    if (field in obj):\n        raise JSONReaderError(('You have a repeat field: %s' % field))\n    obj[field] = value\n", "label": 1}
{"function": "\n\ndef GetItem(self, user, route, has_perm=False, need_perm=False):\n    self.CheckUpdate()\n    if (self.mtitle.find('(BM:') != (- 1)):\n        if (Board.Board.IsBM(user, self.mtitle[4:]) or user.IsSysop()):\n            has_perm = True\n        elif (need_perm and (not has_perm)):\n            return None\n    if ((self.mtitle.find('(BM: BMS)') != (- 1)) or (self.mtitle.find('(BM: SECRET)') != (- 1)) or (self.mtitle.find('(BM: SYSOPS)') != (- 1))):\n        need_perm = True\n    if (len(route) == 0):\n        return self\n    target = (route[0] - 1)\n    _id = target\n    if (_id >= len(self.items)):\n        return None\n    while (self.items[_id].EffectiveId(user) < target):\n        _id += 1\n        if (_id >= len(self.items)):\n            return None\n    item = self.items[_id]\n    item.mtitle = item.title\n    if (len(route) == 1):\n        return item\n    elif item.IsDir():\n        if (not item.CheckUpdate()):\n            return None\n        return item.GetItem(user, route[1:], has_perm, need_perm)\n    else:\n        return None\n", "label": 1}
{"function": "\n\n@newick.sniffer()\ndef _newick_sniffer(fh):\n    operators = set(',;:()')\n    empty = True\n    last_token = ','\n    indent = 0\n    try:\n        for (token, _) in zip(_tokenize_newick(fh), range(100)):\n            if (token not in operators):\n                pass\n            elif ((token == ',') and (last_token != ':') and (indent > 0)):\n                pass\n            elif ((token == ':') and (last_token != ':')):\n                pass\n            elif ((token == ';') and (last_token != ':') and (indent == 0)):\n                pass\n            elif ((token == ')') and (last_token != ':')):\n                indent -= 1\n            elif ((token == '(') and ((last_token == '(') or (last_token == ','))):\n                indent += 1\n            else:\n                raise NewickFormatError()\n            last_token = token\n            empty = False\n    except NewickFormatError:\n        return (False, {\n            \n        })\n    return ((not empty), {\n        \n    })\n", "label": 1}
{"function": "\n\ndef convert(self, element, base=None):\n    'Convert ``element`` to ``self.dtype``. '\n    if (base is not None):\n        return self.convert_from(element, base)\n    if self.of_type(element):\n        return element\n    from sympy.polys.domains import PythonIntegerRing, GMPYIntegerRing, GMPYRationalField, RealField, ComplexField\n    if isinstance(element, integer_types):\n        return self.convert_from(element, PythonIntegerRing())\n    if HAS_GMPY:\n        integers = GMPYIntegerRing()\n        if isinstance(element, integers.tp):\n            return self.convert_from(element, integers)\n        rationals = GMPYRationalField()\n        if isinstance(element, rationals.tp):\n            return self.convert_from(element, rationals)\n    if isinstance(element, float):\n        parent = RealField(tol=False)\n        return self.convert_from(parent(element), parent)\n    if isinstance(element, complex):\n        parent = ComplexField(tol=False)\n        return self.convert_from(parent(element), parent)\n    if isinstance(element, DomainElement):\n        return self.convert_from(element, element.parent())\n    if (self.is_Numerical and getattr(element, 'is_ground', False)):\n        return self.convert(element.LC())\n    if isinstance(element, Basic):\n        try:\n            return self.from_sympy(element)\n        except (TypeError, ValueError):\n            pass\n    elif (not is_sequence(element)):\n        try:\n            element = sympify(element)\n            if isinstance(element, Basic):\n                return self.from_sympy(element)\n        except (TypeError, ValueError):\n            pass\n    raise CoercionFailed((\"can't convert %s of type %s to %s\" % (element, type(element), self)))\n", "label": 1}
{"function": "\n\ndef send(self, message, _sender=None):\n    'Sends a message to the actor represented by this `Ref`.'\n    if (not _sender):\n        context = get_context()\n        if context:\n            _sender = context.ref\n    if self._cell:\n        if (not self._cell.stopped):\n            self._cell.receive(message, _sender)\n            return\n        else:\n            self._cell = None\n    if (not self.is_local):\n        if (self.uri.node != self.node.nid):\n            self.node.send_message(message, remote_ref=self, sender=_sender)\n        else:\n            self._cell = self.node.guardian.lookup_cell(self.uri)\n            self.is_local = True\n            self._cell.receive(message, _sender)\n    else:\n        if (self.node and self.node.guardian):\n            cell = self.node.guardian.lookup_cell(self.uri)\n            if cell:\n                cell.receive(message, _sender)\n                return\n        if (('_watched', ANY) == message):\n            message[1].send(('terminated', self), _sender=self)\n        elif ((message == ('terminated', ANY)) or (message == ('_unwatched', ANY)) or (message == ('_node_down', ANY)) or (message == '_stop') or (message == '_kill') or (message == '__done')):\n            pass\n        else:\n            Events.log(DeadLetter(self, message, _sender))\n", "label": 1}
{"function": "\n\ndef get_pdf_url(self):\n    if self.checked_pdf_url:\n        return self.pdf_url\n    url = None\n    if self.aliases.display_pmc:\n        url = 'http://ukpmc.ac.uk/articles/{pmcid}?pdf=render'.format(pmcid=self.aliases.pmc[0])\n    elif self.aliases.display_arxiv:\n        url = 'http://arxiv.org/pdf/{arxiv_id}.pdf'.format(arxiv_id=self.aliases.display_arxiv)\n    elif (hasattr(self.biblio, 'free_fulltext_url') and self.biblio.free_fulltext_url):\n        if ('pdf' in self.biblio.free_fulltext_url):\n            url = self.biblio.free_fulltext_url\n        elif (self.aliases.resolved_url and ('sagepub.com/' in self.aliases.resolved_url)):\n            url = (self.aliases.resolved_url + '.full.pdf')\n        if (not url):\n            url = embed_markup.extract_pdf_link_from_html(self.biblio.free_fulltext_url)\n    if (not url):\n        if (self.aliases.resolved_url and ('pdf' in self.aliases.resolved_url)):\n            url = self.aliases.resolved_url\n    if (url and ('.pdf+html' in url)):\n        url = url.replace('.pdf+html', '.pdf')\n    if (url and ('jstor.org/' in url)):\n        url = None\n    self.checked_pdf_url = True\n    self.pdf_url = url\n    return url\n", "label": 1}
{"function": "\n\n@staticmethod\ndef _traverse_grammar_postfix(grammar, start='root'):\n    '\\n        Convert infix regexp re to postfix notation.\\n        Insert explicit concatenation operator.\\n        '\n    (nalt, natom) = ([], 0)\n    parens = []\n    for c in GrammarCracker._traverse_grammar_infix(grammar, start):\n        if (c is GrammarCracker._lparen):\n            if (natom > 1):\n                natom -= 1\n                (yield GrammarCracker._concat)\n            parens.append((nalt, natom))\n            (nalt, natom) = ([], 0)\n        elif isinstance(c, _bstate):\n            assert (natom > 0)\n            for _ in range((natom - 1)):\n                (yield GrammarCracker._concat)\n            natom = 0\n            nalt.append(c)\n        elif (c is GrammarCracker._rparen):\n            assert len(parens)\n            assert (natom > 0)\n            for _ in range((natom - 1)):\n                (yield GrammarCracker._concat)\n            for i in reversed(nalt):\n                (yield i)\n            (nalt, natom) = parens.pop()\n            natom += 1\n        elif isinstance(c, tuple):\n            assert (natom > 0)\n            (yield c)\n        else:\n            if (natom > 1):\n                (yield GrammarCracker._concat)\n            else:\n                natom += 1\n            (yield c)\n    assert (not parens)\n    for _ in range((natom - 1)):\n        (yield GrammarCracker._concat)\n    for i in reversed(nalt):\n        (yield i)\n", "label": 1}
{"function": "\n\ndef test_iterable_list(self):\n    for args in (('name',), ('name', 'prefix_')):\n        l = IterableList('name')\n        m1 = TestIterableMember('one')\n        m2 = TestIterableMember('two')\n        l.extend((m1, m2))\n        assert (len(l) == 2)\n        assert (m1.name in l)\n        assert (m2.name in l)\n        assert (m2 in l)\n        assert (m2 in l)\n        assert ('invalid' not in l)\n        assert (l[m1.name] is m1)\n        assert (l[m2.name] is m2)\n        assert (l[0] is m1)\n        assert (l[1] is m2)\n        assert (l.one is m1)\n        assert (l.two is m2)\n        self.failUnlessRaises(AttributeError, getattr, l, 'something')\n        self.failUnlessRaises(IndexError, l.__getitem__, 'something')\n        self.failUnlessRaises(IndexError, l.__delitem__, 'something')\n        del l[m2.name]\n        assert (len(l) == 1)\n        assert ((m2.name not in l) and (m1.name in l))\n        del l[0]\n        assert (m1.name not in l)\n        assert (len(l) == 0)\n        self.failUnlessRaises(IndexError, l.__delitem__, 0)\n        self.failUnlessRaises(IndexError, l.__delitem__, 'something')\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_key_ != x.has_key_):\n        return 0\n    if (self.has_key_ and (self.key_ != x.key_)):\n        return 0\n    if (self.has_value_ != x.has_value_):\n        return 0\n    if (self.has_value_ and (self.value_ != x.value_)):\n        return 0\n    if (self.has_flags_ != x.has_flags_):\n        return 0\n    if (self.has_flags_ and (self.flags_ != x.flags_)):\n        return 0\n    if (self.has_cas_id_ != x.has_cas_id_):\n        return 0\n    if (self.has_cas_id_ and (self.cas_id_ != x.cas_id_)):\n        return 0\n    if (self.has_expires_in_seconds_ != x.has_expires_in_seconds_):\n        return 0\n    if (self.has_expires_in_seconds_ and (self.expires_in_seconds_ != x.expires_in_seconds_)):\n        return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef check_permissions(permission_type, user, project):\n    '\\n    Here we check permission types, and see if a user has proper perms.\\n    If a user has \"edit\" permissions on a project, they pretty much have\\n    carte blanche to do as they please, so we kick that back as true. \\n    Otherwise we go more fine grained and check their view and comment \\n    permissions\\n    '\n    try:\n        groups = user.groups.all()\n    except AttributeError:\n        groups = None\n    if (user in project.users_can_edit.all()):\n        return True\n    for x in groups:\n        if (x in project.groups_can_edit.all()):\n            return True\n    if (permission_type == 'edit'):\n        if project.allow_anon_editing:\n            return True\n    if (permission_type == 'view'):\n        if (user in project.users_can_view.all()):\n            return True\n        for x in groups:\n            if (x in project.groups_can_view.all()):\n                return True\n        if (project.allow_anon_viewing is True):\n            return True\n    if (permission_type == 'comment'):\n        if (user in project.users_can_comment.all()):\n            return True\n        for x in groups:\n            if (x in project.groups_can_comment.all()):\n                return True\n        if (project.allow_anon_comment is True):\n            return True\n    return False\n", "label": 1}
{"function": "\n\n@operation(pipeline_facts={\n    'file': 'name',\n})\ndef file(state, host, name, present=True, user=None, group=None, mode=None, touch=False):\n    '\\n    Manage the state of files.\\n\\n    + name: name/path of the remote file\\n    + present: whether the file should exist\\n    + user: user to own the files\\n    + group: group to own the files\\n    + mode: permissions of the files as an integer, eg: 755\\n    + touch: whether to touch the file\\n    '\n    mode = ensure_mode_int(mode)\n    info = host.fact.file(name)\n    commands = []\n    if (info is False):\n        raise OperationError('{0} exists and is not a file'.format(name))\n    if ((info is None) and present):\n        commands.append('touch {0}'.format(name))\n        if mode:\n            commands.append(chmod(name, mode))\n        if (user or group):\n            commands.append(chown(name, user, group))\n    elif (info and (not present)):\n        commands.append('rm -f {0}'.format(name))\n    elif (info and present):\n        if touch:\n            commands.append('touch {0}'.format(name))\n        if (mode and (info['mode'] != mode)):\n            commands.append(chmod(name, mode))\n        if ((user and (info['user'] != user)) or (group and (info['group'] != group))):\n            commands.append(chown(name, user, group))\n    return commands\n", "label": 1}
{"function": "\n\ndef my_fn_PDxEV(xc, p, contextItem, args):\n    if (len(args) != 2):\n        raise XPathContext.FunctionNumArgs()\n    PDseq = (args[0] if isinstance(args[0], (list, tuple)) else (args[0],))\n    EVseq = (args[1] if isinstance(args[1], (list, tuple)) else (args[1],))\n    dimQname = qname('{http://www.example.com/wgt-avg}ExposuresDimension')\n    PDxEV = []\n    for pd in PDseq:\n        if (pd.context is not None):\n            pdDim = pd.context.dimValue(dimQname)\n            for ev in EVseq:\n                if (ev.context is not None):\n                    evDim = ev.context.dimValue(dimQname)\n                    if ((pdDim is not None) and isinstance(pdDim, ModelDimensionValue)):\n                        dimEqual = pdDim.isEqualTo(evDim, equalMode=XbrlUtil.S_EQUAL2)\n                    elif ((evDim is not None) and isinstance(evDim, ModelDimensionValue)):\n                        dimEqual = evDim.isEqualTo(pdDim, equalMode=XbrlUtil.S_EQUAL2)\n                    else:\n                        dimEqual = (pdDim == evDim)\n                    if dimEqual:\n                        pdX = pd.xValue\n                        evX = ev.xValue\n                        if (isinstance(pdX, Decimal) and isinstance(evX, float)):\n                            pdX = float(pdX)\n                        elif (isinstance(evX, Decimal) and isinstance(pdX, float)):\n                            pdX = float(evX)\n                        PDxEV.append((pdX * evX))\n                        break\n    return PDxEV\n", "label": 1}
{"function": "\n\n@transaction.commit_on_success\ndef __getitem__(self, idx):\n    queryset = self.transitions.order_by('start_time')\n    if isinstance(idx, slice):\n        if (idx.step is not None):\n            raise IndexError('Index stepping is not supported.')\n        if ((idx.start is None) and (idx.stop is None)):\n            raise ValueError('List cloning is not supported.')\n        if ((idx.start is not None) and (idx.stop is not None)):\n            if ((idx.stop < idx.start) or ((idx.start < 0) and (idx.stop > 0))):\n                return []\n            if (idx.stop == idx.start):\n                return []\n        if ((idx.start is not None) and (idx.start < 0)):\n            if (idx.stop is None):\n                start = None\n                stop = abs(idx.start)\n            else:\n                start = abs(idx.stop)\n                stop = abs((idx.stop + idx.start))\n            idx = slice(start, stop)\n            trans = list(queryset.order_by('-start_time')[idx])\n            trans.reverse()\n        elif ((idx.stop is not None) and (idx.stop < 0)):\n            start = idx.start\n            stop = (self.length + idx.stop)\n            idx = slice(start, stop)\n            trans = list(queryset[idx])\n        else:\n            trans = list(queryset[idx])\n    elif (idx < 0):\n        trans = queryset.order_by('-start_time')[(abs(idx) - 1)]\n    else:\n        try:\n            trans = queryset[idx]\n        except Transition.DoesNotExist:\n            raise IndexError\n    return trans\n", "label": 1}
{"function": "\n\ndef TryMerge(self, d):\n    while (d.avail() > 0):\n        tt = d.getVarInt32()\n        if (tt == 8):\n            self.set_width(d.getVarInt32())\n            continue\n        if (tt == 16):\n            self.set_height(d.getVarInt32())\n            continue\n        if (tt == 24):\n            self.set_rotate(d.getVarInt32())\n            continue\n        if (tt == 32):\n            self.set_horizontal_flip(d.getBoolean())\n            continue\n        if (tt == 40):\n            self.set_vertical_flip(d.getBoolean())\n            continue\n        if (tt == 53):\n            self.set_crop_left_x(d.getFloat())\n            continue\n        if (tt == 61):\n            self.set_crop_top_y(d.getFloat())\n            continue\n        if (tt == 69):\n            self.set_crop_right_x(d.getFloat())\n            continue\n        if (tt == 77):\n            self.set_crop_bottom_y(d.getFloat())\n            continue\n        if (tt == 80):\n            self.set_autolevels(d.getBoolean())\n            continue\n        if (tt == 88):\n            self.set_crop_to_fit(d.getBoolean())\n            continue\n        if (tt == 101):\n            self.set_crop_offset_x(d.getFloat())\n            continue\n        if (tt == 109):\n            self.set_crop_offset_y(d.getFloat())\n            continue\n        if (tt == 112):\n            self.set_allow_stretch(d.getBoolean())\n            continue\n        if (tt == 0):\n            raise ProtocolBuffer.ProtocolBufferDecodeError\n        d.skipData(tt)\n", "label": 1}
{"function": "\n\ndef _tostring_query(varname, value, explode, operator, safe=''):\n    joiner = operator\n    varprefix = ''\n    if (operator == '?'):\n        joiner = '&'\n        varprefix = (varname + '=')\n    if (type(value) == type([])):\n        if (0 == len(value)):\n            return ''\n        if (explode == '+'):\n            return joiner.join([((varname + '=') + urllib.quote(x, safe)) for x in value])\n        elif (explode == '*'):\n            return joiner.join([urllib.quote(x, safe) for x in value])\n        else:\n            return (varprefix + ','.join([urllib.quote(x, safe) for x in value]))\n    elif (type(value) == type({\n        \n    })):\n        if (0 == len(value)):\n            return ''\n        keys = value.keys()\n        keys.sort()\n        if (explode == '+'):\n            return joiner.join([((((varname + '.') + urllib.quote(key, safe)) + '=') + urllib.quote(value[key], safe)) for key in keys])\n        elif (explode == '*'):\n            return joiner.join([((urllib.quote(key, safe) + '=') + urllib.quote(value[key], safe)) for key in keys])\n        else:\n            return (varprefix + ','.join([((urllib.quote(key, safe) + ',') + urllib.quote(value[key], safe)) for key in keys]))\n    elif value:\n        return ((varname + '=') + urllib.quote(value, safe))\n    else:\n        return varname\n", "label": 1}
{"function": "\n\ndef _find_K_cut(self, k):\n    '\\n        Find the lowest level cut that has k connected components. If there are\\n        no levels that have k components, then find the lowest level that has\\n        at least k components. If no levels have > k components, find the\\n        lowest level that has the maximum number of components.\\n\\n        Parameters\\n        ----------\\n        k : int\\n            Desired number of clusters/nodes/components.\\n\\n        Returns\\n        -------\\n        cut : float\\n            Lowest density level where there are k nodes.\\n        '\n    starts = [v.start_level for v in self.nodes.itervalues()]\n    ends = [v.end_level for v in self.nodes.itervalues()]\n    crits = _np.unique((starts + ends))\n    nclust = {\n        \n    }\n    for c in crits:\n        nclust[c] = len([e for (e, v) in self.nodes.iteritems() if ((v.start_level <= c) and (v.end_level > c))])\n    width = _np.max(nclust.values())\n    if (k in nclust.values()):\n        cut = _np.min([e for (e, v) in nclust.iteritems() if (v == k)])\n    elif (width < k):\n        cut = _np.min([e for (e, v) in nclust.iteritems() if (v == width)])\n    else:\n        ktemp = _np.min([v for v in nclust.itervalues() if (v > k)])\n        cut = _np.min([e for (e, v) in nclust.iteritems() if (v == ktemp)])\n    return cut\n", "label": 1}
{"function": "\n\ndef join(a, *p):\n    'Join two or more pathname components, inserting \"\\\\\" as needed.\\n    If any component is an absolute path, all previous path components\\n    will be discarded.'\n    path = a\n    for b in p:\n        b_wins = 0\n        if (path == ''):\n            b_wins = 1\n        elif isabs(b):\n            if ((path[1:2] != ':') or (b[1:2] == ':')):\n                b_wins = 1\n            elif ((len(path) > 3) or ((len(path) == 3) and (path[(- 1)] not in '/\\\\'))):\n                b_wins = 1\n        if b_wins:\n            path = b\n        else:\n            assert (len(path) > 0)\n            if (path[(- 1)] in '/\\\\'):\n                if (b and (b[0] in '/\\\\')):\n                    path += b[1:]\n                else:\n                    path += b\n            elif (path[(- 1)] == ':'):\n                path += b\n            elif b:\n                if (b[0] in '/\\\\'):\n                    path += b\n                else:\n                    path += ('\\\\' + b)\n            else:\n                path += '\\\\'\n    return path\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    parents = [b for b in bases if isinstance(b, ModelMetaclass)]\n    if (not parents):\n        return super(ModelMetaclass, cls).__new__(cls, name, bases, attrs)\n    fields = {\n        \n    }\n    for base in bases:\n        if (isinstance(base, ModelMetaclass) and hasattr(base, 'fields')):\n            fields.update(base.fields)\n    new_fields = {\n        \n    }\n    managers = {\n        \n    }\n    for (attrname, field) in attrs.items():\n        if isinstance(field, Field):\n            new_fields[attrname] = field\n            if field.name:\n                if ('_name_field' in attrs):\n                    raise FieldError((\"Multiple key fields defined for model '%s'\" % name))\n                attrs['_name_field'] = attrname\n        elif (attrname in fields):\n            del fields[attrname]\n        if isinstance(field, Manager):\n            managers[attrname] = field\n    fields.update(new_fields)\n    attrs['fields'] = fields\n    new_cls = super(ModelMetaclass, cls).__new__(cls, name, bases, attrs)\n    for (field, value) in new_fields.items():\n        new_cls.add_to_class(field, value)\n    if (not managers):\n        managers['objects'] = Manager()\n    for (field, value) in managers.items():\n        new_cls.add_to_class(field, value)\n    if hasattr(new_cls, 'Meta'):\n        if isinstance(new_cls.Meta.domain, basestring):\n            new_cls.Meta.domain = simpledb.Domain(new_cls.Meta.domain, new_cls.Meta.connection)\n        new_cls.Meta.domain.model = new_cls\n        new_cls.Meta.connection.encoder = FieldEncoder(fields)\n    return new_cls\n", "label": 1}
{"function": "\n\ndef __call__(self, *args):\n    c = args[0]\n    c = list(c)\n    (letter, n) = (c[0], int(c[1]))\n    if (n < 0):\n        raise ValueError('Lie algebra rank cannot be negative')\n    if (letter == 'A'):\n        if (n >= 0):\n            from . import type_a\n            return type_a.TypeA(n)\n    if (letter == 'B'):\n        if (n >= 0):\n            from . import type_b\n            return type_b.TypeB(n)\n    if (letter == 'C'):\n        if (n >= 0):\n            from . import type_c\n            return type_c.TypeC(n)\n    if (letter == 'D'):\n        if (n >= 0):\n            from . import type_d\n            return type_d.TypeD(n)\n    if (letter == 'E'):\n        if ((n >= 6) and (n <= 8)):\n            from . import type_e\n            return type_e.TypeE(n)\n    if (letter == 'F'):\n        if (n == 4):\n            from . import type_f\n            return type_f.TypeF(n)\n    if (letter == 'G'):\n        if (n == 2):\n            from . import type_g\n            return type_g.TypeG(n)\n", "label": 1}
{"function": "\n\ndef condition_ok(self, lax=False):\n    if self.test:\n        lax = True\n    assert self.assertion.conditions\n    conditions = self.assertion.conditions\n    logger.debug(('conditions: %s' % conditions))\n    if (not conditions.keyswv()):\n        return True\n    if (conditions.not_before and conditions.not_on_or_after):\n        if (not later_than(conditions.not_on_or_after, conditions.not_before)):\n            return False\n    try:\n        if conditions.not_on_or_after:\n            self.not_on_or_after = validate_on_or_after(conditions.not_on_or_after, self.timeslack)\n        if conditions.not_before:\n            validate_before(conditions.not_before, self.timeslack)\n    except Exception as excp:\n        logger.error(('Exception on conditions: %s' % (excp,)))\n        if (not lax):\n            raise\n        else:\n            self.not_on_or_after = 0\n    if (not self.allow_unsolicited):\n        if (not for_me(conditions, self.entity_id)):\n            if (not lax):\n                raise Exception('Not for me!!!')\n    if conditions.condition:\n        for cond in conditions.condition:\n            try:\n                if (cond.extension_attributes[XSI_TYPE] in self.extension_schema):\n                    pass\n                else:\n                    raise Exception('Unknown condition')\n            except KeyError:\n                raise Exception('Missing xsi:type specification')\n    return True\n", "label": 1}
{"function": "\n\ndef fitness_and_quality_parsed(mime_type, parsed_ranges):\n    \"Find the best match for a given mime-type against \\n       a list of media_ranges that have already been \\n       parsed by parse_media_range(). Returns a tuple of\\n       the fitness value and the value of the 'q' quality\\n       parameter of the best match, or (-1, 0) if no match\\n       was found. Just as for quality_parsed(), 'parsed_ranges'\\n       must be a list of parsed media ranges. \"\n    best_fitness = (- 1)\n    best_fit_q = 0\n    (target_type, target_subtype, target_params) = parse_media_range(mime_type)\n    for (type, subtype, params) in parsed_ranges:\n        if (((type == target_type) or (type == '*') or (target_type == '*')) and ((subtype == target_subtype) or (subtype == '*') or (target_subtype == '*'))):\n            param_matches = reduce((lambda x, y: (x + y)), [1 for (key, value) in target_params.iteritems() if ((key != 'q') and params.has_key(key) and (value == params[key]))], 0)\n            fitness = (((type == target_type) and 100) or 0)\n            fitness += (((subtype == target_subtype) and 10) or 0)\n            fitness += param_matches\n            if (fitness > best_fitness):\n                best_fitness = fitness\n                best_fit_q = params['q']\n    return (best_fitness, float(best_fit_q))\n", "label": 1}
{"function": "\n\ndef get_path(self, executable):\n    if (len(self.userpath) == 0):\n        if (sublime.platform() == 'osx'):\n            updated_path = self.get_mac_path()\n            os.environ['PATH'] = updated_path\n            self.userpath = updated_path\n        elif (sublime.platform == 'windows'):\n            pass\n        elif (sublime.platform == 'linux'):\n            self.userpath = os.environ['PATH']\n    elif (sublime.platform() == 'osx'):\n        os.environ['PATH'] = self.userpath\n    if (';' in self.userpath):\n        paths = self.userpath.split(';')\n        for path in paths:\n            test_path = os.path.join(path, executable)\n            if (version_info[0] == 2):\n                if os.path.isfile(unicode(test_path)):\n                    return path\n            if os.path.isfile(test_path):\n                return path\n            elif os.path.islink(test_path):\n                return os.path.dirname(os.path.realpath(test_path))\n        return ''\n    elif (':' in self.userpath):\n        paths = self.userpath.split(':')\n        for path in paths:\n            test_path = os.path.join(path, executable)\n            if (version_info[0] == 2):\n                if os.path.isfile(unicode(test_path)):\n                    return path\n            if os.path.isfile(test_path):\n                return path\n            elif os.path.islink(test_path):\n                return os.path.dirname(os.path.realpath(test_path))\n        return ''\n    else:\n        return self.userpath\n", "label": 1}
{"function": "\n\ndef stop(self, timeout=5):\n    while (self._get_qsize() > 0):\n        conn = self.get()\n        if (conn is not _SHUTDOWNREQUEST):\n            conn.close()\n    for worker in self._threads:\n        self._queue.put(_SHUTDOWNREQUEST)\n    current = threading.currentThread()\n    if (timeout and (timeout >= 0)):\n        endtime = (time.time() + timeout)\n    while self._threads:\n        worker = self._threads.pop()\n        if ((worker is not current) and worker.isAlive()):\n            try:\n                if ((timeout is None) or (timeout < 0)):\n                    worker.join()\n                else:\n                    remaining_time = (endtime - time.time())\n                    if (remaining_time > 0):\n                        worker.join(remaining_time)\n                    if worker.isAlive():\n                        c = worker.conn\n                        if (c and (not c.rfile.closed)):\n                            try:\n                                c.socket.shutdown(socket.SHUT_RD)\n                            except TypeError:\n                                c.socket.shutdown()\n                        worker.join()\n            except (AssertionError, KeyboardInterrupt):\n                pass\n", "label": 1}
{"function": "\n\ndef test_barrel_list():\n    bl = BarrelList()\n    bl.insert(0, 0)\n    assert (bl[0] == 0)\n    assert (len(bl) == 1)\n    bl.insert(1, 1)\n    assert (list(bl) == [0, 1])\n    bl.insert(0, (- 1))\n    assert (list(bl) == [(- 1), 0, 1])\n    bl.extend(range(int(100000.0)))\n    assert (len(bl) == (100000.0 + 3))\n    bl._balance_list(0)\n    assert (len(bl) == (100000.0 + 3))\n    bl.pop(50000)\n    assert (len(bl) == ((100000.0 + 3) - 1))\n    bl2 = BarrelList(TEST_INTS)\n    bl2.sort()\n    assert (list(bl2[:5]) == [0, 74, 80, 96, 150])\n    assert (list(bl2[:(- 5):(- 1)]) == [50508, 46607, 46428, 43442])\n    bl3 = BarrelList(range(int(100000.0)))\n    for i in range(10000):\n        bl3.insert(0, bl3.pop((len(bl3) // 2)))\n    assert (len(bl3) == 100000.0)\n    assert (bl3[0] == 40001)\n    assert (bl3[(- 1)] == sorted(bl3)[(- 1)])\n    del bl3[10:5000]\n    assert (bl3[0] == 40001)\n    assert (len(bl3) == (100000.0 - (5000 - 10)))\n    bl3[:20:2] = range(0, (- 10), (- 1))\n    assert (bl3[6] == (- 3))\n", "label": 1}
{"function": "\n\ndef construct_object(self, node, deep=False):\n    if (node in self.constructed_objects):\n        return self.constructed_objects[node]\n    if deep:\n        old_deep = self.deep_construct\n        self.deep_construct = True\n    if (node in self.recursive_objects):\n        raise ConstructorError(None, None, 'found unconstructable recursive node', node.start_mark)\n    self.recursive_objects[node] = None\n    constructor = None\n    tag_suffix = None\n    if (node.tag in self.yaml_constructors):\n        constructor = self.yaml_constructors[node.tag]\n    else:\n        for tag_prefix in self.yaml_multi_constructors:\n            if node.tag.startswith(tag_prefix):\n                tag_suffix = node.tag[len(tag_prefix):]\n                constructor = self.yaml_multi_constructors[tag_prefix]\n                break\n        else:\n            if (None in self.yaml_multi_constructors):\n                tag_suffix = node.tag\n                constructor = self.yaml_multi_constructors[None]\n            elif (None in self.yaml_constructors):\n                constructor = self.yaml_constructors[None]\n            elif isinstance(node, ScalarNode):\n                constructor = self.__class__.construct_scalar\n            elif isinstance(node, SequenceNode):\n                constructor = self.__class__.construct_sequence\n            elif isinstance(node, MappingNode):\n                constructor = self.__class__.construct_mapping\n    if (tag_suffix is None):\n        data = constructor(self, node)\n    else:\n        data = constructor(self, tag_suffix, node)\n    if isinstance(data, types.GeneratorType):\n        generator = data\n        data = next(generator)\n        if self.deep_construct:\n            for dummy in generator:\n                pass\n        else:\n            self.state_generators.append(generator)\n    self.constructed_objects[node] = data\n    del self.recursive_objects[node]\n    if deep:\n        self.deep_construct = old_deep\n    return data\n", "label": 1}
{"function": "\n\ndef load_middleware(self):\n    self.ignore_keywords = ['reversion.middleware', 'MaintenanceModeMiddleware']\n    super(LinkCheckHandler, self).load_middleware()\n    new_request_middleware = []\n    for method in self._request_middleware:\n        ignored = False\n        for keyword in self.ignore_keywords:\n            if method.__str__().count(keyword):\n                ignored = True\n                break\n        if (not ignored):\n            new_request_middleware.append(method)\n    self._request_middleware = new_request_middleware\n    new_view_middleware = []\n    for method in self._view_middleware:\n        ignored = False\n        for keyword in self.ignore_keywords:\n            if method.__str__().count(keyword):\n                ignored = True\n                break\n        if (not ignored):\n            new_view_middleware.append(method)\n    self._view_middleware = new_view_middleware\n    new_response_middleware = []\n    for method in self._response_middleware:\n        ignored = False\n        for keyword in self.ignore_keywords:\n            if method.__str__().count(keyword):\n                ignored = True\n                break\n        if (not ignored):\n            new_response_middleware.append(method)\n    self._response_middleware = new_response_middleware\n    new_exception_middleware = []\n    for method in self._exception_middleware:\n        ignored = False\n        for keyword in self.ignore_keywords:\n            if method.__str__().count(keyword):\n                ignored = True\n                break\n        if (not ignored):\n            new_exception_middleware.append(method)\n    self._exception_middleware = new_exception_middleware\n", "label": 1}
{"function": "\n\ndef is_waf(self):\n    detected = False\n    if (self.matchcookie('^LastMRH_Session') and self.matchcookie('^MRHSession')):\n        return True\n    elif (self.matchheader(('server', 'BigIP|BIG-IP|BIGIP')) and self.matchcookie('^MRHSession')):\n        return True\n    if (self.matchheader(('Location', '\\\\/my.policy')) and self.matchheader(('server', 'BigIP|BIG-IP|BIGIP'))):\n        return True\n    elif (self.matchheader(('Location', '\\\\/my\\\\.logout\\\\.php3')) and self.matchheader(('server', 'BigIP|BIG-IP|BIGIP'))):\n        return True\n    elif (self.matchheader(('Location', '.+\\\\/f5\\\\-w\\\\-68747470.+')) and self.matchheader(('server', 'BigIP|BIG-IP|BIGIP'))):\n        return True\n    elif self.matchheader(('server', 'BigIP|BIG-IP|BIGIP')):\n        return True\n    elif (self.matchcookie('^F5_fullWT') or self.matchcookie('^F5_ST') or self.matchcookie('^F5_HT_shrinked')):\n        return True\n    elif (self.matchcookie('^MRHSequence') or self.matchcookie('^MRHSHint') or self.matchcookie('^LastMRH_Session')):\n        return True\n    else:\n        return False\n", "label": 1}
{"function": "\n\n@opt.register_specialize('fast_compile_gpu')\n@gof.local_optimizer([subtensor.AdvancedSubtensor, tensor.log])\ndef local_advanced_indexing_crossentropy_onehot(node):\n    log = None\n    sm = None\n    if isinstance(node.op, subtensor.AdvancedSubtensor):\n        try:\n            (log, rows, labels) = node.inputs\n        except Exception:\n            pass\n        if (log and log.owner and (log.owner.op == tensor.log)):\n            sm = log.owner.inputs[0]\n    elif (node.op == tensor.log):\n        pre_log = node.inputs[0].owner\n        if (pre_log and isinstance(pre_log.op, subtensor.AdvancedSubtensor)):\n            try:\n                (sm, rows, labels) = pre_log.inputs\n            except Exception:\n                pass\n    if ((sm is not None) and sm.owner and (sm.owner.op in (softmax_op, softmax_with_bias))):\n        sm_w_bias = local_softmax_with_bias.transform(sm.owner)\n        if sm_w_bias:\n            assert (sm_w_bias[0].owner.op == softmax_with_bias)\n            (x_var, b_var) = sm_w_bias[0].owner.inputs\n        else:\n            x_var = sm.owner.inputs[0]\n            b_var = tensor.zeros_like(x_var[0])\n        if _check_rows_is_arange_len_labels(rows, labels):\n            if ((labels.ndim == 1) and (x_var.ndim == 2)):\n                minus_ret = crossentropy_softmax_argmax_1hot_with_bias(x_var, b_var, labels)[0]\n                ret = (- minus_ret)\n                copy_stack_trace(node.outputs[0], [minus_ret, ret])\n                return [ret]\n", "label": 1}
{"function": "\n\ndef assert_common_signed_properties(self, info):\n    assert ('Executable' in info)\n    assert ('Identifier' in info)\n    assert ('CodeDirectory' in info)\n    codedirectory_info = info['CodeDirectory'][0]\n    assert (codedirectory_info['location'] == 'embedded')\n    assert ('Hash' in info)\n    hashes = self.get_dict_with_key(info['Hash'], '_')\n    assert (hashes is not None)\n    assert ('CDHash' in info)\n    assert ('Signature' in info)\n    assert ('Authority' in info)\n    assert ('Info.plist' in info)\n    assert (self.get_dict_with_key(info['Info.plist'], 'entries') is not None)\n    assert ('TeamIdentifier' in info)\n    assert (info['TeamIdentifier'][0] == self.OU)\n    assert ('designated' in info)\n    assert ('anchor apple generic' in info['designated'][0])\n    assert (self.ERROR_KEY not in info)\n", "label": 1}
{"function": "\n\ndef _parse_relation(chunk, type='O'):\n    ' Returns a string of the roles and relations parsed from the given <chunk> element.\\n        The chunk type (which is part of the relation string) can be given as parameter.\\n    '\n    r1 = chunk.get(XML_RELATION)\n    r2 = chunk.get(XML_ID, chunk.get(XML_OF))\n    r1 = ([(((x != '-') and x) or None) for x in r1.split('|')] or [None])\n    r2 = ([(((x != '-') and x) or None) for x in r2.split('|')] or [None])\n    r2 = [(((x is not None) and x.split(_UID_SEPARATOR)[(- 1)]) or x) for x in r2]\n    if (len(r1) < len(r2)):\n        r1 = (r1 + (r1 * (len(r2) - len(r1))))\n    if (len(r2) < len(r1)):\n        r2 = (r2 + (r2 * (len(r1) - len(r2))))\n    return ';'.join(['-'.join([x for x in (type, r1, r2) if x]) for (r1, r2) in zip(r1, r2)])\n", "label": 1}
{"function": "\n\ndef getmodule(object, _filename=None):\n    'Return the module an object was defined in, or None if not found.'\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    if ((_filename is not None) and (_filename in modulesbyfile)):\n        return sys.modules.get(modulesbyfile[_filename])\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if (file in modulesbyfile):\n        return sys.modules.get(modulesbyfile[file])\n    for (modname, module) in sys.modules.items():\n        if (ismodule(module) and hasattr(module, '__file__')):\n            f = module.__file__\n            if (f == _filesbymodname.get(modname, None)):\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            modulesbyfile[f] = modulesbyfile[os.path.realpath(f)] = module.__name__\n    if (file in modulesbyfile):\n        return sys.modules.get(modulesbyfile[file])\n    main = sys.modules['__main__']\n    if (not hasattr(object, '__name__')):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if (mainobject is object):\n            return main\n    builtin = sys.modules['__builtin__']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if (builtinobject is object):\n            return builtin\n", "label": 1}
{"function": "\n\ndef authorize(request, page=None, edit=False, add=False, delete=False, view=False, do_raise=True):\n    if isinstance(page, basestring):\n        page = Page.objects.get(slug=page).get_content_model()\n    user = get_user(request)\n    request.user = user\n    auth = True\n    if (auth and (page is not None)):\n        request.user = user\n        if edit:\n            auth = page.can_change(request)\n        if add:\n            auth = (auth and page.can_add(request))\n        if delete:\n            auth = (auth and page.can_delete(request))\n        elif view:\n            auth = ((auth and (not hasattr(page, 'can_view'))) or (auth and hasattr(page, 'can_view') and page.can_view(request)))\n    if (do_raise and (not auth)):\n        raise PermissionDenied(json.dumps({\n            'error': 'Unauthorized',\n            'user': (user.email if user.is_authenticated() else None),\n            'page': (page.slug if page else None),\n            'edit': edit,\n            'add': add,\n            'delete': delete,\n            'view': view,\n        }))\n    return user\n", "label": 1}
{"function": "\n\ndef from_instance(self, instance, keyonly=False, attrs=None, eagerload=[]):\n    if keyonly:\n        attrs = (self._pkey_attrs + self._ref_attrs)\n    for attr in self._wsme_attributes:\n        if (not isinstance(attr, wsattr)):\n            continue\n        if (attrs and (not attr.isrelation) and (attr.name not in attrs)):\n            continue\n        if (attr.isrelation and (attr.name not in eagerload)):\n            continue\n        value = getattr(instance, attr.saname)\n        if attr.isrelation:\n            attr_keyonly = (attr.name not in eagerload)\n            attr_attrs = None\n            attr_eagerload = []\n            if (not attr_keyonly):\n                attr_attrs = [aname[(len(attr.name) + 1):] for aname in attrs if aname.startswith((attr.name + '.'))]\n                attr_eagerload = [aname[(len(attr.name) + 1):] for aname in eagerload if aname.startswith((attr.name + '.'))]\n            if attr.saproperty.uselist:\n                value = [attr.datatype.item_type(o, keyonly=attr_keyonly, attrs=attr_attrs, eagerload=attr_eagerload) for o in value]\n            else:\n                value = attr.datatype(value, keyonly=attr_keyonly, attrs=attr_attrs, eagerload=attr_eagerload)\n        attr.__set__(self, value)\n", "label": 1}
{"function": "\n\ndef __getitem__(self, key):\n    \"\\n        Return an element for any key chosen key (in'bounded mode') or\\n        for a previously generated key that is still in the cache\\n        (for one of the 'open' modes)\\n        \"\n    tuple_key = util.wrap_tuple(key)\n    if (self.mode == 'bounded'):\n        if (key == slice(None, None, None)):\n            return self.clone(self)\n        slices = [el for el in tuple_key if isinstance(el, slice)]\n        if any((el.step for el in slices)):\n            raise Exception('Slices cannot have a step argument in DynamicMap bounded mode ')\n        if (len(slices) not in [0, len(tuple_key)]):\n            raise Exception('Slices must be used exclusively or not at all')\n        if slices:\n            return self._slice_bounded(tuple_key)\n    try:\n        cache = super(DynamicMap, self).__getitem__(key)\n        if (isinstance(cache, DynamicMap) and (self.mode == 'open')):\n            cache = self.clone(cache)\n    except KeyError as e:\n        cache = None\n        if ((self.mode == 'open') and (len(self.data) > 0)):\n            raise KeyError((str(e) + ' Note: Cannot index outside available cache in open interval mode.'))\n    product = self._cross_product(tuple_key, (cache.data if cache else {\n        \n    }))\n    if (product is not None):\n        return product\n    if cache:\n        return cache\n    val = self._execute_callback(*tuple_key)\n    if (self.call_mode == 'counter'):\n        val = val[1]\n    self._cache(tuple_key, val)\n    return val\n", "label": 1}
{"function": "\n\ndef match_gitignore(path):\n    \"Match a path that contains .gitignore files at multiple levels.\\n\\n    Git's matching algorithm is described in gitignore(5).\\n    \"\n    elem = path[(- 1)][0]\n    is_directory = stat.S_ISDIR(path[(- 1)][2])\n    if (is_directory and (elem == '.git')):\n        return TRUNCATE\n    if (len(path) > 1):\n        ignored = path[(- 2)][2]\n    else:\n        ignored = False\n    for i in range(len(path)):\n        parsed_ignore_file = path[i][1]\n        if (not parsed_ignore_file):\n            continue\n        for (pattern, flags) in parsed_ignore_file:\n            sense = (not bool((flags & MATCH_INVERSE)))\n            if (len(pattern) == 1):\n                if pattern[0].match(elem):\n                    if ((not (flags & MATCH_END_WITH_DIRECTORY)) or is_directory):\n                        ignored = sense\n            elif (len(pattern) == (len(path) - i)):\n                for j in range(len(pattern)):\n                    if (not pattern[j].match(path[(i + j)][0])):\n                        break\n                else:\n                    if ((not (flags & MATCH_END_WITH_DIRECTORY)) or is_directory):\n                        ignored = sense\n    return (INCLUDE if (not ignored) else EXCLUDE)\n", "label": 1}
{"function": "\n\ndef process_api_request(self, request):\n    reference = request.reference\n    if (reference.kind == 'subdocument'):\n        (_, auth_target, _, _) = reference.value\n    else:\n        auth_target = reference.value\n    permission_name = self._get_permission_name(request)\n    authentication_target = self.authentication_requirement(auth_target, permission_name)\n    authorization_target = self.authorization_requirement(auth_target, permission_name)\n    if (authentication_target is None):\n        return request\n    if (reference.format == 'schema'):\n        return request\n    if (reference.format == 'help'):\n        return request\n    auth_token = request.api_arguments.get('_auth', None)\n    if (not auth_token):\n        bearer = request.headers.get('Authorization', None)\n        if bearer:\n            auth_token = bearer[7:]\n    if auth_token:\n        user = request.suite['auth'].check(auth_token)\n        request.user = user\n    else:\n        request.user = None\n        user = None\n    if (((authentication_target is not None) or (authorization_target is not None)) and (not user)):\n        print('Permission error!!!!!')\n        raise PermissionError('Target {url} requires authentication or authorization, but user is anonymous'.format(url=reference.url))\n    if (user and user['admin']):\n        return request\n    if (authorization_target is None):\n        return request\n    for role in user.fetch('roles'):\n        if role.authorizes(authorization_target, permission_name):\n            return request\n    else:\n        msg = \"Permission '{name}' denied for '{user}' accessing '{url}'\".format(user=user, url=reference.url, name=permission_name)\n        request.suite.log.error(msg)\n        raise PermissionError(msg)\n", "label": 1}
{"function": "\n\ndef test_fastx(filepath):\n    with open(filepath, encoding='latin-1') as fastx_file:\n        sequences = OrderedDict()\n        seq = []\n        header = ''\n        found_caret = False\n        for (row_index, row) in enumerate(fastx_file, 1):\n            if (row_index > 30):\n                break\n            if (not row.strip()):\n                continue\n            if ((found_caret is False) and (row[0] != '>')):\n                if (row[0] == ';'):\n                    continue\n                break\n            elif ((found_caret is False) and (row[0] == '>')):\n                found_caret = True\n            if (row and (row[0] == '>')):\n                if seq:\n                    sequences[header] = ''.join(seq)\n                    seq = []\n                header = row\n            elif row:\n                seq.append(row)\n        if (seq and header):\n            sequences[header] = ''.join(seq)\n        if sequences:\n            rows = []\n            [rows.extend([i, v]) for (i, v) in six.iteritems(sequences)]\n            return (True, rows)\n    return (False, None)\n", "label": 1}
{"function": "\n\ndef Equals(self, x):\n    if (x is self):\n        return 1\n    if (self.has_topic_ != x.has_topic_):\n        return 0\n    if (self.has_topic_ and (self.topic_ != x.topic_)):\n        return 0\n    if (self.has_sub_id_ != x.has_sub_id_):\n        return 0\n    if (self.has_sub_id_ and (self.sub_id_ != x.sub_id_)):\n        return 0\n    if (self.has_lease_duration_sec_ != x.has_lease_duration_sec_):\n        return 0\n    if (self.has_lease_duration_sec_ and (self.lease_duration_sec_ != x.lease_duration_sec_)):\n        return 0\n    if (self.has_vanilla_query_ != x.has_vanilla_query_):\n        return 0\n    if (self.has_vanilla_query_ and (self.vanilla_query_ != x.vanilla_query_)):\n        return 0\n    if (len(self.schema_entry_) != len(x.schema_entry_)):\n        return 0\n    for (e1, e2) in zip(self.schema_entry_, x.schema_entry_):\n        if (e1 != e2):\n            return 0\n    return 1\n", "label": 1}
{"function": "\n\ndef test_fraction():\n    (x, y, z) = map(Symbol, 'xyz')\n    A = Symbol('A', commutative=False)\n    assert (fraction(Rational(1, 2)) == (1, 2))\n    assert (fraction(x) == (x, 1))\n    assert (fraction((1 / x)) == (1, x))\n    assert (fraction((x / y)) == (x, y))\n    assert (fraction((x / 2)) == (x, 2))\n    assert (fraction(((x * y) / z)) == ((x * y), z))\n    assert (fraction((x / (y * z))) == (x, (y * z)))\n    assert (fraction((1 / (y ** 2))) == (1, (y ** 2)))\n    assert (fraction((x / (y ** 2))) == (x, (y ** 2)))\n    assert (fraction((((x ** 2) + 1) / y)) == (((x ** 2) + 1), y))\n    assert (fraction(((x * (y + 1)) / (y ** 7))) == ((x * (y + 1)), (y ** 7)))\n    assert (fraction(exp((- x)), exact=True) == (exp((- x)), 1))\n    assert (fraction(((x * A) / y)) == ((x * A), y))\n    assert (fraction(((x * (A ** (- 1))) / y)) == ((x * (A ** (- 1))), y))\n    n = symbols('n', negative=True)\n    assert (fraction(exp(n)) == (1, exp((- n))))\n    assert (fraction(exp((- n))) == (exp((- n)), 1))\n", "label": 1}
{"function": "\n\ndef _get_gs_outputs(self, mode, vois):\n    '\\n        Linear Gauss-Siedel can limit the outputs when calling apply. This\\n        calculates and caches the list of outputs to be updated for each voi.\\n        '\n    if (self._gs_outputs is None):\n        self._gs_outputs = {\n            \n        }\n    if (mode not in self._gs_outputs):\n        dumat = self.dumat\n        gs_outputs = self._gs_outputs[mode] = OrderedDict()\n        if (mode == 'fwd'):\n            for sub in self._local_subsystems:\n                gs_outputs[sub.name] = outs = OrderedDict()\n                for voi in vois:\n                    if (voi in dumat):\n                        outs[voi] = set([x for x in dumat[voi]._dat if (sub.dumat and (x not in sub.dumat[voi]))])\n        else:\n            for sub in self._local_subsystems:\n                gs_outputs[sub.name] = outs = OrderedDict()\n                for voi in vois:\n                    if (voi in dumat):\n                        outs[voi] = set([x for x in dumat[voi]._dat if ((not sub.dumat) or (sub.dumat and (x not in sub.dumat[voi])))])\n    return self._gs_outputs\n", "label": 1}
{"function": "\n\n@command\ndef help(self, mask, target, args):\n    'Show help\\n\\n            %%help [<cmd>]\\n        '\n    if args['<cmd>']:\n        args = args['<cmd>']\n        if args.startswith(self.context.config.cmd):\n            args = args[len(self.context.config.cmd):]\n        (predicates, meth) = self.get(args, (None, None))\n        if (meth is not None):\n            doc = (meth.__doc__ or '')\n            doc = [l.strip() for l in doc.split('\\n') if l.strip()]\n            buf = ''\n            for line in doc:\n                if (('%%' not in line) and (buf is not None)):\n                    buf += (line + ' ')\n                else:\n                    if (buf is not None):\n                        for b in utils.split_message(buf, 160):\n                            (yield b)\n                        buf = None\n                    line = line.replace('%%', self.context.config.cmd)\n                    (yield line)\n        else:\n            (yield ('No such command. Try %shelp for an overview of all commands.' % self.context.config.cmd))\n    else:\n        cmds = sorted((k for (k, (p, m)) in self.items() if p.get('show_in_help_list', True)))\n        cmds_str = ', '.join([(self.cmd + k) for k in cmds])\n        lines = utils.split_message(('Available commands: %s ' % cmds_str), 160)\n        for line in lines:\n            (yield line)\n        url = self.config.get('url')\n        if url:\n            (yield ('Full help is available at ' + url))\n", "label": 1}
{"function": "\n\ndef parseBytecode(dex, insns_start_pos, shorts, catch_addrs):\n    ops = []\n    pos = 0\n    while (pos < len(shorts)):\n        (pos, op) = parseInstruction(dex, insns_start_pos, shorts, pos)\n        ops.append(op)\n    for (instr, instr2) in zip(ops, ops[1:]):\n        if (not (instr2.type == MoveResult)):\n            continue\n        if (instr.type in INVOKE_TYPES):\n            called_id = dex.method_id(instr.args[0])\n            if (called_id.return_type != b'V'):\n                instr2.prev_result = called_id.return_type\n        elif (instr.type == FilledNewArray):\n            instr2.prev_result = dex.type(instr.args[0])\n        elif (instr2.pos in catch_addrs):\n            instr2.prev_result = b'Ljava/lang/Throwable;'\n    assert (0 not in catch_addrs)\n    for (i, instr) in enumerate(ops):\n        if (instr.opcode in (56, 57)):\n            if ((i > 0) and (ops[(i - 1)].type == InstanceOf)):\n                prev = ops[(i - 1)]\n                desc_ind = prev.args[2]\n                regs = {prev.args[1]}\n                if ((i > 1) and (ops[(i - 2)].type == Move)):\n                    prev2 = ops[(i - 2)]\n                    if (prev2.args[0] == prev.args[1]):\n                        regs.add(prev2.args[1])\n                regs.discard(prev.args[0])\n                if regs:\n                    instr.implicit_casts = (desc_ind, sorted(regs))\n    return ops\n", "label": 1}
{"function": "\n\ndef get_completions(self, view, prefix):\n    skip_deleted = Pref.forget_deleted_files\n    completions = list(Pref.always_on_auto_completions)\n    already_in = []\n    for (file, data) in self.files.items():\n        if ((not skip_deleted) or (skip_deleted and os.path.lexists(file))):\n            location = basename(file)\n            for function in data:\n                if (prefix in function[self.NAME]):\n                    already_in.append(function[self.NAME])\n                    completion = self.create_function_completion(function, location)\n                    completions.append(completion)\n    location = (basename(view.file_name()) if view.file_name() else '')\n    [completions.append(self.create_function_completion(self.parse_line(view.substr(view.line(selection))), location)) for selection in view.find_by_selector('entity.name.function') if ((view.substr(selection) not in already_in) and (already_in.append(view.substr(selection)) or True))]\n    vars = []\n    [view.substr(selection) for selection in view.find_all('([var\\\\s+]|\\\\.)(\\\\w+)\\\\s*[=|:]', 0, '$2', vars)]\n    [completions.append(self.create_var_completion(var, location)) for var in list(set(vars)) if ((len(var) > 1) and (var not in already_in))]\n    if debug:\n        print('Completions')\n        print(completions)\n    return completions\n", "label": 1}
{"function": "\n\ndef _find(self, tests, obj, name, module, source_lines, globs, seen):\n    '\\n        Find tests for the given object and any contained objects, and\\n        add them to `tests`.\\n        '\n    if hasattr(obj, 'skip_doctest'):\n        obj = DocTestSkip(obj)\n    doctest.DocTestFinder._find(self, tests, obj, name, module, source_lines, globs, seen)\n    from inspect import isroutine, isclass, ismodule\n    if (inspect.ismodule(obj) and self._recurse):\n        for (valname, val) in list(obj.__dict__.items()):\n            valname1 = ('%s.%s' % (name, valname))\n            if ((isroutine(val) or isclass(val)) and self._from_module(module, val)):\n                self._find(tests, val, valname1, module, source_lines, globs, seen)\n    if (inspect.isclass(obj) and self._recurse):\n        for (valname, val) in list(obj.__dict__.items()):\n            if isinstance(val, staticmethod):\n                val = getattr(obj, valname)\n            if isinstance(val, classmethod):\n                val = getattr(obj, valname).__func__\n            if ((inspect.isfunction(val) or inspect.isclass(val) or inspect.ismethod(val) or isinstance(val, property)) and self._from_module(module, val)):\n                valname = ('%s.%s' % (name, valname))\n                self._find(tests, val, valname, module, source_lines, globs, seen)\n", "label": 1}
{"function": "\n\ndef preproc_utts(utts):\n    '\\n    Convert raw text into character language model format\\n    Split characters up, add start and end symbols, replace spaces\\n    Avoid splitting up characters in specials tokens list\\n    Also remove hesitations and parentheses\\n    '\n    utt_sents = [utt.split(' ') for utt in utts]\n    utt_sents = [[word for word in utt if (word != '(%hesitation)')] for utt in utt_sents]\n    utt_sents = [[(word[1:(- 1)] if word.startswith('(') else word) for word in utt] for utt in utt_sents]\n    text = [[(list((sent[k] + (' ' if (k < (len(sent) - 1)) else ''))) if (sent[k] not in SPECIALS_LIST) else [sent[k]]) for k in xrange(len(sent))] for sent in utt_sents]\n    text = [[c for w in s for c in w] for s in text]\n    text = [((['<s>'] + [(c if (c != ' ') else SPACE) for c in utt]) + ['</s>']) for utt in text]\n    return text\n", "label": 1}
{"function": "\n\ndef test_updates_add(self):\n    up1 = OrderedUpdates()\n    up2 = OrderedUpdates()\n    a = theano.shared('a')\n    b = theano.shared('b')\n    assert (not (up1 + up2))\n    up1[a] = 5\n    assert up1\n    assert (up1 + up2)\n    assert (not up2)\n    assert (len((up1 + up2)) == 1)\n    assert ((up1 + up2)[a] == 5)\n    up2[b] = 7\n    assert up1\n    assert (up1 + up2)\n    assert up2\n    assert (len((up1 + up2)) == 2)\n    assert ((up1 + up2)[a] == 5)\n    assert ((up1 + up2)[b] == 7)\n    assert (a in (up1 + up2))\n    assert (b in (up1 + up2))\n    assert (len(((up1 + up1) + up1)) == 1)\n    up2[a] = 8\n    try:\n        (up1 + up2)\n        assert 0\n    except KeyError:\n        pass\n    up2[a] = 10\n", "label": 1}
{"function": "\n\ndef transform_Attribute(self, expr):\n    v = self.transform_expr(expr.value)\n    if ((v.__class__ is Var) and (v.name in self.bindings)):\n        stored_v = self.bindings[v.name]\n        c = stored_v.__class__\n        if ((c is Var) or (c is Struct)):\n            v = stored_v\n        elif (c is ArrayView):\n            if (expr.name == 'shape'):\n                return self.transform_expr(stored_v.shape)\n            elif (expr.name == 'strides'):\n                return self.transform_expr(stored_v.strides)\n            elif (expr.name == 'data'):\n                return self.transform_expr(stored_v.data)\n        elif (c is AllocArray):\n            if (expr.name == 'shape'):\n                return self.transform_expr(stored_v.shape)\n        elif (c is Slice):\n            if (expr.name == 'start'):\n                return self.transform_expr(stored_v.start)\n            elif (expr.name == 'stop'):\n                return self.transform_expr(stored_v.stop)\n            else:\n                assert (expr.name == 'step'), ('Unexpected attribute for slice: %s' % expr.name)\n                return self.transform_expr(stored_v.step)\n    if (v.__class__ is Struct):\n        idx = v.type.field_pos(expr.name)\n        return v.args[idx]\n    elif (v.__class__ is not Var):\n        v = self.temp(v, 'struct')\n    if (expr.value == v):\n        return expr\n    else:\n        return Attribute(value=v, name=expr.name, type=expr.type)\n", "label": 1}
{"function": "\n\ndef reindent(text, filename):\n    new_lines = []\n    k = 0\n    c = 0\n    for (n, raw_line) in enumerate(text.splitlines()):\n        line = raw_line.strip()\n        if ((not line) or (line[0] == '#')):\n            new_lines.append(line)\n            continue\n        line3 = line[:3]\n        line4 = line[:4]\n        line5 = line[:5]\n        line6 = line[:6]\n        line7 = line[:7]\n        if ((line3 == 'if ') or (line4 in ('def ', 'for ', 'try:')) or (line6 == 'while ') or (line6 == 'class ') or (line5 == 'with ')):\n            new_lines.append((('    ' * k) + line))\n            k += 1\n            continue\n        elif ((line5 == 'elif ') or (line5 == 'else:') or (line7 == 'except:') or (line7 == 'except ') or (line7 == 'finally:')):\n            c = (k - 1)\n            if (c < 0):\n                raise ParseError(('Extra pass founded on line %s:%d' % (filename, n)))\n            new_lines.append((('    ' * c) + line))\n            continue\n        else:\n            new_lines.append((('    ' * k) + line))\n        if ((line == 'pass') or (line5 == 'pass ')):\n            k -= 1\n        if (k < 0):\n            k = 0\n    text = '\\n'.join(new_lines)\n    return text\n", "label": 1}
{"function": "\n\ndef load_suite_tests(only=None):\n    (only_module, only_test_case) = (None, None)\n    if only:\n        args = only.split('.')\n        only_module = args[0]\n        only_test_case = ((args[1:] and args[1]) or None)\n        only_function = ((args[2:] and args[2]) or None)\n    suites = []\n    for (dirpath, dirnames, filenames) in os.walk(REGRESSION_TEST_DIR):\n        for f in filenames:\n            (basename, ext) = os.path.splitext(f)\n            if ((ext == '.py') and ((not only_module) or (only_module == basename))):\n                modname = ('%s.%s' % ('.'.join(dirpath.split('/')), basename))\n                package = __import__(modname, globals(), locals(), [], (- 1))\n                mod = sys.modules[modname]\n                if hasattr(mod, 'suite'):\n                    suite = mod.suite()\n                    if only_test_case:\n                        suite._tests = [t for t in suite._tests if (t.__class__.__name__ == only_test_case)]\n                        if only_function:\n                            suite._tests = [t for t in suite._tests if (t._testMethodName == only_function)]\n                    suites.append(suite)\n    return suites\n", "label": 1}
{"function": "\n\ndef nmap_udppacket_sig(S, T):\n    r = {\n        \n    }\n    if (T is None):\n        r['Resp'] = 'N'\n    else:\n        r['DF'] = (((T.flags & 2) and 'Y') or 'N')\n        r['TOS'] = ('%X' % T.tos)\n        r['IPLEN'] = ('%X' % T.len)\n        r['RIPTL'] = ('%X' % T.payload.payload.len)\n        r['RID'] = (((S.id == T.payload.payload.id) and 'E') or 'F')\n        r['RIPCK'] = (((S.chksum == T.getlayer(IPerror).chksum) and 'E') or ((T.getlayer(IPerror).chksum == 0) and '0') or 'F')\n        r['UCK'] = (((S.payload.chksum == T.getlayer(UDPerror).chksum) and 'E') or ((T.getlayer(UDPerror).chksum == 0) and '0') or 'F')\n        r['ULEN'] = ('%X' % T.getlayer(UDPerror).len)\n        r['DAT'] = (((T.getlayer(conf.raw_layer) is None) and 'E') or ((S.getlayer(conf.raw_layer).load == T.getlayer(conf.raw_layer).load) and 'E') or 'F')\n    return r\n", "label": 1}
{"function": "\n\ndef _check_bad_constraint(self, violation, djoint_feature_mean, loss, old_constraints, break_on_bad, tol=None):\n    violation_difference = (violation - self.last_slack_)\n    if (self.verbose > 1):\n        print(('New violation: %f difference to last: %f' % (violation, violation_difference)))\n    if ((violation_difference < 0) and (violation > 0) and break_on_bad):\n        raise ValueError('Bad inference: new violation is smaller than old.')\n    if (tol is None):\n        tol = self.tol\n    if (violation_difference < tol):\n        if self.verbose:\n            print('new constraint too weak.')\n        return True\n    equals = [True for (djoint_feature_, loss_) in old_constraints if (np.all((djoint_feature_ == djoint_feature_mean)) and (loss == loss_))]\n    if np.any(equals):\n        return True\n    if self.check_constraints:\n        for con in old_constraints:\n            violation_tmp = max((con[1] - np.dot(self.w, con[0])), 0)\n            if (self.verbose > 5):\n                print(('violation old constraint: %f' % violation_tmp))\n            if ((violation - violation_tmp) < (- 1e-05)):\n                if self.verbose:\n                    print(('bad inference: %f' % (violation_tmp - violation)))\n                if break_on_bad:\n                    raise ValueError('Bad inference: new violation is weaker than previous constraint.')\n                return True\n    return False\n", "label": 1}
{"function": "\n\ndef add(self, *statement, **kwargs):\n    for k in kwargs.keys():\n        if (k not in ('keep', 'delay', 'cond', 'lazy_cond', 'eager_val')):\n            raise NameError(('Keyword argument %s is not supported.' % k))\n    keep = (kwargs['keep'] if ('keep' in kwargs) else None)\n    delay = (kwargs['delay'] if ('delay' in kwargs) else None)\n    cond = (kwargs['cond'] if ('cond' in kwargs) else None)\n    lazy_cond = (kwargs['lazy_cond'] if ('lazy_cond' in kwargs) else False)\n    eager_val = (kwargs['eager_val'] if ('eager_val' in kwargs) else False)\n    if (keep is not None):\n        del kwargs['keep']\n        for i in range(keep):\n            kwargs['delay'] = (i if (delay is None) else (delay + i))\n            self.add(*statement, **kwargs)\n        return self\n    if ((delay is not None) and (delay > 0)):\n        if eager_val:\n            statement = [self._add_delayed_subst(s, delay) for s in statement]\n        if (cond is not None):\n            if (not lazy_cond):\n                cond = self._add_delayed_cond(cond, delay)\n            statement = [vtypes.If(cond)(*statement)]\n        self.delayed_body[delay].extend(statement)\n        self._add_dst_var(statement)\n        return self\n    if (cond is not None):\n        statement = [vtypes.If(cond)(*statement)]\n    self.body.extend(statement)\n    self._add_dst_var(statement)\n    return self\n", "label": 1}
{"function": "\n\ndef _iterencode_dict(self, dct, markers=None):\n    if (not dct):\n        (yield '{}')\n        return\n    if (markers is not None):\n        markerid = id(dct)\n        if (markerid in markers):\n            raise ValueError('Circular reference detected')\n        markers[markerid] = dct\n    (yield '{')\n    first = True\n    if self.ensure_ascii:\n        encoder = encode_basestring_ascii\n    else:\n        encoder = encode_basestring\n    allow_nan = self.allow_nan\n    if self.sort_keys:\n        keys = dct.keys()\n        keys.sort()\n        items = [(k, dct[k]) for k in keys]\n    else:\n        items = dct.iteritems()\n    for (key, value) in items:\n        if isinstance(key, (str, unicode)):\n            pass\n        elif isinstance(key, float):\n            key = floatstr(key, allow_nan)\n        elif isinstance(key, (int, long)):\n            key = str(key)\n        elif (key is True):\n            key = 'true'\n        elif (key is False):\n            key = 'false'\n        elif (key is None):\n            key = 'null'\n        elif self.skipkeys:\n            continue\n        else:\n            raise TypeError(('key %r is not a string' % (key,)))\n        if first:\n            first = False\n        else:\n            (yield ', ')\n        (yield encoder(key))\n        (yield ': ')\n        for chunk in self._iterencode(value, markers):\n            (yield chunk)\n    (yield '}')\n    if (markers is not None):\n        del markers[markerid]\n", "label": 1}
{"function": "\n\ndef changeVariables(self, source, variables):\n    self._changingVariables = True\n    try:\n        super(AbstractTextField, self).changeVariables(source, variables)\n        if (VTextField.VAR_CURSOR in variables):\n            obj = variables.get(VTextField.VAR_CURSOR)\n            self._lastKnownCursorPosition = int(obj)\n        if (VTextField.VAR_CUR_TEXT in variables):\n            self.handleInputEventTextChange(variables)\n        if (('text' in variables) and (not self.isReadOnly())):\n            newValue = variables.get('text')\n            if ((self.getMaxLength() != (- 1)) and (len(newValue) > self.getMaxLength())):\n                newValue = newValue[:self.getMaxLength()]\n            oldValue = self.getFormattedValue()\n            if ((newValue is not None) and ((oldValue is None) or self.isNullSettingAllowed()) and (newValue == self.getNullRepresentation())):\n                newValue = None\n            if ((newValue != oldValue) and ((newValue is None) or (newValue != oldValue))):\n                wasModified = self.isModified()\n                self.setValue(newValue, True)\n                if ((self._format is not None) or (wasModified != self.isModified())):\n                    self.requestRepaint()\n        self.firePendingTextChangeEvent()\n        if (FocusEvent.EVENT_ID in variables):\n            self.fireEvent(FocusEvent(self))\n        if (BlurEvent.EVENT_ID in variables):\n            self.fireEvent(BlurEvent(self))\n    finally:\n        self._changingVariables = False\n", "label": 1}
{"function": "\n\ndef __init__(self, initial_items=None, **params):\n    if isinstance(initial_items, NdMapping):\n        map_type = type(initial_items)\n        own_params = self.params()\n        new_params = dict(initial_items.get_param_values(onlychanged=True))\n        if (new_params.get('group') == map_type.__name__):\n            new_params.pop('group')\n        params = dict({name: value for (name, value) in new_params.items() if (name in own_params)}, **params)\n    super(MultiDimensionalMapping, self).__init__(OrderedDict(), **params)\n    self._next_ind = 0\n    self._check_key_type = True\n    self._cached_index_types = [d.type for d in self.kdims]\n    self._cached_index_values = {d.name: d.values for d in self.kdims}\n    self._cached_categorical = any((d.values for d in self.kdims))\n    self._instantiated = (not any(((v == 'initial') for v in self._cached_index_values.values())))\n    if (initial_items is None):\n        initial_items = []\n    if isinstance(initial_items, tuple):\n        self._add_item(initial_items[0], initial_items[1])\n    elif ((not self._check_items) and self._instantiated):\n        if isinstance(initial_items, dict):\n            initial_items = initial_items.items()\n        elif isinstance(initial_items, MultiDimensionalMapping):\n            initial_items = initial_items.data.items()\n        self.data = OrderedDict((((k if isinstance(k, tuple) else (k,)), v) for (k, v) in initial_items))\n        self._resort()\n    elif (initial_items is not None):\n        self.update(OrderedDict(initial_items))\n    self._instantiated = True\n", "label": 1}
{"function": "\n\ndef _fill_in_vars(self, args):\n    defaults = symbols('x,y,z,u,v')\n    if (len(args) == 0):\n        return defaults\n    if (not isinstance(args, (tuple, list))):\n        raise v_error\n    if (len(args) == 0):\n        return defaults\n    for s in args:\n        if ((s is not None) and (not isinstance(s, Symbol))):\n            raise v_error\n    vars = [Symbol(('unbound%i' % i)) for i in range(1, 6)]\n    if (len(args) == 1):\n        vars[3] = args[0]\n    elif (len(args) == 2):\n        if (args[0] is not None):\n            vars[3] = args[0]\n        if (args[1] is not None):\n            vars[4] = args[1]\n    elif (len(args) >= 3):\n        if (args[0] is not None):\n            vars[0] = args[0]\n        if (args[1] is not None):\n            vars[1] = args[1]\n        if (args[2] is not None):\n            vars[2] = args[2]\n        if (len(args) >= 4):\n            vars[3] = args[3]\n            if (len(args) >= 5):\n                vars[4] = args[4]\n    return vars\n", "label": 1}
{"function": "\n\ndef output(self, passed, test):\n    if test.isFunctional():\n        if (passed is None):\n            res = '? Unsupported'\n        else:\n            res = (': Accepted' if passed else '- REJECTED')\n        self.opt.log('{:>16} {:}'.format(test.getTestName(), res))\n    elif test.isOverflow():\n        res = ('| Terminated' if passed else ': Connected')\n        self.opt.log('{:>24} {:}'.format(test.getTestName(), res))\n    else:\n        if ((passed and (not self.opt.quiet)) or (not passed)):\n            if (passed is None):\n                res = 'Unsupported'\n            else:\n                res = ('Passed' if passed else 'FAILED')\n            kind = ('positive' if test.getTestType() else 'negative')\n            self.opt.log(((((res + ' ') + kind) + ' test: ') + test.getTestName()))\n            test.printMsg(passed)\n        if ((not passed) and test.getCritical() and (not self.opt.all)):\n            self.opt.log(('- failed a dependency test, excluding ' + 'descendant test cases...'))\n            for c in self.testCases[:]:\n                for p in test.__class__.__bases__:\n                    if isinstance(c, p):\n                        self.opt.log(('  > ' + str(c.getTestName())))\n                        self.testCases.remove(c)\n            return True\n    return False\n", "label": 1}
{"function": "\n\ndef _is_valid_ip(self, ip_str):\n    'Ensure we have a valid IPv6 address.\\n\\n        Probably not as exhaustive as it should be.\\n\\n        Args:\\n            ip_str: A string, the IPv6 address.\\n\\n        Returns:\\n            A boolean, True if this is a valid IPv6 address.\\n\\n        '\n    if (':' not in ip_str):\n        return False\n    if (ip_str.count('::') > 1):\n        return False\n    if (':::' in ip_str):\n        return False\n    if ((ip_str.startswith(':') and (not ip_str.startswith('::'))) or (ip_str.endswith(':') and (not ip_str.endswith('::')))):\n        return False\n    if (('::' not in ip_str) and (ip_str.count(':') != 7)):\n        if (ip_str.count('.') != 3):\n            return False\n    ip_str = self._explode_shorthand_ip_string(ip_str)\n    for hextet in ip_str.split(':'):\n        if (hextet.count('.') == 3):\n            if (not (ip_str.split(':')[(- 1)] == hextet)):\n                return False\n            try:\n                IPv4Network(hextet)\n            except AddressValueError:\n                return False\n        else:\n            try:\n                if ((int(hextet, 16) < 0) or (int(hextet, 16) > 65535)):\n                    return False\n            except ValueError:\n                return False\n    return True\n", "label": 1}
{"function": "\n\ndef showHierarchy(self):\n    'Prints out Framework Hierachy for this framer\\n        '\n    console.terse('\\nFramework Hierarchy for {0}:\\n'.format(self.name))\n    names = self.frameNames\n    tops = [x for x in names.itervalues() if ((not x.over) and x.unders)]\n    console.terse('Tops: {0}\\n'.format(' '.join([x.name for x in tops])))\n    bottoms = [x for x in names.itervalues() if (x.over and (not x.unders))]\n    console.terse('Bottoms: {0}\\n'.format(' '.join([x.name for x in bottoms])))\n    loose = [x for x in names.itervalues() if ((not x.over) and (not x.unders))]\n    console.terse('Loose: {0}\\n'.format(' '.join([x.name for x in loose])))\n    console.terse('Hierarchy: \\n')\n    upper = tops\n    lower = []\n    count = 0\n    while upper:\n        lframes = []\n        for u in upper:\n            path = u.name\n            over = u.over\n            while over:\n                path = ((over.name + '>') + path)\n                over = over.over\n            lframes.append(path)\n        lower = []\n        for u in upper:\n            for b in u.unders:\n                lower.append(b)\n        upper = lower\n        count += 1\n        console.terse('Level {0}: {1}\\n'.format(count, ' '.join(lframes)))\n    console.terse('\\n')\n", "label": 1}
{"function": "\n\ndef mpf_div(s, t, prec, rnd=round_fast):\n    'Floating-point division'\n    (ssign, sman, sexp, sbc) = s\n    (tsign, tman, texp, tbc) = t\n    if ((not sman) or (not tman)):\n        if (s == fzero):\n            if (t == fzero):\n                raise ZeroDivisionError\n            if (t == fnan):\n                return fnan\n            return fzero\n        if (t == fzero):\n            raise ZeroDivisionError\n        s_special = ((not sman) and sexp)\n        t_special = ((not tman) and texp)\n        if (s_special and t_special):\n            return fnan\n        if ((s == fnan) or (t == fnan)):\n            return fnan\n        if (not t_special):\n            if (t == fzero):\n                return fnan\n            return {\n                1: finf,\n                (- 1): fninf,\n            }[(mpf_sign(s) * mpf_sign(t))]\n        return fzero\n    sign = (ssign ^ tsign)\n    if (tman == 1):\n        return normalize1(sign, sman, (sexp - texp), sbc, prec, rnd)\n    extra = (((prec - sbc) + tbc) + 5)\n    if (extra < 5):\n        extra = 5\n    (quot, rem) = divmod((sman << extra), tman)\n    if rem:\n        quot = ((quot << 1) + 1)\n        extra += 1\n        return normalize1(sign, quot, ((sexp - texp) - extra), bitcount(quot), prec, rnd)\n    return normalize(sign, quot, ((sexp - texp) - extra), bitcount(quot), prec, rnd)\n", "label": 1}
{"function": "\n\n@pytest.mark.django_db\ndef test_orm():\n    Conf.ORM = 'default'\n    broker = get_broker(list_key='orm_test')\n    assert (broker.ping() is True)\n    assert (broker.info() is not None)\n    broker.delete_queue()\n    broker.enqueue('test')\n    assert (broker.queue_size() == 1)\n    task = broker.dequeue()[0]\n    assert (task[1] == 'test')\n    broker.acknowledge(task[0])\n    assert (broker.queue_size() == 0)\n    Conf.RETRY = 1\n    broker.enqueue('test')\n    assert (broker.queue_size() == 1)\n    broker.dequeue()\n    assert (broker.queue_size() == 0)\n    sleep(1.5)\n    assert (broker.queue_size() == 1)\n    task = broker.dequeue()[0]\n    assert (broker.queue_size() == 0)\n    broker.acknowledge(task[0])\n    sleep(1.5)\n    assert (broker.queue_size() == 0)\n    task_id = broker.enqueue('test')\n    broker.delete(task_id)\n    assert (broker.dequeue() is None)\n    task_id = broker.enqueue('test')\n    broker.fail(task_id)\n    for i in range(5):\n        broker.enqueue('test')\n    Conf.BULK = 5\n    tasks = broker.dequeue()\n    assert (broker.lock_size() == Conf.BULK)\n    for task in tasks:\n        assert (task is not None)\n        broker.acknowledge(task[0])\n    assert (broker.lock_size() == 0)\n    broker.acknowledge(task[0])\n    broker.enqueue('test')\n    broker.enqueue('test')\n    broker.delete_queue()\n    assert (broker.queue_size() == 0)\n    Conf.ORM = None\n", "label": 1}
{"function": "\n\ndef __update_status(self):\n    if (self.pid is None):\n        if ((self.status == Status.UP) or (self.status == Status.DECOMMISSIONED)):\n            self.status = Status.DOWN\n        return\n    old_status = self.status\n    if common.is_win():\n        self.__update_status_win()\n    else:\n        try:\n            os.kill(self.pid, 0)\n        except OSError as err:\n            if (err.errno == errno.ESRCH):\n                if ((self.status == Status.UP) or (self.status == Status.DECOMMISSIONED)):\n                    self.status = Status.DOWN\n            elif (err.errno == errno.EPERM):\n                if ((self.status == Status.UP) or (self.status == Status.DECOMMISSIONED)):\n                    self.status = Status.DOWN\n            else:\n                raise err\n        else:\n            if ((self.status == Status.DOWN) or (self.status == Status.UNINITIALIZED)):\n                self.status = Status.UP\n    if (not (old_status == self.status)):\n        if ((old_status == Status.UP) and (self.status == Status.DOWN)):\n            self.pid = None\n        self._update_config()\n", "label": 1}
{"function": "\n\ndef _Dynamic_Set(self, request, response):\n    'Implementation of MemcacheService::Set().\\n\\n    Args:\\n      request: A MemcacheSetRequest.\\n      response: A MemcacheSetResponse.\\n    '\n    namespace = request.name_space()\n    for item in request.item_list():\n        key = item.key()\n        set_policy = item.set_policy()\n        old_entry = self._GetKey(namespace, key)\n        set_status = MemcacheSetResponse.NOT_STORED\n        if ((set_policy == MemcacheSetRequest.SET) or ((set_policy == MemcacheSetRequest.ADD) and (old_entry is None)) or ((set_policy == MemcacheSetRequest.REPLACE) and (old_entry is not None))):\n            if ((old_entry is None) or (set_policy == MemcacheSetRequest.SET) or (not old_entry.CheckLocked())):\n                set_status = MemcacheSetResponse.STORED\n        elif ((set_policy == MemcacheSetRequest.CAS) and item.for_cas() and item.has_cas_id()):\n            if ((old_entry is None) or old_entry.CheckLocked()):\n                set_status = MemcacheSetResponse.NOT_STORED\n            elif (old_entry.cas_id != item.cas_id()):\n                set_status = MemcacheSetResponse.EXISTS\n            else:\n                set_status = MemcacheSetResponse.STORED\n        if (set_status == MemcacheSetResponse.STORED):\n            if (namespace not in self._the_cache):\n                self._the_cache[namespace] = {\n                    \n                }\n            self._the_cache[namespace][key] = CacheEntry(item.value(), item.expiration_time(), item.flags(), self._next_cas_id, gettime=self._gettime)\n            self._next_cas_id += 1\n        response.add_set_status(set_status)\n", "label": 1}
{"function": "\n\ndef Chunks(self):\n    for (ident, section) in self.sections.items():\n        (yield section)\n        if (ident == 'functions'):\n            for block in self.GetCollection(vimdoc.FUNCTION):\n                if (('dict' not in block.locals) and ('exception' not in block.locals)):\n                    (yield block)\n        if (ident == 'commands'):\n            for block in self.GetCollection(vimdoc.COMMAND):\n                (yield block)\n        if (ident == 'dicts'):\n            for block in self.GetCollection(vimdoc.DICTIONARY):\n                (yield block)\n                for func in self.GetCollection(vimdoc.FUNCTION):\n                    if (func.locals.get('dict') == block.locals['dict']):\n                        (yield func)\n        if (ident == 'exceptions'):\n            for block in self.GetCollection(vimdoc.EXCEPTION):\n                (yield block)\n        if (ident == 'config'):\n            for block in self.GetCollection(vimdoc.FLAG):\n                (yield block)\n            for block in self.GetCollection(vimdoc.SETTING):\n                (yield block)\n        if (ident in self.backmatters):\n            (yield self.backmatters[ident])\n", "label": 1}
{"function": "\n\ndef l2cn(l1, l2, c, n):\n    ''\n    for i in range(n):\n        c.append(0)\n    f = 0\n    if (f or (n == 8)):\n        c[7] = int(((l2 >> 24) & U32(255)))\n        f = 1\n    if (f or (n == 7)):\n        c[6] = int(((l2 >> 16) & U32(255)))\n        f = 1\n    if (f or (n == 6)):\n        c[5] = int(((l2 >> 8) & U32(255)))\n        f = 1\n    if (f or (n == 5)):\n        c[4] = int((l2 & U32(255)))\n        f = 1\n    if (f or (n == 4)):\n        c[3] = int(((l1 >> 24) & U32(255)))\n        f = 1\n    if (f or (n == 3)):\n        c[2] = int(((l1 >> 16) & U32(255)))\n        f = 1\n    if (f or (n == 2)):\n        c[1] = int(((l1 >> 8) & U32(255)))\n        f = 1\n    if (f or (n == 1)):\n        c[0] = int((l1 & U32(255)))\n        f = 1\n    return c[:n]\n", "label": 1}
{"function": "\n\ndef _break_cycles(self, order, graph):\n    'Keep breaking cycles until the graph is a DAG.\\n        '\n    broken_edges = []\n    strong = [s for s in nx.strongly_connected_components(graph) if (len(s) > 1)]\n    if strong:\n        graph = graph.subgraph(graph.nodes_iter())\n    while strong:\n        in_edges = []\n        start = None\n        if (len(strong[0]) < len(graph)):\n            for s in strong[0]:\n                count = len([u for (u, v) in graph.in_edges(s) if (u not in strong[0])])\n                in_edges.append((count, s))\n            in_edges = sorted(in_edges)\n            if (in_edges[(- 1)][0] > 0):\n                start = in_edges[(- 1)][1]\n        if (start is None):\n            for node in order:\n                if self.pathname:\n                    node = '.'.join((self.pathname, node))\n                if (node in strong[0]):\n                    start = node\n                    break\n        for p in graph.predecessors(start):\n            if (p in strong[0]):\n                graph.remove_edge(p, start)\n                broken_edges.append((p, start))\n        strong = [s for s in nx.strongly_connected_components(graph) if (len(s) > 1)]\n    return (graph, broken_edges)\n", "label": 1}
{"function": "\n\ndef __new__(cls, name, bases, attrs):\n    if (not bases):\n        return super(BaseModel, cls).__new__(cls, name, bases, attrs)\n    ignore = set()\n    primary_key = None\n    for (key, value) in attrs.items():\n        if (isinstance(value, Field) and value._primary_key):\n            primary_key = (key, value)\n    for base in bases:\n        for (key, value) in base.__dict__.items():\n            if (key in attrs):\n                continue\n            if isinstance(value, Field):\n                if (value._primary_key and primary_key):\n                    ignore.add(key)\n                else:\n                    if value._primary_key:\n                        primary_key = (key, value)\n                    attrs[key] = deepcopy(value)\n    if (not primary_key):\n        attrs['_id'] = AutoIncrementField()\n        primary_key = ('_id', attrs['_id'])\n    model_class = super(BaseModel, cls).__new__(cls, name, bases, attrs)\n    model_class._data = None\n    defaults = {\n        \n    }\n    fields = {\n        \n    }\n    indexes = []\n    for (key, value) in model_class.__dict__.items():\n        if (isinstance(value, Field) and (key not in ignore)):\n            value.add_to_class(model_class, key)\n            if value._index:\n                indexes.append(value)\n            fields[key] = value\n            if (value._default is not None):\n                defaults[key] = value._default\n    model_class._defaults = defaults\n    model_class._fields = fields\n    model_class._indexes = indexes\n    model_class._primary_key = primary_key[0]\n    model_class._query = Query(model_class)\n    return model_class\n", "label": 1}
{"function": "\n\ndef remove_template(self, tree):\n    if (self.num_urls < self.min_templates):\n        return False\n    for node in tree.iter():\n        if self.possible_author(node):\n            continue\n        if (node.tag == 'meta'):\n            if (('property' in node.attrib) and ('content' in node.attrib)):\n                fp = (node.attrib['property'], node.attrib['content'])\n                if ((fp in self) and ((self[fp] / self.num_urls) > self.template_proportion)):\n                    node.set('content', '')\n        elif ((node.tag in ['img', 'iframe']) and ('src' in node.attrib)):\n            fp = (node.tag, node.attrib['src'])\n            if ((fp in self) and ((self[fp] / self.num_urls) > self.template_proportion)):\n                node.set('src', '')\n                node.set('alt', '')\n        else:\n            for fp in self.get_fingerprints(node):\n                if ((fp in self) and ((self[fp] / self.num_urls) > self.template_proportion)):\n                    node.text = ''\n                    node.tail = ''\n                    if (node.tag == 'a'):\n                        for child in node.iter():\n                            child.text = ''\n                            child.tail = ''\n    return True\n", "label": 1}
{"function": "\n\ndef _generate_parts(self):\n    'Return a list of callables that can be composed to build a query generator'\n    tokens = self.tokens\n    parts = []\n    self.column_classes = ([self._column_builder(c) for c in tokens.columns] if (tokens.columns != '*') else None)\n    if (not isinstance(tokens.tables[0][0], basestring)):\n        self._table_subquery = Query(tokens.tables[0][0])\n    if tokens.where:\n        func = eval(('lambda row:' + self._filter_builder(tokens.where)))\n        parts.append(partial(Filter, function=func))\n    if tokens.groupby:\n        parts.append(partial(GroupBy, group_by=[c[0] for c in tokens.groupby], columns=self.column_classes))\n    elif (self.column_classes and any(((len(c.name) > 1) for c in tokens.columns))):\n        parts.append(partial(Aggregator, columns=self.column_classes))\n    else:\n        parts.append(partial(Selector, columns=([(c.name[0], c.alias) for c in tokens.columns] if (tokens.columns != '*') else None)))\n    if tokens.orderby:\n        order = tokens.orderby\n        parts.append(partial(OrderBy, order_by=order[0][0], descending=((order[1] == 'DESC') if (len(order) > 1) else False)))\n    if (tokens.limit or tokens.offset):\n        parts.append(partial(LimitOffset, limit=(int(tokens.limit) if tokens.limit else None), offset=(int(tokens.offset) if tokens.offset else 0)))\n    return parts\n", "label": 1}
{"function": "\n\ndef _complement(self, other):\n    if isinstance(other, Interval):\n        nums = sorted((m for m in self.args if m.is_number))\n        if ((other == S.Reals) and (nums != [])):\n            syms = [m for m in self.args if m.is_Symbol]\n            intervals = []\n            intervals += [Interval(S.NegativeInfinity, nums[0], True, True)]\n            for (a, b) in zip(nums[:(- 1)], nums[1:]):\n                intervals.append(Interval(a, b, True, True))\n            intervals.append(Interval(nums[(- 1)], S.Infinity, True, True))\n            if (syms != []):\n                return Complement(Union(intervals, evaluate=False), FiniteSet(*syms), evaluate=False)\n            else:\n                return Union(intervals, evaluate=False)\n        elif (nums == []):\n            return None\n    elif isinstance(other, FiniteSet):\n        unk = []\n        for i in self:\n            c = sympify(other.contains(i))\n            if ((c is not S.true) and (c is not S.false)):\n                unk.append(i)\n        unk = FiniteSet(*unk)\n        if (unk == self):\n            return\n        not_true = []\n        for i in other:\n            c = sympify(self.contains(i))\n            if (c is not S.true):\n                not_true.append(i)\n        return Complement(FiniteSet(*not_true), unk)\n    return Set._complement(self, other)\n", "label": 1}
{"function": "\n\ndef process(self, data):\n    if ((self.state >= HTTP_PARSER_STATE_HEADERS_COMPLETE) and ((self.method == b'POST') or (self.type == HTTP_RESPONSE_PARSER))):\n        if (not self.body):\n            self.body = b''\n        if (b'content-length' in self.headers):\n            self.state = HTTP_PARSER_STATE_RCVING_BODY\n            self.body += data\n            if (len(self.body) >= int(self.headers[b'content-length'][1])):\n                self.state = HTTP_PARSER_STATE_COMPLETE\n        elif ((b'transfer-encoding' in self.headers) and (self.headers[b'transfer-encoding'][1].lower() == b'chunked')):\n            if (not self.chunker):\n                self.chunker = ChunkParser()\n            self.chunker.parse(data)\n            if (self.chunker.state == CHUNK_PARSER_STATE_COMPLETE):\n                self.body = self.chunker.body\n                self.state = HTTP_PARSER_STATE_COMPLETE\n        return (False, b'')\n    (line, data) = HttpParser.split(data)\n    if (line == False):\n        return (line, data)\n    if (self.state < HTTP_PARSER_STATE_LINE_RCVD):\n        self.process_line(line)\n    elif (self.state < HTTP_PARSER_STATE_HEADERS_COMPLETE):\n        self.process_header(line)\n    if ((self.state == HTTP_PARSER_STATE_HEADERS_COMPLETE) and (self.type == HTTP_REQUEST_PARSER) and (not (self.method == b'POST')) and self.raw.endswith((CRLF * 2))):\n        self.state = HTTP_PARSER_STATE_COMPLETE\n    return ((len(data) > 0), data)\n", "label": 1}
{"function": "\n\ndef fill_scan(self, point, old_colour, colour):\n    \"\\n        Scanline flood-fill. Fast, but I'm not entirely sure what it's doing.\\n        \"\n    to_fill = [point]\n    while to_fill:\n        (x, y) = to_fill.pop()\n        while ((y > 0) and (self.pixel(x, (y - 1)) == old_colour)):\n            y -= 1\n        lspan = False\n        rspan = False\n        while ((y < self.size[1]) and (self.pixel(x, y) == old_colour)):\n            self.set_pixel(x, y, colour)\n            if ((not lspan) and (x > 0) and (self.pixel((x - 1), y) == old_colour)):\n                to_fill.append(((x - 1), y))\n                lspan = True\n            elif (lspan and (x > 0) and (self.pixel((x - 1), y) == old_colour)):\n                lspan = False\n            if ((not rspan) and (x < (self.size[0] - 1)) and (self.pixel((x + 1), y) == old_colour)):\n                to_fill.append(((x + 1), y))\n                rspan = True\n            elif (rspan and (x < (self.size[0] - 1)) and (self.pixel((x + 1), y) == old_colour)):\n                rspan = False\n            y += 1\n", "label": 1}
{"function": "\n\ndef column_sql(self, model, field, include_default=False):\n    '\\n        Takes a field and returns its column definition.\\n        The field must already have had set_attributes_from_name called.\\n        '\n    db_params = field.db_parameters(connection=self.connection)\n    sql = db_params['type']\n    params = []\n    if (sql is None):\n        return (None, None)\n    null = field.null\n    include_default = (include_default and (not self.skip_default(field)))\n    if include_default:\n        default_value = self.effective_default(field)\n        if (default_value is not None):\n            if self.connection.features.requires_literal_defaults:\n                sql += (' DEFAULT %s' % self.prepare_default(default_value))\n            else:\n                sql += ' DEFAULT %s'\n                params += [default_value]\n    if (field.empty_strings_allowed and (not field.primary_key) and self.connection.features.interprets_empty_strings_as_nulls):\n        null = True\n    if (null and (not self.connection.features.implied_column_null)):\n        sql += ' NULL'\n    elif (not null):\n        sql += ' NOT NULL'\n    if field.primary_key:\n        sql += ' PRIMARY KEY'\n    elif field.unique:\n        sql += ' UNIQUE'\n    tablespace = (field.db_tablespace or model._meta.db_tablespace)\n    if (tablespace and self.connection.features.supports_tablespaces and field.unique):\n        sql += (' %s' % self.connection.ops.tablespace_sql(tablespace, inline=True))\n    return (sql, params)\n", "label": 1}
{"function": "\n\ndef test_bool():\n    assert (Eq(0, 0) is S.true)\n    assert (Eq(1, 0) is S.false)\n    assert (Ne(0, 0) is S.false)\n    assert (Ne(1, 0) is S.true)\n    assert (Lt(0, 1) is S.true)\n    assert (Lt(1, 0) is S.false)\n    assert (Le(0, 1) is S.true)\n    assert (Le(1, 0) is S.false)\n    assert (Le(0, 0) is S.true)\n    assert (Gt(1, 0) is S.true)\n    assert (Gt(0, 1) is S.false)\n    assert (Ge(1, 0) is S.true)\n    assert (Ge(0, 1) is S.false)\n    assert (Ge(1, 1) is S.true)\n    assert (Eq(I, 2) is S.false)\n    assert (Ne(I, 2) is S.true)\n    raises(TypeError, (lambda : Gt(I, 2)))\n    raises(TypeError, (lambda : Ge(I, 2)))\n    raises(TypeError, (lambda : Lt(I, 2)))\n    raises(TypeError, (lambda : Le(I, 2)))\n    a = Float('.000000000000000000001', '')\n    b = Float('.0000000000000000000001', '')\n    assert (Eq((pi + a), (pi + b)) is S.false)\n", "label": 1}
{"function": "\n\ndef _filter_columns(self, index, col_names):\n    'Returns the column names specified by index (which may be a slice)'\n    if isinstance(index, slice):\n        cols = [col for col in sorted(col_names)]\n        if index.start:\n            cols = [col for col in cols if (col > index.start)]\n        if index.stop:\n            cols = [col for col in cols if (col < index.stop)]\n        cols = (cols[::index.step] if index.step else cols)\n    elif isinstance(index, (set, list)):\n        nomatch = [val for val in index if (val not in col_names)]\n        if nomatch:\n            raise KeyError(('No columns with dimension labels %r' % nomatch))\n        cols = [col for col in col_names if (col in index)]\n    elif (index not in col_names):\n        raise KeyError(('No column with dimension label %r' % index))\n    else:\n        cols = [index]\n    if (cols == []):\n        raise KeyError('No columns selected in the given slice')\n    return cols\n", "label": 1}
{"function": "\n\ndef test(self):\n    m = range(10)\n    n = range(10)\n    for i in xrange(self.rounds):\n        l = [x for x in n for y in m]\n        l = [y for x in n for y in m]\n        l = [x for x in n for y in m if y]\n        l = [y for x in n for y in m if x]\n        l = [x for x in n for y in m if (not y)]\n        l = [y for x in n for y in m if (not x)]\n", "label": 1}
{"function": "\n\ndef spec_check(self, auth_list, fun, form):\n    '\\n        Check special API permissions\\n        '\n    if (form != 'cloud'):\n        comps = fun.split('.')\n        if (len(comps) != 2):\n            return False\n        mod = comps[0]\n        fun = comps[1]\n    else:\n        mod = fun\n    for ind in auth_list:\n        if isinstance(ind, six.string_types):\n            if (ind.startswith('@') and (ind[1:] == mod)):\n                return True\n            if (ind == '@{0}'.format(form)):\n                return True\n            if (ind == '@{0}s'.format(form)):\n                return True\n        elif isinstance(ind, dict):\n            if (len(ind) != 1):\n                continue\n            valid = next(six.iterkeys(ind))\n            if (valid.startswith('@') and (valid[1:] == mod)):\n                if isinstance(ind[valid], six.string_types):\n                    if self.match_check(ind[valid], fun):\n                        return True\n                elif isinstance(ind[valid], list):\n                    for regex in ind[valid]:\n                        if self.match_check(regex, fun):\n                            return True\n    return False\n", "label": 1}
{"function": "\n\ndef ok(self, event=None):\n    selection = self.treeView.selection()\n    if (len(selection) > 0):\n        if hasattr(self, 'taxonomyPackage'):\n            self.filesource.mappedPaths = self.taxonomyPackage['remappings']\n        filename = None\n        if (self.openType in (ARCHIVE, DISCLOSURE_SYSTEM)):\n            filename = self.filenames[int(selection[0][4:])]\n            if isinstance(filename, tuple):\n                if self.isRss:\n                    filename = filename[4]\n                else:\n                    filename = filename[0]\n        elif (self.openType == ENTRY_POINTS):\n            epName = selection[0]\n            filename = self.taxonomyPackage['nameToUrls'][epName][0]\n            if (not filename.endswith('/')):\n                if ((not isHttpUrl(filename)) and (self.metadataFilePrefix != self.taxonomyPkgMetaInf)):\n                    filename = (self.metadataFilePrefix + filename)\n        elif (self.openType == PLUGIN):\n            filename = self.filenames[int(selection[0][4:])][2]\n        if ((filename is not None) and (not filename.endswith('/'))):\n            if hasattr(self, 'taxonomyPackage'):\n                for (prefix, remapping) in self.taxonomyPackage['remappings'].items():\n                    if isHttpUrl(remapping):\n                        remapStart = remapping\n                    else:\n                        remapStart = (self.metadataFilePrefix + remapping)\n                    if filename.startswith(remapStart):\n                        filename = (prefix + filename[len(remapStart):])\n                        break\n            if (self.openType == PLUGIN):\n                self.filesource.selection = filename\n            else:\n                self.filesource.select(filename)\n            self.accepted = True\n            self.close()\n", "label": 1}
{"function": "\n\ndef __call__(self):\n    logger.info('Running source code watcher')\n    while True:\n        app_path = os.path.abspath(self.path)\n        css_path = os.path.join(app_path, 'static', 'css')\n        js_path = os.path.join(app_path, 'static', 'js')\n        static_files = [f for f in glob.glob((css_path + '/*')) if os.path.isfile(f)]\n        static_files += [f for f in glob.glob((css_path + '/**/*')) if os.path.isfile(f)]\n        static_files += [f for f in glob.glob((js_path + '/*')) if os.path.isfile(f)]\n        static_files += [f for f in glob.glob((js_path + '/**/*')) if os.path.isfile(f)]\n        for f in static_files:\n            try:\n                mtime = os.path.getmtime(f)\n                if ((f not in self.statics) or (mtime > self.statics[f]['mtime'])):\n                    if ((f in self.statics) and (mtime > self.statics[f]['mtime'])):\n                        logger.debug('Found new file update for: {0}'.format(f))\n                    if f.startswith(css_path):\n                        data = self.process_css(f)\n                    elif f.startswith(js_path):\n                        data = self.process_js(f)\n                    self.statics[f] = {\n                        'mtime': mtime,\n                        'data': data,\n                    }\n            except OSError as e:\n                pass\n        time.sleep(1.0)\n", "label": 1}
{"function": "\n\n@pytest.mark.parametrize('colors', [True, False, None])\n@pytest.mark.parametrize('light_bg', [True, False, None])\ndef test_piped(colors, light_bg):\n    'Test script with output piped to non-tty (this pytest process).\\n\\n    :param bool colors: Enable, disable, or omit color arguments (default is no colors due to no tty).\\n    :param bool light_bg: Enable light, dark, or omit light/dark arguments.\\n    '\n    command = [sys.executable, str(PROJECT_ROOT.join('example.py')), 'print']\n    if (colors is True):\n        command.append('--colors')\n    elif (colors is False):\n        command.append('--no-colors')\n    if (light_bg is True):\n        command.append('--light-bg')\n    elif (light_bg is False):\n        command.append('--dark-bg')\n    proc = subprocess.Popen(command, stderr=subprocess.STDOUT, stdout=subprocess.PIPE)\n    output = proc.communicate()[0].decode()\n    assert (proc.poll() == 0)\n    assert ('Autocolors for all backgrounds' in output)\n    assert ('Red' in output)\n    if ((not colors) or IS_WINDOWS):\n        assert ('\\x1b[' not in output)\n        assert ('Black Red Green Yellow Blue Magenta Cyan White' in output)\n        return\n    assert ('\\x1b[' in output)\n    count_dark_fg = output.count('\\x1b[31mRed')\n    count_light_fg = output.count('\\x1b[91mRed')\n    if light_bg:\n        assert (count_dark_fg == 2)\n        assert (count_light_fg == 1)\n    else:\n        assert (count_dark_fg == 1)\n        assert (count_light_fg == 2)\n", "label": 1}
{"function": "\n\ndef _print_Mul(self, expr):\n    prec = precedence(expr)\n    (c, e) = expr.as_coeff_Mul()\n    if (c < 0):\n        expr = _keep_coeff((- c), e)\n        sign = '-'\n    else:\n        sign = ''\n    a = []\n    b = []\n    if (self.order not in ('old', 'none')):\n        args = expr.as_ordered_factors()\n    else:\n        args = Mul.make_args(expr)\n    for item in args:\n        if (item.is_commutative and item.is_Pow and item.exp.is_Rational and item.exp.is_negative):\n            if (item.exp != (- 1)):\n                b.append(Pow(item.base, (- item.exp), evaluate=False))\n            else:\n                b.append(Pow(item.base, (- item.exp)))\n        elif (item.is_Rational and (item is not S.Infinity)):\n            if (item.p != 1):\n                a.append(Rational(item.p))\n            if (item.q != 1):\n                b.append(Rational(item.q))\n        else:\n            a.append(item)\n    a = (a or [S.One])\n    a_str = [self.parenthesize(x, prec, strict=False) for x in a]\n    b_str = [self.parenthesize(x, prec, strict=False) for x in b]\n    if (len(b) == 0):\n        return (sign + '*'.join(a_str))\n    elif (len(b) == 1):\n        return (((sign + '*'.join(a_str)) + '/') + b_str[0])\n    else:\n        return ((sign + '*'.join(a_str)) + ('/(%s)' % '*'.join(b_str)))\n", "label": 1}
{"function": "\n\ndef test_basic(self):\n    wo = WalkOptions()\n    assert wo\n    assert (wo.transfer_penalty == 0)\n    assert (wo.turn_penalty == 0)\n    assert (((wo.walking_speed * 100) // 1) == 607.0)\n    assert (wo.walking_reluctance == 1.0)\n    assert (wo.max_walk == 10000)\n    assert (round(wo.walking_overage, 3) == 0.1)\n    wo.transfer_penalty = 50\n    assert (wo.transfer_penalty == 50)\n    wo.turn_penalty = 3\n    assert (wo.turn_penalty == 3)\n    wo.walking_speed = 1.05\n    assert (round((wo.walking_speed * 100)) == 105.0)\n    wo.walking_reluctance = 2.0\n    assert (wo.walking_reluctance == 2.0)\n    wo.max_walk = 100\n    assert (wo.max_walk == 100)\n    wo.walking_overage = 1.0\n    assert (wo.walking_overage == 1.0)\n    wo.uphill_slowness = 1.5\n    assert (wo.uphill_slowness == 1.5)\n    wo.downhill_fastness = 3.4\n    assert (round(wo.downhill_fastness, 3) == 3.4)\n    wo.hill_reluctance = 1.4\n    assert (round(wo.hill_reluctance, 3) == 1.4)\n    wo.destroy()\n    assert (wo.soul == None)\n", "label": 1}
{"function": "\n\n@feature('cc', 'cxx')\n@after('apply_type_vars', 'apply_lib_vars', 'apply_core')\ndef apply_incpaths(self):\n    'used by the scanner\\n\\tafter processing the uselib for CPPPATH\\n\\tafter apply_core because some processing may add include paths\\n\\t'\n    lst = []\n    for lib in self.to_list(self.uselib):\n        for path in self.env[('CPPPATH_' + lib)]:\n            if (not (path in lst)):\n                lst.append(path)\n    if preproc.go_absolute:\n        for path in preproc.standard_includes:\n            if (not (path in lst)):\n                lst.append(path)\n    for path in self.to_list(self.includes):\n        if (not (path in lst)):\n            if (preproc.go_absolute or (not os.path.isabs(path))):\n                lst.append(path)\n            else:\n                self.env.prepend_value('CPPPATH', path)\n    for path in lst:\n        node = None\n        if os.path.isabs(path):\n            if preproc.go_absolute:\n                node = self.bld.root.find_dir(path)\n        elif (path[0] == '#'):\n            node = self.bld.srcnode\n            if (len(path) > 1):\n                node = node.find_dir(path[1:])\n        else:\n            node = self.path.find_dir(path)\n        if node:\n            self.env.append_value('INC_PATHS', node)\n    if USE_TOP_LEVEL:\n        self.env.append_value('INC_PATHS', self.bld.srcnode)\n", "label": 1}
{"function": "\n\ndef _generate_report_test(self, rows, cid, tid, n, t, o, e):\n    has_output = bool((o or e))\n    tid = ((((n == 0) and 'p') or 'f') + ('t%s.%s' % ((cid + 1), (tid + 1))))\n    name = t.id().split('.')[(- 1)]\n    doc = (t.shortDescription() or '')\n    desc = ((doc and ('%s: %s' % (name, doc))) or name)\n    tmpl = ((has_output and self.REPORT_TEST_WITH_OUTPUT_TMPL) or self.REPORT_TEST_NO_OUTPUT_TMPL)\n    if isinstance(o, str):\n        uo = o.decode('latin-1')\n    else:\n        uo = o\n    if isinstance(e, str):\n        ue = e.decode('latin-1')\n    else:\n        ue = e\n    script = (self.REPORT_TEST_OUTPUT_TMPL % dict(id=tid, output=(uo + ue)))\n    row = (tmpl % dict(tid=tid, Class=(((n == 0) and 'hiddenRow') or 'none'), style=(((n == 2) and 'errorCase') or (((n == 1) and 'failCase') or 'none')), desc=desc, script=script, status=self.STATUS[n]))\n    rows.append(row)\n    if (not has_output):\n        return\n", "label": 1}
{"function": "\n\ndef action(self, **kwa):\n    '\\n        Join with all masters\\n        '\n    stack = self.stack.value\n    if (stack and isinstance(stack, RoadStack)):\n        refresh_masters = (self.opts.value.get('raet_clear_remote_masters', True) or (not stack.remotes))\n        refresh_all = (self.opts.value.get('raet_clear_remotes', True) or (not stack.remotes))\n        if refresh_masters:\n            for remote in list(stack.remotes.values()):\n                if (remote.kind == kinds.applKinds.master):\n                    stack.removeRemote(remote, clear=True)\n        if refresh_all:\n            for remote in list(stack.remotes.values()):\n                stack.removeRemote(remote, clear=True)\n        if (refresh_all or refresh_masters):\n            stack.puid = stack.Uid\n            ex = SaltException('Unable to connect to any master')\n            for master in self.ushers.value:\n                try:\n                    mha = master['external']\n                    stack.addRemote(RemoteEstate(stack=stack, fuid=0, sid=0, ha=mha, kind=kinds.applKinds.master))\n                except gaierror as ex:\n                    log.warning('Unable to connect to master {0}: {1}'.format(mha, ex))\n                    if (self.opts.value.get('master_type') != 'failover'):\n                        raise ex\n            if (not stack.remotes):\n                raise ex\n        for remote in list(stack.remotes.values()):\n            if (remote.kind == kinds.applKinds.master):\n                stack.join(uid=remote.uid, timeout=0.0)\n", "label": 1}
{"function": "\n\ndef visit(self, visitor, pipeline, visited):\n    'Visits all nodes reachable from the current node.'\n    for pval in self.inputs:\n        if ((pval not in visited) and (not isinstance(pval, pvalue.PBegin))):\n            assert (pval.producer is not None)\n            pval.producer.visit(visitor, pipeline, visited)\n            assert (pval in visited)\n    for pval in self.side_inputs:\n        if (isinstance(pval, pvalue.PCollectionView) and (pval not in visited)):\n            assert (pval.producer is not None)\n            pval.producer.visit(visitor, pipeline, visited)\n            assert (pval in visited)\n    if self.is_composite():\n        visitor.enter_composite_transform(self)\n        for part in self.parts:\n            part.visit(visitor, pipeline, visited)\n        visitor.leave_composite_transform(self)\n    else:\n        visitor.visit_transform(self)\n    for pval in self.outputs:\n        if isinstance(pval, pvalue.DoOutputsTuple):\n            pvals = (v for v in pval)\n        else:\n            pvals = (pval,)\n        for v in pvals:\n            if (v not in visited):\n                visited.add(v)\n                visitor.visit_value(v, self)\n", "label": 1}
{"function": "\n\ndef _group_fieldsets(self, fieldsets):\n    if ((not self.declared_fieldsets) and (self.group_fieldsets is True)):\n        flattened_fieldsets = flatten_fieldsets(fieldsets)\n        untranslated_fields = [f.name for f in self.opts.fields if ((f is not self.opts.auto_field) and f.editable and (not hasattr(f, 'translated_field')) and (f.name in flattened_fieldsets))]\n        fieldsets = ([('', {\n            'fields': untranslated_fields,\n        })] if untranslated_fields else [])\n        temp_fieldsets = {\n            \n        }\n        for (orig_field, trans_fields) in self.trans_opts.fields.items():\n            trans_fieldnames = [f.name for f in sorted(trans_fields, key=(lambda x: x.name))]\n            if any(((f in trans_fieldnames) for f in flattened_fieldsets)):\n                label = self.model._meta.get_field(orig_field).verbose_name.capitalize()\n                temp_fieldsets[orig_field] = (label, {\n                    'fields': trans_fieldnames,\n                    'classes': ('mt-fieldset',),\n                })\n        fields_order = unique((f.translated_field.name for f in self.opts.fields if (hasattr(f, 'translated_field') and (f.name in flattened_fieldsets))))\n        for field_name in fields_order:\n            fieldsets.append(temp_fieldsets.pop(field_name))\n        assert (not temp_fieldsets)\n    return fieldsets\n", "label": 1}
{"function": "\n\ndef check_ops_properties(self, props, filter=None, ignore_failures=False):\n    for op in props:\n        for o in self.is_valid_objs:\n            if (filter is not None):\n                filt = (o.index if isinstance(o, Series) else o)\n                if (not filter(filt)):\n                    continue\n            try:\n                if isinstance(o, Series):\n                    expected = Series(getattr(o.index, op), index=o.index, name='a')\n                else:\n                    expected = getattr(o, op)\n            except AttributeError:\n                if ignore_failures:\n                    continue\n            result = getattr(o, op)\n            if (isinstance(result, Series) and isinstance(expected, Series)):\n                tm.assert_series_equal(result, expected)\n            elif (isinstance(result, Index) and isinstance(expected, Index)):\n                tm.assert_index_equal(result, expected)\n            elif (isinstance(result, np.ndarray) and isinstance(expected, np.ndarray)):\n                self.assert_numpy_array_equal(result, expected)\n            else:\n                self.assertEqual(result, expected)\n        if (not ignore_failures):\n            for o in self.not_valid_objs:\n                if issubclass(type(o), DatetimeIndexOpsMixin):\n                    self.assertRaises(TypeError, (lambda : getattr(o, op)))\n                else:\n                    self.assertRaises(AttributeError, (lambda : getattr(o, op)))\n", "label": 1}
{"function": "\n\ndef _setup_subnet_parameters(self, params, data, is_create=True):\n    'Setup subnet parameters\\n\\n        This methods setups subnet parameters which are available\\n        in both create and update.\\n        '\n    is_update = (not is_create)\n    params['enable_dhcp'] = data['enable_dhcp']\n    if (int(data['ip_version']) == 6):\n        ipv6_modes = utils.get_ipv6_modes_attrs_from_menu(data['ipv6_modes'])\n        if (ipv6_modes[0] or is_update):\n            params['ipv6_ra_mode'] = ipv6_modes[0]\n        if (ipv6_modes[1] or is_update):\n            params['ipv6_address_mode'] = ipv6_modes[1]\n    if (is_create and data['allocation_pools']):\n        pools = [dict(zip(['start', 'end'], pool.strip().split(','))) for pool in data['allocation_pools'].split('\\n') if pool.strip()]\n        params['allocation_pools'] = pools\n    if (data['host_routes'] or is_update):\n        routes = [dict(zip(['destination', 'nexthop'], route.strip().split(','))) for route in data['host_routes'].split('\\n') if route.strip()]\n        params['host_routes'] = routes\n    if (data['dns_nameservers'] or is_update):\n        nameservers = [ns.strip() for ns in data['dns_nameservers'].split('\\n') if ns.strip()]\n        params['dns_nameservers'] = nameservers\n", "label": 1}
{"function": "\n\ndef set_align(self, alignment):\n    '\\n        Set the Format cell alignment.\\n\\n        Args:\\n            alignment: String representing alignment. No default.\\n\\n        Returns:\\n            Nothing.\\n        '\n    alignment = alignment.lower()\n    if (alignment == 'left'):\n        self.set_text_h_align(1)\n    if (alignment == 'centre'):\n        self.set_text_h_align(2)\n    if (alignment == 'center'):\n        self.set_text_h_align(2)\n    if (alignment == 'right'):\n        self.set_text_h_align(3)\n    if (alignment == 'fill'):\n        self.set_text_h_align(4)\n    if (alignment == 'justify'):\n        self.set_text_h_align(5)\n    if (alignment == 'center_across'):\n        self.set_text_h_align(6)\n    if (alignment == 'centre_across'):\n        self.set_text_h_align(6)\n    if (alignment == 'distributed'):\n        self.set_text_h_align(7)\n    if (alignment == 'justify_distributed'):\n        self.set_text_h_align(7)\n    if (alignment == 'justify_distributed'):\n        self.just_distrib = 1\n    if (alignment == 'top'):\n        self.set_text_v_align(1)\n    if (alignment == 'vcentre'):\n        self.set_text_v_align(2)\n    if (alignment == 'vcenter'):\n        self.set_text_v_align(2)\n    if (alignment == 'bottom'):\n        self.set_text_v_align(3)\n    if (alignment == 'vjustify'):\n        self.set_text_v_align(4)\n    if (alignment == 'vdistributed'):\n        self.set_text_v_align(5)\n", "label": 1}
